[
    {
        "func_name": "__init__",
        "original": "def __init__(self, k_endog, k_states, k_posdef=None, results_class=None, kalman_smoother_classes=None, **kwargs):\n    if results_class is None:\n        results_class = SmootherResults\n    keys = ['smoother_output'] + KalmanSmoother.smoother_outputs\n    smoother_output_kwargs = {key: kwargs.pop(key) for key in keys if key in kwargs}\n    keys = ['smooth_method'] + KalmanSmoother.smooth_methods\n    smooth_method_kwargs = {key: kwargs.pop(key) for key in keys if key in kwargs}\n    super(KalmanSmoother, self).__init__(k_endog, k_states, k_posdef, results_class=results_class, **kwargs)\n    self.prefix_kalman_smoother_map = kalman_smoother_classes if kalman_smoother_classes is not None else tools.prefix_kalman_smoother_map.copy()\n    self._kalman_smoothers = {}\n    self.set_smoother_output(**smoother_output_kwargs)\n    self.set_smooth_method(**smooth_method_kwargs)",
        "mutated": [
            "def __init__(self, k_endog, k_states, k_posdef=None, results_class=None, kalman_smoother_classes=None, **kwargs):\n    if False:\n        i = 10\n    if results_class is None:\n        results_class = SmootherResults\n    keys = ['smoother_output'] + KalmanSmoother.smoother_outputs\n    smoother_output_kwargs = {key: kwargs.pop(key) for key in keys if key in kwargs}\n    keys = ['smooth_method'] + KalmanSmoother.smooth_methods\n    smooth_method_kwargs = {key: kwargs.pop(key) for key in keys if key in kwargs}\n    super(KalmanSmoother, self).__init__(k_endog, k_states, k_posdef, results_class=results_class, **kwargs)\n    self.prefix_kalman_smoother_map = kalman_smoother_classes if kalman_smoother_classes is not None else tools.prefix_kalman_smoother_map.copy()\n    self._kalman_smoothers = {}\n    self.set_smoother_output(**smoother_output_kwargs)\n    self.set_smooth_method(**smooth_method_kwargs)",
            "def __init__(self, k_endog, k_states, k_posdef=None, results_class=None, kalman_smoother_classes=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if results_class is None:\n        results_class = SmootherResults\n    keys = ['smoother_output'] + KalmanSmoother.smoother_outputs\n    smoother_output_kwargs = {key: kwargs.pop(key) for key in keys if key in kwargs}\n    keys = ['smooth_method'] + KalmanSmoother.smooth_methods\n    smooth_method_kwargs = {key: kwargs.pop(key) for key in keys if key in kwargs}\n    super(KalmanSmoother, self).__init__(k_endog, k_states, k_posdef, results_class=results_class, **kwargs)\n    self.prefix_kalman_smoother_map = kalman_smoother_classes if kalman_smoother_classes is not None else tools.prefix_kalman_smoother_map.copy()\n    self._kalman_smoothers = {}\n    self.set_smoother_output(**smoother_output_kwargs)\n    self.set_smooth_method(**smooth_method_kwargs)",
            "def __init__(self, k_endog, k_states, k_posdef=None, results_class=None, kalman_smoother_classes=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if results_class is None:\n        results_class = SmootherResults\n    keys = ['smoother_output'] + KalmanSmoother.smoother_outputs\n    smoother_output_kwargs = {key: kwargs.pop(key) for key in keys if key in kwargs}\n    keys = ['smooth_method'] + KalmanSmoother.smooth_methods\n    smooth_method_kwargs = {key: kwargs.pop(key) for key in keys if key in kwargs}\n    super(KalmanSmoother, self).__init__(k_endog, k_states, k_posdef, results_class=results_class, **kwargs)\n    self.prefix_kalman_smoother_map = kalman_smoother_classes if kalman_smoother_classes is not None else tools.prefix_kalman_smoother_map.copy()\n    self._kalman_smoothers = {}\n    self.set_smoother_output(**smoother_output_kwargs)\n    self.set_smooth_method(**smooth_method_kwargs)",
            "def __init__(self, k_endog, k_states, k_posdef=None, results_class=None, kalman_smoother_classes=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if results_class is None:\n        results_class = SmootherResults\n    keys = ['smoother_output'] + KalmanSmoother.smoother_outputs\n    smoother_output_kwargs = {key: kwargs.pop(key) for key in keys if key in kwargs}\n    keys = ['smooth_method'] + KalmanSmoother.smooth_methods\n    smooth_method_kwargs = {key: kwargs.pop(key) for key in keys if key in kwargs}\n    super(KalmanSmoother, self).__init__(k_endog, k_states, k_posdef, results_class=results_class, **kwargs)\n    self.prefix_kalman_smoother_map = kalman_smoother_classes if kalman_smoother_classes is not None else tools.prefix_kalman_smoother_map.copy()\n    self._kalman_smoothers = {}\n    self.set_smoother_output(**smoother_output_kwargs)\n    self.set_smooth_method(**smooth_method_kwargs)",
            "def __init__(self, k_endog, k_states, k_posdef=None, results_class=None, kalman_smoother_classes=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if results_class is None:\n        results_class = SmootherResults\n    keys = ['smoother_output'] + KalmanSmoother.smoother_outputs\n    smoother_output_kwargs = {key: kwargs.pop(key) for key in keys if key in kwargs}\n    keys = ['smooth_method'] + KalmanSmoother.smooth_methods\n    smooth_method_kwargs = {key: kwargs.pop(key) for key in keys if key in kwargs}\n    super(KalmanSmoother, self).__init__(k_endog, k_states, k_posdef, results_class=results_class, **kwargs)\n    self.prefix_kalman_smoother_map = kalman_smoother_classes if kalman_smoother_classes is not None else tools.prefix_kalman_smoother_map.copy()\n    self._kalman_smoothers = {}\n    self.set_smoother_output(**smoother_output_kwargs)\n    self.set_smooth_method(**smooth_method_kwargs)"
        ]
    },
    {
        "func_name": "_clone_kwargs",
        "original": "def _clone_kwargs(self, endog, **kwargs):\n    kwargs = super(KalmanSmoother, self)._clone_kwargs(endog, **kwargs)\n    kwargs.setdefault('smoother_output', self.smoother_output)\n    kwargs.setdefault('smooth_method', self.smooth_method)\n    return kwargs",
        "mutated": [
            "def _clone_kwargs(self, endog, **kwargs):\n    if False:\n        i = 10\n    kwargs = super(KalmanSmoother, self)._clone_kwargs(endog, **kwargs)\n    kwargs.setdefault('smoother_output', self.smoother_output)\n    kwargs.setdefault('smooth_method', self.smooth_method)\n    return kwargs",
            "def _clone_kwargs(self, endog, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    kwargs = super(KalmanSmoother, self)._clone_kwargs(endog, **kwargs)\n    kwargs.setdefault('smoother_output', self.smoother_output)\n    kwargs.setdefault('smooth_method', self.smooth_method)\n    return kwargs",
            "def _clone_kwargs(self, endog, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    kwargs = super(KalmanSmoother, self)._clone_kwargs(endog, **kwargs)\n    kwargs.setdefault('smoother_output', self.smoother_output)\n    kwargs.setdefault('smooth_method', self.smooth_method)\n    return kwargs",
            "def _clone_kwargs(self, endog, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    kwargs = super(KalmanSmoother, self)._clone_kwargs(endog, **kwargs)\n    kwargs.setdefault('smoother_output', self.smoother_output)\n    kwargs.setdefault('smooth_method', self.smooth_method)\n    return kwargs",
            "def _clone_kwargs(self, endog, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    kwargs = super(KalmanSmoother, self)._clone_kwargs(endog, **kwargs)\n    kwargs.setdefault('smoother_output', self.smoother_output)\n    kwargs.setdefault('smooth_method', self.smooth_method)\n    return kwargs"
        ]
    },
    {
        "func_name": "_kalman_smoother",
        "original": "@property\ndef _kalman_smoother(self):\n    prefix = self.prefix\n    if prefix in self._kalman_smoothers:\n        return self._kalman_smoothers[prefix]\n    return None",
        "mutated": [
            "@property\ndef _kalman_smoother(self):\n    if False:\n        i = 10\n    prefix = self.prefix\n    if prefix in self._kalman_smoothers:\n        return self._kalman_smoothers[prefix]\n    return None",
            "@property\ndef _kalman_smoother(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    prefix = self.prefix\n    if prefix in self._kalman_smoothers:\n        return self._kalman_smoothers[prefix]\n    return None",
            "@property\ndef _kalman_smoother(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    prefix = self.prefix\n    if prefix in self._kalman_smoothers:\n        return self._kalman_smoothers[prefix]\n    return None",
            "@property\ndef _kalman_smoother(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    prefix = self.prefix\n    if prefix in self._kalman_smoothers:\n        return self._kalman_smoothers[prefix]\n    return None",
            "@property\ndef _kalman_smoother(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    prefix = self.prefix\n    if prefix in self._kalman_smoothers:\n        return self._kalman_smoothers[prefix]\n    return None"
        ]
    },
    {
        "func_name": "_initialize_smoother",
        "original": "def _initialize_smoother(self, smoother_output=None, smooth_method=None, prefix=None, **kwargs):\n    if smoother_output is None:\n        smoother_output = self.smoother_output\n    if smooth_method is None:\n        smooth_method = self.smooth_method\n    (prefix, dtype, create_filter, create_statespace) = self._initialize_filter(prefix, **kwargs)\n    create_smoother = create_filter or prefix not in self._kalman_smoothers\n    if not create_smoother:\n        kalman_smoother = self._kalman_smoothers[prefix]\n        create_smoother = kalman_smoother.kfilter is not self._kalman_filters[prefix]\n    if create_smoother:\n        cls = self.prefix_kalman_smoother_map[prefix]\n        self._kalman_smoothers[prefix] = cls(self._statespaces[prefix], self._kalman_filters[prefix], smoother_output, smooth_method)\n    else:\n        self._kalman_smoothers[prefix].set_smoother_output(smoother_output, False)\n        self._kalman_smoothers[prefix].set_smooth_method(smooth_method)\n    return (prefix, dtype, create_smoother, create_filter, create_statespace)",
        "mutated": [
            "def _initialize_smoother(self, smoother_output=None, smooth_method=None, prefix=None, **kwargs):\n    if False:\n        i = 10\n    if smoother_output is None:\n        smoother_output = self.smoother_output\n    if smooth_method is None:\n        smooth_method = self.smooth_method\n    (prefix, dtype, create_filter, create_statespace) = self._initialize_filter(prefix, **kwargs)\n    create_smoother = create_filter or prefix not in self._kalman_smoothers\n    if not create_smoother:\n        kalman_smoother = self._kalman_smoothers[prefix]\n        create_smoother = kalman_smoother.kfilter is not self._kalman_filters[prefix]\n    if create_smoother:\n        cls = self.prefix_kalman_smoother_map[prefix]\n        self._kalman_smoothers[prefix] = cls(self._statespaces[prefix], self._kalman_filters[prefix], smoother_output, smooth_method)\n    else:\n        self._kalman_smoothers[prefix].set_smoother_output(smoother_output, False)\n        self._kalman_smoothers[prefix].set_smooth_method(smooth_method)\n    return (prefix, dtype, create_smoother, create_filter, create_statespace)",
            "def _initialize_smoother(self, smoother_output=None, smooth_method=None, prefix=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if smoother_output is None:\n        smoother_output = self.smoother_output\n    if smooth_method is None:\n        smooth_method = self.smooth_method\n    (prefix, dtype, create_filter, create_statespace) = self._initialize_filter(prefix, **kwargs)\n    create_smoother = create_filter or prefix not in self._kalman_smoothers\n    if not create_smoother:\n        kalman_smoother = self._kalman_smoothers[prefix]\n        create_smoother = kalman_smoother.kfilter is not self._kalman_filters[prefix]\n    if create_smoother:\n        cls = self.prefix_kalman_smoother_map[prefix]\n        self._kalman_smoothers[prefix] = cls(self._statespaces[prefix], self._kalman_filters[prefix], smoother_output, smooth_method)\n    else:\n        self._kalman_smoothers[prefix].set_smoother_output(smoother_output, False)\n        self._kalman_smoothers[prefix].set_smooth_method(smooth_method)\n    return (prefix, dtype, create_smoother, create_filter, create_statespace)",
            "def _initialize_smoother(self, smoother_output=None, smooth_method=None, prefix=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if smoother_output is None:\n        smoother_output = self.smoother_output\n    if smooth_method is None:\n        smooth_method = self.smooth_method\n    (prefix, dtype, create_filter, create_statespace) = self._initialize_filter(prefix, **kwargs)\n    create_smoother = create_filter or prefix not in self._kalman_smoothers\n    if not create_smoother:\n        kalman_smoother = self._kalman_smoothers[prefix]\n        create_smoother = kalman_smoother.kfilter is not self._kalman_filters[prefix]\n    if create_smoother:\n        cls = self.prefix_kalman_smoother_map[prefix]\n        self._kalman_smoothers[prefix] = cls(self._statespaces[prefix], self._kalman_filters[prefix], smoother_output, smooth_method)\n    else:\n        self._kalman_smoothers[prefix].set_smoother_output(smoother_output, False)\n        self._kalman_smoothers[prefix].set_smooth_method(smooth_method)\n    return (prefix, dtype, create_smoother, create_filter, create_statespace)",
            "def _initialize_smoother(self, smoother_output=None, smooth_method=None, prefix=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if smoother_output is None:\n        smoother_output = self.smoother_output\n    if smooth_method is None:\n        smooth_method = self.smooth_method\n    (prefix, dtype, create_filter, create_statespace) = self._initialize_filter(prefix, **kwargs)\n    create_smoother = create_filter or prefix not in self._kalman_smoothers\n    if not create_smoother:\n        kalman_smoother = self._kalman_smoothers[prefix]\n        create_smoother = kalman_smoother.kfilter is not self._kalman_filters[prefix]\n    if create_smoother:\n        cls = self.prefix_kalman_smoother_map[prefix]\n        self._kalman_smoothers[prefix] = cls(self._statespaces[prefix], self._kalman_filters[prefix], smoother_output, smooth_method)\n    else:\n        self._kalman_smoothers[prefix].set_smoother_output(smoother_output, False)\n        self._kalman_smoothers[prefix].set_smooth_method(smooth_method)\n    return (prefix, dtype, create_smoother, create_filter, create_statespace)",
            "def _initialize_smoother(self, smoother_output=None, smooth_method=None, prefix=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if smoother_output is None:\n        smoother_output = self.smoother_output\n    if smooth_method is None:\n        smooth_method = self.smooth_method\n    (prefix, dtype, create_filter, create_statespace) = self._initialize_filter(prefix, **kwargs)\n    create_smoother = create_filter or prefix not in self._kalman_smoothers\n    if not create_smoother:\n        kalman_smoother = self._kalman_smoothers[prefix]\n        create_smoother = kalman_smoother.kfilter is not self._kalman_filters[prefix]\n    if create_smoother:\n        cls = self.prefix_kalman_smoother_map[prefix]\n        self._kalman_smoothers[prefix] = cls(self._statespaces[prefix], self._kalman_filters[prefix], smoother_output, smooth_method)\n    else:\n        self._kalman_smoothers[prefix].set_smoother_output(smoother_output, False)\n        self._kalman_smoothers[prefix].set_smooth_method(smooth_method)\n    return (prefix, dtype, create_smoother, create_filter, create_statespace)"
        ]
    },
    {
        "func_name": "set_smoother_output",
        "original": "def set_smoother_output(self, smoother_output=None, **kwargs):\n    \"\"\"\n        Set the smoother output\n\n        The smoother can produce several types of results. The smoother output\n        variable controls which are calculated and returned.\n\n        Parameters\n        ----------\n        smoother_output : int, optional\n            Bitmask value to set the smoother output to. See notes for details.\n        **kwargs\n            Keyword arguments may be used to influence the smoother output by\n            setting individual boolean flags. See notes for details.\n\n        Notes\n        -----\n        The smoother output is defined by a collection of boolean flags, and\n        is internally stored as a bitmask. The methods available are:\n\n        SMOOTHER_STATE = 0x01\n            Calculate and return the smoothed states.\n        SMOOTHER_STATE_COV = 0x02\n            Calculate and return the smoothed state covariance matrices.\n        SMOOTHER_STATE_AUTOCOV = 0x10\n            Calculate and return the smoothed state lag-one autocovariance\n            matrices.\n        SMOOTHER_DISTURBANCE = 0x04\n            Calculate and return the smoothed state and observation\n            disturbances.\n        SMOOTHER_DISTURBANCE_COV = 0x08\n            Calculate and return the covariance matrices for the smoothed state\n            and observation disturbances.\n        SMOOTHER_ALL\n            Calculate and return all results.\n\n        If the bitmask is set directly via the `smoother_output` argument, then\n        the full method must be provided.\n\n        If keyword arguments are used to set individual boolean flags, then\n        the lowercase of the method must be used as an argument name, and the\n        value is the desired value of the boolean flag (True or False).\n\n        Note that the smoother output may also be specified by directly\n        modifying the class attributes which are defined similarly to the\n        keyword arguments.\n\n        The default smoother output is SMOOTHER_ALL.\n\n        If performance is a concern, only those results which are needed should\n        be specified as any results that are not specified will not be\n        calculated. For example, if the smoother output is set to only include\n        SMOOTHER_STATE, the smoother operates much more quickly than if all\n        output is required.\n\n        Examples\n        --------\n        >>> import statsmodels.tsa.statespace.kalman_smoother as ks\n        >>> mod = ks.KalmanSmoother(1,1)\n        >>> mod.smoother_output\n        15\n        >>> mod.set_smoother_output(smoother_output=0)\n        >>> mod.smoother_state = True\n        >>> mod.smoother_output\n        1\n        >>> mod.smoother_state\n        True\n        \"\"\"\n    if smoother_output is not None:\n        self.smoother_output = smoother_output\n    for name in KalmanSmoother.smoother_outputs:\n        if name in kwargs:\n            setattr(self, name, kwargs[name])",
        "mutated": [
            "def set_smoother_output(self, smoother_output=None, **kwargs):\n    if False:\n        i = 10\n    '\\n        Set the smoother output\\n\\n        The smoother can produce several types of results. The smoother output\\n        variable controls which are calculated and returned.\\n\\n        Parameters\\n        ----------\\n        smoother_output : int, optional\\n            Bitmask value to set the smoother output to. See notes for details.\\n        **kwargs\\n            Keyword arguments may be used to influence the smoother output by\\n            setting individual boolean flags. See notes for details.\\n\\n        Notes\\n        -----\\n        The smoother output is defined by a collection of boolean flags, and\\n        is internally stored as a bitmask. The methods available are:\\n\\n        SMOOTHER_STATE = 0x01\\n            Calculate and return the smoothed states.\\n        SMOOTHER_STATE_COV = 0x02\\n            Calculate and return the smoothed state covariance matrices.\\n        SMOOTHER_STATE_AUTOCOV = 0x10\\n            Calculate and return the smoothed state lag-one autocovariance\\n            matrices.\\n        SMOOTHER_DISTURBANCE = 0x04\\n            Calculate and return the smoothed state and observation\\n            disturbances.\\n        SMOOTHER_DISTURBANCE_COV = 0x08\\n            Calculate and return the covariance matrices for the smoothed state\\n            and observation disturbances.\\n        SMOOTHER_ALL\\n            Calculate and return all results.\\n\\n        If the bitmask is set directly via the `smoother_output` argument, then\\n        the full method must be provided.\\n\\n        If keyword arguments are used to set individual boolean flags, then\\n        the lowercase of the method must be used as an argument name, and the\\n        value is the desired value of the boolean flag (True or False).\\n\\n        Note that the smoother output may also be specified by directly\\n        modifying the class attributes which are defined similarly to the\\n        keyword arguments.\\n\\n        The default smoother output is SMOOTHER_ALL.\\n\\n        If performance is a concern, only those results which are needed should\\n        be specified as any results that are not specified will not be\\n        calculated. For example, if the smoother output is set to only include\\n        SMOOTHER_STATE, the smoother operates much more quickly than if all\\n        output is required.\\n\\n        Examples\\n        --------\\n        >>> import statsmodels.tsa.statespace.kalman_smoother as ks\\n        >>> mod = ks.KalmanSmoother(1,1)\\n        >>> mod.smoother_output\\n        15\\n        >>> mod.set_smoother_output(smoother_output=0)\\n        >>> mod.smoother_state = True\\n        >>> mod.smoother_output\\n        1\\n        >>> mod.smoother_state\\n        True\\n        '\n    if smoother_output is not None:\n        self.smoother_output = smoother_output\n    for name in KalmanSmoother.smoother_outputs:\n        if name in kwargs:\n            setattr(self, name, kwargs[name])",
            "def set_smoother_output(self, smoother_output=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Set the smoother output\\n\\n        The smoother can produce several types of results. The smoother output\\n        variable controls which are calculated and returned.\\n\\n        Parameters\\n        ----------\\n        smoother_output : int, optional\\n            Bitmask value to set the smoother output to. See notes for details.\\n        **kwargs\\n            Keyword arguments may be used to influence the smoother output by\\n            setting individual boolean flags. See notes for details.\\n\\n        Notes\\n        -----\\n        The smoother output is defined by a collection of boolean flags, and\\n        is internally stored as a bitmask. The methods available are:\\n\\n        SMOOTHER_STATE = 0x01\\n            Calculate and return the smoothed states.\\n        SMOOTHER_STATE_COV = 0x02\\n            Calculate and return the smoothed state covariance matrices.\\n        SMOOTHER_STATE_AUTOCOV = 0x10\\n            Calculate and return the smoothed state lag-one autocovariance\\n            matrices.\\n        SMOOTHER_DISTURBANCE = 0x04\\n            Calculate and return the smoothed state and observation\\n            disturbances.\\n        SMOOTHER_DISTURBANCE_COV = 0x08\\n            Calculate and return the covariance matrices for the smoothed state\\n            and observation disturbances.\\n        SMOOTHER_ALL\\n            Calculate and return all results.\\n\\n        If the bitmask is set directly via the `smoother_output` argument, then\\n        the full method must be provided.\\n\\n        If keyword arguments are used to set individual boolean flags, then\\n        the lowercase of the method must be used as an argument name, and the\\n        value is the desired value of the boolean flag (True or False).\\n\\n        Note that the smoother output may also be specified by directly\\n        modifying the class attributes which are defined similarly to the\\n        keyword arguments.\\n\\n        The default smoother output is SMOOTHER_ALL.\\n\\n        If performance is a concern, only those results which are needed should\\n        be specified as any results that are not specified will not be\\n        calculated. For example, if the smoother output is set to only include\\n        SMOOTHER_STATE, the smoother operates much more quickly than if all\\n        output is required.\\n\\n        Examples\\n        --------\\n        >>> import statsmodels.tsa.statespace.kalman_smoother as ks\\n        >>> mod = ks.KalmanSmoother(1,1)\\n        >>> mod.smoother_output\\n        15\\n        >>> mod.set_smoother_output(smoother_output=0)\\n        >>> mod.smoother_state = True\\n        >>> mod.smoother_output\\n        1\\n        >>> mod.smoother_state\\n        True\\n        '\n    if smoother_output is not None:\n        self.smoother_output = smoother_output\n    for name in KalmanSmoother.smoother_outputs:\n        if name in kwargs:\n            setattr(self, name, kwargs[name])",
            "def set_smoother_output(self, smoother_output=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Set the smoother output\\n\\n        The smoother can produce several types of results. The smoother output\\n        variable controls which are calculated and returned.\\n\\n        Parameters\\n        ----------\\n        smoother_output : int, optional\\n            Bitmask value to set the smoother output to. See notes for details.\\n        **kwargs\\n            Keyword arguments may be used to influence the smoother output by\\n            setting individual boolean flags. See notes for details.\\n\\n        Notes\\n        -----\\n        The smoother output is defined by a collection of boolean flags, and\\n        is internally stored as a bitmask. The methods available are:\\n\\n        SMOOTHER_STATE = 0x01\\n            Calculate and return the smoothed states.\\n        SMOOTHER_STATE_COV = 0x02\\n            Calculate and return the smoothed state covariance matrices.\\n        SMOOTHER_STATE_AUTOCOV = 0x10\\n            Calculate and return the smoothed state lag-one autocovariance\\n            matrices.\\n        SMOOTHER_DISTURBANCE = 0x04\\n            Calculate and return the smoothed state and observation\\n            disturbances.\\n        SMOOTHER_DISTURBANCE_COV = 0x08\\n            Calculate and return the covariance matrices for the smoothed state\\n            and observation disturbances.\\n        SMOOTHER_ALL\\n            Calculate and return all results.\\n\\n        If the bitmask is set directly via the `smoother_output` argument, then\\n        the full method must be provided.\\n\\n        If keyword arguments are used to set individual boolean flags, then\\n        the lowercase of the method must be used as an argument name, and the\\n        value is the desired value of the boolean flag (True or False).\\n\\n        Note that the smoother output may also be specified by directly\\n        modifying the class attributes which are defined similarly to the\\n        keyword arguments.\\n\\n        The default smoother output is SMOOTHER_ALL.\\n\\n        If performance is a concern, only those results which are needed should\\n        be specified as any results that are not specified will not be\\n        calculated. For example, if the smoother output is set to only include\\n        SMOOTHER_STATE, the smoother operates much more quickly than if all\\n        output is required.\\n\\n        Examples\\n        --------\\n        >>> import statsmodels.tsa.statespace.kalman_smoother as ks\\n        >>> mod = ks.KalmanSmoother(1,1)\\n        >>> mod.smoother_output\\n        15\\n        >>> mod.set_smoother_output(smoother_output=0)\\n        >>> mod.smoother_state = True\\n        >>> mod.smoother_output\\n        1\\n        >>> mod.smoother_state\\n        True\\n        '\n    if smoother_output is not None:\n        self.smoother_output = smoother_output\n    for name in KalmanSmoother.smoother_outputs:\n        if name in kwargs:\n            setattr(self, name, kwargs[name])",
            "def set_smoother_output(self, smoother_output=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Set the smoother output\\n\\n        The smoother can produce several types of results. The smoother output\\n        variable controls which are calculated and returned.\\n\\n        Parameters\\n        ----------\\n        smoother_output : int, optional\\n            Bitmask value to set the smoother output to. See notes for details.\\n        **kwargs\\n            Keyword arguments may be used to influence the smoother output by\\n            setting individual boolean flags. See notes for details.\\n\\n        Notes\\n        -----\\n        The smoother output is defined by a collection of boolean flags, and\\n        is internally stored as a bitmask. The methods available are:\\n\\n        SMOOTHER_STATE = 0x01\\n            Calculate and return the smoothed states.\\n        SMOOTHER_STATE_COV = 0x02\\n            Calculate and return the smoothed state covariance matrices.\\n        SMOOTHER_STATE_AUTOCOV = 0x10\\n            Calculate and return the smoothed state lag-one autocovariance\\n            matrices.\\n        SMOOTHER_DISTURBANCE = 0x04\\n            Calculate and return the smoothed state and observation\\n            disturbances.\\n        SMOOTHER_DISTURBANCE_COV = 0x08\\n            Calculate and return the covariance matrices for the smoothed state\\n            and observation disturbances.\\n        SMOOTHER_ALL\\n            Calculate and return all results.\\n\\n        If the bitmask is set directly via the `smoother_output` argument, then\\n        the full method must be provided.\\n\\n        If keyword arguments are used to set individual boolean flags, then\\n        the lowercase of the method must be used as an argument name, and the\\n        value is the desired value of the boolean flag (True or False).\\n\\n        Note that the smoother output may also be specified by directly\\n        modifying the class attributes which are defined similarly to the\\n        keyword arguments.\\n\\n        The default smoother output is SMOOTHER_ALL.\\n\\n        If performance is a concern, only those results which are needed should\\n        be specified as any results that are not specified will not be\\n        calculated. For example, if the smoother output is set to only include\\n        SMOOTHER_STATE, the smoother operates much more quickly than if all\\n        output is required.\\n\\n        Examples\\n        --------\\n        >>> import statsmodels.tsa.statespace.kalman_smoother as ks\\n        >>> mod = ks.KalmanSmoother(1,1)\\n        >>> mod.smoother_output\\n        15\\n        >>> mod.set_smoother_output(smoother_output=0)\\n        >>> mod.smoother_state = True\\n        >>> mod.smoother_output\\n        1\\n        >>> mod.smoother_state\\n        True\\n        '\n    if smoother_output is not None:\n        self.smoother_output = smoother_output\n    for name in KalmanSmoother.smoother_outputs:\n        if name in kwargs:\n            setattr(self, name, kwargs[name])",
            "def set_smoother_output(self, smoother_output=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Set the smoother output\\n\\n        The smoother can produce several types of results. The smoother output\\n        variable controls which are calculated and returned.\\n\\n        Parameters\\n        ----------\\n        smoother_output : int, optional\\n            Bitmask value to set the smoother output to. See notes for details.\\n        **kwargs\\n            Keyword arguments may be used to influence the smoother output by\\n            setting individual boolean flags. See notes for details.\\n\\n        Notes\\n        -----\\n        The smoother output is defined by a collection of boolean flags, and\\n        is internally stored as a bitmask. The methods available are:\\n\\n        SMOOTHER_STATE = 0x01\\n            Calculate and return the smoothed states.\\n        SMOOTHER_STATE_COV = 0x02\\n            Calculate and return the smoothed state covariance matrices.\\n        SMOOTHER_STATE_AUTOCOV = 0x10\\n            Calculate and return the smoothed state lag-one autocovariance\\n            matrices.\\n        SMOOTHER_DISTURBANCE = 0x04\\n            Calculate and return the smoothed state and observation\\n            disturbances.\\n        SMOOTHER_DISTURBANCE_COV = 0x08\\n            Calculate and return the covariance matrices for the smoothed state\\n            and observation disturbances.\\n        SMOOTHER_ALL\\n            Calculate and return all results.\\n\\n        If the bitmask is set directly via the `smoother_output` argument, then\\n        the full method must be provided.\\n\\n        If keyword arguments are used to set individual boolean flags, then\\n        the lowercase of the method must be used as an argument name, and the\\n        value is the desired value of the boolean flag (True or False).\\n\\n        Note that the smoother output may also be specified by directly\\n        modifying the class attributes which are defined similarly to the\\n        keyword arguments.\\n\\n        The default smoother output is SMOOTHER_ALL.\\n\\n        If performance is a concern, only those results which are needed should\\n        be specified as any results that are not specified will not be\\n        calculated. For example, if the smoother output is set to only include\\n        SMOOTHER_STATE, the smoother operates much more quickly than if all\\n        output is required.\\n\\n        Examples\\n        --------\\n        >>> import statsmodels.tsa.statespace.kalman_smoother as ks\\n        >>> mod = ks.KalmanSmoother(1,1)\\n        >>> mod.smoother_output\\n        15\\n        >>> mod.set_smoother_output(smoother_output=0)\\n        >>> mod.smoother_state = True\\n        >>> mod.smoother_output\\n        1\\n        >>> mod.smoother_state\\n        True\\n        '\n    if smoother_output is not None:\n        self.smoother_output = smoother_output\n    for name in KalmanSmoother.smoother_outputs:\n        if name in kwargs:\n            setattr(self, name, kwargs[name])"
        ]
    },
    {
        "func_name": "set_smooth_method",
        "original": "def set_smooth_method(self, smooth_method=None, **kwargs):\n    \"\"\"\n        Set the smoothing method\n\n        The smoothing method can be used to override the Kalman smoother\n        approach used. By default, the Kalman smoother used depends on the\n        Kalman filter method.\n\n        Parameters\n        ----------\n        smooth_method : int, optional\n            Bitmask value to set the filter method to. See notes for details.\n        **kwargs\n            Keyword arguments may be used to influence the filter method by\n            setting individual boolean flags. See notes for details.\n\n        Notes\n        -----\n        The smoothing method is defined by a collection of boolean flags, and\n        is internally stored as a bitmask. The methods available are:\n\n        SMOOTH_CONVENTIONAL = 0x01\n            Default Kalman smoother, as presented in Durbin and Koopman, 2012\n            chapter 4.\n        SMOOTH_CLASSICAL = 0x02\n            Classical Kalman smoother, as presented in Anderson and Moore, 1979\n            or Durbin and Koopman, 2012 chapter 4.6.1.\n        SMOOTH_ALTERNATIVE = 0x04\n            Modified Bryson-Frazier Kalman smoother method; this is identical\n            to the conventional method of Durbin and Koopman, 2012, except that\n            an additional intermediate step is included.\n        SMOOTH_UNIVARIATE = 0x08\n            Univariate Kalman smoother, as presented in Durbin and Koopman,\n            2012 chapter 6, except with modified Bryson-Frazier timing.\n\n        Practically speaking, these methods should all produce the same output\n        but different computational implications, numerical stability\n        implications, or internal timing assumptions.\n\n        Note that only the first method is available if using a Scipy version\n        older than 0.16.\n\n        If the bitmask is set directly via the `smooth_method` argument, then\n        the full method must be provided.\n\n        If keyword arguments are used to set individual boolean flags, then\n        the lowercase of the method must be used as an argument name, and the\n        value is the desired value of the boolean flag (True or False).\n\n        Note that the filter method may also be specified by directly modifying\n        the class attributes which are defined similarly to the keyword\n        arguments.\n\n        The default filtering method is SMOOTH_CONVENTIONAL.\n\n        Examples\n        --------\n        >>> mod = sm.tsa.statespace.SARIMAX(range(10))\n        >>> mod.smooth_method\n        1\n        >>> mod.filter_conventional\n        True\n        >>> mod.filter_univariate = True\n        >>> mod.smooth_method\n        17\n        >>> mod.set_smooth_method(filter_univariate=False,\n                                  filter_collapsed=True)\n        >>> mod.smooth_method\n        33\n        >>> mod.set_smooth_method(smooth_method=1)\n        >>> mod.filter_conventional\n        True\n        >>> mod.filter_univariate\n        False\n        >>> mod.filter_collapsed\n        False\n        >>> mod.filter_univariate = True\n        >>> mod.smooth_method\n        17\n        \"\"\"\n    if smooth_method is not None:\n        self.smooth_method = smooth_method\n    for name in KalmanSmoother.smooth_methods:\n        if name in kwargs:\n            setattr(self, name, kwargs[name])",
        "mutated": [
            "def set_smooth_method(self, smooth_method=None, **kwargs):\n    if False:\n        i = 10\n    '\\n        Set the smoothing method\\n\\n        The smoothing method can be used to override the Kalman smoother\\n        approach used. By default, the Kalman smoother used depends on the\\n        Kalman filter method.\\n\\n        Parameters\\n        ----------\\n        smooth_method : int, optional\\n            Bitmask value to set the filter method to. See notes for details.\\n        **kwargs\\n            Keyword arguments may be used to influence the filter method by\\n            setting individual boolean flags. See notes for details.\\n\\n        Notes\\n        -----\\n        The smoothing method is defined by a collection of boolean flags, and\\n        is internally stored as a bitmask. The methods available are:\\n\\n        SMOOTH_CONVENTIONAL = 0x01\\n            Default Kalman smoother, as presented in Durbin and Koopman, 2012\\n            chapter 4.\\n        SMOOTH_CLASSICAL = 0x02\\n            Classical Kalman smoother, as presented in Anderson and Moore, 1979\\n            or Durbin and Koopman, 2012 chapter 4.6.1.\\n        SMOOTH_ALTERNATIVE = 0x04\\n            Modified Bryson-Frazier Kalman smoother method; this is identical\\n            to the conventional method of Durbin and Koopman, 2012, except that\\n            an additional intermediate step is included.\\n        SMOOTH_UNIVARIATE = 0x08\\n            Univariate Kalman smoother, as presented in Durbin and Koopman,\\n            2012 chapter 6, except with modified Bryson-Frazier timing.\\n\\n        Practically speaking, these methods should all produce the same output\\n        but different computational implications, numerical stability\\n        implications, or internal timing assumptions.\\n\\n        Note that only the first method is available if using a Scipy version\\n        older than 0.16.\\n\\n        If the bitmask is set directly via the `smooth_method` argument, then\\n        the full method must be provided.\\n\\n        If keyword arguments are used to set individual boolean flags, then\\n        the lowercase of the method must be used as an argument name, and the\\n        value is the desired value of the boolean flag (True or False).\\n\\n        Note that the filter method may also be specified by directly modifying\\n        the class attributes which are defined similarly to the keyword\\n        arguments.\\n\\n        The default filtering method is SMOOTH_CONVENTIONAL.\\n\\n        Examples\\n        --------\\n        >>> mod = sm.tsa.statespace.SARIMAX(range(10))\\n        >>> mod.smooth_method\\n        1\\n        >>> mod.filter_conventional\\n        True\\n        >>> mod.filter_univariate = True\\n        >>> mod.smooth_method\\n        17\\n        >>> mod.set_smooth_method(filter_univariate=False,\\n                                  filter_collapsed=True)\\n        >>> mod.smooth_method\\n        33\\n        >>> mod.set_smooth_method(smooth_method=1)\\n        >>> mod.filter_conventional\\n        True\\n        >>> mod.filter_univariate\\n        False\\n        >>> mod.filter_collapsed\\n        False\\n        >>> mod.filter_univariate = True\\n        >>> mod.smooth_method\\n        17\\n        '\n    if smooth_method is not None:\n        self.smooth_method = smooth_method\n    for name in KalmanSmoother.smooth_methods:\n        if name in kwargs:\n            setattr(self, name, kwargs[name])",
            "def set_smooth_method(self, smooth_method=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Set the smoothing method\\n\\n        The smoothing method can be used to override the Kalman smoother\\n        approach used. By default, the Kalman smoother used depends on the\\n        Kalman filter method.\\n\\n        Parameters\\n        ----------\\n        smooth_method : int, optional\\n            Bitmask value to set the filter method to. See notes for details.\\n        **kwargs\\n            Keyword arguments may be used to influence the filter method by\\n            setting individual boolean flags. See notes for details.\\n\\n        Notes\\n        -----\\n        The smoothing method is defined by a collection of boolean flags, and\\n        is internally stored as a bitmask. The methods available are:\\n\\n        SMOOTH_CONVENTIONAL = 0x01\\n            Default Kalman smoother, as presented in Durbin and Koopman, 2012\\n            chapter 4.\\n        SMOOTH_CLASSICAL = 0x02\\n            Classical Kalman smoother, as presented in Anderson and Moore, 1979\\n            or Durbin and Koopman, 2012 chapter 4.6.1.\\n        SMOOTH_ALTERNATIVE = 0x04\\n            Modified Bryson-Frazier Kalman smoother method; this is identical\\n            to the conventional method of Durbin and Koopman, 2012, except that\\n            an additional intermediate step is included.\\n        SMOOTH_UNIVARIATE = 0x08\\n            Univariate Kalman smoother, as presented in Durbin and Koopman,\\n            2012 chapter 6, except with modified Bryson-Frazier timing.\\n\\n        Practically speaking, these methods should all produce the same output\\n        but different computational implications, numerical stability\\n        implications, or internal timing assumptions.\\n\\n        Note that only the first method is available if using a Scipy version\\n        older than 0.16.\\n\\n        If the bitmask is set directly via the `smooth_method` argument, then\\n        the full method must be provided.\\n\\n        If keyword arguments are used to set individual boolean flags, then\\n        the lowercase of the method must be used as an argument name, and the\\n        value is the desired value of the boolean flag (True or False).\\n\\n        Note that the filter method may also be specified by directly modifying\\n        the class attributes which are defined similarly to the keyword\\n        arguments.\\n\\n        The default filtering method is SMOOTH_CONVENTIONAL.\\n\\n        Examples\\n        --------\\n        >>> mod = sm.tsa.statespace.SARIMAX(range(10))\\n        >>> mod.smooth_method\\n        1\\n        >>> mod.filter_conventional\\n        True\\n        >>> mod.filter_univariate = True\\n        >>> mod.smooth_method\\n        17\\n        >>> mod.set_smooth_method(filter_univariate=False,\\n                                  filter_collapsed=True)\\n        >>> mod.smooth_method\\n        33\\n        >>> mod.set_smooth_method(smooth_method=1)\\n        >>> mod.filter_conventional\\n        True\\n        >>> mod.filter_univariate\\n        False\\n        >>> mod.filter_collapsed\\n        False\\n        >>> mod.filter_univariate = True\\n        >>> mod.smooth_method\\n        17\\n        '\n    if smooth_method is not None:\n        self.smooth_method = smooth_method\n    for name in KalmanSmoother.smooth_methods:\n        if name in kwargs:\n            setattr(self, name, kwargs[name])",
            "def set_smooth_method(self, smooth_method=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Set the smoothing method\\n\\n        The smoothing method can be used to override the Kalman smoother\\n        approach used. By default, the Kalman smoother used depends on the\\n        Kalman filter method.\\n\\n        Parameters\\n        ----------\\n        smooth_method : int, optional\\n            Bitmask value to set the filter method to. See notes for details.\\n        **kwargs\\n            Keyword arguments may be used to influence the filter method by\\n            setting individual boolean flags. See notes for details.\\n\\n        Notes\\n        -----\\n        The smoothing method is defined by a collection of boolean flags, and\\n        is internally stored as a bitmask. The methods available are:\\n\\n        SMOOTH_CONVENTIONAL = 0x01\\n            Default Kalman smoother, as presented in Durbin and Koopman, 2012\\n            chapter 4.\\n        SMOOTH_CLASSICAL = 0x02\\n            Classical Kalman smoother, as presented in Anderson and Moore, 1979\\n            or Durbin and Koopman, 2012 chapter 4.6.1.\\n        SMOOTH_ALTERNATIVE = 0x04\\n            Modified Bryson-Frazier Kalman smoother method; this is identical\\n            to the conventional method of Durbin and Koopman, 2012, except that\\n            an additional intermediate step is included.\\n        SMOOTH_UNIVARIATE = 0x08\\n            Univariate Kalman smoother, as presented in Durbin and Koopman,\\n            2012 chapter 6, except with modified Bryson-Frazier timing.\\n\\n        Practically speaking, these methods should all produce the same output\\n        but different computational implications, numerical stability\\n        implications, or internal timing assumptions.\\n\\n        Note that only the first method is available if using a Scipy version\\n        older than 0.16.\\n\\n        If the bitmask is set directly via the `smooth_method` argument, then\\n        the full method must be provided.\\n\\n        If keyword arguments are used to set individual boolean flags, then\\n        the lowercase of the method must be used as an argument name, and the\\n        value is the desired value of the boolean flag (True or False).\\n\\n        Note that the filter method may also be specified by directly modifying\\n        the class attributes which are defined similarly to the keyword\\n        arguments.\\n\\n        The default filtering method is SMOOTH_CONVENTIONAL.\\n\\n        Examples\\n        --------\\n        >>> mod = sm.tsa.statespace.SARIMAX(range(10))\\n        >>> mod.smooth_method\\n        1\\n        >>> mod.filter_conventional\\n        True\\n        >>> mod.filter_univariate = True\\n        >>> mod.smooth_method\\n        17\\n        >>> mod.set_smooth_method(filter_univariate=False,\\n                                  filter_collapsed=True)\\n        >>> mod.smooth_method\\n        33\\n        >>> mod.set_smooth_method(smooth_method=1)\\n        >>> mod.filter_conventional\\n        True\\n        >>> mod.filter_univariate\\n        False\\n        >>> mod.filter_collapsed\\n        False\\n        >>> mod.filter_univariate = True\\n        >>> mod.smooth_method\\n        17\\n        '\n    if smooth_method is not None:\n        self.smooth_method = smooth_method\n    for name in KalmanSmoother.smooth_methods:\n        if name in kwargs:\n            setattr(self, name, kwargs[name])",
            "def set_smooth_method(self, smooth_method=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Set the smoothing method\\n\\n        The smoothing method can be used to override the Kalman smoother\\n        approach used. By default, the Kalman smoother used depends on the\\n        Kalman filter method.\\n\\n        Parameters\\n        ----------\\n        smooth_method : int, optional\\n            Bitmask value to set the filter method to. See notes for details.\\n        **kwargs\\n            Keyword arguments may be used to influence the filter method by\\n            setting individual boolean flags. See notes for details.\\n\\n        Notes\\n        -----\\n        The smoothing method is defined by a collection of boolean flags, and\\n        is internally stored as a bitmask. The methods available are:\\n\\n        SMOOTH_CONVENTIONAL = 0x01\\n            Default Kalman smoother, as presented in Durbin and Koopman, 2012\\n            chapter 4.\\n        SMOOTH_CLASSICAL = 0x02\\n            Classical Kalman smoother, as presented in Anderson and Moore, 1979\\n            or Durbin and Koopman, 2012 chapter 4.6.1.\\n        SMOOTH_ALTERNATIVE = 0x04\\n            Modified Bryson-Frazier Kalman smoother method; this is identical\\n            to the conventional method of Durbin and Koopman, 2012, except that\\n            an additional intermediate step is included.\\n        SMOOTH_UNIVARIATE = 0x08\\n            Univariate Kalman smoother, as presented in Durbin and Koopman,\\n            2012 chapter 6, except with modified Bryson-Frazier timing.\\n\\n        Practically speaking, these methods should all produce the same output\\n        but different computational implications, numerical stability\\n        implications, or internal timing assumptions.\\n\\n        Note that only the first method is available if using a Scipy version\\n        older than 0.16.\\n\\n        If the bitmask is set directly via the `smooth_method` argument, then\\n        the full method must be provided.\\n\\n        If keyword arguments are used to set individual boolean flags, then\\n        the lowercase of the method must be used as an argument name, and the\\n        value is the desired value of the boolean flag (True or False).\\n\\n        Note that the filter method may also be specified by directly modifying\\n        the class attributes which are defined similarly to the keyword\\n        arguments.\\n\\n        The default filtering method is SMOOTH_CONVENTIONAL.\\n\\n        Examples\\n        --------\\n        >>> mod = sm.tsa.statespace.SARIMAX(range(10))\\n        >>> mod.smooth_method\\n        1\\n        >>> mod.filter_conventional\\n        True\\n        >>> mod.filter_univariate = True\\n        >>> mod.smooth_method\\n        17\\n        >>> mod.set_smooth_method(filter_univariate=False,\\n                                  filter_collapsed=True)\\n        >>> mod.smooth_method\\n        33\\n        >>> mod.set_smooth_method(smooth_method=1)\\n        >>> mod.filter_conventional\\n        True\\n        >>> mod.filter_univariate\\n        False\\n        >>> mod.filter_collapsed\\n        False\\n        >>> mod.filter_univariate = True\\n        >>> mod.smooth_method\\n        17\\n        '\n    if smooth_method is not None:\n        self.smooth_method = smooth_method\n    for name in KalmanSmoother.smooth_methods:\n        if name in kwargs:\n            setattr(self, name, kwargs[name])",
            "def set_smooth_method(self, smooth_method=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Set the smoothing method\\n\\n        The smoothing method can be used to override the Kalman smoother\\n        approach used. By default, the Kalman smoother used depends on the\\n        Kalman filter method.\\n\\n        Parameters\\n        ----------\\n        smooth_method : int, optional\\n            Bitmask value to set the filter method to. See notes for details.\\n        **kwargs\\n            Keyword arguments may be used to influence the filter method by\\n            setting individual boolean flags. See notes for details.\\n\\n        Notes\\n        -----\\n        The smoothing method is defined by a collection of boolean flags, and\\n        is internally stored as a bitmask. The methods available are:\\n\\n        SMOOTH_CONVENTIONAL = 0x01\\n            Default Kalman smoother, as presented in Durbin and Koopman, 2012\\n            chapter 4.\\n        SMOOTH_CLASSICAL = 0x02\\n            Classical Kalman smoother, as presented in Anderson and Moore, 1979\\n            or Durbin and Koopman, 2012 chapter 4.6.1.\\n        SMOOTH_ALTERNATIVE = 0x04\\n            Modified Bryson-Frazier Kalman smoother method; this is identical\\n            to the conventional method of Durbin and Koopman, 2012, except that\\n            an additional intermediate step is included.\\n        SMOOTH_UNIVARIATE = 0x08\\n            Univariate Kalman smoother, as presented in Durbin and Koopman,\\n            2012 chapter 6, except with modified Bryson-Frazier timing.\\n\\n        Practically speaking, these methods should all produce the same output\\n        but different computational implications, numerical stability\\n        implications, or internal timing assumptions.\\n\\n        Note that only the first method is available if using a Scipy version\\n        older than 0.16.\\n\\n        If the bitmask is set directly via the `smooth_method` argument, then\\n        the full method must be provided.\\n\\n        If keyword arguments are used to set individual boolean flags, then\\n        the lowercase of the method must be used as an argument name, and the\\n        value is the desired value of the boolean flag (True or False).\\n\\n        Note that the filter method may also be specified by directly modifying\\n        the class attributes which are defined similarly to the keyword\\n        arguments.\\n\\n        The default filtering method is SMOOTH_CONVENTIONAL.\\n\\n        Examples\\n        --------\\n        >>> mod = sm.tsa.statespace.SARIMAX(range(10))\\n        >>> mod.smooth_method\\n        1\\n        >>> mod.filter_conventional\\n        True\\n        >>> mod.filter_univariate = True\\n        >>> mod.smooth_method\\n        17\\n        >>> mod.set_smooth_method(filter_univariate=False,\\n                                  filter_collapsed=True)\\n        >>> mod.smooth_method\\n        33\\n        >>> mod.set_smooth_method(smooth_method=1)\\n        >>> mod.filter_conventional\\n        True\\n        >>> mod.filter_univariate\\n        False\\n        >>> mod.filter_collapsed\\n        False\\n        >>> mod.filter_univariate = True\\n        >>> mod.smooth_method\\n        17\\n        '\n    if smooth_method is not None:\n        self.smooth_method = smooth_method\n    for name in KalmanSmoother.smooth_methods:\n        if name in kwargs:\n            setattr(self, name, kwargs[name])"
        ]
    },
    {
        "func_name": "_smooth",
        "original": "def _smooth(self, smoother_output=None, smooth_method=None, prefix=None, complex_step=False, results=None, **kwargs):\n    (prefix, dtype, create_smoother, create_filter, create_statespace) = self._initialize_smoother(smoother_output, smooth_method, prefix=prefix, **kwargs)\n    if create_filter or create_statespace:\n        raise ValueError('Passed settings forced re-creation of the Kalman filter. Please run `_filter` before running `_smooth`.')\n    smoother = self._kalman_smoothers[prefix]\n    smoother()\n    return smoother",
        "mutated": [
            "def _smooth(self, smoother_output=None, smooth_method=None, prefix=None, complex_step=False, results=None, **kwargs):\n    if False:\n        i = 10\n    (prefix, dtype, create_smoother, create_filter, create_statespace) = self._initialize_smoother(smoother_output, smooth_method, prefix=prefix, **kwargs)\n    if create_filter or create_statespace:\n        raise ValueError('Passed settings forced re-creation of the Kalman filter. Please run `_filter` before running `_smooth`.')\n    smoother = self._kalman_smoothers[prefix]\n    smoother()\n    return smoother",
            "def _smooth(self, smoother_output=None, smooth_method=None, prefix=None, complex_step=False, results=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (prefix, dtype, create_smoother, create_filter, create_statespace) = self._initialize_smoother(smoother_output, smooth_method, prefix=prefix, **kwargs)\n    if create_filter or create_statespace:\n        raise ValueError('Passed settings forced re-creation of the Kalman filter. Please run `_filter` before running `_smooth`.')\n    smoother = self._kalman_smoothers[prefix]\n    smoother()\n    return smoother",
            "def _smooth(self, smoother_output=None, smooth_method=None, prefix=None, complex_step=False, results=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (prefix, dtype, create_smoother, create_filter, create_statespace) = self._initialize_smoother(smoother_output, smooth_method, prefix=prefix, **kwargs)\n    if create_filter or create_statespace:\n        raise ValueError('Passed settings forced re-creation of the Kalman filter. Please run `_filter` before running `_smooth`.')\n    smoother = self._kalman_smoothers[prefix]\n    smoother()\n    return smoother",
            "def _smooth(self, smoother_output=None, smooth_method=None, prefix=None, complex_step=False, results=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (prefix, dtype, create_smoother, create_filter, create_statespace) = self._initialize_smoother(smoother_output, smooth_method, prefix=prefix, **kwargs)\n    if create_filter or create_statespace:\n        raise ValueError('Passed settings forced re-creation of the Kalman filter. Please run `_filter` before running `_smooth`.')\n    smoother = self._kalman_smoothers[prefix]\n    smoother()\n    return smoother",
            "def _smooth(self, smoother_output=None, smooth_method=None, prefix=None, complex_step=False, results=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (prefix, dtype, create_smoother, create_filter, create_statespace) = self._initialize_smoother(smoother_output, smooth_method, prefix=prefix, **kwargs)\n    if create_filter or create_statespace:\n        raise ValueError('Passed settings forced re-creation of the Kalman filter. Please run `_filter` before running `_smooth`.')\n    smoother = self._kalman_smoothers[prefix]\n    smoother()\n    return smoother"
        ]
    },
    {
        "func_name": "smooth",
        "original": "def smooth(self, smoother_output=None, smooth_method=None, results=None, run_filter=True, prefix=None, complex_step=False, update_representation=True, update_filter=True, update_smoother=True, **kwargs):\n    \"\"\"\n        Apply the Kalman smoother to the statespace model.\n\n        Parameters\n        ----------\n        smoother_output : int, optional\n            Determines which Kalman smoother output calculate. Default is all\n            (including state, disturbances, and all covariances).\n        results : class or object, optional\n            If a class, then that class is instantiated and returned with the\n            result of both filtering and smoothing.\n            If an object, then that object is updated with the smoothing data.\n            If None, then a SmootherResults object is returned with both\n            filtering and smoothing results.\n        run_filter : bool, optional\n            Whether or not to run the Kalman filter prior to smoothing. Default\n            is True.\n        prefix : str\n            The prefix of the datatype. Usually only used internally.\n\n        Returns\n        -------\n        SmootherResults object\n        \"\"\"\n    kfilter = self._filter(**kwargs)\n    results = self.results_class(self)\n    if update_representation:\n        results.update_representation(self)\n    if update_filter:\n        results.update_filter(kfilter)\n    else:\n        results.nobs_diffuse = kfilter.nobs_diffuse\n    if smoother_output is None:\n        smoother_output = self.smoother_output\n    smoother = self._smooth(smoother_output, results=results, **kwargs)\n    if update_smoother:\n        results.update_smoother(smoother)\n    return results",
        "mutated": [
            "def smooth(self, smoother_output=None, smooth_method=None, results=None, run_filter=True, prefix=None, complex_step=False, update_representation=True, update_filter=True, update_smoother=True, **kwargs):\n    if False:\n        i = 10\n    '\\n        Apply the Kalman smoother to the statespace model.\\n\\n        Parameters\\n        ----------\\n        smoother_output : int, optional\\n            Determines which Kalman smoother output calculate. Default is all\\n            (including state, disturbances, and all covariances).\\n        results : class or object, optional\\n            If a class, then that class is instantiated and returned with the\\n            result of both filtering and smoothing.\\n            If an object, then that object is updated with the smoothing data.\\n            If None, then a SmootherResults object is returned with both\\n            filtering and smoothing results.\\n        run_filter : bool, optional\\n            Whether or not to run the Kalman filter prior to smoothing. Default\\n            is True.\\n        prefix : str\\n            The prefix of the datatype. Usually only used internally.\\n\\n        Returns\\n        -------\\n        SmootherResults object\\n        '\n    kfilter = self._filter(**kwargs)\n    results = self.results_class(self)\n    if update_representation:\n        results.update_representation(self)\n    if update_filter:\n        results.update_filter(kfilter)\n    else:\n        results.nobs_diffuse = kfilter.nobs_diffuse\n    if smoother_output is None:\n        smoother_output = self.smoother_output\n    smoother = self._smooth(smoother_output, results=results, **kwargs)\n    if update_smoother:\n        results.update_smoother(smoother)\n    return results",
            "def smooth(self, smoother_output=None, smooth_method=None, results=None, run_filter=True, prefix=None, complex_step=False, update_representation=True, update_filter=True, update_smoother=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Apply the Kalman smoother to the statespace model.\\n\\n        Parameters\\n        ----------\\n        smoother_output : int, optional\\n            Determines which Kalman smoother output calculate. Default is all\\n            (including state, disturbances, and all covariances).\\n        results : class or object, optional\\n            If a class, then that class is instantiated and returned with the\\n            result of both filtering and smoothing.\\n            If an object, then that object is updated with the smoothing data.\\n            If None, then a SmootherResults object is returned with both\\n            filtering and smoothing results.\\n        run_filter : bool, optional\\n            Whether or not to run the Kalman filter prior to smoothing. Default\\n            is True.\\n        prefix : str\\n            The prefix of the datatype. Usually only used internally.\\n\\n        Returns\\n        -------\\n        SmootherResults object\\n        '\n    kfilter = self._filter(**kwargs)\n    results = self.results_class(self)\n    if update_representation:\n        results.update_representation(self)\n    if update_filter:\n        results.update_filter(kfilter)\n    else:\n        results.nobs_diffuse = kfilter.nobs_diffuse\n    if smoother_output is None:\n        smoother_output = self.smoother_output\n    smoother = self._smooth(smoother_output, results=results, **kwargs)\n    if update_smoother:\n        results.update_smoother(smoother)\n    return results",
            "def smooth(self, smoother_output=None, smooth_method=None, results=None, run_filter=True, prefix=None, complex_step=False, update_representation=True, update_filter=True, update_smoother=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Apply the Kalman smoother to the statespace model.\\n\\n        Parameters\\n        ----------\\n        smoother_output : int, optional\\n            Determines which Kalman smoother output calculate. Default is all\\n            (including state, disturbances, and all covariances).\\n        results : class or object, optional\\n            If a class, then that class is instantiated and returned with the\\n            result of both filtering and smoothing.\\n            If an object, then that object is updated with the smoothing data.\\n            If None, then a SmootherResults object is returned with both\\n            filtering and smoothing results.\\n        run_filter : bool, optional\\n            Whether or not to run the Kalman filter prior to smoothing. Default\\n            is True.\\n        prefix : str\\n            The prefix of the datatype. Usually only used internally.\\n\\n        Returns\\n        -------\\n        SmootherResults object\\n        '\n    kfilter = self._filter(**kwargs)\n    results = self.results_class(self)\n    if update_representation:\n        results.update_representation(self)\n    if update_filter:\n        results.update_filter(kfilter)\n    else:\n        results.nobs_diffuse = kfilter.nobs_diffuse\n    if smoother_output is None:\n        smoother_output = self.smoother_output\n    smoother = self._smooth(smoother_output, results=results, **kwargs)\n    if update_smoother:\n        results.update_smoother(smoother)\n    return results",
            "def smooth(self, smoother_output=None, smooth_method=None, results=None, run_filter=True, prefix=None, complex_step=False, update_representation=True, update_filter=True, update_smoother=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Apply the Kalman smoother to the statespace model.\\n\\n        Parameters\\n        ----------\\n        smoother_output : int, optional\\n            Determines which Kalman smoother output calculate. Default is all\\n            (including state, disturbances, and all covariances).\\n        results : class or object, optional\\n            If a class, then that class is instantiated and returned with the\\n            result of both filtering and smoothing.\\n            If an object, then that object is updated with the smoothing data.\\n            If None, then a SmootherResults object is returned with both\\n            filtering and smoothing results.\\n        run_filter : bool, optional\\n            Whether or not to run the Kalman filter prior to smoothing. Default\\n            is True.\\n        prefix : str\\n            The prefix of the datatype. Usually only used internally.\\n\\n        Returns\\n        -------\\n        SmootherResults object\\n        '\n    kfilter = self._filter(**kwargs)\n    results = self.results_class(self)\n    if update_representation:\n        results.update_representation(self)\n    if update_filter:\n        results.update_filter(kfilter)\n    else:\n        results.nobs_diffuse = kfilter.nobs_diffuse\n    if smoother_output is None:\n        smoother_output = self.smoother_output\n    smoother = self._smooth(smoother_output, results=results, **kwargs)\n    if update_smoother:\n        results.update_smoother(smoother)\n    return results",
            "def smooth(self, smoother_output=None, smooth_method=None, results=None, run_filter=True, prefix=None, complex_step=False, update_representation=True, update_filter=True, update_smoother=True, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Apply the Kalman smoother to the statespace model.\\n\\n        Parameters\\n        ----------\\n        smoother_output : int, optional\\n            Determines which Kalman smoother output calculate. Default is all\\n            (including state, disturbances, and all covariances).\\n        results : class or object, optional\\n            If a class, then that class is instantiated and returned with the\\n            result of both filtering and smoothing.\\n            If an object, then that object is updated with the smoothing data.\\n            If None, then a SmootherResults object is returned with both\\n            filtering and smoothing results.\\n        run_filter : bool, optional\\n            Whether or not to run the Kalman filter prior to smoothing. Default\\n            is True.\\n        prefix : str\\n            The prefix of the datatype. Usually only used internally.\\n\\n        Returns\\n        -------\\n        SmootherResults object\\n        '\n    kfilter = self._filter(**kwargs)\n    results = self.results_class(self)\n    if update_representation:\n        results.update_representation(self)\n    if update_filter:\n        results.update_filter(kfilter)\n    else:\n        results.nobs_diffuse = kfilter.nobs_diffuse\n    if smoother_output is None:\n        smoother_output = self.smoother_output\n    smoother = self._smooth(smoother_output, results=results, **kwargs)\n    if update_smoother:\n        results.update_smoother(smoother)\n    return results"
        ]
    },
    {
        "func_name": "update_representation",
        "original": "def update_representation(self, model, only_options=False):\n    \"\"\"\n        Update the results to match a given model\n\n        Parameters\n        ----------\n        model : Representation\n            The model object from which to take the updated values.\n        only_options : bool, optional\n            If set to true, only the smoother and filter options are updated,\n            and the state space representation is not updated. Default is\n            False.\n\n        Notes\n        -----\n        This method is rarely required except for internal usage.\n        \"\"\"\n    super(SmootherResults, self).update_representation(model, only_options)\n    for name in self._smoother_options:\n        setattr(self, name, getattr(model, name, None))\n    self._smoothed_forecasts = None\n    self._smoothed_forecasts_error = None\n    self._smoothed_forecasts_error_cov = None",
        "mutated": [
            "def update_representation(self, model, only_options=False):\n    if False:\n        i = 10\n    '\\n        Update the results to match a given model\\n\\n        Parameters\\n        ----------\\n        model : Representation\\n            The model object from which to take the updated values.\\n        only_options : bool, optional\\n            If set to true, only the smoother and filter options are updated,\\n            and the state space representation is not updated. Default is\\n            False.\\n\\n        Notes\\n        -----\\n        This method is rarely required except for internal usage.\\n        '\n    super(SmootherResults, self).update_representation(model, only_options)\n    for name in self._smoother_options:\n        setattr(self, name, getattr(model, name, None))\n    self._smoothed_forecasts = None\n    self._smoothed_forecasts_error = None\n    self._smoothed_forecasts_error_cov = None",
            "def update_representation(self, model, only_options=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Update the results to match a given model\\n\\n        Parameters\\n        ----------\\n        model : Representation\\n            The model object from which to take the updated values.\\n        only_options : bool, optional\\n            If set to true, only the smoother and filter options are updated,\\n            and the state space representation is not updated. Default is\\n            False.\\n\\n        Notes\\n        -----\\n        This method is rarely required except for internal usage.\\n        '\n    super(SmootherResults, self).update_representation(model, only_options)\n    for name in self._smoother_options:\n        setattr(self, name, getattr(model, name, None))\n    self._smoothed_forecasts = None\n    self._smoothed_forecasts_error = None\n    self._smoothed_forecasts_error_cov = None",
            "def update_representation(self, model, only_options=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Update the results to match a given model\\n\\n        Parameters\\n        ----------\\n        model : Representation\\n            The model object from which to take the updated values.\\n        only_options : bool, optional\\n            If set to true, only the smoother and filter options are updated,\\n            and the state space representation is not updated. Default is\\n            False.\\n\\n        Notes\\n        -----\\n        This method is rarely required except for internal usage.\\n        '\n    super(SmootherResults, self).update_representation(model, only_options)\n    for name in self._smoother_options:\n        setattr(self, name, getattr(model, name, None))\n    self._smoothed_forecasts = None\n    self._smoothed_forecasts_error = None\n    self._smoothed_forecasts_error_cov = None",
            "def update_representation(self, model, only_options=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Update the results to match a given model\\n\\n        Parameters\\n        ----------\\n        model : Representation\\n            The model object from which to take the updated values.\\n        only_options : bool, optional\\n            If set to true, only the smoother and filter options are updated,\\n            and the state space representation is not updated. Default is\\n            False.\\n\\n        Notes\\n        -----\\n        This method is rarely required except for internal usage.\\n        '\n    super(SmootherResults, self).update_representation(model, only_options)\n    for name in self._smoother_options:\n        setattr(self, name, getattr(model, name, None))\n    self._smoothed_forecasts = None\n    self._smoothed_forecasts_error = None\n    self._smoothed_forecasts_error_cov = None",
            "def update_representation(self, model, only_options=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Update the results to match a given model\\n\\n        Parameters\\n        ----------\\n        model : Representation\\n            The model object from which to take the updated values.\\n        only_options : bool, optional\\n            If set to true, only the smoother and filter options are updated,\\n            and the state space representation is not updated. Default is\\n            False.\\n\\n        Notes\\n        -----\\n        This method is rarely required except for internal usage.\\n        '\n    super(SmootherResults, self).update_representation(model, only_options)\n    for name in self._smoother_options:\n        setattr(self, name, getattr(model, name, None))\n    self._smoothed_forecasts = None\n    self._smoothed_forecasts_error = None\n    self._smoothed_forecasts_error_cov = None"
        ]
    },
    {
        "func_name": "update_smoother",
        "original": "def update_smoother(self, smoother):\n    \"\"\"\n        Update the smoother results\n\n        Parameters\n        ----------\n        smoother : KalmanSmoother\n            The model object from which to take the updated values.\n\n        Notes\n        -----\n        This method is rarely required except for internal usage.\n        \"\"\"\n    attributes = []\n    if self.smoother_state or self.smoother_disturbance:\n        attributes.append('scaled_smoothed_estimator')\n    if self.smoother_state_cov or self.smoother_disturbance_cov:\n        attributes.append('scaled_smoothed_estimator_cov')\n    if self.smoother_state:\n        attributes.append('smoothed_state')\n    if self.smoother_state_cov:\n        attributes.append('smoothed_state_cov')\n    if self.smoother_state_autocov:\n        attributes.append('smoothed_state_autocov')\n    if self.smoother_disturbance:\n        attributes += ['smoothing_error', 'smoothed_measurement_disturbance', 'smoothed_state_disturbance']\n    if self.smoother_disturbance_cov:\n        attributes += ['smoothed_measurement_disturbance_cov', 'smoothed_state_disturbance_cov']\n    has_missing = np.sum(self.nmissing) > 0\n    for name in self._smoother_attributes:\n        if name == 'smoother_output':\n            pass\n        elif name in attributes:\n            if name in ['smoothing_error', 'smoothed_measurement_disturbance']:\n                vector = getattr(smoother, name, None)\n                if vector is not None and has_missing:\n                    vector = np.array(reorder_missing_vector(vector, self.missing, prefix=self.prefix))\n                else:\n                    vector = np.array(vector, copy=True)\n                setattr(self, name, vector)\n            elif name == 'smoothed_measurement_disturbance_cov':\n                matrix = getattr(smoother, name, None)\n                if matrix is not None and has_missing:\n                    matrix = reorder_missing_matrix(matrix, self.missing, reorder_rows=True, reorder_cols=True, prefix=self.prefix)\n                    copy_index_matrix(self.obs_cov, matrix, self.missing, index_rows=True, index_cols=True, inplace=True, prefix=self.prefix)\n                else:\n                    matrix = np.array(matrix, copy=True)\n                setattr(self, name, matrix)\n            else:\n                setattr(self, name, np.array(getattr(smoother, name, None), copy=True))\n        else:\n            setattr(self, name, None)\n    self.innovations_transition = np.array(smoother.innovations_transition, copy=True)\n    self.scaled_smoothed_diffuse_estimator = None\n    self.scaled_smoothed_diffuse1_estimator_cov = None\n    self.scaled_smoothed_diffuse2_estimator_cov = None\n    if self.nobs_diffuse > 0:\n        self.scaled_smoothed_diffuse_estimator = np.array(smoother.scaled_smoothed_diffuse_estimator, copy=True)\n        self.scaled_smoothed_diffuse1_estimator_cov = np.array(smoother.scaled_smoothed_diffuse1_estimator_cov, copy=True)\n        self.scaled_smoothed_diffuse2_estimator_cov = np.array(smoother.scaled_smoothed_diffuse2_estimator_cov, copy=True)\n    start = 1\n    end = None\n    if 'scaled_smoothed_estimator' in attributes:\n        self.scaled_smoothed_estimator_presample = self.scaled_smoothed_estimator[:, 0]\n        self.scaled_smoothed_estimator = self.scaled_smoothed_estimator[:, start:end]\n    if 'scaled_smoothed_estimator_cov' in attributes:\n        self.scaled_smoothed_estimator_cov_presample = self.scaled_smoothed_estimator_cov[:, :, 0]\n        self.scaled_smoothed_estimator_cov = self.scaled_smoothed_estimator_cov[:, :, start:end]\n    self._smoothed_forecasts = None\n    self._smoothed_forecasts_error = None\n    self._smoothed_forecasts_error_cov = None\n    if self.filter_concentrated and self.model._scale is None:\n        self.smoothed_state_cov *= self.scale\n        self.smoothed_state_autocov *= self.scale\n        self.smoothed_state_disturbance_cov *= self.scale\n        self.smoothed_measurement_disturbance_cov *= self.scale\n        self.scaled_smoothed_estimator_presample /= self.scale\n        self.scaled_smoothed_estimator /= self.scale\n        self.scaled_smoothed_estimator_cov_presample /= self.scale\n        self.scaled_smoothed_estimator_cov /= self.scale\n        self.smoothing_error /= self.scale\n    self.__smoothed_state_autocovariance = {}",
        "mutated": [
            "def update_smoother(self, smoother):\n    if False:\n        i = 10\n    '\\n        Update the smoother results\\n\\n        Parameters\\n        ----------\\n        smoother : KalmanSmoother\\n            The model object from which to take the updated values.\\n\\n        Notes\\n        -----\\n        This method is rarely required except for internal usage.\\n        '\n    attributes = []\n    if self.smoother_state or self.smoother_disturbance:\n        attributes.append('scaled_smoothed_estimator')\n    if self.smoother_state_cov or self.smoother_disturbance_cov:\n        attributes.append('scaled_smoothed_estimator_cov')\n    if self.smoother_state:\n        attributes.append('smoothed_state')\n    if self.smoother_state_cov:\n        attributes.append('smoothed_state_cov')\n    if self.smoother_state_autocov:\n        attributes.append('smoothed_state_autocov')\n    if self.smoother_disturbance:\n        attributes += ['smoothing_error', 'smoothed_measurement_disturbance', 'smoothed_state_disturbance']\n    if self.smoother_disturbance_cov:\n        attributes += ['smoothed_measurement_disturbance_cov', 'smoothed_state_disturbance_cov']\n    has_missing = np.sum(self.nmissing) > 0\n    for name in self._smoother_attributes:\n        if name == 'smoother_output':\n            pass\n        elif name in attributes:\n            if name in ['smoothing_error', 'smoothed_measurement_disturbance']:\n                vector = getattr(smoother, name, None)\n                if vector is not None and has_missing:\n                    vector = np.array(reorder_missing_vector(vector, self.missing, prefix=self.prefix))\n                else:\n                    vector = np.array(vector, copy=True)\n                setattr(self, name, vector)\n            elif name == 'smoothed_measurement_disturbance_cov':\n                matrix = getattr(smoother, name, None)\n                if matrix is not None and has_missing:\n                    matrix = reorder_missing_matrix(matrix, self.missing, reorder_rows=True, reorder_cols=True, prefix=self.prefix)\n                    copy_index_matrix(self.obs_cov, matrix, self.missing, index_rows=True, index_cols=True, inplace=True, prefix=self.prefix)\n                else:\n                    matrix = np.array(matrix, copy=True)\n                setattr(self, name, matrix)\n            else:\n                setattr(self, name, np.array(getattr(smoother, name, None), copy=True))\n        else:\n            setattr(self, name, None)\n    self.innovations_transition = np.array(smoother.innovations_transition, copy=True)\n    self.scaled_smoothed_diffuse_estimator = None\n    self.scaled_smoothed_diffuse1_estimator_cov = None\n    self.scaled_smoothed_diffuse2_estimator_cov = None\n    if self.nobs_diffuse > 0:\n        self.scaled_smoothed_diffuse_estimator = np.array(smoother.scaled_smoothed_diffuse_estimator, copy=True)\n        self.scaled_smoothed_diffuse1_estimator_cov = np.array(smoother.scaled_smoothed_diffuse1_estimator_cov, copy=True)\n        self.scaled_smoothed_diffuse2_estimator_cov = np.array(smoother.scaled_smoothed_diffuse2_estimator_cov, copy=True)\n    start = 1\n    end = None\n    if 'scaled_smoothed_estimator' in attributes:\n        self.scaled_smoothed_estimator_presample = self.scaled_smoothed_estimator[:, 0]\n        self.scaled_smoothed_estimator = self.scaled_smoothed_estimator[:, start:end]\n    if 'scaled_smoothed_estimator_cov' in attributes:\n        self.scaled_smoothed_estimator_cov_presample = self.scaled_smoothed_estimator_cov[:, :, 0]\n        self.scaled_smoothed_estimator_cov = self.scaled_smoothed_estimator_cov[:, :, start:end]\n    self._smoothed_forecasts = None\n    self._smoothed_forecasts_error = None\n    self._smoothed_forecasts_error_cov = None\n    if self.filter_concentrated and self.model._scale is None:\n        self.smoothed_state_cov *= self.scale\n        self.smoothed_state_autocov *= self.scale\n        self.smoothed_state_disturbance_cov *= self.scale\n        self.smoothed_measurement_disturbance_cov *= self.scale\n        self.scaled_smoothed_estimator_presample /= self.scale\n        self.scaled_smoothed_estimator /= self.scale\n        self.scaled_smoothed_estimator_cov_presample /= self.scale\n        self.scaled_smoothed_estimator_cov /= self.scale\n        self.smoothing_error /= self.scale\n    self.__smoothed_state_autocovariance = {}",
            "def update_smoother(self, smoother):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Update the smoother results\\n\\n        Parameters\\n        ----------\\n        smoother : KalmanSmoother\\n            The model object from which to take the updated values.\\n\\n        Notes\\n        -----\\n        This method is rarely required except for internal usage.\\n        '\n    attributes = []\n    if self.smoother_state or self.smoother_disturbance:\n        attributes.append('scaled_smoothed_estimator')\n    if self.smoother_state_cov or self.smoother_disturbance_cov:\n        attributes.append('scaled_smoothed_estimator_cov')\n    if self.smoother_state:\n        attributes.append('smoothed_state')\n    if self.smoother_state_cov:\n        attributes.append('smoothed_state_cov')\n    if self.smoother_state_autocov:\n        attributes.append('smoothed_state_autocov')\n    if self.smoother_disturbance:\n        attributes += ['smoothing_error', 'smoothed_measurement_disturbance', 'smoothed_state_disturbance']\n    if self.smoother_disturbance_cov:\n        attributes += ['smoothed_measurement_disturbance_cov', 'smoothed_state_disturbance_cov']\n    has_missing = np.sum(self.nmissing) > 0\n    for name in self._smoother_attributes:\n        if name == 'smoother_output':\n            pass\n        elif name in attributes:\n            if name in ['smoothing_error', 'smoothed_measurement_disturbance']:\n                vector = getattr(smoother, name, None)\n                if vector is not None and has_missing:\n                    vector = np.array(reorder_missing_vector(vector, self.missing, prefix=self.prefix))\n                else:\n                    vector = np.array(vector, copy=True)\n                setattr(self, name, vector)\n            elif name == 'smoothed_measurement_disturbance_cov':\n                matrix = getattr(smoother, name, None)\n                if matrix is not None and has_missing:\n                    matrix = reorder_missing_matrix(matrix, self.missing, reorder_rows=True, reorder_cols=True, prefix=self.prefix)\n                    copy_index_matrix(self.obs_cov, matrix, self.missing, index_rows=True, index_cols=True, inplace=True, prefix=self.prefix)\n                else:\n                    matrix = np.array(matrix, copy=True)\n                setattr(self, name, matrix)\n            else:\n                setattr(self, name, np.array(getattr(smoother, name, None), copy=True))\n        else:\n            setattr(self, name, None)\n    self.innovations_transition = np.array(smoother.innovations_transition, copy=True)\n    self.scaled_smoothed_diffuse_estimator = None\n    self.scaled_smoothed_diffuse1_estimator_cov = None\n    self.scaled_smoothed_diffuse2_estimator_cov = None\n    if self.nobs_diffuse > 0:\n        self.scaled_smoothed_diffuse_estimator = np.array(smoother.scaled_smoothed_diffuse_estimator, copy=True)\n        self.scaled_smoothed_diffuse1_estimator_cov = np.array(smoother.scaled_smoothed_diffuse1_estimator_cov, copy=True)\n        self.scaled_smoothed_diffuse2_estimator_cov = np.array(smoother.scaled_smoothed_diffuse2_estimator_cov, copy=True)\n    start = 1\n    end = None\n    if 'scaled_smoothed_estimator' in attributes:\n        self.scaled_smoothed_estimator_presample = self.scaled_smoothed_estimator[:, 0]\n        self.scaled_smoothed_estimator = self.scaled_smoothed_estimator[:, start:end]\n    if 'scaled_smoothed_estimator_cov' in attributes:\n        self.scaled_smoothed_estimator_cov_presample = self.scaled_smoothed_estimator_cov[:, :, 0]\n        self.scaled_smoothed_estimator_cov = self.scaled_smoothed_estimator_cov[:, :, start:end]\n    self._smoothed_forecasts = None\n    self._smoothed_forecasts_error = None\n    self._smoothed_forecasts_error_cov = None\n    if self.filter_concentrated and self.model._scale is None:\n        self.smoothed_state_cov *= self.scale\n        self.smoothed_state_autocov *= self.scale\n        self.smoothed_state_disturbance_cov *= self.scale\n        self.smoothed_measurement_disturbance_cov *= self.scale\n        self.scaled_smoothed_estimator_presample /= self.scale\n        self.scaled_smoothed_estimator /= self.scale\n        self.scaled_smoothed_estimator_cov_presample /= self.scale\n        self.scaled_smoothed_estimator_cov /= self.scale\n        self.smoothing_error /= self.scale\n    self.__smoothed_state_autocovariance = {}",
            "def update_smoother(self, smoother):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Update the smoother results\\n\\n        Parameters\\n        ----------\\n        smoother : KalmanSmoother\\n            The model object from which to take the updated values.\\n\\n        Notes\\n        -----\\n        This method is rarely required except for internal usage.\\n        '\n    attributes = []\n    if self.smoother_state or self.smoother_disturbance:\n        attributes.append('scaled_smoothed_estimator')\n    if self.smoother_state_cov or self.smoother_disturbance_cov:\n        attributes.append('scaled_smoothed_estimator_cov')\n    if self.smoother_state:\n        attributes.append('smoothed_state')\n    if self.smoother_state_cov:\n        attributes.append('smoothed_state_cov')\n    if self.smoother_state_autocov:\n        attributes.append('smoothed_state_autocov')\n    if self.smoother_disturbance:\n        attributes += ['smoothing_error', 'smoothed_measurement_disturbance', 'smoothed_state_disturbance']\n    if self.smoother_disturbance_cov:\n        attributes += ['smoothed_measurement_disturbance_cov', 'smoothed_state_disturbance_cov']\n    has_missing = np.sum(self.nmissing) > 0\n    for name in self._smoother_attributes:\n        if name == 'smoother_output':\n            pass\n        elif name in attributes:\n            if name in ['smoothing_error', 'smoothed_measurement_disturbance']:\n                vector = getattr(smoother, name, None)\n                if vector is not None and has_missing:\n                    vector = np.array(reorder_missing_vector(vector, self.missing, prefix=self.prefix))\n                else:\n                    vector = np.array(vector, copy=True)\n                setattr(self, name, vector)\n            elif name == 'smoothed_measurement_disturbance_cov':\n                matrix = getattr(smoother, name, None)\n                if matrix is not None and has_missing:\n                    matrix = reorder_missing_matrix(matrix, self.missing, reorder_rows=True, reorder_cols=True, prefix=self.prefix)\n                    copy_index_matrix(self.obs_cov, matrix, self.missing, index_rows=True, index_cols=True, inplace=True, prefix=self.prefix)\n                else:\n                    matrix = np.array(matrix, copy=True)\n                setattr(self, name, matrix)\n            else:\n                setattr(self, name, np.array(getattr(smoother, name, None), copy=True))\n        else:\n            setattr(self, name, None)\n    self.innovations_transition = np.array(smoother.innovations_transition, copy=True)\n    self.scaled_smoothed_diffuse_estimator = None\n    self.scaled_smoothed_diffuse1_estimator_cov = None\n    self.scaled_smoothed_diffuse2_estimator_cov = None\n    if self.nobs_diffuse > 0:\n        self.scaled_smoothed_diffuse_estimator = np.array(smoother.scaled_smoothed_diffuse_estimator, copy=True)\n        self.scaled_smoothed_diffuse1_estimator_cov = np.array(smoother.scaled_smoothed_diffuse1_estimator_cov, copy=True)\n        self.scaled_smoothed_diffuse2_estimator_cov = np.array(smoother.scaled_smoothed_diffuse2_estimator_cov, copy=True)\n    start = 1\n    end = None\n    if 'scaled_smoothed_estimator' in attributes:\n        self.scaled_smoothed_estimator_presample = self.scaled_smoothed_estimator[:, 0]\n        self.scaled_smoothed_estimator = self.scaled_smoothed_estimator[:, start:end]\n    if 'scaled_smoothed_estimator_cov' in attributes:\n        self.scaled_smoothed_estimator_cov_presample = self.scaled_smoothed_estimator_cov[:, :, 0]\n        self.scaled_smoothed_estimator_cov = self.scaled_smoothed_estimator_cov[:, :, start:end]\n    self._smoothed_forecasts = None\n    self._smoothed_forecasts_error = None\n    self._smoothed_forecasts_error_cov = None\n    if self.filter_concentrated and self.model._scale is None:\n        self.smoothed_state_cov *= self.scale\n        self.smoothed_state_autocov *= self.scale\n        self.smoothed_state_disturbance_cov *= self.scale\n        self.smoothed_measurement_disturbance_cov *= self.scale\n        self.scaled_smoothed_estimator_presample /= self.scale\n        self.scaled_smoothed_estimator /= self.scale\n        self.scaled_smoothed_estimator_cov_presample /= self.scale\n        self.scaled_smoothed_estimator_cov /= self.scale\n        self.smoothing_error /= self.scale\n    self.__smoothed_state_autocovariance = {}",
            "def update_smoother(self, smoother):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Update the smoother results\\n\\n        Parameters\\n        ----------\\n        smoother : KalmanSmoother\\n            The model object from which to take the updated values.\\n\\n        Notes\\n        -----\\n        This method is rarely required except for internal usage.\\n        '\n    attributes = []\n    if self.smoother_state or self.smoother_disturbance:\n        attributes.append('scaled_smoothed_estimator')\n    if self.smoother_state_cov or self.smoother_disturbance_cov:\n        attributes.append('scaled_smoothed_estimator_cov')\n    if self.smoother_state:\n        attributes.append('smoothed_state')\n    if self.smoother_state_cov:\n        attributes.append('smoothed_state_cov')\n    if self.smoother_state_autocov:\n        attributes.append('smoothed_state_autocov')\n    if self.smoother_disturbance:\n        attributes += ['smoothing_error', 'smoothed_measurement_disturbance', 'smoothed_state_disturbance']\n    if self.smoother_disturbance_cov:\n        attributes += ['smoothed_measurement_disturbance_cov', 'smoothed_state_disturbance_cov']\n    has_missing = np.sum(self.nmissing) > 0\n    for name in self._smoother_attributes:\n        if name == 'smoother_output':\n            pass\n        elif name in attributes:\n            if name in ['smoothing_error', 'smoothed_measurement_disturbance']:\n                vector = getattr(smoother, name, None)\n                if vector is not None and has_missing:\n                    vector = np.array(reorder_missing_vector(vector, self.missing, prefix=self.prefix))\n                else:\n                    vector = np.array(vector, copy=True)\n                setattr(self, name, vector)\n            elif name == 'smoothed_measurement_disturbance_cov':\n                matrix = getattr(smoother, name, None)\n                if matrix is not None and has_missing:\n                    matrix = reorder_missing_matrix(matrix, self.missing, reorder_rows=True, reorder_cols=True, prefix=self.prefix)\n                    copy_index_matrix(self.obs_cov, matrix, self.missing, index_rows=True, index_cols=True, inplace=True, prefix=self.prefix)\n                else:\n                    matrix = np.array(matrix, copy=True)\n                setattr(self, name, matrix)\n            else:\n                setattr(self, name, np.array(getattr(smoother, name, None), copy=True))\n        else:\n            setattr(self, name, None)\n    self.innovations_transition = np.array(smoother.innovations_transition, copy=True)\n    self.scaled_smoothed_diffuse_estimator = None\n    self.scaled_smoothed_diffuse1_estimator_cov = None\n    self.scaled_smoothed_diffuse2_estimator_cov = None\n    if self.nobs_diffuse > 0:\n        self.scaled_smoothed_diffuse_estimator = np.array(smoother.scaled_smoothed_diffuse_estimator, copy=True)\n        self.scaled_smoothed_diffuse1_estimator_cov = np.array(smoother.scaled_smoothed_diffuse1_estimator_cov, copy=True)\n        self.scaled_smoothed_diffuse2_estimator_cov = np.array(smoother.scaled_smoothed_diffuse2_estimator_cov, copy=True)\n    start = 1\n    end = None\n    if 'scaled_smoothed_estimator' in attributes:\n        self.scaled_smoothed_estimator_presample = self.scaled_smoothed_estimator[:, 0]\n        self.scaled_smoothed_estimator = self.scaled_smoothed_estimator[:, start:end]\n    if 'scaled_smoothed_estimator_cov' in attributes:\n        self.scaled_smoothed_estimator_cov_presample = self.scaled_smoothed_estimator_cov[:, :, 0]\n        self.scaled_smoothed_estimator_cov = self.scaled_smoothed_estimator_cov[:, :, start:end]\n    self._smoothed_forecasts = None\n    self._smoothed_forecasts_error = None\n    self._smoothed_forecasts_error_cov = None\n    if self.filter_concentrated and self.model._scale is None:\n        self.smoothed_state_cov *= self.scale\n        self.smoothed_state_autocov *= self.scale\n        self.smoothed_state_disturbance_cov *= self.scale\n        self.smoothed_measurement_disturbance_cov *= self.scale\n        self.scaled_smoothed_estimator_presample /= self.scale\n        self.scaled_smoothed_estimator /= self.scale\n        self.scaled_smoothed_estimator_cov_presample /= self.scale\n        self.scaled_smoothed_estimator_cov /= self.scale\n        self.smoothing_error /= self.scale\n    self.__smoothed_state_autocovariance = {}",
            "def update_smoother(self, smoother):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Update the smoother results\\n\\n        Parameters\\n        ----------\\n        smoother : KalmanSmoother\\n            The model object from which to take the updated values.\\n\\n        Notes\\n        -----\\n        This method is rarely required except for internal usage.\\n        '\n    attributes = []\n    if self.smoother_state or self.smoother_disturbance:\n        attributes.append('scaled_smoothed_estimator')\n    if self.smoother_state_cov or self.smoother_disturbance_cov:\n        attributes.append('scaled_smoothed_estimator_cov')\n    if self.smoother_state:\n        attributes.append('smoothed_state')\n    if self.smoother_state_cov:\n        attributes.append('smoothed_state_cov')\n    if self.smoother_state_autocov:\n        attributes.append('smoothed_state_autocov')\n    if self.smoother_disturbance:\n        attributes += ['smoothing_error', 'smoothed_measurement_disturbance', 'smoothed_state_disturbance']\n    if self.smoother_disturbance_cov:\n        attributes += ['smoothed_measurement_disturbance_cov', 'smoothed_state_disturbance_cov']\n    has_missing = np.sum(self.nmissing) > 0\n    for name in self._smoother_attributes:\n        if name == 'smoother_output':\n            pass\n        elif name in attributes:\n            if name in ['smoothing_error', 'smoothed_measurement_disturbance']:\n                vector = getattr(smoother, name, None)\n                if vector is not None and has_missing:\n                    vector = np.array(reorder_missing_vector(vector, self.missing, prefix=self.prefix))\n                else:\n                    vector = np.array(vector, copy=True)\n                setattr(self, name, vector)\n            elif name == 'smoothed_measurement_disturbance_cov':\n                matrix = getattr(smoother, name, None)\n                if matrix is not None and has_missing:\n                    matrix = reorder_missing_matrix(matrix, self.missing, reorder_rows=True, reorder_cols=True, prefix=self.prefix)\n                    copy_index_matrix(self.obs_cov, matrix, self.missing, index_rows=True, index_cols=True, inplace=True, prefix=self.prefix)\n                else:\n                    matrix = np.array(matrix, copy=True)\n                setattr(self, name, matrix)\n            else:\n                setattr(self, name, np.array(getattr(smoother, name, None), copy=True))\n        else:\n            setattr(self, name, None)\n    self.innovations_transition = np.array(smoother.innovations_transition, copy=True)\n    self.scaled_smoothed_diffuse_estimator = None\n    self.scaled_smoothed_diffuse1_estimator_cov = None\n    self.scaled_smoothed_diffuse2_estimator_cov = None\n    if self.nobs_diffuse > 0:\n        self.scaled_smoothed_diffuse_estimator = np.array(smoother.scaled_smoothed_diffuse_estimator, copy=True)\n        self.scaled_smoothed_diffuse1_estimator_cov = np.array(smoother.scaled_smoothed_diffuse1_estimator_cov, copy=True)\n        self.scaled_smoothed_diffuse2_estimator_cov = np.array(smoother.scaled_smoothed_diffuse2_estimator_cov, copy=True)\n    start = 1\n    end = None\n    if 'scaled_smoothed_estimator' in attributes:\n        self.scaled_smoothed_estimator_presample = self.scaled_smoothed_estimator[:, 0]\n        self.scaled_smoothed_estimator = self.scaled_smoothed_estimator[:, start:end]\n    if 'scaled_smoothed_estimator_cov' in attributes:\n        self.scaled_smoothed_estimator_cov_presample = self.scaled_smoothed_estimator_cov[:, :, 0]\n        self.scaled_smoothed_estimator_cov = self.scaled_smoothed_estimator_cov[:, :, start:end]\n    self._smoothed_forecasts = None\n    self._smoothed_forecasts_error = None\n    self._smoothed_forecasts_error_cov = None\n    if self.filter_concentrated and self.model._scale is None:\n        self.smoothed_state_cov *= self.scale\n        self.smoothed_state_autocov *= self.scale\n        self.smoothed_state_disturbance_cov *= self.scale\n        self.smoothed_measurement_disturbance_cov *= self.scale\n        self.scaled_smoothed_estimator_presample /= self.scale\n        self.scaled_smoothed_estimator /= self.scale\n        self.scaled_smoothed_estimator_cov_presample /= self.scale\n        self.scaled_smoothed_estimator_cov /= self.scale\n        self.smoothing_error /= self.scale\n    self.__smoothed_state_autocovariance = {}"
        ]
    },
    {
        "func_name": "_smoothed_state_autocovariance",
        "original": "def _smoothed_state_autocovariance(self, shift, start, end, extend_kwargs=None):\n    \"\"\"\n        Compute \"forward\" autocovariances, Cov(t, t+j)\n\n        Parameters\n        ----------\n        shift : int\n            The number of period to shift forwards when computing the\n            autocovariance. This has the opposite sign as `lag` from the\n            `smoothed_state_autocovariance` method.\n        start : int, optional\n            The start of the interval (inclusive) of autocovariances to compute\n            and return.\n        end : int, optional\n            The end of the interval (exclusive) autocovariances to compute and\n            return. Note that since it is an exclusive endpoint, the returned\n            autocovariances do not include the value at this index.\n        extend_kwargs : dict, optional\n            Keyword arguments containing updated state space system matrices\n            for handling out-of-sample autocovariance computations in\n            time-varying state space models.\n\n        \"\"\"\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    n = end - start\n    if shift == 0:\n        max_insample = self.nobs - shift\n    else:\n        max_insample = self.nobs - shift + 1\n    n_postsample = max(0, end - max_insample)\n    if shift != 0:\n        L = self.innovations_transition\n        P = self.predicted_state_cov\n        N = self.scaled_smoothed_estimator_cov\n    else:\n        acov = self.smoothed_state_cov\n    if n_postsample > 0:\n        endog = np.zeros((n_postsample, self.k_endog)) * np.nan\n        mod = self.model.extend(endog, start=self.nobs, **extend_kwargs)\n        mod.initialize_known(self.predicted_state[..., self.nobs], self.predicted_state_cov[..., self.nobs])\n        res = mod.smooth()\n        if shift != 0:\n            start_insample = max(0, start)\n            L = np.concatenate((L[..., start_insample:], res.innovations_transition), axis=2)\n            P = np.concatenate((P[..., start_insample:], res.predicted_state_cov[..., 1:]), axis=2)\n            N = np.concatenate((N[..., start_insample:], res.scaled_smoothed_estimator_cov), axis=2)\n            end -= start_insample\n            start -= start_insample\n        else:\n            acov = np.concatenate((acov, res.predicted_state_cov), axis=2)\n    if shift != 0:\n        start_insample = max(0, start)\n        LT = L[..., start_insample:end + shift - 1].T\n        P = P[..., start_insample:end + shift].T\n        N = N[..., start_insample:end + shift - 1].T\n        tmpLT = np.eye(self.k_states)[None, :, :]\n        length = P.shape[0] - shift\n        for i in range(1, shift + 1):\n            tmpLT = LT[shift - i:length + shift - i] @ tmpLT\n        eye = np.eye(self.k_states)[None, ...]\n        acov = np.zeros((n, self.k_states, self.k_states))\n        acov[:start_insample - start] = np.nan\n        acov[start_insample - start:] = P[:-shift] @ tmpLT @ (eye - N[shift - 1:] @ P[shift:])\n    else:\n        acov = acov.T[start:end]\n    return acov",
        "mutated": [
            "def _smoothed_state_autocovariance(self, shift, start, end, extend_kwargs=None):\n    if False:\n        i = 10\n    '\\n        Compute \"forward\" autocovariances, Cov(t, t+j)\\n\\n        Parameters\\n        ----------\\n        shift : int\\n            The number of period to shift forwards when computing the\\n            autocovariance. This has the opposite sign as `lag` from the\\n            `smoothed_state_autocovariance` method.\\n        start : int, optional\\n            The start of the interval (inclusive) of autocovariances to compute\\n            and return.\\n        end : int, optional\\n            The end of the interval (exclusive) autocovariances to compute and\\n            return. Note that since it is an exclusive endpoint, the returned\\n            autocovariances do not include the value at this index.\\n        extend_kwargs : dict, optional\\n            Keyword arguments containing updated state space system matrices\\n            for handling out-of-sample autocovariance computations in\\n            time-varying state space models.\\n\\n        '\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    n = end - start\n    if shift == 0:\n        max_insample = self.nobs - shift\n    else:\n        max_insample = self.nobs - shift + 1\n    n_postsample = max(0, end - max_insample)\n    if shift != 0:\n        L = self.innovations_transition\n        P = self.predicted_state_cov\n        N = self.scaled_smoothed_estimator_cov\n    else:\n        acov = self.smoothed_state_cov\n    if n_postsample > 0:\n        endog = np.zeros((n_postsample, self.k_endog)) * np.nan\n        mod = self.model.extend(endog, start=self.nobs, **extend_kwargs)\n        mod.initialize_known(self.predicted_state[..., self.nobs], self.predicted_state_cov[..., self.nobs])\n        res = mod.smooth()\n        if shift != 0:\n            start_insample = max(0, start)\n            L = np.concatenate((L[..., start_insample:], res.innovations_transition), axis=2)\n            P = np.concatenate((P[..., start_insample:], res.predicted_state_cov[..., 1:]), axis=2)\n            N = np.concatenate((N[..., start_insample:], res.scaled_smoothed_estimator_cov), axis=2)\n            end -= start_insample\n            start -= start_insample\n        else:\n            acov = np.concatenate((acov, res.predicted_state_cov), axis=2)\n    if shift != 0:\n        start_insample = max(0, start)\n        LT = L[..., start_insample:end + shift - 1].T\n        P = P[..., start_insample:end + shift].T\n        N = N[..., start_insample:end + shift - 1].T\n        tmpLT = np.eye(self.k_states)[None, :, :]\n        length = P.shape[0] - shift\n        for i in range(1, shift + 1):\n            tmpLT = LT[shift - i:length + shift - i] @ tmpLT\n        eye = np.eye(self.k_states)[None, ...]\n        acov = np.zeros((n, self.k_states, self.k_states))\n        acov[:start_insample - start] = np.nan\n        acov[start_insample - start:] = P[:-shift] @ tmpLT @ (eye - N[shift - 1:] @ P[shift:])\n    else:\n        acov = acov.T[start:end]\n    return acov",
            "def _smoothed_state_autocovariance(self, shift, start, end, extend_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Compute \"forward\" autocovariances, Cov(t, t+j)\\n\\n        Parameters\\n        ----------\\n        shift : int\\n            The number of period to shift forwards when computing the\\n            autocovariance. This has the opposite sign as `lag` from the\\n            `smoothed_state_autocovariance` method.\\n        start : int, optional\\n            The start of the interval (inclusive) of autocovariances to compute\\n            and return.\\n        end : int, optional\\n            The end of the interval (exclusive) autocovariances to compute and\\n            return. Note that since it is an exclusive endpoint, the returned\\n            autocovariances do not include the value at this index.\\n        extend_kwargs : dict, optional\\n            Keyword arguments containing updated state space system matrices\\n            for handling out-of-sample autocovariance computations in\\n            time-varying state space models.\\n\\n        '\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    n = end - start\n    if shift == 0:\n        max_insample = self.nobs - shift\n    else:\n        max_insample = self.nobs - shift + 1\n    n_postsample = max(0, end - max_insample)\n    if shift != 0:\n        L = self.innovations_transition\n        P = self.predicted_state_cov\n        N = self.scaled_smoothed_estimator_cov\n    else:\n        acov = self.smoothed_state_cov\n    if n_postsample > 0:\n        endog = np.zeros((n_postsample, self.k_endog)) * np.nan\n        mod = self.model.extend(endog, start=self.nobs, **extend_kwargs)\n        mod.initialize_known(self.predicted_state[..., self.nobs], self.predicted_state_cov[..., self.nobs])\n        res = mod.smooth()\n        if shift != 0:\n            start_insample = max(0, start)\n            L = np.concatenate((L[..., start_insample:], res.innovations_transition), axis=2)\n            P = np.concatenate((P[..., start_insample:], res.predicted_state_cov[..., 1:]), axis=2)\n            N = np.concatenate((N[..., start_insample:], res.scaled_smoothed_estimator_cov), axis=2)\n            end -= start_insample\n            start -= start_insample\n        else:\n            acov = np.concatenate((acov, res.predicted_state_cov), axis=2)\n    if shift != 0:\n        start_insample = max(0, start)\n        LT = L[..., start_insample:end + shift - 1].T\n        P = P[..., start_insample:end + shift].T\n        N = N[..., start_insample:end + shift - 1].T\n        tmpLT = np.eye(self.k_states)[None, :, :]\n        length = P.shape[0] - shift\n        for i in range(1, shift + 1):\n            tmpLT = LT[shift - i:length + shift - i] @ tmpLT\n        eye = np.eye(self.k_states)[None, ...]\n        acov = np.zeros((n, self.k_states, self.k_states))\n        acov[:start_insample - start] = np.nan\n        acov[start_insample - start:] = P[:-shift] @ tmpLT @ (eye - N[shift - 1:] @ P[shift:])\n    else:\n        acov = acov.T[start:end]\n    return acov",
            "def _smoothed_state_autocovariance(self, shift, start, end, extend_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Compute \"forward\" autocovariances, Cov(t, t+j)\\n\\n        Parameters\\n        ----------\\n        shift : int\\n            The number of period to shift forwards when computing the\\n            autocovariance. This has the opposite sign as `lag` from the\\n            `smoothed_state_autocovariance` method.\\n        start : int, optional\\n            The start of the interval (inclusive) of autocovariances to compute\\n            and return.\\n        end : int, optional\\n            The end of the interval (exclusive) autocovariances to compute and\\n            return. Note that since it is an exclusive endpoint, the returned\\n            autocovariances do not include the value at this index.\\n        extend_kwargs : dict, optional\\n            Keyword arguments containing updated state space system matrices\\n            for handling out-of-sample autocovariance computations in\\n            time-varying state space models.\\n\\n        '\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    n = end - start\n    if shift == 0:\n        max_insample = self.nobs - shift\n    else:\n        max_insample = self.nobs - shift + 1\n    n_postsample = max(0, end - max_insample)\n    if shift != 0:\n        L = self.innovations_transition\n        P = self.predicted_state_cov\n        N = self.scaled_smoothed_estimator_cov\n    else:\n        acov = self.smoothed_state_cov\n    if n_postsample > 0:\n        endog = np.zeros((n_postsample, self.k_endog)) * np.nan\n        mod = self.model.extend(endog, start=self.nobs, **extend_kwargs)\n        mod.initialize_known(self.predicted_state[..., self.nobs], self.predicted_state_cov[..., self.nobs])\n        res = mod.smooth()\n        if shift != 0:\n            start_insample = max(0, start)\n            L = np.concatenate((L[..., start_insample:], res.innovations_transition), axis=2)\n            P = np.concatenate((P[..., start_insample:], res.predicted_state_cov[..., 1:]), axis=2)\n            N = np.concatenate((N[..., start_insample:], res.scaled_smoothed_estimator_cov), axis=2)\n            end -= start_insample\n            start -= start_insample\n        else:\n            acov = np.concatenate((acov, res.predicted_state_cov), axis=2)\n    if shift != 0:\n        start_insample = max(0, start)\n        LT = L[..., start_insample:end + shift - 1].T\n        P = P[..., start_insample:end + shift].T\n        N = N[..., start_insample:end + shift - 1].T\n        tmpLT = np.eye(self.k_states)[None, :, :]\n        length = P.shape[0] - shift\n        for i in range(1, shift + 1):\n            tmpLT = LT[shift - i:length + shift - i] @ tmpLT\n        eye = np.eye(self.k_states)[None, ...]\n        acov = np.zeros((n, self.k_states, self.k_states))\n        acov[:start_insample - start] = np.nan\n        acov[start_insample - start:] = P[:-shift] @ tmpLT @ (eye - N[shift - 1:] @ P[shift:])\n    else:\n        acov = acov.T[start:end]\n    return acov",
            "def _smoothed_state_autocovariance(self, shift, start, end, extend_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Compute \"forward\" autocovariances, Cov(t, t+j)\\n\\n        Parameters\\n        ----------\\n        shift : int\\n            The number of period to shift forwards when computing the\\n            autocovariance. This has the opposite sign as `lag` from the\\n            `smoothed_state_autocovariance` method.\\n        start : int, optional\\n            The start of the interval (inclusive) of autocovariances to compute\\n            and return.\\n        end : int, optional\\n            The end of the interval (exclusive) autocovariances to compute and\\n            return. Note that since it is an exclusive endpoint, the returned\\n            autocovariances do not include the value at this index.\\n        extend_kwargs : dict, optional\\n            Keyword arguments containing updated state space system matrices\\n            for handling out-of-sample autocovariance computations in\\n            time-varying state space models.\\n\\n        '\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    n = end - start\n    if shift == 0:\n        max_insample = self.nobs - shift\n    else:\n        max_insample = self.nobs - shift + 1\n    n_postsample = max(0, end - max_insample)\n    if shift != 0:\n        L = self.innovations_transition\n        P = self.predicted_state_cov\n        N = self.scaled_smoothed_estimator_cov\n    else:\n        acov = self.smoothed_state_cov\n    if n_postsample > 0:\n        endog = np.zeros((n_postsample, self.k_endog)) * np.nan\n        mod = self.model.extend(endog, start=self.nobs, **extend_kwargs)\n        mod.initialize_known(self.predicted_state[..., self.nobs], self.predicted_state_cov[..., self.nobs])\n        res = mod.smooth()\n        if shift != 0:\n            start_insample = max(0, start)\n            L = np.concatenate((L[..., start_insample:], res.innovations_transition), axis=2)\n            P = np.concatenate((P[..., start_insample:], res.predicted_state_cov[..., 1:]), axis=2)\n            N = np.concatenate((N[..., start_insample:], res.scaled_smoothed_estimator_cov), axis=2)\n            end -= start_insample\n            start -= start_insample\n        else:\n            acov = np.concatenate((acov, res.predicted_state_cov), axis=2)\n    if shift != 0:\n        start_insample = max(0, start)\n        LT = L[..., start_insample:end + shift - 1].T\n        P = P[..., start_insample:end + shift].T\n        N = N[..., start_insample:end + shift - 1].T\n        tmpLT = np.eye(self.k_states)[None, :, :]\n        length = P.shape[0] - shift\n        for i in range(1, shift + 1):\n            tmpLT = LT[shift - i:length + shift - i] @ tmpLT\n        eye = np.eye(self.k_states)[None, ...]\n        acov = np.zeros((n, self.k_states, self.k_states))\n        acov[:start_insample - start] = np.nan\n        acov[start_insample - start:] = P[:-shift] @ tmpLT @ (eye - N[shift - 1:] @ P[shift:])\n    else:\n        acov = acov.T[start:end]\n    return acov",
            "def _smoothed_state_autocovariance(self, shift, start, end, extend_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Compute \"forward\" autocovariances, Cov(t, t+j)\\n\\n        Parameters\\n        ----------\\n        shift : int\\n            The number of period to shift forwards when computing the\\n            autocovariance. This has the opposite sign as `lag` from the\\n            `smoothed_state_autocovariance` method.\\n        start : int, optional\\n            The start of the interval (inclusive) of autocovariances to compute\\n            and return.\\n        end : int, optional\\n            The end of the interval (exclusive) autocovariances to compute and\\n            return. Note that since it is an exclusive endpoint, the returned\\n            autocovariances do not include the value at this index.\\n        extend_kwargs : dict, optional\\n            Keyword arguments containing updated state space system matrices\\n            for handling out-of-sample autocovariance computations in\\n            time-varying state space models.\\n\\n        '\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    n = end - start\n    if shift == 0:\n        max_insample = self.nobs - shift\n    else:\n        max_insample = self.nobs - shift + 1\n    n_postsample = max(0, end - max_insample)\n    if shift != 0:\n        L = self.innovations_transition\n        P = self.predicted_state_cov\n        N = self.scaled_smoothed_estimator_cov\n    else:\n        acov = self.smoothed_state_cov\n    if n_postsample > 0:\n        endog = np.zeros((n_postsample, self.k_endog)) * np.nan\n        mod = self.model.extend(endog, start=self.nobs, **extend_kwargs)\n        mod.initialize_known(self.predicted_state[..., self.nobs], self.predicted_state_cov[..., self.nobs])\n        res = mod.smooth()\n        if shift != 0:\n            start_insample = max(0, start)\n            L = np.concatenate((L[..., start_insample:], res.innovations_transition), axis=2)\n            P = np.concatenate((P[..., start_insample:], res.predicted_state_cov[..., 1:]), axis=2)\n            N = np.concatenate((N[..., start_insample:], res.scaled_smoothed_estimator_cov), axis=2)\n            end -= start_insample\n            start -= start_insample\n        else:\n            acov = np.concatenate((acov, res.predicted_state_cov), axis=2)\n    if shift != 0:\n        start_insample = max(0, start)\n        LT = L[..., start_insample:end + shift - 1].T\n        P = P[..., start_insample:end + shift].T\n        N = N[..., start_insample:end + shift - 1].T\n        tmpLT = np.eye(self.k_states)[None, :, :]\n        length = P.shape[0] - shift\n        for i in range(1, shift + 1):\n            tmpLT = LT[shift - i:length + shift - i] @ tmpLT\n        eye = np.eye(self.k_states)[None, ...]\n        acov = np.zeros((n, self.k_states, self.k_states))\n        acov[:start_insample - start] = np.nan\n        acov[start_insample - start:] = P[:-shift] @ tmpLT @ (eye - N[shift - 1:] @ P[shift:])\n    else:\n        acov = acov.T[start:end]\n    return acov"
        ]
    },
    {
        "func_name": "smoothed_state_autocovariance",
        "original": "def smoothed_state_autocovariance(self, lag=1, t=None, start=None, end=None, extend_kwargs=None):\n    \"\"\"\n        Compute state vector autocovariances, conditional on the full dataset\n\n        Computes:\n\n        .. math::\n\n            Cov(\\\\alpha_t - \\\\hat \\\\alpha_t, \\\\alpha_{t - j} - \\\\hat \\\\alpha_{t - j})\n\n        where the `lag` argument gives the value for :math:`j`. Thus when\n        the `lag` argument is positive, the autocovariance is between the\n        current and previous periods, while if `lag` is negative the\n        autocovariance is between the current and future periods.\n\n        Parameters\n        ----------\n        lag : int, optional\n            The number of period to shift when computing the autocovariance.\n            Default is 1.\n        t : int, optional\n            A specific period for which to compute and return the\n            autocovariance. Cannot be used in combination with `start` or\n            `end`. See the Returns section for details on how this\n            parameter affects what is what is returned.\n        start : int, optional\n            The start of the interval (inclusive) of autocovariances to compute\n            and return. Cannot be used in combination with the `t` argument.\n            See the Returns section for details on how this parameter affects\n            what is what is returned. Default is 0.\n        end : int, optional\n            The end of the interval (exclusive) autocovariances to compute and\n            return. Note that since it is an exclusive endpoint, the returned\n            autocovariances do not include the value at this index. Cannot be\n            used in combination with the `t` argument. See the Returns section\n            for details on how this parameter affects what is what is returned\n            and what the default value is.\n        extend_kwargs : dict, optional\n            Keyword arguments containing updated state space system matrices\n            for handling out-of-sample autocovariance computations in\n            time-varying state space models.\n\n        Returns\n        -------\n        acov : ndarray\n            Array of autocovariance matrices. If the argument `t` is not\n            provided, then it is shaped `(k_states, k_states, n)`, while if `t`\n            given then the third axis is dropped and the array is shaped\n            `(k_states, k_states)`.\n\n            The output under the default case differs somewhat based on the\n            state space model and the sign of the lag. To see how these cases\n            differ, denote the output at each time point as Cov(t, t-j). Then:\n\n            - If `lag > 0` (and the model is either time-varying or\n              time-invariant), then the returned array is shaped `(*, *, nobs)`\n              and each entry [:, :, t] contains Cov(t, t-j). However, the model\n              does not have enough information to compute autocovariances in\n              the pre-sample period, so that we cannot compute Cov(1, 1-lag),\n              Cov(2, 2-lag), ..., Cov(lag, 0). Thus the first `lag` entries\n              have all values set to NaN.\n\n            - If the model is time-invariant and `lag < -1` or if `lag` is\n              0 or -1, and the model is either time-invariant or time-varying,\n              then the returned array is shaped `(*, *, nobs)` and each\n              entry [:, :, t] contains Cov(t, t+j). Moreover, all entries are\n              available (i.e. there are no NaNs).\n\n            - If the model is time-varying and `lag < -1` and `extend_kwargs`\n              is not provided, then the returned array is shaped\n              `(*, *, nobs - lag + 1)`.\n\n            - However, if the model is time-varying and `lag < -1`, then\n              `extend_kwargs` can be provided with `lag - 1` additional\n              matrices so that the returned array is shaped `(*, *, nobs)` as\n              usual.\n\n            More generally, the dimension of the last axis will be\n            `start - end`.\n\n        Notes\n        -----\n        This method computes:\n\n        .. math::\n\n            Cov(\\\\alpha_t - \\\\hat \\\\alpha_t, \\\\alpha_{t - j} - \\\\hat \\\\alpha_{t - j})\n\n        where the `lag` argument determines the autocovariance order :math:`j`,\n        and `lag` is an integer (positive, zero, or negative). This method\n        cannot compute values associated with time points prior to the sample,\n        and so it returns a matrix of NaN values for these time points.\n        For example, if `start=0` and `lag=2`, then assuming the output is\n        assigned to the variable `acov`, we will have `acov[..., 0]` and\n        `acov[..., 1]` as matrices filled with NaN values.\n\n        Based only on the \"current\" results object (i.e. the Kalman smoother\n        applied to the sample), there is not enough information to compute\n        Cov(t, t+j) for the last `lag - 1` observations of the sample. However,\n        the values can be computed for these time points using the transition\n        equation of the state space representation, and so for time-invariant\n        state space models we do compute these values. For time-varying models,\n        this can also be done, but updated state space matrices for the\n        out-of-sample time points must be provided via the `extend_kwargs`\n        argument.\n\n        See [1]_, Chapter 4.7, for all details about how these autocovariances\n        are computed.\n\n        The `t` and `start`/`end` parameters compute and return only the\n        requested autocovariances. As a result, using these parameters is\n        recommended to reduce the computational burden, particularly if the\n        number of observations and/or the dimension of the state vector is\n        large.\n\n        References\n        ----------\n        .. [1] Durbin, James, and Siem Jan Koopman. 2012.\n               Time Series Analysis by State Space Methods: Second Edition.\n               Oxford University Press.\n        \"\"\"\n    cache_key = None\n    if extend_kwargs is None or len(extend_kwargs) == 0:\n        cache_key = (lag, t, start, end)\n    if cache_key is not None and cache_key in self.__smoothed_state_autocovariance:\n        return self.__smoothed_state_autocovariance[cache_key]\n    forward_autocovariances = False\n    if lag < 0:\n        lag = -lag\n        forward_autocovariances = True\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = 0\n    if end is None:\n        if forward_autocovariances and lag > 1 and (extend_kwargs is None):\n            end = self.nobs - lag + 1\n        else:\n            end = self.nobs\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end < start:\n        raise ValueError('`end` must be after `start`')\n    if lag == 0 and self.smoothed_state_cov is None:\n        raise RuntimeError('Cannot return smoothed state covariances if those values have not been computed by Kalman smoothing.')\n    if lag == 0 and end <= self.nobs + 1:\n        acov = self.smoothed_state_cov\n        if end == self.nobs + 1:\n            acov = np.concatenate((acov[..., start:], self.predicted_state_cov[..., -1:]), axis=2).T\n        else:\n            acov = acov.T[start:end]\n    elif lag == 1 and self.smoothed_state_autocov is not None and (not forward_autocovariances) and (end <= self.nobs + 1):\n        if start == 0:\n            nans = np.zeros((self.k_states, self.k_states, lag)) * np.nan\n            acov = np.concatenate((nans, self.smoothed_state_autocov[..., :end - 1]), axis=2)\n        else:\n            acov = self.smoothed_state_autocov[..., start - 1:end - 1]\n        acov = acov.transpose(2, 0, 1)\n    elif lag == 1 and self.smoothed_state_autocov is not None and forward_autocovariances and (end < self.nobs + 1):\n        acov = self.smoothed_state_autocov.T[start:end]\n    elif forward_autocovariances:\n        acov = self._smoothed_state_autocovariance(lag, start, end, extend_kwargs=extend_kwargs)\n    else:\n        out = self._smoothed_state_autocovariance(lag, start - lag, end - lag, extend_kwargs=extend_kwargs)\n        acov = out.transpose(0, 2, 1)\n    if t is not None:\n        acov = acov[0]\n    else:\n        acov = acov.transpose(1, 2, 0)\n    if cache_key is not None:\n        self.__smoothed_state_autocovariance[cache_key] = acov\n    return acov",
        "mutated": [
            "def smoothed_state_autocovariance(self, lag=1, t=None, start=None, end=None, extend_kwargs=None):\n    if False:\n        i = 10\n    '\\n        Compute state vector autocovariances, conditional on the full dataset\\n\\n        Computes:\\n\\n        .. math::\\n\\n            Cov(\\\\alpha_t - \\\\hat \\\\alpha_t, \\\\alpha_{t - j} - \\\\hat \\\\alpha_{t - j})\\n\\n        where the `lag` argument gives the value for :math:`j`. Thus when\\n        the `lag` argument is positive, the autocovariance is between the\\n        current and previous periods, while if `lag` is negative the\\n        autocovariance is between the current and future periods.\\n\\n        Parameters\\n        ----------\\n        lag : int, optional\\n            The number of period to shift when computing the autocovariance.\\n            Default is 1.\\n        t : int, optional\\n            A specific period for which to compute and return the\\n            autocovariance. Cannot be used in combination with `start` or\\n            `end`. See the Returns section for details on how this\\n            parameter affects what is what is returned.\\n        start : int, optional\\n            The start of the interval (inclusive) of autocovariances to compute\\n            and return. Cannot be used in combination with the `t` argument.\\n            See the Returns section for details on how this parameter affects\\n            what is what is returned. Default is 0.\\n        end : int, optional\\n            The end of the interval (exclusive) autocovariances to compute and\\n            return. Note that since it is an exclusive endpoint, the returned\\n            autocovariances do not include the value at this index. Cannot be\\n            used in combination with the `t` argument. See the Returns section\\n            for details on how this parameter affects what is what is returned\\n            and what the default value is.\\n        extend_kwargs : dict, optional\\n            Keyword arguments containing updated state space system matrices\\n            for handling out-of-sample autocovariance computations in\\n            time-varying state space models.\\n\\n        Returns\\n        -------\\n        acov : ndarray\\n            Array of autocovariance matrices. If the argument `t` is not\\n            provided, then it is shaped `(k_states, k_states, n)`, while if `t`\\n            given then the third axis is dropped and the array is shaped\\n            `(k_states, k_states)`.\\n\\n            The output under the default case differs somewhat based on the\\n            state space model and the sign of the lag. To see how these cases\\n            differ, denote the output at each time point as Cov(t, t-j). Then:\\n\\n            - If `lag > 0` (and the model is either time-varying or\\n              time-invariant), then the returned array is shaped `(*, *, nobs)`\\n              and each entry [:, :, t] contains Cov(t, t-j). However, the model\\n              does not have enough information to compute autocovariances in\\n              the pre-sample period, so that we cannot compute Cov(1, 1-lag),\\n              Cov(2, 2-lag), ..., Cov(lag, 0). Thus the first `lag` entries\\n              have all values set to NaN.\\n\\n            - If the model is time-invariant and `lag < -1` or if `lag` is\\n              0 or -1, and the model is either time-invariant or time-varying,\\n              then the returned array is shaped `(*, *, nobs)` and each\\n              entry [:, :, t] contains Cov(t, t+j). Moreover, all entries are\\n              available (i.e. there are no NaNs).\\n\\n            - If the model is time-varying and `lag < -1` and `extend_kwargs`\\n              is not provided, then the returned array is shaped\\n              `(*, *, nobs - lag + 1)`.\\n\\n            - However, if the model is time-varying and `lag < -1`, then\\n              `extend_kwargs` can be provided with `lag - 1` additional\\n              matrices so that the returned array is shaped `(*, *, nobs)` as\\n              usual.\\n\\n            More generally, the dimension of the last axis will be\\n            `start - end`.\\n\\n        Notes\\n        -----\\n        This method computes:\\n\\n        .. math::\\n\\n            Cov(\\\\alpha_t - \\\\hat \\\\alpha_t, \\\\alpha_{t - j} - \\\\hat \\\\alpha_{t - j})\\n\\n        where the `lag` argument determines the autocovariance order :math:`j`,\\n        and `lag` is an integer (positive, zero, or negative). This method\\n        cannot compute values associated with time points prior to the sample,\\n        and so it returns a matrix of NaN values for these time points.\\n        For example, if `start=0` and `lag=2`, then assuming the output is\\n        assigned to the variable `acov`, we will have `acov[..., 0]` and\\n        `acov[..., 1]` as matrices filled with NaN values.\\n\\n        Based only on the \"current\" results object (i.e. the Kalman smoother\\n        applied to the sample), there is not enough information to compute\\n        Cov(t, t+j) for the last `lag - 1` observations of the sample. However,\\n        the values can be computed for these time points using the transition\\n        equation of the state space representation, and so for time-invariant\\n        state space models we do compute these values. For time-varying models,\\n        this can also be done, but updated state space matrices for the\\n        out-of-sample time points must be provided via the `extend_kwargs`\\n        argument.\\n\\n        See [1]_, Chapter 4.7, for all details about how these autocovariances\\n        are computed.\\n\\n        The `t` and `start`/`end` parameters compute and return only the\\n        requested autocovariances. As a result, using these parameters is\\n        recommended to reduce the computational burden, particularly if the\\n        number of observations and/or the dimension of the state vector is\\n        large.\\n\\n        References\\n        ----------\\n        .. [1] Durbin, James, and Siem Jan Koopman. 2012.\\n               Time Series Analysis by State Space Methods: Second Edition.\\n               Oxford University Press.\\n        '\n    cache_key = None\n    if extend_kwargs is None or len(extend_kwargs) == 0:\n        cache_key = (lag, t, start, end)\n    if cache_key is not None and cache_key in self.__smoothed_state_autocovariance:\n        return self.__smoothed_state_autocovariance[cache_key]\n    forward_autocovariances = False\n    if lag < 0:\n        lag = -lag\n        forward_autocovariances = True\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = 0\n    if end is None:\n        if forward_autocovariances and lag > 1 and (extend_kwargs is None):\n            end = self.nobs - lag + 1\n        else:\n            end = self.nobs\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end < start:\n        raise ValueError('`end` must be after `start`')\n    if lag == 0 and self.smoothed_state_cov is None:\n        raise RuntimeError('Cannot return smoothed state covariances if those values have not been computed by Kalman smoothing.')\n    if lag == 0 and end <= self.nobs + 1:\n        acov = self.smoothed_state_cov\n        if end == self.nobs + 1:\n            acov = np.concatenate((acov[..., start:], self.predicted_state_cov[..., -1:]), axis=2).T\n        else:\n            acov = acov.T[start:end]\n    elif lag == 1 and self.smoothed_state_autocov is not None and (not forward_autocovariances) and (end <= self.nobs + 1):\n        if start == 0:\n            nans = np.zeros((self.k_states, self.k_states, lag)) * np.nan\n            acov = np.concatenate((nans, self.smoothed_state_autocov[..., :end - 1]), axis=2)\n        else:\n            acov = self.smoothed_state_autocov[..., start - 1:end - 1]\n        acov = acov.transpose(2, 0, 1)\n    elif lag == 1 and self.smoothed_state_autocov is not None and forward_autocovariances and (end < self.nobs + 1):\n        acov = self.smoothed_state_autocov.T[start:end]\n    elif forward_autocovariances:\n        acov = self._smoothed_state_autocovariance(lag, start, end, extend_kwargs=extend_kwargs)\n    else:\n        out = self._smoothed_state_autocovariance(lag, start - lag, end - lag, extend_kwargs=extend_kwargs)\n        acov = out.transpose(0, 2, 1)\n    if t is not None:\n        acov = acov[0]\n    else:\n        acov = acov.transpose(1, 2, 0)\n    if cache_key is not None:\n        self.__smoothed_state_autocovariance[cache_key] = acov\n    return acov",
            "def smoothed_state_autocovariance(self, lag=1, t=None, start=None, end=None, extend_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Compute state vector autocovariances, conditional on the full dataset\\n\\n        Computes:\\n\\n        .. math::\\n\\n            Cov(\\\\alpha_t - \\\\hat \\\\alpha_t, \\\\alpha_{t - j} - \\\\hat \\\\alpha_{t - j})\\n\\n        where the `lag` argument gives the value for :math:`j`. Thus when\\n        the `lag` argument is positive, the autocovariance is between the\\n        current and previous periods, while if `lag` is negative the\\n        autocovariance is between the current and future periods.\\n\\n        Parameters\\n        ----------\\n        lag : int, optional\\n            The number of period to shift when computing the autocovariance.\\n            Default is 1.\\n        t : int, optional\\n            A specific period for which to compute and return the\\n            autocovariance. Cannot be used in combination with `start` or\\n            `end`. See the Returns section for details on how this\\n            parameter affects what is what is returned.\\n        start : int, optional\\n            The start of the interval (inclusive) of autocovariances to compute\\n            and return. Cannot be used in combination with the `t` argument.\\n            See the Returns section for details on how this parameter affects\\n            what is what is returned. Default is 0.\\n        end : int, optional\\n            The end of the interval (exclusive) autocovariances to compute and\\n            return. Note that since it is an exclusive endpoint, the returned\\n            autocovariances do not include the value at this index. Cannot be\\n            used in combination with the `t` argument. See the Returns section\\n            for details on how this parameter affects what is what is returned\\n            and what the default value is.\\n        extend_kwargs : dict, optional\\n            Keyword arguments containing updated state space system matrices\\n            for handling out-of-sample autocovariance computations in\\n            time-varying state space models.\\n\\n        Returns\\n        -------\\n        acov : ndarray\\n            Array of autocovariance matrices. If the argument `t` is not\\n            provided, then it is shaped `(k_states, k_states, n)`, while if `t`\\n            given then the third axis is dropped and the array is shaped\\n            `(k_states, k_states)`.\\n\\n            The output under the default case differs somewhat based on the\\n            state space model and the sign of the lag. To see how these cases\\n            differ, denote the output at each time point as Cov(t, t-j). Then:\\n\\n            - If `lag > 0` (and the model is either time-varying or\\n              time-invariant), then the returned array is shaped `(*, *, nobs)`\\n              and each entry [:, :, t] contains Cov(t, t-j). However, the model\\n              does not have enough information to compute autocovariances in\\n              the pre-sample period, so that we cannot compute Cov(1, 1-lag),\\n              Cov(2, 2-lag), ..., Cov(lag, 0). Thus the first `lag` entries\\n              have all values set to NaN.\\n\\n            - If the model is time-invariant and `lag < -1` or if `lag` is\\n              0 or -1, and the model is either time-invariant or time-varying,\\n              then the returned array is shaped `(*, *, nobs)` and each\\n              entry [:, :, t] contains Cov(t, t+j). Moreover, all entries are\\n              available (i.e. there are no NaNs).\\n\\n            - If the model is time-varying and `lag < -1` and `extend_kwargs`\\n              is not provided, then the returned array is shaped\\n              `(*, *, nobs - lag + 1)`.\\n\\n            - However, if the model is time-varying and `lag < -1`, then\\n              `extend_kwargs` can be provided with `lag - 1` additional\\n              matrices so that the returned array is shaped `(*, *, nobs)` as\\n              usual.\\n\\n            More generally, the dimension of the last axis will be\\n            `start - end`.\\n\\n        Notes\\n        -----\\n        This method computes:\\n\\n        .. math::\\n\\n            Cov(\\\\alpha_t - \\\\hat \\\\alpha_t, \\\\alpha_{t - j} - \\\\hat \\\\alpha_{t - j})\\n\\n        where the `lag` argument determines the autocovariance order :math:`j`,\\n        and `lag` is an integer (positive, zero, or negative). This method\\n        cannot compute values associated with time points prior to the sample,\\n        and so it returns a matrix of NaN values for these time points.\\n        For example, if `start=0` and `lag=2`, then assuming the output is\\n        assigned to the variable `acov`, we will have `acov[..., 0]` and\\n        `acov[..., 1]` as matrices filled with NaN values.\\n\\n        Based only on the \"current\" results object (i.e. the Kalman smoother\\n        applied to the sample), there is not enough information to compute\\n        Cov(t, t+j) for the last `lag - 1` observations of the sample. However,\\n        the values can be computed for these time points using the transition\\n        equation of the state space representation, and so for time-invariant\\n        state space models we do compute these values. For time-varying models,\\n        this can also be done, but updated state space matrices for the\\n        out-of-sample time points must be provided via the `extend_kwargs`\\n        argument.\\n\\n        See [1]_, Chapter 4.7, for all details about how these autocovariances\\n        are computed.\\n\\n        The `t` and `start`/`end` parameters compute and return only the\\n        requested autocovariances. As a result, using these parameters is\\n        recommended to reduce the computational burden, particularly if the\\n        number of observations and/or the dimension of the state vector is\\n        large.\\n\\n        References\\n        ----------\\n        .. [1] Durbin, James, and Siem Jan Koopman. 2012.\\n               Time Series Analysis by State Space Methods: Second Edition.\\n               Oxford University Press.\\n        '\n    cache_key = None\n    if extend_kwargs is None or len(extend_kwargs) == 0:\n        cache_key = (lag, t, start, end)\n    if cache_key is not None and cache_key in self.__smoothed_state_autocovariance:\n        return self.__smoothed_state_autocovariance[cache_key]\n    forward_autocovariances = False\n    if lag < 0:\n        lag = -lag\n        forward_autocovariances = True\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = 0\n    if end is None:\n        if forward_autocovariances and lag > 1 and (extend_kwargs is None):\n            end = self.nobs - lag + 1\n        else:\n            end = self.nobs\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end < start:\n        raise ValueError('`end` must be after `start`')\n    if lag == 0 and self.smoothed_state_cov is None:\n        raise RuntimeError('Cannot return smoothed state covariances if those values have not been computed by Kalman smoothing.')\n    if lag == 0 and end <= self.nobs + 1:\n        acov = self.smoothed_state_cov\n        if end == self.nobs + 1:\n            acov = np.concatenate((acov[..., start:], self.predicted_state_cov[..., -1:]), axis=2).T\n        else:\n            acov = acov.T[start:end]\n    elif lag == 1 and self.smoothed_state_autocov is not None and (not forward_autocovariances) and (end <= self.nobs + 1):\n        if start == 0:\n            nans = np.zeros((self.k_states, self.k_states, lag)) * np.nan\n            acov = np.concatenate((nans, self.smoothed_state_autocov[..., :end - 1]), axis=2)\n        else:\n            acov = self.smoothed_state_autocov[..., start - 1:end - 1]\n        acov = acov.transpose(2, 0, 1)\n    elif lag == 1 and self.smoothed_state_autocov is not None and forward_autocovariances and (end < self.nobs + 1):\n        acov = self.smoothed_state_autocov.T[start:end]\n    elif forward_autocovariances:\n        acov = self._smoothed_state_autocovariance(lag, start, end, extend_kwargs=extend_kwargs)\n    else:\n        out = self._smoothed_state_autocovariance(lag, start - lag, end - lag, extend_kwargs=extend_kwargs)\n        acov = out.transpose(0, 2, 1)\n    if t is not None:\n        acov = acov[0]\n    else:\n        acov = acov.transpose(1, 2, 0)\n    if cache_key is not None:\n        self.__smoothed_state_autocovariance[cache_key] = acov\n    return acov",
            "def smoothed_state_autocovariance(self, lag=1, t=None, start=None, end=None, extend_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Compute state vector autocovariances, conditional on the full dataset\\n\\n        Computes:\\n\\n        .. math::\\n\\n            Cov(\\\\alpha_t - \\\\hat \\\\alpha_t, \\\\alpha_{t - j} - \\\\hat \\\\alpha_{t - j})\\n\\n        where the `lag` argument gives the value for :math:`j`. Thus when\\n        the `lag` argument is positive, the autocovariance is between the\\n        current and previous periods, while if `lag` is negative the\\n        autocovariance is between the current and future periods.\\n\\n        Parameters\\n        ----------\\n        lag : int, optional\\n            The number of period to shift when computing the autocovariance.\\n            Default is 1.\\n        t : int, optional\\n            A specific period for which to compute and return the\\n            autocovariance. Cannot be used in combination with `start` or\\n            `end`. See the Returns section for details on how this\\n            parameter affects what is what is returned.\\n        start : int, optional\\n            The start of the interval (inclusive) of autocovariances to compute\\n            and return. Cannot be used in combination with the `t` argument.\\n            See the Returns section for details on how this parameter affects\\n            what is what is returned. Default is 0.\\n        end : int, optional\\n            The end of the interval (exclusive) autocovariances to compute and\\n            return. Note that since it is an exclusive endpoint, the returned\\n            autocovariances do not include the value at this index. Cannot be\\n            used in combination with the `t` argument. See the Returns section\\n            for details on how this parameter affects what is what is returned\\n            and what the default value is.\\n        extend_kwargs : dict, optional\\n            Keyword arguments containing updated state space system matrices\\n            for handling out-of-sample autocovariance computations in\\n            time-varying state space models.\\n\\n        Returns\\n        -------\\n        acov : ndarray\\n            Array of autocovariance matrices. If the argument `t` is not\\n            provided, then it is shaped `(k_states, k_states, n)`, while if `t`\\n            given then the third axis is dropped and the array is shaped\\n            `(k_states, k_states)`.\\n\\n            The output under the default case differs somewhat based on the\\n            state space model and the sign of the lag. To see how these cases\\n            differ, denote the output at each time point as Cov(t, t-j). Then:\\n\\n            - If `lag > 0` (and the model is either time-varying or\\n              time-invariant), then the returned array is shaped `(*, *, nobs)`\\n              and each entry [:, :, t] contains Cov(t, t-j). However, the model\\n              does not have enough information to compute autocovariances in\\n              the pre-sample period, so that we cannot compute Cov(1, 1-lag),\\n              Cov(2, 2-lag), ..., Cov(lag, 0). Thus the first `lag` entries\\n              have all values set to NaN.\\n\\n            - If the model is time-invariant and `lag < -1` or if `lag` is\\n              0 or -1, and the model is either time-invariant or time-varying,\\n              then the returned array is shaped `(*, *, nobs)` and each\\n              entry [:, :, t] contains Cov(t, t+j). Moreover, all entries are\\n              available (i.e. there are no NaNs).\\n\\n            - If the model is time-varying and `lag < -1` and `extend_kwargs`\\n              is not provided, then the returned array is shaped\\n              `(*, *, nobs - lag + 1)`.\\n\\n            - However, if the model is time-varying and `lag < -1`, then\\n              `extend_kwargs` can be provided with `lag - 1` additional\\n              matrices so that the returned array is shaped `(*, *, nobs)` as\\n              usual.\\n\\n            More generally, the dimension of the last axis will be\\n            `start - end`.\\n\\n        Notes\\n        -----\\n        This method computes:\\n\\n        .. math::\\n\\n            Cov(\\\\alpha_t - \\\\hat \\\\alpha_t, \\\\alpha_{t - j} - \\\\hat \\\\alpha_{t - j})\\n\\n        where the `lag` argument determines the autocovariance order :math:`j`,\\n        and `lag` is an integer (positive, zero, or negative). This method\\n        cannot compute values associated with time points prior to the sample,\\n        and so it returns a matrix of NaN values for these time points.\\n        For example, if `start=0` and `lag=2`, then assuming the output is\\n        assigned to the variable `acov`, we will have `acov[..., 0]` and\\n        `acov[..., 1]` as matrices filled with NaN values.\\n\\n        Based only on the \"current\" results object (i.e. the Kalman smoother\\n        applied to the sample), there is not enough information to compute\\n        Cov(t, t+j) for the last `lag - 1` observations of the sample. However,\\n        the values can be computed for these time points using the transition\\n        equation of the state space representation, and so for time-invariant\\n        state space models we do compute these values. For time-varying models,\\n        this can also be done, but updated state space matrices for the\\n        out-of-sample time points must be provided via the `extend_kwargs`\\n        argument.\\n\\n        See [1]_, Chapter 4.7, for all details about how these autocovariances\\n        are computed.\\n\\n        The `t` and `start`/`end` parameters compute and return only the\\n        requested autocovariances. As a result, using these parameters is\\n        recommended to reduce the computational burden, particularly if the\\n        number of observations and/or the dimension of the state vector is\\n        large.\\n\\n        References\\n        ----------\\n        .. [1] Durbin, James, and Siem Jan Koopman. 2012.\\n               Time Series Analysis by State Space Methods: Second Edition.\\n               Oxford University Press.\\n        '\n    cache_key = None\n    if extend_kwargs is None or len(extend_kwargs) == 0:\n        cache_key = (lag, t, start, end)\n    if cache_key is not None and cache_key in self.__smoothed_state_autocovariance:\n        return self.__smoothed_state_autocovariance[cache_key]\n    forward_autocovariances = False\n    if lag < 0:\n        lag = -lag\n        forward_autocovariances = True\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = 0\n    if end is None:\n        if forward_autocovariances and lag > 1 and (extend_kwargs is None):\n            end = self.nobs - lag + 1\n        else:\n            end = self.nobs\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end < start:\n        raise ValueError('`end` must be after `start`')\n    if lag == 0 and self.smoothed_state_cov is None:\n        raise RuntimeError('Cannot return smoothed state covariances if those values have not been computed by Kalman smoothing.')\n    if lag == 0 and end <= self.nobs + 1:\n        acov = self.smoothed_state_cov\n        if end == self.nobs + 1:\n            acov = np.concatenate((acov[..., start:], self.predicted_state_cov[..., -1:]), axis=2).T\n        else:\n            acov = acov.T[start:end]\n    elif lag == 1 and self.smoothed_state_autocov is not None and (not forward_autocovariances) and (end <= self.nobs + 1):\n        if start == 0:\n            nans = np.zeros((self.k_states, self.k_states, lag)) * np.nan\n            acov = np.concatenate((nans, self.smoothed_state_autocov[..., :end - 1]), axis=2)\n        else:\n            acov = self.smoothed_state_autocov[..., start - 1:end - 1]\n        acov = acov.transpose(2, 0, 1)\n    elif lag == 1 and self.smoothed_state_autocov is not None and forward_autocovariances and (end < self.nobs + 1):\n        acov = self.smoothed_state_autocov.T[start:end]\n    elif forward_autocovariances:\n        acov = self._smoothed_state_autocovariance(lag, start, end, extend_kwargs=extend_kwargs)\n    else:\n        out = self._smoothed_state_autocovariance(lag, start - lag, end - lag, extend_kwargs=extend_kwargs)\n        acov = out.transpose(0, 2, 1)\n    if t is not None:\n        acov = acov[0]\n    else:\n        acov = acov.transpose(1, 2, 0)\n    if cache_key is not None:\n        self.__smoothed_state_autocovariance[cache_key] = acov\n    return acov",
            "def smoothed_state_autocovariance(self, lag=1, t=None, start=None, end=None, extend_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Compute state vector autocovariances, conditional on the full dataset\\n\\n        Computes:\\n\\n        .. math::\\n\\n            Cov(\\\\alpha_t - \\\\hat \\\\alpha_t, \\\\alpha_{t - j} - \\\\hat \\\\alpha_{t - j})\\n\\n        where the `lag` argument gives the value for :math:`j`. Thus when\\n        the `lag` argument is positive, the autocovariance is between the\\n        current and previous periods, while if `lag` is negative the\\n        autocovariance is between the current and future periods.\\n\\n        Parameters\\n        ----------\\n        lag : int, optional\\n            The number of period to shift when computing the autocovariance.\\n            Default is 1.\\n        t : int, optional\\n            A specific period for which to compute and return the\\n            autocovariance. Cannot be used in combination with `start` or\\n            `end`. See the Returns section for details on how this\\n            parameter affects what is what is returned.\\n        start : int, optional\\n            The start of the interval (inclusive) of autocovariances to compute\\n            and return. Cannot be used in combination with the `t` argument.\\n            See the Returns section for details on how this parameter affects\\n            what is what is returned. Default is 0.\\n        end : int, optional\\n            The end of the interval (exclusive) autocovariances to compute and\\n            return. Note that since it is an exclusive endpoint, the returned\\n            autocovariances do not include the value at this index. Cannot be\\n            used in combination with the `t` argument. See the Returns section\\n            for details on how this parameter affects what is what is returned\\n            and what the default value is.\\n        extend_kwargs : dict, optional\\n            Keyword arguments containing updated state space system matrices\\n            for handling out-of-sample autocovariance computations in\\n            time-varying state space models.\\n\\n        Returns\\n        -------\\n        acov : ndarray\\n            Array of autocovariance matrices. If the argument `t` is not\\n            provided, then it is shaped `(k_states, k_states, n)`, while if `t`\\n            given then the third axis is dropped and the array is shaped\\n            `(k_states, k_states)`.\\n\\n            The output under the default case differs somewhat based on the\\n            state space model and the sign of the lag. To see how these cases\\n            differ, denote the output at each time point as Cov(t, t-j). Then:\\n\\n            - If `lag > 0` (and the model is either time-varying or\\n              time-invariant), then the returned array is shaped `(*, *, nobs)`\\n              and each entry [:, :, t] contains Cov(t, t-j). However, the model\\n              does not have enough information to compute autocovariances in\\n              the pre-sample period, so that we cannot compute Cov(1, 1-lag),\\n              Cov(2, 2-lag), ..., Cov(lag, 0). Thus the first `lag` entries\\n              have all values set to NaN.\\n\\n            - If the model is time-invariant and `lag < -1` or if `lag` is\\n              0 or -1, and the model is either time-invariant or time-varying,\\n              then the returned array is shaped `(*, *, nobs)` and each\\n              entry [:, :, t] contains Cov(t, t+j). Moreover, all entries are\\n              available (i.e. there are no NaNs).\\n\\n            - If the model is time-varying and `lag < -1` and `extend_kwargs`\\n              is not provided, then the returned array is shaped\\n              `(*, *, nobs - lag + 1)`.\\n\\n            - However, if the model is time-varying and `lag < -1`, then\\n              `extend_kwargs` can be provided with `lag - 1` additional\\n              matrices so that the returned array is shaped `(*, *, nobs)` as\\n              usual.\\n\\n            More generally, the dimension of the last axis will be\\n            `start - end`.\\n\\n        Notes\\n        -----\\n        This method computes:\\n\\n        .. math::\\n\\n            Cov(\\\\alpha_t - \\\\hat \\\\alpha_t, \\\\alpha_{t - j} - \\\\hat \\\\alpha_{t - j})\\n\\n        where the `lag` argument determines the autocovariance order :math:`j`,\\n        and `lag` is an integer (positive, zero, or negative). This method\\n        cannot compute values associated with time points prior to the sample,\\n        and so it returns a matrix of NaN values for these time points.\\n        For example, if `start=0` and `lag=2`, then assuming the output is\\n        assigned to the variable `acov`, we will have `acov[..., 0]` and\\n        `acov[..., 1]` as matrices filled with NaN values.\\n\\n        Based only on the \"current\" results object (i.e. the Kalman smoother\\n        applied to the sample), there is not enough information to compute\\n        Cov(t, t+j) for the last `lag - 1` observations of the sample. However,\\n        the values can be computed for these time points using the transition\\n        equation of the state space representation, and so for time-invariant\\n        state space models we do compute these values. For time-varying models,\\n        this can also be done, but updated state space matrices for the\\n        out-of-sample time points must be provided via the `extend_kwargs`\\n        argument.\\n\\n        See [1]_, Chapter 4.7, for all details about how these autocovariances\\n        are computed.\\n\\n        The `t` and `start`/`end` parameters compute and return only the\\n        requested autocovariances. As a result, using these parameters is\\n        recommended to reduce the computational burden, particularly if the\\n        number of observations and/or the dimension of the state vector is\\n        large.\\n\\n        References\\n        ----------\\n        .. [1] Durbin, James, and Siem Jan Koopman. 2012.\\n               Time Series Analysis by State Space Methods: Second Edition.\\n               Oxford University Press.\\n        '\n    cache_key = None\n    if extend_kwargs is None or len(extend_kwargs) == 0:\n        cache_key = (lag, t, start, end)\n    if cache_key is not None and cache_key in self.__smoothed_state_autocovariance:\n        return self.__smoothed_state_autocovariance[cache_key]\n    forward_autocovariances = False\n    if lag < 0:\n        lag = -lag\n        forward_autocovariances = True\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = 0\n    if end is None:\n        if forward_autocovariances and lag > 1 and (extend_kwargs is None):\n            end = self.nobs - lag + 1\n        else:\n            end = self.nobs\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end < start:\n        raise ValueError('`end` must be after `start`')\n    if lag == 0 and self.smoothed_state_cov is None:\n        raise RuntimeError('Cannot return smoothed state covariances if those values have not been computed by Kalman smoothing.')\n    if lag == 0 and end <= self.nobs + 1:\n        acov = self.smoothed_state_cov\n        if end == self.nobs + 1:\n            acov = np.concatenate((acov[..., start:], self.predicted_state_cov[..., -1:]), axis=2).T\n        else:\n            acov = acov.T[start:end]\n    elif lag == 1 and self.smoothed_state_autocov is not None and (not forward_autocovariances) and (end <= self.nobs + 1):\n        if start == 0:\n            nans = np.zeros((self.k_states, self.k_states, lag)) * np.nan\n            acov = np.concatenate((nans, self.smoothed_state_autocov[..., :end - 1]), axis=2)\n        else:\n            acov = self.smoothed_state_autocov[..., start - 1:end - 1]\n        acov = acov.transpose(2, 0, 1)\n    elif lag == 1 and self.smoothed_state_autocov is not None and forward_autocovariances and (end < self.nobs + 1):\n        acov = self.smoothed_state_autocov.T[start:end]\n    elif forward_autocovariances:\n        acov = self._smoothed_state_autocovariance(lag, start, end, extend_kwargs=extend_kwargs)\n    else:\n        out = self._smoothed_state_autocovariance(lag, start - lag, end - lag, extend_kwargs=extend_kwargs)\n        acov = out.transpose(0, 2, 1)\n    if t is not None:\n        acov = acov[0]\n    else:\n        acov = acov.transpose(1, 2, 0)\n    if cache_key is not None:\n        self.__smoothed_state_autocovariance[cache_key] = acov\n    return acov",
            "def smoothed_state_autocovariance(self, lag=1, t=None, start=None, end=None, extend_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Compute state vector autocovariances, conditional on the full dataset\\n\\n        Computes:\\n\\n        .. math::\\n\\n            Cov(\\\\alpha_t - \\\\hat \\\\alpha_t, \\\\alpha_{t - j} - \\\\hat \\\\alpha_{t - j})\\n\\n        where the `lag` argument gives the value for :math:`j`. Thus when\\n        the `lag` argument is positive, the autocovariance is between the\\n        current and previous periods, while if `lag` is negative the\\n        autocovariance is between the current and future periods.\\n\\n        Parameters\\n        ----------\\n        lag : int, optional\\n            The number of period to shift when computing the autocovariance.\\n            Default is 1.\\n        t : int, optional\\n            A specific period for which to compute and return the\\n            autocovariance. Cannot be used in combination with `start` or\\n            `end`. See the Returns section for details on how this\\n            parameter affects what is what is returned.\\n        start : int, optional\\n            The start of the interval (inclusive) of autocovariances to compute\\n            and return. Cannot be used in combination with the `t` argument.\\n            See the Returns section for details on how this parameter affects\\n            what is what is returned. Default is 0.\\n        end : int, optional\\n            The end of the interval (exclusive) autocovariances to compute and\\n            return. Note that since it is an exclusive endpoint, the returned\\n            autocovariances do not include the value at this index. Cannot be\\n            used in combination with the `t` argument. See the Returns section\\n            for details on how this parameter affects what is what is returned\\n            and what the default value is.\\n        extend_kwargs : dict, optional\\n            Keyword arguments containing updated state space system matrices\\n            for handling out-of-sample autocovariance computations in\\n            time-varying state space models.\\n\\n        Returns\\n        -------\\n        acov : ndarray\\n            Array of autocovariance matrices. If the argument `t` is not\\n            provided, then it is shaped `(k_states, k_states, n)`, while if `t`\\n            given then the third axis is dropped and the array is shaped\\n            `(k_states, k_states)`.\\n\\n            The output under the default case differs somewhat based on the\\n            state space model and the sign of the lag. To see how these cases\\n            differ, denote the output at each time point as Cov(t, t-j). Then:\\n\\n            - If `lag > 0` (and the model is either time-varying or\\n              time-invariant), then the returned array is shaped `(*, *, nobs)`\\n              and each entry [:, :, t] contains Cov(t, t-j). However, the model\\n              does not have enough information to compute autocovariances in\\n              the pre-sample period, so that we cannot compute Cov(1, 1-lag),\\n              Cov(2, 2-lag), ..., Cov(lag, 0). Thus the first `lag` entries\\n              have all values set to NaN.\\n\\n            - If the model is time-invariant and `lag < -1` or if `lag` is\\n              0 or -1, and the model is either time-invariant or time-varying,\\n              then the returned array is shaped `(*, *, nobs)` and each\\n              entry [:, :, t] contains Cov(t, t+j). Moreover, all entries are\\n              available (i.e. there are no NaNs).\\n\\n            - If the model is time-varying and `lag < -1` and `extend_kwargs`\\n              is not provided, then the returned array is shaped\\n              `(*, *, nobs - lag + 1)`.\\n\\n            - However, if the model is time-varying and `lag < -1`, then\\n              `extend_kwargs` can be provided with `lag - 1` additional\\n              matrices so that the returned array is shaped `(*, *, nobs)` as\\n              usual.\\n\\n            More generally, the dimension of the last axis will be\\n            `start - end`.\\n\\n        Notes\\n        -----\\n        This method computes:\\n\\n        .. math::\\n\\n            Cov(\\\\alpha_t - \\\\hat \\\\alpha_t, \\\\alpha_{t - j} - \\\\hat \\\\alpha_{t - j})\\n\\n        where the `lag` argument determines the autocovariance order :math:`j`,\\n        and `lag` is an integer (positive, zero, or negative). This method\\n        cannot compute values associated with time points prior to the sample,\\n        and so it returns a matrix of NaN values for these time points.\\n        For example, if `start=0` and `lag=2`, then assuming the output is\\n        assigned to the variable `acov`, we will have `acov[..., 0]` and\\n        `acov[..., 1]` as matrices filled with NaN values.\\n\\n        Based only on the \"current\" results object (i.e. the Kalman smoother\\n        applied to the sample), there is not enough information to compute\\n        Cov(t, t+j) for the last `lag - 1` observations of the sample. However,\\n        the values can be computed for these time points using the transition\\n        equation of the state space representation, and so for time-invariant\\n        state space models we do compute these values. For time-varying models,\\n        this can also be done, but updated state space matrices for the\\n        out-of-sample time points must be provided via the `extend_kwargs`\\n        argument.\\n\\n        See [1]_, Chapter 4.7, for all details about how these autocovariances\\n        are computed.\\n\\n        The `t` and `start`/`end` parameters compute and return only the\\n        requested autocovariances. As a result, using these parameters is\\n        recommended to reduce the computational burden, particularly if the\\n        number of observations and/or the dimension of the state vector is\\n        large.\\n\\n        References\\n        ----------\\n        .. [1] Durbin, James, and Siem Jan Koopman. 2012.\\n               Time Series Analysis by State Space Methods: Second Edition.\\n               Oxford University Press.\\n        '\n    cache_key = None\n    if extend_kwargs is None or len(extend_kwargs) == 0:\n        cache_key = (lag, t, start, end)\n    if cache_key is not None and cache_key in self.__smoothed_state_autocovariance:\n        return self.__smoothed_state_autocovariance[cache_key]\n    forward_autocovariances = False\n    if lag < 0:\n        lag = -lag\n        forward_autocovariances = True\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = 0\n    if end is None:\n        if forward_autocovariances and lag > 1 and (extend_kwargs is None):\n            end = self.nobs - lag + 1\n        else:\n            end = self.nobs\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end < start:\n        raise ValueError('`end` must be after `start`')\n    if lag == 0 and self.smoothed_state_cov is None:\n        raise RuntimeError('Cannot return smoothed state covariances if those values have not been computed by Kalman smoothing.')\n    if lag == 0 and end <= self.nobs + 1:\n        acov = self.smoothed_state_cov\n        if end == self.nobs + 1:\n            acov = np.concatenate((acov[..., start:], self.predicted_state_cov[..., -1:]), axis=2).T\n        else:\n            acov = acov.T[start:end]\n    elif lag == 1 and self.smoothed_state_autocov is not None and (not forward_autocovariances) and (end <= self.nobs + 1):\n        if start == 0:\n            nans = np.zeros((self.k_states, self.k_states, lag)) * np.nan\n            acov = np.concatenate((nans, self.smoothed_state_autocov[..., :end - 1]), axis=2)\n        else:\n            acov = self.smoothed_state_autocov[..., start - 1:end - 1]\n        acov = acov.transpose(2, 0, 1)\n    elif lag == 1 and self.smoothed_state_autocov is not None and forward_autocovariances and (end < self.nobs + 1):\n        acov = self.smoothed_state_autocov.T[start:end]\n    elif forward_autocovariances:\n        acov = self._smoothed_state_autocovariance(lag, start, end, extend_kwargs=extend_kwargs)\n    else:\n        out = self._smoothed_state_autocovariance(lag, start - lag, end - lag, extend_kwargs=extend_kwargs)\n        acov = out.transpose(0, 2, 1)\n    if t is not None:\n        acov = acov[0]\n    else:\n        acov = acov.transpose(1, 2, 0)\n    if cache_key is not None:\n        self.__smoothed_state_autocovariance[cache_key] = acov\n    return acov"
        ]
    },
    {
        "func_name": "news",
        "original": "def news(self, previous, t=None, start=None, end=None, revisions_details_start=True, design=None, state_index=None):\n    \"\"\"\n        Compute the news and impacts associated with a data release\n\n        Parameters\n        ----------\n        previous : SmootherResults\n            Prior results object relative to which to compute the news. This\n            results object must have identical state space representation for\n            the prior sample period so that the only difference is that this\n            results object has updates to the observed data.\n        t : int, optional\n            A specific period for which to compute the news. Cannot be used in\n            combination with `start` or `end`.\n        start : int, optional\n            The start of the interval (inclusive) of news to compute. Cannot be\n            used in combination with the `t` argument. Default is the last\n            period of the sample (`nobs - 1`).\n        end : int, optional\n            The end of the interval (exclusive) of news to compute. Note that\n            since it is an exclusive endpoint, the returned news do not include\n            the value at this index. Cannot be used in combination with the `t`\n            argument.\n        revisions_details_start : bool or int, optional\n            The period at which to beging computing the detailed impacts of\n            data revisions. Any revisions prior to this period will have their\n            impacts grouped together. If a negative integer, interpreted as\n            an offset from the end of the dataset. If set to True, detailed\n            impacts are computed for all revisions, while if set to False, all\n            revisions are grouped together. Default is False. Note that for\n            large models, setting this to be near the beginning of the sample\n            can cause this function to be slow.\n        design : array, optional\n            Design matrix for the period `t` in time-varying models. If this\n            model has a time-varying design matrix, and the argument `t` is out\n            of this model's sample, then a new design matrix for period `t`\n            must be provided. Unused otherwise.\n        state_index : array_like, optional\n            An optional index specifying a subset of states to use when\n            constructing the impacts of revisions and news. For example, if\n            `state_index=[0, 1]` is passed, then only the impacts to the\n            observed variables arising from the impacts to the first two\n            states will be returned.\n\n        Returns\n        -------\n        news_results : SimpleNamespace\n            News and impacts associated with a data release. Includes the\n            following attributes:\n\n            - `update_impacts`: update to forecasts of impacted variables from\n              the news. It is equivalent to E[y^i | post] - E[y^i | revision],\n              where y^i are the variables of interest. In [1]_, this is\n              described as \"revision\" in equation (17).\n            - `revision_detailed_impacts`: update to forecasts of variables\n              impacted variables from data revisions. It is\n              E[y^i | revision] - E[y^i | previous], and does not have a\n              specific notation in [1]_, since there for simplicity they assume\n              that there are no revisions.\n            - `news`: the unexpected component of the updated data. Denoted\n              I = y^u - E[y^u | previous], where y^u are the data points that\n              were newly incorporated in a data release (but not including\n              revisions to data points that already existed in the previous\n              release). In [1]_, this is described as \"news\" in equation (17).\n            - `revisions`: y^r(updated) - y^r(previous) for periods in\n              which detailed impacts were computed\n            - `revisions_all` : y^r(updated) - y^r(previous) for all revisions\n            - `gain`: the gain matrix associated with the \"Kalman-like\" update\n              from the news, E[y I'] E[I I']^{-1}. In [1]_, this can be found\n              in the equation For E[y_{k,t_k} \\\\mid I_{v+1}] in the middle of\n              page 17.\n            - `revision_weights` weights on observations for the smoothed\n              signal\n            - `update_forecasts`: forecasts of the updated periods used to\n              construct the news, E[y^u | previous].\n            - `update_realized`: realizations of the updated periods used to\n              construct the news, y^u.\n            - `revised`: revised observations of the periods that were revised\n              and for which detailed impacts were computed\n            - `revised`: revised observations of the periods that were revised\n            - `revised_prev`: previous observations of the periods that were\n              revised and for which detailed impacts were computed\n            - `revised_prev_all`: previous observations of the periods that\n              were revised and for which detailed impacts were computed\n            - `prev_impacted_forecasts`: previous forecast of the periods of\n              interest, E[y^i | previous].\n            - `post_impacted_forecasts`: forecast of the periods of interest\n              after taking into account both revisions and updates,\n              E[y^i | post].\n            - `revision_results`: results object that updates the `previous`\n              results to take into account data revisions.\n            - `revision_results`: results object associated with the revisions\n            - `revision_impacts`: total impacts from all revisions (both\n              grouped and detailed)\n            - `revisions_ix`: list of `(t, i)` positions of revisions in endog\n            - `revisions_details`: list of `(t, i)` positions of revisions to\n              endog for which details of impacts were computed\n            - `revisions_grouped`: list of `(t, i)` positions of revisions to\n              endog for which impacts were grouped\n            - `revisions_details_start`: period in which revision details start\n              to be computed\n            - `updates_ix`: list of `(t, i)` positions of updates to endog\n            - `state_index`: index of state variables used to compute impacts\n\n        Notes\n        -----\n        This method computes the effect of new data (e.g. from a new data\n        release) on smoothed forecasts produced by a state space model, as\n        described in [1]_. It also computes the effect of revised data on\n        smoothed forecasts.\n\n        References\n        ----------\n        .. [1] Ba\u0144bura, Marta and Modugno, Michele. 2010.\n               \"Maximum likelihood estimation of factor models on data sets\n               with arbitrary pattern of missing data.\"\n               No 1189, Working Paper Series, European Central Bank.\n               https://EconPapers.repec.org/RePEc:ecb:ecbwps:20101189.\n        .. [2] Ba\u0144bura, Marta, and Michele Modugno.\n               \"Maximum likelihood estimation of factor models on datasets with\n               arbitrary pattern of missing data.\"\n               Journal of Applied Econometrics 29, no. 1 (2014): 133-160.\n\n        \"\"\"\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = self.nobs - 1\n    if end is None:\n        end = self.nobs\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end <= start:\n        raise ValueError('`end` must be after `start`')\n    if self.smoothed_state_cov is None:\n        raise ValueError('Cannot compute news without having applied the Kalman smoother first.')\n    error_ss = 'This results object has %s and so it does not appear to by an extension of `previous`. Can only compute the news by comparing this results set to previous results objects.'\n    if self.nobs < previous.nobs:\n        raise ValueError(error_ss % 'fewer observations than `previous`')\n    if not (self.k_endog == previous.k_endog and self.k_states == previous.k_states and (self.k_posdef == previous.k_posdef)):\n        raise ValueError(error_ss % 'different state space dimensions than `previous`')\n    for key in self.model.shapes.keys():\n        if key == 'obs':\n            continue\n        tv = getattr(self, key).shape[-1] > 1\n        tv_prev = getattr(previous, key).shape[-1] > 1\n        if tv and (not tv_prev):\n            raise ValueError(error_ss % f'time-varying {key} while `previous` does not')\n        if not tv and tv_prev:\n            raise ValueError(error_ss % f'time-invariant {key} while `previous` does not')\n    if state_index is not None:\n        state_index = np.atleast_1d(np.sort(np.array(state_index, dtype=int)))\n    if end > self.nobs and (not self.model.time_invariant):\n        raise RuntimeError('Cannot compute the impacts of news on periods outside of the sample in time-varying models.')\n    extend_kwargs = {}\n    for key in self.model.shapes.keys():\n        if key == 'obs':\n            continue\n        mat = getattr(self, key)\n        prev_mat = getattr(previous, key)\n        if mat.shape[-1] > prev_mat.shape[-1]:\n            extend_kwargs[key] = mat[..., prev_mat.shape[-1]:]\n    (revisions_ix, updates_ix) = previous.model.diff_endog(self.endog.T)\n    prev_impacted_forecasts = previous.predict(start=start, end=end, **extend_kwargs).smoothed_forecasts\n    post_impacted_forecasts = self.predict(start=start, end=end).smoothed_forecasts\n    if revisions_details_start is True:\n        revisions_details_start = 0\n    elif revisions_details_start is False:\n        revisions_details_start = previous.nobs\n    elif revisions_details_start < 0:\n        revisions_details_start = previous.nobs + revisions_details_start\n    revisions_grouped = []\n    revisions_details = []\n    if revisions_details_start > 0:\n        for (s, i) in revisions_ix:\n            if s < revisions_details_start:\n                revisions_grouped.append((s, i))\n            else:\n                revisions_details.append((s, i))\n    else:\n        revisions_details = revisions_ix\n    if len(revisions_ix) > 0:\n        revisions_details_start = max(revisions_ix[0][0], revisions_details_start)\n    revised_endog = None\n    revised_all = None\n    revised_prev_all = None\n    revisions_all = None\n    revised = None\n    revised_prev = None\n    revisions = None\n    revision_weights = None\n    revision_detailed_impacts = None\n    revision_results = None\n    revision_impacts = None\n    if len(revisions_ix) > 0:\n        (revised_j, revised_p) = zip(*revisions_ix)\n        compute_j = np.arange(revised_j[0], revised_j[-1] + 1)\n        revised_endog = self.endog[:, :previous.nobs].copy()\n        revised_endog[previous.missing.astype(bool)] = np.nan\n        revised_all = revised_endog.T[compute_j]\n        revised_prev_all = previous.endog.T[compute_j]\n        revisions_all = revised_all - revised_prev_all\n        tmp_endog = revised_endog.T.copy()\n        tmp_nobs = max(end, previous.nobs)\n        oos_nobs = tmp_nobs - previous.nobs\n        if oos_nobs > 0:\n            tmp_endog = np.concatenate([tmp_endog, np.zeros((oos_nobs, self.k_endog)) * np.nan], axis=0)\n        clone_kwargs = {}\n        for key in self.model.shapes.keys():\n            if key == 'obs':\n                continue\n            mat = getattr(self, key)\n            if mat.shape[-1] > 1:\n                clone_kwargs[key] = mat[..., :tmp_nobs]\n        rev_mod = previous.model.clone(tmp_endog, **clone_kwargs)\n        init = initialization.Initialization.from_results(self)\n        rev_mod.initialize(init)\n        revision_results = rev_mod.smooth()\n        if len(revisions_details) > 0:\n            compute_j = np.arange(revisions_details_start, revised_j[-1] + 1)\n            offset = revisions_details_start - revised_j[0]\n            revised = revised_all[offset:]\n            revised_prev = revised_prev_all[offset:]\n            revisions = revisions_all[offset:]\n            compute_t = np.arange(start, end)\n            (smoothed_state_weights, _, _) = tools._compute_smoothed_state_weights(rev_mod, compute_t=compute_t, compute_j=compute_j, compute_prior_weights=False, scale=previous.scale)\n            ZT = rev_mod.design.T\n            if ZT.shape[0] > 1:\n                ZT = ZT[compute_t]\n            if state_index is not None:\n                ZT = ZT[:, state_index, :]\n                smoothed_state_weights = smoothed_state_weights[:, :, state_index]\n            revision_weights = np.nansum(smoothed_state_weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n            (revised_j, revised_p) = zip(*[s for s in revisions_ix if s[0] >= revisions_details_start])\n            ix_j = revised_j - revised_j[0]\n            revision_weights = revision_weights.transpose(0, 2, 1, 3)[:, :, ix_j, revised_p]\n            revisions = revisions[ix_j, revised_p]\n            revision_detailed_impacts = revision_weights @ revisions\n            revised = revised[ix_j, revised_p]\n            revised_prev = revised_prev[ix_j, revised_p]\n            if t is not None:\n                revision_weights = revision_weights[0]\n                revision_detailed_impacts = revision_detailed_impacts[0]\n        revised_impact_forecasts = revision_results.smoothed_forecasts[..., start:end]\n        if end > revision_results.nobs:\n            predict_start = max(start, revision_results.nobs)\n            p = revision_results.predict(start=predict_start, end=end, **extend_kwargs)\n            revised_impact_forecasts = np.concatenate((revised_impact_forecasts, p.forecasts), axis=1)\n        revision_impacts = (revised_impact_forecasts - prev_impacted_forecasts).T\n        if t is not None:\n            revision_impacts = revision_impacts[0]\n    if len(revisions_ix) > 0:\n        (revised_j, revised_p) = zip(*revisions_ix)\n        ix_j = revised_j - revised_j[0]\n        revisions_all = revisions_all[ix_j, revised_p]\n        revised_all = revised_all[ix_j, revised_p]\n        revised_prev_all = revised_prev_all[ix_j, revised_p]\n    if len(updates_ix) > 0:\n        (update_t, update_k) = zip(*updates_ix)\n        update_start_t = np.min(update_t)\n        update_end_t = np.max(update_t)\n        if revision_results is None:\n            forecasts = previous.predict(start=update_start_t, end=update_end_t + 1, **extend_kwargs).smoothed_forecasts.T\n        else:\n            forecasts = revision_results.predict(start=update_start_t, end=update_end_t + 1).smoothed_forecasts.T\n        realized = self.endog.T[update_start_t:update_end_t + 1]\n        forecasts_error = realized - forecasts\n        ix_t = update_t - update_start_t\n        update_realized = realized[ix_t, update_k]\n        update_forecasts = forecasts[ix_t, update_k]\n        update_forecasts_error = forecasts_error[ix_t, update_k]\n        if self.design.shape[2] == 1:\n            design = self.design[..., 0][None, ...]\n        elif end <= self.nobs:\n            design = self.design[..., start:end].transpose(2, 0, 1)\n        elif design is None:\n            raise ValueError('Model has time-varying design matrix, so an updated time-varying matrix for period `t` is required.')\n        elif design.ndim == 2:\n            design = design[None, ...]\n        else:\n            design = design.transpose(2, 0, 1)\n        state_gain = previous.smoothed_state_gain(updates_ix, start=start, end=end, extend_kwargs=extend_kwargs)\n        if state_index is not None:\n            design = design[:, :, state_index]\n            state_gain = state_gain[:, state_index]\n        obs_gain = design @ state_gain\n        update_impacts = obs_gain @ update_forecasts_error\n        if t is not None:\n            obs_gain = obs_gain[0]\n            update_impacts = update_impacts[0]\n    else:\n        update_impacts = None\n        update_forecasts = None\n        update_realized = None\n        update_forecasts_error = None\n        obs_gain = None\n    out = SimpleNamespace(update_impacts=update_impacts, revision_detailed_impacts=revision_detailed_impacts, news=update_forecasts_error, revisions=revisions, revisions_all=revisions_all, gain=obs_gain, revision_weights=revision_weights, update_forecasts=update_forecasts, update_realized=update_realized, revised=revised, revised_all=revised_all, revised_prev=revised_prev, revised_prev_all=revised_prev_all, prev_impacted_forecasts=prev_impacted_forecasts, post_impacted_forecasts=post_impacted_forecasts, revision_results=revision_results, revision_impacts=revision_impacts, revisions_ix=revisions_ix, revisions_details=revisions_details, revisions_grouped=revisions_grouped, revisions_details_start=revisions_details_start, updates_ix=updates_ix, state_index=state_index)\n    return out",
        "mutated": [
            "def news(self, previous, t=None, start=None, end=None, revisions_details_start=True, design=None, state_index=None):\n    if False:\n        i = 10\n    '\\n        Compute the news and impacts associated with a data release\\n\\n        Parameters\\n        ----------\\n        previous : SmootherResults\\n            Prior results object relative to which to compute the news. This\\n            results object must have identical state space representation for\\n            the prior sample period so that the only difference is that this\\n            results object has updates to the observed data.\\n        t : int, optional\\n            A specific period for which to compute the news. Cannot be used in\\n            combination with `start` or `end`.\\n        start : int, optional\\n            The start of the interval (inclusive) of news to compute. Cannot be\\n            used in combination with the `t` argument. Default is the last\\n            period of the sample (`nobs - 1`).\\n        end : int, optional\\n            The end of the interval (exclusive) of news to compute. Note that\\n            since it is an exclusive endpoint, the returned news do not include\\n            the value at this index. Cannot be used in combination with the `t`\\n            argument.\\n        revisions_details_start : bool or int, optional\\n            The period at which to beging computing the detailed impacts of\\n            data revisions. Any revisions prior to this period will have their\\n            impacts grouped together. If a negative integer, interpreted as\\n            an offset from the end of the dataset. If set to True, detailed\\n            impacts are computed for all revisions, while if set to False, all\\n            revisions are grouped together. Default is False. Note that for\\n            large models, setting this to be near the beginning of the sample\\n            can cause this function to be slow.\\n        design : array, optional\\n            Design matrix for the period `t` in time-varying models. If this\\n            model has a time-varying design matrix, and the argument `t` is out\\n            of this model\\'s sample, then a new design matrix for period `t`\\n            must be provided. Unused otherwise.\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the impacts of revisions and news. For example, if\\n            `state_index=[0, 1]` is passed, then only the impacts to the\\n            observed variables arising from the impacts to the first two\\n            states will be returned.\\n\\n        Returns\\n        -------\\n        news_results : SimpleNamespace\\n            News and impacts associated with a data release. Includes the\\n            following attributes:\\n\\n            - `update_impacts`: update to forecasts of impacted variables from\\n              the news. It is equivalent to E[y^i | post] - E[y^i | revision],\\n              where y^i are the variables of interest. In [1]_, this is\\n              described as \"revision\" in equation (17).\\n            - `revision_detailed_impacts`: update to forecasts of variables\\n              impacted variables from data revisions. It is\\n              E[y^i | revision] - E[y^i | previous], and does not have a\\n              specific notation in [1]_, since there for simplicity they assume\\n              that there are no revisions.\\n            - `news`: the unexpected component of the updated data. Denoted\\n              I = y^u - E[y^u | previous], where y^u are the data points that\\n              were newly incorporated in a data release (but not including\\n              revisions to data points that already existed in the previous\\n              release). In [1]_, this is described as \"news\" in equation (17).\\n            - `revisions`: y^r(updated) - y^r(previous) for periods in\\n              which detailed impacts were computed\\n            - `revisions_all` : y^r(updated) - y^r(previous) for all revisions\\n            - `gain`: the gain matrix associated with the \"Kalman-like\" update\\n              from the news, E[y I\\'] E[I I\\']^{-1}. In [1]_, this can be found\\n              in the equation For E[y_{k,t_k} \\\\mid I_{v+1}] in the middle of\\n              page 17.\\n            - `revision_weights` weights on observations for the smoothed\\n              signal\\n            - `update_forecasts`: forecasts of the updated periods used to\\n              construct the news, E[y^u | previous].\\n            - `update_realized`: realizations of the updated periods used to\\n              construct the news, y^u.\\n            - `revised`: revised observations of the periods that were revised\\n              and for which detailed impacts were computed\\n            - `revised`: revised observations of the periods that were revised\\n            - `revised_prev`: previous observations of the periods that were\\n              revised and for which detailed impacts were computed\\n            - `revised_prev_all`: previous observations of the periods that\\n              were revised and for which detailed impacts were computed\\n            - `prev_impacted_forecasts`: previous forecast of the periods of\\n              interest, E[y^i | previous].\\n            - `post_impacted_forecasts`: forecast of the periods of interest\\n              after taking into account both revisions and updates,\\n              E[y^i | post].\\n            - `revision_results`: results object that updates the `previous`\\n              results to take into account data revisions.\\n            - `revision_results`: results object associated with the revisions\\n            - `revision_impacts`: total impacts from all revisions (both\\n              grouped and detailed)\\n            - `revisions_ix`: list of `(t, i)` positions of revisions in endog\\n            - `revisions_details`: list of `(t, i)` positions of revisions to\\n              endog for which details of impacts were computed\\n            - `revisions_grouped`: list of `(t, i)` positions of revisions to\\n              endog for which impacts were grouped\\n            - `revisions_details_start`: period in which revision details start\\n              to be computed\\n            - `updates_ix`: list of `(t, i)` positions of updates to endog\\n            - `state_index`: index of state variables used to compute impacts\\n\\n        Notes\\n        -----\\n        This method computes the effect of new data (e.g. from a new data\\n        release) on smoothed forecasts produced by a state space model, as\\n        described in [1]_. It also computes the effect of revised data on\\n        smoothed forecasts.\\n\\n        References\\n        ----------\\n        .. [1] Ba\u0144bura, Marta and Modugno, Michele. 2010.\\n               \"Maximum likelihood estimation of factor models on data sets\\n               with arbitrary pattern of missing data.\"\\n               No 1189, Working Paper Series, European Central Bank.\\n               https://EconPapers.repec.org/RePEc:ecb:ecbwps:20101189.\\n        .. [2] Ba\u0144bura, Marta, and Michele Modugno.\\n               \"Maximum likelihood estimation of factor models on datasets with\\n               arbitrary pattern of missing data.\"\\n               Journal of Applied Econometrics 29, no. 1 (2014): 133-160.\\n\\n        '\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = self.nobs - 1\n    if end is None:\n        end = self.nobs\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end <= start:\n        raise ValueError('`end` must be after `start`')\n    if self.smoothed_state_cov is None:\n        raise ValueError('Cannot compute news without having applied the Kalman smoother first.')\n    error_ss = 'This results object has %s and so it does not appear to by an extension of `previous`. Can only compute the news by comparing this results set to previous results objects.'\n    if self.nobs < previous.nobs:\n        raise ValueError(error_ss % 'fewer observations than `previous`')\n    if not (self.k_endog == previous.k_endog and self.k_states == previous.k_states and (self.k_posdef == previous.k_posdef)):\n        raise ValueError(error_ss % 'different state space dimensions than `previous`')\n    for key in self.model.shapes.keys():\n        if key == 'obs':\n            continue\n        tv = getattr(self, key).shape[-1] > 1\n        tv_prev = getattr(previous, key).shape[-1] > 1\n        if tv and (not tv_prev):\n            raise ValueError(error_ss % f'time-varying {key} while `previous` does not')\n        if not tv and tv_prev:\n            raise ValueError(error_ss % f'time-invariant {key} while `previous` does not')\n    if state_index is not None:\n        state_index = np.atleast_1d(np.sort(np.array(state_index, dtype=int)))\n    if end > self.nobs and (not self.model.time_invariant):\n        raise RuntimeError('Cannot compute the impacts of news on periods outside of the sample in time-varying models.')\n    extend_kwargs = {}\n    for key in self.model.shapes.keys():\n        if key == 'obs':\n            continue\n        mat = getattr(self, key)\n        prev_mat = getattr(previous, key)\n        if mat.shape[-1] > prev_mat.shape[-1]:\n            extend_kwargs[key] = mat[..., prev_mat.shape[-1]:]\n    (revisions_ix, updates_ix) = previous.model.diff_endog(self.endog.T)\n    prev_impacted_forecasts = previous.predict(start=start, end=end, **extend_kwargs).smoothed_forecasts\n    post_impacted_forecasts = self.predict(start=start, end=end).smoothed_forecasts\n    if revisions_details_start is True:\n        revisions_details_start = 0\n    elif revisions_details_start is False:\n        revisions_details_start = previous.nobs\n    elif revisions_details_start < 0:\n        revisions_details_start = previous.nobs + revisions_details_start\n    revisions_grouped = []\n    revisions_details = []\n    if revisions_details_start > 0:\n        for (s, i) in revisions_ix:\n            if s < revisions_details_start:\n                revisions_grouped.append((s, i))\n            else:\n                revisions_details.append((s, i))\n    else:\n        revisions_details = revisions_ix\n    if len(revisions_ix) > 0:\n        revisions_details_start = max(revisions_ix[0][0], revisions_details_start)\n    revised_endog = None\n    revised_all = None\n    revised_prev_all = None\n    revisions_all = None\n    revised = None\n    revised_prev = None\n    revisions = None\n    revision_weights = None\n    revision_detailed_impacts = None\n    revision_results = None\n    revision_impacts = None\n    if len(revisions_ix) > 0:\n        (revised_j, revised_p) = zip(*revisions_ix)\n        compute_j = np.arange(revised_j[0], revised_j[-1] + 1)\n        revised_endog = self.endog[:, :previous.nobs].copy()\n        revised_endog[previous.missing.astype(bool)] = np.nan\n        revised_all = revised_endog.T[compute_j]\n        revised_prev_all = previous.endog.T[compute_j]\n        revisions_all = revised_all - revised_prev_all\n        tmp_endog = revised_endog.T.copy()\n        tmp_nobs = max(end, previous.nobs)\n        oos_nobs = tmp_nobs - previous.nobs\n        if oos_nobs > 0:\n            tmp_endog = np.concatenate([tmp_endog, np.zeros((oos_nobs, self.k_endog)) * np.nan], axis=0)\n        clone_kwargs = {}\n        for key in self.model.shapes.keys():\n            if key == 'obs':\n                continue\n            mat = getattr(self, key)\n            if mat.shape[-1] > 1:\n                clone_kwargs[key] = mat[..., :tmp_nobs]\n        rev_mod = previous.model.clone(tmp_endog, **clone_kwargs)\n        init = initialization.Initialization.from_results(self)\n        rev_mod.initialize(init)\n        revision_results = rev_mod.smooth()\n        if len(revisions_details) > 0:\n            compute_j = np.arange(revisions_details_start, revised_j[-1] + 1)\n            offset = revisions_details_start - revised_j[0]\n            revised = revised_all[offset:]\n            revised_prev = revised_prev_all[offset:]\n            revisions = revisions_all[offset:]\n            compute_t = np.arange(start, end)\n            (smoothed_state_weights, _, _) = tools._compute_smoothed_state_weights(rev_mod, compute_t=compute_t, compute_j=compute_j, compute_prior_weights=False, scale=previous.scale)\n            ZT = rev_mod.design.T\n            if ZT.shape[0] > 1:\n                ZT = ZT[compute_t]\n            if state_index is not None:\n                ZT = ZT[:, state_index, :]\n                smoothed_state_weights = smoothed_state_weights[:, :, state_index]\n            revision_weights = np.nansum(smoothed_state_weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n            (revised_j, revised_p) = zip(*[s for s in revisions_ix if s[0] >= revisions_details_start])\n            ix_j = revised_j - revised_j[0]\n            revision_weights = revision_weights.transpose(0, 2, 1, 3)[:, :, ix_j, revised_p]\n            revisions = revisions[ix_j, revised_p]\n            revision_detailed_impacts = revision_weights @ revisions\n            revised = revised[ix_j, revised_p]\n            revised_prev = revised_prev[ix_j, revised_p]\n            if t is not None:\n                revision_weights = revision_weights[0]\n                revision_detailed_impacts = revision_detailed_impacts[0]\n        revised_impact_forecasts = revision_results.smoothed_forecasts[..., start:end]\n        if end > revision_results.nobs:\n            predict_start = max(start, revision_results.nobs)\n            p = revision_results.predict(start=predict_start, end=end, **extend_kwargs)\n            revised_impact_forecasts = np.concatenate((revised_impact_forecasts, p.forecasts), axis=1)\n        revision_impacts = (revised_impact_forecasts - prev_impacted_forecasts).T\n        if t is not None:\n            revision_impacts = revision_impacts[0]\n    if len(revisions_ix) > 0:\n        (revised_j, revised_p) = zip(*revisions_ix)\n        ix_j = revised_j - revised_j[0]\n        revisions_all = revisions_all[ix_j, revised_p]\n        revised_all = revised_all[ix_j, revised_p]\n        revised_prev_all = revised_prev_all[ix_j, revised_p]\n    if len(updates_ix) > 0:\n        (update_t, update_k) = zip(*updates_ix)\n        update_start_t = np.min(update_t)\n        update_end_t = np.max(update_t)\n        if revision_results is None:\n            forecasts = previous.predict(start=update_start_t, end=update_end_t + 1, **extend_kwargs).smoothed_forecasts.T\n        else:\n            forecasts = revision_results.predict(start=update_start_t, end=update_end_t + 1).smoothed_forecasts.T\n        realized = self.endog.T[update_start_t:update_end_t + 1]\n        forecasts_error = realized - forecasts\n        ix_t = update_t - update_start_t\n        update_realized = realized[ix_t, update_k]\n        update_forecasts = forecasts[ix_t, update_k]\n        update_forecasts_error = forecasts_error[ix_t, update_k]\n        if self.design.shape[2] == 1:\n            design = self.design[..., 0][None, ...]\n        elif end <= self.nobs:\n            design = self.design[..., start:end].transpose(2, 0, 1)\n        elif design is None:\n            raise ValueError('Model has time-varying design matrix, so an updated time-varying matrix for period `t` is required.')\n        elif design.ndim == 2:\n            design = design[None, ...]\n        else:\n            design = design.transpose(2, 0, 1)\n        state_gain = previous.smoothed_state_gain(updates_ix, start=start, end=end, extend_kwargs=extend_kwargs)\n        if state_index is not None:\n            design = design[:, :, state_index]\n            state_gain = state_gain[:, state_index]\n        obs_gain = design @ state_gain\n        update_impacts = obs_gain @ update_forecasts_error\n        if t is not None:\n            obs_gain = obs_gain[0]\n            update_impacts = update_impacts[0]\n    else:\n        update_impacts = None\n        update_forecasts = None\n        update_realized = None\n        update_forecasts_error = None\n        obs_gain = None\n    out = SimpleNamespace(update_impacts=update_impacts, revision_detailed_impacts=revision_detailed_impacts, news=update_forecasts_error, revisions=revisions, revisions_all=revisions_all, gain=obs_gain, revision_weights=revision_weights, update_forecasts=update_forecasts, update_realized=update_realized, revised=revised, revised_all=revised_all, revised_prev=revised_prev, revised_prev_all=revised_prev_all, prev_impacted_forecasts=prev_impacted_forecasts, post_impacted_forecasts=post_impacted_forecasts, revision_results=revision_results, revision_impacts=revision_impacts, revisions_ix=revisions_ix, revisions_details=revisions_details, revisions_grouped=revisions_grouped, revisions_details_start=revisions_details_start, updates_ix=updates_ix, state_index=state_index)\n    return out",
            "def news(self, previous, t=None, start=None, end=None, revisions_details_start=True, design=None, state_index=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Compute the news and impacts associated with a data release\\n\\n        Parameters\\n        ----------\\n        previous : SmootherResults\\n            Prior results object relative to which to compute the news. This\\n            results object must have identical state space representation for\\n            the prior sample period so that the only difference is that this\\n            results object has updates to the observed data.\\n        t : int, optional\\n            A specific period for which to compute the news. Cannot be used in\\n            combination with `start` or `end`.\\n        start : int, optional\\n            The start of the interval (inclusive) of news to compute. Cannot be\\n            used in combination with the `t` argument. Default is the last\\n            period of the sample (`nobs - 1`).\\n        end : int, optional\\n            The end of the interval (exclusive) of news to compute. Note that\\n            since it is an exclusive endpoint, the returned news do not include\\n            the value at this index. Cannot be used in combination with the `t`\\n            argument.\\n        revisions_details_start : bool or int, optional\\n            The period at which to beging computing the detailed impacts of\\n            data revisions. Any revisions prior to this period will have their\\n            impacts grouped together. If a negative integer, interpreted as\\n            an offset from the end of the dataset. If set to True, detailed\\n            impacts are computed for all revisions, while if set to False, all\\n            revisions are grouped together. Default is False. Note that for\\n            large models, setting this to be near the beginning of the sample\\n            can cause this function to be slow.\\n        design : array, optional\\n            Design matrix for the period `t` in time-varying models. If this\\n            model has a time-varying design matrix, and the argument `t` is out\\n            of this model\\'s sample, then a new design matrix for period `t`\\n            must be provided. Unused otherwise.\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the impacts of revisions and news. For example, if\\n            `state_index=[0, 1]` is passed, then only the impacts to the\\n            observed variables arising from the impacts to the first two\\n            states will be returned.\\n\\n        Returns\\n        -------\\n        news_results : SimpleNamespace\\n            News and impacts associated with a data release. Includes the\\n            following attributes:\\n\\n            - `update_impacts`: update to forecasts of impacted variables from\\n              the news. It is equivalent to E[y^i | post] - E[y^i | revision],\\n              where y^i are the variables of interest. In [1]_, this is\\n              described as \"revision\" in equation (17).\\n            - `revision_detailed_impacts`: update to forecasts of variables\\n              impacted variables from data revisions. It is\\n              E[y^i | revision] - E[y^i | previous], and does not have a\\n              specific notation in [1]_, since there for simplicity they assume\\n              that there are no revisions.\\n            - `news`: the unexpected component of the updated data. Denoted\\n              I = y^u - E[y^u | previous], where y^u are the data points that\\n              were newly incorporated in a data release (but not including\\n              revisions to data points that already existed in the previous\\n              release). In [1]_, this is described as \"news\" in equation (17).\\n            - `revisions`: y^r(updated) - y^r(previous) for periods in\\n              which detailed impacts were computed\\n            - `revisions_all` : y^r(updated) - y^r(previous) for all revisions\\n            - `gain`: the gain matrix associated with the \"Kalman-like\" update\\n              from the news, E[y I\\'] E[I I\\']^{-1}. In [1]_, this can be found\\n              in the equation For E[y_{k,t_k} \\\\mid I_{v+1}] in the middle of\\n              page 17.\\n            - `revision_weights` weights on observations for the smoothed\\n              signal\\n            - `update_forecasts`: forecasts of the updated periods used to\\n              construct the news, E[y^u | previous].\\n            - `update_realized`: realizations of the updated periods used to\\n              construct the news, y^u.\\n            - `revised`: revised observations of the periods that were revised\\n              and for which detailed impacts were computed\\n            - `revised`: revised observations of the periods that were revised\\n            - `revised_prev`: previous observations of the periods that were\\n              revised and for which detailed impacts were computed\\n            - `revised_prev_all`: previous observations of the periods that\\n              were revised and for which detailed impacts were computed\\n            - `prev_impacted_forecasts`: previous forecast of the periods of\\n              interest, E[y^i | previous].\\n            - `post_impacted_forecasts`: forecast of the periods of interest\\n              after taking into account both revisions and updates,\\n              E[y^i | post].\\n            - `revision_results`: results object that updates the `previous`\\n              results to take into account data revisions.\\n            - `revision_results`: results object associated with the revisions\\n            - `revision_impacts`: total impacts from all revisions (both\\n              grouped and detailed)\\n            - `revisions_ix`: list of `(t, i)` positions of revisions in endog\\n            - `revisions_details`: list of `(t, i)` positions of revisions to\\n              endog for which details of impacts were computed\\n            - `revisions_grouped`: list of `(t, i)` positions of revisions to\\n              endog for which impacts were grouped\\n            - `revisions_details_start`: period in which revision details start\\n              to be computed\\n            - `updates_ix`: list of `(t, i)` positions of updates to endog\\n            - `state_index`: index of state variables used to compute impacts\\n\\n        Notes\\n        -----\\n        This method computes the effect of new data (e.g. from a new data\\n        release) on smoothed forecasts produced by a state space model, as\\n        described in [1]_. It also computes the effect of revised data on\\n        smoothed forecasts.\\n\\n        References\\n        ----------\\n        .. [1] Ba\u0144bura, Marta and Modugno, Michele. 2010.\\n               \"Maximum likelihood estimation of factor models on data sets\\n               with arbitrary pattern of missing data.\"\\n               No 1189, Working Paper Series, European Central Bank.\\n               https://EconPapers.repec.org/RePEc:ecb:ecbwps:20101189.\\n        .. [2] Ba\u0144bura, Marta, and Michele Modugno.\\n               \"Maximum likelihood estimation of factor models on datasets with\\n               arbitrary pattern of missing data.\"\\n               Journal of Applied Econometrics 29, no. 1 (2014): 133-160.\\n\\n        '\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = self.nobs - 1\n    if end is None:\n        end = self.nobs\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end <= start:\n        raise ValueError('`end` must be after `start`')\n    if self.smoothed_state_cov is None:\n        raise ValueError('Cannot compute news without having applied the Kalman smoother first.')\n    error_ss = 'This results object has %s and so it does not appear to by an extension of `previous`. Can only compute the news by comparing this results set to previous results objects.'\n    if self.nobs < previous.nobs:\n        raise ValueError(error_ss % 'fewer observations than `previous`')\n    if not (self.k_endog == previous.k_endog and self.k_states == previous.k_states and (self.k_posdef == previous.k_posdef)):\n        raise ValueError(error_ss % 'different state space dimensions than `previous`')\n    for key in self.model.shapes.keys():\n        if key == 'obs':\n            continue\n        tv = getattr(self, key).shape[-1] > 1\n        tv_prev = getattr(previous, key).shape[-1] > 1\n        if tv and (not tv_prev):\n            raise ValueError(error_ss % f'time-varying {key} while `previous` does not')\n        if not tv and tv_prev:\n            raise ValueError(error_ss % f'time-invariant {key} while `previous` does not')\n    if state_index is not None:\n        state_index = np.atleast_1d(np.sort(np.array(state_index, dtype=int)))\n    if end > self.nobs and (not self.model.time_invariant):\n        raise RuntimeError('Cannot compute the impacts of news on periods outside of the sample in time-varying models.')\n    extend_kwargs = {}\n    for key in self.model.shapes.keys():\n        if key == 'obs':\n            continue\n        mat = getattr(self, key)\n        prev_mat = getattr(previous, key)\n        if mat.shape[-1] > prev_mat.shape[-1]:\n            extend_kwargs[key] = mat[..., prev_mat.shape[-1]:]\n    (revisions_ix, updates_ix) = previous.model.diff_endog(self.endog.T)\n    prev_impacted_forecasts = previous.predict(start=start, end=end, **extend_kwargs).smoothed_forecasts\n    post_impacted_forecasts = self.predict(start=start, end=end).smoothed_forecasts\n    if revisions_details_start is True:\n        revisions_details_start = 0\n    elif revisions_details_start is False:\n        revisions_details_start = previous.nobs\n    elif revisions_details_start < 0:\n        revisions_details_start = previous.nobs + revisions_details_start\n    revisions_grouped = []\n    revisions_details = []\n    if revisions_details_start > 0:\n        for (s, i) in revisions_ix:\n            if s < revisions_details_start:\n                revisions_grouped.append((s, i))\n            else:\n                revisions_details.append((s, i))\n    else:\n        revisions_details = revisions_ix\n    if len(revisions_ix) > 0:\n        revisions_details_start = max(revisions_ix[0][0], revisions_details_start)\n    revised_endog = None\n    revised_all = None\n    revised_prev_all = None\n    revisions_all = None\n    revised = None\n    revised_prev = None\n    revisions = None\n    revision_weights = None\n    revision_detailed_impacts = None\n    revision_results = None\n    revision_impacts = None\n    if len(revisions_ix) > 0:\n        (revised_j, revised_p) = zip(*revisions_ix)\n        compute_j = np.arange(revised_j[0], revised_j[-1] + 1)\n        revised_endog = self.endog[:, :previous.nobs].copy()\n        revised_endog[previous.missing.astype(bool)] = np.nan\n        revised_all = revised_endog.T[compute_j]\n        revised_prev_all = previous.endog.T[compute_j]\n        revisions_all = revised_all - revised_prev_all\n        tmp_endog = revised_endog.T.copy()\n        tmp_nobs = max(end, previous.nobs)\n        oos_nobs = tmp_nobs - previous.nobs\n        if oos_nobs > 0:\n            tmp_endog = np.concatenate([tmp_endog, np.zeros((oos_nobs, self.k_endog)) * np.nan], axis=0)\n        clone_kwargs = {}\n        for key in self.model.shapes.keys():\n            if key == 'obs':\n                continue\n            mat = getattr(self, key)\n            if mat.shape[-1] > 1:\n                clone_kwargs[key] = mat[..., :tmp_nobs]\n        rev_mod = previous.model.clone(tmp_endog, **clone_kwargs)\n        init = initialization.Initialization.from_results(self)\n        rev_mod.initialize(init)\n        revision_results = rev_mod.smooth()\n        if len(revisions_details) > 0:\n            compute_j = np.arange(revisions_details_start, revised_j[-1] + 1)\n            offset = revisions_details_start - revised_j[0]\n            revised = revised_all[offset:]\n            revised_prev = revised_prev_all[offset:]\n            revisions = revisions_all[offset:]\n            compute_t = np.arange(start, end)\n            (smoothed_state_weights, _, _) = tools._compute_smoothed_state_weights(rev_mod, compute_t=compute_t, compute_j=compute_j, compute_prior_weights=False, scale=previous.scale)\n            ZT = rev_mod.design.T\n            if ZT.shape[0] > 1:\n                ZT = ZT[compute_t]\n            if state_index is not None:\n                ZT = ZT[:, state_index, :]\n                smoothed_state_weights = smoothed_state_weights[:, :, state_index]\n            revision_weights = np.nansum(smoothed_state_weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n            (revised_j, revised_p) = zip(*[s for s in revisions_ix if s[0] >= revisions_details_start])\n            ix_j = revised_j - revised_j[0]\n            revision_weights = revision_weights.transpose(0, 2, 1, 3)[:, :, ix_j, revised_p]\n            revisions = revisions[ix_j, revised_p]\n            revision_detailed_impacts = revision_weights @ revisions\n            revised = revised[ix_j, revised_p]\n            revised_prev = revised_prev[ix_j, revised_p]\n            if t is not None:\n                revision_weights = revision_weights[0]\n                revision_detailed_impacts = revision_detailed_impacts[0]\n        revised_impact_forecasts = revision_results.smoothed_forecasts[..., start:end]\n        if end > revision_results.nobs:\n            predict_start = max(start, revision_results.nobs)\n            p = revision_results.predict(start=predict_start, end=end, **extend_kwargs)\n            revised_impact_forecasts = np.concatenate((revised_impact_forecasts, p.forecasts), axis=1)\n        revision_impacts = (revised_impact_forecasts - prev_impacted_forecasts).T\n        if t is not None:\n            revision_impacts = revision_impacts[0]\n    if len(revisions_ix) > 0:\n        (revised_j, revised_p) = zip(*revisions_ix)\n        ix_j = revised_j - revised_j[0]\n        revisions_all = revisions_all[ix_j, revised_p]\n        revised_all = revised_all[ix_j, revised_p]\n        revised_prev_all = revised_prev_all[ix_j, revised_p]\n    if len(updates_ix) > 0:\n        (update_t, update_k) = zip(*updates_ix)\n        update_start_t = np.min(update_t)\n        update_end_t = np.max(update_t)\n        if revision_results is None:\n            forecasts = previous.predict(start=update_start_t, end=update_end_t + 1, **extend_kwargs).smoothed_forecasts.T\n        else:\n            forecasts = revision_results.predict(start=update_start_t, end=update_end_t + 1).smoothed_forecasts.T\n        realized = self.endog.T[update_start_t:update_end_t + 1]\n        forecasts_error = realized - forecasts\n        ix_t = update_t - update_start_t\n        update_realized = realized[ix_t, update_k]\n        update_forecasts = forecasts[ix_t, update_k]\n        update_forecasts_error = forecasts_error[ix_t, update_k]\n        if self.design.shape[2] == 1:\n            design = self.design[..., 0][None, ...]\n        elif end <= self.nobs:\n            design = self.design[..., start:end].transpose(2, 0, 1)\n        elif design is None:\n            raise ValueError('Model has time-varying design matrix, so an updated time-varying matrix for period `t` is required.')\n        elif design.ndim == 2:\n            design = design[None, ...]\n        else:\n            design = design.transpose(2, 0, 1)\n        state_gain = previous.smoothed_state_gain(updates_ix, start=start, end=end, extend_kwargs=extend_kwargs)\n        if state_index is not None:\n            design = design[:, :, state_index]\n            state_gain = state_gain[:, state_index]\n        obs_gain = design @ state_gain\n        update_impacts = obs_gain @ update_forecasts_error\n        if t is not None:\n            obs_gain = obs_gain[0]\n            update_impacts = update_impacts[0]\n    else:\n        update_impacts = None\n        update_forecasts = None\n        update_realized = None\n        update_forecasts_error = None\n        obs_gain = None\n    out = SimpleNamespace(update_impacts=update_impacts, revision_detailed_impacts=revision_detailed_impacts, news=update_forecasts_error, revisions=revisions, revisions_all=revisions_all, gain=obs_gain, revision_weights=revision_weights, update_forecasts=update_forecasts, update_realized=update_realized, revised=revised, revised_all=revised_all, revised_prev=revised_prev, revised_prev_all=revised_prev_all, prev_impacted_forecasts=prev_impacted_forecasts, post_impacted_forecasts=post_impacted_forecasts, revision_results=revision_results, revision_impacts=revision_impacts, revisions_ix=revisions_ix, revisions_details=revisions_details, revisions_grouped=revisions_grouped, revisions_details_start=revisions_details_start, updates_ix=updates_ix, state_index=state_index)\n    return out",
            "def news(self, previous, t=None, start=None, end=None, revisions_details_start=True, design=None, state_index=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Compute the news and impacts associated with a data release\\n\\n        Parameters\\n        ----------\\n        previous : SmootherResults\\n            Prior results object relative to which to compute the news. This\\n            results object must have identical state space representation for\\n            the prior sample period so that the only difference is that this\\n            results object has updates to the observed data.\\n        t : int, optional\\n            A specific period for which to compute the news. Cannot be used in\\n            combination with `start` or `end`.\\n        start : int, optional\\n            The start of the interval (inclusive) of news to compute. Cannot be\\n            used in combination with the `t` argument. Default is the last\\n            period of the sample (`nobs - 1`).\\n        end : int, optional\\n            The end of the interval (exclusive) of news to compute. Note that\\n            since it is an exclusive endpoint, the returned news do not include\\n            the value at this index. Cannot be used in combination with the `t`\\n            argument.\\n        revisions_details_start : bool or int, optional\\n            The period at which to beging computing the detailed impacts of\\n            data revisions. Any revisions prior to this period will have their\\n            impacts grouped together. If a negative integer, interpreted as\\n            an offset from the end of the dataset. If set to True, detailed\\n            impacts are computed for all revisions, while if set to False, all\\n            revisions are grouped together. Default is False. Note that for\\n            large models, setting this to be near the beginning of the sample\\n            can cause this function to be slow.\\n        design : array, optional\\n            Design matrix for the period `t` in time-varying models. If this\\n            model has a time-varying design matrix, and the argument `t` is out\\n            of this model\\'s sample, then a new design matrix for period `t`\\n            must be provided. Unused otherwise.\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the impacts of revisions and news. For example, if\\n            `state_index=[0, 1]` is passed, then only the impacts to the\\n            observed variables arising from the impacts to the first two\\n            states will be returned.\\n\\n        Returns\\n        -------\\n        news_results : SimpleNamespace\\n            News and impacts associated with a data release. Includes the\\n            following attributes:\\n\\n            - `update_impacts`: update to forecasts of impacted variables from\\n              the news. It is equivalent to E[y^i | post] - E[y^i | revision],\\n              where y^i are the variables of interest. In [1]_, this is\\n              described as \"revision\" in equation (17).\\n            - `revision_detailed_impacts`: update to forecasts of variables\\n              impacted variables from data revisions. It is\\n              E[y^i | revision] - E[y^i | previous], and does not have a\\n              specific notation in [1]_, since there for simplicity they assume\\n              that there are no revisions.\\n            - `news`: the unexpected component of the updated data. Denoted\\n              I = y^u - E[y^u | previous], where y^u are the data points that\\n              were newly incorporated in a data release (but not including\\n              revisions to data points that already existed in the previous\\n              release). In [1]_, this is described as \"news\" in equation (17).\\n            - `revisions`: y^r(updated) - y^r(previous) for periods in\\n              which detailed impacts were computed\\n            - `revisions_all` : y^r(updated) - y^r(previous) for all revisions\\n            - `gain`: the gain matrix associated with the \"Kalman-like\" update\\n              from the news, E[y I\\'] E[I I\\']^{-1}. In [1]_, this can be found\\n              in the equation For E[y_{k,t_k} \\\\mid I_{v+1}] in the middle of\\n              page 17.\\n            - `revision_weights` weights on observations for the smoothed\\n              signal\\n            - `update_forecasts`: forecasts of the updated periods used to\\n              construct the news, E[y^u | previous].\\n            - `update_realized`: realizations of the updated periods used to\\n              construct the news, y^u.\\n            - `revised`: revised observations of the periods that were revised\\n              and for which detailed impacts were computed\\n            - `revised`: revised observations of the periods that were revised\\n            - `revised_prev`: previous observations of the periods that were\\n              revised and for which detailed impacts were computed\\n            - `revised_prev_all`: previous observations of the periods that\\n              were revised and for which detailed impacts were computed\\n            - `prev_impacted_forecasts`: previous forecast of the periods of\\n              interest, E[y^i | previous].\\n            - `post_impacted_forecasts`: forecast of the periods of interest\\n              after taking into account both revisions and updates,\\n              E[y^i | post].\\n            - `revision_results`: results object that updates the `previous`\\n              results to take into account data revisions.\\n            - `revision_results`: results object associated with the revisions\\n            - `revision_impacts`: total impacts from all revisions (both\\n              grouped and detailed)\\n            - `revisions_ix`: list of `(t, i)` positions of revisions in endog\\n            - `revisions_details`: list of `(t, i)` positions of revisions to\\n              endog for which details of impacts were computed\\n            - `revisions_grouped`: list of `(t, i)` positions of revisions to\\n              endog for which impacts were grouped\\n            - `revisions_details_start`: period in which revision details start\\n              to be computed\\n            - `updates_ix`: list of `(t, i)` positions of updates to endog\\n            - `state_index`: index of state variables used to compute impacts\\n\\n        Notes\\n        -----\\n        This method computes the effect of new data (e.g. from a new data\\n        release) on smoothed forecasts produced by a state space model, as\\n        described in [1]_. It also computes the effect of revised data on\\n        smoothed forecasts.\\n\\n        References\\n        ----------\\n        .. [1] Ba\u0144bura, Marta and Modugno, Michele. 2010.\\n               \"Maximum likelihood estimation of factor models on data sets\\n               with arbitrary pattern of missing data.\"\\n               No 1189, Working Paper Series, European Central Bank.\\n               https://EconPapers.repec.org/RePEc:ecb:ecbwps:20101189.\\n        .. [2] Ba\u0144bura, Marta, and Michele Modugno.\\n               \"Maximum likelihood estimation of factor models on datasets with\\n               arbitrary pattern of missing data.\"\\n               Journal of Applied Econometrics 29, no. 1 (2014): 133-160.\\n\\n        '\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = self.nobs - 1\n    if end is None:\n        end = self.nobs\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end <= start:\n        raise ValueError('`end` must be after `start`')\n    if self.smoothed_state_cov is None:\n        raise ValueError('Cannot compute news without having applied the Kalman smoother first.')\n    error_ss = 'This results object has %s and so it does not appear to by an extension of `previous`. Can only compute the news by comparing this results set to previous results objects.'\n    if self.nobs < previous.nobs:\n        raise ValueError(error_ss % 'fewer observations than `previous`')\n    if not (self.k_endog == previous.k_endog and self.k_states == previous.k_states and (self.k_posdef == previous.k_posdef)):\n        raise ValueError(error_ss % 'different state space dimensions than `previous`')\n    for key in self.model.shapes.keys():\n        if key == 'obs':\n            continue\n        tv = getattr(self, key).shape[-1] > 1\n        tv_prev = getattr(previous, key).shape[-1] > 1\n        if tv and (not tv_prev):\n            raise ValueError(error_ss % f'time-varying {key} while `previous` does not')\n        if not tv and tv_prev:\n            raise ValueError(error_ss % f'time-invariant {key} while `previous` does not')\n    if state_index is not None:\n        state_index = np.atleast_1d(np.sort(np.array(state_index, dtype=int)))\n    if end > self.nobs and (not self.model.time_invariant):\n        raise RuntimeError('Cannot compute the impacts of news on periods outside of the sample in time-varying models.')\n    extend_kwargs = {}\n    for key in self.model.shapes.keys():\n        if key == 'obs':\n            continue\n        mat = getattr(self, key)\n        prev_mat = getattr(previous, key)\n        if mat.shape[-1] > prev_mat.shape[-1]:\n            extend_kwargs[key] = mat[..., prev_mat.shape[-1]:]\n    (revisions_ix, updates_ix) = previous.model.diff_endog(self.endog.T)\n    prev_impacted_forecasts = previous.predict(start=start, end=end, **extend_kwargs).smoothed_forecasts\n    post_impacted_forecasts = self.predict(start=start, end=end).smoothed_forecasts\n    if revisions_details_start is True:\n        revisions_details_start = 0\n    elif revisions_details_start is False:\n        revisions_details_start = previous.nobs\n    elif revisions_details_start < 0:\n        revisions_details_start = previous.nobs + revisions_details_start\n    revisions_grouped = []\n    revisions_details = []\n    if revisions_details_start > 0:\n        for (s, i) in revisions_ix:\n            if s < revisions_details_start:\n                revisions_grouped.append((s, i))\n            else:\n                revisions_details.append((s, i))\n    else:\n        revisions_details = revisions_ix\n    if len(revisions_ix) > 0:\n        revisions_details_start = max(revisions_ix[0][0], revisions_details_start)\n    revised_endog = None\n    revised_all = None\n    revised_prev_all = None\n    revisions_all = None\n    revised = None\n    revised_prev = None\n    revisions = None\n    revision_weights = None\n    revision_detailed_impacts = None\n    revision_results = None\n    revision_impacts = None\n    if len(revisions_ix) > 0:\n        (revised_j, revised_p) = zip(*revisions_ix)\n        compute_j = np.arange(revised_j[0], revised_j[-1] + 1)\n        revised_endog = self.endog[:, :previous.nobs].copy()\n        revised_endog[previous.missing.astype(bool)] = np.nan\n        revised_all = revised_endog.T[compute_j]\n        revised_prev_all = previous.endog.T[compute_j]\n        revisions_all = revised_all - revised_prev_all\n        tmp_endog = revised_endog.T.copy()\n        tmp_nobs = max(end, previous.nobs)\n        oos_nobs = tmp_nobs - previous.nobs\n        if oos_nobs > 0:\n            tmp_endog = np.concatenate([tmp_endog, np.zeros((oos_nobs, self.k_endog)) * np.nan], axis=0)\n        clone_kwargs = {}\n        for key in self.model.shapes.keys():\n            if key == 'obs':\n                continue\n            mat = getattr(self, key)\n            if mat.shape[-1] > 1:\n                clone_kwargs[key] = mat[..., :tmp_nobs]\n        rev_mod = previous.model.clone(tmp_endog, **clone_kwargs)\n        init = initialization.Initialization.from_results(self)\n        rev_mod.initialize(init)\n        revision_results = rev_mod.smooth()\n        if len(revisions_details) > 0:\n            compute_j = np.arange(revisions_details_start, revised_j[-1] + 1)\n            offset = revisions_details_start - revised_j[0]\n            revised = revised_all[offset:]\n            revised_prev = revised_prev_all[offset:]\n            revisions = revisions_all[offset:]\n            compute_t = np.arange(start, end)\n            (smoothed_state_weights, _, _) = tools._compute_smoothed_state_weights(rev_mod, compute_t=compute_t, compute_j=compute_j, compute_prior_weights=False, scale=previous.scale)\n            ZT = rev_mod.design.T\n            if ZT.shape[0] > 1:\n                ZT = ZT[compute_t]\n            if state_index is not None:\n                ZT = ZT[:, state_index, :]\n                smoothed_state_weights = smoothed_state_weights[:, :, state_index]\n            revision_weights = np.nansum(smoothed_state_weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n            (revised_j, revised_p) = zip(*[s for s in revisions_ix if s[0] >= revisions_details_start])\n            ix_j = revised_j - revised_j[0]\n            revision_weights = revision_weights.transpose(0, 2, 1, 3)[:, :, ix_j, revised_p]\n            revisions = revisions[ix_j, revised_p]\n            revision_detailed_impacts = revision_weights @ revisions\n            revised = revised[ix_j, revised_p]\n            revised_prev = revised_prev[ix_j, revised_p]\n            if t is not None:\n                revision_weights = revision_weights[0]\n                revision_detailed_impacts = revision_detailed_impacts[0]\n        revised_impact_forecasts = revision_results.smoothed_forecasts[..., start:end]\n        if end > revision_results.nobs:\n            predict_start = max(start, revision_results.nobs)\n            p = revision_results.predict(start=predict_start, end=end, **extend_kwargs)\n            revised_impact_forecasts = np.concatenate((revised_impact_forecasts, p.forecasts), axis=1)\n        revision_impacts = (revised_impact_forecasts - prev_impacted_forecasts).T\n        if t is not None:\n            revision_impacts = revision_impacts[0]\n    if len(revisions_ix) > 0:\n        (revised_j, revised_p) = zip(*revisions_ix)\n        ix_j = revised_j - revised_j[0]\n        revisions_all = revisions_all[ix_j, revised_p]\n        revised_all = revised_all[ix_j, revised_p]\n        revised_prev_all = revised_prev_all[ix_j, revised_p]\n    if len(updates_ix) > 0:\n        (update_t, update_k) = zip(*updates_ix)\n        update_start_t = np.min(update_t)\n        update_end_t = np.max(update_t)\n        if revision_results is None:\n            forecasts = previous.predict(start=update_start_t, end=update_end_t + 1, **extend_kwargs).smoothed_forecasts.T\n        else:\n            forecasts = revision_results.predict(start=update_start_t, end=update_end_t + 1).smoothed_forecasts.T\n        realized = self.endog.T[update_start_t:update_end_t + 1]\n        forecasts_error = realized - forecasts\n        ix_t = update_t - update_start_t\n        update_realized = realized[ix_t, update_k]\n        update_forecasts = forecasts[ix_t, update_k]\n        update_forecasts_error = forecasts_error[ix_t, update_k]\n        if self.design.shape[2] == 1:\n            design = self.design[..., 0][None, ...]\n        elif end <= self.nobs:\n            design = self.design[..., start:end].transpose(2, 0, 1)\n        elif design is None:\n            raise ValueError('Model has time-varying design matrix, so an updated time-varying matrix for period `t` is required.')\n        elif design.ndim == 2:\n            design = design[None, ...]\n        else:\n            design = design.transpose(2, 0, 1)\n        state_gain = previous.smoothed_state_gain(updates_ix, start=start, end=end, extend_kwargs=extend_kwargs)\n        if state_index is not None:\n            design = design[:, :, state_index]\n            state_gain = state_gain[:, state_index]\n        obs_gain = design @ state_gain\n        update_impacts = obs_gain @ update_forecasts_error\n        if t is not None:\n            obs_gain = obs_gain[0]\n            update_impacts = update_impacts[0]\n    else:\n        update_impacts = None\n        update_forecasts = None\n        update_realized = None\n        update_forecasts_error = None\n        obs_gain = None\n    out = SimpleNamespace(update_impacts=update_impacts, revision_detailed_impacts=revision_detailed_impacts, news=update_forecasts_error, revisions=revisions, revisions_all=revisions_all, gain=obs_gain, revision_weights=revision_weights, update_forecasts=update_forecasts, update_realized=update_realized, revised=revised, revised_all=revised_all, revised_prev=revised_prev, revised_prev_all=revised_prev_all, prev_impacted_forecasts=prev_impacted_forecasts, post_impacted_forecasts=post_impacted_forecasts, revision_results=revision_results, revision_impacts=revision_impacts, revisions_ix=revisions_ix, revisions_details=revisions_details, revisions_grouped=revisions_grouped, revisions_details_start=revisions_details_start, updates_ix=updates_ix, state_index=state_index)\n    return out",
            "def news(self, previous, t=None, start=None, end=None, revisions_details_start=True, design=None, state_index=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Compute the news and impacts associated with a data release\\n\\n        Parameters\\n        ----------\\n        previous : SmootherResults\\n            Prior results object relative to which to compute the news. This\\n            results object must have identical state space representation for\\n            the prior sample period so that the only difference is that this\\n            results object has updates to the observed data.\\n        t : int, optional\\n            A specific period for which to compute the news. Cannot be used in\\n            combination with `start` or `end`.\\n        start : int, optional\\n            The start of the interval (inclusive) of news to compute. Cannot be\\n            used in combination with the `t` argument. Default is the last\\n            period of the sample (`nobs - 1`).\\n        end : int, optional\\n            The end of the interval (exclusive) of news to compute. Note that\\n            since it is an exclusive endpoint, the returned news do not include\\n            the value at this index. Cannot be used in combination with the `t`\\n            argument.\\n        revisions_details_start : bool or int, optional\\n            The period at which to beging computing the detailed impacts of\\n            data revisions. Any revisions prior to this period will have their\\n            impacts grouped together. If a negative integer, interpreted as\\n            an offset from the end of the dataset. If set to True, detailed\\n            impacts are computed for all revisions, while if set to False, all\\n            revisions are grouped together. Default is False. Note that for\\n            large models, setting this to be near the beginning of the sample\\n            can cause this function to be slow.\\n        design : array, optional\\n            Design matrix for the period `t` in time-varying models. If this\\n            model has a time-varying design matrix, and the argument `t` is out\\n            of this model\\'s sample, then a new design matrix for period `t`\\n            must be provided. Unused otherwise.\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the impacts of revisions and news. For example, if\\n            `state_index=[0, 1]` is passed, then only the impacts to the\\n            observed variables arising from the impacts to the first two\\n            states will be returned.\\n\\n        Returns\\n        -------\\n        news_results : SimpleNamespace\\n            News and impacts associated with a data release. Includes the\\n            following attributes:\\n\\n            - `update_impacts`: update to forecasts of impacted variables from\\n              the news. It is equivalent to E[y^i | post] - E[y^i | revision],\\n              where y^i are the variables of interest. In [1]_, this is\\n              described as \"revision\" in equation (17).\\n            - `revision_detailed_impacts`: update to forecasts of variables\\n              impacted variables from data revisions. It is\\n              E[y^i | revision] - E[y^i | previous], and does not have a\\n              specific notation in [1]_, since there for simplicity they assume\\n              that there are no revisions.\\n            - `news`: the unexpected component of the updated data. Denoted\\n              I = y^u - E[y^u | previous], where y^u are the data points that\\n              were newly incorporated in a data release (but not including\\n              revisions to data points that already existed in the previous\\n              release). In [1]_, this is described as \"news\" in equation (17).\\n            - `revisions`: y^r(updated) - y^r(previous) for periods in\\n              which detailed impacts were computed\\n            - `revisions_all` : y^r(updated) - y^r(previous) for all revisions\\n            - `gain`: the gain matrix associated with the \"Kalman-like\" update\\n              from the news, E[y I\\'] E[I I\\']^{-1}. In [1]_, this can be found\\n              in the equation For E[y_{k,t_k} \\\\mid I_{v+1}] in the middle of\\n              page 17.\\n            - `revision_weights` weights on observations for the smoothed\\n              signal\\n            - `update_forecasts`: forecasts of the updated periods used to\\n              construct the news, E[y^u | previous].\\n            - `update_realized`: realizations of the updated periods used to\\n              construct the news, y^u.\\n            - `revised`: revised observations of the periods that were revised\\n              and for which detailed impacts were computed\\n            - `revised`: revised observations of the periods that were revised\\n            - `revised_prev`: previous observations of the periods that were\\n              revised and for which detailed impacts were computed\\n            - `revised_prev_all`: previous observations of the periods that\\n              were revised and for which detailed impacts were computed\\n            - `prev_impacted_forecasts`: previous forecast of the periods of\\n              interest, E[y^i | previous].\\n            - `post_impacted_forecasts`: forecast of the periods of interest\\n              after taking into account both revisions and updates,\\n              E[y^i | post].\\n            - `revision_results`: results object that updates the `previous`\\n              results to take into account data revisions.\\n            - `revision_results`: results object associated with the revisions\\n            - `revision_impacts`: total impacts from all revisions (both\\n              grouped and detailed)\\n            - `revisions_ix`: list of `(t, i)` positions of revisions in endog\\n            - `revisions_details`: list of `(t, i)` positions of revisions to\\n              endog for which details of impacts were computed\\n            - `revisions_grouped`: list of `(t, i)` positions of revisions to\\n              endog for which impacts were grouped\\n            - `revisions_details_start`: period in which revision details start\\n              to be computed\\n            - `updates_ix`: list of `(t, i)` positions of updates to endog\\n            - `state_index`: index of state variables used to compute impacts\\n\\n        Notes\\n        -----\\n        This method computes the effect of new data (e.g. from a new data\\n        release) on smoothed forecasts produced by a state space model, as\\n        described in [1]_. It also computes the effect of revised data on\\n        smoothed forecasts.\\n\\n        References\\n        ----------\\n        .. [1] Ba\u0144bura, Marta and Modugno, Michele. 2010.\\n               \"Maximum likelihood estimation of factor models on data sets\\n               with arbitrary pattern of missing data.\"\\n               No 1189, Working Paper Series, European Central Bank.\\n               https://EconPapers.repec.org/RePEc:ecb:ecbwps:20101189.\\n        .. [2] Ba\u0144bura, Marta, and Michele Modugno.\\n               \"Maximum likelihood estimation of factor models on datasets with\\n               arbitrary pattern of missing data.\"\\n               Journal of Applied Econometrics 29, no. 1 (2014): 133-160.\\n\\n        '\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = self.nobs - 1\n    if end is None:\n        end = self.nobs\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end <= start:\n        raise ValueError('`end` must be after `start`')\n    if self.smoothed_state_cov is None:\n        raise ValueError('Cannot compute news without having applied the Kalman smoother first.')\n    error_ss = 'This results object has %s and so it does not appear to by an extension of `previous`. Can only compute the news by comparing this results set to previous results objects.'\n    if self.nobs < previous.nobs:\n        raise ValueError(error_ss % 'fewer observations than `previous`')\n    if not (self.k_endog == previous.k_endog and self.k_states == previous.k_states and (self.k_posdef == previous.k_posdef)):\n        raise ValueError(error_ss % 'different state space dimensions than `previous`')\n    for key in self.model.shapes.keys():\n        if key == 'obs':\n            continue\n        tv = getattr(self, key).shape[-1] > 1\n        tv_prev = getattr(previous, key).shape[-1] > 1\n        if tv and (not tv_prev):\n            raise ValueError(error_ss % f'time-varying {key} while `previous` does not')\n        if not tv and tv_prev:\n            raise ValueError(error_ss % f'time-invariant {key} while `previous` does not')\n    if state_index is not None:\n        state_index = np.atleast_1d(np.sort(np.array(state_index, dtype=int)))\n    if end > self.nobs and (not self.model.time_invariant):\n        raise RuntimeError('Cannot compute the impacts of news on periods outside of the sample in time-varying models.')\n    extend_kwargs = {}\n    for key in self.model.shapes.keys():\n        if key == 'obs':\n            continue\n        mat = getattr(self, key)\n        prev_mat = getattr(previous, key)\n        if mat.shape[-1] > prev_mat.shape[-1]:\n            extend_kwargs[key] = mat[..., prev_mat.shape[-1]:]\n    (revisions_ix, updates_ix) = previous.model.diff_endog(self.endog.T)\n    prev_impacted_forecasts = previous.predict(start=start, end=end, **extend_kwargs).smoothed_forecasts\n    post_impacted_forecasts = self.predict(start=start, end=end).smoothed_forecasts\n    if revisions_details_start is True:\n        revisions_details_start = 0\n    elif revisions_details_start is False:\n        revisions_details_start = previous.nobs\n    elif revisions_details_start < 0:\n        revisions_details_start = previous.nobs + revisions_details_start\n    revisions_grouped = []\n    revisions_details = []\n    if revisions_details_start > 0:\n        for (s, i) in revisions_ix:\n            if s < revisions_details_start:\n                revisions_grouped.append((s, i))\n            else:\n                revisions_details.append((s, i))\n    else:\n        revisions_details = revisions_ix\n    if len(revisions_ix) > 0:\n        revisions_details_start = max(revisions_ix[0][0], revisions_details_start)\n    revised_endog = None\n    revised_all = None\n    revised_prev_all = None\n    revisions_all = None\n    revised = None\n    revised_prev = None\n    revisions = None\n    revision_weights = None\n    revision_detailed_impacts = None\n    revision_results = None\n    revision_impacts = None\n    if len(revisions_ix) > 0:\n        (revised_j, revised_p) = zip(*revisions_ix)\n        compute_j = np.arange(revised_j[0], revised_j[-1] + 1)\n        revised_endog = self.endog[:, :previous.nobs].copy()\n        revised_endog[previous.missing.astype(bool)] = np.nan\n        revised_all = revised_endog.T[compute_j]\n        revised_prev_all = previous.endog.T[compute_j]\n        revisions_all = revised_all - revised_prev_all\n        tmp_endog = revised_endog.T.copy()\n        tmp_nobs = max(end, previous.nobs)\n        oos_nobs = tmp_nobs - previous.nobs\n        if oos_nobs > 0:\n            tmp_endog = np.concatenate([tmp_endog, np.zeros((oos_nobs, self.k_endog)) * np.nan], axis=0)\n        clone_kwargs = {}\n        for key in self.model.shapes.keys():\n            if key == 'obs':\n                continue\n            mat = getattr(self, key)\n            if mat.shape[-1] > 1:\n                clone_kwargs[key] = mat[..., :tmp_nobs]\n        rev_mod = previous.model.clone(tmp_endog, **clone_kwargs)\n        init = initialization.Initialization.from_results(self)\n        rev_mod.initialize(init)\n        revision_results = rev_mod.smooth()\n        if len(revisions_details) > 0:\n            compute_j = np.arange(revisions_details_start, revised_j[-1] + 1)\n            offset = revisions_details_start - revised_j[0]\n            revised = revised_all[offset:]\n            revised_prev = revised_prev_all[offset:]\n            revisions = revisions_all[offset:]\n            compute_t = np.arange(start, end)\n            (smoothed_state_weights, _, _) = tools._compute_smoothed_state_weights(rev_mod, compute_t=compute_t, compute_j=compute_j, compute_prior_weights=False, scale=previous.scale)\n            ZT = rev_mod.design.T\n            if ZT.shape[0] > 1:\n                ZT = ZT[compute_t]\n            if state_index is not None:\n                ZT = ZT[:, state_index, :]\n                smoothed_state_weights = smoothed_state_weights[:, :, state_index]\n            revision_weights = np.nansum(smoothed_state_weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n            (revised_j, revised_p) = zip(*[s for s in revisions_ix if s[0] >= revisions_details_start])\n            ix_j = revised_j - revised_j[0]\n            revision_weights = revision_weights.transpose(0, 2, 1, 3)[:, :, ix_j, revised_p]\n            revisions = revisions[ix_j, revised_p]\n            revision_detailed_impacts = revision_weights @ revisions\n            revised = revised[ix_j, revised_p]\n            revised_prev = revised_prev[ix_j, revised_p]\n            if t is not None:\n                revision_weights = revision_weights[0]\n                revision_detailed_impacts = revision_detailed_impacts[0]\n        revised_impact_forecasts = revision_results.smoothed_forecasts[..., start:end]\n        if end > revision_results.nobs:\n            predict_start = max(start, revision_results.nobs)\n            p = revision_results.predict(start=predict_start, end=end, **extend_kwargs)\n            revised_impact_forecasts = np.concatenate((revised_impact_forecasts, p.forecasts), axis=1)\n        revision_impacts = (revised_impact_forecasts - prev_impacted_forecasts).T\n        if t is not None:\n            revision_impacts = revision_impacts[0]\n    if len(revisions_ix) > 0:\n        (revised_j, revised_p) = zip(*revisions_ix)\n        ix_j = revised_j - revised_j[0]\n        revisions_all = revisions_all[ix_j, revised_p]\n        revised_all = revised_all[ix_j, revised_p]\n        revised_prev_all = revised_prev_all[ix_j, revised_p]\n    if len(updates_ix) > 0:\n        (update_t, update_k) = zip(*updates_ix)\n        update_start_t = np.min(update_t)\n        update_end_t = np.max(update_t)\n        if revision_results is None:\n            forecasts = previous.predict(start=update_start_t, end=update_end_t + 1, **extend_kwargs).smoothed_forecasts.T\n        else:\n            forecasts = revision_results.predict(start=update_start_t, end=update_end_t + 1).smoothed_forecasts.T\n        realized = self.endog.T[update_start_t:update_end_t + 1]\n        forecasts_error = realized - forecasts\n        ix_t = update_t - update_start_t\n        update_realized = realized[ix_t, update_k]\n        update_forecasts = forecasts[ix_t, update_k]\n        update_forecasts_error = forecasts_error[ix_t, update_k]\n        if self.design.shape[2] == 1:\n            design = self.design[..., 0][None, ...]\n        elif end <= self.nobs:\n            design = self.design[..., start:end].transpose(2, 0, 1)\n        elif design is None:\n            raise ValueError('Model has time-varying design matrix, so an updated time-varying matrix for period `t` is required.')\n        elif design.ndim == 2:\n            design = design[None, ...]\n        else:\n            design = design.transpose(2, 0, 1)\n        state_gain = previous.smoothed_state_gain(updates_ix, start=start, end=end, extend_kwargs=extend_kwargs)\n        if state_index is not None:\n            design = design[:, :, state_index]\n            state_gain = state_gain[:, state_index]\n        obs_gain = design @ state_gain\n        update_impacts = obs_gain @ update_forecasts_error\n        if t is not None:\n            obs_gain = obs_gain[0]\n            update_impacts = update_impacts[0]\n    else:\n        update_impacts = None\n        update_forecasts = None\n        update_realized = None\n        update_forecasts_error = None\n        obs_gain = None\n    out = SimpleNamespace(update_impacts=update_impacts, revision_detailed_impacts=revision_detailed_impacts, news=update_forecasts_error, revisions=revisions, revisions_all=revisions_all, gain=obs_gain, revision_weights=revision_weights, update_forecasts=update_forecasts, update_realized=update_realized, revised=revised, revised_all=revised_all, revised_prev=revised_prev, revised_prev_all=revised_prev_all, prev_impacted_forecasts=prev_impacted_forecasts, post_impacted_forecasts=post_impacted_forecasts, revision_results=revision_results, revision_impacts=revision_impacts, revisions_ix=revisions_ix, revisions_details=revisions_details, revisions_grouped=revisions_grouped, revisions_details_start=revisions_details_start, updates_ix=updates_ix, state_index=state_index)\n    return out",
            "def news(self, previous, t=None, start=None, end=None, revisions_details_start=True, design=None, state_index=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Compute the news and impacts associated with a data release\\n\\n        Parameters\\n        ----------\\n        previous : SmootherResults\\n            Prior results object relative to which to compute the news. This\\n            results object must have identical state space representation for\\n            the prior sample period so that the only difference is that this\\n            results object has updates to the observed data.\\n        t : int, optional\\n            A specific period for which to compute the news. Cannot be used in\\n            combination with `start` or `end`.\\n        start : int, optional\\n            The start of the interval (inclusive) of news to compute. Cannot be\\n            used in combination with the `t` argument. Default is the last\\n            period of the sample (`nobs - 1`).\\n        end : int, optional\\n            The end of the interval (exclusive) of news to compute. Note that\\n            since it is an exclusive endpoint, the returned news do not include\\n            the value at this index. Cannot be used in combination with the `t`\\n            argument.\\n        revisions_details_start : bool or int, optional\\n            The period at which to beging computing the detailed impacts of\\n            data revisions. Any revisions prior to this period will have their\\n            impacts grouped together. If a negative integer, interpreted as\\n            an offset from the end of the dataset. If set to True, detailed\\n            impacts are computed for all revisions, while if set to False, all\\n            revisions are grouped together. Default is False. Note that for\\n            large models, setting this to be near the beginning of the sample\\n            can cause this function to be slow.\\n        design : array, optional\\n            Design matrix for the period `t` in time-varying models. If this\\n            model has a time-varying design matrix, and the argument `t` is out\\n            of this model\\'s sample, then a new design matrix for period `t`\\n            must be provided. Unused otherwise.\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the impacts of revisions and news. For example, if\\n            `state_index=[0, 1]` is passed, then only the impacts to the\\n            observed variables arising from the impacts to the first two\\n            states will be returned.\\n\\n        Returns\\n        -------\\n        news_results : SimpleNamespace\\n            News and impacts associated with a data release. Includes the\\n            following attributes:\\n\\n            - `update_impacts`: update to forecasts of impacted variables from\\n              the news. It is equivalent to E[y^i | post] - E[y^i | revision],\\n              where y^i are the variables of interest. In [1]_, this is\\n              described as \"revision\" in equation (17).\\n            - `revision_detailed_impacts`: update to forecasts of variables\\n              impacted variables from data revisions. It is\\n              E[y^i | revision] - E[y^i | previous], and does not have a\\n              specific notation in [1]_, since there for simplicity they assume\\n              that there are no revisions.\\n            - `news`: the unexpected component of the updated data. Denoted\\n              I = y^u - E[y^u | previous], where y^u are the data points that\\n              were newly incorporated in a data release (but not including\\n              revisions to data points that already existed in the previous\\n              release). In [1]_, this is described as \"news\" in equation (17).\\n            - `revisions`: y^r(updated) - y^r(previous) for periods in\\n              which detailed impacts were computed\\n            - `revisions_all` : y^r(updated) - y^r(previous) for all revisions\\n            - `gain`: the gain matrix associated with the \"Kalman-like\" update\\n              from the news, E[y I\\'] E[I I\\']^{-1}. In [1]_, this can be found\\n              in the equation For E[y_{k,t_k} \\\\mid I_{v+1}] in the middle of\\n              page 17.\\n            - `revision_weights` weights on observations for the smoothed\\n              signal\\n            - `update_forecasts`: forecasts of the updated periods used to\\n              construct the news, E[y^u | previous].\\n            - `update_realized`: realizations of the updated periods used to\\n              construct the news, y^u.\\n            - `revised`: revised observations of the periods that were revised\\n              and for which detailed impacts were computed\\n            - `revised`: revised observations of the periods that were revised\\n            - `revised_prev`: previous observations of the periods that were\\n              revised and for which detailed impacts were computed\\n            - `revised_prev_all`: previous observations of the periods that\\n              were revised and for which detailed impacts were computed\\n            - `prev_impacted_forecasts`: previous forecast of the periods of\\n              interest, E[y^i | previous].\\n            - `post_impacted_forecasts`: forecast of the periods of interest\\n              after taking into account both revisions and updates,\\n              E[y^i | post].\\n            - `revision_results`: results object that updates the `previous`\\n              results to take into account data revisions.\\n            - `revision_results`: results object associated with the revisions\\n            - `revision_impacts`: total impacts from all revisions (both\\n              grouped and detailed)\\n            - `revisions_ix`: list of `(t, i)` positions of revisions in endog\\n            - `revisions_details`: list of `(t, i)` positions of revisions to\\n              endog for which details of impacts were computed\\n            - `revisions_grouped`: list of `(t, i)` positions of revisions to\\n              endog for which impacts were grouped\\n            - `revisions_details_start`: period in which revision details start\\n              to be computed\\n            - `updates_ix`: list of `(t, i)` positions of updates to endog\\n            - `state_index`: index of state variables used to compute impacts\\n\\n        Notes\\n        -----\\n        This method computes the effect of new data (e.g. from a new data\\n        release) on smoothed forecasts produced by a state space model, as\\n        described in [1]_. It also computes the effect of revised data on\\n        smoothed forecasts.\\n\\n        References\\n        ----------\\n        .. [1] Ba\u0144bura, Marta and Modugno, Michele. 2010.\\n               \"Maximum likelihood estimation of factor models on data sets\\n               with arbitrary pattern of missing data.\"\\n               No 1189, Working Paper Series, European Central Bank.\\n               https://EconPapers.repec.org/RePEc:ecb:ecbwps:20101189.\\n        .. [2] Ba\u0144bura, Marta, and Michele Modugno.\\n               \"Maximum likelihood estimation of factor models on datasets with\\n               arbitrary pattern of missing data.\"\\n               Journal of Applied Econometrics 29, no. 1 (2014): 133-160.\\n\\n        '\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = self.nobs - 1\n    if end is None:\n        end = self.nobs\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end <= start:\n        raise ValueError('`end` must be after `start`')\n    if self.smoothed_state_cov is None:\n        raise ValueError('Cannot compute news without having applied the Kalman smoother first.')\n    error_ss = 'This results object has %s and so it does not appear to by an extension of `previous`. Can only compute the news by comparing this results set to previous results objects.'\n    if self.nobs < previous.nobs:\n        raise ValueError(error_ss % 'fewer observations than `previous`')\n    if not (self.k_endog == previous.k_endog and self.k_states == previous.k_states and (self.k_posdef == previous.k_posdef)):\n        raise ValueError(error_ss % 'different state space dimensions than `previous`')\n    for key in self.model.shapes.keys():\n        if key == 'obs':\n            continue\n        tv = getattr(self, key).shape[-1] > 1\n        tv_prev = getattr(previous, key).shape[-1] > 1\n        if tv and (not tv_prev):\n            raise ValueError(error_ss % f'time-varying {key} while `previous` does not')\n        if not tv and tv_prev:\n            raise ValueError(error_ss % f'time-invariant {key} while `previous` does not')\n    if state_index is not None:\n        state_index = np.atleast_1d(np.sort(np.array(state_index, dtype=int)))\n    if end > self.nobs and (not self.model.time_invariant):\n        raise RuntimeError('Cannot compute the impacts of news on periods outside of the sample in time-varying models.')\n    extend_kwargs = {}\n    for key in self.model.shapes.keys():\n        if key == 'obs':\n            continue\n        mat = getattr(self, key)\n        prev_mat = getattr(previous, key)\n        if mat.shape[-1] > prev_mat.shape[-1]:\n            extend_kwargs[key] = mat[..., prev_mat.shape[-1]:]\n    (revisions_ix, updates_ix) = previous.model.diff_endog(self.endog.T)\n    prev_impacted_forecasts = previous.predict(start=start, end=end, **extend_kwargs).smoothed_forecasts\n    post_impacted_forecasts = self.predict(start=start, end=end).smoothed_forecasts\n    if revisions_details_start is True:\n        revisions_details_start = 0\n    elif revisions_details_start is False:\n        revisions_details_start = previous.nobs\n    elif revisions_details_start < 0:\n        revisions_details_start = previous.nobs + revisions_details_start\n    revisions_grouped = []\n    revisions_details = []\n    if revisions_details_start > 0:\n        for (s, i) in revisions_ix:\n            if s < revisions_details_start:\n                revisions_grouped.append((s, i))\n            else:\n                revisions_details.append((s, i))\n    else:\n        revisions_details = revisions_ix\n    if len(revisions_ix) > 0:\n        revisions_details_start = max(revisions_ix[0][0], revisions_details_start)\n    revised_endog = None\n    revised_all = None\n    revised_prev_all = None\n    revisions_all = None\n    revised = None\n    revised_prev = None\n    revisions = None\n    revision_weights = None\n    revision_detailed_impacts = None\n    revision_results = None\n    revision_impacts = None\n    if len(revisions_ix) > 0:\n        (revised_j, revised_p) = zip(*revisions_ix)\n        compute_j = np.arange(revised_j[0], revised_j[-1] + 1)\n        revised_endog = self.endog[:, :previous.nobs].copy()\n        revised_endog[previous.missing.astype(bool)] = np.nan\n        revised_all = revised_endog.T[compute_j]\n        revised_prev_all = previous.endog.T[compute_j]\n        revisions_all = revised_all - revised_prev_all\n        tmp_endog = revised_endog.T.copy()\n        tmp_nobs = max(end, previous.nobs)\n        oos_nobs = tmp_nobs - previous.nobs\n        if oos_nobs > 0:\n            tmp_endog = np.concatenate([tmp_endog, np.zeros((oos_nobs, self.k_endog)) * np.nan], axis=0)\n        clone_kwargs = {}\n        for key in self.model.shapes.keys():\n            if key == 'obs':\n                continue\n            mat = getattr(self, key)\n            if mat.shape[-1] > 1:\n                clone_kwargs[key] = mat[..., :tmp_nobs]\n        rev_mod = previous.model.clone(tmp_endog, **clone_kwargs)\n        init = initialization.Initialization.from_results(self)\n        rev_mod.initialize(init)\n        revision_results = rev_mod.smooth()\n        if len(revisions_details) > 0:\n            compute_j = np.arange(revisions_details_start, revised_j[-1] + 1)\n            offset = revisions_details_start - revised_j[0]\n            revised = revised_all[offset:]\n            revised_prev = revised_prev_all[offset:]\n            revisions = revisions_all[offset:]\n            compute_t = np.arange(start, end)\n            (smoothed_state_weights, _, _) = tools._compute_smoothed_state_weights(rev_mod, compute_t=compute_t, compute_j=compute_j, compute_prior_weights=False, scale=previous.scale)\n            ZT = rev_mod.design.T\n            if ZT.shape[0] > 1:\n                ZT = ZT[compute_t]\n            if state_index is not None:\n                ZT = ZT[:, state_index, :]\n                smoothed_state_weights = smoothed_state_weights[:, :, state_index]\n            revision_weights = np.nansum(smoothed_state_weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n            (revised_j, revised_p) = zip(*[s for s in revisions_ix if s[0] >= revisions_details_start])\n            ix_j = revised_j - revised_j[0]\n            revision_weights = revision_weights.transpose(0, 2, 1, 3)[:, :, ix_j, revised_p]\n            revisions = revisions[ix_j, revised_p]\n            revision_detailed_impacts = revision_weights @ revisions\n            revised = revised[ix_j, revised_p]\n            revised_prev = revised_prev[ix_j, revised_p]\n            if t is not None:\n                revision_weights = revision_weights[0]\n                revision_detailed_impacts = revision_detailed_impacts[0]\n        revised_impact_forecasts = revision_results.smoothed_forecasts[..., start:end]\n        if end > revision_results.nobs:\n            predict_start = max(start, revision_results.nobs)\n            p = revision_results.predict(start=predict_start, end=end, **extend_kwargs)\n            revised_impact_forecasts = np.concatenate((revised_impact_forecasts, p.forecasts), axis=1)\n        revision_impacts = (revised_impact_forecasts - prev_impacted_forecasts).T\n        if t is not None:\n            revision_impacts = revision_impacts[0]\n    if len(revisions_ix) > 0:\n        (revised_j, revised_p) = zip(*revisions_ix)\n        ix_j = revised_j - revised_j[0]\n        revisions_all = revisions_all[ix_j, revised_p]\n        revised_all = revised_all[ix_j, revised_p]\n        revised_prev_all = revised_prev_all[ix_j, revised_p]\n    if len(updates_ix) > 0:\n        (update_t, update_k) = zip(*updates_ix)\n        update_start_t = np.min(update_t)\n        update_end_t = np.max(update_t)\n        if revision_results is None:\n            forecasts = previous.predict(start=update_start_t, end=update_end_t + 1, **extend_kwargs).smoothed_forecasts.T\n        else:\n            forecasts = revision_results.predict(start=update_start_t, end=update_end_t + 1).smoothed_forecasts.T\n        realized = self.endog.T[update_start_t:update_end_t + 1]\n        forecasts_error = realized - forecasts\n        ix_t = update_t - update_start_t\n        update_realized = realized[ix_t, update_k]\n        update_forecasts = forecasts[ix_t, update_k]\n        update_forecasts_error = forecasts_error[ix_t, update_k]\n        if self.design.shape[2] == 1:\n            design = self.design[..., 0][None, ...]\n        elif end <= self.nobs:\n            design = self.design[..., start:end].transpose(2, 0, 1)\n        elif design is None:\n            raise ValueError('Model has time-varying design matrix, so an updated time-varying matrix for period `t` is required.')\n        elif design.ndim == 2:\n            design = design[None, ...]\n        else:\n            design = design.transpose(2, 0, 1)\n        state_gain = previous.smoothed_state_gain(updates_ix, start=start, end=end, extend_kwargs=extend_kwargs)\n        if state_index is not None:\n            design = design[:, :, state_index]\n            state_gain = state_gain[:, state_index]\n        obs_gain = design @ state_gain\n        update_impacts = obs_gain @ update_forecasts_error\n        if t is not None:\n            obs_gain = obs_gain[0]\n            update_impacts = update_impacts[0]\n    else:\n        update_impacts = None\n        update_forecasts = None\n        update_realized = None\n        update_forecasts_error = None\n        obs_gain = None\n    out = SimpleNamespace(update_impacts=update_impacts, revision_detailed_impacts=revision_detailed_impacts, news=update_forecasts_error, revisions=revisions, revisions_all=revisions_all, gain=obs_gain, revision_weights=revision_weights, update_forecasts=update_forecasts, update_realized=update_realized, revised=revised, revised_all=revised_all, revised_prev=revised_prev, revised_prev_all=revised_prev_all, prev_impacted_forecasts=prev_impacted_forecasts, post_impacted_forecasts=post_impacted_forecasts, revision_results=revision_results, revision_impacts=revision_impacts, revisions_ix=revisions_ix, revisions_details=revisions_details, revisions_grouped=revisions_grouped, revisions_details_start=revisions_details_start, updates_ix=updates_ix, state_index=state_index)\n    return out"
        ]
    },
    {
        "func_name": "get_mat",
        "original": "def get_mat(which, t):\n    mat = getattr(self, which)\n    if mat.shape[-1] > 1:\n        if t < self.nobs:\n            out = mat[..., t]\n        else:\n            if which not in extend_kwargs or extend_kwargs[which].shape[-1] <= t - self.nobs:\n                raise ValueError(f'Model has time-varying {which} matrix, so an updated time-varying matrix for the extension period is required.')\n            out = extend_kwargs[which][..., t - self.nobs]\n    else:\n        out = mat[..., 0]\n    return out",
        "mutated": [
            "def get_mat(which, t):\n    if False:\n        i = 10\n    mat = getattr(self, which)\n    if mat.shape[-1] > 1:\n        if t < self.nobs:\n            out = mat[..., t]\n        else:\n            if which not in extend_kwargs or extend_kwargs[which].shape[-1] <= t - self.nobs:\n                raise ValueError(f'Model has time-varying {which} matrix, so an updated time-varying matrix for the extension period is required.')\n            out = extend_kwargs[which][..., t - self.nobs]\n    else:\n        out = mat[..., 0]\n    return out",
            "def get_mat(which, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    mat = getattr(self, which)\n    if mat.shape[-1] > 1:\n        if t < self.nobs:\n            out = mat[..., t]\n        else:\n            if which not in extend_kwargs or extend_kwargs[which].shape[-1] <= t - self.nobs:\n                raise ValueError(f'Model has time-varying {which} matrix, so an updated time-varying matrix for the extension period is required.')\n            out = extend_kwargs[which][..., t - self.nobs]\n    else:\n        out = mat[..., 0]\n    return out",
            "def get_mat(which, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    mat = getattr(self, which)\n    if mat.shape[-1] > 1:\n        if t < self.nobs:\n            out = mat[..., t]\n        else:\n            if which not in extend_kwargs or extend_kwargs[which].shape[-1] <= t - self.nobs:\n                raise ValueError(f'Model has time-varying {which} matrix, so an updated time-varying matrix for the extension period is required.')\n            out = extend_kwargs[which][..., t - self.nobs]\n    else:\n        out = mat[..., 0]\n    return out",
            "def get_mat(which, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    mat = getattr(self, which)\n    if mat.shape[-1] > 1:\n        if t < self.nobs:\n            out = mat[..., t]\n        else:\n            if which not in extend_kwargs or extend_kwargs[which].shape[-1] <= t - self.nobs:\n                raise ValueError(f'Model has time-varying {which} matrix, so an updated time-varying matrix for the extension period is required.')\n            out = extend_kwargs[which][..., t - self.nobs]\n    else:\n        out = mat[..., 0]\n    return out",
            "def get_mat(which, t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    mat = getattr(self, which)\n    if mat.shape[-1] > 1:\n        if t < self.nobs:\n            out = mat[..., t]\n        else:\n            if which not in extend_kwargs or extend_kwargs[which].shape[-1] <= t - self.nobs:\n                raise ValueError(f'Model has time-varying {which} matrix, so an updated time-varying matrix for the extension period is required.')\n            out = extend_kwargs[which][..., t - self.nobs]\n    else:\n        out = mat[..., 0]\n    return out"
        ]
    },
    {
        "func_name": "get_cov_state_revision",
        "original": "def get_cov_state_revision(t):\n    tmp1 = np.zeros((self.k_states, n_updates))\n    for i in range(n_updates):\n        (t_i, k_i) = updates_ix[i]\n        acov = self.smoothed_state_autocovariance(lag=t - t_i, t=t, extend_kwargs=extend_kwargs)\n        Z_i = get_mat('design', t_i)\n        tmp1[:, i:i + 1] = acov @ Z_i[k_i:k_i + 1].T\n    return tmp1",
        "mutated": [
            "def get_cov_state_revision(t):\n    if False:\n        i = 10\n    tmp1 = np.zeros((self.k_states, n_updates))\n    for i in range(n_updates):\n        (t_i, k_i) = updates_ix[i]\n        acov = self.smoothed_state_autocovariance(lag=t - t_i, t=t, extend_kwargs=extend_kwargs)\n        Z_i = get_mat('design', t_i)\n        tmp1[:, i:i + 1] = acov @ Z_i[k_i:k_i + 1].T\n    return tmp1",
            "def get_cov_state_revision(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tmp1 = np.zeros((self.k_states, n_updates))\n    for i in range(n_updates):\n        (t_i, k_i) = updates_ix[i]\n        acov = self.smoothed_state_autocovariance(lag=t - t_i, t=t, extend_kwargs=extend_kwargs)\n        Z_i = get_mat('design', t_i)\n        tmp1[:, i:i + 1] = acov @ Z_i[k_i:k_i + 1].T\n    return tmp1",
            "def get_cov_state_revision(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tmp1 = np.zeros((self.k_states, n_updates))\n    for i in range(n_updates):\n        (t_i, k_i) = updates_ix[i]\n        acov = self.smoothed_state_autocovariance(lag=t - t_i, t=t, extend_kwargs=extend_kwargs)\n        Z_i = get_mat('design', t_i)\n        tmp1[:, i:i + 1] = acov @ Z_i[k_i:k_i + 1].T\n    return tmp1",
            "def get_cov_state_revision(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tmp1 = np.zeros((self.k_states, n_updates))\n    for i in range(n_updates):\n        (t_i, k_i) = updates_ix[i]\n        acov = self.smoothed_state_autocovariance(lag=t - t_i, t=t, extend_kwargs=extend_kwargs)\n        Z_i = get_mat('design', t_i)\n        tmp1[:, i:i + 1] = acov @ Z_i[k_i:k_i + 1].T\n    return tmp1",
            "def get_cov_state_revision(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tmp1 = np.zeros((self.k_states, n_updates))\n    for i in range(n_updates):\n        (t_i, k_i) = updates_ix[i]\n        acov = self.smoothed_state_autocovariance(lag=t - t_i, t=t, extend_kwargs=extend_kwargs)\n        Z_i = get_mat('design', t_i)\n        tmp1[:, i:i + 1] = acov @ Z_i[k_i:k_i + 1].T\n    return tmp1"
        ]
    },
    {
        "func_name": "smoothed_state_gain",
        "original": "def smoothed_state_gain(self, updates_ix, t=None, start=None, end=None, extend_kwargs=None):\n    \"\"\"\n        Cov(\\\\tilde \\\\alpha_{t}, I) Var(I, I)^{-1}\n\n        where I is a vector of forecast errors associated with\n        `update_indices`.\n\n        Parameters\n        ----------\n        updates_ix : list\n            List of indices `(t, i)`, where `t` denotes a zero-indexed time\n            location and `i` denotes a zero-indexed endog variable.\n        \"\"\"\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = self.nobs - 1\n    if end is None:\n        end = self.nobs\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end <= start:\n        raise ValueError('`end` must be after `start`')\n    n_periods = end - start\n    n_updates = len(updates_ix)\n\n    def get_mat(which, t):\n        mat = getattr(self, which)\n        if mat.shape[-1] > 1:\n            if t < self.nobs:\n                out = mat[..., t]\n            else:\n                if which not in extend_kwargs or extend_kwargs[which].shape[-1] <= t - self.nobs:\n                    raise ValueError(f'Model has time-varying {which} matrix, so an updated time-varying matrix for the extension period is required.')\n                out = extend_kwargs[which][..., t - self.nobs]\n        else:\n            out = mat[..., 0]\n        return out\n\n    def get_cov_state_revision(t):\n        tmp1 = np.zeros((self.k_states, n_updates))\n        for i in range(n_updates):\n            (t_i, k_i) = updates_ix[i]\n            acov = self.smoothed_state_autocovariance(lag=t - t_i, t=t, extend_kwargs=extend_kwargs)\n            Z_i = get_mat('design', t_i)\n            tmp1[:, i:i + 1] = acov @ Z_i[k_i:k_i + 1].T\n        return tmp1\n    tmp1 = np.zeros((n_periods, self.k_states, n_updates))\n    for s in range(start, end):\n        tmp1[s - start] = get_cov_state_revision(s)\n    tmp2 = np.zeros((n_updates, n_updates))\n    for i in range(n_updates):\n        (t_i, k_i) = updates_ix[i]\n        for j in range(i + 1):\n            (t_j, k_j) = updates_ix[j]\n            Z_i = get_mat('design', t_i)\n            Z_j = get_mat('design', t_j)\n            acov = self.smoothed_state_autocovariance(lag=t_i - t_j, t=t_i, extend_kwargs=extend_kwargs)\n            tmp2[i, j] = tmp2[j, i] = np.squeeze(Z_i[k_i:k_i + 1] @ acov @ Z_j[k_j:k_j + 1].T)\n            if t_i == t_j:\n                H = get_mat('obs_cov', t_i)\n                if i == j:\n                    tmp2[i, j] += H[k_i, k_j]\n                else:\n                    tmp2[i, j] += H[k_i, k_j]\n                    tmp2[j, i] += H[k_i, k_j]\n    gain = tmp1 @ np.linalg.inv(tmp2)\n    if t is not None:\n        gain = gain[0]\n    return gain",
        "mutated": [
            "def smoothed_state_gain(self, updates_ix, t=None, start=None, end=None, extend_kwargs=None):\n    if False:\n        i = 10\n    '\\n        Cov(\\\\tilde \\\\alpha_{t}, I) Var(I, I)^{-1}\\n\\n        where I is a vector of forecast errors associated with\\n        `update_indices`.\\n\\n        Parameters\\n        ----------\\n        updates_ix : list\\n            List of indices `(t, i)`, where `t` denotes a zero-indexed time\\n            location and `i` denotes a zero-indexed endog variable.\\n        '\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = self.nobs - 1\n    if end is None:\n        end = self.nobs\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end <= start:\n        raise ValueError('`end` must be after `start`')\n    n_periods = end - start\n    n_updates = len(updates_ix)\n\n    def get_mat(which, t):\n        mat = getattr(self, which)\n        if mat.shape[-1] > 1:\n            if t < self.nobs:\n                out = mat[..., t]\n            else:\n                if which not in extend_kwargs or extend_kwargs[which].shape[-1] <= t - self.nobs:\n                    raise ValueError(f'Model has time-varying {which} matrix, so an updated time-varying matrix for the extension period is required.')\n                out = extend_kwargs[which][..., t - self.nobs]\n        else:\n            out = mat[..., 0]\n        return out\n\n    def get_cov_state_revision(t):\n        tmp1 = np.zeros((self.k_states, n_updates))\n        for i in range(n_updates):\n            (t_i, k_i) = updates_ix[i]\n            acov = self.smoothed_state_autocovariance(lag=t - t_i, t=t, extend_kwargs=extend_kwargs)\n            Z_i = get_mat('design', t_i)\n            tmp1[:, i:i + 1] = acov @ Z_i[k_i:k_i + 1].T\n        return tmp1\n    tmp1 = np.zeros((n_periods, self.k_states, n_updates))\n    for s in range(start, end):\n        tmp1[s - start] = get_cov_state_revision(s)\n    tmp2 = np.zeros((n_updates, n_updates))\n    for i in range(n_updates):\n        (t_i, k_i) = updates_ix[i]\n        for j in range(i + 1):\n            (t_j, k_j) = updates_ix[j]\n            Z_i = get_mat('design', t_i)\n            Z_j = get_mat('design', t_j)\n            acov = self.smoothed_state_autocovariance(lag=t_i - t_j, t=t_i, extend_kwargs=extend_kwargs)\n            tmp2[i, j] = tmp2[j, i] = np.squeeze(Z_i[k_i:k_i + 1] @ acov @ Z_j[k_j:k_j + 1].T)\n            if t_i == t_j:\n                H = get_mat('obs_cov', t_i)\n                if i == j:\n                    tmp2[i, j] += H[k_i, k_j]\n                else:\n                    tmp2[i, j] += H[k_i, k_j]\n                    tmp2[j, i] += H[k_i, k_j]\n    gain = tmp1 @ np.linalg.inv(tmp2)\n    if t is not None:\n        gain = gain[0]\n    return gain",
            "def smoothed_state_gain(self, updates_ix, t=None, start=None, end=None, extend_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Cov(\\\\tilde \\\\alpha_{t}, I) Var(I, I)^{-1}\\n\\n        where I is a vector of forecast errors associated with\\n        `update_indices`.\\n\\n        Parameters\\n        ----------\\n        updates_ix : list\\n            List of indices `(t, i)`, where `t` denotes a zero-indexed time\\n            location and `i` denotes a zero-indexed endog variable.\\n        '\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = self.nobs - 1\n    if end is None:\n        end = self.nobs\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end <= start:\n        raise ValueError('`end` must be after `start`')\n    n_periods = end - start\n    n_updates = len(updates_ix)\n\n    def get_mat(which, t):\n        mat = getattr(self, which)\n        if mat.shape[-1] > 1:\n            if t < self.nobs:\n                out = mat[..., t]\n            else:\n                if which not in extend_kwargs or extend_kwargs[which].shape[-1] <= t - self.nobs:\n                    raise ValueError(f'Model has time-varying {which} matrix, so an updated time-varying matrix for the extension period is required.')\n                out = extend_kwargs[which][..., t - self.nobs]\n        else:\n            out = mat[..., 0]\n        return out\n\n    def get_cov_state_revision(t):\n        tmp1 = np.zeros((self.k_states, n_updates))\n        for i in range(n_updates):\n            (t_i, k_i) = updates_ix[i]\n            acov = self.smoothed_state_autocovariance(lag=t - t_i, t=t, extend_kwargs=extend_kwargs)\n            Z_i = get_mat('design', t_i)\n            tmp1[:, i:i + 1] = acov @ Z_i[k_i:k_i + 1].T\n        return tmp1\n    tmp1 = np.zeros((n_periods, self.k_states, n_updates))\n    for s in range(start, end):\n        tmp1[s - start] = get_cov_state_revision(s)\n    tmp2 = np.zeros((n_updates, n_updates))\n    for i in range(n_updates):\n        (t_i, k_i) = updates_ix[i]\n        for j in range(i + 1):\n            (t_j, k_j) = updates_ix[j]\n            Z_i = get_mat('design', t_i)\n            Z_j = get_mat('design', t_j)\n            acov = self.smoothed_state_autocovariance(lag=t_i - t_j, t=t_i, extend_kwargs=extend_kwargs)\n            tmp2[i, j] = tmp2[j, i] = np.squeeze(Z_i[k_i:k_i + 1] @ acov @ Z_j[k_j:k_j + 1].T)\n            if t_i == t_j:\n                H = get_mat('obs_cov', t_i)\n                if i == j:\n                    tmp2[i, j] += H[k_i, k_j]\n                else:\n                    tmp2[i, j] += H[k_i, k_j]\n                    tmp2[j, i] += H[k_i, k_j]\n    gain = tmp1 @ np.linalg.inv(tmp2)\n    if t is not None:\n        gain = gain[0]\n    return gain",
            "def smoothed_state_gain(self, updates_ix, t=None, start=None, end=None, extend_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Cov(\\\\tilde \\\\alpha_{t}, I) Var(I, I)^{-1}\\n\\n        where I is a vector of forecast errors associated with\\n        `update_indices`.\\n\\n        Parameters\\n        ----------\\n        updates_ix : list\\n            List of indices `(t, i)`, where `t` denotes a zero-indexed time\\n            location and `i` denotes a zero-indexed endog variable.\\n        '\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = self.nobs - 1\n    if end is None:\n        end = self.nobs\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end <= start:\n        raise ValueError('`end` must be after `start`')\n    n_periods = end - start\n    n_updates = len(updates_ix)\n\n    def get_mat(which, t):\n        mat = getattr(self, which)\n        if mat.shape[-1] > 1:\n            if t < self.nobs:\n                out = mat[..., t]\n            else:\n                if which not in extend_kwargs or extend_kwargs[which].shape[-1] <= t - self.nobs:\n                    raise ValueError(f'Model has time-varying {which} matrix, so an updated time-varying matrix for the extension period is required.')\n                out = extend_kwargs[which][..., t - self.nobs]\n        else:\n            out = mat[..., 0]\n        return out\n\n    def get_cov_state_revision(t):\n        tmp1 = np.zeros((self.k_states, n_updates))\n        for i in range(n_updates):\n            (t_i, k_i) = updates_ix[i]\n            acov = self.smoothed_state_autocovariance(lag=t - t_i, t=t, extend_kwargs=extend_kwargs)\n            Z_i = get_mat('design', t_i)\n            tmp1[:, i:i + 1] = acov @ Z_i[k_i:k_i + 1].T\n        return tmp1\n    tmp1 = np.zeros((n_periods, self.k_states, n_updates))\n    for s in range(start, end):\n        tmp1[s - start] = get_cov_state_revision(s)\n    tmp2 = np.zeros((n_updates, n_updates))\n    for i in range(n_updates):\n        (t_i, k_i) = updates_ix[i]\n        for j in range(i + 1):\n            (t_j, k_j) = updates_ix[j]\n            Z_i = get_mat('design', t_i)\n            Z_j = get_mat('design', t_j)\n            acov = self.smoothed_state_autocovariance(lag=t_i - t_j, t=t_i, extend_kwargs=extend_kwargs)\n            tmp2[i, j] = tmp2[j, i] = np.squeeze(Z_i[k_i:k_i + 1] @ acov @ Z_j[k_j:k_j + 1].T)\n            if t_i == t_j:\n                H = get_mat('obs_cov', t_i)\n                if i == j:\n                    tmp2[i, j] += H[k_i, k_j]\n                else:\n                    tmp2[i, j] += H[k_i, k_j]\n                    tmp2[j, i] += H[k_i, k_j]\n    gain = tmp1 @ np.linalg.inv(tmp2)\n    if t is not None:\n        gain = gain[0]\n    return gain",
            "def smoothed_state_gain(self, updates_ix, t=None, start=None, end=None, extend_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Cov(\\\\tilde \\\\alpha_{t}, I) Var(I, I)^{-1}\\n\\n        where I is a vector of forecast errors associated with\\n        `update_indices`.\\n\\n        Parameters\\n        ----------\\n        updates_ix : list\\n            List of indices `(t, i)`, where `t` denotes a zero-indexed time\\n            location and `i` denotes a zero-indexed endog variable.\\n        '\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = self.nobs - 1\n    if end is None:\n        end = self.nobs\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end <= start:\n        raise ValueError('`end` must be after `start`')\n    n_periods = end - start\n    n_updates = len(updates_ix)\n\n    def get_mat(which, t):\n        mat = getattr(self, which)\n        if mat.shape[-1] > 1:\n            if t < self.nobs:\n                out = mat[..., t]\n            else:\n                if which not in extend_kwargs or extend_kwargs[which].shape[-1] <= t - self.nobs:\n                    raise ValueError(f'Model has time-varying {which} matrix, so an updated time-varying matrix for the extension period is required.')\n                out = extend_kwargs[which][..., t - self.nobs]\n        else:\n            out = mat[..., 0]\n        return out\n\n    def get_cov_state_revision(t):\n        tmp1 = np.zeros((self.k_states, n_updates))\n        for i in range(n_updates):\n            (t_i, k_i) = updates_ix[i]\n            acov = self.smoothed_state_autocovariance(lag=t - t_i, t=t, extend_kwargs=extend_kwargs)\n            Z_i = get_mat('design', t_i)\n            tmp1[:, i:i + 1] = acov @ Z_i[k_i:k_i + 1].T\n        return tmp1\n    tmp1 = np.zeros((n_periods, self.k_states, n_updates))\n    for s in range(start, end):\n        tmp1[s - start] = get_cov_state_revision(s)\n    tmp2 = np.zeros((n_updates, n_updates))\n    for i in range(n_updates):\n        (t_i, k_i) = updates_ix[i]\n        for j in range(i + 1):\n            (t_j, k_j) = updates_ix[j]\n            Z_i = get_mat('design', t_i)\n            Z_j = get_mat('design', t_j)\n            acov = self.smoothed_state_autocovariance(lag=t_i - t_j, t=t_i, extend_kwargs=extend_kwargs)\n            tmp2[i, j] = tmp2[j, i] = np.squeeze(Z_i[k_i:k_i + 1] @ acov @ Z_j[k_j:k_j + 1].T)\n            if t_i == t_j:\n                H = get_mat('obs_cov', t_i)\n                if i == j:\n                    tmp2[i, j] += H[k_i, k_j]\n                else:\n                    tmp2[i, j] += H[k_i, k_j]\n                    tmp2[j, i] += H[k_i, k_j]\n    gain = tmp1 @ np.linalg.inv(tmp2)\n    if t is not None:\n        gain = gain[0]\n    return gain",
            "def smoothed_state_gain(self, updates_ix, t=None, start=None, end=None, extend_kwargs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Cov(\\\\tilde \\\\alpha_{t}, I) Var(I, I)^{-1}\\n\\n        where I is a vector of forecast errors associated with\\n        `update_indices`.\\n\\n        Parameters\\n        ----------\\n        updates_ix : list\\n            List of indices `(t, i)`, where `t` denotes a zero-indexed time\\n            location and `i` denotes a zero-indexed endog variable.\\n        '\n    if t is not None and (start is not None or end is not None):\n        raise ValueError('Cannot specify both `t` and `start` or `end`.')\n    if t is not None:\n        start = t\n        end = t + 1\n    if start is None:\n        start = self.nobs - 1\n    if end is None:\n        end = self.nobs\n    if extend_kwargs is None:\n        extend_kwargs = {}\n    if start < 0 or end < 0:\n        raise ValueError('Negative `t`, `start`, or `end` is not allowed.')\n    if end <= start:\n        raise ValueError('`end` must be after `start`')\n    n_periods = end - start\n    n_updates = len(updates_ix)\n\n    def get_mat(which, t):\n        mat = getattr(self, which)\n        if mat.shape[-1] > 1:\n            if t < self.nobs:\n                out = mat[..., t]\n            else:\n                if which not in extend_kwargs or extend_kwargs[which].shape[-1] <= t - self.nobs:\n                    raise ValueError(f'Model has time-varying {which} matrix, so an updated time-varying matrix for the extension period is required.')\n                out = extend_kwargs[which][..., t - self.nobs]\n        else:\n            out = mat[..., 0]\n        return out\n\n    def get_cov_state_revision(t):\n        tmp1 = np.zeros((self.k_states, n_updates))\n        for i in range(n_updates):\n            (t_i, k_i) = updates_ix[i]\n            acov = self.smoothed_state_autocovariance(lag=t - t_i, t=t, extend_kwargs=extend_kwargs)\n            Z_i = get_mat('design', t_i)\n            tmp1[:, i:i + 1] = acov @ Z_i[k_i:k_i + 1].T\n        return tmp1\n    tmp1 = np.zeros((n_periods, self.k_states, n_updates))\n    for s in range(start, end):\n        tmp1[s - start] = get_cov_state_revision(s)\n    tmp2 = np.zeros((n_updates, n_updates))\n    for i in range(n_updates):\n        (t_i, k_i) = updates_ix[i]\n        for j in range(i + 1):\n            (t_j, k_j) = updates_ix[j]\n            Z_i = get_mat('design', t_i)\n            Z_j = get_mat('design', t_j)\n            acov = self.smoothed_state_autocovariance(lag=t_i - t_j, t=t_i, extend_kwargs=extend_kwargs)\n            tmp2[i, j] = tmp2[j, i] = np.squeeze(Z_i[k_i:k_i + 1] @ acov @ Z_j[k_j:k_j + 1].T)\n            if t_i == t_j:\n                H = get_mat('obs_cov', t_i)\n                if i == j:\n                    tmp2[i, j] += H[k_i, k_j]\n                else:\n                    tmp2[i, j] += H[k_i, k_j]\n                    tmp2[j, i] += H[k_i, k_j]\n    gain = tmp1 @ np.linalg.inv(tmp2)\n    if t is not None:\n        gain = gain[0]\n    return gain"
        ]
    },
    {
        "func_name": "_get_smoothed_forecasts",
        "original": "def _get_smoothed_forecasts(self):\n    if self._smoothed_forecasts is None:\n        self._smoothed_forecasts = np.zeros(self.forecasts.shape, dtype=self.dtype)\n        self._smoothed_forecasts_error = np.zeros(self.forecasts_error.shape, dtype=self.dtype)\n        self._smoothed_forecasts_error_cov = np.zeros(self.forecasts_error_cov.shape, dtype=self.dtype)\n        for t in range(self.nobs):\n            design_t = 0 if self.design.shape[2] == 1 else t\n            obs_cov_t = 0 if self.obs_cov.shape[2] == 1 else t\n            obs_intercept_t = 0 if self.obs_intercept.shape[1] == 1 else t\n            mask = ~self.missing[:, t].astype(bool)\n            self._smoothed_forecasts[:, t] = np.dot(self.design[:, :, design_t], self.smoothed_state[:, t]) + self.obs_intercept[:, obs_intercept_t]\n            if self.nmissing[t] > 0:\n                self._smoothed_forecasts_error[:, t] = np.nan\n            self._smoothed_forecasts_error[mask, t] = self.endog[mask, t] - self._smoothed_forecasts[mask, t]\n            self._smoothed_forecasts_error_cov[:, :, t] = np.dot(np.dot(self.design[:, :, design_t], self.smoothed_state_cov[:, :, t]), self.design[:, :, design_t].T) + self.obs_cov[:, :, obs_cov_t]\n    return (self._smoothed_forecasts, self._smoothed_forecasts_error, self._smoothed_forecasts_error_cov)",
        "mutated": [
            "def _get_smoothed_forecasts(self):\n    if False:\n        i = 10\n    if self._smoothed_forecasts is None:\n        self._smoothed_forecasts = np.zeros(self.forecasts.shape, dtype=self.dtype)\n        self._smoothed_forecasts_error = np.zeros(self.forecasts_error.shape, dtype=self.dtype)\n        self._smoothed_forecasts_error_cov = np.zeros(self.forecasts_error_cov.shape, dtype=self.dtype)\n        for t in range(self.nobs):\n            design_t = 0 if self.design.shape[2] == 1 else t\n            obs_cov_t = 0 if self.obs_cov.shape[2] == 1 else t\n            obs_intercept_t = 0 if self.obs_intercept.shape[1] == 1 else t\n            mask = ~self.missing[:, t].astype(bool)\n            self._smoothed_forecasts[:, t] = np.dot(self.design[:, :, design_t], self.smoothed_state[:, t]) + self.obs_intercept[:, obs_intercept_t]\n            if self.nmissing[t] > 0:\n                self._smoothed_forecasts_error[:, t] = np.nan\n            self._smoothed_forecasts_error[mask, t] = self.endog[mask, t] - self._smoothed_forecasts[mask, t]\n            self._smoothed_forecasts_error_cov[:, :, t] = np.dot(np.dot(self.design[:, :, design_t], self.smoothed_state_cov[:, :, t]), self.design[:, :, design_t].T) + self.obs_cov[:, :, obs_cov_t]\n    return (self._smoothed_forecasts, self._smoothed_forecasts_error, self._smoothed_forecasts_error_cov)",
            "def _get_smoothed_forecasts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if self._smoothed_forecasts is None:\n        self._smoothed_forecasts = np.zeros(self.forecasts.shape, dtype=self.dtype)\n        self._smoothed_forecasts_error = np.zeros(self.forecasts_error.shape, dtype=self.dtype)\n        self._smoothed_forecasts_error_cov = np.zeros(self.forecasts_error_cov.shape, dtype=self.dtype)\n        for t in range(self.nobs):\n            design_t = 0 if self.design.shape[2] == 1 else t\n            obs_cov_t = 0 if self.obs_cov.shape[2] == 1 else t\n            obs_intercept_t = 0 if self.obs_intercept.shape[1] == 1 else t\n            mask = ~self.missing[:, t].astype(bool)\n            self._smoothed_forecasts[:, t] = np.dot(self.design[:, :, design_t], self.smoothed_state[:, t]) + self.obs_intercept[:, obs_intercept_t]\n            if self.nmissing[t] > 0:\n                self._smoothed_forecasts_error[:, t] = np.nan\n            self._smoothed_forecasts_error[mask, t] = self.endog[mask, t] - self._smoothed_forecasts[mask, t]\n            self._smoothed_forecasts_error_cov[:, :, t] = np.dot(np.dot(self.design[:, :, design_t], self.smoothed_state_cov[:, :, t]), self.design[:, :, design_t].T) + self.obs_cov[:, :, obs_cov_t]\n    return (self._smoothed_forecasts, self._smoothed_forecasts_error, self._smoothed_forecasts_error_cov)",
            "def _get_smoothed_forecasts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if self._smoothed_forecasts is None:\n        self._smoothed_forecasts = np.zeros(self.forecasts.shape, dtype=self.dtype)\n        self._smoothed_forecasts_error = np.zeros(self.forecasts_error.shape, dtype=self.dtype)\n        self._smoothed_forecasts_error_cov = np.zeros(self.forecasts_error_cov.shape, dtype=self.dtype)\n        for t in range(self.nobs):\n            design_t = 0 if self.design.shape[2] == 1 else t\n            obs_cov_t = 0 if self.obs_cov.shape[2] == 1 else t\n            obs_intercept_t = 0 if self.obs_intercept.shape[1] == 1 else t\n            mask = ~self.missing[:, t].astype(bool)\n            self._smoothed_forecasts[:, t] = np.dot(self.design[:, :, design_t], self.smoothed_state[:, t]) + self.obs_intercept[:, obs_intercept_t]\n            if self.nmissing[t] > 0:\n                self._smoothed_forecasts_error[:, t] = np.nan\n            self._smoothed_forecasts_error[mask, t] = self.endog[mask, t] - self._smoothed_forecasts[mask, t]\n            self._smoothed_forecasts_error_cov[:, :, t] = np.dot(np.dot(self.design[:, :, design_t], self.smoothed_state_cov[:, :, t]), self.design[:, :, design_t].T) + self.obs_cov[:, :, obs_cov_t]\n    return (self._smoothed_forecasts, self._smoothed_forecasts_error, self._smoothed_forecasts_error_cov)",
            "def _get_smoothed_forecasts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if self._smoothed_forecasts is None:\n        self._smoothed_forecasts = np.zeros(self.forecasts.shape, dtype=self.dtype)\n        self._smoothed_forecasts_error = np.zeros(self.forecasts_error.shape, dtype=self.dtype)\n        self._smoothed_forecasts_error_cov = np.zeros(self.forecasts_error_cov.shape, dtype=self.dtype)\n        for t in range(self.nobs):\n            design_t = 0 if self.design.shape[2] == 1 else t\n            obs_cov_t = 0 if self.obs_cov.shape[2] == 1 else t\n            obs_intercept_t = 0 if self.obs_intercept.shape[1] == 1 else t\n            mask = ~self.missing[:, t].astype(bool)\n            self._smoothed_forecasts[:, t] = np.dot(self.design[:, :, design_t], self.smoothed_state[:, t]) + self.obs_intercept[:, obs_intercept_t]\n            if self.nmissing[t] > 0:\n                self._smoothed_forecasts_error[:, t] = np.nan\n            self._smoothed_forecasts_error[mask, t] = self.endog[mask, t] - self._smoothed_forecasts[mask, t]\n            self._smoothed_forecasts_error_cov[:, :, t] = np.dot(np.dot(self.design[:, :, design_t], self.smoothed_state_cov[:, :, t]), self.design[:, :, design_t].T) + self.obs_cov[:, :, obs_cov_t]\n    return (self._smoothed_forecasts, self._smoothed_forecasts_error, self._smoothed_forecasts_error_cov)",
            "def _get_smoothed_forecasts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if self._smoothed_forecasts is None:\n        self._smoothed_forecasts = np.zeros(self.forecasts.shape, dtype=self.dtype)\n        self._smoothed_forecasts_error = np.zeros(self.forecasts_error.shape, dtype=self.dtype)\n        self._smoothed_forecasts_error_cov = np.zeros(self.forecasts_error_cov.shape, dtype=self.dtype)\n        for t in range(self.nobs):\n            design_t = 0 if self.design.shape[2] == 1 else t\n            obs_cov_t = 0 if self.obs_cov.shape[2] == 1 else t\n            obs_intercept_t = 0 if self.obs_intercept.shape[1] == 1 else t\n            mask = ~self.missing[:, t].astype(bool)\n            self._smoothed_forecasts[:, t] = np.dot(self.design[:, :, design_t], self.smoothed_state[:, t]) + self.obs_intercept[:, obs_intercept_t]\n            if self.nmissing[t] > 0:\n                self._smoothed_forecasts_error[:, t] = np.nan\n            self._smoothed_forecasts_error[mask, t] = self.endog[mask, t] - self._smoothed_forecasts[mask, t]\n            self._smoothed_forecasts_error_cov[:, :, t] = np.dot(np.dot(self.design[:, :, design_t], self.smoothed_state_cov[:, :, t]), self.design[:, :, design_t].T) + self.obs_cov[:, :, obs_cov_t]\n    return (self._smoothed_forecasts, self._smoothed_forecasts_error, self._smoothed_forecasts_error_cov)"
        ]
    },
    {
        "func_name": "smoothed_forecasts",
        "original": "@property\ndef smoothed_forecasts(self):\n    return self._get_smoothed_forecasts()[0]",
        "mutated": [
            "@property\ndef smoothed_forecasts(self):\n    if False:\n        i = 10\n    return self._get_smoothed_forecasts()[0]",
            "@property\ndef smoothed_forecasts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self._get_smoothed_forecasts()[0]",
            "@property\ndef smoothed_forecasts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self._get_smoothed_forecasts()[0]",
            "@property\ndef smoothed_forecasts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self._get_smoothed_forecasts()[0]",
            "@property\ndef smoothed_forecasts(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self._get_smoothed_forecasts()[0]"
        ]
    },
    {
        "func_name": "smoothed_forecasts_error",
        "original": "@property\ndef smoothed_forecasts_error(self):\n    return self._get_smoothed_forecasts()[1]",
        "mutated": [
            "@property\ndef smoothed_forecasts_error(self):\n    if False:\n        i = 10\n    return self._get_smoothed_forecasts()[1]",
            "@property\ndef smoothed_forecasts_error(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self._get_smoothed_forecasts()[1]",
            "@property\ndef smoothed_forecasts_error(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self._get_smoothed_forecasts()[1]",
            "@property\ndef smoothed_forecasts_error(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self._get_smoothed_forecasts()[1]",
            "@property\ndef smoothed_forecasts_error(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self._get_smoothed_forecasts()[1]"
        ]
    },
    {
        "func_name": "smoothed_forecasts_error_cov",
        "original": "@property\ndef smoothed_forecasts_error_cov(self):\n    return self._get_smoothed_forecasts()[2]",
        "mutated": [
            "@property\ndef smoothed_forecasts_error_cov(self):\n    if False:\n        i = 10\n    return self._get_smoothed_forecasts()[2]",
            "@property\ndef smoothed_forecasts_error_cov(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self._get_smoothed_forecasts()[2]",
            "@property\ndef smoothed_forecasts_error_cov(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self._get_smoothed_forecasts()[2]",
            "@property\ndef smoothed_forecasts_error_cov(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self._get_smoothed_forecasts()[2]",
            "@property\ndef smoothed_forecasts_error_cov(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self._get_smoothed_forecasts()[2]"
        ]
    },
    {
        "func_name": "get_smoothed_decomposition",
        "original": "def get_smoothed_decomposition(self, decomposition_of='smoothed_state', state_index=None):\n    \"\"\"\n        Decompose smoothed output into contributions from observations\n\n        Parameters\n        ----------\n        decomposition_of : {\"smoothed_state\", \"smoothed_signal\"}\n            The object to perform a decomposition of. If it is set to\n            \"smoothed_state\", then the elements of the smoothed state vector\n            are decomposed into the contributions of each observation. If it\n            is set to \"smoothed_signal\", then the predictions of the\n            observation vector based on the smoothed state vector are\n            decomposed. Default is \"smoothed_state\".\n        state_index : array_like, optional\n            An optional index specifying a subset of states to use when\n            constructing the decomposition of the \"smoothed_signal\". For\n            example, if `state_index=[0, 1]` is passed, then only the\n            contributions of observed variables to the smoothed signal arising\n            from the first two states will be returned. Note that if not all\n            states are used, the contributions will not sum to the smoothed\n            signal. Default is to use all states.\n\n        Returns\n        -------\n        data_contributions : array\n            Contributions of observations to the decomposed object. If the\n            smoothed state is being decomposed, then `data_contributions` are\n            shaped `(nobs, k_states, nobs, k_endog)`, where the\n            `(t, m, j, p)`-th element is the contribution of the `p`-th\n            observation at time `j` to the `m`-th state at time `t`. If the\n            smoothed signal is being decomposed, then `data_contributions` are\n            shaped `(nobs, k_endog, nobs, k_endog)`, where the\n            `(t, k, j, p)`-th element is the contribution of the `p`-th\n            observation at time `j` to the smoothed prediction of the `k`-th\n            observation at time `t`.\n        obs_intercept_contributions : array\n            Contributions of the observation intercept to the decomposed\n            object. If the smoothed state is being decomposed, then\n            `obs_intercept_contributions` are shaped\n            `(nobs, k_states, nobs, k_endog)`, where the `(t, m, j, p)`-th\n            element is the contribution of the `p`-th observation intercept at\n            time `j` to the `m`-th state at time `t`. If the smoothed signal\n            is being decomposed, then `obs_intercept_contributions` are shaped\n            `(nobs, k_endog, nobs, k_endog)`, where the `(t, k, j, p)`-th\n            element is the contribution of the `p`-th observation at time `j`\n            to the smoothed prediction of the `k`-th observation at time `t`.\n        state_intercept_contributions : array\n            Contributions of the state intercept to the decomposed object. If\n            the smoothed state is being decomposed, then\n            `state_intercept_contributions` are shaped\n            `(nobs, k_states, nobs, k_states)`, where the `(t, m, j, l)`-th\n            element is the contribution of the `l`-th state intercept at\n            time `j` to the `m`-th state at time `t`. If the smoothed signal\n            is being decomposed, then `state_intercept_contributions` are\n            shaped `(nobs, k_endog, nobs, k_endog)`, where the\n            `(t, k, j, l)`-th element is the contribution of the `p`-th\n            observation at time `j` to the smoothed prediction of the `k`-th\n            observation at time `t`.\n        prior_contributions : array\n            Contributions of the prior to the decomposed object. If the\n            smoothed state is being decomposed, then `prior_contributions` are\n            shaped `(nobs, k_states, k_states)`, where the `(t, m, l)`-th\n            element is the contribution of the `l`-th element of the prior\n            mean to the `m`-th state at time `t`. If the smoothed signal is\n            being decomposed, then `prior_contributions` are shaped\n            `(nobs, k_endog, k_states)`, where the `(t, k, l)`-th\n            element is the contribution of the `l`-th element of the prior mean\n            to the smoothed prediction of the `k`-th observation at time `t`.\n\n        Notes\n        -----\n        Denote the smoothed state at time :math:`t` by :math:`\\\\alpha_t`. Then\n        the smoothed signal is :math:`Z_t \\\\alpha_t`, where :math:`Z_t` is the\n        design matrix operative at time :math:`t`.\n        \"\"\"\n    if decomposition_of not in ['smoothed_state', 'smoothed_signal']:\n        raise ValueError('Invalid value for `decomposition_of`. Must be one of \"smoothed_state\" or \"smoothed_signal\".')\n    (weights, state_intercept_weights, prior_weights) = tools._compute_smoothed_state_weights(self.model, compute_prior_weights=True, scale=self.scale)\n    ZT = self.model.design.T\n    dT = self.model.obs_intercept.T\n    cT = self.model.state_intercept.T\n    if decomposition_of == 'smoothed_signal' and state_index is not None:\n        ZT = ZT[:, state_index, :]\n        weights = weights[:, :, state_index]\n        prior_weights = prior_weights[:, state_index, :]\n    if decomposition_of == 'smoothed_signal':\n        weights = np.nansum(weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n        state_intercept_weights = np.nansum(state_intercept_weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n        prior_weights = np.nansum(prior_weights[..., None] * ZT[:, :, None, :], axis=1).transpose(0, 2, 1)\n    data_contributions = weights * self.model.endog.T[None, :, None, :]\n    data_contributions = data_contributions.transpose(0, 2, 1, 3)\n    obs_intercept_contributions = -weights * dT[None, :, None, :]\n    obs_intercept_contributions = obs_intercept_contributions.transpose(0, 2, 1, 3)\n    state_intercept_contributions = state_intercept_weights * cT[None, :, None, :]\n    state_intercept_contributions = state_intercept_contributions.transpose(0, 2, 1, 3)\n    prior_contributions = prior_weights * self.initial_state[None, None, :]\n    return (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions)",
        "mutated": [
            "def get_smoothed_decomposition(self, decomposition_of='smoothed_state', state_index=None):\n    if False:\n        i = 10\n    '\\n        Decompose smoothed output into contributions from observations\\n\\n        Parameters\\n        ----------\\n        decomposition_of : {\"smoothed_state\", \"smoothed_signal\"}\\n            The object to perform a decomposition of. If it is set to\\n            \"smoothed_state\", then the elements of the smoothed state vector\\n            are decomposed into the contributions of each observation. If it\\n            is set to \"smoothed_signal\", then the predictions of the\\n            observation vector based on the smoothed state vector are\\n            decomposed. Default is \"smoothed_state\".\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the decomposition of the \"smoothed_signal\". For\\n            example, if `state_index=[0, 1]` is passed, then only the\\n            contributions of observed variables to the smoothed signal arising\\n            from the first two states will be returned. Note that if not all\\n            states are used, the contributions will not sum to the smoothed\\n            signal. Default is to use all states.\\n\\n        Returns\\n        -------\\n        data_contributions : array\\n            Contributions of observations to the decomposed object. If the\\n            smoothed state is being decomposed, then `data_contributions` are\\n            shaped `(nobs, k_states, nobs, k_endog)`, where the\\n            `(t, m, j, p)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the `m`-th state at time `t`. If the\\n            smoothed signal is being decomposed, then `data_contributions` are\\n            shaped `(nobs, k_endog, nobs, k_endog)`, where the\\n            `(t, k, j, p)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the smoothed prediction of the `k`-th\\n            observation at time `t`.\\n        obs_intercept_contributions : array\\n            Contributions of the observation intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `obs_intercept_contributions` are shaped\\n            `(nobs, k_states, nobs, k_endog)`, where the `(t, m, j, p)`-th\\n            element is the contribution of the `p`-th observation intercept at\\n            time `j` to the `m`-th state at time `t`. If the smoothed signal\\n            is being decomposed, then `obs_intercept_contributions` are shaped\\n            `(nobs, k_endog, nobs, k_endog)`, where the `(t, k, j, p)`-th\\n            element is the contribution of the `p`-th observation at time `j`\\n            to the smoothed prediction of the `k`-th observation at time `t`.\\n        state_intercept_contributions : array\\n            Contributions of the state intercept to the decomposed object. If\\n            the smoothed state is being decomposed, then\\n            `state_intercept_contributions` are shaped\\n            `(nobs, k_states, nobs, k_states)`, where the `(t, m, j, l)`-th\\n            element is the contribution of the `l`-th state intercept at\\n            time `j` to the `m`-th state at time `t`. If the smoothed signal\\n            is being decomposed, then `state_intercept_contributions` are\\n            shaped `(nobs, k_endog, nobs, k_endog)`, where the\\n            `(t, k, j, l)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the smoothed prediction of the `k`-th\\n            observation at time `t`.\\n        prior_contributions : array\\n            Contributions of the prior to the decomposed object. If the\\n            smoothed state is being decomposed, then `prior_contributions` are\\n            shaped `(nobs, k_states, k_states)`, where the `(t, m, l)`-th\\n            element is the contribution of the `l`-th element of the prior\\n            mean to the `m`-th state at time `t`. If the smoothed signal is\\n            being decomposed, then `prior_contributions` are shaped\\n            `(nobs, k_endog, k_states)`, where the `(t, k, l)`-th\\n            element is the contribution of the `l`-th element of the prior mean\\n            to the smoothed prediction of the `k`-th observation at time `t`.\\n\\n        Notes\\n        -----\\n        Denote the smoothed state at time :math:`t` by :math:`\\\\alpha_t`. Then\\n        the smoothed signal is :math:`Z_t \\\\alpha_t`, where :math:`Z_t` is the\\n        design matrix operative at time :math:`t`.\\n        '\n    if decomposition_of not in ['smoothed_state', 'smoothed_signal']:\n        raise ValueError('Invalid value for `decomposition_of`. Must be one of \"smoothed_state\" or \"smoothed_signal\".')\n    (weights, state_intercept_weights, prior_weights) = tools._compute_smoothed_state_weights(self.model, compute_prior_weights=True, scale=self.scale)\n    ZT = self.model.design.T\n    dT = self.model.obs_intercept.T\n    cT = self.model.state_intercept.T\n    if decomposition_of == 'smoothed_signal' and state_index is not None:\n        ZT = ZT[:, state_index, :]\n        weights = weights[:, :, state_index]\n        prior_weights = prior_weights[:, state_index, :]\n    if decomposition_of == 'smoothed_signal':\n        weights = np.nansum(weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n        state_intercept_weights = np.nansum(state_intercept_weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n        prior_weights = np.nansum(prior_weights[..., None] * ZT[:, :, None, :], axis=1).transpose(0, 2, 1)\n    data_contributions = weights * self.model.endog.T[None, :, None, :]\n    data_contributions = data_contributions.transpose(0, 2, 1, 3)\n    obs_intercept_contributions = -weights * dT[None, :, None, :]\n    obs_intercept_contributions = obs_intercept_contributions.transpose(0, 2, 1, 3)\n    state_intercept_contributions = state_intercept_weights * cT[None, :, None, :]\n    state_intercept_contributions = state_intercept_contributions.transpose(0, 2, 1, 3)\n    prior_contributions = prior_weights * self.initial_state[None, None, :]\n    return (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions)",
            "def get_smoothed_decomposition(self, decomposition_of='smoothed_state', state_index=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Decompose smoothed output into contributions from observations\\n\\n        Parameters\\n        ----------\\n        decomposition_of : {\"smoothed_state\", \"smoothed_signal\"}\\n            The object to perform a decomposition of. If it is set to\\n            \"smoothed_state\", then the elements of the smoothed state vector\\n            are decomposed into the contributions of each observation. If it\\n            is set to \"smoothed_signal\", then the predictions of the\\n            observation vector based on the smoothed state vector are\\n            decomposed. Default is \"smoothed_state\".\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the decomposition of the \"smoothed_signal\". For\\n            example, if `state_index=[0, 1]` is passed, then only the\\n            contributions of observed variables to the smoothed signal arising\\n            from the first two states will be returned. Note that if not all\\n            states are used, the contributions will not sum to the smoothed\\n            signal. Default is to use all states.\\n\\n        Returns\\n        -------\\n        data_contributions : array\\n            Contributions of observations to the decomposed object. If the\\n            smoothed state is being decomposed, then `data_contributions` are\\n            shaped `(nobs, k_states, nobs, k_endog)`, where the\\n            `(t, m, j, p)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the `m`-th state at time `t`. If the\\n            smoothed signal is being decomposed, then `data_contributions` are\\n            shaped `(nobs, k_endog, nobs, k_endog)`, where the\\n            `(t, k, j, p)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the smoothed prediction of the `k`-th\\n            observation at time `t`.\\n        obs_intercept_contributions : array\\n            Contributions of the observation intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `obs_intercept_contributions` are shaped\\n            `(nobs, k_states, nobs, k_endog)`, where the `(t, m, j, p)`-th\\n            element is the contribution of the `p`-th observation intercept at\\n            time `j` to the `m`-th state at time `t`. If the smoothed signal\\n            is being decomposed, then `obs_intercept_contributions` are shaped\\n            `(nobs, k_endog, nobs, k_endog)`, where the `(t, k, j, p)`-th\\n            element is the contribution of the `p`-th observation at time `j`\\n            to the smoothed prediction of the `k`-th observation at time `t`.\\n        state_intercept_contributions : array\\n            Contributions of the state intercept to the decomposed object. If\\n            the smoothed state is being decomposed, then\\n            `state_intercept_contributions` are shaped\\n            `(nobs, k_states, nobs, k_states)`, where the `(t, m, j, l)`-th\\n            element is the contribution of the `l`-th state intercept at\\n            time `j` to the `m`-th state at time `t`. If the smoothed signal\\n            is being decomposed, then `state_intercept_contributions` are\\n            shaped `(nobs, k_endog, nobs, k_endog)`, where the\\n            `(t, k, j, l)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the smoothed prediction of the `k`-th\\n            observation at time `t`.\\n        prior_contributions : array\\n            Contributions of the prior to the decomposed object. If the\\n            smoothed state is being decomposed, then `prior_contributions` are\\n            shaped `(nobs, k_states, k_states)`, where the `(t, m, l)`-th\\n            element is the contribution of the `l`-th element of the prior\\n            mean to the `m`-th state at time `t`. If the smoothed signal is\\n            being decomposed, then `prior_contributions` are shaped\\n            `(nobs, k_endog, k_states)`, where the `(t, k, l)`-th\\n            element is the contribution of the `l`-th element of the prior mean\\n            to the smoothed prediction of the `k`-th observation at time `t`.\\n\\n        Notes\\n        -----\\n        Denote the smoothed state at time :math:`t` by :math:`\\\\alpha_t`. Then\\n        the smoothed signal is :math:`Z_t \\\\alpha_t`, where :math:`Z_t` is the\\n        design matrix operative at time :math:`t`.\\n        '\n    if decomposition_of not in ['smoothed_state', 'smoothed_signal']:\n        raise ValueError('Invalid value for `decomposition_of`. Must be one of \"smoothed_state\" or \"smoothed_signal\".')\n    (weights, state_intercept_weights, prior_weights) = tools._compute_smoothed_state_weights(self.model, compute_prior_weights=True, scale=self.scale)\n    ZT = self.model.design.T\n    dT = self.model.obs_intercept.T\n    cT = self.model.state_intercept.T\n    if decomposition_of == 'smoothed_signal' and state_index is not None:\n        ZT = ZT[:, state_index, :]\n        weights = weights[:, :, state_index]\n        prior_weights = prior_weights[:, state_index, :]\n    if decomposition_of == 'smoothed_signal':\n        weights = np.nansum(weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n        state_intercept_weights = np.nansum(state_intercept_weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n        prior_weights = np.nansum(prior_weights[..., None] * ZT[:, :, None, :], axis=1).transpose(0, 2, 1)\n    data_contributions = weights * self.model.endog.T[None, :, None, :]\n    data_contributions = data_contributions.transpose(0, 2, 1, 3)\n    obs_intercept_contributions = -weights * dT[None, :, None, :]\n    obs_intercept_contributions = obs_intercept_contributions.transpose(0, 2, 1, 3)\n    state_intercept_contributions = state_intercept_weights * cT[None, :, None, :]\n    state_intercept_contributions = state_intercept_contributions.transpose(0, 2, 1, 3)\n    prior_contributions = prior_weights * self.initial_state[None, None, :]\n    return (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions)",
            "def get_smoothed_decomposition(self, decomposition_of='smoothed_state', state_index=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Decompose smoothed output into contributions from observations\\n\\n        Parameters\\n        ----------\\n        decomposition_of : {\"smoothed_state\", \"smoothed_signal\"}\\n            The object to perform a decomposition of. If it is set to\\n            \"smoothed_state\", then the elements of the smoothed state vector\\n            are decomposed into the contributions of each observation. If it\\n            is set to \"smoothed_signal\", then the predictions of the\\n            observation vector based on the smoothed state vector are\\n            decomposed. Default is \"smoothed_state\".\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the decomposition of the \"smoothed_signal\". For\\n            example, if `state_index=[0, 1]` is passed, then only the\\n            contributions of observed variables to the smoothed signal arising\\n            from the first two states will be returned. Note that if not all\\n            states are used, the contributions will not sum to the smoothed\\n            signal. Default is to use all states.\\n\\n        Returns\\n        -------\\n        data_contributions : array\\n            Contributions of observations to the decomposed object. If the\\n            smoothed state is being decomposed, then `data_contributions` are\\n            shaped `(nobs, k_states, nobs, k_endog)`, where the\\n            `(t, m, j, p)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the `m`-th state at time `t`. If the\\n            smoothed signal is being decomposed, then `data_contributions` are\\n            shaped `(nobs, k_endog, nobs, k_endog)`, where the\\n            `(t, k, j, p)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the smoothed prediction of the `k`-th\\n            observation at time `t`.\\n        obs_intercept_contributions : array\\n            Contributions of the observation intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `obs_intercept_contributions` are shaped\\n            `(nobs, k_states, nobs, k_endog)`, where the `(t, m, j, p)`-th\\n            element is the contribution of the `p`-th observation intercept at\\n            time `j` to the `m`-th state at time `t`. If the smoothed signal\\n            is being decomposed, then `obs_intercept_contributions` are shaped\\n            `(nobs, k_endog, nobs, k_endog)`, where the `(t, k, j, p)`-th\\n            element is the contribution of the `p`-th observation at time `j`\\n            to the smoothed prediction of the `k`-th observation at time `t`.\\n        state_intercept_contributions : array\\n            Contributions of the state intercept to the decomposed object. If\\n            the smoothed state is being decomposed, then\\n            `state_intercept_contributions` are shaped\\n            `(nobs, k_states, nobs, k_states)`, where the `(t, m, j, l)`-th\\n            element is the contribution of the `l`-th state intercept at\\n            time `j` to the `m`-th state at time `t`. If the smoothed signal\\n            is being decomposed, then `state_intercept_contributions` are\\n            shaped `(nobs, k_endog, nobs, k_endog)`, where the\\n            `(t, k, j, l)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the smoothed prediction of the `k`-th\\n            observation at time `t`.\\n        prior_contributions : array\\n            Contributions of the prior to the decomposed object. If the\\n            smoothed state is being decomposed, then `prior_contributions` are\\n            shaped `(nobs, k_states, k_states)`, where the `(t, m, l)`-th\\n            element is the contribution of the `l`-th element of the prior\\n            mean to the `m`-th state at time `t`. If the smoothed signal is\\n            being decomposed, then `prior_contributions` are shaped\\n            `(nobs, k_endog, k_states)`, where the `(t, k, l)`-th\\n            element is the contribution of the `l`-th element of the prior mean\\n            to the smoothed prediction of the `k`-th observation at time `t`.\\n\\n        Notes\\n        -----\\n        Denote the smoothed state at time :math:`t` by :math:`\\\\alpha_t`. Then\\n        the smoothed signal is :math:`Z_t \\\\alpha_t`, where :math:`Z_t` is the\\n        design matrix operative at time :math:`t`.\\n        '\n    if decomposition_of not in ['smoothed_state', 'smoothed_signal']:\n        raise ValueError('Invalid value for `decomposition_of`. Must be one of \"smoothed_state\" or \"smoothed_signal\".')\n    (weights, state_intercept_weights, prior_weights) = tools._compute_smoothed_state_weights(self.model, compute_prior_weights=True, scale=self.scale)\n    ZT = self.model.design.T\n    dT = self.model.obs_intercept.T\n    cT = self.model.state_intercept.T\n    if decomposition_of == 'smoothed_signal' and state_index is not None:\n        ZT = ZT[:, state_index, :]\n        weights = weights[:, :, state_index]\n        prior_weights = prior_weights[:, state_index, :]\n    if decomposition_of == 'smoothed_signal':\n        weights = np.nansum(weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n        state_intercept_weights = np.nansum(state_intercept_weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n        prior_weights = np.nansum(prior_weights[..., None] * ZT[:, :, None, :], axis=1).transpose(0, 2, 1)\n    data_contributions = weights * self.model.endog.T[None, :, None, :]\n    data_contributions = data_contributions.transpose(0, 2, 1, 3)\n    obs_intercept_contributions = -weights * dT[None, :, None, :]\n    obs_intercept_contributions = obs_intercept_contributions.transpose(0, 2, 1, 3)\n    state_intercept_contributions = state_intercept_weights * cT[None, :, None, :]\n    state_intercept_contributions = state_intercept_contributions.transpose(0, 2, 1, 3)\n    prior_contributions = prior_weights * self.initial_state[None, None, :]\n    return (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions)",
            "def get_smoothed_decomposition(self, decomposition_of='smoothed_state', state_index=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Decompose smoothed output into contributions from observations\\n\\n        Parameters\\n        ----------\\n        decomposition_of : {\"smoothed_state\", \"smoothed_signal\"}\\n            The object to perform a decomposition of. If it is set to\\n            \"smoothed_state\", then the elements of the smoothed state vector\\n            are decomposed into the contributions of each observation. If it\\n            is set to \"smoothed_signal\", then the predictions of the\\n            observation vector based on the smoothed state vector are\\n            decomposed. Default is \"smoothed_state\".\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the decomposition of the \"smoothed_signal\". For\\n            example, if `state_index=[0, 1]` is passed, then only the\\n            contributions of observed variables to the smoothed signal arising\\n            from the first two states will be returned. Note that if not all\\n            states are used, the contributions will not sum to the smoothed\\n            signal. Default is to use all states.\\n\\n        Returns\\n        -------\\n        data_contributions : array\\n            Contributions of observations to the decomposed object. If the\\n            smoothed state is being decomposed, then `data_contributions` are\\n            shaped `(nobs, k_states, nobs, k_endog)`, where the\\n            `(t, m, j, p)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the `m`-th state at time `t`. If the\\n            smoothed signal is being decomposed, then `data_contributions` are\\n            shaped `(nobs, k_endog, nobs, k_endog)`, where the\\n            `(t, k, j, p)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the smoothed prediction of the `k`-th\\n            observation at time `t`.\\n        obs_intercept_contributions : array\\n            Contributions of the observation intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `obs_intercept_contributions` are shaped\\n            `(nobs, k_states, nobs, k_endog)`, where the `(t, m, j, p)`-th\\n            element is the contribution of the `p`-th observation intercept at\\n            time `j` to the `m`-th state at time `t`. If the smoothed signal\\n            is being decomposed, then `obs_intercept_contributions` are shaped\\n            `(nobs, k_endog, nobs, k_endog)`, where the `(t, k, j, p)`-th\\n            element is the contribution of the `p`-th observation at time `j`\\n            to the smoothed prediction of the `k`-th observation at time `t`.\\n        state_intercept_contributions : array\\n            Contributions of the state intercept to the decomposed object. If\\n            the smoothed state is being decomposed, then\\n            `state_intercept_contributions` are shaped\\n            `(nobs, k_states, nobs, k_states)`, where the `(t, m, j, l)`-th\\n            element is the contribution of the `l`-th state intercept at\\n            time `j` to the `m`-th state at time `t`. If the smoothed signal\\n            is being decomposed, then `state_intercept_contributions` are\\n            shaped `(nobs, k_endog, nobs, k_endog)`, where the\\n            `(t, k, j, l)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the smoothed prediction of the `k`-th\\n            observation at time `t`.\\n        prior_contributions : array\\n            Contributions of the prior to the decomposed object. If the\\n            smoothed state is being decomposed, then `prior_contributions` are\\n            shaped `(nobs, k_states, k_states)`, where the `(t, m, l)`-th\\n            element is the contribution of the `l`-th element of the prior\\n            mean to the `m`-th state at time `t`. If the smoothed signal is\\n            being decomposed, then `prior_contributions` are shaped\\n            `(nobs, k_endog, k_states)`, where the `(t, k, l)`-th\\n            element is the contribution of the `l`-th element of the prior mean\\n            to the smoothed prediction of the `k`-th observation at time `t`.\\n\\n        Notes\\n        -----\\n        Denote the smoothed state at time :math:`t` by :math:`\\\\alpha_t`. Then\\n        the smoothed signal is :math:`Z_t \\\\alpha_t`, where :math:`Z_t` is the\\n        design matrix operative at time :math:`t`.\\n        '\n    if decomposition_of not in ['smoothed_state', 'smoothed_signal']:\n        raise ValueError('Invalid value for `decomposition_of`. Must be one of \"smoothed_state\" or \"smoothed_signal\".')\n    (weights, state_intercept_weights, prior_weights) = tools._compute_smoothed_state_weights(self.model, compute_prior_weights=True, scale=self.scale)\n    ZT = self.model.design.T\n    dT = self.model.obs_intercept.T\n    cT = self.model.state_intercept.T\n    if decomposition_of == 'smoothed_signal' and state_index is not None:\n        ZT = ZT[:, state_index, :]\n        weights = weights[:, :, state_index]\n        prior_weights = prior_weights[:, state_index, :]\n    if decomposition_of == 'smoothed_signal':\n        weights = np.nansum(weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n        state_intercept_weights = np.nansum(state_intercept_weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n        prior_weights = np.nansum(prior_weights[..., None] * ZT[:, :, None, :], axis=1).transpose(0, 2, 1)\n    data_contributions = weights * self.model.endog.T[None, :, None, :]\n    data_contributions = data_contributions.transpose(0, 2, 1, 3)\n    obs_intercept_contributions = -weights * dT[None, :, None, :]\n    obs_intercept_contributions = obs_intercept_contributions.transpose(0, 2, 1, 3)\n    state_intercept_contributions = state_intercept_weights * cT[None, :, None, :]\n    state_intercept_contributions = state_intercept_contributions.transpose(0, 2, 1, 3)\n    prior_contributions = prior_weights * self.initial_state[None, None, :]\n    return (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions)",
            "def get_smoothed_decomposition(self, decomposition_of='smoothed_state', state_index=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Decompose smoothed output into contributions from observations\\n\\n        Parameters\\n        ----------\\n        decomposition_of : {\"smoothed_state\", \"smoothed_signal\"}\\n            The object to perform a decomposition of. If it is set to\\n            \"smoothed_state\", then the elements of the smoothed state vector\\n            are decomposed into the contributions of each observation. If it\\n            is set to \"smoothed_signal\", then the predictions of the\\n            observation vector based on the smoothed state vector are\\n            decomposed. Default is \"smoothed_state\".\\n        state_index : array_like, optional\\n            An optional index specifying a subset of states to use when\\n            constructing the decomposition of the \"smoothed_signal\". For\\n            example, if `state_index=[0, 1]` is passed, then only the\\n            contributions of observed variables to the smoothed signal arising\\n            from the first two states will be returned. Note that if not all\\n            states are used, the contributions will not sum to the smoothed\\n            signal. Default is to use all states.\\n\\n        Returns\\n        -------\\n        data_contributions : array\\n            Contributions of observations to the decomposed object. If the\\n            smoothed state is being decomposed, then `data_contributions` are\\n            shaped `(nobs, k_states, nobs, k_endog)`, where the\\n            `(t, m, j, p)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the `m`-th state at time `t`. If the\\n            smoothed signal is being decomposed, then `data_contributions` are\\n            shaped `(nobs, k_endog, nobs, k_endog)`, where the\\n            `(t, k, j, p)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the smoothed prediction of the `k`-th\\n            observation at time `t`.\\n        obs_intercept_contributions : array\\n            Contributions of the observation intercept to the decomposed\\n            object. If the smoothed state is being decomposed, then\\n            `obs_intercept_contributions` are shaped\\n            `(nobs, k_states, nobs, k_endog)`, where the `(t, m, j, p)`-th\\n            element is the contribution of the `p`-th observation intercept at\\n            time `j` to the `m`-th state at time `t`. If the smoothed signal\\n            is being decomposed, then `obs_intercept_contributions` are shaped\\n            `(nobs, k_endog, nobs, k_endog)`, where the `(t, k, j, p)`-th\\n            element is the contribution of the `p`-th observation at time `j`\\n            to the smoothed prediction of the `k`-th observation at time `t`.\\n        state_intercept_contributions : array\\n            Contributions of the state intercept to the decomposed object. If\\n            the smoothed state is being decomposed, then\\n            `state_intercept_contributions` are shaped\\n            `(nobs, k_states, nobs, k_states)`, where the `(t, m, j, l)`-th\\n            element is the contribution of the `l`-th state intercept at\\n            time `j` to the `m`-th state at time `t`. If the smoothed signal\\n            is being decomposed, then `state_intercept_contributions` are\\n            shaped `(nobs, k_endog, nobs, k_endog)`, where the\\n            `(t, k, j, l)`-th element is the contribution of the `p`-th\\n            observation at time `j` to the smoothed prediction of the `k`-th\\n            observation at time `t`.\\n        prior_contributions : array\\n            Contributions of the prior to the decomposed object. If the\\n            smoothed state is being decomposed, then `prior_contributions` are\\n            shaped `(nobs, k_states, k_states)`, where the `(t, m, l)`-th\\n            element is the contribution of the `l`-th element of the prior\\n            mean to the `m`-th state at time `t`. If the smoothed signal is\\n            being decomposed, then `prior_contributions` are shaped\\n            `(nobs, k_endog, k_states)`, where the `(t, k, l)`-th\\n            element is the contribution of the `l`-th element of the prior mean\\n            to the smoothed prediction of the `k`-th observation at time `t`.\\n\\n        Notes\\n        -----\\n        Denote the smoothed state at time :math:`t` by :math:`\\\\alpha_t`. Then\\n        the smoothed signal is :math:`Z_t \\\\alpha_t`, where :math:`Z_t` is the\\n        design matrix operative at time :math:`t`.\\n        '\n    if decomposition_of not in ['smoothed_state', 'smoothed_signal']:\n        raise ValueError('Invalid value for `decomposition_of`. Must be one of \"smoothed_state\" or \"smoothed_signal\".')\n    (weights, state_intercept_weights, prior_weights) = tools._compute_smoothed_state_weights(self.model, compute_prior_weights=True, scale=self.scale)\n    ZT = self.model.design.T\n    dT = self.model.obs_intercept.T\n    cT = self.model.state_intercept.T\n    if decomposition_of == 'smoothed_signal' and state_index is not None:\n        ZT = ZT[:, state_index, :]\n        weights = weights[:, :, state_index]\n        prior_weights = prior_weights[:, state_index, :]\n    if decomposition_of == 'smoothed_signal':\n        weights = np.nansum(weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n        state_intercept_weights = np.nansum(state_intercept_weights[..., None] * ZT[:, None, :, None, :], axis=2).transpose(0, 1, 3, 2)\n        prior_weights = np.nansum(prior_weights[..., None] * ZT[:, :, None, :], axis=1).transpose(0, 2, 1)\n    data_contributions = weights * self.model.endog.T[None, :, None, :]\n    data_contributions = data_contributions.transpose(0, 2, 1, 3)\n    obs_intercept_contributions = -weights * dT[None, :, None, :]\n    obs_intercept_contributions = obs_intercept_contributions.transpose(0, 2, 1, 3)\n    state_intercept_contributions = state_intercept_weights * cT[None, :, None, :]\n    state_intercept_contributions = state_intercept_contributions.transpose(0, 2, 1, 3)\n    prior_contributions = prior_weights * self.initial_state[None, None, :]\n    return (data_contributions, obs_intercept_contributions, state_intercept_contributions, prior_contributions)"
        ]
    }
]