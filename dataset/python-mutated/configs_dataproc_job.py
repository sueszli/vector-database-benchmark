"""NOTE: THIS FILE IS AUTO-GENERATED. DO NOT EDIT.

@generated

Produced via:
parse_dataproc_configs.py 
"""
from dagster import Bool, Field, Int, Permissive, Shape, String

def define_dataproc_job_config():
    if False:
        return 10
    return Field(Shape(fields={'status': Field(Shape(fields={}), description='Cloud Dataproc job status.', is_required=False), 'placement': Field(Shape(fields={'clusterName': Field(String, description='Required. The name of the cluster where the job will\n                                be submitted.', is_required=False)}), description='Cloud Dataproc job config.', is_required=False), 'scheduling': Field(Shape(fields={'maxFailuresPerHour': Field(Int, description='Optional. Maximum number of times per hour a driver\n                                may be restarted as a result of driver terminating with non-zero\n                                code before job is reported failed.A job may be reported as\n                                thrashing if driver exits with non-zero code 4 times within 10\n                                minute window.Maximum value is 10.', is_required=False)}), description='Job scheduling options.', is_required=False), 'pigJob': Field(Shape(fields={'queryFileUri': Field(String, description='The HCFS URI of the script that contains the Pig\n                                queries.', is_required=False), 'queryList': Field(Shape(fields={'queries': Field([String], description='Required. The queries to execute. You do\n                                            not need to terminate a query with a semicolon. Multiple\n                                            queries can be specified in one string by separating\n                                            each with a semicolon. Here is an example of an Cloud\n                                            Dataproc API snippet that uses a QueryList to specify a\n                                            HiveJob: "hiveJob": {   "queryList": {     "queries": [\n                                            "query1",       "query2",       "query3;query4",     ]\n                                            } } ', is_required=False)}), description='A list of queries to run on a cluster.', is_required=False), 'jarFileUris': Field([String], description='Optional. HCFS URIs of jar files to add to the\n                                CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can\n                                contain Pig UDFs.', is_required=False), 'scriptVariables': Field(Permissive(), description='Optional. Mapping of query variable names to values\n                                (equivalent to the Pig command: name=[value]).', is_required=False), 'loggingConfig': Field(Shape(fields={'driverLogLevels': Field(Permissive(), description='The per-package log levels for the\n                                            driver. This may include "root" package name to\n                                            configure rootLogger. Examples:  \'com.google = FATAL\',\n                                            \'root = INFO\', \'org.apache = DEBUG\'', is_required=False)}), description='The runtime logging config of the job.', is_required=False), 'properties': Field(Permissive(), description='Optional. A mapping of property names to values, used\n                                to configure Pig. Properties that conflict with values set by the\n                                Cloud Dataproc API may be overwritten. Can include properties set in\n                                /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and\n                                classes in user code.', is_required=False), 'continueOnFailure': Field(Bool, description='Optional. Whether to continue executing queries if a\n                                query fails. The default value is false. Setting to true can be\n                                useful when executing independent parallel queries.', is_required=False)}), description='A Cloud Dataproc job for running Apache Pig\n                    (https://pig.apache.org/) queries on YARN.', is_required=False), 'hiveJob': Field(Shape(fields={'continueOnFailure': Field(Bool, description='Optional. Whether to continue executing queries if a\n                                query fails. The default value is false. Setting to true can be\n                                useful when executing independent parallel queries.', is_required=False), 'queryFileUri': Field(String, description='The HCFS URI of the script that contains Hive\n                                queries.', is_required=False), 'queryList': Field(Shape(fields={'queries': Field([String], description='Required. The queries to execute. You do\n                                            not need to terminate a query with a semicolon. Multiple\n                                            queries can be specified in one string by separating\n                                            each with a semicolon. Here is an example of an Cloud\n                                            Dataproc API snippet that uses a QueryList to specify a\n                                            HiveJob: "hiveJob": {   "queryList": {     "queries": [\n                                            "query1",       "query2",       "query3;query4",     ]\n                                            } } ', is_required=False)}), description='A list of queries to run on a cluster.', is_required=False), 'jarFileUris': Field([String], description='Optional. HCFS URIs of jar files to add to the\n                                CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can\n                                contain Hive SerDes and UDFs.', is_required=False), 'scriptVariables': Field(Permissive(), description='Optional. Mapping of query variable names to values\n                                (equivalent to the Hive command: SET name="value";).', is_required=False), 'properties': Field(Permissive(), description='Optional. A mapping of property names and values,\n                                used to configure Hive. Properties that conflict with values set by\n                                the Cloud Dataproc API may be overwritten. Can include properties\n                                set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml,\n                                and classes in user code.', is_required=False)}), description='A Cloud Dataproc job for running Apache Hive\n                    (https://hive.apache.org/) queries on YARN.', is_required=False), 'labels': Field(Permissive(), description='Optional. The labels to associate with this job. Label keys must\n                    contain 1 to 63 characters, and must conform to RFC 1035\n                    (https://www.ietf.org/rfc/rfc1035.txt). Label values may be empty, but, if\n                    present, must contain 1 to 63 characters, and must conform to RFC 1035\n                    (https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated\n                    with a job.', is_required=False), 'sparkJob': Field(Shape(fields={'archiveUris': Field([String], description='Optional. HCFS URIs of archives to be extracted in\n                                the working directory of Spark drivers and tasks. Supported file\n                                types: .jar, .tar, .tar.gz, .tgz, and .zip.', is_required=False), 'mainJarFileUri': Field(String, description='The HCFS URI of the jar file that contains the main\n                                class.', is_required=False), 'jarFileUris': Field([String], description='Optional. HCFS URIs of jar files to add to the\n                                CLASSPATHs of the Spark driver and tasks.', is_required=False), 'loggingConfig': Field(Shape(fields={'driverLogLevels': Field(Permissive(), description='The per-package log levels for the\n                                            driver. This may include "root" package name to\n                                            configure rootLogger. Examples:  \'com.google = FATAL\',\n                                            \'root = INFO\', \'org.apache = DEBUG\'', is_required=False)}), description='The runtime logging config of the job.', is_required=False), 'properties': Field(Permissive(), description='Optional. A mapping of property names to values, used\n                                to configure Spark. Properties that conflict with values set by the\n                                Cloud Dataproc API may be overwritten. Can include properties set in\n                                /etc/spark/conf/spark-defaults.conf and classes in user code.', is_required=False), 'args': Field([String], description='Optional. The arguments to pass to the driver. Do not\n                                include arguments, such as --conf, that can be set as job\n                                properties, since a collision may occur that causes an incorrect job\n                                submission.', is_required=False), 'fileUris': Field([String], description='Optional. HCFS URIs of files to be copied to the\n                                working directory of Spark drivers and distributed tasks. Useful for\n                                naively parallel tasks.', is_required=False), 'mainClass': Field(String, description="The name of the driver's main class. The jar file\n                                that contains the class must be in the default CLASSPATH or\n                                specified in jar_file_uris.", is_required=False)}), description='A Cloud Dataproc job for running Apache Spark\n                    (http://spark.apache.org/) applications on YARN.', is_required=False), 'sparkSqlJob': Field(Shape(fields={'queryList': Field(Shape(fields={'queries': Field([String], description='Required. The queries to execute. You do\n                                            not need to terminate a query with a semicolon. Multiple\n                                            queries can be specified in one string by separating\n                                            each with a semicolon. Here is an example of an Cloud\n                                            Dataproc API snippet that uses a QueryList to specify a\n                                            HiveJob: "hiveJob": {   "queryList": {     "queries": [\n                                            "query1",       "query2",       "query3;query4",     ]\n                                            } } ', is_required=False)}), description='A list of queries to run on a cluster.', is_required=False), 'queryFileUri': Field(String, description='The HCFS URI of the script that contains SQL\n                                queries.', is_required=False), 'scriptVariables': Field(Permissive(), description='Optional. Mapping of query variable names to values\n                                (equivalent to the Spark SQL command: SET name="value";).', is_required=False), 'jarFileUris': Field([String], description='Optional. HCFS URIs of jar files to be added to the\n                                Spark CLASSPATH.', is_required=False), 'loggingConfig': Field(Shape(fields={'driverLogLevels': Field(Permissive(), description='The per-package log levels for the\n                                            driver. This may include "root" package name to\n                                            configure rootLogger. Examples:  \'com.google = FATAL\',\n                                            \'root = INFO\', \'org.apache = DEBUG\'', is_required=False)}), description='The runtime logging config of the job.', is_required=False), 'properties': Field(Permissive(), description="Optional. A mapping of property names to values, used\n                                to configure Spark SQL's SparkConf. Properties that conflict with\n                                values set by the Cloud Dataproc API may be overwritten.", is_required=False)}), description='A Cloud Dataproc job for running Apache Spark SQL\n                    (http://spark.apache.org/sql/) queries.', is_required=False), 'pysparkJob': Field(Shape(fields={'jarFileUris': Field([String], description='Optional. HCFS URIs of jar files to add to the\n                                CLASSPATHs of the Python driver and tasks.', is_required=False), 'loggingConfig': Field(Shape(fields={'driverLogLevels': Field(Permissive(), description='The per-package log levels for the\n                                            driver. This may include "root" package name to\n                                            configure rootLogger. Examples:  \'com.google = FATAL\',\n                                            \'root = INFO\', \'org.apache = DEBUG\'', is_required=False)}), description='The runtime logging config of the job.', is_required=False), 'properties': Field(Permissive(), description='Optional. A mapping of property names to values, used\n                                to configure PySpark. Properties that conflict with values set by\n                                the Cloud Dataproc API may be overwritten. Can include properties\n                                set in /etc/spark/conf/spark-defaults.conf and classes in user\n                                code.', is_required=False), 'args': Field([String], description='Optional. The arguments to pass to the driver. Do not\n                                include arguments, such as --conf, that can be set as job\n                                properties, since a collision may occur that causes an incorrect job\n                                submission.', is_required=False), 'fileUris': Field([String], description='Optional. HCFS URIs of files to be copied to the\n                                working directory of Python drivers and distributed tasks. Useful\n                                for naively parallel tasks.', is_required=False), 'pythonFileUris': Field([String], description='Optional. HCFS file URIs of Python files to pass to\n                                the PySpark framework. Supported file types: .py, .egg, and\n                                .zip.', is_required=False), 'mainPythonFileUri': Field(String, description='Required. The HCFS URI of the main Python file to use\n                                as the driver. Must be a .py file.', is_required=False), 'archiveUris': Field([String], description='Optional. HCFS URIs of archives to be extracted in\n                                the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.', is_required=False)}), description='A Cloud Dataproc job for running Apache PySpark\n                    (https://spark.apache.org/docs/0.9.0/python-programming-guide.html) applications\n                    on YARN.', is_required=False), 'reference': Field(Shape(fields={'projectId': Field(String, description='Required. The ID of the Google Cloud Platform project\n                                that the job belongs to.', is_required=False), 'jobId': Field(String, description='Optional. The job ID, which must be unique within the\n                                project.The ID must contain only letters (a-z, A-Z), numbers (0-9),\n                                underscores (_), or hyphens (-). The maximum length is 100\n                                characters.If not specified by the caller, the job ID will be\n                                provided by the server.', is_required=False)}), description='Encapsulates the full scoping used to reference a job.', is_required=False), 'hadoopJob': Field(Shape(fields={'jarFileUris': Field([String], description='Optional. Jar file URIs to add to the CLASSPATHs of\n                                the Hadoop driver and tasks.', is_required=False), 'loggingConfig': Field(Shape(fields={'driverLogLevels': Field(Permissive(), description='The per-package log levels for the\n                                            driver. This may include "root" package name to\n                                            configure rootLogger. Examples:  \'com.google = FATAL\',\n                                            \'root = INFO\', \'org.apache = DEBUG\'', is_required=False)}), description='The runtime logging config of the job.', is_required=False), 'properties': Field(Permissive(), description='Optional. A mapping of property names to values, used\n                                to configure Hadoop. Properties that conflict with values set by the\n                                Cloud Dataproc API may be overwritten. Can include properties set in\n                                /etc/hadoop/conf/*-site and classes in user code.', is_required=False), 'args': Field([String], description='Optional. The arguments to pass to the driver. Do not\n                                include arguments, such as -libjars or -Dfoo=bar, that can be set as\n                                job properties, since a collision may occur that causes an incorrect\n                                job submission.', is_required=False), 'fileUris': Field([String], description='Optional. HCFS (Hadoop Compatible Filesystem) URIs of\n                                files to be copied to the working directory of Hadoop drivers and\n                                distributed tasks. Useful for naively parallel tasks.', is_required=False), 'mainClass': Field(String, description="The name of the driver's main class. The jar file\n                                containing the class must be in the default CLASSPATH or specified\n                                in jar_file_uris.", is_required=False), 'archiveUris': Field([String], description='Optional. HCFS URIs of archives to be extracted in\n                                the working directory of Hadoop drivers and tasks. Supported file\n                                types: .jar, .tar, .tar.gz, .tgz, or .zip.', is_required=False), 'mainJarFileUri': Field(String, description="The HCFS URI of the jar file containing the main\n                                class. Examples:\n                                'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar'\n                                'hdfs:/tmp/test-samples/custom-wordcount.jar'\n                                'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'", is_required=False)}), description='A Cloud Dataproc job for running Apache Hadoop MapReduce\n                    (https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)\n                    jobs on Apache Hadoop YARN\n                    (https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).', is_required=False)}), description='A Cloud Dataproc job resource.', is_required=False)