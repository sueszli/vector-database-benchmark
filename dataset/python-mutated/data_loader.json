[
    {
        "func_name": "load_json",
        "original": "def load_json(fname):\n    \"\"\"\n    load json object from file\n    \"\"\"\n    with open(fname) as f:\n        data = json.load(f)\n    return data",
        "mutated": [
            "def load_json(fname):\n    if False:\n        i = 10\n    '\\n    load json object from file\\n    '\n    with open(fname) as f:\n        data = json.load(f)\n    return data",
            "def load_json(fname):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    load json object from file\\n    '\n    with open(fname) as f:\n        data = json.load(f)\n    return data",
            "def load_json(fname):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    load json object from file\\n    '\n    with open(fname) as f:\n        data = json.load(f)\n    return data",
            "def load_json(fname):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    load json object from file\\n    '\n    with open(fname) as f:\n        data = json.load(f)\n    return data",
            "def load_json(fname):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    load json object from file\\n    '\n    with open(fname) as f:\n        data = json.load(f)\n    return data"
        ]
    },
    {
        "func_name": "load_json_sent",
        "original": "def load_json_sent(flist_json, subset_pct):\n    \"\"\"\n    load all the sentences from a list of JSON files (with out data format)\n    and return a list\n    \"\"\"\n    subset_fnum = int(np.ceil(subset_pct / 100.0 * len(flist_json)))\n    all_sent = []\n    for f in flist_json[:subset_fnum]:\n        data = load_json(f)\n        num_sent = len(data['text'])\n        if num_sent <= 0:\n            continue\n        num_sent = len(data['text'])\n        sent = [data['text'][i]['sentence'] for i in range(num_sent)]\n        all_sent += sent\n    return all_sent",
        "mutated": [
            "def load_json_sent(flist_json, subset_pct):\n    if False:\n        i = 10\n    '\\n    load all the sentences from a list of JSON files (with out data format)\\n    and return a list\\n    '\n    subset_fnum = int(np.ceil(subset_pct / 100.0 * len(flist_json)))\n    all_sent = []\n    for f in flist_json[:subset_fnum]:\n        data = load_json(f)\n        num_sent = len(data['text'])\n        if num_sent <= 0:\n            continue\n        num_sent = len(data['text'])\n        sent = [data['text'][i]['sentence'] for i in range(num_sent)]\n        all_sent += sent\n    return all_sent",
            "def load_json_sent(flist_json, subset_pct):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    load all the sentences from a list of JSON files (with out data format)\\n    and return a list\\n    '\n    subset_fnum = int(np.ceil(subset_pct / 100.0 * len(flist_json)))\n    all_sent = []\n    for f in flist_json[:subset_fnum]:\n        data = load_json(f)\n        num_sent = len(data['text'])\n        if num_sent <= 0:\n            continue\n        num_sent = len(data['text'])\n        sent = [data['text'][i]['sentence'] for i in range(num_sent)]\n        all_sent += sent\n    return all_sent",
            "def load_json_sent(flist_json, subset_pct):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    load all the sentences from a list of JSON files (with out data format)\\n    and return a list\\n    '\n    subset_fnum = int(np.ceil(subset_pct / 100.0 * len(flist_json)))\n    all_sent = []\n    for f in flist_json[:subset_fnum]:\n        data = load_json(f)\n        num_sent = len(data['text'])\n        if num_sent <= 0:\n            continue\n        num_sent = len(data['text'])\n        sent = [data['text'][i]['sentence'] for i in range(num_sent)]\n        all_sent += sent\n    return all_sent",
            "def load_json_sent(flist_json, subset_pct):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    load all the sentences from a list of JSON files (with out data format)\\n    and return a list\\n    '\n    subset_fnum = int(np.ceil(subset_pct / 100.0 * len(flist_json)))\n    all_sent = []\n    for f in flist_json[:subset_fnum]:\n        data = load_json(f)\n        num_sent = len(data['text'])\n        if num_sent <= 0:\n            continue\n        num_sent = len(data['text'])\n        sent = [data['text'][i]['sentence'] for i in range(num_sent)]\n        all_sent += sent\n    return all_sent",
            "def load_json_sent(flist_json, subset_pct):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    load all the sentences from a list of JSON files (with out data format)\\n    and return a list\\n    '\n    subset_fnum = int(np.ceil(subset_pct / 100.0 * len(flist_json)))\n    all_sent = []\n    for f in flist_json[:subset_fnum]:\n        data = load_json(f)\n        num_sent = len(data['text'])\n        if num_sent <= 0:\n            continue\n        num_sent = len(data['text'])\n        sent = [data['text'][i]['sentence'] for i in range(num_sent)]\n        all_sent += sent\n    return all_sent"
        ]
    },
    {
        "func_name": "load_txt_sent",
        "original": "def load_txt_sent(flist_txt, subset_pct):\n    \"\"\"\n    load all the senteces from a list of txt files using standard file io\n    \"\"\"\n    total_size = sum(map(os.path.getsize, flist_txt))\n    subset_size = int(subset_pct / 100.0 * total_size)\n    all_sent = []\n    for txt_file in flist_txt:\n        if subset_size > 0:\n            with open(txt_file, 'r') as f:\n                data = f.read(subset_size)\n            subset_size -= sys.getsizeof(data)\n            sent = data.split('\\n')\n            if subset_size <= 0:\n                sent = sent[:-1]\n            all_sent += sent\n    return all_sent",
        "mutated": [
            "def load_txt_sent(flist_txt, subset_pct):\n    if False:\n        i = 10\n    '\\n    load all the senteces from a list of txt files using standard file io\\n    '\n    total_size = sum(map(os.path.getsize, flist_txt))\n    subset_size = int(subset_pct / 100.0 * total_size)\n    all_sent = []\n    for txt_file in flist_txt:\n        if subset_size > 0:\n            with open(txt_file, 'r') as f:\n                data = f.read(subset_size)\n            subset_size -= sys.getsizeof(data)\n            sent = data.split('\\n')\n            if subset_size <= 0:\n                sent = sent[:-1]\n            all_sent += sent\n    return all_sent",
            "def load_txt_sent(flist_txt, subset_pct):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    load all the senteces from a list of txt files using standard file io\\n    '\n    total_size = sum(map(os.path.getsize, flist_txt))\n    subset_size = int(subset_pct / 100.0 * total_size)\n    all_sent = []\n    for txt_file in flist_txt:\n        if subset_size > 0:\n            with open(txt_file, 'r') as f:\n                data = f.read(subset_size)\n            subset_size -= sys.getsizeof(data)\n            sent = data.split('\\n')\n            if subset_size <= 0:\n                sent = sent[:-1]\n            all_sent += sent\n    return all_sent",
            "def load_txt_sent(flist_txt, subset_pct):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    load all the senteces from a list of txt files using standard file io\\n    '\n    total_size = sum(map(os.path.getsize, flist_txt))\n    subset_size = int(subset_pct / 100.0 * total_size)\n    all_sent = []\n    for txt_file in flist_txt:\n        if subset_size > 0:\n            with open(txt_file, 'r') as f:\n                data = f.read(subset_size)\n            subset_size -= sys.getsizeof(data)\n            sent = data.split('\\n')\n            if subset_size <= 0:\n                sent = sent[:-1]\n            all_sent += sent\n    return all_sent",
            "def load_txt_sent(flist_txt, subset_pct):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    load all the senteces from a list of txt files using standard file io\\n    '\n    total_size = sum(map(os.path.getsize, flist_txt))\n    subset_size = int(subset_pct / 100.0 * total_size)\n    all_sent = []\n    for txt_file in flist_txt:\n        if subset_size > 0:\n            with open(txt_file, 'r') as f:\n                data = f.read(subset_size)\n            subset_size -= sys.getsizeof(data)\n            sent = data.split('\\n')\n            if subset_size <= 0:\n                sent = sent[:-1]\n            all_sent += sent\n    return all_sent",
            "def load_txt_sent(flist_txt, subset_pct):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    load all the senteces from a list of txt files using standard file io\\n    '\n    total_size = sum(map(os.path.getsize, flist_txt))\n    subset_size = int(subset_pct / 100.0 * total_size)\n    all_sent = []\n    for txt_file in flist_txt:\n        if subset_size > 0:\n            with open(txt_file, 'r') as f:\n                data = f.read(subset_size)\n            subset_size -= sys.getsizeof(data)\n            sent = data.split('\\n')\n            if subset_size <= 0:\n                sent = sent[:-1]\n            all_sent += sent\n    return all_sent"
        ]
    },
    {
        "func_name": "get_file_list",
        "original": "def get_file_list(data_dir, file_ext):\n    \"\"\"\n    Return list of files with the given extension in the data_dir\n    \"\"\"\n    file_ext = file_ext if isinstance(file_ext, list) else [file_ext]\n    file_names = [os.path.join(data_dir, fn) for fn in os.listdir(data_dir) if any((fn.endswith(ext) for ext in file_ext))]\n    return file_names",
        "mutated": [
            "def get_file_list(data_dir, file_ext):\n    if False:\n        i = 10\n    '\\n    Return list of files with the given extension in the data_dir\\n    '\n    file_ext = file_ext if isinstance(file_ext, list) else [file_ext]\n    file_names = [os.path.join(data_dir, fn) for fn in os.listdir(data_dir) if any((fn.endswith(ext) for ext in file_ext))]\n    return file_names",
            "def get_file_list(data_dir, file_ext):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Return list of files with the given extension in the data_dir\\n    '\n    file_ext = file_ext if isinstance(file_ext, list) else [file_ext]\n    file_names = [os.path.join(data_dir, fn) for fn in os.listdir(data_dir) if any((fn.endswith(ext) for ext in file_ext))]\n    return file_names",
            "def get_file_list(data_dir, file_ext):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Return list of files with the given extension in the data_dir\\n    '\n    file_ext = file_ext if isinstance(file_ext, list) else [file_ext]\n    file_names = [os.path.join(data_dir, fn) for fn in os.listdir(data_dir) if any((fn.endswith(ext) for ext in file_ext))]\n    return file_names",
            "def get_file_list(data_dir, file_ext):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Return list of files with the given extension in the data_dir\\n    '\n    file_ext = file_ext if isinstance(file_ext, list) else [file_ext]\n    file_names = [os.path.join(data_dir, fn) for fn in os.listdir(data_dir) if any((fn.endswith(ext) for ext in file_ext))]\n    return file_names",
            "def get_file_list(data_dir, file_ext):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Return list of files with the given extension in the data_dir\\n    '\n    file_ext = file_ext if isinstance(file_ext, list) else [file_ext]\n    file_names = [os.path.join(data_dir, fn) for fn in os.listdir(data_dir) if any((fn.endswith(ext) for ext in file_ext))]\n    return file_names"
        ]
    },
    {
        "func_name": "get_file_str",
        "original": "def get_file_str(path, num_files, labelled=False, valid_split=None, split_count_thre=None, subset_pct=100):\n    \"\"\"\n    Create unique file name for processed data from the number of files, directory name,\n    validation split type, and subset percentage\n    \"\"\"\n    dir_name = path.split('/')[-1] if len(path.split('/')[-1]) > 0 else path.split('/')[-2]\n    label_str = 'labelled' if labelled else ''\n    split_thre_str = 'thre_{}'.format(split_count_thre) if split_count_thre else ''\n    dir_str = 'doc_{}_{}_{}_{}'.format(label_str, dir_name, num_files, split_thre_str)\n    if valid_split:\n        split_str = '_split_{}'.format(valid_split * 100)\n    else:\n        split_str = ''\n    if subset_pct != 100:\n        subset_str = '_subset_{}'.format(subset_pct)\n    else:\n        subset_str = ''\n    file_str = dir_str + split_str + subset_str\n    return file_str",
        "mutated": [
            "def get_file_str(path, num_files, labelled=False, valid_split=None, split_count_thre=None, subset_pct=100):\n    if False:\n        i = 10\n    '\\n    Create unique file name for processed data from the number of files, directory name,\\n    validation split type, and subset percentage\\n    '\n    dir_name = path.split('/')[-1] if len(path.split('/')[-1]) > 0 else path.split('/')[-2]\n    label_str = 'labelled' if labelled else ''\n    split_thre_str = 'thre_{}'.format(split_count_thre) if split_count_thre else ''\n    dir_str = 'doc_{}_{}_{}_{}'.format(label_str, dir_name, num_files, split_thre_str)\n    if valid_split:\n        split_str = '_split_{}'.format(valid_split * 100)\n    else:\n        split_str = ''\n    if subset_pct != 100:\n        subset_str = '_subset_{}'.format(subset_pct)\n    else:\n        subset_str = ''\n    file_str = dir_str + split_str + subset_str\n    return file_str",
            "def get_file_str(path, num_files, labelled=False, valid_split=None, split_count_thre=None, subset_pct=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Create unique file name for processed data from the number of files, directory name,\\n    validation split type, and subset percentage\\n    '\n    dir_name = path.split('/')[-1] if len(path.split('/')[-1]) > 0 else path.split('/')[-2]\n    label_str = 'labelled' if labelled else ''\n    split_thre_str = 'thre_{}'.format(split_count_thre) if split_count_thre else ''\n    dir_str = 'doc_{}_{}_{}_{}'.format(label_str, dir_name, num_files, split_thre_str)\n    if valid_split:\n        split_str = '_split_{}'.format(valid_split * 100)\n    else:\n        split_str = ''\n    if subset_pct != 100:\n        subset_str = '_subset_{}'.format(subset_pct)\n    else:\n        subset_str = ''\n    file_str = dir_str + split_str + subset_str\n    return file_str",
            "def get_file_str(path, num_files, labelled=False, valid_split=None, split_count_thre=None, subset_pct=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Create unique file name for processed data from the number of files, directory name,\\n    validation split type, and subset percentage\\n    '\n    dir_name = path.split('/')[-1] if len(path.split('/')[-1]) > 0 else path.split('/')[-2]\n    label_str = 'labelled' if labelled else ''\n    split_thre_str = 'thre_{}'.format(split_count_thre) if split_count_thre else ''\n    dir_str = 'doc_{}_{}_{}_{}'.format(label_str, dir_name, num_files, split_thre_str)\n    if valid_split:\n        split_str = '_split_{}'.format(valid_split * 100)\n    else:\n        split_str = ''\n    if subset_pct != 100:\n        subset_str = '_subset_{}'.format(subset_pct)\n    else:\n        subset_str = ''\n    file_str = dir_str + split_str + subset_str\n    return file_str",
            "def get_file_str(path, num_files, labelled=False, valid_split=None, split_count_thre=None, subset_pct=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Create unique file name for processed data from the number of files, directory name,\\n    validation split type, and subset percentage\\n    '\n    dir_name = path.split('/')[-1] if len(path.split('/')[-1]) > 0 else path.split('/')[-2]\n    label_str = 'labelled' if labelled else ''\n    split_thre_str = 'thre_{}'.format(split_count_thre) if split_count_thre else ''\n    dir_str = 'doc_{}_{}_{}_{}'.format(label_str, dir_name, num_files, split_thre_str)\n    if valid_split:\n        split_str = '_split_{}'.format(valid_split * 100)\n    else:\n        split_str = ''\n    if subset_pct != 100:\n        subset_str = '_subset_{}'.format(subset_pct)\n    else:\n        subset_str = ''\n    file_str = dir_str + split_str + subset_str\n    return file_str",
            "def get_file_str(path, num_files, labelled=False, valid_split=None, split_count_thre=None, subset_pct=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Create unique file name for processed data from the number of files, directory name,\\n    validation split type, and subset percentage\\n    '\n    dir_name = path.split('/')[-1] if len(path.split('/')[-1]) > 0 else path.split('/')[-2]\n    label_str = 'labelled' if labelled else ''\n    split_thre_str = 'thre_{}'.format(split_count_thre) if split_count_thre else ''\n    dir_str = 'doc_{}_{}_{}_{}'.format(label_str, dir_name, num_files, split_thre_str)\n    if valid_split:\n        split_str = '_split_{}'.format(valid_split * 100)\n    else:\n        split_str = ''\n    if subset_pct != 100:\n        subset_str = '_subset_{}'.format(subset_pct)\n    else:\n        subset_str = ''\n    file_str = dir_str + split_str + subset_str\n    return file_str"
        ]
    },
    {
        "func_name": "load_data",
        "original": "def load_data(path, file_ext=['txt'], valid_split=None, vocab_file_name=None, max_vocab_size=None, max_len_w=None, output_path=None, subset_pct=100):\n    \"\"\"\n    Given a path where data are saved, look for the ones with the right extensions\n    If a split factor is given, it will split all the files into training and valid\n    set. Then build vocabulary from the training and validation sets.\n\n    Arguments:\n        path: which directory to look for all the documents\n        file_ext: what extension of the files to look for\n        valid_split: to split the data into train/valid set. If None, no split\n        vocab_file_name: optional file name. If None, the script will decide a name\n                         given path and split\n        max_vocab_size: maximum number of words to use in vocabulary (by most frequent)\n        max_len_w: maximum length of sentences in words\n        output_path: path used to save preprocessed data and resuts\n        subset_pct: subset of dataset to load into H5 file (percentage)\n\n    Returns:\n        The function saves 2 files:\n        h5 file with preprocessed data\n        vocabulary file with: vocab, reverse_vocab, word_count\n    \"\"\"\n    file_names = get_file_list(path, file_ext)\n    file_str = get_file_str(path, len(file_names), labelled=False, valid_split=valid_split, subset_pct=subset_pct)\n    if not os.path.isdir(output_path):\n        os.makedirs(output_path)\n    if vocab_file_name is None:\n        vocab_file_name = file_str + '.vocab'\n        vocab_file_name = os.path.join(output_path, vocab_file_name)\n    if not max_len_w:\n        max_len_w = sys.maxsize\n    if not max_vocab_size:\n        max_vocab_size = sys.maxsize\n    h5_file_name = os.path.join(output_path, file_str + '.h5')\n    if os.path.exists(h5_file_name) and os.path.exists(vocab_file_name):\n        neon_logger.display('dataset files {} and vocabulary file {} already exist. will use cached data. '.format(h5_file_name, vocab_file_name))\n        return (h5_file_name, vocab_file_name)\n    if valid_split is not None:\n        if 'json' in file_ext:\n            train_split = int(np.ceil(len(file_names) * (1 - valid_split)))\n            train_files = file_names[:train_split]\n            valid_files = file_names[train_split:]\n            train_sent = load_json_sent(train_files, subset_pct)\n            valid_sent = load_json_sent(valid_files, subset_pct)\n            all_sent = train_sent + valid_sent\n        elif 'txt' in file_ext:\n            all_sent = load_txt_sent(file_names, subset_pct)\n            train_split = int(np.ceil(len(all_sent) * (1 - valid_split)))\n            train_sent = all_sent[:train_split]\n            valid_sent = all_sent[train_split:]\n        else:\n            neon_logger.display(\"Unsure how to load file_ext {}, please use 'json' or 'txt'.\".format(file_ext))\n    else:\n        train_files = file_names\n        if 'json' in file_ext:\n            train_sent = load_json_sent(train_files, subset_pct)\n        elif 'txt' in file_ext:\n            train_sent = load_txt_sent(train_files, subset_pct)\n        else:\n            neon_logger.display(\"Unsure how to load file_ext {}, please use 'json' or 'txt'.\".format(file_ext))\n        all_sent = train_sent\n    if os.path.exists(vocab_file_name):\n        neon_logger.display('open existing vocab file: {}'.format(vocab_file_name))\n        (vocab, rev_vocab, word_count) = load_obj(vocab_file_name)\n    else:\n        neon_logger.display('Building  vocab file')\n        word_count = defaultdict(int)\n        for sent in all_sent:\n            sent_words = tokenize(sent)\n            if len(sent_words) > max_len_w or len(sent_words) == 0:\n                continue\n            for word in sent_words:\n                word_count[word] += 1\n        vocab_sorted = sorted(word_count.items(), key=lambda kv: kv[1], reverse=True)\n        vocab = OrderedDict()\n        word_count_ = np.zeros((len(word_count),), dtype=np.int64)\n        for (i, t) in enumerate(list(zip(*vocab_sorted))[0][:max_vocab_size]):\n            word_count_[i] = word_count[t]\n            vocab[t] = i\n        word_count = word_count_\n        rev_vocab = dict(((wrd_id, wrd) for (wrd, wrd_id) in vocab.items()))\n        neon_logger.display('vocabulary from {} is saved into {}'.format(path, vocab_file_name))\n        save_obj((vocab, rev_vocab, word_count), vocab_file_name)\n    vocab_size = len(vocab)\n    neon_logger.display('\\nVocab size from the dataset is: {}'.format(vocab_size))\n    neon_logger.display('\\nProcessing and saving training data into {}'.format(h5_file_name))\n    h5f = h5py.File(h5_file_name, 'w', libver='latest')\n    (shape, maxshape) = ((len(train_sent),), None)\n    dt = np.dtype([('text', h5py.special_dtype(vlen=str)), ('num_words', np.uint16)])\n    report_text_train = h5f.create_dataset('report_train', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n    report_train = h5f.create_dataset('train', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n    wdata = np.zeros((1,), dtype=dt)\n    ntrain = 0\n    for sent in train_sent:\n        text_int = [-1 if t not in vocab else vocab[t] for t in tokenize(sent)]\n        if len(text_int) > max_len_w or len(text_int) == 0:\n            continue\n        report_train[ntrain] = text_int\n        wdata['text'] = clean_string(sent)\n        wdata['num_words'] = len(text_int)\n        report_text_train[ntrain] = wdata\n        ntrain += 1\n    report_train.attrs['nsample'] = ntrain\n    report_train.attrs['vocab_size'] = vocab_size\n    report_text_train.attrs['nsample'] = ntrain\n    report_text_train.attrs['vocab_size'] = vocab_size\n    if valid_split:\n        neon_logger.display('\\nProcessing and saving validation data into {}'.format(h5_file_name))\n        shape = (len(valid_sent),)\n        report_text_valid = h5f.create_dataset('report_valid', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n        report_valid = h5f.create_dataset('valid', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        nvalid = 0\n        for sent in valid_sent:\n            text_int = [-1 if t not in vocab else vocab[t] for t in tokenize(sent)]\n            if len(text_int) > max_len_w or len(text_int) == 0:\n                continue\n            report_valid[nvalid] = text_int\n            wdata['text'] = clean_string(sent)\n            wdata['num_words'] = len(text_int)\n            report_text_valid[nvalid] = wdata\n            nvalid += 1\n        report_valid.attrs['nsample'] = nvalid\n        report_valid.attrs['vocab_size'] = vocab_size\n        report_text_valid.attrs['nsample'] = nvalid\n        report_text_valid.attrs['vocab_size'] = vocab_size\n    h5f.close()\n    return (h5_file_name, vocab_file_name)",
        "mutated": [
            "def load_data(path, file_ext=['txt'], valid_split=None, vocab_file_name=None, max_vocab_size=None, max_len_w=None, output_path=None, subset_pct=100):\n    if False:\n        i = 10\n    '\\n    Given a path where data are saved, look for the ones with the right extensions\\n    If a split factor is given, it will split all the files into training and valid\\n    set. Then build vocabulary from the training and validation sets.\\n\\n    Arguments:\\n        path: which directory to look for all the documents\\n        file_ext: what extension of the files to look for\\n        valid_split: to split the data into train/valid set. If None, no split\\n        vocab_file_name: optional file name. If None, the script will decide a name\\n                         given path and split\\n        max_vocab_size: maximum number of words to use in vocabulary (by most frequent)\\n        max_len_w: maximum length of sentences in words\\n        output_path: path used to save preprocessed data and resuts\\n        subset_pct: subset of dataset to load into H5 file (percentage)\\n\\n    Returns:\\n        The function saves 2 files:\\n        h5 file with preprocessed data\\n        vocabulary file with: vocab, reverse_vocab, word_count\\n    '\n    file_names = get_file_list(path, file_ext)\n    file_str = get_file_str(path, len(file_names), labelled=False, valid_split=valid_split, subset_pct=subset_pct)\n    if not os.path.isdir(output_path):\n        os.makedirs(output_path)\n    if vocab_file_name is None:\n        vocab_file_name = file_str + '.vocab'\n        vocab_file_name = os.path.join(output_path, vocab_file_name)\n    if not max_len_w:\n        max_len_w = sys.maxsize\n    if not max_vocab_size:\n        max_vocab_size = sys.maxsize\n    h5_file_name = os.path.join(output_path, file_str + '.h5')\n    if os.path.exists(h5_file_name) and os.path.exists(vocab_file_name):\n        neon_logger.display('dataset files {} and vocabulary file {} already exist. will use cached data. '.format(h5_file_name, vocab_file_name))\n        return (h5_file_name, vocab_file_name)\n    if valid_split is not None:\n        if 'json' in file_ext:\n            train_split = int(np.ceil(len(file_names) * (1 - valid_split)))\n            train_files = file_names[:train_split]\n            valid_files = file_names[train_split:]\n            train_sent = load_json_sent(train_files, subset_pct)\n            valid_sent = load_json_sent(valid_files, subset_pct)\n            all_sent = train_sent + valid_sent\n        elif 'txt' in file_ext:\n            all_sent = load_txt_sent(file_names, subset_pct)\n            train_split = int(np.ceil(len(all_sent) * (1 - valid_split)))\n            train_sent = all_sent[:train_split]\n            valid_sent = all_sent[train_split:]\n        else:\n            neon_logger.display(\"Unsure how to load file_ext {}, please use 'json' or 'txt'.\".format(file_ext))\n    else:\n        train_files = file_names\n        if 'json' in file_ext:\n            train_sent = load_json_sent(train_files, subset_pct)\n        elif 'txt' in file_ext:\n            train_sent = load_txt_sent(train_files, subset_pct)\n        else:\n            neon_logger.display(\"Unsure how to load file_ext {}, please use 'json' or 'txt'.\".format(file_ext))\n        all_sent = train_sent\n    if os.path.exists(vocab_file_name):\n        neon_logger.display('open existing vocab file: {}'.format(vocab_file_name))\n        (vocab, rev_vocab, word_count) = load_obj(vocab_file_name)\n    else:\n        neon_logger.display('Building  vocab file')\n        word_count = defaultdict(int)\n        for sent in all_sent:\n            sent_words = tokenize(sent)\n            if len(sent_words) > max_len_w or len(sent_words) == 0:\n                continue\n            for word in sent_words:\n                word_count[word] += 1\n        vocab_sorted = sorted(word_count.items(), key=lambda kv: kv[1], reverse=True)\n        vocab = OrderedDict()\n        word_count_ = np.zeros((len(word_count),), dtype=np.int64)\n        for (i, t) in enumerate(list(zip(*vocab_sorted))[0][:max_vocab_size]):\n            word_count_[i] = word_count[t]\n            vocab[t] = i\n        word_count = word_count_\n        rev_vocab = dict(((wrd_id, wrd) for (wrd, wrd_id) in vocab.items()))\n        neon_logger.display('vocabulary from {} is saved into {}'.format(path, vocab_file_name))\n        save_obj((vocab, rev_vocab, word_count), vocab_file_name)\n    vocab_size = len(vocab)\n    neon_logger.display('\\nVocab size from the dataset is: {}'.format(vocab_size))\n    neon_logger.display('\\nProcessing and saving training data into {}'.format(h5_file_name))\n    h5f = h5py.File(h5_file_name, 'w', libver='latest')\n    (shape, maxshape) = ((len(train_sent),), None)\n    dt = np.dtype([('text', h5py.special_dtype(vlen=str)), ('num_words', np.uint16)])\n    report_text_train = h5f.create_dataset('report_train', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n    report_train = h5f.create_dataset('train', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n    wdata = np.zeros((1,), dtype=dt)\n    ntrain = 0\n    for sent in train_sent:\n        text_int = [-1 if t not in vocab else vocab[t] for t in tokenize(sent)]\n        if len(text_int) > max_len_w or len(text_int) == 0:\n            continue\n        report_train[ntrain] = text_int\n        wdata['text'] = clean_string(sent)\n        wdata['num_words'] = len(text_int)\n        report_text_train[ntrain] = wdata\n        ntrain += 1\n    report_train.attrs['nsample'] = ntrain\n    report_train.attrs['vocab_size'] = vocab_size\n    report_text_train.attrs['nsample'] = ntrain\n    report_text_train.attrs['vocab_size'] = vocab_size\n    if valid_split:\n        neon_logger.display('\\nProcessing and saving validation data into {}'.format(h5_file_name))\n        shape = (len(valid_sent),)\n        report_text_valid = h5f.create_dataset('report_valid', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n        report_valid = h5f.create_dataset('valid', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        nvalid = 0\n        for sent in valid_sent:\n            text_int = [-1 if t not in vocab else vocab[t] for t in tokenize(sent)]\n            if len(text_int) > max_len_w or len(text_int) == 0:\n                continue\n            report_valid[nvalid] = text_int\n            wdata['text'] = clean_string(sent)\n            wdata['num_words'] = len(text_int)\n            report_text_valid[nvalid] = wdata\n            nvalid += 1\n        report_valid.attrs['nsample'] = nvalid\n        report_valid.attrs['vocab_size'] = vocab_size\n        report_text_valid.attrs['nsample'] = nvalid\n        report_text_valid.attrs['vocab_size'] = vocab_size\n    h5f.close()\n    return (h5_file_name, vocab_file_name)",
            "def load_data(path, file_ext=['txt'], valid_split=None, vocab_file_name=None, max_vocab_size=None, max_len_w=None, output_path=None, subset_pct=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Given a path where data are saved, look for the ones with the right extensions\\n    If a split factor is given, it will split all the files into training and valid\\n    set. Then build vocabulary from the training and validation sets.\\n\\n    Arguments:\\n        path: which directory to look for all the documents\\n        file_ext: what extension of the files to look for\\n        valid_split: to split the data into train/valid set. If None, no split\\n        vocab_file_name: optional file name. If None, the script will decide a name\\n                         given path and split\\n        max_vocab_size: maximum number of words to use in vocabulary (by most frequent)\\n        max_len_w: maximum length of sentences in words\\n        output_path: path used to save preprocessed data and resuts\\n        subset_pct: subset of dataset to load into H5 file (percentage)\\n\\n    Returns:\\n        The function saves 2 files:\\n        h5 file with preprocessed data\\n        vocabulary file with: vocab, reverse_vocab, word_count\\n    '\n    file_names = get_file_list(path, file_ext)\n    file_str = get_file_str(path, len(file_names), labelled=False, valid_split=valid_split, subset_pct=subset_pct)\n    if not os.path.isdir(output_path):\n        os.makedirs(output_path)\n    if vocab_file_name is None:\n        vocab_file_name = file_str + '.vocab'\n        vocab_file_name = os.path.join(output_path, vocab_file_name)\n    if not max_len_w:\n        max_len_w = sys.maxsize\n    if not max_vocab_size:\n        max_vocab_size = sys.maxsize\n    h5_file_name = os.path.join(output_path, file_str + '.h5')\n    if os.path.exists(h5_file_name) and os.path.exists(vocab_file_name):\n        neon_logger.display('dataset files {} and vocabulary file {} already exist. will use cached data. '.format(h5_file_name, vocab_file_name))\n        return (h5_file_name, vocab_file_name)\n    if valid_split is not None:\n        if 'json' in file_ext:\n            train_split = int(np.ceil(len(file_names) * (1 - valid_split)))\n            train_files = file_names[:train_split]\n            valid_files = file_names[train_split:]\n            train_sent = load_json_sent(train_files, subset_pct)\n            valid_sent = load_json_sent(valid_files, subset_pct)\n            all_sent = train_sent + valid_sent\n        elif 'txt' in file_ext:\n            all_sent = load_txt_sent(file_names, subset_pct)\n            train_split = int(np.ceil(len(all_sent) * (1 - valid_split)))\n            train_sent = all_sent[:train_split]\n            valid_sent = all_sent[train_split:]\n        else:\n            neon_logger.display(\"Unsure how to load file_ext {}, please use 'json' or 'txt'.\".format(file_ext))\n    else:\n        train_files = file_names\n        if 'json' in file_ext:\n            train_sent = load_json_sent(train_files, subset_pct)\n        elif 'txt' in file_ext:\n            train_sent = load_txt_sent(train_files, subset_pct)\n        else:\n            neon_logger.display(\"Unsure how to load file_ext {}, please use 'json' or 'txt'.\".format(file_ext))\n        all_sent = train_sent\n    if os.path.exists(vocab_file_name):\n        neon_logger.display('open existing vocab file: {}'.format(vocab_file_name))\n        (vocab, rev_vocab, word_count) = load_obj(vocab_file_name)\n    else:\n        neon_logger.display('Building  vocab file')\n        word_count = defaultdict(int)\n        for sent in all_sent:\n            sent_words = tokenize(sent)\n            if len(sent_words) > max_len_w or len(sent_words) == 0:\n                continue\n            for word in sent_words:\n                word_count[word] += 1\n        vocab_sorted = sorted(word_count.items(), key=lambda kv: kv[1], reverse=True)\n        vocab = OrderedDict()\n        word_count_ = np.zeros((len(word_count),), dtype=np.int64)\n        for (i, t) in enumerate(list(zip(*vocab_sorted))[0][:max_vocab_size]):\n            word_count_[i] = word_count[t]\n            vocab[t] = i\n        word_count = word_count_\n        rev_vocab = dict(((wrd_id, wrd) for (wrd, wrd_id) in vocab.items()))\n        neon_logger.display('vocabulary from {} is saved into {}'.format(path, vocab_file_name))\n        save_obj((vocab, rev_vocab, word_count), vocab_file_name)\n    vocab_size = len(vocab)\n    neon_logger.display('\\nVocab size from the dataset is: {}'.format(vocab_size))\n    neon_logger.display('\\nProcessing and saving training data into {}'.format(h5_file_name))\n    h5f = h5py.File(h5_file_name, 'w', libver='latest')\n    (shape, maxshape) = ((len(train_sent),), None)\n    dt = np.dtype([('text', h5py.special_dtype(vlen=str)), ('num_words', np.uint16)])\n    report_text_train = h5f.create_dataset('report_train', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n    report_train = h5f.create_dataset('train', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n    wdata = np.zeros((1,), dtype=dt)\n    ntrain = 0\n    for sent in train_sent:\n        text_int = [-1 if t not in vocab else vocab[t] for t in tokenize(sent)]\n        if len(text_int) > max_len_w or len(text_int) == 0:\n            continue\n        report_train[ntrain] = text_int\n        wdata['text'] = clean_string(sent)\n        wdata['num_words'] = len(text_int)\n        report_text_train[ntrain] = wdata\n        ntrain += 1\n    report_train.attrs['nsample'] = ntrain\n    report_train.attrs['vocab_size'] = vocab_size\n    report_text_train.attrs['nsample'] = ntrain\n    report_text_train.attrs['vocab_size'] = vocab_size\n    if valid_split:\n        neon_logger.display('\\nProcessing and saving validation data into {}'.format(h5_file_name))\n        shape = (len(valid_sent),)\n        report_text_valid = h5f.create_dataset('report_valid', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n        report_valid = h5f.create_dataset('valid', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        nvalid = 0\n        for sent in valid_sent:\n            text_int = [-1 if t not in vocab else vocab[t] for t in tokenize(sent)]\n            if len(text_int) > max_len_w or len(text_int) == 0:\n                continue\n            report_valid[nvalid] = text_int\n            wdata['text'] = clean_string(sent)\n            wdata['num_words'] = len(text_int)\n            report_text_valid[nvalid] = wdata\n            nvalid += 1\n        report_valid.attrs['nsample'] = nvalid\n        report_valid.attrs['vocab_size'] = vocab_size\n        report_text_valid.attrs['nsample'] = nvalid\n        report_text_valid.attrs['vocab_size'] = vocab_size\n    h5f.close()\n    return (h5_file_name, vocab_file_name)",
            "def load_data(path, file_ext=['txt'], valid_split=None, vocab_file_name=None, max_vocab_size=None, max_len_w=None, output_path=None, subset_pct=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Given a path where data are saved, look for the ones with the right extensions\\n    If a split factor is given, it will split all the files into training and valid\\n    set. Then build vocabulary from the training and validation sets.\\n\\n    Arguments:\\n        path: which directory to look for all the documents\\n        file_ext: what extension of the files to look for\\n        valid_split: to split the data into train/valid set. If None, no split\\n        vocab_file_name: optional file name. If None, the script will decide a name\\n                         given path and split\\n        max_vocab_size: maximum number of words to use in vocabulary (by most frequent)\\n        max_len_w: maximum length of sentences in words\\n        output_path: path used to save preprocessed data and resuts\\n        subset_pct: subset of dataset to load into H5 file (percentage)\\n\\n    Returns:\\n        The function saves 2 files:\\n        h5 file with preprocessed data\\n        vocabulary file with: vocab, reverse_vocab, word_count\\n    '\n    file_names = get_file_list(path, file_ext)\n    file_str = get_file_str(path, len(file_names), labelled=False, valid_split=valid_split, subset_pct=subset_pct)\n    if not os.path.isdir(output_path):\n        os.makedirs(output_path)\n    if vocab_file_name is None:\n        vocab_file_name = file_str + '.vocab'\n        vocab_file_name = os.path.join(output_path, vocab_file_name)\n    if not max_len_w:\n        max_len_w = sys.maxsize\n    if not max_vocab_size:\n        max_vocab_size = sys.maxsize\n    h5_file_name = os.path.join(output_path, file_str + '.h5')\n    if os.path.exists(h5_file_name) and os.path.exists(vocab_file_name):\n        neon_logger.display('dataset files {} and vocabulary file {} already exist. will use cached data. '.format(h5_file_name, vocab_file_name))\n        return (h5_file_name, vocab_file_name)\n    if valid_split is not None:\n        if 'json' in file_ext:\n            train_split = int(np.ceil(len(file_names) * (1 - valid_split)))\n            train_files = file_names[:train_split]\n            valid_files = file_names[train_split:]\n            train_sent = load_json_sent(train_files, subset_pct)\n            valid_sent = load_json_sent(valid_files, subset_pct)\n            all_sent = train_sent + valid_sent\n        elif 'txt' in file_ext:\n            all_sent = load_txt_sent(file_names, subset_pct)\n            train_split = int(np.ceil(len(all_sent) * (1 - valid_split)))\n            train_sent = all_sent[:train_split]\n            valid_sent = all_sent[train_split:]\n        else:\n            neon_logger.display(\"Unsure how to load file_ext {}, please use 'json' or 'txt'.\".format(file_ext))\n    else:\n        train_files = file_names\n        if 'json' in file_ext:\n            train_sent = load_json_sent(train_files, subset_pct)\n        elif 'txt' in file_ext:\n            train_sent = load_txt_sent(train_files, subset_pct)\n        else:\n            neon_logger.display(\"Unsure how to load file_ext {}, please use 'json' or 'txt'.\".format(file_ext))\n        all_sent = train_sent\n    if os.path.exists(vocab_file_name):\n        neon_logger.display('open existing vocab file: {}'.format(vocab_file_name))\n        (vocab, rev_vocab, word_count) = load_obj(vocab_file_name)\n    else:\n        neon_logger.display('Building  vocab file')\n        word_count = defaultdict(int)\n        for sent in all_sent:\n            sent_words = tokenize(sent)\n            if len(sent_words) > max_len_w or len(sent_words) == 0:\n                continue\n            for word in sent_words:\n                word_count[word] += 1\n        vocab_sorted = sorted(word_count.items(), key=lambda kv: kv[1], reverse=True)\n        vocab = OrderedDict()\n        word_count_ = np.zeros((len(word_count),), dtype=np.int64)\n        for (i, t) in enumerate(list(zip(*vocab_sorted))[0][:max_vocab_size]):\n            word_count_[i] = word_count[t]\n            vocab[t] = i\n        word_count = word_count_\n        rev_vocab = dict(((wrd_id, wrd) for (wrd, wrd_id) in vocab.items()))\n        neon_logger.display('vocabulary from {} is saved into {}'.format(path, vocab_file_name))\n        save_obj((vocab, rev_vocab, word_count), vocab_file_name)\n    vocab_size = len(vocab)\n    neon_logger.display('\\nVocab size from the dataset is: {}'.format(vocab_size))\n    neon_logger.display('\\nProcessing and saving training data into {}'.format(h5_file_name))\n    h5f = h5py.File(h5_file_name, 'w', libver='latest')\n    (shape, maxshape) = ((len(train_sent),), None)\n    dt = np.dtype([('text', h5py.special_dtype(vlen=str)), ('num_words', np.uint16)])\n    report_text_train = h5f.create_dataset('report_train', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n    report_train = h5f.create_dataset('train', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n    wdata = np.zeros((1,), dtype=dt)\n    ntrain = 0\n    for sent in train_sent:\n        text_int = [-1 if t not in vocab else vocab[t] for t in tokenize(sent)]\n        if len(text_int) > max_len_w or len(text_int) == 0:\n            continue\n        report_train[ntrain] = text_int\n        wdata['text'] = clean_string(sent)\n        wdata['num_words'] = len(text_int)\n        report_text_train[ntrain] = wdata\n        ntrain += 1\n    report_train.attrs['nsample'] = ntrain\n    report_train.attrs['vocab_size'] = vocab_size\n    report_text_train.attrs['nsample'] = ntrain\n    report_text_train.attrs['vocab_size'] = vocab_size\n    if valid_split:\n        neon_logger.display('\\nProcessing and saving validation data into {}'.format(h5_file_name))\n        shape = (len(valid_sent),)\n        report_text_valid = h5f.create_dataset('report_valid', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n        report_valid = h5f.create_dataset('valid', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        nvalid = 0\n        for sent in valid_sent:\n            text_int = [-1 if t not in vocab else vocab[t] for t in tokenize(sent)]\n            if len(text_int) > max_len_w or len(text_int) == 0:\n                continue\n            report_valid[nvalid] = text_int\n            wdata['text'] = clean_string(sent)\n            wdata['num_words'] = len(text_int)\n            report_text_valid[nvalid] = wdata\n            nvalid += 1\n        report_valid.attrs['nsample'] = nvalid\n        report_valid.attrs['vocab_size'] = vocab_size\n        report_text_valid.attrs['nsample'] = nvalid\n        report_text_valid.attrs['vocab_size'] = vocab_size\n    h5f.close()\n    return (h5_file_name, vocab_file_name)",
            "def load_data(path, file_ext=['txt'], valid_split=None, vocab_file_name=None, max_vocab_size=None, max_len_w=None, output_path=None, subset_pct=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Given a path where data are saved, look for the ones with the right extensions\\n    If a split factor is given, it will split all the files into training and valid\\n    set. Then build vocabulary from the training and validation sets.\\n\\n    Arguments:\\n        path: which directory to look for all the documents\\n        file_ext: what extension of the files to look for\\n        valid_split: to split the data into train/valid set. If None, no split\\n        vocab_file_name: optional file name. If None, the script will decide a name\\n                         given path and split\\n        max_vocab_size: maximum number of words to use in vocabulary (by most frequent)\\n        max_len_w: maximum length of sentences in words\\n        output_path: path used to save preprocessed data and resuts\\n        subset_pct: subset of dataset to load into H5 file (percentage)\\n\\n    Returns:\\n        The function saves 2 files:\\n        h5 file with preprocessed data\\n        vocabulary file with: vocab, reverse_vocab, word_count\\n    '\n    file_names = get_file_list(path, file_ext)\n    file_str = get_file_str(path, len(file_names), labelled=False, valid_split=valid_split, subset_pct=subset_pct)\n    if not os.path.isdir(output_path):\n        os.makedirs(output_path)\n    if vocab_file_name is None:\n        vocab_file_name = file_str + '.vocab'\n        vocab_file_name = os.path.join(output_path, vocab_file_name)\n    if not max_len_w:\n        max_len_w = sys.maxsize\n    if not max_vocab_size:\n        max_vocab_size = sys.maxsize\n    h5_file_name = os.path.join(output_path, file_str + '.h5')\n    if os.path.exists(h5_file_name) and os.path.exists(vocab_file_name):\n        neon_logger.display('dataset files {} and vocabulary file {} already exist. will use cached data. '.format(h5_file_name, vocab_file_name))\n        return (h5_file_name, vocab_file_name)\n    if valid_split is not None:\n        if 'json' in file_ext:\n            train_split = int(np.ceil(len(file_names) * (1 - valid_split)))\n            train_files = file_names[:train_split]\n            valid_files = file_names[train_split:]\n            train_sent = load_json_sent(train_files, subset_pct)\n            valid_sent = load_json_sent(valid_files, subset_pct)\n            all_sent = train_sent + valid_sent\n        elif 'txt' in file_ext:\n            all_sent = load_txt_sent(file_names, subset_pct)\n            train_split = int(np.ceil(len(all_sent) * (1 - valid_split)))\n            train_sent = all_sent[:train_split]\n            valid_sent = all_sent[train_split:]\n        else:\n            neon_logger.display(\"Unsure how to load file_ext {}, please use 'json' or 'txt'.\".format(file_ext))\n    else:\n        train_files = file_names\n        if 'json' in file_ext:\n            train_sent = load_json_sent(train_files, subset_pct)\n        elif 'txt' in file_ext:\n            train_sent = load_txt_sent(train_files, subset_pct)\n        else:\n            neon_logger.display(\"Unsure how to load file_ext {}, please use 'json' or 'txt'.\".format(file_ext))\n        all_sent = train_sent\n    if os.path.exists(vocab_file_name):\n        neon_logger.display('open existing vocab file: {}'.format(vocab_file_name))\n        (vocab, rev_vocab, word_count) = load_obj(vocab_file_name)\n    else:\n        neon_logger.display('Building  vocab file')\n        word_count = defaultdict(int)\n        for sent in all_sent:\n            sent_words = tokenize(sent)\n            if len(sent_words) > max_len_w or len(sent_words) == 0:\n                continue\n            for word in sent_words:\n                word_count[word] += 1\n        vocab_sorted = sorted(word_count.items(), key=lambda kv: kv[1], reverse=True)\n        vocab = OrderedDict()\n        word_count_ = np.zeros((len(word_count),), dtype=np.int64)\n        for (i, t) in enumerate(list(zip(*vocab_sorted))[0][:max_vocab_size]):\n            word_count_[i] = word_count[t]\n            vocab[t] = i\n        word_count = word_count_\n        rev_vocab = dict(((wrd_id, wrd) for (wrd, wrd_id) in vocab.items()))\n        neon_logger.display('vocabulary from {} is saved into {}'.format(path, vocab_file_name))\n        save_obj((vocab, rev_vocab, word_count), vocab_file_name)\n    vocab_size = len(vocab)\n    neon_logger.display('\\nVocab size from the dataset is: {}'.format(vocab_size))\n    neon_logger.display('\\nProcessing and saving training data into {}'.format(h5_file_name))\n    h5f = h5py.File(h5_file_name, 'w', libver='latest')\n    (shape, maxshape) = ((len(train_sent),), None)\n    dt = np.dtype([('text', h5py.special_dtype(vlen=str)), ('num_words', np.uint16)])\n    report_text_train = h5f.create_dataset('report_train', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n    report_train = h5f.create_dataset('train', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n    wdata = np.zeros((1,), dtype=dt)\n    ntrain = 0\n    for sent in train_sent:\n        text_int = [-1 if t not in vocab else vocab[t] for t in tokenize(sent)]\n        if len(text_int) > max_len_w or len(text_int) == 0:\n            continue\n        report_train[ntrain] = text_int\n        wdata['text'] = clean_string(sent)\n        wdata['num_words'] = len(text_int)\n        report_text_train[ntrain] = wdata\n        ntrain += 1\n    report_train.attrs['nsample'] = ntrain\n    report_train.attrs['vocab_size'] = vocab_size\n    report_text_train.attrs['nsample'] = ntrain\n    report_text_train.attrs['vocab_size'] = vocab_size\n    if valid_split:\n        neon_logger.display('\\nProcessing and saving validation data into {}'.format(h5_file_name))\n        shape = (len(valid_sent),)\n        report_text_valid = h5f.create_dataset('report_valid', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n        report_valid = h5f.create_dataset('valid', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        nvalid = 0\n        for sent in valid_sent:\n            text_int = [-1 if t not in vocab else vocab[t] for t in tokenize(sent)]\n            if len(text_int) > max_len_w or len(text_int) == 0:\n                continue\n            report_valid[nvalid] = text_int\n            wdata['text'] = clean_string(sent)\n            wdata['num_words'] = len(text_int)\n            report_text_valid[nvalid] = wdata\n            nvalid += 1\n        report_valid.attrs['nsample'] = nvalid\n        report_valid.attrs['vocab_size'] = vocab_size\n        report_text_valid.attrs['nsample'] = nvalid\n        report_text_valid.attrs['vocab_size'] = vocab_size\n    h5f.close()\n    return (h5_file_name, vocab_file_name)",
            "def load_data(path, file_ext=['txt'], valid_split=None, vocab_file_name=None, max_vocab_size=None, max_len_w=None, output_path=None, subset_pct=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Given a path where data are saved, look for the ones with the right extensions\\n    If a split factor is given, it will split all the files into training and valid\\n    set. Then build vocabulary from the training and validation sets.\\n\\n    Arguments:\\n        path: which directory to look for all the documents\\n        file_ext: what extension of the files to look for\\n        valid_split: to split the data into train/valid set. If None, no split\\n        vocab_file_name: optional file name. If None, the script will decide a name\\n                         given path and split\\n        max_vocab_size: maximum number of words to use in vocabulary (by most frequent)\\n        max_len_w: maximum length of sentences in words\\n        output_path: path used to save preprocessed data and resuts\\n        subset_pct: subset of dataset to load into H5 file (percentage)\\n\\n    Returns:\\n        The function saves 2 files:\\n        h5 file with preprocessed data\\n        vocabulary file with: vocab, reverse_vocab, word_count\\n    '\n    file_names = get_file_list(path, file_ext)\n    file_str = get_file_str(path, len(file_names), labelled=False, valid_split=valid_split, subset_pct=subset_pct)\n    if not os.path.isdir(output_path):\n        os.makedirs(output_path)\n    if vocab_file_name is None:\n        vocab_file_name = file_str + '.vocab'\n        vocab_file_name = os.path.join(output_path, vocab_file_name)\n    if not max_len_w:\n        max_len_w = sys.maxsize\n    if not max_vocab_size:\n        max_vocab_size = sys.maxsize\n    h5_file_name = os.path.join(output_path, file_str + '.h5')\n    if os.path.exists(h5_file_name) and os.path.exists(vocab_file_name):\n        neon_logger.display('dataset files {} and vocabulary file {} already exist. will use cached data. '.format(h5_file_name, vocab_file_name))\n        return (h5_file_name, vocab_file_name)\n    if valid_split is not None:\n        if 'json' in file_ext:\n            train_split = int(np.ceil(len(file_names) * (1 - valid_split)))\n            train_files = file_names[:train_split]\n            valid_files = file_names[train_split:]\n            train_sent = load_json_sent(train_files, subset_pct)\n            valid_sent = load_json_sent(valid_files, subset_pct)\n            all_sent = train_sent + valid_sent\n        elif 'txt' in file_ext:\n            all_sent = load_txt_sent(file_names, subset_pct)\n            train_split = int(np.ceil(len(all_sent) * (1 - valid_split)))\n            train_sent = all_sent[:train_split]\n            valid_sent = all_sent[train_split:]\n        else:\n            neon_logger.display(\"Unsure how to load file_ext {}, please use 'json' or 'txt'.\".format(file_ext))\n    else:\n        train_files = file_names\n        if 'json' in file_ext:\n            train_sent = load_json_sent(train_files, subset_pct)\n        elif 'txt' in file_ext:\n            train_sent = load_txt_sent(train_files, subset_pct)\n        else:\n            neon_logger.display(\"Unsure how to load file_ext {}, please use 'json' or 'txt'.\".format(file_ext))\n        all_sent = train_sent\n    if os.path.exists(vocab_file_name):\n        neon_logger.display('open existing vocab file: {}'.format(vocab_file_name))\n        (vocab, rev_vocab, word_count) = load_obj(vocab_file_name)\n    else:\n        neon_logger.display('Building  vocab file')\n        word_count = defaultdict(int)\n        for sent in all_sent:\n            sent_words = tokenize(sent)\n            if len(sent_words) > max_len_w or len(sent_words) == 0:\n                continue\n            for word in sent_words:\n                word_count[word] += 1\n        vocab_sorted = sorted(word_count.items(), key=lambda kv: kv[1], reverse=True)\n        vocab = OrderedDict()\n        word_count_ = np.zeros((len(word_count),), dtype=np.int64)\n        for (i, t) in enumerate(list(zip(*vocab_sorted))[0][:max_vocab_size]):\n            word_count_[i] = word_count[t]\n            vocab[t] = i\n        word_count = word_count_\n        rev_vocab = dict(((wrd_id, wrd) for (wrd, wrd_id) in vocab.items()))\n        neon_logger.display('vocabulary from {} is saved into {}'.format(path, vocab_file_name))\n        save_obj((vocab, rev_vocab, word_count), vocab_file_name)\n    vocab_size = len(vocab)\n    neon_logger.display('\\nVocab size from the dataset is: {}'.format(vocab_size))\n    neon_logger.display('\\nProcessing and saving training data into {}'.format(h5_file_name))\n    h5f = h5py.File(h5_file_name, 'w', libver='latest')\n    (shape, maxshape) = ((len(train_sent),), None)\n    dt = np.dtype([('text', h5py.special_dtype(vlen=str)), ('num_words', np.uint16)])\n    report_text_train = h5f.create_dataset('report_train', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n    report_train = h5f.create_dataset('train', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n    wdata = np.zeros((1,), dtype=dt)\n    ntrain = 0\n    for sent in train_sent:\n        text_int = [-1 if t not in vocab else vocab[t] for t in tokenize(sent)]\n        if len(text_int) > max_len_w or len(text_int) == 0:\n            continue\n        report_train[ntrain] = text_int\n        wdata['text'] = clean_string(sent)\n        wdata['num_words'] = len(text_int)\n        report_text_train[ntrain] = wdata\n        ntrain += 1\n    report_train.attrs['nsample'] = ntrain\n    report_train.attrs['vocab_size'] = vocab_size\n    report_text_train.attrs['nsample'] = ntrain\n    report_text_train.attrs['vocab_size'] = vocab_size\n    if valid_split:\n        neon_logger.display('\\nProcessing and saving validation data into {}'.format(h5_file_name))\n        shape = (len(valid_sent),)\n        report_text_valid = h5f.create_dataset('report_valid', shape=shape, maxshape=maxshape, dtype=dt, compression='gzip')\n        report_valid = h5f.create_dataset('valid', shape=shape, maxshape=maxshape, dtype=h5py.special_dtype(vlen=np.int32), compression='gzip')\n        nvalid = 0\n        for sent in valid_sent:\n            text_int = [-1 if t not in vocab else vocab[t] for t in tokenize(sent)]\n            if len(text_int) > max_len_w or len(text_int) == 0:\n                continue\n            report_valid[nvalid] = text_int\n            wdata['text'] = clean_string(sent)\n            wdata['num_words'] = len(text_int)\n            report_text_valid[nvalid] = wdata\n            nvalid += 1\n        report_valid.attrs['nsample'] = nvalid\n        report_valid.attrs['vocab_size'] = vocab_size\n        report_text_valid.attrs['nsample'] = nvalid\n        report_text_valid.attrs['vocab_size'] = vocab_size\n    h5f.close()\n    return (h5_file_name, vocab_file_name)"
        ]
    }
]