[
    {
        "func_name": "__init__",
        "original": "def __init__(self, corpus=None, time_slice=None, id2word=None, alphas=0.01, num_topics=10, initialize='gensim', sstats=None, lda_model=None, obs_variance=0.5, chain_variance=0.005, passes=10, random_state=None, lda_inference_max_iter=25, em_min_iter=6, em_max_iter=20, chunksize=100):\n    \"\"\"\n\n        Parameters\n        ----------\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\n            If not given, the model is left untrained (presumably because you want to call\n            :meth:`~gensim.models.ldamodel.LdaSeqModel.update` manually).\n        time_slice : list of int, optional\n            Number of documents in each time-slice. Each time slice could for example represent a year's published\n            papers, in case the corpus comes from a journal publishing over multiple years.\n            It is assumed that `sum(time_slice) == num_documents`.\n        id2word : dict of (int, str), optional\n            Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for\n            debugging and topic printing.\n        alphas : float, optional\n            The prior probability for the model.\n        num_topics : int, optional\n            The number of requested latent topics to be extracted from the training corpus.\n        initialize : {'gensim', 'own', 'ldamodel'}, optional\n            Controls the initialization of the DTM model. Supports three different modes:\n                * 'gensim': Uses gensim's LDA initialization.\n                * 'own': Uses your own initialization matrix of an LDA model that has been previously trained.\n                * 'lda_model': Use a previously used LDA model, passing it through the `lda_model` argument.\n        sstats : numpy.ndarray , optional\n            Sufficient statistics used for initializing the model if `initialize == 'own'`. Corresponds to matrix\n            beta in the linked paper for time slice 0, expected shape (`self.vocab_len`, `num_topics`).\n        lda_model : :class:`~gensim.models.ldamodel.LdaModel`\n            Model whose sufficient statistics will be used to initialize the current object if `initialize == 'gensim'`.\n        obs_variance : float, optional\n            Observed variance used to approximate the true and forward variance as shown in\n            `David M. Blei, John D. Lafferty: \"Dynamic Topic Models\"\n            <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\n        chain_variance : float, optional\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\n        passes : int, optional\n            Number of passes over the corpus for the initial :class:`~gensim.models.ldamodel.LdaModel`\n        random_state : {numpy.random.RandomState, int}, optional\n            Can be a np.random.RandomState object, or the seed to generate one. Used for reproducibility of results.\n        lda_inference_max_iter : int, optional\n            Maximum number of iterations in the inference step of the LDA training.\n        em_min_iter : int, optional\n            Minimum number of iterations until converge of the Expectation-Maximization algorithm\n        em_max_iter : int, optional\n            Maximum number of iterations until converge of the Expectation-Maximization algorithm.\n        chunksize : int, optional\n            Number of documents in the corpus do be processed in in a chunk.\n\n        \"\"\"\n    self.id2word = id2word\n    if corpus is None and self.id2word is None:\n        raise ValueError('at least one of corpus/id2word must be specified, to establish input space dimensionality')\n    if self.id2word is None:\n        logger.warning('no word id mapping provided; initializing from corpus, assuming identity')\n        self.id2word = utils.dict_from_corpus(corpus)\n        self.vocab_len = len(self.id2word)\n    elif self.id2word:\n        self.vocab_len = len(self.id2word)\n    else:\n        self.vocab_len = 0\n    if corpus is not None:\n        try:\n            self.corpus_len = len(corpus)\n        except TypeError:\n            logger.warning('input corpus stream has no len(); counting documents')\n            self.corpus_len = sum((1 for _ in corpus))\n    self.time_slice = time_slice\n    if self.time_slice is not None:\n        self.num_time_slices = len(time_slice)\n    self.num_topics = num_topics\n    self.num_time_slices = len(time_slice)\n    self.alphas = np.full(num_topics, alphas)\n    self.topic_chains = []\n    for topic in range(num_topics):\n        sslm_ = sslm(num_time_slices=self.num_time_slices, vocab_len=self.vocab_len, num_topics=self.num_topics, chain_variance=chain_variance, obs_variance=obs_variance)\n        self.topic_chains.append(sslm_)\n    self.top_doc_phis = None\n    self.influence = None\n    self.renormalized_influence = None\n    self.influence_sum_lgl = None\n    if corpus is not None and time_slice is not None:\n        self.max_doc_len = max((len(line) for line in corpus))\n        if initialize == 'gensim':\n            lda_model = ldamodel.LdaModel(corpus, id2word=self.id2word, num_topics=self.num_topics, passes=passes, alpha=self.alphas, random_state=random_state, dtype=np.float64)\n            self.sstats = np.transpose(lda_model.state.sstats)\n        if initialize == 'ldamodel':\n            self.sstats = np.transpose(lda_model.state.sstats)\n        if initialize == 'own':\n            self.sstats = sstats\n        self.init_ldaseq_ss(chain_variance, obs_variance, self.alphas, self.sstats)\n        self.fit_lda_seq(corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)",
        "mutated": [
            "def __init__(self, corpus=None, time_slice=None, id2word=None, alphas=0.01, num_topics=10, initialize='gensim', sstats=None, lda_model=None, obs_variance=0.5, chain_variance=0.005, passes=10, random_state=None, lda_inference_max_iter=25, em_min_iter=6, em_max_iter=20, chunksize=100):\n    if False:\n        i = 10\n    '\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n            If not given, the model is left untrained (presumably because you want to call\\n            :meth:`~gensim.models.ldamodel.LdaSeqModel.update` manually).\\n        time_slice : list of int, optional\\n            Number of documents in each time-slice. Each time slice could for example represent a year\\'s published\\n            papers, in case the corpus comes from a journal publishing over multiple years.\\n            It is assumed that `sum(time_slice) == num_documents`.\\n        id2word : dict of (int, str), optional\\n            Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for\\n            debugging and topic printing.\\n        alphas : float, optional\\n            The prior probability for the model.\\n        num_topics : int, optional\\n            The number of requested latent topics to be extracted from the training corpus.\\n        initialize : {\\'gensim\\', \\'own\\', \\'ldamodel\\'}, optional\\n            Controls the initialization of the DTM model. Supports three different modes:\\n                * \\'gensim\\': Uses gensim\\'s LDA initialization.\\n                * \\'own\\': Uses your own initialization matrix of an LDA model that has been previously trained.\\n                * \\'lda_model\\': Use a previously used LDA model, passing it through the `lda_model` argument.\\n        sstats : numpy.ndarray , optional\\n            Sufficient statistics used for initializing the model if `initialize == \\'own\\'`. Corresponds to matrix\\n            beta in the linked paper for time slice 0, expected shape (`self.vocab_len`, `num_topics`).\\n        lda_model : :class:`~gensim.models.ldamodel.LdaModel`\\n            Model whose sufficient statistics will be used to initialize the current object if `initialize == \\'gensim\\'`.\\n        obs_variance : float, optional\\n            Observed variance used to approximate the true and forward variance as shown in\\n            `David M. Blei, John D. Lafferty: \"Dynamic Topic Models\"\\n            <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n        chain_variance : float, optional\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n        passes : int, optional\\n            Number of passes over the corpus for the initial :class:`~gensim.models.ldamodel.LdaModel`\\n        random_state : {numpy.random.RandomState, int}, optional\\n            Can be a np.random.RandomState object, or the seed to generate one. Used for reproducibility of results.\\n        lda_inference_max_iter : int, optional\\n            Maximum number of iterations in the inference step of the LDA training.\\n        em_min_iter : int, optional\\n            Minimum number of iterations until converge of the Expectation-Maximization algorithm\\n        em_max_iter : int, optional\\n            Maximum number of iterations until converge of the Expectation-Maximization algorithm.\\n        chunksize : int, optional\\n            Number of documents in the corpus do be processed in in a chunk.\\n\\n        '\n    self.id2word = id2word\n    if corpus is None and self.id2word is None:\n        raise ValueError('at least one of corpus/id2word must be specified, to establish input space dimensionality')\n    if self.id2word is None:\n        logger.warning('no word id mapping provided; initializing from corpus, assuming identity')\n        self.id2word = utils.dict_from_corpus(corpus)\n        self.vocab_len = len(self.id2word)\n    elif self.id2word:\n        self.vocab_len = len(self.id2word)\n    else:\n        self.vocab_len = 0\n    if corpus is not None:\n        try:\n            self.corpus_len = len(corpus)\n        except TypeError:\n            logger.warning('input corpus stream has no len(); counting documents')\n            self.corpus_len = sum((1 for _ in corpus))\n    self.time_slice = time_slice\n    if self.time_slice is not None:\n        self.num_time_slices = len(time_slice)\n    self.num_topics = num_topics\n    self.num_time_slices = len(time_slice)\n    self.alphas = np.full(num_topics, alphas)\n    self.topic_chains = []\n    for topic in range(num_topics):\n        sslm_ = sslm(num_time_slices=self.num_time_slices, vocab_len=self.vocab_len, num_topics=self.num_topics, chain_variance=chain_variance, obs_variance=obs_variance)\n        self.topic_chains.append(sslm_)\n    self.top_doc_phis = None\n    self.influence = None\n    self.renormalized_influence = None\n    self.influence_sum_lgl = None\n    if corpus is not None and time_slice is not None:\n        self.max_doc_len = max((len(line) for line in corpus))\n        if initialize == 'gensim':\n            lda_model = ldamodel.LdaModel(corpus, id2word=self.id2word, num_topics=self.num_topics, passes=passes, alpha=self.alphas, random_state=random_state, dtype=np.float64)\n            self.sstats = np.transpose(lda_model.state.sstats)\n        if initialize == 'ldamodel':\n            self.sstats = np.transpose(lda_model.state.sstats)\n        if initialize == 'own':\n            self.sstats = sstats\n        self.init_ldaseq_ss(chain_variance, obs_variance, self.alphas, self.sstats)\n        self.fit_lda_seq(corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)",
            "def __init__(self, corpus=None, time_slice=None, id2word=None, alphas=0.01, num_topics=10, initialize='gensim', sstats=None, lda_model=None, obs_variance=0.5, chain_variance=0.005, passes=10, random_state=None, lda_inference_max_iter=25, em_min_iter=6, em_max_iter=20, chunksize=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n            If not given, the model is left untrained (presumably because you want to call\\n            :meth:`~gensim.models.ldamodel.LdaSeqModel.update` manually).\\n        time_slice : list of int, optional\\n            Number of documents in each time-slice. Each time slice could for example represent a year\\'s published\\n            papers, in case the corpus comes from a journal publishing over multiple years.\\n            It is assumed that `sum(time_slice) == num_documents`.\\n        id2word : dict of (int, str), optional\\n            Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for\\n            debugging and topic printing.\\n        alphas : float, optional\\n            The prior probability for the model.\\n        num_topics : int, optional\\n            The number of requested latent topics to be extracted from the training corpus.\\n        initialize : {\\'gensim\\', \\'own\\', \\'ldamodel\\'}, optional\\n            Controls the initialization of the DTM model. Supports three different modes:\\n                * \\'gensim\\': Uses gensim\\'s LDA initialization.\\n                * \\'own\\': Uses your own initialization matrix of an LDA model that has been previously trained.\\n                * \\'lda_model\\': Use a previously used LDA model, passing it through the `lda_model` argument.\\n        sstats : numpy.ndarray , optional\\n            Sufficient statistics used for initializing the model if `initialize == \\'own\\'`. Corresponds to matrix\\n            beta in the linked paper for time slice 0, expected shape (`self.vocab_len`, `num_topics`).\\n        lda_model : :class:`~gensim.models.ldamodel.LdaModel`\\n            Model whose sufficient statistics will be used to initialize the current object if `initialize == \\'gensim\\'`.\\n        obs_variance : float, optional\\n            Observed variance used to approximate the true and forward variance as shown in\\n            `David M. Blei, John D. Lafferty: \"Dynamic Topic Models\"\\n            <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n        chain_variance : float, optional\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n        passes : int, optional\\n            Number of passes over the corpus for the initial :class:`~gensim.models.ldamodel.LdaModel`\\n        random_state : {numpy.random.RandomState, int}, optional\\n            Can be a np.random.RandomState object, or the seed to generate one. Used for reproducibility of results.\\n        lda_inference_max_iter : int, optional\\n            Maximum number of iterations in the inference step of the LDA training.\\n        em_min_iter : int, optional\\n            Minimum number of iterations until converge of the Expectation-Maximization algorithm\\n        em_max_iter : int, optional\\n            Maximum number of iterations until converge of the Expectation-Maximization algorithm.\\n        chunksize : int, optional\\n            Number of documents in the corpus do be processed in in a chunk.\\n\\n        '\n    self.id2word = id2word\n    if corpus is None and self.id2word is None:\n        raise ValueError('at least one of corpus/id2word must be specified, to establish input space dimensionality')\n    if self.id2word is None:\n        logger.warning('no word id mapping provided; initializing from corpus, assuming identity')\n        self.id2word = utils.dict_from_corpus(corpus)\n        self.vocab_len = len(self.id2word)\n    elif self.id2word:\n        self.vocab_len = len(self.id2word)\n    else:\n        self.vocab_len = 0\n    if corpus is not None:\n        try:\n            self.corpus_len = len(corpus)\n        except TypeError:\n            logger.warning('input corpus stream has no len(); counting documents')\n            self.corpus_len = sum((1 for _ in corpus))\n    self.time_slice = time_slice\n    if self.time_slice is not None:\n        self.num_time_slices = len(time_slice)\n    self.num_topics = num_topics\n    self.num_time_slices = len(time_slice)\n    self.alphas = np.full(num_topics, alphas)\n    self.topic_chains = []\n    for topic in range(num_topics):\n        sslm_ = sslm(num_time_slices=self.num_time_slices, vocab_len=self.vocab_len, num_topics=self.num_topics, chain_variance=chain_variance, obs_variance=obs_variance)\n        self.topic_chains.append(sslm_)\n    self.top_doc_phis = None\n    self.influence = None\n    self.renormalized_influence = None\n    self.influence_sum_lgl = None\n    if corpus is not None and time_slice is not None:\n        self.max_doc_len = max((len(line) for line in corpus))\n        if initialize == 'gensim':\n            lda_model = ldamodel.LdaModel(corpus, id2word=self.id2word, num_topics=self.num_topics, passes=passes, alpha=self.alphas, random_state=random_state, dtype=np.float64)\n            self.sstats = np.transpose(lda_model.state.sstats)\n        if initialize == 'ldamodel':\n            self.sstats = np.transpose(lda_model.state.sstats)\n        if initialize == 'own':\n            self.sstats = sstats\n        self.init_ldaseq_ss(chain_variance, obs_variance, self.alphas, self.sstats)\n        self.fit_lda_seq(corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)",
            "def __init__(self, corpus=None, time_slice=None, id2word=None, alphas=0.01, num_topics=10, initialize='gensim', sstats=None, lda_model=None, obs_variance=0.5, chain_variance=0.005, passes=10, random_state=None, lda_inference_max_iter=25, em_min_iter=6, em_max_iter=20, chunksize=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n            If not given, the model is left untrained (presumably because you want to call\\n            :meth:`~gensim.models.ldamodel.LdaSeqModel.update` manually).\\n        time_slice : list of int, optional\\n            Number of documents in each time-slice. Each time slice could for example represent a year\\'s published\\n            papers, in case the corpus comes from a journal publishing over multiple years.\\n            It is assumed that `sum(time_slice) == num_documents`.\\n        id2word : dict of (int, str), optional\\n            Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for\\n            debugging and topic printing.\\n        alphas : float, optional\\n            The prior probability for the model.\\n        num_topics : int, optional\\n            The number of requested latent topics to be extracted from the training corpus.\\n        initialize : {\\'gensim\\', \\'own\\', \\'ldamodel\\'}, optional\\n            Controls the initialization of the DTM model. Supports three different modes:\\n                * \\'gensim\\': Uses gensim\\'s LDA initialization.\\n                * \\'own\\': Uses your own initialization matrix of an LDA model that has been previously trained.\\n                * \\'lda_model\\': Use a previously used LDA model, passing it through the `lda_model` argument.\\n        sstats : numpy.ndarray , optional\\n            Sufficient statistics used for initializing the model if `initialize == \\'own\\'`. Corresponds to matrix\\n            beta in the linked paper for time slice 0, expected shape (`self.vocab_len`, `num_topics`).\\n        lda_model : :class:`~gensim.models.ldamodel.LdaModel`\\n            Model whose sufficient statistics will be used to initialize the current object if `initialize == \\'gensim\\'`.\\n        obs_variance : float, optional\\n            Observed variance used to approximate the true and forward variance as shown in\\n            `David M. Blei, John D. Lafferty: \"Dynamic Topic Models\"\\n            <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n        chain_variance : float, optional\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n        passes : int, optional\\n            Number of passes over the corpus for the initial :class:`~gensim.models.ldamodel.LdaModel`\\n        random_state : {numpy.random.RandomState, int}, optional\\n            Can be a np.random.RandomState object, or the seed to generate one. Used for reproducibility of results.\\n        lda_inference_max_iter : int, optional\\n            Maximum number of iterations in the inference step of the LDA training.\\n        em_min_iter : int, optional\\n            Minimum number of iterations until converge of the Expectation-Maximization algorithm\\n        em_max_iter : int, optional\\n            Maximum number of iterations until converge of the Expectation-Maximization algorithm.\\n        chunksize : int, optional\\n            Number of documents in the corpus do be processed in in a chunk.\\n\\n        '\n    self.id2word = id2word\n    if corpus is None and self.id2word is None:\n        raise ValueError('at least one of corpus/id2word must be specified, to establish input space dimensionality')\n    if self.id2word is None:\n        logger.warning('no word id mapping provided; initializing from corpus, assuming identity')\n        self.id2word = utils.dict_from_corpus(corpus)\n        self.vocab_len = len(self.id2word)\n    elif self.id2word:\n        self.vocab_len = len(self.id2word)\n    else:\n        self.vocab_len = 0\n    if corpus is not None:\n        try:\n            self.corpus_len = len(corpus)\n        except TypeError:\n            logger.warning('input corpus stream has no len(); counting documents')\n            self.corpus_len = sum((1 for _ in corpus))\n    self.time_slice = time_slice\n    if self.time_slice is not None:\n        self.num_time_slices = len(time_slice)\n    self.num_topics = num_topics\n    self.num_time_slices = len(time_slice)\n    self.alphas = np.full(num_topics, alphas)\n    self.topic_chains = []\n    for topic in range(num_topics):\n        sslm_ = sslm(num_time_slices=self.num_time_slices, vocab_len=self.vocab_len, num_topics=self.num_topics, chain_variance=chain_variance, obs_variance=obs_variance)\n        self.topic_chains.append(sslm_)\n    self.top_doc_phis = None\n    self.influence = None\n    self.renormalized_influence = None\n    self.influence_sum_lgl = None\n    if corpus is not None and time_slice is not None:\n        self.max_doc_len = max((len(line) for line in corpus))\n        if initialize == 'gensim':\n            lda_model = ldamodel.LdaModel(corpus, id2word=self.id2word, num_topics=self.num_topics, passes=passes, alpha=self.alphas, random_state=random_state, dtype=np.float64)\n            self.sstats = np.transpose(lda_model.state.sstats)\n        if initialize == 'ldamodel':\n            self.sstats = np.transpose(lda_model.state.sstats)\n        if initialize == 'own':\n            self.sstats = sstats\n        self.init_ldaseq_ss(chain_variance, obs_variance, self.alphas, self.sstats)\n        self.fit_lda_seq(corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)",
            "def __init__(self, corpus=None, time_slice=None, id2word=None, alphas=0.01, num_topics=10, initialize='gensim', sstats=None, lda_model=None, obs_variance=0.5, chain_variance=0.005, passes=10, random_state=None, lda_inference_max_iter=25, em_min_iter=6, em_max_iter=20, chunksize=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n            If not given, the model is left untrained (presumably because you want to call\\n            :meth:`~gensim.models.ldamodel.LdaSeqModel.update` manually).\\n        time_slice : list of int, optional\\n            Number of documents in each time-slice. Each time slice could for example represent a year\\'s published\\n            papers, in case the corpus comes from a journal publishing over multiple years.\\n            It is assumed that `sum(time_slice) == num_documents`.\\n        id2word : dict of (int, str), optional\\n            Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for\\n            debugging and topic printing.\\n        alphas : float, optional\\n            The prior probability for the model.\\n        num_topics : int, optional\\n            The number of requested latent topics to be extracted from the training corpus.\\n        initialize : {\\'gensim\\', \\'own\\', \\'ldamodel\\'}, optional\\n            Controls the initialization of the DTM model. Supports three different modes:\\n                * \\'gensim\\': Uses gensim\\'s LDA initialization.\\n                * \\'own\\': Uses your own initialization matrix of an LDA model that has been previously trained.\\n                * \\'lda_model\\': Use a previously used LDA model, passing it through the `lda_model` argument.\\n        sstats : numpy.ndarray , optional\\n            Sufficient statistics used for initializing the model if `initialize == \\'own\\'`. Corresponds to matrix\\n            beta in the linked paper for time slice 0, expected shape (`self.vocab_len`, `num_topics`).\\n        lda_model : :class:`~gensim.models.ldamodel.LdaModel`\\n            Model whose sufficient statistics will be used to initialize the current object if `initialize == \\'gensim\\'`.\\n        obs_variance : float, optional\\n            Observed variance used to approximate the true and forward variance as shown in\\n            `David M. Blei, John D. Lafferty: \"Dynamic Topic Models\"\\n            <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n        chain_variance : float, optional\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n        passes : int, optional\\n            Number of passes over the corpus for the initial :class:`~gensim.models.ldamodel.LdaModel`\\n        random_state : {numpy.random.RandomState, int}, optional\\n            Can be a np.random.RandomState object, or the seed to generate one. Used for reproducibility of results.\\n        lda_inference_max_iter : int, optional\\n            Maximum number of iterations in the inference step of the LDA training.\\n        em_min_iter : int, optional\\n            Minimum number of iterations until converge of the Expectation-Maximization algorithm\\n        em_max_iter : int, optional\\n            Maximum number of iterations until converge of the Expectation-Maximization algorithm.\\n        chunksize : int, optional\\n            Number of documents in the corpus do be processed in in a chunk.\\n\\n        '\n    self.id2word = id2word\n    if corpus is None and self.id2word is None:\n        raise ValueError('at least one of corpus/id2word must be specified, to establish input space dimensionality')\n    if self.id2word is None:\n        logger.warning('no word id mapping provided; initializing from corpus, assuming identity')\n        self.id2word = utils.dict_from_corpus(corpus)\n        self.vocab_len = len(self.id2word)\n    elif self.id2word:\n        self.vocab_len = len(self.id2word)\n    else:\n        self.vocab_len = 0\n    if corpus is not None:\n        try:\n            self.corpus_len = len(corpus)\n        except TypeError:\n            logger.warning('input corpus stream has no len(); counting documents')\n            self.corpus_len = sum((1 for _ in corpus))\n    self.time_slice = time_slice\n    if self.time_slice is not None:\n        self.num_time_slices = len(time_slice)\n    self.num_topics = num_topics\n    self.num_time_slices = len(time_slice)\n    self.alphas = np.full(num_topics, alphas)\n    self.topic_chains = []\n    for topic in range(num_topics):\n        sslm_ = sslm(num_time_slices=self.num_time_slices, vocab_len=self.vocab_len, num_topics=self.num_topics, chain_variance=chain_variance, obs_variance=obs_variance)\n        self.topic_chains.append(sslm_)\n    self.top_doc_phis = None\n    self.influence = None\n    self.renormalized_influence = None\n    self.influence_sum_lgl = None\n    if corpus is not None and time_slice is not None:\n        self.max_doc_len = max((len(line) for line in corpus))\n        if initialize == 'gensim':\n            lda_model = ldamodel.LdaModel(corpus, id2word=self.id2word, num_topics=self.num_topics, passes=passes, alpha=self.alphas, random_state=random_state, dtype=np.float64)\n            self.sstats = np.transpose(lda_model.state.sstats)\n        if initialize == 'ldamodel':\n            self.sstats = np.transpose(lda_model.state.sstats)\n        if initialize == 'own':\n            self.sstats = sstats\n        self.init_ldaseq_ss(chain_variance, obs_variance, self.alphas, self.sstats)\n        self.fit_lda_seq(corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)",
            "def __init__(self, corpus=None, time_slice=None, id2word=None, alphas=0.01, num_topics=10, initialize='gensim', sstats=None, lda_model=None, obs_variance=0.5, chain_variance=0.005, passes=10, random_state=None, lda_inference_max_iter=25, em_min_iter=6, em_max_iter=20, chunksize=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n            If not given, the model is left untrained (presumably because you want to call\\n            :meth:`~gensim.models.ldamodel.LdaSeqModel.update` manually).\\n        time_slice : list of int, optional\\n            Number of documents in each time-slice. Each time slice could for example represent a year\\'s published\\n            papers, in case the corpus comes from a journal publishing over multiple years.\\n            It is assumed that `sum(time_slice) == num_documents`.\\n        id2word : dict of (int, str), optional\\n            Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for\\n            debugging and topic printing.\\n        alphas : float, optional\\n            The prior probability for the model.\\n        num_topics : int, optional\\n            The number of requested latent topics to be extracted from the training corpus.\\n        initialize : {\\'gensim\\', \\'own\\', \\'ldamodel\\'}, optional\\n            Controls the initialization of the DTM model. Supports three different modes:\\n                * \\'gensim\\': Uses gensim\\'s LDA initialization.\\n                * \\'own\\': Uses your own initialization matrix of an LDA model that has been previously trained.\\n                * \\'lda_model\\': Use a previously used LDA model, passing it through the `lda_model` argument.\\n        sstats : numpy.ndarray , optional\\n            Sufficient statistics used for initializing the model if `initialize == \\'own\\'`. Corresponds to matrix\\n            beta in the linked paper for time slice 0, expected shape (`self.vocab_len`, `num_topics`).\\n        lda_model : :class:`~gensim.models.ldamodel.LdaModel`\\n            Model whose sufficient statistics will be used to initialize the current object if `initialize == \\'gensim\\'`.\\n        obs_variance : float, optional\\n            Observed variance used to approximate the true and forward variance as shown in\\n            `David M. Blei, John D. Lafferty: \"Dynamic Topic Models\"\\n            <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n        chain_variance : float, optional\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n        passes : int, optional\\n            Number of passes over the corpus for the initial :class:`~gensim.models.ldamodel.LdaModel`\\n        random_state : {numpy.random.RandomState, int}, optional\\n            Can be a np.random.RandomState object, or the seed to generate one. Used for reproducibility of results.\\n        lda_inference_max_iter : int, optional\\n            Maximum number of iterations in the inference step of the LDA training.\\n        em_min_iter : int, optional\\n            Minimum number of iterations until converge of the Expectation-Maximization algorithm\\n        em_max_iter : int, optional\\n            Maximum number of iterations until converge of the Expectation-Maximization algorithm.\\n        chunksize : int, optional\\n            Number of documents in the corpus do be processed in in a chunk.\\n\\n        '\n    self.id2word = id2word\n    if corpus is None and self.id2word is None:\n        raise ValueError('at least one of corpus/id2word must be specified, to establish input space dimensionality')\n    if self.id2word is None:\n        logger.warning('no word id mapping provided; initializing from corpus, assuming identity')\n        self.id2word = utils.dict_from_corpus(corpus)\n        self.vocab_len = len(self.id2word)\n    elif self.id2word:\n        self.vocab_len = len(self.id2word)\n    else:\n        self.vocab_len = 0\n    if corpus is not None:\n        try:\n            self.corpus_len = len(corpus)\n        except TypeError:\n            logger.warning('input corpus stream has no len(); counting documents')\n            self.corpus_len = sum((1 for _ in corpus))\n    self.time_slice = time_slice\n    if self.time_slice is not None:\n        self.num_time_slices = len(time_slice)\n    self.num_topics = num_topics\n    self.num_time_slices = len(time_slice)\n    self.alphas = np.full(num_topics, alphas)\n    self.topic_chains = []\n    for topic in range(num_topics):\n        sslm_ = sslm(num_time_slices=self.num_time_slices, vocab_len=self.vocab_len, num_topics=self.num_topics, chain_variance=chain_variance, obs_variance=obs_variance)\n        self.topic_chains.append(sslm_)\n    self.top_doc_phis = None\n    self.influence = None\n    self.renormalized_influence = None\n    self.influence_sum_lgl = None\n    if corpus is not None and time_slice is not None:\n        self.max_doc_len = max((len(line) for line in corpus))\n        if initialize == 'gensim':\n            lda_model = ldamodel.LdaModel(corpus, id2word=self.id2word, num_topics=self.num_topics, passes=passes, alpha=self.alphas, random_state=random_state, dtype=np.float64)\n            self.sstats = np.transpose(lda_model.state.sstats)\n        if initialize == 'ldamodel':\n            self.sstats = np.transpose(lda_model.state.sstats)\n        if initialize == 'own':\n            self.sstats = sstats\n        self.init_ldaseq_ss(chain_variance, obs_variance, self.alphas, self.sstats)\n        self.fit_lda_seq(corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)"
        ]
    },
    {
        "func_name": "init_ldaseq_ss",
        "original": "def init_ldaseq_ss(self, topic_chain_variance, topic_obs_variance, alpha, init_suffstats):\n    \"\"\"Initialize State Space Language Model, topic-wise.\n\n        Parameters\n        ----------\n        topic_chain_variance : float\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve.\n        topic_obs_variance : float\n            Observed variance used to approximate the true and forward variance as shown in\n            `David M. Blei, John D. Lafferty: \"Dynamic Topic Models\"\n            <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\n        alpha : float\n            The prior probability for the model.\n        init_suffstats : numpy.ndarray\n            Sufficient statistics used for initializing the model, expected shape (`self.vocab_len`, `num_topics`).\n\n        \"\"\"\n    self.alphas = alpha\n    for (k, chain) in enumerate(self.topic_chains):\n        sstats = init_suffstats[:, k]\n        sslm.sslm_counts_init(chain, topic_obs_variance, topic_chain_variance, sstats)",
        "mutated": [
            "def init_ldaseq_ss(self, topic_chain_variance, topic_obs_variance, alpha, init_suffstats):\n    if False:\n        i = 10\n    'Initialize State Space Language Model, topic-wise.\\n\\n        Parameters\\n        ----------\\n        topic_chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve.\\n        topic_obs_variance : float\\n            Observed variance used to approximate the true and forward variance as shown in\\n            `David M. Blei, John D. Lafferty: \"Dynamic Topic Models\"\\n            <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n        alpha : float\\n            The prior probability for the model.\\n        init_suffstats : numpy.ndarray\\n            Sufficient statistics used for initializing the model, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        '\n    self.alphas = alpha\n    for (k, chain) in enumerate(self.topic_chains):\n        sstats = init_suffstats[:, k]\n        sslm.sslm_counts_init(chain, topic_obs_variance, topic_chain_variance, sstats)",
            "def init_ldaseq_ss(self, topic_chain_variance, topic_obs_variance, alpha, init_suffstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize State Space Language Model, topic-wise.\\n\\n        Parameters\\n        ----------\\n        topic_chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve.\\n        topic_obs_variance : float\\n            Observed variance used to approximate the true and forward variance as shown in\\n            `David M. Blei, John D. Lafferty: \"Dynamic Topic Models\"\\n            <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n        alpha : float\\n            The prior probability for the model.\\n        init_suffstats : numpy.ndarray\\n            Sufficient statistics used for initializing the model, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        '\n    self.alphas = alpha\n    for (k, chain) in enumerate(self.topic_chains):\n        sstats = init_suffstats[:, k]\n        sslm.sslm_counts_init(chain, topic_obs_variance, topic_chain_variance, sstats)",
            "def init_ldaseq_ss(self, topic_chain_variance, topic_obs_variance, alpha, init_suffstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize State Space Language Model, topic-wise.\\n\\n        Parameters\\n        ----------\\n        topic_chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve.\\n        topic_obs_variance : float\\n            Observed variance used to approximate the true and forward variance as shown in\\n            `David M. Blei, John D. Lafferty: \"Dynamic Topic Models\"\\n            <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n        alpha : float\\n            The prior probability for the model.\\n        init_suffstats : numpy.ndarray\\n            Sufficient statistics used for initializing the model, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        '\n    self.alphas = alpha\n    for (k, chain) in enumerate(self.topic_chains):\n        sstats = init_suffstats[:, k]\n        sslm.sslm_counts_init(chain, topic_obs_variance, topic_chain_variance, sstats)",
            "def init_ldaseq_ss(self, topic_chain_variance, topic_obs_variance, alpha, init_suffstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize State Space Language Model, topic-wise.\\n\\n        Parameters\\n        ----------\\n        topic_chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve.\\n        topic_obs_variance : float\\n            Observed variance used to approximate the true and forward variance as shown in\\n            `David M. Blei, John D. Lafferty: \"Dynamic Topic Models\"\\n            <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n        alpha : float\\n            The prior probability for the model.\\n        init_suffstats : numpy.ndarray\\n            Sufficient statistics used for initializing the model, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        '\n    self.alphas = alpha\n    for (k, chain) in enumerate(self.topic_chains):\n        sstats = init_suffstats[:, k]\n        sslm.sslm_counts_init(chain, topic_obs_variance, topic_chain_variance, sstats)",
            "def init_ldaseq_ss(self, topic_chain_variance, topic_obs_variance, alpha, init_suffstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize State Space Language Model, topic-wise.\\n\\n        Parameters\\n        ----------\\n        topic_chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve.\\n        topic_obs_variance : float\\n            Observed variance used to approximate the true and forward variance as shown in\\n            `David M. Blei, John D. Lafferty: \"Dynamic Topic Models\"\\n            <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n        alpha : float\\n            The prior probability for the model.\\n        init_suffstats : numpy.ndarray\\n            Sufficient statistics used for initializing the model, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        '\n    self.alphas = alpha\n    for (k, chain) in enumerate(self.topic_chains):\n        sstats = init_suffstats[:, k]\n        sslm.sslm_counts_init(chain, topic_obs_variance, topic_chain_variance, sstats)"
        ]
    },
    {
        "func_name": "fit_lda_seq",
        "original": "def fit_lda_seq(self, corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize):\n    \"\"\"Fit a LDA Sequence model (DTM).\n\n        This method will iteratively setup LDA models and perform EM steps until the sufficient statistics convergence,\n        or until the maximum number of iterations is reached. Because the true posterior is intractable, an\n        appropriately tight lower bound must be used instead. This function will optimize this bound, by minimizing\n        its true Kullback-Liebler Divergence with the true posterior.\n\n        Parameters\n        ----------\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\n        lda_inference_max_iter : int\n            Maximum number of iterations for the inference step of LDA.\n        em_min_iter : int\n            Minimum number of time slices to be inspected.\n        em_max_iter : int\n            Maximum number of time slices to be inspected.\n        chunksize : int\n            Number of documents to be processed in each chunk.\n\n        Returns\n        -------\n        float\n            The highest lower bound for the true posterior produced after all iterations.\n\n       \"\"\"\n    LDASQE_EM_THRESHOLD = 0.0001\n    LOWER_ITER = 10\n    ITER_MULT_LOW = 2\n    MAX_ITER = 500\n    num_topics = self.num_topics\n    vocab_len = self.vocab_len\n    data_len = self.num_time_slices\n    corpus_len = self.corpus_len\n    bound = 0\n    convergence = LDASQE_EM_THRESHOLD + 1\n    iter_ = 0\n    while iter_ < em_min_iter or (convergence > LDASQE_EM_THRESHOLD and iter_ <= em_max_iter):\n        logger.info(' EM iter %i', iter_)\n        logger.info('E Step')\n        old_bound = bound\n        topic_suffstats = []\n        for topic in range(num_topics):\n            topic_suffstats.append(np.zeros((vocab_len, data_len)))\n        gammas = np.zeros((corpus_len, num_topics))\n        lhoods = np.zeros((corpus_len, num_topics + 1))\n        (bound, gammas) = self.lda_seq_infer(corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)\n        self.gammas = gammas\n        logger.info('M Step')\n        topic_bound = self.fit_lda_seq_topics(topic_suffstats)\n        bound += topic_bound\n        if bound - old_bound < 0:\n            if lda_inference_max_iter < LOWER_ITER:\n                lda_inference_max_iter *= ITER_MULT_LOW\n            logger.info('Bound went down, increasing iterations to %i', lda_inference_max_iter)\n        convergence = np.fabs((bound - old_bound) / old_bound)\n        if convergence < LDASQE_EM_THRESHOLD:\n            lda_inference_max_iter = MAX_ITER\n            logger.info('Starting final iterations, max iter is %i', lda_inference_max_iter)\n            convergence = 1.0\n        logger.info('iteration %i iteration lda seq bound is %f convergence is %f', iter_, bound, convergence)\n        iter_ += 1\n    return bound",
        "mutated": [
            "def fit_lda_seq(self, corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize):\n    if False:\n        i = 10\n    'Fit a LDA Sequence model (DTM).\\n\\n        This method will iteratively setup LDA models and perform EM steps until the sufficient statistics convergence,\\n        or until the maximum number of iterations is reached. Because the true posterior is intractable, an\\n        appropriately tight lower bound must be used instead. This function will optimize this bound, by minimizing\\n        its true Kullback-Liebler Divergence with the true posterior.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        em_min_iter : int\\n            Minimum number of time slices to be inspected.\\n        em_max_iter : int\\n            Maximum number of time slices to be inspected.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        float\\n            The highest lower bound for the true posterior produced after all iterations.\\n\\n       '\n    LDASQE_EM_THRESHOLD = 0.0001\n    LOWER_ITER = 10\n    ITER_MULT_LOW = 2\n    MAX_ITER = 500\n    num_topics = self.num_topics\n    vocab_len = self.vocab_len\n    data_len = self.num_time_slices\n    corpus_len = self.corpus_len\n    bound = 0\n    convergence = LDASQE_EM_THRESHOLD + 1\n    iter_ = 0\n    while iter_ < em_min_iter or (convergence > LDASQE_EM_THRESHOLD and iter_ <= em_max_iter):\n        logger.info(' EM iter %i', iter_)\n        logger.info('E Step')\n        old_bound = bound\n        topic_suffstats = []\n        for topic in range(num_topics):\n            topic_suffstats.append(np.zeros((vocab_len, data_len)))\n        gammas = np.zeros((corpus_len, num_topics))\n        lhoods = np.zeros((corpus_len, num_topics + 1))\n        (bound, gammas) = self.lda_seq_infer(corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)\n        self.gammas = gammas\n        logger.info('M Step')\n        topic_bound = self.fit_lda_seq_topics(topic_suffstats)\n        bound += topic_bound\n        if bound - old_bound < 0:\n            if lda_inference_max_iter < LOWER_ITER:\n                lda_inference_max_iter *= ITER_MULT_LOW\n            logger.info('Bound went down, increasing iterations to %i', lda_inference_max_iter)\n        convergence = np.fabs((bound - old_bound) / old_bound)\n        if convergence < LDASQE_EM_THRESHOLD:\n            lda_inference_max_iter = MAX_ITER\n            logger.info('Starting final iterations, max iter is %i', lda_inference_max_iter)\n            convergence = 1.0\n        logger.info('iteration %i iteration lda seq bound is %f convergence is %f', iter_, bound, convergence)\n        iter_ += 1\n    return bound",
            "def fit_lda_seq(self, corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Fit a LDA Sequence model (DTM).\\n\\n        This method will iteratively setup LDA models and perform EM steps until the sufficient statistics convergence,\\n        or until the maximum number of iterations is reached. Because the true posterior is intractable, an\\n        appropriately tight lower bound must be used instead. This function will optimize this bound, by minimizing\\n        its true Kullback-Liebler Divergence with the true posterior.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        em_min_iter : int\\n            Minimum number of time slices to be inspected.\\n        em_max_iter : int\\n            Maximum number of time slices to be inspected.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        float\\n            The highest lower bound for the true posterior produced after all iterations.\\n\\n       '\n    LDASQE_EM_THRESHOLD = 0.0001\n    LOWER_ITER = 10\n    ITER_MULT_LOW = 2\n    MAX_ITER = 500\n    num_topics = self.num_topics\n    vocab_len = self.vocab_len\n    data_len = self.num_time_slices\n    corpus_len = self.corpus_len\n    bound = 0\n    convergence = LDASQE_EM_THRESHOLD + 1\n    iter_ = 0\n    while iter_ < em_min_iter or (convergence > LDASQE_EM_THRESHOLD and iter_ <= em_max_iter):\n        logger.info(' EM iter %i', iter_)\n        logger.info('E Step')\n        old_bound = bound\n        topic_suffstats = []\n        for topic in range(num_topics):\n            topic_suffstats.append(np.zeros((vocab_len, data_len)))\n        gammas = np.zeros((corpus_len, num_topics))\n        lhoods = np.zeros((corpus_len, num_topics + 1))\n        (bound, gammas) = self.lda_seq_infer(corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)\n        self.gammas = gammas\n        logger.info('M Step')\n        topic_bound = self.fit_lda_seq_topics(topic_suffstats)\n        bound += topic_bound\n        if bound - old_bound < 0:\n            if lda_inference_max_iter < LOWER_ITER:\n                lda_inference_max_iter *= ITER_MULT_LOW\n            logger.info('Bound went down, increasing iterations to %i', lda_inference_max_iter)\n        convergence = np.fabs((bound - old_bound) / old_bound)\n        if convergence < LDASQE_EM_THRESHOLD:\n            lda_inference_max_iter = MAX_ITER\n            logger.info('Starting final iterations, max iter is %i', lda_inference_max_iter)\n            convergence = 1.0\n        logger.info('iteration %i iteration lda seq bound is %f convergence is %f', iter_, bound, convergence)\n        iter_ += 1\n    return bound",
            "def fit_lda_seq(self, corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Fit a LDA Sequence model (DTM).\\n\\n        This method will iteratively setup LDA models and perform EM steps until the sufficient statistics convergence,\\n        or until the maximum number of iterations is reached. Because the true posterior is intractable, an\\n        appropriately tight lower bound must be used instead. This function will optimize this bound, by minimizing\\n        its true Kullback-Liebler Divergence with the true posterior.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        em_min_iter : int\\n            Minimum number of time slices to be inspected.\\n        em_max_iter : int\\n            Maximum number of time slices to be inspected.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        float\\n            The highest lower bound for the true posterior produced after all iterations.\\n\\n       '\n    LDASQE_EM_THRESHOLD = 0.0001\n    LOWER_ITER = 10\n    ITER_MULT_LOW = 2\n    MAX_ITER = 500\n    num_topics = self.num_topics\n    vocab_len = self.vocab_len\n    data_len = self.num_time_slices\n    corpus_len = self.corpus_len\n    bound = 0\n    convergence = LDASQE_EM_THRESHOLD + 1\n    iter_ = 0\n    while iter_ < em_min_iter or (convergence > LDASQE_EM_THRESHOLD and iter_ <= em_max_iter):\n        logger.info(' EM iter %i', iter_)\n        logger.info('E Step')\n        old_bound = bound\n        topic_suffstats = []\n        for topic in range(num_topics):\n            topic_suffstats.append(np.zeros((vocab_len, data_len)))\n        gammas = np.zeros((corpus_len, num_topics))\n        lhoods = np.zeros((corpus_len, num_topics + 1))\n        (bound, gammas) = self.lda_seq_infer(corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)\n        self.gammas = gammas\n        logger.info('M Step')\n        topic_bound = self.fit_lda_seq_topics(topic_suffstats)\n        bound += topic_bound\n        if bound - old_bound < 0:\n            if lda_inference_max_iter < LOWER_ITER:\n                lda_inference_max_iter *= ITER_MULT_LOW\n            logger.info('Bound went down, increasing iterations to %i', lda_inference_max_iter)\n        convergence = np.fabs((bound - old_bound) / old_bound)\n        if convergence < LDASQE_EM_THRESHOLD:\n            lda_inference_max_iter = MAX_ITER\n            logger.info('Starting final iterations, max iter is %i', lda_inference_max_iter)\n            convergence = 1.0\n        logger.info('iteration %i iteration lda seq bound is %f convergence is %f', iter_, bound, convergence)\n        iter_ += 1\n    return bound",
            "def fit_lda_seq(self, corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Fit a LDA Sequence model (DTM).\\n\\n        This method will iteratively setup LDA models and perform EM steps until the sufficient statistics convergence,\\n        or until the maximum number of iterations is reached. Because the true posterior is intractable, an\\n        appropriately tight lower bound must be used instead. This function will optimize this bound, by minimizing\\n        its true Kullback-Liebler Divergence with the true posterior.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        em_min_iter : int\\n            Minimum number of time slices to be inspected.\\n        em_max_iter : int\\n            Maximum number of time slices to be inspected.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        float\\n            The highest lower bound for the true posterior produced after all iterations.\\n\\n       '\n    LDASQE_EM_THRESHOLD = 0.0001\n    LOWER_ITER = 10\n    ITER_MULT_LOW = 2\n    MAX_ITER = 500\n    num_topics = self.num_topics\n    vocab_len = self.vocab_len\n    data_len = self.num_time_slices\n    corpus_len = self.corpus_len\n    bound = 0\n    convergence = LDASQE_EM_THRESHOLD + 1\n    iter_ = 0\n    while iter_ < em_min_iter or (convergence > LDASQE_EM_THRESHOLD and iter_ <= em_max_iter):\n        logger.info(' EM iter %i', iter_)\n        logger.info('E Step')\n        old_bound = bound\n        topic_suffstats = []\n        for topic in range(num_topics):\n            topic_suffstats.append(np.zeros((vocab_len, data_len)))\n        gammas = np.zeros((corpus_len, num_topics))\n        lhoods = np.zeros((corpus_len, num_topics + 1))\n        (bound, gammas) = self.lda_seq_infer(corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)\n        self.gammas = gammas\n        logger.info('M Step')\n        topic_bound = self.fit_lda_seq_topics(topic_suffstats)\n        bound += topic_bound\n        if bound - old_bound < 0:\n            if lda_inference_max_iter < LOWER_ITER:\n                lda_inference_max_iter *= ITER_MULT_LOW\n            logger.info('Bound went down, increasing iterations to %i', lda_inference_max_iter)\n        convergence = np.fabs((bound - old_bound) / old_bound)\n        if convergence < LDASQE_EM_THRESHOLD:\n            lda_inference_max_iter = MAX_ITER\n            logger.info('Starting final iterations, max iter is %i', lda_inference_max_iter)\n            convergence = 1.0\n        logger.info('iteration %i iteration lda seq bound is %f convergence is %f', iter_, bound, convergence)\n        iter_ += 1\n    return bound",
            "def fit_lda_seq(self, corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Fit a LDA Sequence model (DTM).\\n\\n        This method will iteratively setup LDA models and perform EM steps until the sufficient statistics convergence,\\n        or until the maximum number of iterations is reached. Because the true posterior is intractable, an\\n        appropriately tight lower bound must be used instead. This function will optimize this bound, by minimizing\\n        its true Kullback-Liebler Divergence with the true posterior.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        em_min_iter : int\\n            Minimum number of time slices to be inspected.\\n        em_max_iter : int\\n            Maximum number of time slices to be inspected.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        float\\n            The highest lower bound for the true posterior produced after all iterations.\\n\\n       '\n    LDASQE_EM_THRESHOLD = 0.0001\n    LOWER_ITER = 10\n    ITER_MULT_LOW = 2\n    MAX_ITER = 500\n    num_topics = self.num_topics\n    vocab_len = self.vocab_len\n    data_len = self.num_time_slices\n    corpus_len = self.corpus_len\n    bound = 0\n    convergence = LDASQE_EM_THRESHOLD + 1\n    iter_ = 0\n    while iter_ < em_min_iter or (convergence > LDASQE_EM_THRESHOLD and iter_ <= em_max_iter):\n        logger.info(' EM iter %i', iter_)\n        logger.info('E Step')\n        old_bound = bound\n        topic_suffstats = []\n        for topic in range(num_topics):\n            topic_suffstats.append(np.zeros((vocab_len, data_len)))\n        gammas = np.zeros((corpus_len, num_topics))\n        lhoods = np.zeros((corpus_len, num_topics + 1))\n        (bound, gammas) = self.lda_seq_infer(corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)\n        self.gammas = gammas\n        logger.info('M Step')\n        topic_bound = self.fit_lda_seq_topics(topic_suffstats)\n        bound += topic_bound\n        if bound - old_bound < 0:\n            if lda_inference_max_iter < LOWER_ITER:\n                lda_inference_max_iter *= ITER_MULT_LOW\n            logger.info('Bound went down, increasing iterations to %i', lda_inference_max_iter)\n        convergence = np.fabs((bound - old_bound) / old_bound)\n        if convergence < LDASQE_EM_THRESHOLD:\n            lda_inference_max_iter = MAX_ITER\n            logger.info('Starting final iterations, max iter is %i', lda_inference_max_iter)\n            convergence = 1.0\n        logger.info('iteration %i iteration lda seq bound is %f convergence is %f', iter_, bound, convergence)\n        iter_ += 1\n    return bound"
        ]
    },
    {
        "func_name": "lda_seq_infer",
        "original": "def lda_seq_infer(self, corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize):\n    \"\"\"Inference (or E-step) for the lower bound EM optimization.\n\n        This is used to set up the gensim :class:`~gensim.models.ldamodel.LdaModel` to be used for each time-slice.\n        It also allows for Document Influence Model code to be written in.\n\n        Parameters\n        ----------\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\n        topic_suffstats : numpy.ndarray\n            Sufficient statistics for time slice 0, used for initializing the model if `initialize == 'own'`,\n            expected shape (`self.vocab_len`, `num_topics`).\n        gammas : numpy.ndarray\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\n        lhoods : list of float\n            The total log probability lower bound for each topic. Corresponds to the phi variational parameters in the\n            linked paper.\n        iter_ : int\n            Current iteration.\n        lda_inference_max_iter : int\n            Maximum number of iterations for the inference step of LDA.\n        chunksize : int\n            Number of documents to be processed in each chunk.\n\n        Returns\n        -------\n        (float, list of float)\n            The first value is the highest lower bound for the true posterior.\n            The second value is the list of optimized dirichlet variational parameters for the approximation of\n            the posterior.\n\n        \"\"\"\n    num_topics = self.num_topics\n    vocab_len = self.vocab_len\n    bound = 0.0\n    lda = ldamodel.LdaModel(num_topics=num_topics, alpha=self.alphas, id2word=self.id2word, dtype=np.float64)\n    lda.topics = np.zeros((vocab_len, num_topics))\n    ldapost = LdaPost(max_doc_len=self.max_doc_len, num_topics=num_topics, lda=lda)\n    model = 'DTM'\n    if model == 'DTM':\n        (bound, gammas) = self.inferDTMseq(corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\n    elif model == 'DIM':\n        self.InfluenceTotalFixed(corpus)\n        (bound, gammas) = self.inferDIMseq(corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\n    return (bound, gammas)",
        "mutated": [
            "def lda_seq_infer(self, corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize):\n    if False:\n        i = 10\n    \"Inference (or E-step) for the lower bound EM optimization.\\n\\n        This is used to set up the gensim :class:`~gensim.models.ldamodel.LdaModel` to be used for each time-slice.\\n        It also allows for Document Influence Model code to be written in.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics for time slice 0, used for initializing the model if `initialize == 'own'`,\\n            expected shape (`self.vocab_len`, `num_topics`).\\n        gammas : numpy.ndarray\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhoods : list of float\\n            The total log probability lower bound for each topic. Corresponds to the phi variational parameters in the\\n            linked paper.\\n        iter_ : int\\n            Current iteration.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        (float, list of float)\\n            The first value is the highest lower bound for the true posterior.\\n            The second value is the list of optimized dirichlet variational parameters for the approximation of\\n            the posterior.\\n\\n        \"\n    num_topics = self.num_topics\n    vocab_len = self.vocab_len\n    bound = 0.0\n    lda = ldamodel.LdaModel(num_topics=num_topics, alpha=self.alphas, id2word=self.id2word, dtype=np.float64)\n    lda.topics = np.zeros((vocab_len, num_topics))\n    ldapost = LdaPost(max_doc_len=self.max_doc_len, num_topics=num_topics, lda=lda)\n    model = 'DTM'\n    if model == 'DTM':\n        (bound, gammas) = self.inferDTMseq(corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\n    elif model == 'DIM':\n        self.InfluenceTotalFixed(corpus)\n        (bound, gammas) = self.inferDIMseq(corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\n    return (bound, gammas)",
            "def lda_seq_infer(self, corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Inference (or E-step) for the lower bound EM optimization.\\n\\n        This is used to set up the gensim :class:`~gensim.models.ldamodel.LdaModel` to be used for each time-slice.\\n        It also allows for Document Influence Model code to be written in.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics for time slice 0, used for initializing the model if `initialize == 'own'`,\\n            expected shape (`self.vocab_len`, `num_topics`).\\n        gammas : numpy.ndarray\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhoods : list of float\\n            The total log probability lower bound for each topic. Corresponds to the phi variational parameters in the\\n            linked paper.\\n        iter_ : int\\n            Current iteration.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        (float, list of float)\\n            The first value is the highest lower bound for the true posterior.\\n            The second value is the list of optimized dirichlet variational parameters for the approximation of\\n            the posterior.\\n\\n        \"\n    num_topics = self.num_topics\n    vocab_len = self.vocab_len\n    bound = 0.0\n    lda = ldamodel.LdaModel(num_topics=num_topics, alpha=self.alphas, id2word=self.id2word, dtype=np.float64)\n    lda.topics = np.zeros((vocab_len, num_topics))\n    ldapost = LdaPost(max_doc_len=self.max_doc_len, num_topics=num_topics, lda=lda)\n    model = 'DTM'\n    if model == 'DTM':\n        (bound, gammas) = self.inferDTMseq(corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\n    elif model == 'DIM':\n        self.InfluenceTotalFixed(corpus)\n        (bound, gammas) = self.inferDIMseq(corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\n    return (bound, gammas)",
            "def lda_seq_infer(self, corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Inference (or E-step) for the lower bound EM optimization.\\n\\n        This is used to set up the gensim :class:`~gensim.models.ldamodel.LdaModel` to be used for each time-slice.\\n        It also allows for Document Influence Model code to be written in.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics for time slice 0, used for initializing the model if `initialize == 'own'`,\\n            expected shape (`self.vocab_len`, `num_topics`).\\n        gammas : numpy.ndarray\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhoods : list of float\\n            The total log probability lower bound for each topic. Corresponds to the phi variational parameters in the\\n            linked paper.\\n        iter_ : int\\n            Current iteration.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        (float, list of float)\\n            The first value is the highest lower bound for the true posterior.\\n            The second value is the list of optimized dirichlet variational parameters for the approximation of\\n            the posterior.\\n\\n        \"\n    num_topics = self.num_topics\n    vocab_len = self.vocab_len\n    bound = 0.0\n    lda = ldamodel.LdaModel(num_topics=num_topics, alpha=self.alphas, id2word=self.id2word, dtype=np.float64)\n    lda.topics = np.zeros((vocab_len, num_topics))\n    ldapost = LdaPost(max_doc_len=self.max_doc_len, num_topics=num_topics, lda=lda)\n    model = 'DTM'\n    if model == 'DTM':\n        (bound, gammas) = self.inferDTMseq(corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\n    elif model == 'DIM':\n        self.InfluenceTotalFixed(corpus)\n        (bound, gammas) = self.inferDIMseq(corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\n    return (bound, gammas)",
            "def lda_seq_infer(self, corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Inference (or E-step) for the lower bound EM optimization.\\n\\n        This is used to set up the gensim :class:`~gensim.models.ldamodel.LdaModel` to be used for each time-slice.\\n        It also allows for Document Influence Model code to be written in.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics for time slice 0, used for initializing the model if `initialize == 'own'`,\\n            expected shape (`self.vocab_len`, `num_topics`).\\n        gammas : numpy.ndarray\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhoods : list of float\\n            The total log probability lower bound for each topic. Corresponds to the phi variational parameters in the\\n            linked paper.\\n        iter_ : int\\n            Current iteration.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        (float, list of float)\\n            The first value is the highest lower bound for the true posterior.\\n            The second value is the list of optimized dirichlet variational parameters for the approximation of\\n            the posterior.\\n\\n        \"\n    num_topics = self.num_topics\n    vocab_len = self.vocab_len\n    bound = 0.0\n    lda = ldamodel.LdaModel(num_topics=num_topics, alpha=self.alphas, id2word=self.id2word, dtype=np.float64)\n    lda.topics = np.zeros((vocab_len, num_topics))\n    ldapost = LdaPost(max_doc_len=self.max_doc_len, num_topics=num_topics, lda=lda)\n    model = 'DTM'\n    if model == 'DTM':\n        (bound, gammas) = self.inferDTMseq(corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\n    elif model == 'DIM':\n        self.InfluenceTotalFixed(corpus)\n        (bound, gammas) = self.inferDIMseq(corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\n    return (bound, gammas)",
            "def lda_seq_infer(self, corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Inference (or E-step) for the lower bound EM optimization.\\n\\n        This is used to set up the gensim :class:`~gensim.models.ldamodel.LdaModel` to be used for each time-slice.\\n        It also allows for Document Influence Model code to be written in.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics for time slice 0, used for initializing the model if `initialize == 'own'`,\\n            expected shape (`self.vocab_len`, `num_topics`).\\n        gammas : numpy.ndarray\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhoods : list of float\\n            The total log probability lower bound for each topic. Corresponds to the phi variational parameters in the\\n            linked paper.\\n        iter_ : int\\n            Current iteration.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        (float, list of float)\\n            The first value is the highest lower bound for the true posterior.\\n            The second value is the list of optimized dirichlet variational parameters for the approximation of\\n            the posterior.\\n\\n        \"\n    num_topics = self.num_topics\n    vocab_len = self.vocab_len\n    bound = 0.0\n    lda = ldamodel.LdaModel(num_topics=num_topics, alpha=self.alphas, id2word=self.id2word, dtype=np.float64)\n    lda.topics = np.zeros((vocab_len, num_topics))\n    ldapost = LdaPost(max_doc_len=self.max_doc_len, num_topics=num_topics, lda=lda)\n    model = 'DTM'\n    if model == 'DTM':\n        (bound, gammas) = self.inferDTMseq(corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\n    elif model == 'DIM':\n        self.InfluenceTotalFixed(corpus)\n        (bound, gammas) = self.inferDIMseq(corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\n    return (bound, gammas)"
        ]
    },
    {
        "func_name": "inferDTMseq",
        "original": "def inferDTMseq(self, corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize):\n    \"\"\"Compute the likelihood of a sequential corpus under an LDA seq model, and reports the likelihood bound.\n\n        Parameters\n        ----------\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\n        topic_suffstats : numpy.ndarray\n            Sufficient statistics of the current model, expected shape (`self.vocab_len`, `num_topics`).\n        gammas : numpy.ndarray\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\n        lhoods : list of float of length `self.num_topics`\n            The total log probability bound for each topic. Corresponds to phi from the linked paper.\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\n            The trained LDA model of the previous iteration.\n        ldapost : :class:`~gensim.models.ldaseqmodel.LdaPost`\n            Posterior probability variables for the given LDA model. This will be used as the true (but intractable)\n            posterior.\n        iter_ : int\n            The current iteration.\n        bound : float\n            The LDA bound produced after all iterations.\n        lda_inference_max_iter : int\n            Maximum number of iterations for the inference step of LDA.\n        chunksize : int\n            Number of documents to be processed in each chunk.\n\n        Returns\n        -------\n        (float, list of float)\n            The first value is the highest lower bound for the true posterior.\n            The second value is the list of optimized dirichlet variational parameters for the approximation of\n            the posterior.\n\n        \"\"\"\n    doc_index = 0\n    time = 0\n    doc_num = 0\n    lda = self.make_lda_seq_slice(lda, time)\n    time_slice = np.cumsum(np.array(self.time_slice))\n    for (chunk_no, chunk) in enumerate(utils.grouper(corpus, chunksize)):\n        for doc in chunk:\n            if doc_index > time_slice[time]:\n                time += 1\n                lda = self.make_lda_seq_slice(lda, time)\n                doc_num = 0\n            gam = gammas[doc_index]\n            lhood = lhoods[doc_index]\n            ldapost.gamma = gam\n            ldapost.lhood = lhood\n            ldapost.doc = doc\n            if iter_ == 0:\n                doc_lhood = LdaPost.fit_lda_post(ldapost, doc_num, time, None, lda_inference_max_iter=lda_inference_max_iter)\n            else:\n                doc_lhood = LdaPost.fit_lda_post(ldapost, doc_num, time, self, lda_inference_max_iter=lda_inference_max_iter)\n            if topic_suffstats is not None:\n                topic_suffstats = LdaPost.update_lda_seq_ss(ldapost, time, doc, topic_suffstats)\n            gammas[doc_index] = ldapost.gamma\n            bound += doc_lhood\n            doc_index += 1\n            doc_num += 1\n    return (bound, gammas)",
        "mutated": [
            "def inferDTMseq(self, corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize):\n    if False:\n        i = 10\n    'Compute the likelihood of a sequential corpus under an LDA seq model, and reports the likelihood bound.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics of the current model, expected shape (`self.vocab_len`, `num_topics`).\\n        gammas : numpy.ndarray\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhoods : list of float of length `self.num_topics`\\n            The total log probability bound for each topic. Corresponds to phi from the linked paper.\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The trained LDA model of the previous iteration.\\n        ldapost : :class:`~gensim.models.ldaseqmodel.LdaPost`\\n            Posterior probability variables for the given LDA model. This will be used as the true (but intractable)\\n            posterior.\\n        iter_ : int\\n            The current iteration.\\n        bound : float\\n            The LDA bound produced after all iterations.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        (float, list of float)\\n            The first value is the highest lower bound for the true posterior.\\n            The second value is the list of optimized dirichlet variational parameters for the approximation of\\n            the posterior.\\n\\n        '\n    doc_index = 0\n    time = 0\n    doc_num = 0\n    lda = self.make_lda_seq_slice(lda, time)\n    time_slice = np.cumsum(np.array(self.time_slice))\n    for (chunk_no, chunk) in enumerate(utils.grouper(corpus, chunksize)):\n        for doc in chunk:\n            if doc_index > time_slice[time]:\n                time += 1\n                lda = self.make_lda_seq_slice(lda, time)\n                doc_num = 0\n            gam = gammas[doc_index]\n            lhood = lhoods[doc_index]\n            ldapost.gamma = gam\n            ldapost.lhood = lhood\n            ldapost.doc = doc\n            if iter_ == 0:\n                doc_lhood = LdaPost.fit_lda_post(ldapost, doc_num, time, None, lda_inference_max_iter=lda_inference_max_iter)\n            else:\n                doc_lhood = LdaPost.fit_lda_post(ldapost, doc_num, time, self, lda_inference_max_iter=lda_inference_max_iter)\n            if topic_suffstats is not None:\n                topic_suffstats = LdaPost.update_lda_seq_ss(ldapost, time, doc, topic_suffstats)\n            gammas[doc_index] = ldapost.gamma\n            bound += doc_lhood\n            doc_index += 1\n            doc_num += 1\n    return (bound, gammas)",
            "def inferDTMseq(self, corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the likelihood of a sequential corpus under an LDA seq model, and reports the likelihood bound.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics of the current model, expected shape (`self.vocab_len`, `num_topics`).\\n        gammas : numpy.ndarray\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhoods : list of float of length `self.num_topics`\\n            The total log probability bound for each topic. Corresponds to phi from the linked paper.\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The trained LDA model of the previous iteration.\\n        ldapost : :class:`~gensim.models.ldaseqmodel.LdaPost`\\n            Posterior probability variables for the given LDA model. This will be used as the true (but intractable)\\n            posterior.\\n        iter_ : int\\n            The current iteration.\\n        bound : float\\n            The LDA bound produced after all iterations.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        (float, list of float)\\n            The first value is the highest lower bound for the true posterior.\\n            The second value is the list of optimized dirichlet variational parameters for the approximation of\\n            the posterior.\\n\\n        '\n    doc_index = 0\n    time = 0\n    doc_num = 0\n    lda = self.make_lda_seq_slice(lda, time)\n    time_slice = np.cumsum(np.array(self.time_slice))\n    for (chunk_no, chunk) in enumerate(utils.grouper(corpus, chunksize)):\n        for doc in chunk:\n            if doc_index > time_slice[time]:\n                time += 1\n                lda = self.make_lda_seq_slice(lda, time)\n                doc_num = 0\n            gam = gammas[doc_index]\n            lhood = lhoods[doc_index]\n            ldapost.gamma = gam\n            ldapost.lhood = lhood\n            ldapost.doc = doc\n            if iter_ == 0:\n                doc_lhood = LdaPost.fit_lda_post(ldapost, doc_num, time, None, lda_inference_max_iter=lda_inference_max_iter)\n            else:\n                doc_lhood = LdaPost.fit_lda_post(ldapost, doc_num, time, self, lda_inference_max_iter=lda_inference_max_iter)\n            if topic_suffstats is not None:\n                topic_suffstats = LdaPost.update_lda_seq_ss(ldapost, time, doc, topic_suffstats)\n            gammas[doc_index] = ldapost.gamma\n            bound += doc_lhood\n            doc_index += 1\n            doc_num += 1\n    return (bound, gammas)",
            "def inferDTMseq(self, corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the likelihood of a sequential corpus under an LDA seq model, and reports the likelihood bound.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics of the current model, expected shape (`self.vocab_len`, `num_topics`).\\n        gammas : numpy.ndarray\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhoods : list of float of length `self.num_topics`\\n            The total log probability bound for each topic. Corresponds to phi from the linked paper.\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The trained LDA model of the previous iteration.\\n        ldapost : :class:`~gensim.models.ldaseqmodel.LdaPost`\\n            Posterior probability variables for the given LDA model. This will be used as the true (but intractable)\\n            posterior.\\n        iter_ : int\\n            The current iteration.\\n        bound : float\\n            The LDA bound produced after all iterations.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        (float, list of float)\\n            The first value is the highest lower bound for the true posterior.\\n            The second value is the list of optimized dirichlet variational parameters for the approximation of\\n            the posterior.\\n\\n        '\n    doc_index = 0\n    time = 0\n    doc_num = 0\n    lda = self.make_lda_seq_slice(lda, time)\n    time_slice = np.cumsum(np.array(self.time_slice))\n    for (chunk_no, chunk) in enumerate(utils.grouper(corpus, chunksize)):\n        for doc in chunk:\n            if doc_index > time_slice[time]:\n                time += 1\n                lda = self.make_lda_seq_slice(lda, time)\n                doc_num = 0\n            gam = gammas[doc_index]\n            lhood = lhoods[doc_index]\n            ldapost.gamma = gam\n            ldapost.lhood = lhood\n            ldapost.doc = doc\n            if iter_ == 0:\n                doc_lhood = LdaPost.fit_lda_post(ldapost, doc_num, time, None, lda_inference_max_iter=lda_inference_max_iter)\n            else:\n                doc_lhood = LdaPost.fit_lda_post(ldapost, doc_num, time, self, lda_inference_max_iter=lda_inference_max_iter)\n            if topic_suffstats is not None:\n                topic_suffstats = LdaPost.update_lda_seq_ss(ldapost, time, doc, topic_suffstats)\n            gammas[doc_index] = ldapost.gamma\n            bound += doc_lhood\n            doc_index += 1\n            doc_num += 1\n    return (bound, gammas)",
            "def inferDTMseq(self, corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the likelihood of a sequential corpus under an LDA seq model, and reports the likelihood bound.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics of the current model, expected shape (`self.vocab_len`, `num_topics`).\\n        gammas : numpy.ndarray\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhoods : list of float of length `self.num_topics`\\n            The total log probability bound for each topic. Corresponds to phi from the linked paper.\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The trained LDA model of the previous iteration.\\n        ldapost : :class:`~gensim.models.ldaseqmodel.LdaPost`\\n            Posterior probability variables for the given LDA model. This will be used as the true (but intractable)\\n            posterior.\\n        iter_ : int\\n            The current iteration.\\n        bound : float\\n            The LDA bound produced after all iterations.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        (float, list of float)\\n            The first value is the highest lower bound for the true posterior.\\n            The second value is the list of optimized dirichlet variational parameters for the approximation of\\n            the posterior.\\n\\n        '\n    doc_index = 0\n    time = 0\n    doc_num = 0\n    lda = self.make_lda_seq_slice(lda, time)\n    time_slice = np.cumsum(np.array(self.time_slice))\n    for (chunk_no, chunk) in enumerate(utils.grouper(corpus, chunksize)):\n        for doc in chunk:\n            if doc_index > time_slice[time]:\n                time += 1\n                lda = self.make_lda_seq_slice(lda, time)\n                doc_num = 0\n            gam = gammas[doc_index]\n            lhood = lhoods[doc_index]\n            ldapost.gamma = gam\n            ldapost.lhood = lhood\n            ldapost.doc = doc\n            if iter_ == 0:\n                doc_lhood = LdaPost.fit_lda_post(ldapost, doc_num, time, None, lda_inference_max_iter=lda_inference_max_iter)\n            else:\n                doc_lhood = LdaPost.fit_lda_post(ldapost, doc_num, time, self, lda_inference_max_iter=lda_inference_max_iter)\n            if topic_suffstats is not None:\n                topic_suffstats = LdaPost.update_lda_seq_ss(ldapost, time, doc, topic_suffstats)\n            gammas[doc_index] = ldapost.gamma\n            bound += doc_lhood\n            doc_index += 1\n            doc_num += 1\n    return (bound, gammas)",
            "def inferDTMseq(self, corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the likelihood of a sequential corpus under an LDA seq model, and reports the likelihood bound.\\n\\n        Parameters\\n        ----------\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}\\n            Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics of the current model, expected shape (`self.vocab_len`, `num_topics`).\\n        gammas : numpy.ndarray\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhoods : list of float of length `self.num_topics`\\n            The total log probability bound for each topic. Corresponds to phi from the linked paper.\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The trained LDA model of the previous iteration.\\n        ldapost : :class:`~gensim.models.ldaseqmodel.LdaPost`\\n            Posterior probability variables for the given LDA model. This will be used as the true (but intractable)\\n            posterior.\\n        iter_ : int\\n            The current iteration.\\n        bound : float\\n            The LDA bound produced after all iterations.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations for the inference step of LDA.\\n        chunksize : int\\n            Number of documents to be processed in each chunk.\\n\\n        Returns\\n        -------\\n        (float, list of float)\\n            The first value is the highest lower bound for the true posterior.\\n            The second value is the list of optimized dirichlet variational parameters for the approximation of\\n            the posterior.\\n\\n        '\n    doc_index = 0\n    time = 0\n    doc_num = 0\n    lda = self.make_lda_seq_slice(lda, time)\n    time_slice = np.cumsum(np.array(self.time_slice))\n    for (chunk_no, chunk) in enumerate(utils.grouper(corpus, chunksize)):\n        for doc in chunk:\n            if doc_index > time_slice[time]:\n                time += 1\n                lda = self.make_lda_seq_slice(lda, time)\n                doc_num = 0\n            gam = gammas[doc_index]\n            lhood = lhoods[doc_index]\n            ldapost.gamma = gam\n            ldapost.lhood = lhood\n            ldapost.doc = doc\n            if iter_ == 0:\n                doc_lhood = LdaPost.fit_lda_post(ldapost, doc_num, time, None, lda_inference_max_iter=lda_inference_max_iter)\n            else:\n                doc_lhood = LdaPost.fit_lda_post(ldapost, doc_num, time, self, lda_inference_max_iter=lda_inference_max_iter)\n            if topic_suffstats is not None:\n                topic_suffstats = LdaPost.update_lda_seq_ss(ldapost, time, doc, topic_suffstats)\n            gammas[doc_index] = ldapost.gamma\n            bound += doc_lhood\n            doc_index += 1\n            doc_num += 1\n    return (bound, gammas)"
        ]
    },
    {
        "func_name": "make_lda_seq_slice",
        "original": "def make_lda_seq_slice(self, lda, time):\n    \"\"\"Update the LDA model topic-word values using time slices.\n\n        Parameters\n        ----------\n\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\n            The stationary model to be updated\n        time : int\n            The time slice assigned to the stationary model.\n\n        Returns\n        -------\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\n            The stationary model updated to reflect the passed time slice.\n\n        \"\"\"\n    for k in range(self.num_topics):\n        lda.topics[:, k] = self.topic_chains[k].e_log_prob[:, time]\n    lda.alpha = np.copy(self.alphas)\n    return lda",
        "mutated": [
            "def make_lda_seq_slice(self, lda, time):\n    if False:\n        i = 10\n    'Update the LDA model topic-word values using time slices.\\n\\n        Parameters\\n        ----------\\n\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The stationary model to be updated\\n        time : int\\n            The time slice assigned to the stationary model.\\n\\n        Returns\\n        -------\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The stationary model updated to reflect the passed time slice.\\n\\n        '\n    for k in range(self.num_topics):\n        lda.topics[:, k] = self.topic_chains[k].e_log_prob[:, time]\n    lda.alpha = np.copy(self.alphas)\n    return lda",
            "def make_lda_seq_slice(self, lda, time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Update the LDA model topic-word values using time slices.\\n\\n        Parameters\\n        ----------\\n\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The stationary model to be updated\\n        time : int\\n            The time slice assigned to the stationary model.\\n\\n        Returns\\n        -------\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The stationary model updated to reflect the passed time slice.\\n\\n        '\n    for k in range(self.num_topics):\n        lda.topics[:, k] = self.topic_chains[k].e_log_prob[:, time]\n    lda.alpha = np.copy(self.alphas)\n    return lda",
            "def make_lda_seq_slice(self, lda, time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Update the LDA model topic-word values using time slices.\\n\\n        Parameters\\n        ----------\\n\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The stationary model to be updated\\n        time : int\\n            The time slice assigned to the stationary model.\\n\\n        Returns\\n        -------\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The stationary model updated to reflect the passed time slice.\\n\\n        '\n    for k in range(self.num_topics):\n        lda.topics[:, k] = self.topic_chains[k].e_log_prob[:, time]\n    lda.alpha = np.copy(self.alphas)\n    return lda",
            "def make_lda_seq_slice(self, lda, time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Update the LDA model topic-word values using time slices.\\n\\n        Parameters\\n        ----------\\n\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The stationary model to be updated\\n        time : int\\n            The time slice assigned to the stationary model.\\n\\n        Returns\\n        -------\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The stationary model updated to reflect the passed time slice.\\n\\n        '\n    for k in range(self.num_topics):\n        lda.topics[:, k] = self.topic_chains[k].e_log_prob[:, time]\n    lda.alpha = np.copy(self.alphas)\n    return lda",
            "def make_lda_seq_slice(self, lda, time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Update the LDA model topic-word values using time slices.\\n\\n        Parameters\\n        ----------\\n\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The stationary model to be updated\\n        time : int\\n            The time slice assigned to the stationary model.\\n\\n        Returns\\n        -------\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`\\n            The stationary model updated to reflect the passed time slice.\\n\\n        '\n    for k in range(self.num_topics):\n        lda.topics[:, k] = self.topic_chains[k].e_log_prob[:, time]\n    lda.alpha = np.copy(self.alphas)\n    return lda"
        ]
    },
    {
        "func_name": "fit_lda_seq_topics",
        "original": "def fit_lda_seq_topics(self, topic_suffstats):\n    \"\"\"Fit the sequential model topic-wise.\n\n        Parameters\n        ----------\n        topic_suffstats : numpy.ndarray\n            Sufficient statistics of the current model, expected shape (`self.vocab_len`, `num_topics`).\n\n        Returns\n        -------\n        float\n            The sum of the optimized lower bounds for all topics.\n\n        \"\"\"\n    lhood = 0\n    for (k, chain) in enumerate(self.topic_chains):\n        logger.info('Fitting topic number %i', k)\n        lhood_term = sslm.fit_sslm(chain, topic_suffstats[k])\n        lhood += lhood_term\n    return lhood",
        "mutated": [
            "def fit_lda_seq_topics(self, topic_suffstats):\n    if False:\n        i = 10\n    'Fit the sequential model topic-wise.\\n\\n        Parameters\\n        ----------\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics of the current model, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        Returns\\n        -------\\n        float\\n            The sum of the optimized lower bounds for all topics.\\n\\n        '\n    lhood = 0\n    for (k, chain) in enumerate(self.topic_chains):\n        logger.info('Fitting topic number %i', k)\n        lhood_term = sslm.fit_sslm(chain, topic_suffstats[k])\n        lhood += lhood_term\n    return lhood",
            "def fit_lda_seq_topics(self, topic_suffstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Fit the sequential model topic-wise.\\n\\n        Parameters\\n        ----------\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics of the current model, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        Returns\\n        -------\\n        float\\n            The sum of the optimized lower bounds for all topics.\\n\\n        '\n    lhood = 0\n    for (k, chain) in enumerate(self.topic_chains):\n        logger.info('Fitting topic number %i', k)\n        lhood_term = sslm.fit_sslm(chain, topic_suffstats[k])\n        lhood += lhood_term\n    return lhood",
            "def fit_lda_seq_topics(self, topic_suffstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Fit the sequential model topic-wise.\\n\\n        Parameters\\n        ----------\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics of the current model, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        Returns\\n        -------\\n        float\\n            The sum of the optimized lower bounds for all topics.\\n\\n        '\n    lhood = 0\n    for (k, chain) in enumerate(self.topic_chains):\n        logger.info('Fitting topic number %i', k)\n        lhood_term = sslm.fit_sslm(chain, topic_suffstats[k])\n        lhood += lhood_term\n    return lhood",
            "def fit_lda_seq_topics(self, topic_suffstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Fit the sequential model topic-wise.\\n\\n        Parameters\\n        ----------\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics of the current model, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        Returns\\n        -------\\n        float\\n            The sum of the optimized lower bounds for all topics.\\n\\n        '\n    lhood = 0\n    for (k, chain) in enumerate(self.topic_chains):\n        logger.info('Fitting topic number %i', k)\n        lhood_term = sslm.fit_sslm(chain, topic_suffstats[k])\n        lhood += lhood_term\n    return lhood",
            "def fit_lda_seq_topics(self, topic_suffstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Fit the sequential model topic-wise.\\n\\n        Parameters\\n        ----------\\n        topic_suffstats : numpy.ndarray\\n            Sufficient statistics of the current model, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        Returns\\n        -------\\n        float\\n            The sum of the optimized lower bounds for all topics.\\n\\n        '\n    lhood = 0\n    for (k, chain) in enumerate(self.topic_chains):\n        logger.info('Fitting topic number %i', k)\n        lhood_term = sslm.fit_sslm(chain, topic_suffstats[k])\n        lhood += lhood_term\n    return lhood"
        ]
    },
    {
        "func_name": "print_topic_times",
        "original": "def print_topic_times(self, topic, top_terms=20):\n    \"\"\"Get the most relevant words for a topic, for each timeslice. This can be used to inspect the evolution of a\n        topic through time.\n\n        Parameters\n        ----------\n        topic : int\n            The index of the topic.\n        top_terms : int, optional\n            Number of most relevant words associated with the topic to be returned.\n\n        Returns\n        -------\n        list of list of str\n            Top `top_terms` relevant terms for the topic for each time slice.\n\n        \"\"\"\n    topics = []\n    for time in range(self.num_time_slices):\n        topics.append(self.print_topic(topic, time, top_terms))\n    return topics",
        "mutated": [
            "def print_topic_times(self, topic, top_terms=20):\n    if False:\n        i = 10\n    'Get the most relevant words for a topic, for each timeslice. This can be used to inspect the evolution of a\\n        topic through time.\\n\\n        Parameters\\n        ----------\\n        topic : int\\n            The index of the topic.\\n        top_terms : int, optional\\n            Number of most relevant words associated with the topic to be returned.\\n\\n        Returns\\n        -------\\n        list of list of str\\n            Top `top_terms` relevant terms for the topic for each time slice.\\n\\n        '\n    topics = []\n    for time in range(self.num_time_slices):\n        topics.append(self.print_topic(topic, time, top_terms))\n    return topics",
            "def print_topic_times(self, topic, top_terms=20):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get the most relevant words for a topic, for each timeslice. This can be used to inspect the evolution of a\\n        topic through time.\\n\\n        Parameters\\n        ----------\\n        topic : int\\n            The index of the topic.\\n        top_terms : int, optional\\n            Number of most relevant words associated with the topic to be returned.\\n\\n        Returns\\n        -------\\n        list of list of str\\n            Top `top_terms` relevant terms for the topic for each time slice.\\n\\n        '\n    topics = []\n    for time in range(self.num_time_slices):\n        topics.append(self.print_topic(topic, time, top_terms))\n    return topics",
            "def print_topic_times(self, topic, top_terms=20):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get the most relevant words for a topic, for each timeslice. This can be used to inspect the evolution of a\\n        topic through time.\\n\\n        Parameters\\n        ----------\\n        topic : int\\n            The index of the topic.\\n        top_terms : int, optional\\n            Number of most relevant words associated with the topic to be returned.\\n\\n        Returns\\n        -------\\n        list of list of str\\n            Top `top_terms` relevant terms for the topic for each time slice.\\n\\n        '\n    topics = []\n    for time in range(self.num_time_slices):\n        topics.append(self.print_topic(topic, time, top_terms))\n    return topics",
            "def print_topic_times(self, topic, top_terms=20):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get the most relevant words for a topic, for each timeslice. This can be used to inspect the evolution of a\\n        topic through time.\\n\\n        Parameters\\n        ----------\\n        topic : int\\n            The index of the topic.\\n        top_terms : int, optional\\n            Number of most relevant words associated with the topic to be returned.\\n\\n        Returns\\n        -------\\n        list of list of str\\n            Top `top_terms` relevant terms for the topic for each time slice.\\n\\n        '\n    topics = []\n    for time in range(self.num_time_slices):\n        topics.append(self.print_topic(topic, time, top_terms))\n    return topics",
            "def print_topic_times(self, topic, top_terms=20):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get the most relevant words for a topic, for each timeslice. This can be used to inspect the evolution of a\\n        topic through time.\\n\\n        Parameters\\n        ----------\\n        topic : int\\n            The index of the topic.\\n        top_terms : int, optional\\n            Number of most relevant words associated with the topic to be returned.\\n\\n        Returns\\n        -------\\n        list of list of str\\n            Top `top_terms` relevant terms for the topic for each time slice.\\n\\n        '\n    topics = []\n    for time in range(self.num_time_slices):\n        topics.append(self.print_topic(topic, time, top_terms))\n    return topics"
        ]
    },
    {
        "func_name": "print_topics",
        "original": "def print_topics(self, time=0, top_terms=20):\n    \"\"\"Get the most relevant words for every topic.\n\n        Parameters\n        ----------\n        time : int, optional\n            The time slice in which we are interested in (since topics evolve over time, it is expected that the most\n            relevant words will also gradually change).\n        top_terms : int, optional\n            Number of most relevant words to be returned for each topic.\n\n        Returns\n        -------\n        list of list of (str, float)\n            Representation of all topics. Each of them is represented by a list of pairs of words and their assigned\n            probability.\n\n        \"\"\"\n    return [self.print_topic(topic, time, top_terms) for topic in range(self.num_topics)]",
        "mutated": [
            "def print_topics(self, time=0, top_terms=20):\n    if False:\n        i = 10\n    'Get the most relevant words for every topic.\\n\\n        Parameters\\n        ----------\\n        time : int, optional\\n            The time slice in which we are interested in (since topics evolve over time, it is expected that the most\\n            relevant words will also gradually change).\\n        top_terms : int, optional\\n            Number of most relevant words to be returned for each topic.\\n\\n        Returns\\n        -------\\n        list of list of (str, float)\\n            Representation of all topics. Each of them is represented by a list of pairs of words and their assigned\\n            probability.\\n\\n        '\n    return [self.print_topic(topic, time, top_terms) for topic in range(self.num_topics)]",
            "def print_topics(self, time=0, top_terms=20):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get the most relevant words for every topic.\\n\\n        Parameters\\n        ----------\\n        time : int, optional\\n            The time slice in which we are interested in (since topics evolve over time, it is expected that the most\\n            relevant words will also gradually change).\\n        top_terms : int, optional\\n            Number of most relevant words to be returned for each topic.\\n\\n        Returns\\n        -------\\n        list of list of (str, float)\\n            Representation of all topics. Each of them is represented by a list of pairs of words and their assigned\\n            probability.\\n\\n        '\n    return [self.print_topic(topic, time, top_terms) for topic in range(self.num_topics)]",
            "def print_topics(self, time=0, top_terms=20):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get the most relevant words for every topic.\\n\\n        Parameters\\n        ----------\\n        time : int, optional\\n            The time slice in which we are interested in (since topics evolve over time, it is expected that the most\\n            relevant words will also gradually change).\\n        top_terms : int, optional\\n            Number of most relevant words to be returned for each topic.\\n\\n        Returns\\n        -------\\n        list of list of (str, float)\\n            Representation of all topics. Each of them is represented by a list of pairs of words and their assigned\\n            probability.\\n\\n        '\n    return [self.print_topic(topic, time, top_terms) for topic in range(self.num_topics)]",
            "def print_topics(self, time=0, top_terms=20):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get the most relevant words for every topic.\\n\\n        Parameters\\n        ----------\\n        time : int, optional\\n            The time slice in which we are interested in (since topics evolve over time, it is expected that the most\\n            relevant words will also gradually change).\\n        top_terms : int, optional\\n            Number of most relevant words to be returned for each topic.\\n\\n        Returns\\n        -------\\n        list of list of (str, float)\\n            Representation of all topics. Each of them is represented by a list of pairs of words and their assigned\\n            probability.\\n\\n        '\n    return [self.print_topic(topic, time, top_terms) for topic in range(self.num_topics)]",
            "def print_topics(self, time=0, top_terms=20):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get the most relevant words for every topic.\\n\\n        Parameters\\n        ----------\\n        time : int, optional\\n            The time slice in which we are interested in (since topics evolve over time, it is expected that the most\\n            relevant words will also gradually change).\\n        top_terms : int, optional\\n            Number of most relevant words to be returned for each topic.\\n\\n        Returns\\n        -------\\n        list of list of (str, float)\\n            Representation of all topics. Each of them is represented by a list of pairs of words and their assigned\\n            probability.\\n\\n        '\n    return [self.print_topic(topic, time, top_terms) for topic in range(self.num_topics)]"
        ]
    },
    {
        "func_name": "print_topic",
        "original": "def print_topic(self, topic, time=0, top_terms=20):\n    \"\"\"Get the list of words most relevant to the given topic.\n\n        Parameters\n        ----------\n        topic : int\n            The index of the topic to be inspected.\n        time : int, optional\n            The time slice in which we are interested in (since topics evolve over time, it is expected that the most\n            relevant words will also gradually change).\n        top_terms : int, optional\n            Number of words associated with the topic to be returned.\n\n        Returns\n        -------\n        list of (str, float)\n            The representation of this topic. Each element in the list includes the word itself, along with the\n            probability assigned to it by the topic.\n\n        \"\"\"\n    topic = self.topic_chains[topic].e_log_prob\n    topic = np.transpose(topic)\n    topic = np.exp(topic[time])\n    topic = topic / topic.sum()\n    bestn = matutils.argsort(topic, top_terms, reverse=True)\n    beststr = [(self.id2word[id_], topic[id_]) for id_ in bestn]\n    return beststr",
        "mutated": [
            "def print_topic(self, topic, time=0, top_terms=20):\n    if False:\n        i = 10\n    'Get the list of words most relevant to the given topic.\\n\\n        Parameters\\n        ----------\\n        topic : int\\n            The index of the topic to be inspected.\\n        time : int, optional\\n            The time slice in which we are interested in (since topics evolve over time, it is expected that the most\\n            relevant words will also gradually change).\\n        top_terms : int, optional\\n            Number of words associated with the topic to be returned.\\n\\n        Returns\\n        -------\\n        list of (str, float)\\n            The representation of this topic. Each element in the list includes the word itself, along with the\\n            probability assigned to it by the topic.\\n\\n        '\n    topic = self.topic_chains[topic].e_log_prob\n    topic = np.transpose(topic)\n    topic = np.exp(topic[time])\n    topic = topic / topic.sum()\n    bestn = matutils.argsort(topic, top_terms, reverse=True)\n    beststr = [(self.id2word[id_], topic[id_]) for id_ in bestn]\n    return beststr",
            "def print_topic(self, topic, time=0, top_terms=20):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get the list of words most relevant to the given topic.\\n\\n        Parameters\\n        ----------\\n        topic : int\\n            The index of the topic to be inspected.\\n        time : int, optional\\n            The time slice in which we are interested in (since topics evolve over time, it is expected that the most\\n            relevant words will also gradually change).\\n        top_terms : int, optional\\n            Number of words associated with the topic to be returned.\\n\\n        Returns\\n        -------\\n        list of (str, float)\\n            The representation of this topic. Each element in the list includes the word itself, along with the\\n            probability assigned to it by the topic.\\n\\n        '\n    topic = self.topic_chains[topic].e_log_prob\n    topic = np.transpose(topic)\n    topic = np.exp(topic[time])\n    topic = topic / topic.sum()\n    bestn = matutils.argsort(topic, top_terms, reverse=True)\n    beststr = [(self.id2word[id_], topic[id_]) for id_ in bestn]\n    return beststr",
            "def print_topic(self, topic, time=0, top_terms=20):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get the list of words most relevant to the given topic.\\n\\n        Parameters\\n        ----------\\n        topic : int\\n            The index of the topic to be inspected.\\n        time : int, optional\\n            The time slice in which we are interested in (since topics evolve over time, it is expected that the most\\n            relevant words will also gradually change).\\n        top_terms : int, optional\\n            Number of words associated with the topic to be returned.\\n\\n        Returns\\n        -------\\n        list of (str, float)\\n            The representation of this topic. Each element in the list includes the word itself, along with the\\n            probability assigned to it by the topic.\\n\\n        '\n    topic = self.topic_chains[topic].e_log_prob\n    topic = np.transpose(topic)\n    topic = np.exp(topic[time])\n    topic = topic / topic.sum()\n    bestn = matutils.argsort(topic, top_terms, reverse=True)\n    beststr = [(self.id2word[id_], topic[id_]) for id_ in bestn]\n    return beststr",
            "def print_topic(self, topic, time=0, top_terms=20):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get the list of words most relevant to the given topic.\\n\\n        Parameters\\n        ----------\\n        topic : int\\n            The index of the topic to be inspected.\\n        time : int, optional\\n            The time slice in which we are interested in (since topics evolve over time, it is expected that the most\\n            relevant words will also gradually change).\\n        top_terms : int, optional\\n            Number of words associated with the topic to be returned.\\n\\n        Returns\\n        -------\\n        list of (str, float)\\n            The representation of this topic. Each element in the list includes the word itself, along with the\\n            probability assigned to it by the topic.\\n\\n        '\n    topic = self.topic_chains[topic].e_log_prob\n    topic = np.transpose(topic)\n    topic = np.exp(topic[time])\n    topic = topic / topic.sum()\n    bestn = matutils.argsort(topic, top_terms, reverse=True)\n    beststr = [(self.id2word[id_], topic[id_]) for id_ in bestn]\n    return beststr",
            "def print_topic(self, topic, time=0, top_terms=20):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get the list of words most relevant to the given topic.\\n\\n        Parameters\\n        ----------\\n        topic : int\\n            The index of the topic to be inspected.\\n        time : int, optional\\n            The time slice in which we are interested in (since topics evolve over time, it is expected that the most\\n            relevant words will also gradually change).\\n        top_terms : int, optional\\n            Number of words associated with the topic to be returned.\\n\\n        Returns\\n        -------\\n        list of (str, float)\\n            The representation of this topic. Each element in the list includes the word itself, along with the\\n            probability assigned to it by the topic.\\n\\n        '\n    topic = self.topic_chains[topic].e_log_prob\n    topic = np.transpose(topic)\n    topic = np.exp(topic[time])\n    topic = topic / topic.sum()\n    bestn = matutils.argsort(topic, top_terms, reverse=True)\n    beststr = [(self.id2word[id_], topic[id_]) for id_ in bestn]\n    return beststr"
        ]
    },
    {
        "func_name": "doc_topics",
        "original": "def doc_topics(self, doc_number):\n    \"\"\"Get the topic mixture for a document.\n\n        Uses the priors for the dirichlet distribution that approximates the true posterior with the optimal\n        lower bound, and therefore requires the model to be already trained.\n\n\n        Parameters\n        ----------\n        doc_number : int\n            Index of the document for which the mixture is returned.\n\n        Returns\n        -------\n        list of length `self.num_topics`\n            Probability for each topic in the mixture (essentially a point in the `self.num_topics - 1` simplex.\n\n        \"\"\"\n    doc_topic = self.gammas / self.gammas.sum(axis=1)[:, np.newaxis]\n    return doc_topic[doc_number]",
        "mutated": [
            "def doc_topics(self, doc_number):\n    if False:\n        i = 10\n    'Get the topic mixture for a document.\\n\\n        Uses the priors for the dirichlet distribution that approximates the true posterior with the optimal\\n        lower bound, and therefore requires the model to be already trained.\\n\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            Index of the document for which the mixture is returned.\\n\\n        Returns\\n        -------\\n        list of length `self.num_topics`\\n            Probability for each topic in the mixture (essentially a point in the `self.num_topics - 1` simplex.\\n\\n        '\n    doc_topic = self.gammas / self.gammas.sum(axis=1)[:, np.newaxis]\n    return doc_topic[doc_number]",
            "def doc_topics(self, doc_number):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get the topic mixture for a document.\\n\\n        Uses the priors for the dirichlet distribution that approximates the true posterior with the optimal\\n        lower bound, and therefore requires the model to be already trained.\\n\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            Index of the document for which the mixture is returned.\\n\\n        Returns\\n        -------\\n        list of length `self.num_topics`\\n            Probability for each topic in the mixture (essentially a point in the `self.num_topics - 1` simplex.\\n\\n        '\n    doc_topic = self.gammas / self.gammas.sum(axis=1)[:, np.newaxis]\n    return doc_topic[doc_number]",
            "def doc_topics(self, doc_number):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get the topic mixture for a document.\\n\\n        Uses the priors for the dirichlet distribution that approximates the true posterior with the optimal\\n        lower bound, and therefore requires the model to be already trained.\\n\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            Index of the document for which the mixture is returned.\\n\\n        Returns\\n        -------\\n        list of length `self.num_topics`\\n            Probability for each topic in the mixture (essentially a point in the `self.num_topics - 1` simplex.\\n\\n        '\n    doc_topic = self.gammas / self.gammas.sum(axis=1)[:, np.newaxis]\n    return doc_topic[doc_number]",
            "def doc_topics(self, doc_number):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get the topic mixture for a document.\\n\\n        Uses the priors for the dirichlet distribution that approximates the true posterior with the optimal\\n        lower bound, and therefore requires the model to be already trained.\\n\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            Index of the document for which the mixture is returned.\\n\\n        Returns\\n        -------\\n        list of length `self.num_topics`\\n            Probability for each topic in the mixture (essentially a point in the `self.num_topics - 1` simplex.\\n\\n        '\n    doc_topic = self.gammas / self.gammas.sum(axis=1)[:, np.newaxis]\n    return doc_topic[doc_number]",
            "def doc_topics(self, doc_number):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get the topic mixture for a document.\\n\\n        Uses the priors for the dirichlet distribution that approximates the true posterior with the optimal\\n        lower bound, and therefore requires the model to be already trained.\\n\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            Index of the document for which the mixture is returned.\\n\\n        Returns\\n        -------\\n        list of length `self.num_topics`\\n            Probability for each topic in the mixture (essentially a point in the `self.num_topics - 1` simplex.\\n\\n        '\n    doc_topic = self.gammas / self.gammas.sum(axis=1)[:, np.newaxis]\n    return doc_topic[doc_number]"
        ]
    },
    {
        "func_name": "normalize",
        "original": "def normalize(x):\n    return x / x.sum()",
        "mutated": [
            "def normalize(x):\n    if False:\n        i = 10\n    return x / x.sum()",
            "def normalize(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return x / x.sum()",
            "def normalize(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return x / x.sum()",
            "def normalize(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return x / x.sum()",
            "def normalize(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return x / x.sum()"
        ]
    },
    {
        "func_name": "dtm_vis",
        "original": "def dtm_vis(self, time, corpus):\n    \"\"\"Get the information needed to visualize the corpus model at a given time slice, using the pyLDAvis format.\n\n        Parameters\n        ----------\n        time : int\n            The time slice we are interested in.\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n            The corpus we want to visualize at the given time slice.\n\n        Returns\n        -------\n        doc_topics : list of length `self.num_topics`\n            Probability for each topic in the mixture (essentially a point in the `self.num_topics - 1` simplex.\n        topic_term : numpy.ndarray\n            The representation of each topic as a multinomial over words in the vocabulary,\n            expected shape (`num_topics`, vocabulary length).\n        doc_lengths : list of int\n            The number of words in each document. These could be fixed, or drawn from a Poisson distribution.\n        term_frequency : numpy.ndarray\n            The term frequency matrix (denoted as beta in the original Blei paper). This could also be the TF-IDF\n            representation of the corpus, expected shape (number of documents, length of vocabulary).\n        vocab : list of str\n            The set of unique terms existing in the cropuse's vocabulary.\n\n        \"\"\"\n    doc_topic = self.gammas / self.gammas.sum(axis=1)[:, np.newaxis]\n\n    def normalize(x):\n        return x / x.sum()\n    topic_term = [normalize(np.exp(chain.e_log_prob.T[time])) for (k, chain) in enumerate(self.topic_chains)]\n    doc_lengths = []\n    term_frequency = np.zeros(self.vocab_len)\n    for (doc_no, doc) in enumerate(corpus):\n        doc_lengths.append(len(doc))\n        for (term, freq) in doc:\n            term_frequency[term] += freq\n    vocab = [self.id2word[i] for i in range(len(self.id2word))]\n    return (doc_topic, np.array(topic_term), doc_lengths, term_frequency, vocab)",
        "mutated": [
            "def dtm_vis(self, time, corpus):\n    if False:\n        i = 10\n    \"Get the information needed to visualize the corpus model at a given time slice, using the pyLDAvis format.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice we are interested in.\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\\n            The corpus we want to visualize at the given time slice.\\n\\n        Returns\\n        -------\\n        doc_topics : list of length `self.num_topics`\\n            Probability for each topic in the mixture (essentially a point in the `self.num_topics - 1` simplex.\\n        topic_term : numpy.ndarray\\n            The representation of each topic as a multinomial over words in the vocabulary,\\n            expected shape (`num_topics`, vocabulary length).\\n        doc_lengths : list of int\\n            The number of words in each document. These could be fixed, or drawn from a Poisson distribution.\\n        term_frequency : numpy.ndarray\\n            The term frequency matrix (denoted as beta in the original Blei paper). This could also be the TF-IDF\\n            representation of the corpus, expected shape (number of documents, length of vocabulary).\\n        vocab : list of str\\n            The set of unique terms existing in the cropuse's vocabulary.\\n\\n        \"\n    doc_topic = self.gammas / self.gammas.sum(axis=1)[:, np.newaxis]\n\n    def normalize(x):\n        return x / x.sum()\n    topic_term = [normalize(np.exp(chain.e_log_prob.T[time])) for (k, chain) in enumerate(self.topic_chains)]\n    doc_lengths = []\n    term_frequency = np.zeros(self.vocab_len)\n    for (doc_no, doc) in enumerate(corpus):\n        doc_lengths.append(len(doc))\n        for (term, freq) in doc:\n            term_frequency[term] += freq\n    vocab = [self.id2word[i] for i in range(len(self.id2word))]\n    return (doc_topic, np.array(topic_term), doc_lengths, term_frequency, vocab)",
            "def dtm_vis(self, time, corpus):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Get the information needed to visualize the corpus model at a given time slice, using the pyLDAvis format.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice we are interested in.\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\\n            The corpus we want to visualize at the given time slice.\\n\\n        Returns\\n        -------\\n        doc_topics : list of length `self.num_topics`\\n            Probability for each topic in the mixture (essentially a point in the `self.num_topics - 1` simplex.\\n        topic_term : numpy.ndarray\\n            The representation of each topic as a multinomial over words in the vocabulary,\\n            expected shape (`num_topics`, vocabulary length).\\n        doc_lengths : list of int\\n            The number of words in each document. These could be fixed, or drawn from a Poisson distribution.\\n        term_frequency : numpy.ndarray\\n            The term frequency matrix (denoted as beta in the original Blei paper). This could also be the TF-IDF\\n            representation of the corpus, expected shape (number of documents, length of vocabulary).\\n        vocab : list of str\\n            The set of unique terms existing in the cropuse's vocabulary.\\n\\n        \"\n    doc_topic = self.gammas / self.gammas.sum(axis=1)[:, np.newaxis]\n\n    def normalize(x):\n        return x / x.sum()\n    topic_term = [normalize(np.exp(chain.e_log_prob.T[time])) for (k, chain) in enumerate(self.topic_chains)]\n    doc_lengths = []\n    term_frequency = np.zeros(self.vocab_len)\n    for (doc_no, doc) in enumerate(corpus):\n        doc_lengths.append(len(doc))\n        for (term, freq) in doc:\n            term_frequency[term] += freq\n    vocab = [self.id2word[i] for i in range(len(self.id2word))]\n    return (doc_topic, np.array(topic_term), doc_lengths, term_frequency, vocab)",
            "def dtm_vis(self, time, corpus):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Get the information needed to visualize the corpus model at a given time slice, using the pyLDAvis format.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice we are interested in.\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\\n            The corpus we want to visualize at the given time slice.\\n\\n        Returns\\n        -------\\n        doc_topics : list of length `self.num_topics`\\n            Probability for each topic in the mixture (essentially a point in the `self.num_topics - 1` simplex.\\n        topic_term : numpy.ndarray\\n            The representation of each topic as a multinomial over words in the vocabulary,\\n            expected shape (`num_topics`, vocabulary length).\\n        doc_lengths : list of int\\n            The number of words in each document. These could be fixed, or drawn from a Poisson distribution.\\n        term_frequency : numpy.ndarray\\n            The term frequency matrix (denoted as beta in the original Blei paper). This could also be the TF-IDF\\n            representation of the corpus, expected shape (number of documents, length of vocabulary).\\n        vocab : list of str\\n            The set of unique terms existing in the cropuse's vocabulary.\\n\\n        \"\n    doc_topic = self.gammas / self.gammas.sum(axis=1)[:, np.newaxis]\n\n    def normalize(x):\n        return x / x.sum()\n    topic_term = [normalize(np.exp(chain.e_log_prob.T[time])) for (k, chain) in enumerate(self.topic_chains)]\n    doc_lengths = []\n    term_frequency = np.zeros(self.vocab_len)\n    for (doc_no, doc) in enumerate(corpus):\n        doc_lengths.append(len(doc))\n        for (term, freq) in doc:\n            term_frequency[term] += freq\n    vocab = [self.id2word[i] for i in range(len(self.id2word))]\n    return (doc_topic, np.array(topic_term), doc_lengths, term_frequency, vocab)",
            "def dtm_vis(self, time, corpus):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Get the information needed to visualize the corpus model at a given time slice, using the pyLDAvis format.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice we are interested in.\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\\n            The corpus we want to visualize at the given time slice.\\n\\n        Returns\\n        -------\\n        doc_topics : list of length `self.num_topics`\\n            Probability for each topic in the mixture (essentially a point in the `self.num_topics - 1` simplex.\\n        topic_term : numpy.ndarray\\n            The representation of each topic as a multinomial over words in the vocabulary,\\n            expected shape (`num_topics`, vocabulary length).\\n        doc_lengths : list of int\\n            The number of words in each document. These could be fixed, or drawn from a Poisson distribution.\\n        term_frequency : numpy.ndarray\\n            The term frequency matrix (denoted as beta in the original Blei paper). This could also be the TF-IDF\\n            representation of the corpus, expected shape (number of documents, length of vocabulary).\\n        vocab : list of str\\n            The set of unique terms existing in the cropuse's vocabulary.\\n\\n        \"\n    doc_topic = self.gammas / self.gammas.sum(axis=1)[:, np.newaxis]\n\n    def normalize(x):\n        return x / x.sum()\n    topic_term = [normalize(np.exp(chain.e_log_prob.T[time])) for (k, chain) in enumerate(self.topic_chains)]\n    doc_lengths = []\n    term_frequency = np.zeros(self.vocab_len)\n    for (doc_no, doc) in enumerate(corpus):\n        doc_lengths.append(len(doc))\n        for (term, freq) in doc:\n            term_frequency[term] += freq\n    vocab = [self.id2word[i] for i in range(len(self.id2word))]\n    return (doc_topic, np.array(topic_term), doc_lengths, term_frequency, vocab)",
            "def dtm_vis(self, time, corpus):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Get the information needed to visualize the corpus model at a given time slice, using the pyLDAvis format.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice we are interested in.\\n        corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\\n            The corpus we want to visualize at the given time slice.\\n\\n        Returns\\n        -------\\n        doc_topics : list of length `self.num_topics`\\n            Probability for each topic in the mixture (essentially a point in the `self.num_topics - 1` simplex.\\n        topic_term : numpy.ndarray\\n            The representation of each topic as a multinomial over words in the vocabulary,\\n            expected shape (`num_topics`, vocabulary length).\\n        doc_lengths : list of int\\n            The number of words in each document. These could be fixed, or drawn from a Poisson distribution.\\n        term_frequency : numpy.ndarray\\n            The term frequency matrix (denoted as beta in the original Blei paper). This could also be the TF-IDF\\n            representation of the corpus, expected shape (number of documents, length of vocabulary).\\n        vocab : list of str\\n            The set of unique terms existing in the cropuse's vocabulary.\\n\\n        \"\n    doc_topic = self.gammas / self.gammas.sum(axis=1)[:, np.newaxis]\n\n    def normalize(x):\n        return x / x.sum()\n    topic_term = [normalize(np.exp(chain.e_log_prob.T[time])) for (k, chain) in enumerate(self.topic_chains)]\n    doc_lengths = []\n    term_frequency = np.zeros(self.vocab_len)\n    for (doc_no, doc) in enumerate(corpus):\n        doc_lengths.append(len(doc))\n        for (term, freq) in doc:\n            term_frequency[term] += freq\n    vocab = [self.id2word[i] for i in range(len(self.id2word))]\n    return (doc_topic, np.array(topic_term), doc_lengths, term_frequency, vocab)"
        ]
    },
    {
        "func_name": "dtm_coherence",
        "original": "def dtm_coherence(self, time):\n    \"\"\"Get the coherence for each topic.\n\n        Can be used to measure the quality of the model, or to inspect the convergence through training via a callback.\n\n        Parameters\n        ----------\n        time : int\n            The time slice.\n\n        Returns\n        -------\n        list of list of str\n            The word representation for each topic, for each time slice. This can be used to check the time coherence\n            of topics as time evolves: If the most relevant words remain the same then the topic has somehow\n            converged or is relatively static, if they change rapidly the topic is evolving.\n\n        \"\"\"\n    coherence_topics = []\n    for topics in self.print_topics(time):\n        coherence_topic = []\n        for (word, dist) in topics:\n            coherence_topic.append(word)\n        coherence_topics.append(coherence_topic)\n    return coherence_topics",
        "mutated": [
            "def dtm_coherence(self, time):\n    if False:\n        i = 10\n    'Get the coherence for each topic.\\n\\n        Can be used to measure the quality of the model, or to inspect the convergence through training via a callback.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice.\\n\\n        Returns\\n        -------\\n        list of list of str\\n            The word representation for each topic, for each time slice. This can be used to check the time coherence\\n            of topics as time evolves: If the most relevant words remain the same then the topic has somehow\\n            converged or is relatively static, if they change rapidly the topic is evolving.\\n\\n        '\n    coherence_topics = []\n    for topics in self.print_topics(time):\n        coherence_topic = []\n        for (word, dist) in topics:\n            coherence_topic.append(word)\n        coherence_topics.append(coherence_topic)\n    return coherence_topics",
            "def dtm_coherence(self, time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get the coherence for each topic.\\n\\n        Can be used to measure the quality of the model, or to inspect the convergence through training via a callback.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice.\\n\\n        Returns\\n        -------\\n        list of list of str\\n            The word representation for each topic, for each time slice. This can be used to check the time coherence\\n            of topics as time evolves: If the most relevant words remain the same then the topic has somehow\\n            converged or is relatively static, if they change rapidly the topic is evolving.\\n\\n        '\n    coherence_topics = []\n    for topics in self.print_topics(time):\n        coherence_topic = []\n        for (word, dist) in topics:\n            coherence_topic.append(word)\n        coherence_topics.append(coherence_topic)\n    return coherence_topics",
            "def dtm_coherence(self, time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get the coherence for each topic.\\n\\n        Can be used to measure the quality of the model, or to inspect the convergence through training via a callback.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice.\\n\\n        Returns\\n        -------\\n        list of list of str\\n            The word representation for each topic, for each time slice. This can be used to check the time coherence\\n            of topics as time evolves: If the most relevant words remain the same then the topic has somehow\\n            converged or is relatively static, if they change rapidly the topic is evolving.\\n\\n        '\n    coherence_topics = []\n    for topics in self.print_topics(time):\n        coherence_topic = []\n        for (word, dist) in topics:\n            coherence_topic.append(word)\n        coherence_topics.append(coherence_topic)\n    return coherence_topics",
            "def dtm_coherence(self, time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get the coherence for each topic.\\n\\n        Can be used to measure the quality of the model, or to inspect the convergence through training via a callback.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice.\\n\\n        Returns\\n        -------\\n        list of list of str\\n            The word representation for each topic, for each time slice. This can be used to check the time coherence\\n            of topics as time evolves: If the most relevant words remain the same then the topic has somehow\\n            converged or is relatively static, if they change rapidly the topic is evolving.\\n\\n        '\n    coherence_topics = []\n    for topics in self.print_topics(time):\n        coherence_topic = []\n        for (word, dist) in topics:\n            coherence_topic.append(word)\n        coherence_topics.append(coherence_topic)\n    return coherence_topics",
            "def dtm_coherence(self, time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get the coherence for each topic.\\n\\n        Can be used to measure the quality of the model, or to inspect the convergence through training via a callback.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice.\\n\\n        Returns\\n        -------\\n        list of list of str\\n            The word representation for each topic, for each time slice. This can be used to check the time coherence\\n            of topics as time evolves: If the most relevant words remain the same then the topic has somehow\\n            converged or is relatively static, if they change rapidly the topic is evolving.\\n\\n        '\n    coherence_topics = []\n    for topics in self.print_topics(time):\n        coherence_topic = []\n        for (word, dist) in topics:\n            coherence_topic.append(word)\n        coherence_topics.append(coherence_topic)\n    return coherence_topics"
        ]
    },
    {
        "func_name": "__getitem__",
        "original": "def __getitem__(self, doc):\n    \"\"\"Get the topic mixture for the given document, using the inferred approximation of the true posterior.\n\n        Parameters\n        ----------\n        doc : list of (int, float)\n            The doc in BOW format. Can be an unseen document.\n\n        Returns\n        -------\n        list of float\n            Probabilities for each topic in the mixture. This is essentially a point in the `num_topics - 1` simplex.\n\n        \"\"\"\n    lda_model = ldamodel.LdaModel(num_topics=self.num_topics, alpha=self.alphas, id2word=self.id2word, dtype=np.float64)\n    lda_model.topics = np.zeros((self.vocab_len, self.num_topics))\n    ldapost = LdaPost(num_topics=self.num_topics, max_doc_len=len(doc), lda=lda_model, doc=doc)\n    time_lhoods = []\n    for time in range(self.num_time_slices):\n        lda_model = self.make_lda_seq_slice(lda_model, time)\n        lhood = LdaPost.fit_lda_post(ldapost, 0, time, self)\n        time_lhoods.append(lhood)\n    doc_topic = ldapost.gamma / ldapost.gamma.sum()\n    return doc_topic",
        "mutated": [
            "def __getitem__(self, doc):\n    if False:\n        i = 10\n    'Get the topic mixture for the given document, using the inferred approximation of the true posterior.\\n\\n        Parameters\\n        ----------\\n        doc : list of (int, float)\\n            The doc in BOW format. Can be an unseen document.\\n\\n        Returns\\n        -------\\n        list of float\\n            Probabilities for each topic in the mixture. This is essentially a point in the `num_topics - 1` simplex.\\n\\n        '\n    lda_model = ldamodel.LdaModel(num_topics=self.num_topics, alpha=self.alphas, id2word=self.id2word, dtype=np.float64)\n    lda_model.topics = np.zeros((self.vocab_len, self.num_topics))\n    ldapost = LdaPost(num_topics=self.num_topics, max_doc_len=len(doc), lda=lda_model, doc=doc)\n    time_lhoods = []\n    for time in range(self.num_time_slices):\n        lda_model = self.make_lda_seq_slice(lda_model, time)\n        lhood = LdaPost.fit_lda_post(ldapost, 0, time, self)\n        time_lhoods.append(lhood)\n    doc_topic = ldapost.gamma / ldapost.gamma.sum()\n    return doc_topic",
            "def __getitem__(self, doc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get the topic mixture for the given document, using the inferred approximation of the true posterior.\\n\\n        Parameters\\n        ----------\\n        doc : list of (int, float)\\n            The doc in BOW format. Can be an unseen document.\\n\\n        Returns\\n        -------\\n        list of float\\n            Probabilities for each topic in the mixture. This is essentially a point in the `num_topics - 1` simplex.\\n\\n        '\n    lda_model = ldamodel.LdaModel(num_topics=self.num_topics, alpha=self.alphas, id2word=self.id2word, dtype=np.float64)\n    lda_model.topics = np.zeros((self.vocab_len, self.num_topics))\n    ldapost = LdaPost(num_topics=self.num_topics, max_doc_len=len(doc), lda=lda_model, doc=doc)\n    time_lhoods = []\n    for time in range(self.num_time_slices):\n        lda_model = self.make_lda_seq_slice(lda_model, time)\n        lhood = LdaPost.fit_lda_post(ldapost, 0, time, self)\n        time_lhoods.append(lhood)\n    doc_topic = ldapost.gamma / ldapost.gamma.sum()\n    return doc_topic",
            "def __getitem__(self, doc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get the topic mixture for the given document, using the inferred approximation of the true posterior.\\n\\n        Parameters\\n        ----------\\n        doc : list of (int, float)\\n            The doc in BOW format. Can be an unseen document.\\n\\n        Returns\\n        -------\\n        list of float\\n            Probabilities for each topic in the mixture. This is essentially a point in the `num_topics - 1` simplex.\\n\\n        '\n    lda_model = ldamodel.LdaModel(num_topics=self.num_topics, alpha=self.alphas, id2word=self.id2word, dtype=np.float64)\n    lda_model.topics = np.zeros((self.vocab_len, self.num_topics))\n    ldapost = LdaPost(num_topics=self.num_topics, max_doc_len=len(doc), lda=lda_model, doc=doc)\n    time_lhoods = []\n    for time in range(self.num_time_slices):\n        lda_model = self.make_lda_seq_slice(lda_model, time)\n        lhood = LdaPost.fit_lda_post(ldapost, 0, time, self)\n        time_lhoods.append(lhood)\n    doc_topic = ldapost.gamma / ldapost.gamma.sum()\n    return doc_topic",
            "def __getitem__(self, doc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get the topic mixture for the given document, using the inferred approximation of the true posterior.\\n\\n        Parameters\\n        ----------\\n        doc : list of (int, float)\\n            The doc in BOW format. Can be an unseen document.\\n\\n        Returns\\n        -------\\n        list of float\\n            Probabilities for each topic in the mixture. This is essentially a point in the `num_topics - 1` simplex.\\n\\n        '\n    lda_model = ldamodel.LdaModel(num_topics=self.num_topics, alpha=self.alphas, id2word=self.id2word, dtype=np.float64)\n    lda_model.topics = np.zeros((self.vocab_len, self.num_topics))\n    ldapost = LdaPost(num_topics=self.num_topics, max_doc_len=len(doc), lda=lda_model, doc=doc)\n    time_lhoods = []\n    for time in range(self.num_time_slices):\n        lda_model = self.make_lda_seq_slice(lda_model, time)\n        lhood = LdaPost.fit_lda_post(ldapost, 0, time, self)\n        time_lhoods.append(lhood)\n    doc_topic = ldapost.gamma / ldapost.gamma.sum()\n    return doc_topic",
            "def __getitem__(self, doc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get the topic mixture for the given document, using the inferred approximation of the true posterior.\\n\\n        Parameters\\n        ----------\\n        doc : list of (int, float)\\n            The doc in BOW format. Can be an unseen document.\\n\\n        Returns\\n        -------\\n        list of float\\n            Probabilities for each topic in the mixture. This is essentially a point in the `num_topics - 1` simplex.\\n\\n        '\n    lda_model = ldamodel.LdaModel(num_topics=self.num_topics, alpha=self.alphas, id2word=self.id2word, dtype=np.float64)\n    lda_model.topics = np.zeros((self.vocab_len, self.num_topics))\n    ldapost = LdaPost(num_topics=self.num_topics, max_doc_len=len(doc), lda=lda_model, doc=doc)\n    time_lhoods = []\n    for time in range(self.num_time_slices):\n        lda_model = self.make_lda_seq_slice(lda_model, time)\n        lhood = LdaPost.fit_lda_post(ldapost, 0, time, self)\n        time_lhoods.append(lhood)\n    doc_topic = ldapost.gamma / ldapost.gamma.sum()\n    return doc_topic"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, vocab_len=None, num_time_slices=None, num_topics=None, obs_variance=0.5, chain_variance=0.005):\n    self.vocab_len = vocab_len\n    self.num_time_slices = num_time_slices\n    self.obs_variance = obs_variance\n    self.chain_variance = chain_variance\n    self.num_topics = num_topics\n    self.obs = np.zeros((vocab_len, num_time_slices))\n    self.e_log_prob = np.zeros((vocab_len, num_time_slices))\n    self.mean = np.zeros((vocab_len, num_time_slices + 1))\n    self.fwd_mean = np.zeros((vocab_len, num_time_slices + 1))\n    self.fwd_variance = np.zeros((vocab_len, num_time_slices + 1))\n    self.variance = np.zeros((vocab_len, num_time_slices + 1))\n    self.zeta = np.zeros(num_time_slices)\n    self.m_update_coeff = None\n    self.mean_t = None\n    self.variance_t = None\n    self.influence_sum_lgl = None\n    self.w_phi_l = None\n    self.w_phi_sum = None\n    self.w_phi_l_sq = None\n    self.m_update_coeff_g = None",
        "mutated": [
            "def __init__(self, vocab_len=None, num_time_slices=None, num_topics=None, obs_variance=0.5, chain_variance=0.005):\n    if False:\n        i = 10\n    self.vocab_len = vocab_len\n    self.num_time_slices = num_time_slices\n    self.obs_variance = obs_variance\n    self.chain_variance = chain_variance\n    self.num_topics = num_topics\n    self.obs = np.zeros((vocab_len, num_time_slices))\n    self.e_log_prob = np.zeros((vocab_len, num_time_slices))\n    self.mean = np.zeros((vocab_len, num_time_slices + 1))\n    self.fwd_mean = np.zeros((vocab_len, num_time_slices + 1))\n    self.fwd_variance = np.zeros((vocab_len, num_time_slices + 1))\n    self.variance = np.zeros((vocab_len, num_time_slices + 1))\n    self.zeta = np.zeros(num_time_slices)\n    self.m_update_coeff = None\n    self.mean_t = None\n    self.variance_t = None\n    self.influence_sum_lgl = None\n    self.w_phi_l = None\n    self.w_phi_sum = None\n    self.w_phi_l_sq = None\n    self.m_update_coeff_g = None",
            "def __init__(self, vocab_len=None, num_time_slices=None, num_topics=None, obs_variance=0.5, chain_variance=0.005):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.vocab_len = vocab_len\n    self.num_time_slices = num_time_slices\n    self.obs_variance = obs_variance\n    self.chain_variance = chain_variance\n    self.num_topics = num_topics\n    self.obs = np.zeros((vocab_len, num_time_slices))\n    self.e_log_prob = np.zeros((vocab_len, num_time_slices))\n    self.mean = np.zeros((vocab_len, num_time_slices + 1))\n    self.fwd_mean = np.zeros((vocab_len, num_time_slices + 1))\n    self.fwd_variance = np.zeros((vocab_len, num_time_slices + 1))\n    self.variance = np.zeros((vocab_len, num_time_slices + 1))\n    self.zeta = np.zeros(num_time_slices)\n    self.m_update_coeff = None\n    self.mean_t = None\n    self.variance_t = None\n    self.influence_sum_lgl = None\n    self.w_phi_l = None\n    self.w_phi_sum = None\n    self.w_phi_l_sq = None\n    self.m_update_coeff_g = None",
            "def __init__(self, vocab_len=None, num_time_slices=None, num_topics=None, obs_variance=0.5, chain_variance=0.005):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.vocab_len = vocab_len\n    self.num_time_slices = num_time_slices\n    self.obs_variance = obs_variance\n    self.chain_variance = chain_variance\n    self.num_topics = num_topics\n    self.obs = np.zeros((vocab_len, num_time_slices))\n    self.e_log_prob = np.zeros((vocab_len, num_time_slices))\n    self.mean = np.zeros((vocab_len, num_time_slices + 1))\n    self.fwd_mean = np.zeros((vocab_len, num_time_slices + 1))\n    self.fwd_variance = np.zeros((vocab_len, num_time_slices + 1))\n    self.variance = np.zeros((vocab_len, num_time_slices + 1))\n    self.zeta = np.zeros(num_time_slices)\n    self.m_update_coeff = None\n    self.mean_t = None\n    self.variance_t = None\n    self.influence_sum_lgl = None\n    self.w_phi_l = None\n    self.w_phi_sum = None\n    self.w_phi_l_sq = None\n    self.m_update_coeff_g = None",
            "def __init__(self, vocab_len=None, num_time_slices=None, num_topics=None, obs_variance=0.5, chain_variance=0.005):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.vocab_len = vocab_len\n    self.num_time_slices = num_time_slices\n    self.obs_variance = obs_variance\n    self.chain_variance = chain_variance\n    self.num_topics = num_topics\n    self.obs = np.zeros((vocab_len, num_time_slices))\n    self.e_log_prob = np.zeros((vocab_len, num_time_slices))\n    self.mean = np.zeros((vocab_len, num_time_slices + 1))\n    self.fwd_mean = np.zeros((vocab_len, num_time_slices + 1))\n    self.fwd_variance = np.zeros((vocab_len, num_time_slices + 1))\n    self.variance = np.zeros((vocab_len, num_time_slices + 1))\n    self.zeta = np.zeros(num_time_slices)\n    self.m_update_coeff = None\n    self.mean_t = None\n    self.variance_t = None\n    self.influence_sum_lgl = None\n    self.w_phi_l = None\n    self.w_phi_sum = None\n    self.w_phi_l_sq = None\n    self.m_update_coeff_g = None",
            "def __init__(self, vocab_len=None, num_time_slices=None, num_topics=None, obs_variance=0.5, chain_variance=0.005):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.vocab_len = vocab_len\n    self.num_time_slices = num_time_slices\n    self.obs_variance = obs_variance\n    self.chain_variance = chain_variance\n    self.num_topics = num_topics\n    self.obs = np.zeros((vocab_len, num_time_slices))\n    self.e_log_prob = np.zeros((vocab_len, num_time_slices))\n    self.mean = np.zeros((vocab_len, num_time_slices + 1))\n    self.fwd_mean = np.zeros((vocab_len, num_time_slices + 1))\n    self.fwd_variance = np.zeros((vocab_len, num_time_slices + 1))\n    self.variance = np.zeros((vocab_len, num_time_slices + 1))\n    self.zeta = np.zeros(num_time_slices)\n    self.m_update_coeff = None\n    self.mean_t = None\n    self.variance_t = None\n    self.influence_sum_lgl = None\n    self.w_phi_l = None\n    self.w_phi_sum = None\n    self.w_phi_l_sq = None\n    self.m_update_coeff_g = None"
        ]
    },
    {
        "func_name": "update_zeta",
        "original": "def update_zeta(self):\n    \"\"\"Update the Zeta variational parameter.\n\n        Zeta is described in the appendix and is equal to sum (exp(mean[word] + Variance[word] / 2)),\n        over every time-slice. It is the value of variational parameter zeta which maximizes the lower bound.\n\n        Returns\n        -------\n        list of float\n            The updated zeta values for each time slice.\n\n        \"\"\"\n    for (j, val) in enumerate(self.zeta):\n        self.zeta[j] = np.sum(np.exp(self.mean[:, j + 1] + self.variance[:, j + 1] / 2))\n    return self.zeta",
        "mutated": [
            "def update_zeta(self):\n    if False:\n        i = 10\n    'Update the Zeta variational parameter.\\n\\n        Zeta is described in the appendix and is equal to sum (exp(mean[word] + Variance[word] / 2)),\\n        over every time-slice. It is the value of variational parameter zeta which maximizes the lower bound.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated zeta values for each time slice.\\n\\n        '\n    for (j, val) in enumerate(self.zeta):\n        self.zeta[j] = np.sum(np.exp(self.mean[:, j + 1] + self.variance[:, j + 1] / 2))\n    return self.zeta",
            "def update_zeta(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Update the Zeta variational parameter.\\n\\n        Zeta is described in the appendix and is equal to sum (exp(mean[word] + Variance[word] / 2)),\\n        over every time-slice. It is the value of variational parameter zeta which maximizes the lower bound.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated zeta values for each time slice.\\n\\n        '\n    for (j, val) in enumerate(self.zeta):\n        self.zeta[j] = np.sum(np.exp(self.mean[:, j + 1] + self.variance[:, j + 1] / 2))\n    return self.zeta",
            "def update_zeta(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Update the Zeta variational parameter.\\n\\n        Zeta is described in the appendix and is equal to sum (exp(mean[word] + Variance[word] / 2)),\\n        over every time-slice. It is the value of variational parameter zeta which maximizes the lower bound.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated zeta values for each time slice.\\n\\n        '\n    for (j, val) in enumerate(self.zeta):\n        self.zeta[j] = np.sum(np.exp(self.mean[:, j + 1] + self.variance[:, j + 1] / 2))\n    return self.zeta",
            "def update_zeta(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Update the Zeta variational parameter.\\n\\n        Zeta is described in the appendix and is equal to sum (exp(mean[word] + Variance[word] / 2)),\\n        over every time-slice. It is the value of variational parameter zeta which maximizes the lower bound.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated zeta values for each time slice.\\n\\n        '\n    for (j, val) in enumerate(self.zeta):\n        self.zeta[j] = np.sum(np.exp(self.mean[:, j + 1] + self.variance[:, j + 1] / 2))\n    return self.zeta",
            "def update_zeta(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Update the Zeta variational parameter.\\n\\n        Zeta is described in the appendix and is equal to sum (exp(mean[word] + Variance[word] / 2)),\\n        over every time-slice. It is the value of variational parameter zeta which maximizes the lower bound.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated zeta values for each time slice.\\n\\n        '\n    for (j, val) in enumerate(self.zeta):\n        self.zeta[j] = np.sum(np.exp(self.mean[:, j + 1] + self.variance[:, j + 1] / 2))\n    return self.zeta"
        ]
    },
    {
        "func_name": "compute_post_variance",
        "original": "def compute_post_variance(self, word, chain_variance):\n    \"\"\"Get the variance, based on the\n        `Variational Kalman Filtering approach for Approximate Inference (section 3.1)\n        <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\n\n        This function accepts the word to compute variance for, along with the associated sslm class object,\n        and returns the `variance` and the posterior approximation `fwd_variance`.\n\n        Notes\n        -----\n        This function essentially computes Var[\\\\beta_{t,w}] for t = 1:T\n\n        .. :math::\n\n            fwd\\\\_variance[t] \\\\equiv E((beta_{t,w}-mean_{t,w})^2 |beta_{t}\\\\ for\\\\ 1:t) =\n            (obs\\\\_variance / fwd\\\\_variance[t - 1] + chain\\\\_variance + obs\\\\_variance ) *\n            (fwd\\\\_variance[t - 1] + obs\\\\_variance)\n\n        .. :math::\n\n            variance[t] \\\\equiv E((beta_{t,w}-mean\\\\_cap_{t,w})^2 |beta\\\\_cap_{t}\\\\ for\\\\ 1:t) =\n            fwd\\\\_variance[t - 1] + (fwd\\\\_variance[t - 1] / fwd\\\\_variance[t - 1] + obs\\\\_variance)^2 *\n            (variance[t - 1] - (fwd\\\\_variance[t-1] + obs\\\\_variance))\n\n        Parameters\n        ----------\n        word: int\n            The word's ID.\n        chain_variance : float\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\n\n        Returns\n        -------\n        (numpy.ndarray, numpy.ndarray)\n            The first returned value is the variance of each word in each time slice, the second value is the\n            inferred posterior variance for the same pairs.\n\n        \"\"\"\n    INIT_VARIANCE_CONST = 1000\n    T = self.num_time_slices\n    variance = self.variance[word]\n    fwd_variance = self.fwd_variance[word]\n    fwd_variance[0] = chain_variance * INIT_VARIANCE_CONST\n    for t in range(1, T + 1):\n        if self.obs_variance:\n            c = self.obs_variance / (fwd_variance[t - 1] + chain_variance + self.obs_variance)\n        else:\n            c = 0\n        fwd_variance[t] = c * (fwd_variance[t - 1] + chain_variance)\n    variance[T] = fwd_variance[T]\n    for t in range(T - 1, -1, -1):\n        if fwd_variance[t] > 0.0:\n            c = np.power(fwd_variance[t] / (fwd_variance[t] + chain_variance), 2)\n        else:\n            c = 0\n        variance[t] = c * (variance[t + 1] - chain_variance) + (1 - c) * fwd_variance[t]\n    return (variance, fwd_variance)",
        "mutated": [
            "def compute_post_variance(self, word, chain_variance):\n    if False:\n        i = 10\n    \"Get the variance, based on the\\n        `Variational Kalman Filtering approach for Approximate Inference (section 3.1)\\n        <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n\\n        This function accepts the word to compute variance for, along with the associated sslm class object,\\n        and returns the `variance` and the posterior approximation `fwd_variance`.\\n\\n        Notes\\n        -----\\n        This function essentially computes Var[\\\\beta_{t,w}] for t = 1:T\\n\\n        .. :math::\\n\\n            fwd\\\\_variance[t] \\\\equiv E((beta_{t,w}-mean_{t,w})^2 |beta_{t}\\\\ for\\\\ 1:t) =\\n            (obs\\\\_variance / fwd\\\\_variance[t - 1] + chain\\\\_variance + obs\\\\_variance ) *\\n            (fwd\\\\_variance[t - 1] + obs\\\\_variance)\\n\\n        .. :math::\\n\\n            variance[t] \\\\equiv E((beta_{t,w}-mean\\\\_cap_{t,w})^2 |beta\\\\_cap_{t}\\\\ for\\\\ 1:t) =\\n            fwd\\\\_variance[t - 1] + (fwd\\\\_variance[t - 1] / fwd\\\\_variance[t - 1] + obs\\\\_variance)^2 *\\n            (variance[t - 1] - (fwd\\\\_variance[t-1] + obs\\\\_variance))\\n\\n        Parameters\\n        ----------\\n        word: int\\n            The word's ID.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray, numpy.ndarray)\\n            The first returned value is the variance of each word in each time slice, the second value is the\\n            inferred posterior variance for the same pairs.\\n\\n        \"\n    INIT_VARIANCE_CONST = 1000\n    T = self.num_time_slices\n    variance = self.variance[word]\n    fwd_variance = self.fwd_variance[word]\n    fwd_variance[0] = chain_variance * INIT_VARIANCE_CONST\n    for t in range(1, T + 1):\n        if self.obs_variance:\n            c = self.obs_variance / (fwd_variance[t - 1] + chain_variance + self.obs_variance)\n        else:\n            c = 0\n        fwd_variance[t] = c * (fwd_variance[t - 1] + chain_variance)\n    variance[T] = fwd_variance[T]\n    for t in range(T - 1, -1, -1):\n        if fwd_variance[t] > 0.0:\n            c = np.power(fwd_variance[t] / (fwd_variance[t] + chain_variance), 2)\n        else:\n            c = 0\n        variance[t] = c * (variance[t + 1] - chain_variance) + (1 - c) * fwd_variance[t]\n    return (variance, fwd_variance)",
            "def compute_post_variance(self, word, chain_variance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Get the variance, based on the\\n        `Variational Kalman Filtering approach for Approximate Inference (section 3.1)\\n        <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n\\n        This function accepts the word to compute variance for, along with the associated sslm class object,\\n        and returns the `variance` and the posterior approximation `fwd_variance`.\\n\\n        Notes\\n        -----\\n        This function essentially computes Var[\\\\beta_{t,w}] for t = 1:T\\n\\n        .. :math::\\n\\n            fwd\\\\_variance[t] \\\\equiv E((beta_{t,w}-mean_{t,w})^2 |beta_{t}\\\\ for\\\\ 1:t) =\\n            (obs\\\\_variance / fwd\\\\_variance[t - 1] + chain\\\\_variance + obs\\\\_variance ) *\\n            (fwd\\\\_variance[t - 1] + obs\\\\_variance)\\n\\n        .. :math::\\n\\n            variance[t] \\\\equiv E((beta_{t,w}-mean\\\\_cap_{t,w})^2 |beta\\\\_cap_{t}\\\\ for\\\\ 1:t) =\\n            fwd\\\\_variance[t - 1] + (fwd\\\\_variance[t - 1] / fwd\\\\_variance[t - 1] + obs\\\\_variance)^2 *\\n            (variance[t - 1] - (fwd\\\\_variance[t-1] + obs\\\\_variance))\\n\\n        Parameters\\n        ----------\\n        word: int\\n            The word's ID.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray, numpy.ndarray)\\n            The first returned value is the variance of each word in each time slice, the second value is the\\n            inferred posterior variance for the same pairs.\\n\\n        \"\n    INIT_VARIANCE_CONST = 1000\n    T = self.num_time_slices\n    variance = self.variance[word]\n    fwd_variance = self.fwd_variance[word]\n    fwd_variance[0] = chain_variance * INIT_VARIANCE_CONST\n    for t in range(1, T + 1):\n        if self.obs_variance:\n            c = self.obs_variance / (fwd_variance[t - 1] + chain_variance + self.obs_variance)\n        else:\n            c = 0\n        fwd_variance[t] = c * (fwd_variance[t - 1] + chain_variance)\n    variance[T] = fwd_variance[T]\n    for t in range(T - 1, -1, -1):\n        if fwd_variance[t] > 0.0:\n            c = np.power(fwd_variance[t] / (fwd_variance[t] + chain_variance), 2)\n        else:\n            c = 0\n        variance[t] = c * (variance[t + 1] - chain_variance) + (1 - c) * fwd_variance[t]\n    return (variance, fwd_variance)",
            "def compute_post_variance(self, word, chain_variance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Get the variance, based on the\\n        `Variational Kalman Filtering approach for Approximate Inference (section 3.1)\\n        <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n\\n        This function accepts the word to compute variance for, along with the associated sslm class object,\\n        and returns the `variance` and the posterior approximation `fwd_variance`.\\n\\n        Notes\\n        -----\\n        This function essentially computes Var[\\\\beta_{t,w}] for t = 1:T\\n\\n        .. :math::\\n\\n            fwd\\\\_variance[t] \\\\equiv E((beta_{t,w}-mean_{t,w})^2 |beta_{t}\\\\ for\\\\ 1:t) =\\n            (obs\\\\_variance / fwd\\\\_variance[t - 1] + chain\\\\_variance + obs\\\\_variance ) *\\n            (fwd\\\\_variance[t - 1] + obs\\\\_variance)\\n\\n        .. :math::\\n\\n            variance[t] \\\\equiv E((beta_{t,w}-mean\\\\_cap_{t,w})^2 |beta\\\\_cap_{t}\\\\ for\\\\ 1:t) =\\n            fwd\\\\_variance[t - 1] + (fwd\\\\_variance[t - 1] / fwd\\\\_variance[t - 1] + obs\\\\_variance)^2 *\\n            (variance[t - 1] - (fwd\\\\_variance[t-1] + obs\\\\_variance))\\n\\n        Parameters\\n        ----------\\n        word: int\\n            The word's ID.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray, numpy.ndarray)\\n            The first returned value is the variance of each word in each time slice, the second value is the\\n            inferred posterior variance for the same pairs.\\n\\n        \"\n    INIT_VARIANCE_CONST = 1000\n    T = self.num_time_slices\n    variance = self.variance[word]\n    fwd_variance = self.fwd_variance[word]\n    fwd_variance[0] = chain_variance * INIT_VARIANCE_CONST\n    for t in range(1, T + 1):\n        if self.obs_variance:\n            c = self.obs_variance / (fwd_variance[t - 1] + chain_variance + self.obs_variance)\n        else:\n            c = 0\n        fwd_variance[t] = c * (fwd_variance[t - 1] + chain_variance)\n    variance[T] = fwd_variance[T]\n    for t in range(T - 1, -1, -1):\n        if fwd_variance[t] > 0.0:\n            c = np.power(fwd_variance[t] / (fwd_variance[t] + chain_variance), 2)\n        else:\n            c = 0\n        variance[t] = c * (variance[t + 1] - chain_variance) + (1 - c) * fwd_variance[t]\n    return (variance, fwd_variance)",
            "def compute_post_variance(self, word, chain_variance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Get the variance, based on the\\n        `Variational Kalman Filtering approach for Approximate Inference (section 3.1)\\n        <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n\\n        This function accepts the word to compute variance for, along with the associated sslm class object,\\n        and returns the `variance` and the posterior approximation `fwd_variance`.\\n\\n        Notes\\n        -----\\n        This function essentially computes Var[\\\\beta_{t,w}] for t = 1:T\\n\\n        .. :math::\\n\\n            fwd\\\\_variance[t] \\\\equiv E((beta_{t,w}-mean_{t,w})^2 |beta_{t}\\\\ for\\\\ 1:t) =\\n            (obs\\\\_variance / fwd\\\\_variance[t - 1] + chain\\\\_variance + obs\\\\_variance ) *\\n            (fwd\\\\_variance[t - 1] + obs\\\\_variance)\\n\\n        .. :math::\\n\\n            variance[t] \\\\equiv E((beta_{t,w}-mean\\\\_cap_{t,w})^2 |beta\\\\_cap_{t}\\\\ for\\\\ 1:t) =\\n            fwd\\\\_variance[t - 1] + (fwd\\\\_variance[t - 1] / fwd\\\\_variance[t - 1] + obs\\\\_variance)^2 *\\n            (variance[t - 1] - (fwd\\\\_variance[t-1] + obs\\\\_variance))\\n\\n        Parameters\\n        ----------\\n        word: int\\n            The word's ID.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray, numpy.ndarray)\\n            The first returned value is the variance of each word in each time slice, the second value is the\\n            inferred posterior variance for the same pairs.\\n\\n        \"\n    INIT_VARIANCE_CONST = 1000\n    T = self.num_time_slices\n    variance = self.variance[word]\n    fwd_variance = self.fwd_variance[word]\n    fwd_variance[0] = chain_variance * INIT_VARIANCE_CONST\n    for t in range(1, T + 1):\n        if self.obs_variance:\n            c = self.obs_variance / (fwd_variance[t - 1] + chain_variance + self.obs_variance)\n        else:\n            c = 0\n        fwd_variance[t] = c * (fwd_variance[t - 1] + chain_variance)\n    variance[T] = fwd_variance[T]\n    for t in range(T - 1, -1, -1):\n        if fwd_variance[t] > 0.0:\n            c = np.power(fwd_variance[t] / (fwd_variance[t] + chain_variance), 2)\n        else:\n            c = 0\n        variance[t] = c * (variance[t + 1] - chain_variance) + (1 - c) * fwd_variance[t]\n    return (variance, fwd_variance)",
            "def compute_post_variance(self, word, chain_variance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Get the variance, based on the\\n        `Variational Kalman Filtering approach for Approximate Inference (section 3.1)\\n        <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n\\n        This function accepts the word to compute variance for, along with the associated sslm class object,\\n        and returns the `variance` and the posterior approximation `fwd_variance`.\\n\\n        Notes\\n        -----\\n        This function essentially computes Var[\\\\beta_{t,w}] for t = 1:T\\n\\n        .. :math::\\n\\n            fwd\\\\_variance[t] \\\\equiv E((beta_{t,w}-mean_{t,w})^2 |beta_{t}\\\\ for\\\\ 1:t) =\\n            (obs\\\\_variance / fwd\\\\_variance[t - 1] + chain\\\\_variance + obs\\\\_variance ) *\\n            (fwd\\\\_variance[t - 1] + obs\\\\_variance)\\n\\n        .. :math::\\n\\n            variance[t] \\\\equiv E((beta_{t,w}-mean\\\\_cap_{t,w})^2 |beta\\\\_cap_{t}\\\\ for\\\\ 1:t) =\\n            fwd\\\\_variance[t - 1] + (fwd\\\\_variance[t - 1] / fwd\\\\_variance[t - 1] + obs\\\\_variance)^2 *\\n            (variance[t - 1] - (fwd\\\\_variance[t-1] + obs\\\\_variance))\\n\\n        Parameters\\n        ----------\\n        word: int\\n            The word's ID.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray, numpy.ndarray)\\n            The first returned value is the variance of each word in each time slice, the second value is the\\n            inferred posterior variance for the same pairs.\\n\\n        \"\n    INIT_VARIANCE_CONST = 1000\n    T = self.num_time_slices\n    variance = self.variance[word]\n    fwd_variance = self.fwd_variance[word]\n    fwd_variance[0] = chain_variance * INIT_VARIANCE_CONST\n    for t in range(1, T + 1):\n        if self.obs_variance:\n            c = self.obs_variance / (fwd_variance[t - 1] + chain_variance + self.obs_variance)\n        else:\n            c = 0\n        fwd_variance[t] = c * (fwd_variance[t - 1] + chain_variance)\n    variance[T] = fwd_variance[T]\n    for t in range(T - 1, -1, -1):\n        if fwd_variance[t] > 0.0:\n            c = np.power(fwd_variance[t] / (fwd_variance[t] + chain_variance), 2)\n        else:\n            c = 0\n        variance[t] = c * (variance[t + 1] - chain_variance) + (1 - c) * fwd_variance[t]\n    return (variance, fwd_variance)"
        ]
    },
    {
        "func_name": "compute_post_mean",
        "original": "def compute_post_mean(self, word, chain_variance):\n    \"\"\"Get the mean, based on the `Variational Kalman Filtering approach for Approximate Inference (section 3.1)\n        <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\n\n        Notes\n        -----\n        This function essentially computes E[\\x08eta_{t,w}] for t = 1:T.\n\n        .. :math::\n\n            Fwd_Mean(t) \u2261  E(beta_{t,w} | beta_\u02c6 1:t )\n            = (obs_variance / fwd_variance[t - 1] + chain_variance + obs_variance ) * fwd_mean[t - 1] +\n            (1 - (obs_variance / fwd_variance[t - 1] + chain_variance + obs_variance)) * beta\n\n        .. :math::\n\n            Mean(t) \u2261 E(beta_{t,w} | beta_\u02c6 1:T )\n            = fwd_mean[t - 1] + (obs_variance / fwd_variance[t - 1] + obs_variance) +\n            (1 - obs_variance / fwd_variance[t - 1] + obs_variance)) * mean[t]\n\n        Parameters\n        ----------\n        word: int\n            The word's ID.\n        chain_variance : float\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\n\n        Returns\n        -------\n        (numpy.ndarray, numpy.ndarray)\n            The first returned value is the mean of each word in each time slice, the second value is the\n            inferred posterior mean for the same pairs.\n\n        \"\"\"\n    T = self.num_time_slices\n    obs = self.obs[word]\n    fwd_variance = self.fwd_variance[word]\n    mean = self.mean[word]\n    fwd_mean = self.fwd_mean[word]\n    fwd_mean[0] = 0\n    for t in range(1, T + 1):\n        c = self.obs_variance / (fwd_variance[t - 1] + chain_variance + self.obs_variance)\n        fwd_mean[t] = c * fwd_mean[t - 1] + (1 - c) * obs[t - 1]\n    mean[T] = fwd_mean[T]\n    for t in range(T - 1, -1, -1):\n        if chain_variance == 0.0:\n            c = 0.0\n        else:\n            c = chain_variance / (fwd_variance[t] + chain_variance)\n        mean[t] = c * fwd_mean[t] + (1 - c) * mean[t + 1]\n    return (mean, fwd_mean)",
        "mutated": [
            "def compute_post_mean(self, word, chain_variance):\n    if False:\n        i = 10\n    \"Get the mean, based on the `Variational Kalman Filtering approach for Approximate Inference (section 3.1)\\n        <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n\\n        Notes\\n        -----\\n        This function essentially computes E[\\x08eta_{t,w}] for t = 1:T.\\n\\n        .. :math::\\n\\n            Fwd_Mean(t) \u2261  E(beta_{t,w} | beta_\u02c6 1:t )\\n            = (obs_variance / fwd_variance[t - 1] + chain_variance + obs_variance ) * fwd_mean[t - 1] +\\n            (1 - (obs_variance / fwd_variance[t - 1] + chain_variance + obs_variance)) * beta\\n\\n        .. :math::\\n\\n            Mean(t) \u2261 E(beta_{t,w} | beta_\u02c6 1:T )\\n            = fwd_mean[t - 1] + (obs_variance / fwd_variance[t - 1] + obs_variance) +\\n            (1 - obs_variance / fwd_variance[t - 1] + obs_variance)) * mean[t]\\n\\n        Parameters\\n        ----------\\n        word: int\\n            The word's ID.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray, numpy.ndarray)\\n            The first returned value is the mean of each word in each time slice, the second value is the\\n            inferred posterior mean for the same pairs.\\n\\n        \"\n    T = self.num_time_slices\n    obs = self.obs[word]\n    fwd_variance = self.fwd_variance[word]\n    mean = self.mean[word]\n    fwd_mean = self.fwd_mean[word]\n    fwd_mean[0] = 0\n    for t in range(1, T + 1):\n        c = self.obs_variance / (fwd_variance[t - 1] + chain_variance + self.obs_variance)\n        fwd_mean[t] = c * fwd_mean[t - 1] + (1 - c) * obs[t - 1]\n    mean[T] = fwd_mean[T]\n    for t in range(T - 1, -1, -1):\n        if chain_variance == 0.0:\n            c = 0.0\n        else:\n            c = chain_variance / (fwd_variance[t] + chain_variance)\n        mean[t] = c * fwd_mean[t] + (1 - c) * mean[t + 1]\n    return (mean, fwd_mean)",
            "def compute_post_mean(self, word, chain_variance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Get the mean, based on the `Variational Kalman Filtering approach for Approximate Inference (section 3.1)\\n        <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n\\n        Notes\\n        -----\\n        This function essentially computes E[\\x08eta_{t,w}] for t = 1:T.\\n\\n        .. :math::\\n\\n            Fwd_Mean(t) \u2261  E(beta_{t,w} | beta_\u02c6 1:t )\\n            = (obs_variance / fwd_variance[t - 1] + chain_variance + obs_variance ) * fwd_mean[t - 1] +\\n            (1 - (obs_variance / fwd_variance[t - 1] + chain_variance + obs_variance)) * beta\\n\\n        .. :math::\\n\\n            Mean(t) \u2261 E(beta_{t,w} | beta_\u02c6 1:T )\\n            = fwd_mean[t - 1] + (obs_variance / fwd_variance[t - 1] + obs_variance) +\\n            (1 - obs_variance / fwd_variance[t - 1] + obs_variance)) * mean[t]\\n\\n        Parameters\\n        ----------\\n        word: int\\n            The word's ID.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray, numpy.ndarray)\\n            The first returned value is the mean of each word in each time slice, the second value is the\\n            inferred posterior mean for the same pairs.\\n\\n        \"\n    T = self.num_time_slices\n    obs = self.obs[word]\n    fwd_variance = self.fwd_variance[word]\n    mean = self.mean[word]\n    fwd_mean = self.fwd_mean[word]\n    fwd_mean[0] = 0\n    for t in range(1, T + 1):\n        c = self.obs_variance / (fwd_variance[t - 1] + chain_variance + self.obs_variance)\n        fwd_mean[t] = c * fwd_mean[t - 1] + (1 - c) * obs[t - 1]\n    mean[T] = fwd_mean[T]\n    for t in range(T - 1, -1, -1):\n        if chain_variance == 0.0:\n            c = 0.0\n        else:\n            c = chain_variance / (fwd_variance[t] + chain_variance)\n        mean[t] = c * fwd_mean[t] + (1 - c) * mean[t + 1]\n    return (mean, fwd_mean)",
            "def compute_post_mean(self, word, chain_variance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Get the mean, based on the `Variational Kalman Filtering approach for Approximate Inference (section 3.1)\\n        <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n\\n        Notes\\n        -----\\n        This function essentially computes E[\\x08eta_{t,w}] for t = 1:T.\\n\\n        .. :math::\\n\\n            Fwd_Mean(t) \u2261  E(beta_{t,w} | beta_\u02c6 1:t )\\n            = (obs_variance / fwd_variance[t - 1] + chain_variance + obs_variance ) * fwd_mean[t - 1] +\\n            (1 - (obs_variance / fwd_variance[t - 1] + chain_variance + obs_variance)) * beta\\n\\n        .. :math::\\n\\n            Mean(t) \u2261 E(beta_{t,w} | beta_\u02c6 1:T )\\n            = fwd_mean[t - 1] + (obs_variance / fwd_variance[t - 1] + obs_variance) +\\n            (1 - obs_variance / fwd_variance[t - 1] + obs_variance)) * mean[t]\\n\\n        Parameters\\n        ----------\\n        word: int\\n            The word's ID.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray, numpy.ndarray)\\n            The first returned value is the mean of each word in each time slice, the second value is the\\n            inferred posterior mean for the same pairs.\\n\\n        \"\n    T = self.num_time_slices\n    obs = self.obs[word]\n    fwd_variance = self.fwd_variance[word]\n    mean = self.mean[word]\n    fwd_mean = self.fwd_mean[word]\n    fwd_mean[0] = 0\n    for t in range(1, T + 1):\n        c = self.obs_variance / (fwd_variance[t - 1] + chain_variance + self.obs_variance)\n        fwd_mean[t] = c * fwd_mean[t - 1] + (1 - c) * obs[t - 1]\n    mean[T] = fwd_mean[T]\n    for t in range(T - 1, -1, -1):\n        if chain_variance == 0.0:\n            c = 0.0\n        else:\n            c = chain_variance / (fwd_variance[t] + chain_variance)\n        mean[t] = c * fwd_mean[t] + (1 - c) * mean[t + 1]\n    return (mean, fwd_mean)",
            "def compute_post_mean(self, word, chain_variance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Get the mean, based on the `Variational Kalman Filtering approach for Approximate Inference (section 3.1)\\n        <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n\\n        Notes\\n        -----\\n        This function essentially computes E[\\x08eta_{t,w}] for t = 1:T.\\n\\n        .. :math::\\n\\n            Fwd_Mean(t) \u2261  E(beta_{t,w} | beta_\u02c6 1:t )\\n            = (obs_variance / fwd_variance[t - 1] + chain_variance + obs_variance ) * fwd_mean[t - 1] +\\n            (1 - (obs_variance / fwd_variance[t - 1] + chain_variance + obs_variance)) * beta\\n\\n        .. :math::\\n\\n            Mean(t) \u2261 E(beta_{t,w} | beta_\u02c6 1:T )\\n            = fwd_mean[t - 1] + (obs_variance / fwd_variance[t - 1] + obs_variance) +\\n            (1 - obs_variance / fwd_variance[t - 1] + obs_variance)) * mean[t]\\n\\n        Parameters\\n        ----------\\n        word: int\\n            The word's ID.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray, numpy.ndarray)\\n            The first returned value is the mean of each word in each time slice, the second value is the\\n            inferred posterior mean for the same pairs.\\n\\n        \"\n    T = self.num_time_slices\n    obs = self.obs[word]\n    fwd_variance = self.fwd_variance[word]\n    mean = self.mean[word]\n    fwd_mean = self.fwd_mean[word]\n    fwd_mean[0] = 0\n    for t in range(1, T + 1):\n        c = self.obs_variance / (fwd_variance[t - 1] + chain_variance + self.obs_variance)\n        fwd_mean[t] = c * fwd_mean[t - 1] + (1 - c) * obs[t - 1]\n    mean[T] = fwd_mean[T]\n    for t in range(T - 1, -1, -1):\n        if chain_variance == 0.0:\n            c = 0.0\n        else:\n            c = chain_variance / (fwd_variance[t] + chain_variance)\n        mean[t] = c * fwd_mean[t] + (1 - c) * mean[t + 1]\n    return (mean, fwd_mean)",
            "def compute_post_mean(self, word, chain_variance):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Get the mean, based on the `Variational Kalman Filtering approach for Approximate Inference (section 3.1)\\n        <https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf>`_.\\n\\n        Notes\\n        -----\\n        This function essentially computes E[\\x08eta_{t,w}] for t = 1:T.\\n\\n        .. :math::\\n\\n            Fwd_Mean(t) \u2261  E(beta_{t,w} | beta_\u02c6 1:t )\\n            = (obs_variance / fwd_variance[t - 1] + chain_variance + obs_variance ) * fwd_mean[t - 1] +\\n            (1 - (obs_variance / fwd_variance[t - 1] + chain_variance + obs_variance)) * beta\\n\\n        .. :math::\\n\\n            Mean(t) \u2261 E(beta_{t,w} | beta_\u02c6 1:T )\\n            = fwd_mean[t - 1] + (obs_variance / fwd_variance[t - 1] + obs_variance) +\\n            (1 - obs_variance / fwd_variance[t - 1] + obs_variance)) * mean[t]\\n\\n        Parameters\\n        ----------\\n        word: int\\n            The word's ID.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray, numpy.ndarray)\\n            The first returned value is the mean of each word in each time slice, the second value is the\\n            inferred posterior mean for the same pairs.\\n\\n        \"\n    T = self.num_time_slices\n    obs = self.obs[word]\n    fwd_variance = self.fwd_variance[word]\n    mean = self.mean[word]\n    fwd_mean = self.fwd_mean[word]\n    fwd_mean[0] = 0\n    for t in range(1, T + 1):\n        c = self.obs_variance / (fwd_variance[t - 1] + chain_variance + self.obs_variance)\n        fwd_mean[t] = c * fwd_mean[t - 1] + (1 - c) * obs[t - 1]\n    mean[T] = fwd_mean[T]\n    for t in range(T - 1, -1, -1):\n        if chain_variance == 0.0:\n            c = 0.0\n        else:\n            c = chain_variance / (fwd_variance[t] + chain_variance)\n        mean[t] = c * fwd_mean[t] + (1 - c) * mean[t + 1]\n    return (mean, fwd_mean)"
        ]
    },
    {
        "func_name": "compute_expected_log_prob",
        "original": "def compute_expected_log_prob(self):\n    \"\"\"Compute the expected log probability given values of m.\n\n        The appendix describes the Expectation of log-probabilities in equation 5 of the DTM paper;\n        The below implementation is the result of solving the equation and is implemented as in the original\n        Blei DTM code.\n\n        Returns\n        -------\n        numpy.ndarray of float\n            The expected value for the log probabilities for each word and time slice.\n\n        \"\"\"\n    for ((w, t), val) in np.ndenumerate(self.e_log_prob):\n        self.e_log_prob[w][t] = self.mean[w][t + 1] - np.log(self.zeta[t])\n    return self.e_log_prob",
        "mutated": [
            "def compute_expected_log_prob(self):\n    if False:\n        i = 10\n    'Compute the expected log probability given values of m.\\n\\n        The appendix describes the Expectation of log-probabilities in equation 5 of the DTM paper;\\n        The below implementation is the result of solving the equation and is implemented as in the original\\n        Blei DTM code.\\n\\n        Returns\\n        -------\\n        numpy.ndarray of float\\n            The expected value for the log probabilities for each word and time slice.\\n\\n        '\n    for ((w, t), val) in np.ndenumerate(self.e_log_prob):\n        self.e_log_prob[w][t] = self.mean[w][t + 1] - np.log(self.zeta[t])\n    return self.e_log_prob",
            "def compute_expected_log_prob(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the expected log probability given values of m.\\n\\n        The appendix describes the Expectation of log-probabilities in equation 5 of the DTM paper;\\n        The below implementation is the result of solving the equation and is implemented as in the original\\n        Blei DTM code.\\n\\n        Returns\\n        -------\\n        numpy.ndarray of float\\n            The expected value for the log probabilities for each word and time slice.\\n\\n        '\n    for ((w, t), val) in np.ndenumerate(self.e_log_prob):\n        self.e_log_prob[w][t] = self.mean[w][t + 1] - np.log(self.zeta[t])\n    return self.e_log_prob",
            "def compute_expected_log_prob(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the expected log probability given values of m.\\n\\n        The appendix describes the Expectation of log-probabilities in equation 5 of the DTM paper;\\n        The below implementation is the result of solving the equation and is implemented as in the original\\n        Blei DTM code.\\n\\n        Returns\\n        -------\\n        numpy.ndarray of float\\n            The expected value for the log probabilities for each word and time slice.\\n\\n        '\n    for ((w, t), val) in np.ndenumerate(self.e_log_prob):\n        self.e_log_prob[w][t] = self.mean[w][t + 1] - np.log(self.zeta[t])\n    return self.e_log_prob",
            "def compute_expected_log_prob(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the expected log probability given values of m.\\n\\n        The appendix describes the Expectation of log-probabilities in equation 5 of the DTM paper;\\n        The below implementation is the result of solving the equation and is implemented as in the original\\n        Blei DTM code.\\n\\n        Returns\\n        -------\\n        numpy.ndarray of float\\n            The expected value for the log probabilities for each word and time slice.\\n\\n        '\n    for ((w, t), val) in np.ndenumerate(self.e_log_prob):\n        self.e_log_prob[w][t] = self.mean[w][t + 1] - np.log(self.zeta[t])\n    return self.e_log_prob",
            "def compute_expected_log_prob(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the expected log probability given values of m.\\n\\n        The appendix describes the Expectation of log-probabilities in equation 5 of the DTM paper;\\n        The below implementation is the result of solving the equation and is implemented as in the original\\n        Blei DTM code.\\n\\n        Returns\\n        -------\\n        numpy.ndarray of float\\n            The expected value for the log probabilities for each word and time slice.\\n\\n        '\n    for ((w, t), val) in np.ndenumerate(self.e_log_prob):\n        self.e_log_prob[w][t] = self.mean[w][t + 1] - np.log(self.zeta[t])\n    return self.e_log_prob"
        ]
    },
    {
        "func_name": "sslm_counts_init",
        "original": "def sslm_counts_init(self, obs_variance, chain_variance, sstats):\n    \"\"\"Initialize the State Space Language Model with LDA sufficient statistics.\n\n        Called for each topic-chain and initializes initial mean, variance and Topic-Word probabilities\n        for the first time-slice.\n\n        Parameters\n        ----------\n        obs_variance : float, optional\n            Observed variance used to approximate the true and forward variance.\n        chain_variance : float\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\n        sstats : numpy.ndarray\n            Sufficient statistics of the LDA model. Corresponds to matrix beta in the linked paper for time slice 0,\n            expected shape (`self.vocab_len`, `num_topics`).\n\n        \"\"\"\n    W = self.vocab_len\n    T = self.num_time_slices\n    log_norm_counts = np.copy(sstats)\n    log_norm_counts /= sum(log_norm_counts)\n    log_norm_counts += 1.0 / W\n    log_norm_counts /= sum(log_norm_counts)\n    log_norm_counts = np.log(log_norm_counts)\n    self.obs = np.repeat(log_norm_counts, T, axis=0).reshape(W, T)\n    self.obs_variance = obs_variance\n    self.chain_variance = chain_variance\n    for w in range(W):\n        (self.variance[w], self.fwd_variance[w]) = self.compute_post_variance(w, self.chain_variance)\n        (self.mean[w], self.fwd_mean[w]) = self.compute_post_mean(w, self.chain_variance)\n    self.zeta = self.update_zeta()\n    self.e_log_prob = self.compute_expected_log_prob()",
        "mutated": [
            "def sslm_counts_init(self, obs_variance, chain_variance, sstats):\n    if False:\n        i = 10\n    'Initialize the State Space Language Model with LDA sufficient statistics.\\n\\n        Called for each topic-chain and initializes initial mean, variance and Topic-Word probabilities\\n        for the first time-slice.\\n\\n        Parameters\\n        ----------\\n        obs_variance : float, optional\\n            Observed variance used to approximate the true and forward variance.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n        sstats : numpy.ndarray\\n            Sufficient statistics of the LDA model. Corresponds to matrix beta in the linked paper for time slice 0,\\n            expected shape (`self.vocab_len`, `num_topics`).\\n\\n        '\n    W = self.vocab_len\n    T = self.num_time_slices\n    log_norm_counts = np.copy(sstats)\n    log_norm_counts /= sum(log_norm_counts)\n    log_norm_counts += 1.0 / W\n    log_norm_counts /= sum(log_norm_counts)\n    log_norm_counts = np.log(log_norm_counts)\n    self.obs = np.repeat(log_norm_counts, T, axis=0).reshape(W, T)\n    self.obs_variance = obs_variance\n    self.chain_variance = chain_variance\n    for w in range(W):\n        (self.variance[w], self.fwd_variance[w]) = self.compute_post_variance(w, self.chain_variance)\n        (self.mean[w], self.fwd_mean[w]) = self.compute_post_mean(w, self.chain_variance)\n    self.zeta = self.update_zeta()\n    self.e_log_prob = self.compute_expected_log_prob()",
            "def sslm_counts_init(self, obs_variance, chain_variance, sstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize the State Space Language Model with LDA sufficient statistics.\\n\\n        Called for each topic-chain and initializes initial mean, variance and Topic-Word probabilities\\n        for the first time-slice.\\n\\n        Parameters\\n        ----------\\n        obs_variance : float, optional\\n            Observed variance used to approximate the true and forward variance.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n        sstats : numpy.ndarray\\n            Sufficient statistics of the LDA model. Corresponds to matrix beta in the linked paper for time slice 0,\\n            expected shape (`self.vocab_len`, `num_topics`).\\n\\n        '\n    W = self.vocab_len\n    T = self.num_time_slices\n    log_norm_counts = np.copy(sstats)\n    log_norm_counts /= sum(log_norm_counts)\n    log_norm_counts += 1.0 / W\n    log_norm_counts /= sum(log_norm_counts)\n    log_norm_counts = np.log(log_norm_counts)\n    self.obs = np.repeat(log_norm_counts, T, axis=0).reshape(W, T)\n    self.obs_variance = obs_variance\n    self.chain_variance = chain_variance\n    for w in range(W):\n        (self.variance[w], self.fwd_variance[w]) = self.compute_post_variance(w, self.chain_variance)\n        (self.mean[w], self.fwd_mean[w]) = self.compute_post_mean(w, self.chain_variance)\n    self.zeta = self.update_zeta()\n    self.e_log_prob = self.compute_expected_log_prob()",
            "def sslm_counts_init(self, obs_variance, chain_variance, sstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize the State Space Language Model with LDA sufficient statistics.\\n\\n        Called for each topic-chain and initializes initial mean, variance and Topic-Word probabilities\\n        for the first time-slice.\\n\\n        Parameters\\n        ----------\\n        obs_variance : float, optional\\n            Observed variance used to approximate the true and forward variance.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n        sstats : numpy.ndarray\\n            Sufficient statistics of the LDA model. Corresponds to matrix beta in the linked paper for time slice 0,\\n            expected shape (`self.vocab_len`, `num_topics`).\\n\\n        '\n    W = self.vocab_len\n    T = self.num_time_slices\n    log_norm_counts = np.copy(sstats)\n    log_norm_counts /= sum(log_norm_counts)\n    log_norm_counts += 1.0 / W\n    log_norm_counts /= sum(log_norm_counts)\n    log_norm_counts = np.log(log_norm_counts)\n    self.obs = np.repeat(log_norm_counts, T, axis=0).reshape(W, T)\n    self.obs_variance = obs_variance\n    self.chain_variance = chain_variance\n    for w in range(W):\n        (self.variance[w], self.fwd_variance[w]) = self.compute_post_variance(w, self.chain_variance)\n        (self.mean[w], self.fwd_mean[w]) = self.compute_post_mean(w, self.chain_variance)\n    self.zeta = self.update_zeta()\n    self.e_log_prob = self.compute_expected_log_prob()",
            "def sslm_counts_init(self, obs_variance, chain_variance, sstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize the State Space Language Model with LDA sufficient statistics.\\n\\n        Called for each topic-chain and initializes initial mean, variance and Topic-Word probabilities\\n        for the first time-slice.\\n\\n        Parameters\\n        ----------\\n        obs_variance : float, optional\\n            Observed variance used to approximate the true and forward variance.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n        sstats : numpy.ndarray\\n            Sufficient statistics of the LDA model. Corresponds to matrix beta in the linked paper for time slice 0,\\n            expected shape (`self.vocab_len`, `num_topics`).\\n\\n        '\n    W = self.vocab_len\n    T = self.num_time_slices\n    log_norm_counts = np.copy(sstats)\n    log_norm_counts /= sum(log_norm_counts)\n    log_norm_counts += 1.0 / W\n    log_norm_counts /= sum(log_norm_counts)\n    log_norm_counts = np.log(log_norm_counts)\n    self.obs = np.repeat(log_norm_counts, T, axis=0).reshape(W, T)\n    self.obs_variance = obs_variance\n    self.chain_variance = chain_variance\n    for w in range(W):\n        (self.variance[w], self.fwd_variance[w]) = self.compute_post_variance(w, self.chain_variance)\n        (self.mean[w], self.fwd_mean[w]) = self.compute_post_mean(w, self.chain_variance)\n    self.zeta = self.update_zeta()\n    self.e_log_prob = self.compute_expected_log_prob()",
            "def sslm_counts_init(self, obs_variance, chain_variance, sstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize the State Space Language Model with LDA sufficient statistics.\\n\\n        Called for each topic-chain and initializes initial mean, variance and Topic-Word probabilities\\n        for the first time-slice.\\n\\n        Parameters\\n        ----------\\n        obs_variance : float, optional\\n            Observed variance used to approximate the true and forward variance.\\n        chain_variance : float\\n            Gaussian parameter defined in the beta distribution to dictate how the beta values evolve over time.\\n        sstats : numpy.ndarray\\n            Sufficient statistics of the LDA model. Corresponds to matrix beta in the linked paper for time slice 0,\\n            expected shape (`self.vocab_len`, `num_topics`).\\n\\n        '\n    W = self.vocab_len\n    T = self.num_time_slices\n    log_norm_counts = np.copy(sstats)\n    log_norm_counts /= sum(log_norm_counts)\n    log_norm_counts += 1.0 / W\n    log_norm_counts /= sum(log_norm_counts)\n    log_norm_counts = np.log(log_norm_counts)\n    self.obs = np.repeat(log_norm_counts, T, axis=0).reshape(W, T)\n    self.obs_variance = obs_variance\n    self.chain_variance = chain_variance\n    for w in range(W):\n        (self.variance[w], self.fwd_variance[w]) = self.compute_post_variance(w, self.chain_variance)\n        (self.mean[w], self.fwd_mean[w]) = self.compute_post_mean(w, self.chain_variance)\n    self.zeta = self.update_zeta()\n    self.e_log_prob = self.compute_expected_log_prob()"
        ]
    },
    {
        "func_name": "fit_sslm",
        "original": "def fit_sslm(self, sstats):\n    \"\"\"Fits variational distribution.\n\n        This is essentially the m-step.\n        Maximizes the approximation of the true posterior for a particular topic using the provided sufficient\n        statistics. Updates the values using :meth:`~gensim.models.ldaseqmodel.sslm.update_obs` and\n        :meth:`~gensim.models.ldaseqmodel.sslm.compute_expected_log_prob`.\n\n        Parameters\n        ----------\n        sstats : numpy.ndarray\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the\n            current time slice, expected shape (`self.vocab_len`, `num_topics`).\n\n        Returns\n        -------\n        float\n            The lower bound for the true posterior achieved using the fitted approximate distribution.\n\n        \"\"\"\n    W = self.vocab_len\n    bound = 0\n    old_bound = 0\n    sslm_fit_threshold = 1e-06\n    sslm_max_iter = 2\n    converged = sslm_fit_threshold + 1\n    (self.variance, self.fwd_variance) = (np.array(x) for x in zip(*(self.compute_post_variance(w, self.chain_variance) for w in range(W))))\n    totals = sstats.sum(axis=0)\n    iter_ = 0\n    model = 'DTM'\n    if model == 'DTM':\n        bound = self.compute_bound(sstats, totals)\n    if model == 'DIM':\n        bound = self.compute_bound_fixed(sstats, totals)\n    logger.info('initial sslm bound is %f', bound)\n    while converged > sslm_fit_threshold and iter_ < sslm_max_iter:\n        iter_ += 1\n        old_bound = bound\n        (self.obs, self.zeta) = self.update_obs(sstats, totals)\n        if model == 'DTM':\n            bound = self.compute_bound(sstats, totals)\n        if model == 'DIM':\n            bound = self.compute_bound_fixed(sstats, totals)\n        converged = np.fabs((bound - old_bound) / old_bound)\n        logger.info('iteration %i iteration lda seq bound is %f convergence is %f', iter_, bound, converged)\n    self.e_log_prob = self.compute_expected_log_prob()\n    return bound",
        "mutated": [
            "def fit_sslm(self, sstats):\n    if False:\n        i = 10\n    'Fits variational distribution.\\n\\n        This is essentially the m-step.\\n        Maximizes the approximation of the true posterior for a particular topic using the provided sufficient\\n        statistics. Updates the values using :meth:`~gensim.models.ldaseqmodel.sslm.update_obs` and\\n        :meth:`~gensim.models.ldaseqmodel.sslm.compute_expected_log_prob`.\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the\\n            current time slice, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        Returns\\n        -------\\n        float\\n            The lower bound for the true posterior achieved using the fitted approximate distribution.\\n\\n        '\n    W = self.vocab_len\n    bound = 0\n    old_bound = 0\n    sslm_fit_threshold = 1e-06\n    sslm_max_iter = 2\n    converged = sslm_fit_threshold + 1\n    (self.variance, self.fwd_variance) = (np.array(x) for x in zip(*(self.compute_post_variance(w, self.chain_variance) for w in range(W))))\n    totals = sstats.sum(axis=0)\n    iter_ = 0\n    model = 'DTM'\n    if model == 'DTM':\n        bound = self.compute_bound(sstats, totals)\n    if model == 'DIM':\n        bound = self.compute_bound_fixed(sstats, totals)\n    logger.info('initial sslm bound is %f', bound)\n    while converged > sslm_fit_threshold and iter_ < sslm_max_iter:\n        iter_ += 1\n        old_bound = bound\n        (self.obs, self.zeta) = self.update_obs(sstats, totals)\n        if model == 'DTM':\n            bound = self.compute_bound(sstats, totals)\n        if model == 'DIM':\n            bound = self.compute_bound_fixed(sstats, totals)\n        converged = np.fabs((bound - old_bound) / old_bound)\n        logger.info('iteration %i iteration lda seq bound is %f convergence is %f', iter_, bound, converged)\n    self.e_log_prob = self.compute_expected_log_prob()\n    return bound",
            "def fit_sslm(self, sstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Fits variational distribution.\\n\\n        This is essentially the m-step.\\n        Maximizes the approximation of the true posterior for a particular topic using the provided sufficient\\n        statistics. Updates the values using :meth:`~gensim.models.ldaseqmodel.sslm.update_obs` and\\n        :meth:`~gensim.models.ldaseqmodel.sslm.compute_expected_log_prob`.\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the\\n            current time slice, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        Returns\\n        -------\\n        float\\n            The lower bound for the true posterior achieved using the fitted approximate distribution.\\n\\n        '\n    W = self.vocab_len\n    bound = 0\n    old_bound = 0\n    sslm_fit_threshold = 1e-06\n    sslm_max_iter = 2\n    converged = sslm_fit_threshold + 1\n    (self.variance, self.fwd_variance) = (np.array(x) for x in zip(*(self.compute_post_variance(w, self.chain_variance) for w in range(W))))\n    totals = sstats.sum(axis=0)\n    iter_ = 0\n    model = 'DTM'\n    if model == 'DTM':\n        bound = self.compute_bound(sstats, totals)\n    if model == 'DIM':\n        bound = self.compute_bound_fixed(sstats, totals)\n    logger.info('initial sslm bound is %f', bound)\n    while converged > sslm_fit_threshold and iter_ < sslm_max_iter:\n        iter_ += 1\n        old_bound = bound\n        (self.obs, self.zeta) = self.update_obs(sstats, totals)\n        if model == 'DTM':\n            bound = self.compute_bound(sstats, totals)\n        if model == 'DIM':\n            bound = self.compute_bound_fixed(sstats, totals)\n        converged = np.fabs((bound - old_bound) / old_bound)\n        logger.info('iteration %i iteration lda seq bound is %f convergence is %f', iter_, bound, converged)\n    self.e_log_prob = self.compute_expected_log_prob()\n    return bound",
            "def fit_sslm(self, sstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Fits variational distribution.\\n\\n        This is essentially the m-step.\\n        Maximizes the approximation of the true posterior for a particular topic using the provided sufficient\\n        statistics. Updates the values using :meth:`~gensim.models.ldaseqmodel.sslm.update_obs` and\\n        :meth:`~gensim.models.ldaseqmodel.sslm.compute_expected_log_prob`.\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the\\n            current time slice, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        Returns\\n        -------\\n        float\\n            The lower bound for the true posterior achieved using the fitted approximate distribution.\\n\\n        '\n    W = self.vocab_len\n    bound = 0\n    old_bound = 0\n    sslm_fit_threshold = 1e-06\n    sslm_max_iter = 2\n    converged = sslm_fit_threshold + 1\n    (self.variance, self.fwd_variance) = (np.array(x) for x in zip(*(self.compute_post_variance(w, self.chain_variance) for w in range(W))))\n    totals = sstats.sum(axis=0)\n    iter_ = 0\n    model = 'DTM'\n    if model == 'DTM':\n        bound = self.compute_bound(sstats, totals)\n    if model == 'DIM':\n        bound = self.compute_bound_fixed(sstats, totals)\n    logger.info('initial sslm bound is %f', bound)\n    while converged > sslm_fit_threshold and iter_ < sslm_max_iter:\n        iter_ += 1\n        old_bound = bound\n        (self.obs, self.zeta) = self.update_obs(sstats, totals)\n        if model == 'DTM':\n            bound = self.compute_bound(sstats, totals)\n        if model == 'DIM':\n            bound = self.compute_bound_fixed(sstats, totals)\n        converged = np.fabs((bound - old_bound) / old_bound)\n        logger.info('iteration %i iteration lda seq bound is %f convergence is %f', iter_, bound, converged)\n    self.e_log_prob = self.compute_expected_log_prob()\n    return bound",
            "def fit_sslm(self, sstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Fits variational distribution.\\n\\n        This is essentially the m-step.\\n        Maximizes the approximation of the true posterior for a particular topic using the provided sufficient\\n        statistics. Updates the values using :meth:`~gensim.models.ldaseqmodel.sslm.update_obs` and\\n        :meth:`~gensim.models.ldaseqmodel.sslm.compute_expected_log_prob`.\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the\\n            current time slice, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        Returns\\n        -------\\n        float\\n            The lower bound for the true posterior achieved using the fitted approximate distribution.\\n\\n        '\n    W = self.vocab_len\n    bound = 0\n    old_bound = 0\n    sslm_fit_threshold = 1e-06\n    sslm_max_iter = 2\n    converged = sslm_fit_threshold + 1\n    (self.variance, self.fwd_variance) = (np.array(x) for x in zip(*(self.compute_post_variance(w, self.chain_variance) for w in range(W))))\n    totals = sstats.sum(axis=0)\n    iter_ = 0\n    model = 'DTM'\n    if model == 'DTM':\n        bound = self.compute_bound(sstats, totals)\n    if model == 'DIM':\n        bound = self.compute_bound_fixed(sstats, totals)\n    logger.info('initial sslm bound is %f', bound)\n    while converged > sslm_fit_threshold and iter_ < sslm_max_iter:\n        iter_ += 1\n        old_bound = bound\n        (self.obs, self.zeta) = self.update_obs(sstats, totals)\n        if model == 'DTM':\n            bound = self.compute_bound(sstats, totals)\n        if model == 'DIM':\n            bound = self.compute_bound_fixed(sstats, totals)\n        converged = np.fabs((bound - old_bound) / old_bound)\n        logger.info('iteration %i iteration lda seq bound is %f convergence is %f', iter_, bound, converged)\n    self.e_log_prob = self.compute_expected_log_prob()\n    return bound",
            "def fit_sslm(self, sstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Fits variational distribution.\\n\\n        This is essentially the m-step.\\n        Maximizes the approximation of the true posterior for a particular topic using the provided sufficient\\n        statistics. Updates the values using :meth:`~gensim.models.ldaseqmodel.sslm.update_obs` and\\n        :meth:`~gensim.models.ldaseqmodel.sslm.compute_expected_log_prob`.\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the\\n            current time slice, expected shape (`self.vocab_len`, `num_topics`).\\n\\n        Returns\\n        -------\\n        float\\n            The lower bound for the true posterior achieved using the fitted approximate distribution.\\n\\n        '\n    W = self.vocab_len\n    bound = 0\n    old_bound = 0\n    sslm_fit_threshold = 1e-06\n    sslm_max_iter = 2\n    converged = sslm_fit_threshold + 1\n    (self.variance, self.fwd_variance) = (np.array(x) for x in zip(*(self.compute_post_variance(w, self.chain_variance) for w in range(W))))\n    totals = sstats.sum(axis=0)\n    iter_ = 0\n    model = 'DTM'\n    if model == 'DTM':\n        bound = self.compute_bound(sstats, totals)\n    if model == 'DIM':\n        bound = self.compute_bound_fixed(sstats, totals)\n    logger.info('initial sslm bound is %f', bound)\n    while converged > sslm_fit_threshold and iter_ < sslm_max_iter:\n        iter_ += 1\n        old_bound = bound\n        (self.obs, self.zeta) = self.update_obs(sstats, totals)\n        if model == 'DTM':\n            bound = self.compute_bound(sstats, totals)\n        if model == 'DIM':\n            bound = self.compute_bound_fixed(sstats, totals)\n        converged = np.fabs((bound - old_bound) / old_bound)\n        logger.info('iteration %i iteration lda seq bound is %f convergence is %f', iter_, bound, converged)\n    self.e_log_prob = self.compute_expected_log_prob()\n    return bound"
        ]
    },
    {
        "func_name": "compute_bound",
        "original": "def compute_bound(self, sstats, totals):\n    \"\"\"Compute the maximized lower bound achieved for the log probability of the true posterior.\n\n        Uses the formula presented in the appendix of the DTM paper (formula no. 5).\n\n        Parameters\n        ----------\n        sstats : numpy.ndarray\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the first\n            time slice, expected shape (`self.vocab_len`, `num_topics`).\n        totals : list of int of length `len(self.time_slice)`\n            The totals for each time slice.\n\n        Returns\n        -------\n        float\n            The maximized lower bound.\n\n        \"\"\"\n    w = self.vocab_len\n    t = self.num_time_slices\n    term_1 = 0\n    term_2 = 0\n    term_3 = 0\n    val = 0\n    ent = 0\n    chain_variance = self.chain_variance\n    (self.mean, self.fwd_mean) = (np.array(x) for x in zip(*(self.compute_post_mean(w, self.chain_variance) for w in range(w))))\n    self.zeta = self.update_zeta()\n    val = sum((self.variance[w][0] - self.variance[w][t] for w in range(w))) / 2 * chain_variance\n    logger.info('Computing bound, all times')\n    for t in range(1, t + 1):\n        term_1 = 0.0\n        term_2 = 0.0\n        ent = 0.0\n        for w in range(w):\n            m = self.mean[w][t]\n            prev_m = self.mean[w][t - 1]\n            v = self.variance[w][t]\n            term_1 += np.power(m - prev_m, 2) / (2 * chain_variance) - v / chain_variance - np.log(chain_variance)\n            term_2 += sstats[w][t - 1] * m\n            ent += np.log(v) / 2\n        term_3 = -totals[t - 1] * np.log(self.zeta[t - 1])\n        val += term_2 + term_3 + ent - term_1\n    return val",
        "mutated": [
            "def compute_bound(self, sstats, totals):\n    if False:\n        i = 10\n    'Compute the maximized lower bound achieved for the log probability of the true posterior.\\n\\n        Uses the formula presented in the appendix of the DTM paper (formula no. 5).\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the first\\n            time slice, expected shape (`self.vocab_len`, `num_topics`).\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n\\n        Returns\\n        -------\\n        float\\n            The maximized lower bound.\\n\\n        '\n    w = self.vocab_len\n    t = self.num_time_slices\n    term_1 = 0\n    term_2 = 0\n    term_3 = 0\n    val = 0\n    ent = 0\n    chain_variance = self.chain_variance\n    (self.mean, self.fwd_mean) = (np.array(x) for x in zip(*(self.compute_post_mean(w, self.chain_variance) for w in range(w))))\n    self.zeta = self.update_zeta()\n    val = sum((self.variance[w][0] - self.variance[w][t] for w in range(w))) / 2 * chain_variance\n    logger.info('Computing bound, all times')\n    for t in range(1, t + 1):\n        term_1 = 0.0\n        term_2 = 0.0\n        ent = 0.0\n        for w in range(w):\n            m = self.mean[w][t]\n            prev_m = self.mean[w][t - 1]\n            v = self.variance[w][t]\n            term_1 += np.power(m - prev_m, 2) / (2 * chain_variance) - v / chain_variance - np.log(chain_variance)\n            term_2 += sstats[w][t - 1] * m\n            ent += np.log(v) / 2\n        term_3 = -totals[t - 1] * np.log(self.zeta[t - 1])\n        val += term_2 + term_3 + ent - term_1\n    return val",
            "def compute_bound(self, sstats, totals):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the maximized lower bound achieved for the log probability of the true posterior.\\n\\n        Uses the formula presented in the appendix of the DTM paper (formula no. 5).\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the first\\n            time slice, expected shape (`self.vocab_len`, `num_topics`).\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n\\n        Returns\\n        -------\\n        float\\n            The maximized lower bound.\\n\\n        '\n    w = self.vocab_len\n    t = self.num_time_slices\n    term_1 = 0\n    term_2 = 0\n    term_3 = 0\n    val = 0\n    ent = 0\n    chain_variance = self.chain_variance\n    (self.mean, self.fwd_mean) = (np.array(x) for x in zip(*(self.compute_post_mean(w, self.chain_variance) for w in range(w))))\n    self.zeta = self.update_zeta()\n    val = sum((self.variance[w][0] - self.variance[w][t] for w in range(w))) / 2 * chain_variance\n    logger.info('Computing bound, all times')\n    for t in range(1, t + 1):\n        term_1 = 0.0\n        term_2 = 0.0\n        ent = 0.0\n        for w in range(w):\n            m = self.mean[w][t]\n            prev_m = self.mean[w][t - 1]\n            v = self.variance[w][t]\n            term_1 += np.power(m - prev_m, 2) / (2 * chain_variance) - v / chain_variance - np.log(chain_variance)\n            term_2 += sstats[w][t - 1] * m\n            ent += np.log(v) / 2\n        term_3 = -totals[t - 1] * np.log(self.zeta[t - 1])\n        val += term_2 + term_3 + ent - term_1\n    return val",
            "def compute_bound(self, sstats, totals):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the maximized lower bound achieved for the log probability of the true posterior.\\n\\n        Uses the formula presented in the appendix of the DTM paper (formula no. 5).\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the first\\n            time slice, expected shape (`self.vocab_len`, `num_topics`).\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n\\n        Returns\\n        -------\\n        float\\n            The maximized lower bound.\\n\\n        '\n    w = self.vocab_len\n    t = self.num_time_slices\n    term_1 = 0\n    term_2 = 0\n    term_3 = 0\n    val = 0\n    ent = 0\n    chain_variance = self.chain_variance\n    (self.mean, self.fwd_mean) = (np.array(x) for x in zip(*(self.compute_post_mean(w, self.chain_variance) for w in range(w))))\n    self.zeta = self.update_zeta()\n    val = sum((self.variance[w][0] - self.variance[w][t] for w in range(w))) / 2 * chain_variance\n    logger.info('Computing bound, all times')\n    for t in range(1, t + 1):\n        term_1 = 0.0\n        term_2 = 0.0\n        ent = 0.0\n        for w in range(w):\n            m = self.mean[w][t]\n            prev_m = self.mean[w][t - 1]\n            v = self.variance[w][t]\n            term_1 += np.power(m - prev_m, 2) / (2 * chain_variance) - v / chain_variance - np.log(chain_variance)\n            term_2 += sstats[w][t - 1] * m\n            ent += np.log(v) / 2\n        term_3 = -totals[t - 1] * np.log(self.zeta[t - 1])\n        val += term_2 + term_3 + ent - term_1\n    return val",
            "def compute_bound(self, sstats, totals):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the maximized lower bound achieved for the log probability of the true posterior.\\n\\n        Uses the formula presented in the appendix of the DTM paper (formula no. 5).\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the first\\n            time slice, expected shape (`self.vocab_len`, `num_topics`).\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n\\n        Returns\\n        -------\\n        float\\n            The maximized lower bound.\\n\\n        '\n    w = self.vocab_len\n    t = self.num_time_slices\n    term_1 = 0\n    term_2 = 0\n    term_3 = 0\n    val = 0\n    ent = 0\n    chain_variance = self.chain_variance\n    (self.mean, self.fwd_mean) = (np.array(x) for x in zip(*(self.compute_post_mean(w, self.chain_variance) for w in range(w))))\n    self.zeta = self.update_zeta()\n    val = sum((self.variance[w][0] - self.variance[w][t] for w in range(w))) / 2 * chain_variance\n    logger.info('Computing bound, all times')\n    for t in range(1, t + 1):\n        term_1 = 0.0\n        term_2 = 0.0\n        ent = 0.0\n        for w in range(w):\n            m = self.mean[w][t]\n            prev_m = self.mean[w][t - 1]\n            v = self.variance[w][t]\n            term_1 += np.power(m - prev_m, 2) / (2 * chain_variance) - v / chain_variance - np.log(chain_variance)\n            term_2 += sstats[w][t - 1] * m\n            ent += np.log(v) / 2\n        term_3 = -totals[t - 1] * np.log(self.zeta[t - 1])\n        val += term_2 + term_3 + ent - term_1\n    return val",
            "def compute_bound(self, sstats, totals):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the maximized lower bound achieved for the log probability of the true posterior.\\n\\n        Uses the formula presented in the appendix of the DTM paper (formula no. 5).\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the first\\n            time slice, expected shape (`self.vocab_len`, `num_topics`).\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n\\n        Returns\\n        -------\\n        float\\n            The maximized lower bound.\\n\\n        '\n    w = self.vocab_len\n    t = self.num_time_slices\n    term_1 = 0\n    term_2 = 0\n    term_3 = 0\n    val = 0\n    ent = 0\n    chain_variance = self.chain_variance\n    (self.mean, self.fwd_mean) = (np.array(x) for x in zip(*(self.compute_post_mean(w, self.chain_variance) for w in range(w))))\n    self.zeta = self.update_zeta()\n    val = sum((self.variance[w][0] - self.variance[w][t] for w in range(w))) / 2 * chain_variance\n    logger.info('Computing bound, all times')\n    for t in range(1, t + 1):\n        term_1 = 0.0\n        term_2 = 0.0\n        ent = 0.0\n        for w in range(w):\n            m = self.mean[w][t]\n            prev_m = self.mean[w][t - 1]\n            v = self.variance[w][t]\n            term_1 += np.power(m - prev_m, 2) / (2 * chain_variance) - v / chain_variance - np.log(chain_variance)\n            term_2 += sstats[w][t - 1] * m\n            ent += np.log(v) / 2\n        term_3 = -totals[t - 1] * np.log(self.zeta[t - 1])\n        val += term_2 + term_3 + ent - term_1\n    return val"
        ]
    },
    {
        "func_name": "update_obs",
        "original": "def update_obs(self, sstats, totals):\n    \"\"\"Optimize the bound with respect to the observed variables.\n\n        TODO:\n        This is by far the slowest function in the whole algorithm.\n        Replacing or improving the performance of this would greatly speed things up.\n\n        Parameters\n        ----------\n        sstats : numpy.ndarray\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the first\n            time slice, expected shape (`self.vocab_len`, `num_topics`).\n        totals : list of int of length `len(self.time_slice)`\n            The totals for each time slice.\n\n        Returns\n        -------\n        (numpy.ndarray of float, numpy.ndarray of float)\n            The updated optimized values for obs and the zeta variational parameter.\n\n        \"\"\"\n    OBS_NORM_CUTOFF = 2\n    STEP_SIZE = 0.01\n    TOL = 0.001\n    W = self.vocab_len\n    T = self.num_time_slices\n    runs = 0\n    mean_deriv_mtx = np.zeros((T, T + 1))\n    norm_cutoff_obs = None\n    for w in range(W):\n        w_counts = sstats[w]\n        counts_norm = 0\n        for i in range(len(w_counts)):\n            counts_norm += w_counts[i] * w_counts[i]\n        counts_norm = np.sqrt(counts_norm)\n        if counts_norm < OBS_NORM_CUTOFF and norm_cutoff_obs is not None:\n            obs = self.obs[w]\n            norm_cutoff_obs = np.copy(obs)\n        else:\n            if counts_norm < OBS_NORM_CUTOFF:\n                w_counts = np.zeros(len(w_counts))\n            for t in range(T):\n                mean_deriv_mtx[t] = self.compute_mean_deriv(w, t, mean_deriv_mtx[t])\n            deriv = np.zeros(T)\n            args = (self, w_counts, totals, mean_deriv_mtx, w, deriv)\n            obs = self.obs[w]\n            model = 'DTM'\n            if model == 'DTM':\n                obs = optimize.fmin_cg(f=f_obs, fprime=df_obs, x0=obs, gtol=TOL, args=args, epsilon=STEP_SIZE, disp=0)\n            if model == 'DIM':\n                pass\n            runs += 1\n            if counts_norm < OBS_NORM_CUTOFF:\n                norm_cutoff_obs = obs\n            self.obs[w] = obs\n    self.zeta = self.update_zeta()\n    return (self.obs, self.zeta)",
        "mutated": [
            "def update_obs(self, sstats, totals):\n    if False:\n        i = 10\n    'Optimize the bound with respect to the observed variables.\\n\\n        TODO:\\n        This is by far the slowest function in the whole algorithm.\\n        Replacing or improving the performance of this would greatly speed things up.\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the first\\n            time slice, expected shape (`self.vocab_len`, `num_topics`).\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray of float, numpy.ndarray of float)\\n            The updated optimized values for obs and the zeta variational parameter.\\n\\n        '\n    OBS_NORM_CUTOFF = 2\n    STEP_SIZE = 0.01\n    TOL = 0.001\n    W = self.vocab_len\n    T = self.num_time_slices\n    runs = 0\n    mean_deriv_mtx = np.zeros((T, T + 1))\n    norm_cutoff_obs = None\n    for w in range(W):\n        w_counts = sstats[w]\n        counts_norm = 0\n        for i in range(len(w_counts)):\n            counts_norm += w_counts[i] * w_counts[i]\n        counts_norm = np.sqrt(counts_norm)\n        if counts_norm < OBS_NORM_CUTOFF and norm_cutoff_obs is not None:\n            obs = self.obs[w]\n            norm_cutoff_obs = np.copy(obs)\n        else:\n            if counts_norm < OBS_NORM_CUTOFF:\n                w_counts = np.zeros(len(w_counts))\n            for t in range(T):\n                mean_deriv_mtx[t] = self.compute_mean_deriv(w, t, mean_deriv_mtx[t])\n            deriv = np.zeros(T)\n            args = (self, w_counts, totals, mean_deriv_mtx, w, deriv)\n            obs = self.obs[w]\n            model = 'DTM'\n            if model == 'DTM':\n                obs = optimize.fmin_cg(f=f_obs, fprime=df_obs, x0=obs, gtol=TOL, args=args, epsilon=STEP_SIZE, disp=0)\n            if model == 'DIM':\n                pass\n            runs += 1\n            if counts_norm < OBS_NORM_CUTOFF:\n                norm_cutoff_obs = obs\n            self.obs[w] = obs\n    self.zeta = self.update_zeta()\n    return (self.obs, self.zeta)",
            "def update_obs(self, sstats, totals):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Optimize the bound with respect to the observed variables.\\n\\n        TODO:\\n        This is by far the slowest function in the whole algorithm.\\n        Replacing or improving the performance of this would greatly speed things up.\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the first\\n            time slice, expected shape (`self.vocab_len`, `num_topics`).\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray of float, numpy.ndarray of float)\\n            The updated optimized values for obs and the zeta variational parameter.\\n\\n        '\n    OBS_NORM_CUTOFF = 2\n    STEP_SIZE = 0.01\n    TOL = 0.001\n    W = self.vocab_len\n    T = self.num_time_slices\n    runs = 0\n    mean_deriv_mtx = np.zeros((T, T + 1))\n    norm_cutoff_obs = None\n    for w in range(W):\n        w_counts = sstats[w]\n        counts_norm = 0\n        for i in range(len(w_counts)):\n            counts_norm += w_counts[i] * w_counts[i]\n        counts_norm = np.sqrt(counts_norm)\n        if counts_norm < OBS_NORM_CUTOFF and norm_cutoff_obs is not None:\n            obs = self.obs[w]\n            norm_cutoff_obs = np.copy(obs)\n        else:\n            if counts_norm < OBS_NORM_CUTOFF:\n                w_counts = np.zeros(len(w_counts))\n            for t in range(T):\n                mean_deriv_mtx[t] = self.compute_mean_deriv(w, t, mean_deriv_mtx[t])\n            deriv = np.zeros(T)\n            args = (self, w_counts, totals, mean_deriv_mtx, w, deriv)\n            obs = self.obs[w]\n            model = 'DTM'\n            if model == 'DTM':\n                obs = optimize.fmin_cg(f=f_obs, fprime=df_obs, x0=obs, gtol=TOL, args=args, epsilon=STEP_SIZE, disp=0)\n            if model == 'DIM':\n                pass\n            runs += 1\n            if counts_norm < OBS_NORM_CUTOFF:\n                norm_cutoff_obs = obs\n            self.obs[w] = obs\n    self.zeta = self.update_zeta()\n    return (self.obs, self.zeta)",
            "def update_obs(self, sstats, totals):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Optimize the bound with respect to the observed variables.\\n\\n        TODO:\\n        This is by far the slowest function in the whole algorithm.\\n        Replacing or improving the performance of this would greatly speed things up.\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the first\\n            time slice, expected shape (`self.vocab_len`, `num_topics`).\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray of float, numpy.ndarray of float)\\n            The updated optimized values for obs and the zeta variational parameter.\\n\\n        '\n    OBS_NORM_CUTOFF = 2\n    STEP_SIZE = 0.01\n    TOL = 0.001\n    W = self.vocab_len\n    T = self.num_time_slices\n    runs = 0\n    mean_deriv_mtx = np.zeros((T, T + 1))\n    norm_cutoff_obs = None\n    for w in range(W):\n        w_counts = sstats[w]\n        counts_norm = 0\n        for i in range(len(w_counts)):\n            counts_norm += w_counts[i] * w_counts[i]\n        counts_norm = np.sqrt(counts_norm)\n        if counts_norm < OBS_NORM_CUTOFF and norm_cutoff_obs is not None:\n            obs = self.obs[w]\n            norm_cutoff_obs = np.copy(obs)\n        else:\n            if counts_norm < OBS_NORM_CUTOFF:\n                w_counts = np.zeros(len(w_counts))\n            for t in range(T):\n                mean_deriv_mtx[t] = self.compute_mean_deriv(w, t, mean_deriv_mtx[t])\n            deriv = np.zeros(T)\n            args = (self, w_counts, totals, mean_deriv_mtx, w, deriv)\n            obs = self.obs[w]\n            model = 'DTM'\n            if model == 'DTM':\n                obs = optimize.fmin_cg(f=f_obs, fprime=df_obs, x0=obs, gtol=TOL, args=args, epsilon=STEP_SIZE, disp=0)\n            if model == 'DIM':\n                pass\n            runs += 1\n            if counts_norm < OBS_NORM_CUTOFF:\n                norm_cutoff_obs = obs\n            self.obs[w] = obs\n    self.zeta = self.update_zeta()\n    return (self.obs, self.zeta)",
            "def update_obs(self, sstats, totals):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Optimize the bound with respect to the observed variables.\\n\\n        TODO:\\n        This is by far the slowest function in the whole algorithm.\\n        Replacing or improving the performance of this would greatly speed things up.\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the first\\n            time slice, expected shape (`self.vocab_len`, `num_topics`).\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray of float, numpy.ndarray of float)\\n            The updated optimized values for obs and the zeta variational parameter.\\n\\n        '\n    OBS_NORM_CUTOFF = 2\n    STEP_SIZE = 0.01\n    TOL = 0.001\n    W = self.vocab_len\n    T = self.num_time_slices\n    runs = 0\n    mean_deriv_mtx = np.zeros((T, T + 1))\n    norm_cutoff_obs = None\n    for w in range(W):\n        w_counts = sstats[w]\n        counts_norm = 0\n        for i in range(len(w_counts)):\n            counts_norm += w_counts[i] * w_counts[i]\n        counts_norm = np.sqrt(counts_norm)\n        if counts_norm < OBS_NORM_CUTOFF and norm_cutoff_obs is not None:\n            obs = self.obs[w]\n            norm_cutoff_obs = np.copy(obs)\n        else:\n            if counts_norm < OBS_NORM_CUTOFF:\n                w_counts = np.zeros(len(w_counts))\n            for t in range(T):\n                mean_deriv_mtx[t] = self.compute_mean_deriv(w, t, mean_deriv_mtx[t])\n            deriv = np.zeros(T)\n            args = (self, w_counts, totals, mean_deriv_mtx, w, deriv)\n            obs = self.obs[w]\n            model = 'DTM'\n            if model == 'DTM':\n                obs = optimize.fmin_cg(f=f_obs, fprime=df_obs, x0=obs, gtol=TOL, args=args, epsilon=STEP_SIZE, disp=0)\n            if model == 'DIM':\n                pass\n            runs += 1\n            if counts_norm < OBS_NORM_CUTOFF:\n                norm_cutoff_obs = obs\n            self.obs[w] = obs\n    self.zeta = self.update_zeta()\n    return (self.obs, self.zeta)",
            "def update_obs(self, sstats, totals):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Optimize the bound with respect to the observed variables.\\n\\n        TODO:\\n        This is by far the slowest function in the whole algorithm.\\n        Replacing or improving the performance of this would greatly speed things up.\\n\\n        Parameters\\n        ----------\\n        sstats : numpy.ndarray\\n            Sufficient statistics for a particular topic. Corresponds to matrix beta in the linked paper for the first\\n            time slice, expected shape (`self.vocab_len`, `num_topics`).\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n\\n        Returns\\n        -------\\n        (numpy.ndarray of float, numpy.ndarray of float)\\n            The updated optimized values for obs and the zeta variational parameter.\\n\\n        '\n    OBS_NORM_CUTOFF = 2\n    STEP_SIZE = 0.01\n    TOL = 0.001\n    W = self.vocab_len\n    T = self.num_time_slices\n    runs = 0\n    mean_deriv_mtx = np.zeros((T, T + 1))\n    norm_cutoff_obs = None\n    for w in range(W):\n        w_counts = sstats[w]\n        counts_norm = 0\n        for i in range(len(w_counts)):\n            counts_norm += w_counts[i] * w_counts[i]\n        counts_norm = np.sqrt(counts_norm)\n        if counts_norm < OBS_NORM_CUTOFF and norm_cutoff_obs is not None:\n            obs = self.obs[w]\n            norm_cutoff_obs = np.copy(obs)\n        else:\n            if counts_norm < OBS_NORM_CUTOFF:\n                w_counts = np.zeros(len(w_counts))\n            for t in range(T):\n                mean_deriv_mtx[t] = self.compute_mean_deriv(w, t, mean_deriv_mtx[t])\n            deriv = np.zeros(T)\n            args = (self, w_counts, totals, mean_deriv_mtx, w, deriv)\n            obs = self.obs[w]\n            model = 'DTM'\n            if model == 'DTM':\n                obs = optimize.fmin_cg(f=f_obs, fprime=df_obs, x0=obs, gtol=TOL, args=args, epsilon=STEP_SIZE, disp=0)\n            if model == 'DIM':\n                pass\n            runs += 1\n            if counts_norm < OBS_NORM_CUTOFF:\n                norm_cutoff_obs = obs\n            self.obs[w] = obs\n    self.zeta = self.update_zeta()\n    return (self.obs, self.zeta)"
        ]
    },
    {
        "func_name": "compute_mean_deriv",
        "original": "def compute_mean_deriv(self, word, time, deriv):\n    \"\"\"Helper functions for optimizing a function.\n\n        Compute the derivative of:\n\n        .. :math::\n\n            E[\\x08eta_{t,w}]/d obs_{s,w} for t = 1:T.\n\n        Parameters\n        ----------\n        word : int\n            The word's ID.\n        time : int\n            The time slice.\n        deriv : list of float\n            Derivative for each time slice.\n\n        Returns\n        -------\n        list of float\n            Mean derivative for each time slice.\n\n        \"\"\"\n    T = self.num_time_slices\n    fwd_variance = self.variance[word]\n    deriv[0] = 0\n    for t in range(1, T + 1):\n        if self.obs_variance > 0.0:\n            w = self.obs_variance / (fwd_variance[t - 1] + self.chain_variance + self.obs_variance)\n        else:\n            w = 0.0\n        val = w * deriv[t - 1]\n        if time == t - 1:\n            val += 1 - w\n        deriv[t] = val\n    for t in range(T - 1, -1, -1):\n        if self.chain_variance == 0.0:\n            w = 0.0\n        else:\n            w = self.chain_variance / (fwd_variance[t] + self.chain_variance)\n        deriv[t] = w * deriv[t] + (1 - w) * deriv[t + 1]\n    return deriv",
        "mutated": [
            "def compute_mean_deriv(self, word, time, deriv):\n    if False:\n        i = 10\n    \"Helper functions for optimizing a function.\\n\\n        Compute the derivative of:\\n\\n        .. :math::\\n\\n            E[\\x08eta_{t,w}]/d obs_{s,w} for t = 1:T.\\n\\n        Parameters\\n        ----------\\n        word : int\\n            The word's ID.\\n        time : int\\n            The time slice.\\n        deriv : list of float\\n            Derivative for each time slice.\\n\\n        Returns\\n        -------\\n        list of float\\n            Mean derivative for each time slice.\\n\\n        \"\n    T = self.num_time_slices\n    fwd_variance = self.variance[word]\n    deriv[0] = 0\n    for t in range(1, T + 1):\n        if self.obs_variance > 0.0:\n            w = self.obs_variance / (fwd_variance[t - 1] + self.chain_variance + self.obs_variance)\n        else:\n            w = 0.0\n        val = w * deriv[t - 1]\n        if time == t - 1:\n            val += 1 - w\n        deriv[t] = val\n    for t in range(T - 1, -1, -1):\n        if self.chain_variance == 0.0:\n            w = 0.0\n        else:\n            w = self.chain_variance / (fwd_variance[t] + self.chain_variance)\n        deriv[t] = w * deriv[t] + (1 - w) * deriv[t + 1]\n    return deriv",
            "def compute_mean_deriv(self, word, time, deriv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Helper functions for optimizing a function.\\n\\n        Compute the derivative of:\\n\\n        .. :math::\\n\\n            E[\\x08eta_{t,w}]/d obs_{s,w} for t = 1:T.\\n\\n        Parameters\\n        ----------\\n        word : int\\n            The word's ID.\\n        time : int\\n            The time slice.\\n        deriv : list of float\\n            Derivative for each time slice.\\n\\n        Returns\\n        -------\\n        list of float\\n            Mean derivative for each time slice.\\n\\n        \"\n    T = self.num_time_slices\n    fwd_variance = self.variance[word]\n    deriv[0] = 0\n    for t in range(1, T + 1):\n        if self.obs_variance > 0.0:\n            w = self.obs_variance / (fwd_variance[t - 1] + self.chain_variance + self.obs_variance)\n        else:\n            w = 0.0\n        val = w * deriv[t - 1]\n        if time == t - 1:\n            val += 1 - w\n        deriv[t] = val\n    for t in range(T - 1, -1, -1):\n        if self.chain_variance == 0.0:\n            w = 0.0\n        else:\n            w = self.chain_variance / (fwd_variance[t] + self.chain_variance)\n        deriv[t] = w * deriv[t] + (1 - w) * deriv[t + 1]\n    return deriv",
            "def compute_mean_deriv(self, word, time, deriv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Helper functions for optimizing a function.\\n\\n        Compute the derivative of:\\n\\n        .. :math::\\n\\n            E[\\x08eta_{t,w}]/d obs_{s,w} for t = 1:T.\\n\\n        Parameters\\n        ----------\\n        word : int\\n            The word's ID.\\n        time : int\\n            The time slice.\\n        deriv : list of float\\n            Derivative for each time slice.\\n\\n        Returns\\n        -------\\n        list of float\\n            Mean derivative for each time slice.\\n\\n        \"\n    T = self.num_time_slices\n    fwd_variance = self.variance[word]\n    deriv[0] = 0\n    for t in range(1, T + 1):\n        if self.obs_variance > 0.0:\n            w = self.obs_variance / (fwd_variance[t - 1] + self.chain_variance + self.obs_variance)\n        else:\n            w = 0.0\n        val = w * deriv[t - 1]\n        if time == t - 1:\n            val += 1 - w\n        deriv[t] = val\n    for t in range(T - 1, -1, -1):\n        if self.chain_variance == 0.0:\n            w = 0.0\n        else:\n            w = self.chain_variance / (fwd_variance[t] + self.chain_variance)\n        deriv[t] = w * deriv[t] + (1 - w) * deriv[t + 1]\n    return deriv",
            "def compute_mean_deriv(self, word, time, deriv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Helper functions for optimizing a function.\\n\\n        Compute the derivative of:\\n\\n        .. :math::\\n\\n            E[\\x08eta_{t,w}]/d obs_{s,w} for t = 1:T.\\n\\n        Parameters\\n        ----------\\n        word : int\\n            The word's ID.\\n        time : int\\n            The time slice.\\n        deriv : list of float\\n            Derivative for each time slice.\\n\\n        Returns\\n        -------\\n        list of float\\n            Mean derivative for each time slice.\\n\\n        \"\n    T = self.num_time_slices\n    fwd_variance = self.variance[word]\n    deriv[0] = 0\n    for t in range(1, T + 1):\n        if self.obs_variance > 0.0:\n            w = self.obs_variance / (fwd_variance[t - 1] + self.chain_variance + self.obs_variance)\n        else:\n            w = 0.0\n        val = w * deriv[t - 1]\n        if time == t - 1:\n            val += 1 - w\n        deriv[t] = val\n    for t in range(T - 1, -1, -1):\n        if self.chain_variance == 0.0:\n            w = 0.0\n        else:\n            w = self.chain_variance / (fwd_variance[t] + self.chain_variance)\n        deriv[t] = w * deriv[t] + (1 - w) * deriv[t + 1]\n    return deriv",
            "def compute_mean_deriv(self, word, time, deriv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Helper functions for optimizing a function.\\n\\n        Compute the derivative of:\\n\\n        .. :math::\\n\\n            E[\\x08eta_{t,w}]/d obs_{s,w} for t = 1:T.\\n\\n        Parameters\\n        ----------\\n        word : int\\n            The word's ID.\\n        time : int\\n            The time slice.\\n        deriv : list of float\\n            Derivative for each time slice.\\n\\n        Returns\\n        -------\\n        list of float\\n            Mean derivative for each time slice.\\n\\n        \"\n    T = self.num_time_slices\n    fwd_variance = self.variance[word]\n    deriv[0] = 0\n    for t in range(1, T + 1):\n        if self.obs_variance > 0.0:\n            w = self.obs_variance / (fwd_variance[t - 1] + self.chain_variance + self.obs_variance)\n        else:\n            w = 0.0\n        val = w * deriv[t - 1]\n        if time == t - 1:\n            val += 1 - w\n        deriv[t] = val\n    for t in range(T - 1, -1, -1):\n        if self.chain_variance == 0.0:\n            w = 0.0\n        else:\n            w = self.chain_variance / (fwd_variance[t] + self.chain_variance)\n        deriv[t] = w * deriv[t] + (1 - w) * deriv[t + 1]\n    return deriv"
        ]
    },
    {
        "func_name": "compute_obs_deriv",
        "original": "def compute_obs_deriv(self, word, word_counts, totals, mean_deriv_mtx, deriv):\n    \"\"\"Derivation of obs which is used in derivative function `df_obs` while optimizing.\n\n        Parameters\n        ----------\n        word : int\n            The word's ID.\n        word_counts : list of int\n            Total word counts for each time slice.\n        totals : list of int of length `len(self.time_slice)`\n            The totals for each time slice.\n        mean_deriv_mtx : list of float\n            Mean derivative for each time slice.\n        deriv : list of float\n            Mean derivative for each time slice.\n\n        Returns\n        -------\n        list of float\n            Mean derivative for each time slice.\n\n        \"\"\"\n    init_mult = 1000\n    T = self.num_time_slices\n    mean = self.mean[word]\n    variance = self.variance[word]\n    self.temp_vect = np.zeros(T)\n    for u in range(T):\n        self.temp_vect[u] = np.exp(mean[u + 1] + variance[u + 1] / 2)\n    for t in range(T):\n        mean_deriv = mean_deriv_mtx[t]\n        term1 = 0\n        term2 = 0\n        term3 = 0\n        term4 = 0\n        for u in range(1, T + 1):\n            mean_u = mean[u]\n            mean_u_prev = mean[u - 1]\n            dmean_u = mean_deriv[u]\n            dmean_u_prev = mean_deriv[u - 1]\n            term1 += (mean_u - mean_u_prev) * (dmean_u - dmean_u_prev)\n            term2 += (word_counts[u - 1] - totals[u - 1] * self.temp_vect[u - 1] / self.zeta[u - 1]) * dmean_u\n            model = 'DTM'\n            if model == 'DIM':\n                pass\n        if self.chain_variance:\n            term1 = -(term1 / self.chain_variance)\n            term1 = term1 - mean[0] * mean_deriv[0] / (init_mult * self.chain_variance)\n        else:\n            term1 = 0.0\n        deriv[t] = term1 + term2 + term3 + term4\n    return deriv",
        "mutated": [
            "def compute_obs_deriv(self, word, word_counts, totals, mean_deriv_mtx, deriv):\n    if False:\n        i = 10\n    \"Derivation of obs which is used in derivative function `df_obs` while optimizing.\\n\\n        Parameters\\n        ----------\\n        word : int\\n            The word's ID.\\n        word_counts : list of int\\n            Total word counts for each time slice.\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n        mean_deriv_mtx : list of float\\n            Mean derivative for each time slice.\\n        deriv : list of float\\n            Mean derivative for each time slice.\\n\\n        Returns\\n        -------\\n        list of float\\n            Mean derivative for each time slice.\\n\\n        \"\n    init_mult = 1000\n    T = self.num_time_slices\n    mean = self.mean[word]\n    variance = self.variance[word]\n    self.temp_vect = np.zeros(T)\n    for u in range(T):\n        self.temp_vect[u] = np.exp(mean[u + 1] + variance[u + 1] / 2)\n    for t in range(T):\n        mean_deriv = mean_deriv_mtx[t]\n        term1 = 0\n        term2 = 0\n        term3 = 0\n        term4 = 0\n        for u in range(1, T + 1):\n            mean_u = mean[u]\n            mean_u_prev = mean[u - 1]\n            dmean_u = mean_deriv[u]\n            dmean_u_prev = mean_deriv[u - 1]\n            term1 += (mean_u - mean_u_prev) * (dmean_u - dmean_u_prev)\n            term2 += (word_counts[u - 1] - totals[u - 1] * self.temp_vect[u - 1] / self.zeta[u - 1]) * dmean_u\n            model = 'DTM'\n            if model == 'DIM':\n                pass\n        if self.chain_variance:\n            term1 = -(term1 / self.chain_variance)\n            term1 = term1 - mean[0] * mean_deriv[0] / (init_mult * self.chain_variance)\n        else:\n            term1 = 0.0\n        deriv[t] = term1 + term2 + term3 + term4\n    return deriv",
            "def compute_obs_deriv(self, word, word_counts, totals, mean_deriv_mtx, deriv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Derivation of obs which is used in derivative function `df_obs` while optimizing.\\n\\n        Parameters\\n        ----------\\n        word : int\\n            The word's ID.\\n        word_counts : list of int\\n            Total word counts for each time slice.\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n        mean_deriv_mtx : list of float\\n            Mean derivative for each time slice.\\n        deriv : list of float\\n            Mean derivative for each time slice.\\n\\n        Returns\\n        -------\\n        list of float\\n            Mean derivative for each time slice.\\n\\n        \"\n    init_mult = 1000\n    T = self.num_time_slices\n    mean = self.mean[word]\n    variance = self.variance[word]\n    self.temp_vect = np.zeros(T)\n    for u in range(T):\n        self.temp_vect[u] = np.exp(mean[u + 1] + variance[u + 1] / 2)\n    for t in range(T):\n        mean_deriv = mean_deriv_mtx[t]\n        term1 = 0\n        term2 = 0\n        term3 = 0\n        term4 = 0\n        for u in range(1, T + 1):\n            mean_u = mean[u]\n            mean_u_prev = mean[u - 1]\n            dmean_u = mean_deriv[u]\n            dmean_u_prev = mean_deriv[u - 1]\n            term1 += (mean_u - mean_u_prev) * (dmean_u - dmean_u_prev)\n            term2 += (word_counts[u - 1] - totals[u - 1] * self.temp_vect[u - 1] / self.zeta[u - 1]) * dmean_u\n            model = 'DTM'\n            if model == 'DIM':\n                pass\n        if self.chain_variance:\n            term1 = -(term1 / self.chain_variance)\n            term1 = term1 - mean[0] * mean_deriv[0] / (init_mult * self.chain_variance)\n        else:\n            term1 = 0.0\n        deriv[t] = term1 + term2 + term3 + term4\n    return deriv",
            "def compute_obs_deriv(self, word, word_counts, totals, mean_deriv_mtx, deriv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Derivation of obs which is used in derivative function `df_obs` while optimizing.\\n\\n        Parameters\\n        ----------\\n        word : int\\n            The word's ID.\\n        word_counts : list of int\\n            Total word counts for each time slice.\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n        mean_deriv_mtx : list of float\\n            Mean derivative for each time slice.\\n        deriv : list of float\\n            Mean derivative for each time slice.\\n\\n        Returns\\n        -------\\n        list of float\\n            Mean derivative for each time slice.\\n\\n        \"\n    init_mult = 1000\n    T = self.num_time_slices\n    mean = self.mean[word]\n    variance = self.variance[word]\n    self.temp_vect = np.zeros(T)\n    for u in range(T):\n        self.temp_vect[u] = np.exp(mean[u + 1] + variance[u + 1] / 2)\n    for t in range(T):\n        mean_deriv = mean_deriv_mtx[t]\n        term1 = 0\n        term2 = 0\n        term3 = 0\n        term4 = 0\n        for u in range(1, T + 1):\n            mean_u = mean[u]\n            mean_u_prev = mean[u - 1]\n            dmean_u = mean_deriv[u]\n            dmean_u_prev = mean_deriv[u - 1]\n            term1 += (mean_u - mean_u_prev) * (dmean_u - dmean_u_prev)\n            term2 += (word_counts[u - 1] - totals[u - 1] * self.temp_vect[u - 1] / self.zeta[u - 1]) * dmean_u\n            model = 'DTM'\n            if model == 'DIM':\n                pass\n        if self.chain_variance:\n            term1 = -(term1 / self.chain_variance)\n            term1 = term1 - mean[0] * mean_deriv[0] / (init_mult * self.chain_variance)\n        else:\n            term1 = 0.0\n        deriv[t] = term1 + term2 + term3 + term4\n    return deriv",
            "def compute_obs_deriv(self, word, word_counts, totals, mean_deriv_mtx, deriv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Derivation of obs which is used in derivative function `df_obs` while optimizing.\\n\\n        Parameters\\n        ----------\\n        word : int\\n            The word's ID.\\n        word_counts : list of int\\n            Total word counts for each time slice.\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n        mean_deriv_mtx : list of float\\n            Mean derivative for each time slice.\\n        deriv : list of float\\n            Mean derivative for each time slice.\\n\\n        Returns\\n        -------\\n        list of float\\n            Mean derivative for each time slice.\\n\\n        \"\n    init_mult = 1000\n    T = self.num_time_slices\n    mean = self.mean[word]\n    variance = self.variance[word]\n    self.temp_vect = np.zeros(T)\n    for u in range(T):\n        self.temp_vect[u] = np.exp(mean[u + 1] + variance[u + 1] / 2)\n    for t in range(T):\n        mean_deriv = mean_deriv_mtx[t]\n        term1 = 0\n        term2 = 0\n        term3 = 0\n        term4 = 0\n        for u in range(1, T + 1):\n            mean_u = mean[u]\n            mean_u_prev = mean[u - 1]\n            dmean_u = mean_deriv[u]\n            dmean_u_prev = mean_deriv[u - 1]\n            term1 += (mean_u - mean_u_prev) * (dmean_u - dmean_u_prev)\n            term2 += (word_counts[u - 1] - totals[u - 1] * self.temp_vect[u - 1] / self.zeta[u - 1]) * dmean_u\n            model = 'DTM'\n            if model == 'DIM':\n                pass\n        if self.chain_variance:\n            term1 = -(term1 / self.chain_variance)\n            term1 = term1 - mean[0] * mean_deriv[0] / (init_mult * self.chain_variance)\n        else:\n            term1 = 0.0\n        deriv[t] = term1 + term2 + term3 + term4\n    return deriv",
            "def compute_obs_deriv(self, word, word_counts, totals, mean_deriv_mtx, deriv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Derivation of obs which is used in derivative function `df_obs` while optimizing.\\n\\n        Parameters\\n        ----------\\n        word : int\\n            The word's ID.\\n        word_counts : list of int\\n            Total word counts for each time slice.\\n        totals : list of int of length `len(self.time_slice)`\\n            The totals for each time slice.\\n        mean_deriv_mtx : list of float\\n            Mean derivative for each time slice.\\n        deriv : list of float\\n            Mean derivative for each time slice.\\n\\n        Returns\\n        -------\\n        list of float\\n            Mean derivative for each time slice.\\n\\n        \"\n    init_mult = 1000\n    T = self.num_time_slices\n    mean = self.mean[word]\n    variance = self.variance[word]\n    self.temp_vect = np.zeros(T)\n    for u in range(T):\n        self.temp_vect[u] = np.exp(mean[u + 1] + variance[u + 1] / 2)\n    for t in range(T):\n        mean_deriv = mean_deriv_mtx[t]\n        term1 = 0\n        term2 = 0\n        term3 = 0\n        term4 = 0\n        for u in range(1, T + 1):\n            mean_u = mean[u]\n            mean_u_prev = mean[u - 1]\n            dmean_u = mean_deriv[u]\n            dmean_u_prev = mean_deriv[u - 1]\n            term1 += (mean_u - mean_u_prev) * (dmean_u - dmean_u_prev)\n            term2 += (word_counts[u - 1] - totals[u - 1] * self.temp_vect[u - 1] / self.zeta[u - 1]) * dmean_u\n            model = 'DTM'\n            if model == 'DIM':\n                pass\n        if self.chain_variance:\n            term1 = -(term1 / self.chain_variance)\n            term1 = term1 - mean[0] * mean_deriv[0] / (init_mult * self.chain_variance)\n        else:\n            term1 = 0.0\n        deriv[t] = term1 + term2 + term3 + term4\n    return deriv"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, doc=None, lda=None, max_doc_len=None, num_topics=None, gamma=None, lhood=None):\n    \"\"\"Initialize the posterior value structure for the given LDA model.\n\n        Parameters\n        ----------\n        doc : list of (int, int)\n            A BOW representation of the document. Each element in the list is a pair of a word's ID and its number\n            of occurences in the document.\n        lda : :class:`~gensim.models.ldamodel.LdaModel`, optional\n            The underlying LDA model.\n        max_doc_len : int, optional\n            The maximum number of words in a document.\n        num_topics : int, optional\n            Number of topics discovered by the LDA model.\n        gamma : numpy.ndarray, optional\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\n        lhood : float, optional\n            The log likelihood lower bound.\n\n        \"\"\"\n    self.doc = doc\n    self.lda = lda\n    self.gamma = gamma\n    self.lhood = lhood\n    if self.gamma is None:\n        self.gamma = np.zeros(num_topics)\n    if self.lhood is None:\n        self.lhood = np.zeros(num_topics + 1)\n    if max_doc_len is not None and num_topics is not None:\n        self.phi = np.zeros((max_doc_len, num_topics))\n        self.log_phi = np.zeros((max_doc_len, num_topics))\n    self.doc_weight = None\n    self.renormalized_doc_weight = None",
        "mutated": [
            "def __init__(self, doc=None, lda=None, max_doc_len=None, num_topics=None, gamma=None, lhood=None):\n    if False:\n        i = 10\n    \"Initialize the posterior value structure for the given LDA model.\\n\\n        Parameters\\n        ----------\\n        doc : list of (int, int)\\n            A BOW representation of the document. Each element in the list is a pair of a word's ID and its number\\n            of occurences in the document.\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`, optional\\n            The underlying LDA model.\\n        max_doc_len : int, optional\\n            The maximum number of words in a document.\\n        num_topics : int, optional\\n            Number of topics discovered by the LDA model.\\n        gamma : numpy.ndarray, optional\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhood : float, optional\\n            The log likelihood lower bound.\\n\\n        \"\n    self.doc = doc\n    self.lda = lda\n    self.gamma = gamma\n    self.lhood = lhood\n    if self.gamma is None:\n        self.gamma = np.zeros(num_topics)\n    if self.lhood is None:\n        self.lhood = np.zeros(num_topics + 1)\n    if max_doc_len is not None and num_topics is not None:\n        self.phi = np.zeros((max_doc_len, num_topics))\n        self.log_phi = np.zeros((max_doc_len, num_topics))\n    self.doc_weight = None\n    self.renormalized_doc_weight = None",
            "def __init__(self, doc=None, lda=None, max_doc_len=None, num_topics=None, gamma=None, lhood=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Initialize the posterior value structure for the given LDA model.\\n\\n        Parameters\\n        ----------\\n        doc : list of (int, int)\\n            A BOW representation of the document. Each element in the list is a pair of a word's ID and its number\\n            of occurences in the document.\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`, optional\\n            The underlying LDA model.\\n        max_doc_len : int, optional\\n            The maximum number of words in a document.\\n        num_topics : int, optional\\n            Number of topics discovered by the LDA model.\\n        gamma : numpy.ndarray, optional\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhood : float, optional\\n            The log likelihood lower bound.\\n\\n        \"\n    self.doc = doc\n    self.lda = lda\n    self.gamma = gamma\n    self.lhood = lhood\n    if self.gamma is None:\n        self.gamma = np.zeros(num_topics)\n    if self.lhood is None:\n        self.lhood = np.zeros(num_topics + 1)\n    if max_doc_len is not None and num_topics is not None:\n        self.phi = np.zeros((max_doc_len, num_topics))\n        self.log_phi = np.zeros((max_doc_len, num_topics))\n    self.doc_weight = None\n    self.renormalized_doc_weight = None",
            "def __init__(self, doc=None, lda=None, max_doc_len=None, num_topics=None, gamma=None, lhood=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Initialize the posterior value structure for the given LDA model.\\n\\n        Parameters\\n        ----------\\n        doc : list of (int, int)\\n            A BOW representation of the document. Each element in the list is a pair of a word's ID and its number\\n            of occurences in the document.\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`, optional\\n            The underlying LDA model.\\n        max_doc_len : int, optional\\n            The maximum number of words in a document.\\n        num_topics : int, optional\\n            Number of topics discovered by the LDA model.\\n        gamma : numpy.ndarray, optional\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhood : float, optional\\n            The log likelihood lower bound.\\n\\n        \"\n    self.doc = doc\n    self.lda = lda\n    self.gamma = gamma\n    self.lhood = lhood\n    if self.gamma is None:\n        self.gamma = np.zeros(num_topics)\n    if self.lhood is None:\n        self.lhood = np.zeros(num_topics + 1)\n    if max_doc_len is not None and num_topics is not None:\n        self.phi = np.zeros((max_doc_len, num_topics))\n        self.log_phi = np.zeros((max_doc_len, num_topics))\n    self.doc_weight = None\n    self.renormalized_doc_weight = None",
            "def __init__(self, doc=None, lda=None, max_doc_len=None, num_topics=None, gamma=None, lhood=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Initialize the posterior value structure for the given LDA model.\\n\\n        Parameters\\n        ----------\\n        doc : list of (int, int)\\n            A BOW representation of the document. Each element in the list is a pair of a word's ID and its number\\n            of occurences in the document.\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`, optional\\n            The underlying LDA model.\\n        max_doc_len : int, optional\\n            The maximum number of words in a document.\\n        num_topics : int, optional\\n            Number of topics discovered by the LDA model.\\n        gamma : numpy.ndarray, optional\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhood : float, optional\\n            The log likelihood lower bound.\\n\\n        \"\n    self.doc = doc\n    self.lda = lda\n    self.gamma = gamma\n    self.lhood = lhood\n    if self.gamma is None:\n        self.gamma = np.zeros(num_topics)\n    if self.lhood is None:\n        self.lhood = np.zeros(num_topics + 1)\n    if max_doc_len is not None and num_topics is not None:\n        self.phi = np.zeros((max_doc_len, num_topics))\n        self.log_phi = np.zeros((max_doc_len, num_topics))\n    self.doc_weight = None\n    self.renormalized_doc_weight = None",
            "def __init__(self, doc=None, lda=None, max_doc_len=None, num_topics=None, gamma=None, lhood=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Initialize the posterior value structure for the given LDA model.\\n\\n        Parameters\\n        ----------\\n        doc : list of (int, int)\\n            A BOW representation of the document. Each element in the list is a pair of a word's ID and its number\\n            of occurences in the document.\\n        lda : :class:`~gensim.models.ldamodel.LdaModel`, optional\\n            The underlying LDA model.\\n        max_doc_len : int, optional\\n            The maximum number of words in a document.\\n        num_topics : int, optional\\n            Number of topics discovered by the LDA model.\\n        gamma : numpy.ndarray, optional\\n            Topic weight variational parameters for each document. If not supplied, it will be inferred from the model.\\n        lhood : float, optional\\n            The log likelihood lower bound.\\n\\n        \"\n    self.doc = doc\n    self.lda = lda\n    self.gamma = gamma\n    self.lhood = lhood\n    if self.gamma is None:\n        self.gamma = np.zeros(num_topics)\n    if self.lhood is None:\n        self.lhood = np.zeros(num_topics + 1)\n    if max_doc_len is not None and num_topics is not None:\n        self.phi = np.zeros((max_doc_len, num_topics))\n        self.log_phi = np.zeros((max_doc_len, num_topics))\n    self.doc_weight = None\n    self.renormalized_doc_weight = None"
        ]
    },
    {
        "func_name": "update_phi",
        "original": "def update_phi(self, doc_number, time):\n    \"\"\"Update variational multinomial parameters, based on a document and a time-slice.\n\n        This is done based on the original Blei-LDA paper, where:\n        log_phi := beta * exp(\u03a8(gamma)), over every topic for every word.\n\n        TODO: incorporate lee-sueng trick used in\n        **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.\n\n        Parameters\n        ----------\n        doc_number : int\n            Document number. Unused.\n        time : int\n            Time slice. Unused.\n\n        Returns\n        -------\n        (list of float, list of float)\n            Multinomial parameters, and their logarithm, for each word in the document.\n\n        \"\"\"\n    num_topics = self.lda.num_topics\n    dig = np.zeros(num_topics)\n    for k in range(num_topics):\n        dig[k] = digamma(self.gamma[k])\n    n = 0\n    for (word_id, count) in self.doc:\n        for k in range(num_topics):\n            self.log_phi[n][k] = dig[k] + self.lda.topics[word_id][k]\n        log_phi_row = self.log_phi[n]\n        phi_row = self.phi[n]\n        v = log_phi_row[0]\n        for i in range(1, len(log_phi_row)):\n            v = np.logaddexp(v, log_phi_row[i])\n        log_phi_row = log_phi_row - v\n        phi_row = np.exp(log_phi_row)\n        self.log_phi[n] = log_phi_row\n        self.phi[n] = phi_row\n        n += 1\n    return (self.phi, self.log_phi)",
        "mutated": [
            "def update_phi(self, doc_number, time):\n    if False:\n        i = 10\n    'Update variational multinomial parameters, based on a document and a time-slice.\\n\\n        This is done based on the original Blei-LDA paper, where:\\n        log_phi := beta * exp(\u03a8(gamma)), over every topic for every word.\\n\\n        TODO: incorporate lee-sueng trick used in\\n        **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            Document number. Unused.\\n        time : int\\n            Time slice. Unused.\\n\\n        Returns\\n        -------\\n        (list of float, list of float)\\n            Multinomial parameters, and their logarithm, for each word in the document.\\n\\n        '\n    num_topics = self.lda.num_topics\n    dig = np.zeros(num_topics)\n    for k in range(num_topics):\n        dig[k] = digamma(self.gamma[k])\n    n = 0\n    for (word_id, count) in self.doc:\n        for k in range(num_topics):\n            self.log_phi[n][k] = dig[k] + self.lda.topics[word_id][k]\n        log_phi_row = self.log_phi[n]\n        phi_row = self.phi[n]\n        v = log_phi_row[0]\n        for i in range(1, len(log_phi_row)):\n            v = np.logaddexp(v, log_phi_row[i])\n        log_phi_row = log_phi_row - v\n        phi_row = np.exp(log_phi_row)\n        self.log_phi[n] = log_phi_row\n        self.phi[n] = phi_row\n        n += 1\n    return (self.phi, self.log_phi)",
            "def update_phi(self, doc_number, time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Update variational multinomial parameters, based on a document and a time-slice.\\n\\n        This is done based on the original Blei-LDA paper, where:\\n        log_phi := beta * exp(\u03a8(gamma)), over every topic for every word.\\n\\n        TODO: incorporate lee-sueng trick used in\\n        **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            Document number. Unused.\\n        time : int\\n            Time slice. Unused.\\n\\n        Returns\\n        -------\\n        (list of float, list of float)\\n            Multinomial parameters, and their logarithm, for each word in the document.\\n\\n        '\n    num_topics = self.lda.num_topics\n    dig = np.zeros(num_topics)\n    for k in range(num_topics):\n        dig[k] = digamma(self.gamma[k])\n    n = 0\n    for (word_id, count) in self.doc:\n        for k in range(num_topics):\n            self.log_phi[n][k] = dig[k] + self.lda.topics[word_id][k]\n        log_phi_row = self.log_phi[n]\n        phi_row = self.phi[n]\n        v = log_phi_row[0]\n        for i in range(1, len(log_phi_row)):\n            v = np.logaddexp(v, log_phi_row[i])\n        log_phi_row = log_phi_row - v\n        phi_row = np.exp(log_phi_row)\n        self.log_phi[n] = log_phi_row\n        self.phi[n] = phi_row\n        n += 1\n    return (self.phi, self.log_phi)",
            "def update_phi(self, doc_number, time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Update variational multinomial parameters, based on a document and a time-slice.\\n\\n        This is done based on the original Blei-LDA paper, where:\\n        log_phi := beta * exp(\u03a8(gamma)), over every topic for every word.\\n\\n        TODO: incorporate lee-sueng trick used in\\n        **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            Document number. Unused.\\n        time : int\\n            Time slice. Unused.\\n\\n        Returns\\n        -------\\n        (list of float, list of float)\\n            Multinomial parameters, and their logarithm, for each word in the document.\\n\\n        '\n    num_topics = self.lda.num_topics\n    dig = np.zeros(num_topics)\n    for k in range(num_topics):\n        dig[k] = digamma(self.gamma[k])\n    n = 0\n    for (word_id, count) in self.doc:\n        for k in range(num_topics):\n            self.log_phi[n][k] = dig[k] + self.lda.topics[word_id][k]\n        log_phi_row = self.log_phi[n]\n        phi_row = self.phi[n]\n        v = log_phi_row[0]\n        for i in range(1, len(log_phi_row)):\n            v = np.logaddexp(v, log_phi_row[i])\n        log_phi_row = log_phi_row - v\n        phi_row = np.exp(log_phi_row)\n        self.log_phi[n] = log_phi_row\n        self.phi[n] = phi_row\n        n += 1\n    return (self.phi, self.log_phi)",
            "def update_phi(self, doc_number, time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Update variational multinomial parameters, based on a document and a time-slice.\\n\\n        This is done based on the original Blei-LDA paper, where:\\n        log_phi := beta * exp(\u03a8(gamma)), over every topic for every word.\\n\\n        TODO: incorporate lee-sueng trick used in\\n        **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            Document number. Unused.\\n        time : int\\n            Time slice. Unused.\\n\\n        Returns\\n        -------\\n        (list of float, list of float)\\n            Multinomial parameters, and their logarithm, for each word in the document.\\n\\n        '\n    num_topics = self.lda.num_topics\n    dig = np.zeros(num_topics)\n    for k in range(num_topics):\n        dig[k] = digamma(self.gamma[k])\n    n = 0\n    for (word_id, count) in self.doc:\n        for k in range(num_topics):\n            self.log_phi[n][k] = dig[k] + self.lda.topics[word_id][k]\n        log_phi_row = self.log_phi[n]\n        phi_row = self.phi[n]\n        v = log_phi_row[0]\n        for i in range(1, len(log_phi_row)):\n            v = np.logaddexp(v, log_phi_row[i])\n        log_phi_row = log_phi_row - v\n        phi_row = np.exp(log_phi_row)\n        self.log_phi[n] = log_phi_row\n        self.phi[n] = phi_row\n        n += 1\n    return (self.phi, self.log_phi)",
            "def update_phi(self, doc_number, time):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Update variational multinomial parameters, based on a document and a time-slice.\\n\\n        This is done based on the original Blei-LDA paper, where:\\n        log_phi := beta * exp(\u03a8(gamma)), over every topic for every word.\\n\\n        TODO: incorporate lee-sueng trick used in\\n        **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            Document number. Unused.\\n        time : int\\n            Time slice. Unused.\\n\\n        Returns\\n        -------\\n        (list of float, list of float)\\n            Multinomial parameters, and their logarithm, for each word in the document.\\n\\n        '\n    num_topics = self.lda.num_topics\n    dig = np.zeros(num_topics)\n    for k in range(num_topics):\n        dig[k] = digamma(self.gamma[k])\n    n = 0\n    for (word_id, count) in self.doc:\n        for k in range(num_topics):\n            self.log_phi[n][k] = dig[k] + self.lda.topics[word_id][k]\n        log_phi_row = self.log_phi[n]\n        phi_row = self.phi[n]\n        v = log_phi_row[0]\n        for i in range(1, len(log_phi_row)):\n            v = np.logaddexp(v, log_phi_row[i])\n        log_phi_row = log_phi_row - v\n        phi_row = np.exp(log_phi_row)\n        self.log_phi[n] = log_phi_row\n        self.phi[n] = phi_row\n        n += 1\n    return (self.phi, self.log_phi)"
        ]
    },
    {
        "func_name": "update_gamma",
        "original": "def update_gamma(self):\n    \"\"\"Update variational dirichlet parameters.\n\n        This operations is described in the original Blei LDA paper:\n        gamma = alpha + sum(phi), over every topic for every word.\n\n        Returns\n        -------\n        list of float\n            The updated gamma parameters for each word in the document.\n\n        \"\"\"\n    self.gamma = np.copy(self.lda.alpha)\n    n = 0\n    for (word_id, count) in self.doc:\n        phi_row = self.phi[n]\n        for k in range(self.lda.num_topics):\n            self.gamma[k] += phi_row[k] * count\n        n += 1\n    return self.gamma",
        "mutated": [
            "def update_gamma(self):\n    if False:\n        i = 10\n    'Update variational dirichlet parameters.\\n\\n        This operations is described in the original Blei LDA paper:\\n        gamma = alpha + sum(phi), over every topic for every word.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated gamma parameters for each word in the document.\\n\\n        '\n    self.gamma = np.copy(self.lda.alpha)\n    n = 0\n    for (word_id, count) in self.doc:\n        phi_row = self.phi[n]\n        for k in range(self.lda.num_topics):\n            self.gamma[k] += phi_row[k] * count\n        n += 1\n    return self.gamma",
            "def update_gamma(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Update variational dirichlet parameters.\\n\\n        This operations is described in the original Blei LDA paper:\\n        gamma = alpha + sum(phi), over every topic for every word.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated gamma parameters for each word in the document.\\n\\n        '\n    self.gamma = np.copy(self.lda.alpha)\n    n = 0\n    for (word_id, count) in self.doc:\n        phi_row = self.phi[n]\n        for k in range(self.lda.num_topics):\n            self.gamma[k] += phi_row[k] * count\n        n += 1\n    return self.gamma",
            "def update_gamma(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Update variational dirichlet parameters.\\n\\n        This operations is described in the original Blei LDA paper:\\n        gamma = alpha + sum(phi), over every topic for every word.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated gamma parameters for each word in the document.\\n\\n        '\n    self.gamma = np.copy(self.lda.alpha)\n    n = 0\n    for (word_id, count) in self.doc:\n        phi_row = self.phi[n]\n        for k in range(self.lda.num_topics):\n            self.gamma[k] += phi_row[k] * count\n        n += 1\n    return self.gamma",
            "def update_gamma(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Update variational dirichlet parameters.\\n\\n        This operations is described in the original Blei LDA paper:\\n        gamma = alpha + sum(phi), over every topic for every word.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated gamma parameters for each word in the document.\\n\\n        '\n    self.gamma = np.copy(self.lda.alpha)\n    n = 0\n    for (word_id, count) in self.doc:\n        phi_row = self.phi[n]\n        for k in range(self.lda.num_topics):\n            self.gamma[k] += phi_row[k] * count\n        n += 1\n    return self.gamma",
            "def update_gamma(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Update variational dirichlet parameters.\\n\\n        This operations is described in the original Blei LDA paper:\\n        gamma = alpha + sum(phi), over every topic for every word.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated gamma parameters for each word in the document.\\n\\n        '\n    self.gamma = np.copy(self.lda.alpha)\n    n = 0\n    for (word_id, count) in self.doc:\n        phi_row = self.phi[n]\n        for k in range(self.lda.num_topics):\n            self.gamma[k] += phi_row[k] * count\n        n += 1\n    return self.gamma"
        ]
    },
    {
        "func_name": "init_lda_post",
        "original": "def init_lda_post(self):\n    \"\"\"Initialize variational posterior. \"\"\"\n    total = sum((count for (word_id, count) in self.doc))\n    self.gamma.fill(self.lda.alpha[0] + float(total) / self.lda.num_topics)\n    self.phi[:len(self.doc), :] = 1.0 / self.lda.num_topics",
        "mutated": [
            "def init_lda_post(self):\n    if False:\n        i = 10\n    'Initialize variational posterior. '\n    total = sum((count for (word_id, count) in self.doc))\n    self.gamma.fill(self.lda.alpha[0] + float(total) / self.lda.num_topics)\n    self.phi[:len(self.doc), :] = 1.0 / self.lda.num_topics",
            "def init_lda_post(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize variational posterior. '\n    total = sum((count for (word_id, count) in self.doc))\n    self.gamma.fill(self.lda.alpha[0] + float(total) / self.lda.num_topics)\n    self.phi[:len(self.doc), :] = 1.0 / self.lda.num_topics",
            "def init_lda_post(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize variational posterior. '\n    total = sum((count for (word_id, count) in self.doc))\n    self.gamma.fill(self.lda.alpha[0] + float(total) / self.lda.num_topics)\n    self.phi[:len(self.doc), :] = 1.0 / self.lda.num_topics",
            "def init_lda_post(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize variational posterior. '\n    total = sum((count for (word_id, count) in self.doc))\n    self.gamma.fill(self.lda.alpha[0] + float(total) / self.lda.num_topics)\n    self.phi[:len(self.doc), :] = 1.0 / self.lda.num_topics",
            "def init_lda_post(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize variational posterior. '\n    total = sum((count for (word_id, count) in self.doc))\n    self.gamma.fill(self.lda.alpha[0] + float(total) / self.lda.num_topics)\n    self.phi[:len(self.doc), :] = 1.0 / self.lda.num_topics"
        ]
    },
    {
        "func_name": "compute_lda_lhood",
        "original": "def compute_lda_lhood(self):\n    \"\"\"Compute the log likelihood bound.\n\n        Returns\n        -------\n        float\n            The optimal lower bound for the true posterior using the approximate distribution.\n\n        \"\"\"\n    num_topics = self.lda.num_topics\n    gamma_sum = np.sum(self.gamma)\n    lhood = gammaln(np.sum(self.lda.alpha)) - gammaln(gamma_sum)\n    self.lhood[num_topics] = lhood\n    digsum = digamma(gamma_sum)\n    model = 'DTM'\n    for k in range(num_topics):\n        e_log_theta_k = digamma(self.gamma[k]) - digsum\n        lhood_term = (self.lda.alpha[k] - self.gamma[k]) * e_log_theta_k + gammaln(self.gamma[k]) - gammaln(self.lda.alpha[k])\n        n = 0\n        for (word_id, count) in self.doc:\n            if self.phi[n][k] > 0:\n                lhood_term += count * self.phi[n][k] * (e_log_theta_k + self.lda.topics[word_id][k] - self.log_phi[n][k])\n            n += 1\n        self.lhood[k] = lhood_term\n        lhood += lhood_term\n    return lhood",
        "mutated": [
            "def compute_lda_lhood(self):\n    if False:\n        i = 10\n    'Compute the log likelihood bound.\\n\\n        Returns\\n        -------\\n        float\\n            The optimal lower bound for the true posterior using the approximate distribution.\\n\\n        '\n    num_topics = self.lda.num_topics\n    gamma_sum = np.sum(self.gamma)\n    lhood = gammaln(np.sum(self.lda.alpha)) - gammaln(gamma_sum)\n    self.lhood[num_topics] = lhood\n    digsum = digamma(gamma_sum)\n    model = 'DTM'\n    for k in range(num_topics):\n        e_log_theta_k = digamma(self.gamma[k]) - digsum\n        lhood_term = (self.lda.alpha[k] - self.gamma[k]) * e_log_theta_k + gammaln(self.gamma[k]) - gammaln(self.lda.alpha[k])\n        n = 0\n        for (word_id, count) in self.doc:\n            if self.phi[n][k] > 0:\n                lhood_term += count * self.phi[n][k] * (e_log_theta_k + self.lda.topics[word_id][k] - self.log_phi[n][k])\n            n += 1\n        self.lhood[k] = lhood_term\n        lhood += lhood_term\n    return lhood",
            "def compute_lda_lhood(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the log likelihood bound.\\n\\n        Returns\\n        -------\\n        float\\n            The optimal lower bound for the true posterior using the approximate distribution.\\n\\n        '\n    num_topics = self.lda.num_topics\n    gamma_sum = np.sum(self.gamma)\n    lhood = gammaln(np.sum(self.lda.alpha)) - gammaln(gamma_sum)\n    self.lhood[num_topics] = lhood\n    digsum = digamma(gamma_sum)\n    model = 'DTM'\n    for k in range(num_topics):\n        e_log_theta_k = digamma(self.gamma[k]) - digsum\n        lhood_term = (self.lda.alpha[k] - self.gamma[k]) * e_log_theta_k + gammaln(self.gamma[k]) - gammaln(self.lda.alpha[k])\n        n = 0\n        for (word_id, count) in self.doc:\n            if self.phi[n][k] > 0:\n                lhood_term += count * self.phi[n][k] * (e_log_theta_k + self.lda.topics[word_id][k] - self.log_phi[n][k])\n            n += 1\n        self.lhood[k] = lhood_term\n        lhood += lhood_term\n    return lhood",
            "def compute_lda_lhood(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the log likelihood bound.\\n\\n        Returns\\n        -------\\n        float\\n            The optimal lower bound for the true posterior using the approximate distribution.\\n\\n        '\n    num_topics = self.lda.num_topics\n    gamma_sum = np.sum(self.gamma)\n    lhood = gammaln(np.sum(self.lda.alpha)) - gammaln(gamma_sum)\n    self.lhood[num_topics] = lhood\n    digsum = digamma(gamma_sum)\n    model = 'DTM'\n    for k in range(num_topics):\n        e_log_theta_k = digamma(self.gamma[k]) - digsum\n        lhood_term = (self.lda.alpha[k] - self.gamma[k]) * e_log_theta_k + gammaln(self.gamma[k]) - gammaln(self.lda.alpha[k])\n        n = 0\n        for (word_id, count) in self.doc:\n            if self.phi[n][k] > 0:\n                lhood_term += count * self.phi[n][k] * (e_log_theta_k + self.lda.topics[word_id][k] - self.log_phi[n][k])\n            n += 1\n        self.lhood[k] = lhood_term\n        lhood += lhood_term\n    return lhood",
            "def compute_lda_lhood(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the log likelihood bound.\\n\\n        Returns\\n        -------\\n        float\\n            The optimal lower bound for the true posterior using the approximate distribution.\\n\\n        '\n    num_topics = self.lda.num_topics\n    gamma_sum = np.sum(self.gamma)\n    lhood = gammaln(np.sum(self.lda.alpha)) - gammaln(gamma_sum)\n    self.lhood[num_topics] = lhood\n    digsum = digamma(gamma_sum)\n    model = 'DTM'\n    for k in range(num_topics):\n        e_log_theta_k = digamma(self.gamma[k]) - digsum\n        lhood_term = (self.lda.alpha[k] - self.gamma[k]) * e_log_theta_k + gammaln(self.gamma[k]) - gammaln(self.lda.alpha[k])\n        n = 0\n        for (word_id, count) in self.doc:\n            if self.phi[n][k] > 0:\n                lhood_term += count * self.phi[n][k] * (e_log_theta_k + self.lda.topics[word_id][k] - self.log_phi[n][k])\n            n += 1\n        self.lhood[k] = lhood_term\n        lhood += lhood_term\n    return lhood",
            "def compute_lda_lhood(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the log likelihood bound.\\n\\n        Returns\\n        -------\\n        float\\n            The optimal lower bound for the true posterior using the approximate distribution.\\n\\n        '\n    num_topics = self.lda.num_topics\n    gamma_sum = np.sum(self.gamma)\n    lhood = gammaln(np.sum(self.lda.alpha)) - gammaln(gamma_sum)\n    self.lhood[num_topics] = lhood\n    digsum = digamma(gamma_sum)\n    model = 'DTM'\n    for k in range(num_topics):\n        e_log_theta_k = digamma(self.gamma[k]) - digsum\n        lhood_term = (self.lda.alpha[k] - self.gamma[k]) * e_log_theta_k + gammaln(self.gamma[k]) - gammaln(self.lda.alpha[k])\n        n = 0\n        for (word_id, count) in self.doc:\n            if self.phi[n][k] > 0:\n                lhood_term += count * self.phi[n][k] * (e_log_theta_k + self.lda.topics[word_id][k] - self.log_phi[n][k])\n            n += 1\n        self.lhood[k] = lhood_term\n        lhood += lhood_term\n    return lhood"
        ]
    },
    {
        "func_name": "fit_lda_post",
        "original": "def fit_lda_post(self, doc_number, time, ldaseq, LDA_INFERENCE_CONVERGED=1e-08, lda_inference_max_iter=25, g=None, g3_matrix=None, g4_matrix=None, g5_matrix=None):\n    \"\"\"Posterior inference for lda.\n\n        Parameters\n        ----------\n        doc_number : int\n            The documents number.\n        time : int\n            Time slice.\n        ldaseq : object\n            Unused.\n        LDA_INFERENCE_CONVERGED : float\n            Epsilon value used to check whether the inference step has sufficiently converged.\n        lda_inference_max_iter : int\n            Maximum number of iterations in the inference step.\n        g : object\n            Unused. Will be useful when the DIM model is implemented.\n        g3_matrix: object\n            Unused. Will be useful when the DIM model is implemented.\n        g4_matrix: object\n            Unused. Will be useful when the DIM model is implemented.\n        g5_matrix: object\n            Unused. Will be useful when the DIM model is implemented.\n\n        Returns\n        -------\n        float\n            The optimal lower bound for the true posterior using the approximate distribution.\n        \"\"\"\n    self.init_lda_post()\n    total = sum((count for (word_id, count) in self.doc))\n    model = 'DTM'\n    if model == 'DIM':\n        pass\n    lhood = self.compute_lda_lhood()\n    lhood_old = 0\n    converged = 0\n    iter_ = 0\n    iter_ += 1\n    lhood_old = lhood\n    self.gamma = self.update_gamma()\n    model = 'DTM'\n    if model == 'DTM' or sslm is None:\n        (self.phi, self.log_phi) = self.update_phi(doc_number, time)\n    elif model == 'DIM' and sslm is not None:\n        (self.phi, self.log_phi) = self.update_phi_fixed(doc_number, time, sslm, g3_matrix, g4_matrix, g5_matrix)\n    lhood = self.compute_lda_lhood()\n    converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n    while converged > LDA_INFERENCE_CONVERGED and iter_ <= lda_inference_max_iter:\n        iter_ += 1\n        lhood_old = lhood\n        self.gamma = self.update_gamma()\n        model = 'DTM'\n        if model == 'DTM' or sslm is None:\n            (self.phi, self.log_phi) = self.update_phi(doc_number, time)\n        elif model == 'DIM' and sslm is not None:\n            (self.phi, self.log_phi) = self.update_phi_fixed(doc_number, time, sslm, g3_matrix, g4_matrix, g5_matrix)\n        lhood = self.compute_lda_lhood()\n        converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n    return lhood",
        "mutated": [
            "def fit_lda_post(self, doc_number, time, ldaseq, LDA_INFERENCE_CONVERGED=1e-08, lda_inference_max_iter=25, g=None, g3_matrix=None, g4_matrix=None, g5_matrix=None):\n    if False:\n        i = 10\n    'Posterior inference for lda.\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            The documents number.\\n        time : int\\n            Time slice.\\n        ldaseq : object\\n            Unused.\\n        LDA_INFERENCE_CONVERGED : float\\n            Epsilon value used to check whether the inference step has sufficiently converged.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations in the inference step.\\n        g : object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g3_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g4_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g5_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n\\n        Returns\\n        -------\\n        float\\n            The optimal lower bound for the true posterior using the approximate distribution.\\n        '\n    self.init_lda_post()\n    total = sum((count for (word_id, count) in self.doc))\n    model = 'DTM'\n    if model == 'DIM':\n        pass\n    lhood = self.compute_lda_lhood()\n    lhood_old = 0\n    converged = 0\n    iter_ = 0\n    iter_ += 1\n    lhood_old = lhood\n    self.gamma = self.update_gamma()\n    model = 'DTM'\n    if model == 'DTM' or sslm is None:\n        (self.phi, self.log_phi) = self.update_phi(doc_number, time)\n    elif model == 'DIM' and sslm is not None:\n        (self.phi, self.log_phi) = self.update_phi_fixed(doc_number, time, sslm, g3_matrix, g4_matrix, g5_matrix)\n    lhood = self.compute_lda_lhood()\n    converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n    while converged > LDA_INFERENCE_CONVERGED and iter_ <= lda_inference_max_iter:\n        iter_ += 1\n        lhood_old = lhood\n        self.gamma = self.update_gamma()\n        model = 'DTM'\n        if model == 'DTM' or sslm is None:\n            (self.phi, self.log_phi) = self.update_phi(doc_number, time)\n        elif model == 'DIM' and sslm is not None:\n            (self.phi, self.log_phi) = self.update_phi_fixed(doc_number, time, sslm, g3_matrix, g4_matrix, g5_matrix)\n        lhood = self.compute_lda_lhood()\n        converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n    return lhood",
            "def fit_lda_post(self, doc_number, time, ldaseq, LDA_INFERENCE_CONVERGED=1e-08, lda_inference_max_iter=25, g=None, g3_matrix=None, g4_matrix=None, g5_matrix=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Posterior inference for lda.\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            The documents number.\\n        time : int\\n            Time slice.\\n        ldaseq : object\\n            Unused.\\n        LDA_INFERENCE_CONVERGED : float\\n            Epsilon value used to check whether the inference step has sufficiently converged.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations in the inference step.\\n        g : object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g3_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g4_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g5_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n\\n        Returns\\n        -------\\n        float\\n            The optimal lower bound for the true posterior using the approximate distribution.\\n        '\n    self.init_lda_post()\n    total = sum((count for (word_id, count) in self.doc))\n    model = 'DTM'\n    if model == 'DIM':\n        pass\n    lhood = self.compute_lda_lhood()\n    lhood_old = 0\n    converged = 0\n    iter_ = 0\n    iter_ += 1\n    lhood_old = lhood\n    self.gamma = self.update_gamma()\n    model = 'DTM'\n    if model == 'DTM' or sslm is None:\n        (self.phi, self.log_phi) = self.update_phi(doc_number, time)\n    elif model == 'DIM' and sslm is not None:\n        (self.phi, self.log_phi) = self.update_phi_fixed(doc_number, time, sslm, g3_matrix, g4_matrix, g5_matrix)\n    lhood = self.compute_lda_lhood()\n    converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n    while converged > LDA_INFERENCE_CONVERGED and iter_ <= lda_inference_max_iter:\n        iter_ += 1\n        lhood_old = lhood\n        self.gamma = self.update_gamma()\n        model = 'DTM'\n        if model == 'DTM' or sslm is None:\n            (self.phi, self.log_phi) = self.update_phi(doc_number, time)\n        elif model == 'DIM' and sslm is not None:\n            (self.phi, self.log_phi) = self.update_phi_fixed(doc_number, time, sslm, g3_matrix, g4_matrix, g5_matrix)\n        lhood = self.compute_lda_lhood()\n        converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n    return lhood",
            "def fit_lda_post(self, doc_number, time, ldaseq, LDA_INFERENCE_CONVERGED=1e-08, lda_inference_max_iter=25, g=None, g3_matrix=None, g4_matrix=None, g5_matrix=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Posterior inference for lda.\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            The documents number.\\n        time : int\\n            Time slice.\\n        ldaseq : object\\n            Unused.\\n        LDA_INFERENCE_CONVERGED : float\\n            Epsilon value used to check whether the inference step has sufficiently converged.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations in the inference step.\\n        g : object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g3_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g4_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g5_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n\\n        Returns\\n        -------\\n        float\\n            The optimal lower bound for the true posterior using the approximate distribution.\\n        '\n    self.init_lda_post()\n    total = sum((count for (word_id, count) in self.doc))\n    model = 'DTM'\n    if model == 'DIM':\n        pass\n    lhood = self.compute_lda_lhood()\n    lhood_old = 0\n    converged = 0\n    iter_ = 0\n    iter_ += 1\n    lhood_old = lhood\n    self.gamma = self.update_gamma()\n    model = 'DTM'\n    if model == 'DTM' or sslm is None:\n        (self.phi, self.log_phi) = self.update_phi(doc_number, time)\n    elif model == 'DIM' and sslm is not None:\n        (self.phi, self.log_phi) = self.update_phi_fixed(doc_number, time, sslm, g3_matrix, g4_matrix, g5_matrix)\n    lhood = self.compute_lda_lhood()\n    converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n    while converged > LDA_INFERENCE_CONVERGED and iter_ <= lda_inference_max_iter:\n        iter_ += 1\n        lhood_old = lhood\n        self.gamma = self.update_gamma()\n        model = 'DTM'\n        if model == 'DTM' or sslm is None:\n            (self.phi, self.log_phi) = self.update_phi(doc_number, time)\n        elif model == 'DIM' and sslm is not None:\n            (self.phi, self.log_phi) = self.update_phi_fixed(doc_number, time, sslm, g3_matrix, g4_matrix, g5_matrix)\n        lhood = self.compute_lda_lhood()\n        converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n    return lhood",
            "def fit_lda_post(self, doc_number, time, ldaseq, LDA_INFERENCE_CONVERGED=1e-08, lda_inference_max_iter=25, g=None, g3_matrix=None, g4_matrix=None, g5_matrix=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Posterior inference for lda.\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            The documents number.\\n        time : int\\n            Time slice.\\n        ldaseq : object\\n            Unused.\\n        LDA_INFERENCE_CONVERGED : float\\n            Epsilon value used to check whether the inference step has sufficiently converged.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations in the inference step.\\n        g : object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g3_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g4_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g5_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n\\n        Returns\\n        -------\\n        float\\n            The optimal lower bound for the true posterior using the approximate distribution.\\n        '\n    self.init_lda_post()\n    total = sum((count for (word_id, count) in self.doc))\n    model = 'DTM'\n    if model == 'DIM':\n        pass\n    lhood = self.compute_lda_lhood()\n    lhood_old = 0\n    converged = 0\n    iter_ = 0\n    iter_ += 1\n    lhood_old = lhood\n    self.gamma = self.update_gamma()\n    model = 'DTM'\n    if model == 'DTM' or sslm is None:\n        (self.phi, self.log_phi) = self.update_phi(doc_number, time)\n    elif model == 'DIM' and sslm is not None:\n        (self.phi, self.log_phi) = self.update_phi_fixed(doc_number, time, sslm, g3_matrix, g4_matrix, g5_matrix)\n    lhood = self.compute_lda_lhood()\n    converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n    while converged > LDA_INFERENCE_CONVERGED and iter_ <= lda_inference_max_iter:\n        iter_ += 1\n        lhood_old = lhood\n        self.gamma = self.update_gamma()\n        model = 'DTM'\n        if model == 'DTM' or sslm is None:\n            (self.phi, self.log_phi) = self.update_phi(doc_number, time)\n        elif model == 'DIM' and sslm is not None:\n            (self.phi, self.log_phi) = self.update_phi_fixed(doc_number, time, sslm, g3_matrix, g4_matrix, g5_matrix)\n        lhood = self.compute_lda_lhood()\n        converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n    return lhood",
            "def fit_lda_post(self, doc_number, time, ldaseq, LDA_INFERENCE_CONVERGED=1e-08, lda_inference_max_iter=25, g=None, g3_matrix=None, g4_matrix=None, g5_matrix=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Posterior inference for lda.\\n\\n        Parameters\\n        ----------\\n        doc_number : int\\n            The documents number.\\n        time : int\\n            Time slice.\\n        ldaseq : object\\n            Unused.\\n        LDA_INFERENCE_CONVERGED : float\\n            Epsilon value used to check whether the inference step has sufficiently converged.\\n        lda_inference_max_iter : int\\n            Maximum number of iterations in the inference step.\\n        g : object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g3_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g4_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n        g5_matrix: object\\n            Unused. Will be useful when the DIM model is implemented.\\n\\n        Returns\\n        -------\\n        float\\n            The optimal lower bound for the true posterior using the approximate distribution.\\n        '\n    self.init_lda_post()\n    total = sum((count for (word_id, count) in self.doc))\n    model = 'DTM'\n    if model == 'DIM':\n        pass\n    lhood = self.compute_lda_lhood()\n    lhood_old = 0\n    converged = 0\n    iter_ = 0\n    iter_ += 1\n    lhood_old = lhood\n    self.gamma = self.update_gamma()\n    model = 'DTM'\n    if model == 'DTM' or sslm is None:\n        (self.phi, self.log_phi) = self.update_phi(doc_number, time)\n    elif model == 'DIM' and sslm is not None:\n        (self.phi, self.log_phi) = self.update_phi_fixed(doc_number, time, sslm, g3_matrix, g4_matrix, g5_matrix)\n    lhood = self.compute_lda_lhood()\n    converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n    while converged > LDA_INFERENCE_CONVERGED and iter_ <= lda_inference_max_iter:\n        iter_ += 1\n        lhood_old = lhood\n        self.gamma = self.update_gamma()\n        model = 'DTM'\n        if model == 'DTM' or sslm is None:\n            (self.phi, self.log_phi) = self.update_phi(doc_number, time)\n        elif model == 'DIM' and sslm is not None:\n            (self.phi, self.log_phi) = self.update_phi_fixed(doc_number, time, sslm, g3_matrix, g4_matrix, g5_matrix)\n        lhood = self.compute_lda_lhood()\n        converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n    return lhood"
        ]
    },
    {
        "func_name": "update_lda_seq_ss",
        "original": "def update_lda_seq_ss(self, time, doc, topic_suffstats):\n    \"\"\"Update lda sequence sufficient statistics from an lda posterior.\n\n        This is very similar to the :meth:`~gensim.models.ldaseqmodel.LdaPost.update_gamma` method and uses\n        the same formula.\n\n        Parameters\n        ----------\n        time : int\n            The time slice.\n        doc : list of (int, float)\n            Unused but kept here for backwards compatibility. The document set in the constructor (`self.doc`) is used\n            instead.\n        topic_suffstats : list of float\n            Sufficient statistics for each topic.\n\n        Returns\n        -------\n        list of float\n            The updated sufficient statistics for each topic.\n\n        \"\"\"\n    num_topics = self.lda.num_topics\n    for k in range(num_topics):\n        topic_ss = topic_suffstats[k]\n        n = 0\n        for (word_id, count) in self.doc:\n            topic_ss[word_id][time] += count * self.phi[n][k]\n            n += 1\n        topic_suffstats[k] = topic_ss\n    return topic_suffstats",
        "mutated": [
            "def update_lda_seq_ss(self, time, doc, topic_suffstats):\n    if False:\n        i = 10\n    'Update lda sequence sufficient statistics from an lda posterior.\\n\\n        This is very similar to the :meth:`~gensim.models.ldaseqmodel.LdaPost.update_gamma` method and uses\\n        the same formula.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice.\\n        doc : list of (int, float)\\n            Unused but kept here for backwards compatibility. The document set in the constructor (`self.doc`) is used\\n            instead.\\n        topic_suffstats : list of float\\n            Sufficient statistics for each topic.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated sufficient statistics for each topic.\\n\\n        '\n    num_topics = self.lda.num_topics\n    for k in range(num_topics):\n        topic_ss = topic_suffstats[k]\n        n = 0\n        for (word_id, count) in self.doc:\n            topic_ss[word_id][time] += count * self.phi[n][k]\n            n += 1\n        topic_suffstats[k] = topic_ss\n    return topic_suffstats",
            "def update_lda_seq_ss(self, time, doc, topic_suffstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Update lda sequence sufficient statistics from an lda posterior.\\n\\n        This is very similar to the :meth:`~gensim.models.ldaseqmodel.LdaPost.update_gamma` method and uses\\n        the same formula.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice.\\n        doc : list of (int, float)\\n            Unused but kept here for backwards compatibility. The document set in the constructor (`self.doc`) is used\\n            instead.\\n        topic_suffstats : list of float\\n            Sufficient statistics for each topic.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated sufficient statistics for each topic.\\n\\n        '\n    num_topics = self.lda.num_topics\n    for k in range(num_topics):\n        topic_ss = topic_suffstats[k]\n        n = 0\n        for (word_id, count) in self.doc:\n            topic_ss[word_id][time] += count * self.phi[n][k]\n            n += 1\n        topic_suffstats[k] = topic_ss\n    return topic_suffstats",
            "def update_lda_seq_ss(self, time, doc, topic_suffstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Update lda sequence sufficient statistics from an lda posterior.\\n\\n        This is very similar to the :meth:`~gensim.models.ldaseqmodel.LdaPost.update_gamma` method and uses\\n        the same formula.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice.\\n        doc : list of (int, float)\\n            Unused but kept here for backwards compatibility. The document set in the constructor (`self.doc`) is used\\n            instead.\\n        topic_suffstats : list of float\\n            Sufficient statistics for each topic.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated sufficient statistics for each topic.\\n\\n        '\n    num_topics = self.lda.num_topics\n    for k in range(num_topics):\n        topic_ss = topic_suffstats[k]\n        n = 0\n        for (word_id, count) in self.doc:\n            topic_ss[word_id][time] += count * self.phi[n][k]\n            n += 1\n        topic_suffstats[k] = topic_ss\n    return topic_suffstats",
            "def update_lda_seq_ss(self, time, doc, topic_suffstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Update lda sequence sufficient statistics from an lda posterior.\\n\\n        This is very similar to the :meth:`~gensim.models.ldaseqmodel.LdaPost.update_gamma` method and uses\\n        the same formula.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice.\\n        doc : list of (int, float)\\n            Unused but kept here for backwards compatibility. The document set in the constructor (`self.doc`) is used\\n            instead.\\n        topic_suffstats : list of float\\n            Sufficient statistics for each topic.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated sufficient statistics for each topic.\\n\\n        '\n    num_topics = self.lda.num_topics\n    for k in range(num_topics):\n        topic_ss = topic_suffstats[k]\n        n = 0\n        for (word_id, count) in self.doc:\n            topic_ss[word_id][time] += count * self.phi[n][k]\n            n += 1\n        topic_suffstats[k] = topic_ss\n    return topic_suffstats",
            "def update_lda_seq_ss(self, time, doc, topic_suffstats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Update lda sequence sufficient statistics from an lda posterior.\\n\\n        This is very similar to the :meth:`~gensim.models.ldaseqmodel.LdaPost.update_gamma` method and uses\\n        the same formula.\\n\\n        Parameters\\n        ----------\\n        time : int\\n            The time slice.\\n        doc : list of (int, float)\\n            Unused but kept here for backwards compatibility. The document set in the constructor (`self.doc`) is used\\n            instead.\\n        topic_suffstats : list of float\\n            Sufficient statistics for each topic.\\n\\n        Returns\\n        -------\\n        list of float\\n            The updated sufficient statistics for each topic.\\n\\n        '\n    num_topics = self.lda.num_topics\n    for k in range(num_topics):\n        topic_ss = topic_suffstats[k]\n        n = 0\n        for (word_id, count) in self.doc:\n            topic_ss[word_id][time] += count * self.phi[n][k]\n            n += 1\n        topic_suffstats[k] = topic_ss\n    return topic_suffstats"
        ]
    },
    {
        "func_name": "f_obs",
        "original": "def f_obs(x, *args):\n    \"\"\"Function which we are optimising for minimizing obs.\n\n    Parameters\n    ----------\n    x : list of float\n        The obs values for this word.\n    sslm : :class:`~gensim.models.ldaseqmodel.sslm`\n        The State Space Language Model for DTM.\n    word_counts : list of int\n        Total word counts for each time slice.\n    totals : list of int of length `len(self.time_slice)`\n        The totals for each time slice.\n    mean_deriv_mtx : list of float\n        Mean derivative for each time slice.\n    word : int\n        The word's ID.\n    deriv : list of float\n        Mean derivative for each time slice.\n\n    Returns\n    -------\n    list of float\n        The value of the objective function evaluated at point `x`.\n\n    \"\"\"\n    (sslm, word_counts, totals, mean_deriv_mtx, word, deriv) = args\n    init_mult = 1000\n    T = len(x)\n    val = 0\n    term1 = 0\n    term2 = 0\n    term3 = 0\n    term4 = 0\n    sslm.obs[word] = x\n    (sslm.mean[word], sslm.fwd_mean[word]) = sslm.compute_post_mean(word, sslm.chain_variance)\n    mean = sslm.mean[word]\n    variance = sslm.variance[word]\n    for t in range(1, T + 1):\n        mean_t = mean[t]\n        mean_t_prev = mean[t - 1]\n        val = mean_t - mean_t_prev\n        term1 += val * val\n        term2 += word_counts[t - 1] * mean_t - totals[t - 1] * np.exp(mean_t + variance[t] / 2) / sslm.zeta[t - 1]\n        model = 'DTM'\n        if model == 'DIM':\n            pass\n    if sslm.chain_variance > 0.0:\n        term1 = -(term1 / (2 * sslm.chain_variance))\n        term1 = term1 - mean[0] * mean[0] / (2 * init_mult * sslm.chain_variance)\n    else:\n        term1 = 0.0\n    final = -(term1 + term2 + term3 + term4)\n    return final",
        "mutated": [
            "def f_obs(x, *args):\n    if False:\n        i = 10\n    \"Function which we are optimising for minimizing obs.\\n\\n    Parameters\\n    ----------\\n    x : list of float\\n        The obs values for this word.\\n    sslm : :class:`~gensim.models.ldaseqmodel.sslm`\\n        The State Space Language Model for DTM.\\n    word_counts : list of int\\n        Total word counts for each time slice.\\n    totals : list of int of length `len(self.time_slice)`\\n        The totals for each time slice.\\n    mean_deriv_mtx : list of float\\n        Mean derivative for each time slice.\\n    word : int\\n        The word's ID.\\n    deriv : list of float\\n        Mean derivative for each time slice.\\n\\n    Returns\\n    -------\\n    list of float\\n        The value of the objective function evaluated at point `x`.\\n\\n    \"\n    (sslm, word_counts, totals, mean_deriv_mtx, word, deriv) = args\n    init_mult = 1000\n    T = len(x)\n    val = 0\n    term1 = 0\n    term2 = 0\n    term3 = 0\n    term4 = 0\n    sslm.obs[word] = x\n    (sslm.mean[word], sslm.fwd_mean[word]) = sslm.compute_post_mean(word, sslm.chain_variance)\n    mean = sslm.mean[word]\n    variance = sslm.variance[word]\n    for t in range(1, T + 1):\n        mean_t = mean[t]\n        mean_t_prev = mean[t - 1]\n        val = mean_t - mean_t_prev\n        term1 += val * val\n        term2 += word_counts[t - 1] * mean_t - totals[t - 1] * np.exp(mean_t + variance[t] / 2) / sslm.zeta[t - 1]\n        model = 'DTM'\n        if model == 'DIM':\n            pass\n    if sslm.chain_variance > 0.0:\n        term1 = -(term1 / (2 * sslm.chain_variance))\n        term1 = term1 - mean[0] * mean[0] / (2 * init_mult * sslm.chain_variance)\n    else:\n        term1 = 0.0\n    final = -(term1 + term2 + term3 + term4)\n    return final",
            "def f_obs(x, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Function which we are optimising for minimizing obs.\\n\\n    Parameters\\n    ----------\\n    x : list of float\\n        The obs values for this word.\\n    sslm : :class:`~gensim.models.ldaseqmodel.sslm`\\n        The State Space Language Model for DTM.\\n    word_counts : list of int\\n        Total word counts for each time slice.\\n    totals : list of int of length `len(self.time_slice)`\\n        The totals for each time slice.\\n    mean_deriv_mtx : list of float\\n        Mean derivative for each time slice.\\n    word : int\\n        The word's ID.\\n    deriv : list of float\\n        Mean derivative for each time slice.\\n\\n    Returns\\n    -------\\n    list of float\\n        The value of the objective function evaluated at point `x`.\\n\\n    \"\n    (sslm, word_counts, totals, mean_deriv_mtx, word, deriv) = args\n    init_mult = 1000\n    T = len(x)\n    val = 0\n    term1 = 0\n    term2 = 0\n    term3 = 0\n    term4 = 0\n    sslm.obs[word] = x\n    (sslm.mean[word], sslm.fwd_mean[word]) = sslm.compute_post_mean(word, sslm.chain_variance)\n    mean = sslm.mean[word]\n    variance = sslm.variance[word]\n    for t in range(1, T + 1):\n        mean_t = mean[t]\n        mean_t_prev = mean[t - 1]\n        val = mean_t - mean_t_prev\n        term1 += val * val\n        term2 += word_counts[t - 1] * mean_t - totals[t - 1] * np.exp(mean_t + variance[t] / 2) / sslm.zeta[t - 1]\n        model = 'DTM'\n        if model == 'DIM':\n            pass\n    if sslm.chain_variance > 0.0:\n        term1 = -(term1 / (2 * sslm.chain_variance))\n        term1 = term1 - mean[0] * mean[0] / (2 * init_mult * sslm.chain_variance)\n    else:\n        term1 = 0.0\n    final = -(term1 + term2 + term3 + term4)\n    return final",
            "def f_obs(x, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Function which we are optimising for minimizing obs.\\n\\n    Parameters\\n    ----------\\n    x : list of float\\n        The obs values for this word.\\n    sslm : :class:`~gensim.models.ldaseqmodel.sslm`\\n        The State Space Language Model for DTM.\\n    word_counts : list of int\\n        Total word counts for each time slice.\\n    totals : list of int of length `len(self.time_slice)`\\n        The totals for each time slice.\\n    mean_deriv_mtx : list of float\\n        Mean derivative for each time slice.\\n    word : int\\n        The word's ID.\\n    deriv : list of float\\n        Mean derivative for each time slice.\\n\\n    Returns\\n    -------\\n    list of float\\n        The value of the objective function evaluated at point `x`.\\n\\n    \"\n    (sslm, word_counts, totals, mean_deriv_mtx, word, deriv) = args\n    init_mult = 1000\n    T = len(x)\n    val = 0\n    term1 = 0\n    term2 = 0\n    term3 = 0\n    term4 = 0\n    sslm.obs[word] = x\n    (sslm.mean[word], sslm.fwd_mean[word]) = sslm.compute_post_mean(word, sslm.chain_variance)\n    mean = sslm.mean[word]\n    variance = sslm.variance[word]\n    for t in range(1, T + 1):\n        mean_t = mean[t]\n        mean_t_prev = mean[t - 1]\n        val = mean_t - mean_t_prev\n        term1 += val * val\n        term2 += word_counts[t - 1] * mean_t - totals[t - 1] * np.exp(mean_t + variance[t] / 2) / sslm.zeta[t - 1]\n        model = 'DTM'\n        if model == 'DIM':\n            pass\n    if sslm.chain_variance > 0.0:\n        term1 = -(term1 / (2 * sslm.chain_variance))\n        term1 = term1 - mean[0] * mean[0] / (2 * init_mult * sslm.chain_variance)\n    else:\n        term1 = 0.0\n    final = -(term1 + term2 + term3 + term4)\n    return final",
            "def f_obs(x, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Function which we are optimising for minimizing obs.\\n\\n    Parameters\\n    ----------\\n    x : list of float\\n        The obs values for this word.\\n    sslm : :class:`~gensim.models.ldaseqmodel.sslm`\\n        The State Space Language Model for DTM.\\n    word_counts : list of int\\n        Total word counts for each time slice.\\n    totals : list of int of length `len(self.time_slice)`\\n        The totals for each time slice.\\n    mean_deriv_mtx : list of float\\n        Mean derivative for each time slice.\\n    word : int\\n        The word's ID.\\n    deriv : list of float\\n        Mean derivative for each time slice.\\n\\n    Returns\\n    -------\\n    list of float\\n        The value of the objective function evaluated at point `x`.\\n\\n    \"\n    (sslm, word_counts, totals, mean_deriv_mtx, word, deriv) = args\n    init_mult = 1000\n    T = len(x)\n    val = 0\n    term1 = 0\n    term2 = 0\n    term3 = 0\n    term4 = 0\n    sslm.obs[word] = x\n    (sslm.mean[word], sslm.fwd_mean[word]) = sslm.compute_post_mean(word, sslm.chain_variance)\n    mean = sslm.mean[word]\n    variance = sslm.variance[word]\n    for t in range(1, T + 1):\n        mean_t = mean[t]\n        mean_t_prev = mean[t - 1]\n        val = mean_t - mean_t_prev\n        term1 += val * val\n        term2 += word_counts[t - 1] * mean_t - totals[t - 1] * np.exp(mean_t + variance[t] / 2) / sslm.zeta[t - 1]\n        model = 'DTM'\n        if model == 'DIM':\n            pass\n    if sslm.chain_variance > 0.0:\n        term1 = -(term1 / (2 * sslm.chain_variance))\n        term1 = term1 - mean[0] * mean[0] / (2 * init_mult * sslm.chain_variance)\n    else:\n        term1 = 0.0\n    final = -(term1 + term2 + term3 + term4)\n    return final",
            "def f_obs(x, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Function which we are optimising for minimizing obs.\\n\\n    Parameters\\n    ----------\\n    x : list of float\\n        The obs values for this word.\\n    sslm : :class:`~gensim.models.ldaseqmodel.sslm`\\n        The State Space Language Model for DTM.\\n    word_counts : list of int\\n        Total word counts for each time slice.\\n    totals : list of int of length `len(self.time_slice)`\\n        The totals for each time slice.\\n    mean_deriv_mtx : list of float\\n        Mean derivative for each time slice.\\n    word : int\\n        The word's ID.\\n    deriv : list of float\\n        Mean derivative for each time slice.\\n\\n    Returns\\n    -------\\n    list of float\\n        The value of the objective function evaluated at point `x`.\\n\\n    \"\n    (sslm, word_counts, totals, mean_deriv_mtx, word, deriv) = args\n    init_mult = 1000\n    T = len(x)\n    val = 0\n    term1 = 0\n    term2 = 0\n    term3 = 0\n    term4 = 0\n    sslm.obs[word] = x\n    (sslm.mean[word], sslm.fwd_mean[word]) = sslm.compute_post_mean(word, sslm.chain_variance)\n    mean = sslm.mean[word]\n    variance = sslm.variance[word]\n    for t in range(1, T + 1):\n        mean_t = mean[t]\n        mean_t_prev = mean[t - 1]\n        val = mean_t - mean_t_prev\n        term1 += val * val\n        term2 += word_counts[t - 1] * mean_t - totals[t - 1] * np.exp(mean_t + variance[t] / 2) / sslm.zeta[t - 1]\n        model = 'DTM'\n        if model == 'DIM':\n            pass\n    if sslm.chain_variance > 0.0:\n        term1 = -(term1 / (2 * sslm.chain_variance))\n        term1 = term1 - mean[0] * mean[0] / (2 * init_mult * sslm.chain_variance)\n    else:\n        term1 = 0.0\n    final = -(term1 + term2 + term3 + term4)\n    return final"
        ]
    },
    {
        "func_name": "df_obs",
        "original": "def df_obs(x, *args):\n    \"\"\"Derivative of the objective function which optimises obs.\n\n    Parameters\n    ----------\n    x : list of float\n        The obs values for this word.\n    sslm : :class:`~gensim.models.ldaseqmodel.sslm`\n        The State Space Language Model for DTM.\n    word_counts : list of int\n        Total word counts for each time slice.\n    totals : list of int of length `len(self.time_slice)`\n        The totals for each time slice.\n    mean_deriv_mtx : list of float\n        Mean derivative for each time slice.\n    word : int\n        The word's ID.\n    deriv : list of float\n        Mean derivative for each time slice.\n\n    Returns\n    -------\n    list of float\n        The derivative of the objective function evaluated at point `x`.\n\n    \"\"\"\n    (sslm, word_counts, totals, mean_deriv_mtx, word, deriv) = args\n    sslm.obs[word] = x\n    (sslm.mean[word], sslm.fwd_mean[word]) = sslm.compute_post_mean(word, sslm.chain_variance)\n    model = 'DTM'\n    if model == 'DTM':\n        deriv = sslm.compute_obs_deriv(word, word_counts, totals, mean_deriv_mtx, deriv)\n    elif model == 'DIM':\n        deriv = sslm.compute_obs_deriv_fixed(p.word, p.word_counts, p.totals, p.sslm, p.mean_deriv_mtx, deriv)\n    return np.negative(deriv)",
        "mutated": [
            "def df_obs(x, *args):\n    if False:\n        i = 10\n    \"Derivative of the objective function which optimises obs.\\n\\n    Parameters\\n    ----------\\n    x : list of float\\n        The obs values for this word.\\n    sslm : :class:`~gensim.models.ldaseqmodel.sslm`\\n        The State Space Language Model for DTM.\\n    word_counts : list of int\\n        Total word counts for each time slice.\\n    totals : list of int of length `len(self.time_slice)`\\n        The totals for each time slice.\\n    mean_deriv_mtx : list of float\\n        Mean derivative for each time slice.\\n    word : int\\n        The word's ID.\\n    deriv : list of float\\n        Mean derivative for each time slice.\\n\\n    Returns\\n    -------\\n    list of float\\n        The derivative of the objective function evaluated at point `x`.\\n\\n    \"\n    (sslm, word_counts, totals, mean_deriv_mtx, word, deriv) = args\n    sslm.obs[word] = x\n    (sslm.mean[word], sslm.fwd_mean[word]) = sslm.compute_post_mean(word, sslm.chain_variance)\n    model = 'DTM'\n    if model == 'DTM':\n        deriv = sslm.compute_obs_deriv(word, word_counts, totals, mean_deriv_mtx, deriv)\n    elif model == 'DIM':\n        deriv = sslm.compute_obs_deriv_fixed(p.word, p.word_counts, p.totals, p.sslm, p.mean_deriv_mtx, deriv)\n    return np.negative(deriv)",
            "def df_obs(x, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Derivative of the objective function which optimises obs.\\n\\n    Parameters\\n    ----------\\n    x : list of float\\n        The obs values for this word.\\n    sslm : :class:`~gensim.models.ldaseqmodel.sslm`\\n        The State Space Language Model for DTM.\\n    word_counts : list of int\\n        Total word counts for each time slice.\\n    totals : list of int of length `len(self.time_slice)`\\n        The totals for each time slice.\\n    mean_deriv_mtx : list of float\\n        Mean derivative for each time slice.\\n    word : int\\n        The word's ID.\\n    deriv : list of float\\n        Mean derivative for each time slice.\\n\\n    Returns\\n    -------\\n    list of float\\n        The derivative of the objective function evaluated at point `x`.\\n\\n    \"\n    (sslm, word_counts, totals, mean_deriv_mtx, word, deriv) = args\n    sslm.obs[word] = x\n    (sslm.mean[word], sslm.fwd_mean[word]) = sslm.compute_post_mean(word, sslm.chain_variance)\n    model = 'DTM'\n    if model == 'DTM':\n        deriv = sslm.compute_obs_deriv(word, word_counts, totals, mean_deriv_mtx, deriv)\n    elif model == 'DIM':\n        deriv = sslm.compute_obs_deriv_fixed(p.word, p.word_counts, p.totals, p.sslm, p.mean_deriv_mtx, deriv)\n    return np.negative(deriv)",
            "def df_obs(x, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Derivative of the objective function which optimises obs.\\n\\n    Parameters\\n    ----------\\n    x : list of float\\n        The obs values for this word.\\n    sslm : :class:`~gensim.models.ldaseqmodel.sslm`\\n        The State Space Language Model for DTM.\\n    word_counts : list of int\\n        Total word counts for each time slice.\\n    totals : list of int of length `len(self.time_slice)`\\n        The totals for each time slice.\\n    mean_deriv_mtx : list of float\\n        Mean derivative for each time slice.\\n    word : int\\n        The word's ID.\\n    deriv : list of float\\n        Mean derivative for each time slice.\\n\\n    Returns\\n    -------\\n    list of float\\n        The derivative of the objective function evaluated at point `x`.\\n\\n    \"\n    (sslm, word_counts, totals, mean_deriv_mtx, word, deriv) = args\n    sslm.obs[word] = x\n    (sslm.mean[word], sslm.fwd_mean[word]) = sslm.compute_post_mean(word, sslm.chain_variance)\n    model = 'DTM'\n    if model == 'DTM':\n        deriv = sslm.compute_obs_deriv(word, word_counts, totals, mean_deriv_mtx, deriv)\n    elif model == 'DIM':\n        deriv = sslm.compute_obs_deriv_fixed(p.word, p.word_counts, p.totals, p.sslm, p.mean_deriv_mtx, deriv)\n    return np.negative(deriv)",
            "def df_obs(x, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Derivative of the objective function which optimises obs.\\n\\n    Parameters\\n    ----------\\n    x : list of float\\n        The obs values for this word.\\n    sslm : :class:`~gensim.models.ldaseqmodel.sslm`\\n        The State Space Language Model for DTM.\\n    word_counts : list of int\\n        Total word counts for each time slice.\\n    totals : list of int of length `len(self.time_slice)`\\n        The totals for each time slice.\\n    mean_deriv_mtx : list of float\\n        Mean derivative for each time slice.\\n    word : int\\n        The word's ID.\\n    deriv : list of float\\n        Mean derivative for each time slice.\\n\\n    Returns\\n    -------\\n    list of float\\n        The derivative of the objective function evaluated at point `x`.\\n\\n    \"\n    (sslm, word_counts, totals, mean_deriv_mtx, word, deriv) = args\n    sslm.obs[word] = x\n    (sslm.mean[word], sslm.fwd_mean[word]) = sslm.compute_post_mean(word, sslm.chain_variance)\n    model = 'DTM'\n    if model == 'DTM':\n        deriv = sslm.compute_obs_deriv(word, word_counts, totals, mean_deriv_mtx, deriv)\n    elif model == 'DIM':\n        deriv = sslm.compute_obs_deriv_fixed(p.word, p.word_counts, p.totals, p.sslm, p.mean_deriv_mtx, deriv)\n    return np.negative(deriv)",
            "def df_obs(x, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Derivative of the objective function which optimises obs.\\n\\n    Parameters\\n    ----------\\n    x : list of float\\n        The obs values for this word.\\n    sslm : :class:`~gensim.models.ldaseqmodel.sslm`\\n        The State Space Language Model for DTM.\\n    word_counts : list of int\\n        Total word counts for each time slice.\\n    totals : list of int of length `len(self.time_slice)`\\n        The totals for each time slice.\\n    mean_deriv_mtx : list of float\\n        Mean derivative for each time slice.\\n    word : int\\n        The word's ID.\\n    deriv : list of float\\n        Mean derivative for each time slice.\\n\\n    Returns\\n    -------\\n    list of float\\n        The derivative of the objective function evaluated at point `x`.\\n\\n    \"\n    (sslm, word_counts, totals, mean_deriv_mtx, word, deriv) = args\n    sslm.obs[word] = x\n    (sslm.mean[word], sslm.fwd_mean[word]) = sslm.compute_post_mean(word, sslm.chain_variance)\n    model = 'DTM'\n    if model == 'DTM':\n        deriv = sslm.compute_obs_deriv(word, word_counts, totals, mean_deriv_mtx, deriv)\n    elif model == 'DIM':\n        deriv = sslm.compute_obs_deriv_fixed(p.word, p.word_counts, p.totals, p.sslm, p.mean_deriv_mtx, deriv)\n    return np.negative(deriv)"
        ]
    }
]