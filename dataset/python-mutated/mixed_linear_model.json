[
    {
        "func_name": "_dot",
        "original": "def _dot(x, y):\n    \"\"\"\n    Returns the dot product of the arrays, works for sparse and dense.\n    \"\"\"\n    if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        return np.dot(x, y)\n    elif sparse.issparse(x):\n        return x.dot(y)\n    elif sparse.issparse(y):\n        return y.T.dot(x.T).T",
        "mutated": [
            "def _dot(x, y):\n    if False:\n        i = 10\n    '\\n    Returns the dot product of the arrays, works for sparse and dense.\\n    '\n    if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        return np.dot(x, y)\n    elif sparse.issparse(x):\n        return x.dot(y)\n    elif sparse.issparse(y):\n        return y.T.dot(x.T).T",
            "def _dot(x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Returns the dot product of the arrays, works for sparse and dense.\\n    '\n    if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        return np.dot(x, y)\n    elif sparse.issparse(x):\n        return x.dot(y)\n    elif sparse.issparse(y):\n        return y.T.dot(x.T).T",
            "def _dot(x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Returns the dot product of the arrays, works for sparse and dense.\\n    '\n    if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        return np.dot(x, y)\n    elif sparse.issparse(x):\n        return x.dot(y)\n    elif sparse.issparse(y):\n        return y.T.dot(x.T).T",
            "def _dot(x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Returns the dot product of the arrays, works for sparse and dense.\\n    '\n    if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        return np.dot(x, y)\n    elif sparse.issparse(x):\n        return x.dot(y)\n    elif sparse.issparse(y):\n        return y.T.dot(x.T).T",
            "def _dot(x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Returns the dot product of the arrays, works for sparse and dense.\\n    '\n    if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        return np.dot(x, y)\n    elif sparse.issparse(x):\n        return x.dot(y)\n    elif sparse.issparse(y):\n        return y.T.dot(x.T).T"
        ]
    },
    {
        "func_name": "_multi_dot_three",
        "original": "def _multi_dot_three(A, B, C):\n    \"\"\"\n    Find best ordering for three arrays and do the multiplication.\n\n    Doing in manually instead of using dynamic programing is\n    approximately 15 times faster.\n    \"\"\"\n    cost1 = A.shape[0] * A.shape[1] * B.shape[1] + A.shape[0] * B.shape[1] * C.shape[1]\n    cost2 = B.shape[0] * B.shape[1] * C.shape[1] + A.shape[0] * A.shape[1] * C.shape[1]\n    if cost1 < cost2:\n        return _dot(_dot(A, B), C)\n    else:\n        return _dot(A, _dot(B, C))",
        "mutated": [
            "def _multi_dot_three(A, B, C):\n    if False:\n        i = 10\n    '\\n    Find best ordering for three arrays and do the multiplication.\\n\\n    Doing in manually instead of using dynamic programing is\\n    approximately 15 times faster.\\n    '\n    cost1 = A.shape[0] * A.shape[1] * B.shape[1] + A.shape[0] * B.shape[1] * C.shape[1]\n    cost2 = B.shape[0] * B.shape[1] * C.shape[1] + A.shape[0] * A.shape[1] * C.shape[1]\n    if cost1 < cost2:\n        return _dot(_dot(A, B), C)\n    else:\n        return _dot(A, _dot(B, C))",
            "def _multi_dot_three(A, B, C):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Find best ordering for three arrays and do the multiplication.\\n\\n    Doing in manually instead of using dynamic programing is\\n    approximately 15 times faster.\\n    '\n    cost1 = A.shape[0] * A.shape[1] * B.shape[1] + A.shape[0] * B.shape[1] * C.shape[1]\n    cost2 = B.shape[0] * B.shape[1] * C.shape[1] + A.shape[0] * A.shape[1] * C.shape[1]\n    if cost1 < cost2:\n        return _dot(_dot(A, B), C)\n    else:\n        return _dot(A, _dot(B, C))",
            "def _multi_dot_three(A, B, C):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Find best ordering for three arrays and do the multiplication.\\n\\n    Doing in manually instead of using dynamic programing is\\n    approximately 15 times faster.\\n    '\n    cost1 = A.shape[0] * A.shape[1] * B.shape[1] + A.shape[0] * B.shape[1] * C.shape[1]\n    cost2 = B.shape[0] * B.shape[1] * C.shape[1] + A.shape[0] * A.shape[1] * C.shape[1]\n    if cost1 < cost2:\n        return _dot(_dot(A, B), C)\n    else:\n        return _dot(A, _dot(B, C))",
            "def _multi_dot_three(A, B, C):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Find best ordering for three arrays and do the multiplication.\\n\\n    Doing in manually instead of using dynamic programing is\\n    approximately 15 times faster.\\n    '\n    cost1 = A.shape[0] * A.shape[1] * B.shape[1] + A.shape[0] * B.shape[1] * C.shape[1]\n    cost2 = B.shape[0] * B.shape[1] * C.shape[1] + A.shape[0] * A.shape[1] * C.shape[1]\n    if cost1 < cost2:\n        return _dot(_dot(A, B), C)\n    else:\n        return _dot(A, _dot(B, C))",
            "def _multi_dot_three(A, B, C):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Find best ordering for three arrays and do the multiplication.\\n\\n    Doing in manually instead of using dynamic programing is\\n    approximately 15 times faster.\\n    '\n    cost1 = A.shape[0] * A.shape[1] * B.shape[1] + A.shape[0] * B.shape[1] * C.shape[1]\n    cost2 = B.shape[0] * B.shape[1] * C.shape[1] + A.shape[0] * A.shape[1] * C.shape[1]\n    if cost1 < cost2:\n        return _dot(_dot(A, B), C)\n    else:\n        return _dot(A, _dot(B, C))"
        ]
    },
    {
        "func_name": "_dotsum",
        "original": "def _dotsum(x, y):\n    \"\"\"\n    Returns sum(x * y), where '*' is the pointwise product, computed\n    efficiently for dense and sparse matrices.\n    \"\"\"\n    if sparse.issparse(x):\n        return x.multiply(y).sum()\n    else:\n        return np.dot(x.ravel(), y.ravel())",
        "mutated": [
            "def _dotsum(x, y):\n    if False:\n        i = 10\n    \"\\n    Returns sum(x * y), where '*' is the pointwise product, computed\\n    efficiently for dense and sparse matrices.\\n    \"\n    if sparse.issparse(x):\n        return x.multiply(y).sum()\n    else:\n        return np.dot(x.ravel(), y.ravel())",
            "def _dotsum(x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Returns sum(x * y), where '*' is the pointwise product, computed\\n    efficiently for dense and sparse matrices.\\n    \"\n    if sparse.issparse(x):\n        return x.multiply(y).sum()\n    else:\n        return np.dot(x.ravel(), y.ravel())",
            "def _dotsum(x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Returns sum(x * y), where '*' is the pointwise product, computed\\n    efficiently for dense and sparse matrices.\\n    \"\n    if sparse.issparse(x):\n        return x.multiply(y).sum()\n    else:\n        return np.dot(x.ravel(), y.ravel())",
            "def _dotsum(x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Returns sum(x * y), where '*' is the pointwise product, computed\\n    efficiently for dense and sparse matrices.\\n    \"\n    if sparse.issparse(x):\n        return x.multiply(y).sum()\n    else:\n        return np.dot(x.ravel(), y.ravel())",
            "def _dotsum(x, y):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Returns sum(x * y), where '*' is the pointwise product, computed\\n    efficiently for dense and sparse matrices.\\n    \"\n    if sparse.issparse(x):\n        return x.multiply(y).sum()\n    else:\n        return np.dot(x.ravel(), y.ravel())"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, names, colnames, mats):\n    self.names = names\n    self.colnames = colnames\n    self.mats = mats",
        "mutated": [
            "def __init__(self, names, colnames, mats):\n    if False:\n        i = 10\n    self.names = names\n    self.colnames = colnames\n    self.mats = mats",
            "def __init__(self, names, colnames, mats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.names = names\n    self.colnames = colnames\n    self.mats = mats",
            "def __init__(self, names, colnames, mats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.names = names\n    self.colnames = colnames\n    self.mats = mats",
            "def __init__(self, names, colnames, mats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.names = names\n    self.colnames = colnames\n    self.mats = mats",
            "def __init__(self, names, colnames, mats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.names = names\n    self.colnames = colnames\n    self.mats = mats"
        ]
    },
    {
        "func_name": "_get_exog_re_names",
        "original": "def _get_exog_re_names(self, exog_re):\n    \"\"\"\n    Passes through if given a list of names. Otherwise, gets pandas names\n    or creates some generic variable names as needed.\n    \"\"\"\n    if self.k_re == 0:\n        return []\n    if isinstance(exog_re, pd.DataFrame):\n        return exog_re.columns.tolist()\n    elif isinstance(exog_re, pd.Series) and exog_re.name is not None:\n        return [exog_re.name]\n    elif isinstance(exog_re, list):\n        return exog_re\n    defnames = ['x_re{0:1d}'.format(k + 1) for k in range(exog_re.shape[1])]\n    return defnames",
        "mutated": [
            "def _get_exog_re_names(self, exog_re):\n    if False:\n        i = 10\n    '\\n    Passes through if given a list of names. Otherwise, gets pandas names\\n    or creates some generic variable names as needed.\\n    '\n    if self.k_re == 0:\n        return []\n    if isinstance(exog_re, pd.DataFrame):\n        return exog_re.columns.tolist()\n    elif isinstance(exog_re, pd.Series) and exog_re.name is not None:\n        return [exog_re.name]\n    elif isinstance(exog_re, list):\n        return exog_re\n    defnames = ['x_re{0:1d}'.format(k + 1) for k in range(exog_re.shape[1])]\n    return defnames",
            "def _get_exog_re_names(self, exog_re):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Passes through if given a list of names. Otherwise, gets pandas names\\n    or creates some generic variable names as needed.\\n    '\n    if self.k_re == 0:\n        return []\n    if isinstance(exog_re, pd.DataFrame):\n        return exog_re.columns.tolist()\n    elif isinstance(exog_re, pd.Series) and exog_re.name is not None:\n        return [exog_re.name]\n    elif isinstance(exog_re, list):\n        return exog_re\n    defnames = ['x_re{0:1d}'.format(k + 1) for k in range(exog_re.shape[1])]\n    return defnames",
            "def _get_exog_re_names(self, exog_re):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Passes through if given a list of names. Otherwise, gets pandas names\\n    or creates some generic variable names as needed.\\n    '\n    if self.k_re == 0:\n        return []\n    if isinstance(exog_re, pd.DataFrame):\n        return exog_re.columns.tolist()\n    elif isinstance(exog_re, pd.Series) and exog_re.name is not None:\n        return [exog_re.name]\n    elif isinstance(exog_re, list):\n        return exog_re\n    defnames = ['x_re{0:1d}'.format(k + 1) for k in range(exog_re.shape[1])]\n    return defnames",
            "def _get_exog_re_names(self, exog_re):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Passes through if given a list of names. Otherwise, gets pandas names\\n    or creates some generic variable names as needed.\\n    '\n    if self.k_re == 0:\n        return []\n    if isinstance(exog_re, pd.DataFrame):\n        return exog_re.columns.tolist()\n    elif isinstance(exog_re, pd.Series) and exog_re.name is not None:\n        return [exog_re.name]\n    elif isinstance(exog_re, list):\n        return exog_re\n    defnames = ['x_re{0:1d}'.format(k + 1) for k in range(exog_re.shape[1])]\n    return defnames",
            "def _get_exog_re_names(self, exog_re):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Passes through if given a list of names. Otherwise, gets pandas names\\n    or creates some generic variable names as needed.\\n    '\n    if self.k_re == 0:\n        return []\n    if isinstance(exog_re, pd.DataFrame):\n        return exog_re.columns.tolist()\n    elif isinstance(exog_re, pd.Series) and exog_re.name is not None:\n        return [exog_re.name]\n    elif isinstance(exog_re, list):\n        return exog_re\n    defnames = ['x_re{0:1d}'.format(k + 1) for k in range(exog_re.shape[1])]\n    return defnames"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, k_fe, k_re, k_vc):\n    self.k_fe = k_fe\n    self.k_re = k_re\n    self.k_re2 = k_re * (k_re + 1) // 2\n    self.k_vc = k_vc\n    self.k_tot = self.k_fe + self.k_re2 + self.k_vc\n    self._ix = np.tril_indices(self.k_re)",
        "mutated": [
            "def __init__(self, k_fe, k_re, k_vc):\n    if False:\n        i = 10\n    self.k_fe = k_fe\n    self.k_re = k_re\n    self.k_re2 = k_re * (k_re + 1) // 2\n    self.k_vc = k_vc\n    self.k_tot = self.k_fe + self.k_re2 + self.k_vc\n    self._ix = np.tril_indices(self.k_re)",
            "def __init__(self, k_fe, k_re, k_vc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.k_fe = k_fe\n    self.k_re = k_re\n    self.k_re2 = k_re * (k_re + 1) // 2\n    self.k_vc = k_vc\n    self.k_tot = self.k_fe + self.k_re2 + self.k_vc\n    self._ix = np.tril_indices(self.k_re)",
            "def __init__(self, k_fe, k_re, k_vc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.k_fe = k_fe\n    self.k_re = k_re\n    self.k_re2 = k_re * (k_re + 1) // 2\n    self.k_vc = k_vc\n    self.k_tot = self.k_fe + self.k_re2 + self.k_vc\n    self._ix = np.tril_indices(self.k_re)",
            "def __init__(self, k_fe, k_re, k_vc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.k_fe = k_fe\n    self.k_re = k_re\n    self.k_re2 = k_re * (k_re + 1) // 2\n    self.k_vc = k_vc\n    self.k_tot = self.k_fe + self.k_re2 + self.k_vc\n    self._ix = np.tril_indices(self.k_re)",
            "def __init__(self, k_fe, k_re, k_vc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.k_fe = k_fe\n    self.k_re = k_re\n    self.k_re2 = k_re * (k_re + 1) // 2\n    self.k_vc = k_vc\n    self.k_tot = self.k_fe + self.k_re2 + self.k_vc\n    self._ix = np.tril_indices(self.k_re)"
        ]
    },
    {
        "func_name": "from_packed",
        "original": "def from_packed(params, k_fe, k_re, use_sqrt, has_fe):\n    \"\"\"\n        Create a MixedLMParams object from packed parameter vector.\n\n        Parameters\n        ----------\n        params : array_like\n            The mode parameters packed into a single vector.\n        k_fe : int\n            The number of covariates with fixed effects\n        k_re : int\n            The number of covariates with random effects (excluding\n            variance components).\n        use_sqrt : bool\n            If True, the random effects covariance matrix is provided\n            as its Cholesky factor, otherwise the lower triangle of\n            the covariance matrix is stored.\n        has_fe : bool\n            If True, `params` contains fixed effects parameters.\n            Otherwise, the fixed effects parameters are set to zero.\n\n        Returns\n        -------\n        A MixedLMParams object.\n        \"\"\"\n    k_re2 = int(k_re * (k_re + 1) / 2)\n    if has_fe:\n        k_vc = len(params) - k_fe - k_re2\n    else:\n        k_vc = len(params) - k_re2\n    pa = MixedLMParams(k_fe, k_re, k_vc)\n    cov_re = np.zeros((k_re, k_re))\n    ix = pa._ix\n    if has_fe:\n        pa.fe_params = params[0:k_fe]\n        cov_re[ix] = params[k_fe:k_fe + k_re2]\n    else:\n        pa.fe_params = np.zeros(k_fe)\n        cov_re[ix] = params[0:k_re2]\n    if use_sqrt:\n        cov_re = np.dot(cov_re, cov_re.T)\n    else:\n        cov_re = cov_re + cov_re.T - np.diag(np.diag(cov_re))\n    pa.cov_re = cov_re\n    if k_vc > 0:\n        if use_sqrt:\n            pa.vcomp = params[-k_vc:] ** 2\n        else:\n            pa.vcomp = params[-k_vc:]\n    else:\n        pa.vcomp = np.array([])\n    return pa",
        "mutated": [
            "def from_packed(params, k_fe, k_re, use_sqrt, has_fe):\n    if False:\n        i = 10\n    '\\n        Create a MixedLMParams object from packed parameter vector.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            The mode parameters packed into a single vector.\\n        k_fe : int\\n            The number of covariates with fixed effects\\n        k_re : int\\n            The number of covariates with random effects (excluding\\n            variance components).\\n        use_sqrt : bool\\n            If True, the random effects covariance matrix is provided\\n            as its Cholesky factor, otherwise the lower triangle of\\n            the covariance matrix is stored.\\n        has_fe : bool\\n            If True, `params` contains fixed effects parameters.\\n            Otherwise, the fixed effects parameters are set to zero.\\n\\n        Returns\\n        -------\\n        A MixedLMParams object.\\n        '\n    k_re2 = int(k_re * (k_re + 1) / 2)\n    if has_fe:\n        k_vc = len(params) - k_fe - k_re2\n    else:\n        k_vc = len(params) - k_re2\n    pa = MixedLMParams(k_fe, k_re, k_vc)\n    cov_re = np.zeros((k_re, k_re))\n    ix = pa._ix\n    if has_fe:\n        pa.fe_params = params[0:k_fe]\n        cov_re[ix] = params[k_fe:k_fe + k_re2]\n    else:\n        pa.fe_params = np.zeros(k_fe)\n        cov_re[ix] = params[0:k_re2]\n    if use_sqrt:\n        cov_re = np.dot(cov_re, cov_re.T)\n    else:\n        cov_re = cov_re + cov_re.T - np.diag(np.diag(cov_re))\n    pa.cov_re = cov_re\n    if k_vc > 0:\n        if use_sqrt:\n            pa.vcomp = params[-k_vc:] ** 2\n        else:\n            pa.vcomp = params[-k_vc:]\n    else:\n        pa.vcomp = np.array([])\n    return pa",
            "def from_packed(params, k_fe, k_re, use_sqrt, has_fe):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create a MixedLMParams object from packed parameter vector.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            The mode parameters packed into a single vector.\\n        k_fe : int\\n            The number of covariates with fixed effects\\n        k_re : int\\n            The number of covariates with random effects (excluding\\n            variance components).\\n        use_sqrt : bool\\n            If True, the random effects covariance matrix is provided\\n            as its Cholesky factor, otherwise the lower triangle of\\n            the covariance matrix is stored.\\n        has_fe : bool\\n            If True, `params` contains fixed effects parameters.\\n            Otherwise, the fixed effects parameters are set to zero.\\n\\n        Returns\\n        -------\\n        A MixedLMParams object.\\n        '\n    k_re2 = int(k_re * (k_re + 1) / 2)\n    if has_fe:\n        k_vc = len(params) - k_fe - k_re2\n    else:\n        k_vc = len(params) - k_re2\n    pa = MixedLMParams(k_fe, k_re, k_vc)\n    cov_re = np.zeros((k_re, k_re))\n    ix = pa._ix\n    if has_fe:\n        pa.fe_params = params[0:k_fe]\n        cov_re[ix] = params[k_fe:k_fe + k_re2]\n    else:\n        pa.fe_params = np.zeros(k_fe)\n        cov_re[ix] = params[0:k_re2]\n    if use_sqrt:\n        cov_re = np.dot(cov_re, cov_re.T)\n    else:\n        cov_re = cov_re + cov_re.T - np.diag(np.diag(cov_re))\n    pa.cov_re = cov_re\n    if k_vc > 0:\n        if use_sqrt:\n            pa.vcomp = params[-k_vc:] ** 2\n        else:\n            pa.vcomp = params[-k_vc:]\n    else:\n        pa.vcomp = np.array([])\n    return pa",
            "def from_packed(params, k_fe, k_re, use_sqrt, has_fe):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create a MixedLMParams object from packed parameter vector.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            The mode parameters packed into a single vector.\\n        k_fe : int\\n            The number of covariates with fixed effects\\n        k_re : int\\n            The number of covariates with random effects (excluding\\n            variance components).\\n        use_sqrt : bool\\n            If True, the random effects covariance matrix is provided\\n            as its Cholesky factor, otherwise the lower triangle of\\n            the covariance matrix is stored.\\n        has_fe : bool\\n            If True, `params` contains fixed effects parameters.\\n            Otherwise, the fixed effects parameters are set to zero.\\n\\n        Returns\\n        -------\\n        A MixedLMParams object.\\n        '\n    k_re2 = int(k_re * (k_re + 1) / 2)\n    if has_fe:\n        k_vc = len(params) - k_fe - k_re2\n    else:\n        k_vc = len(params) - k_re2\n    pa = MixedLMParams(k_fe, k_re, k_vc)\n    cov_re = np.zeros((k_re, k_re))\n    ix = pa._ix\n    if has_fe:\n        pa.fe_params = params[0:k_fe]\n        cov_re[ix] = params[k_fe:k_fe + k_re2]\n    else:\n        pa.fe_params = np.zeros(k_fe)\n        cov_re[ix] = params[0:k_re2]\n    if use_sqrt:\n        cov_re = np.dot(cov_re, cov_re.T)\n    else:\n        cov_re = cov_re + cov_re.T - np.diag(np.diag(cov_re))\n    pa.cov_re = cov_re\n    if k_vc > 0:\n        if use_sqrt:\n            pa.vcomp = params[-k_vc:] ** 2\n        else:\n            pa.vcomp = params[-k_vc:]\n    else:\n        pa.vcomp = np.array([])\n    return pa",
            "def from_packed(params, k_fe, k_re, use_sqrt, has_fe):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create a MixedLMParams object from packed parameter vector.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            The mode parameters packed into a single vector.\\n        k_fe : int\\n            The number of covariates with fixed effects\\n        k_re : int\\n            The number of covariates with random effects (excluding\\n            variance components).\\n        use_sqrt : bool\\n            If True, the random effects covariance matrix is provided\\n            as its Cholesky factor, otherwise the lower triangle of\\n            the covariance matrix is stored.\\n        has_fe : bool\\n            If True, `params` contains fixed effects parameters.\\n            Otherwise, the fixed effects parameters are set to zero.\\n\\n        Returns\\n        -------\\n        A MixedLMParams object.\\n        '\n    k_re2 = int(k_re * (k_re + 1) / 2)\n    if has_fe:\n        k_vc = len(params) - k_fe - k_re2\n    else:\n        k_vc = len(params) - k_re2\n    pa = MixedLMParams(k_fe, k_re, k_vc)\n    cov_re = np.zeros((k_re, k_re))\n    ix = pa._ix\n    if has_fe:\n        pa.fe_params = params[0:k_fe]\n        cov_re[ix] = params[k_fe:k_fe + k_re2]\n    else:\n        pa.fe_params = np.zeros(k_fe)\n        cov_re[ix] = params[0:k_re2]\n    if use_sqrt:\n        cov_re = np.dot(cov_re, cov_re.T)\n    else:\n        cov_re = cov_re + cov_re.T - np.diag(np.diag(cov_re))\n    pa.cov_re = cov_re\n    if k_vc > 0:\n        if use_sqrt:\n            pa.vcomp = params[-k_vc:] ** 2\n        else:\n            pa.vcomp = params[-k_vc:]\n    else:\n        pa.vcomp = np.array([])\n    return pa",
            "def from_packed(params, k_fe, k_re, use_sqrt, has_fe):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create a MixedLMParams object from packed parameter vector.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            The mode parameters packed into a single vector.\\n        k_fe : int\\n            The number of covariates with fixed effects\\n        k_re : int\\n            The number of covariates with random effects (excluding\\n            variance components).\\n        use_sqrt : bool\\n            If True, the random effects covariance matrix is provided\\n            as its Cholesky factor, otherwise the lower triangle of\\n            the covariance matrix is stored.\\n        has_fe : bool\\n            If True, `params` contains fixed effects parameters.\\n            Otherwise, the fixed effects parameters are set to zero.\\n\\n        Returns\\n        -------\\n        A MixedLMParams object.\\n        '\n    k_re2 = int(k_re * (k_re + 1) / 2)\n    if has_fe:\n        k_vc = len(params) - k_fe - k_re2\n    else:\n        k_vc = len(params) - k_re2\n    pa = MixedLMParams(k_fe, k_re, k_vc)\n    cov_re = np.zeros((k_re, k_re))\n    ix = pa._ix\n    if has_fe:\n        pa.fe_params = params[0:k_fe]\n        cov_re[ix] = params[k_fe:k_fe + k_re2]\n    else:\n        pa.fe_params = np.zeros(k_fe)\n        cov_re[ix] = params[0:k_re2]\n    if use_sqrt:\n        cov_re = np.dot(cov_re, cov_re.T)\n    else:\n        cov_re = cov_re + cov_re.T - np.diag(np.diag(cov_re))\n    pa.cov_re = cov_re\n    if k_vc > 0:\n        if use_sqrt:\n            pa.vcomp = params[-k_vc:] ** 2\n        else:\n            pa.vcomp = params[-k_vc:]\n    else:\n        pa.vcomp = np.array([])\n    return pa"
        ]
    },
    {
        "func_name": "from_components",
        "original": "def from_components(fe_params=None, cov_re=None, cov_re_sqrt=None, vcomp=None):\n    \"\"\"\n        Create a MixedLMParams object from each parameter component.\n\n        Parameters\n        ----------\n        fe_params : array_like\n            The fixed effects parameter (a 1-dimensional array).  If\n            None, there are no fixed effects.\n        cov_re : array_like\n            The random effects covariance matrix (a square, symmetric\n            2-dimensional array).\n        cov_re_sqrt : array_like\n            The Cholesky (lower triangular) square root of the random\n            effects covariance matrix.\n        vcomp : array_like\n            The variance component parameters.  If None, there are no\n            variance components.\n\n        Returns\n        -------\n        A MixedLMParams object.\n        \"\"\"\n    if vcomp is None:\n        vcomp = np.empty(0)\n    if fe_params is None:\n        fe_params = np.empty(0)\n    if cov_re is None and cov_re_sqrt is None:\n        cov_re = np.empty((0, 0))\n    k_fe = len(fe_params)\n    k_vc = len(vcomp)\n    k_re = cov_re.shape[0] if cov_re is not None else cov_re_sqrt.shape[0]\n    pa = MixedLMParams(k_fe, k_re, k_vc)\n    pa.fe_params = fe_params\n    if cov_re_sqrt is not None:\n        pa.cov_re = np.dot(cov_re_sqrt, cov_re_sqrt.T)\n    elif cov_re is not None:\n        pa.cov_re = cov_re\n    pa.vcomp = vcomp\n    return pa",
        "mutated": [
            "def from_components(fe_params=None, cov_re=None, cov_re_sqrt=None, vcomp=None):\n    if False:\n        i = 10\n    '\\n        Create a MixedLMParams object from each parameter component.\\n\\n        Parameters\\n        ----------\\n        fe_params : array_like\\n            The fixed effects parameter (a 1-dimensional array).  If\\n            None, there are no fixed effects.\\n        cov_re : array_like\\n            The random effects covariance matrix (a square, symmetric\\n            2-dimensional array).\\n        cov_re_sqrt : array_like\\n            The Cholesky (lower triangular) square root of the random\\n            effects covariance matrix.\\n        vcomp : array_like\\n            The variance component parameters.  If None, there are no\\n            variance components.\\n\\n        Returns\\n        -------\\n        A MixedLMParams object.\\n        '\n    if vcomp is None:\n        vcomp = np.empty(0)\n    if fe_params is None:\n        fe_params = np.empty(0)\n    if cov_re is None and cov_re_sqrt is None:\n        cov_re = np.empty((0, 0))\n    k_fe = len(fe_params)\n    k_vc = len(vcomp)\n    k_re = cov_re.shape[0] if cov_re is not None else cov_re_sqrt.shape[0]\n    pa = MixedLMParams(k_fe, k_re, k_vc)\n    pa.fe_params = fe_params\n    if cov_re_sqrt is not None:\n        pa.cov_re = np.dot(cov_re_sqrt, cov_re_sqrt.T)\n    elif cov_re is not None:\n        pa.cov_re = cov_re\n    pa.vcomp = vcomp\n    return pa",
            "def from_components(fe_params=None, cov_re=None, cov_re_sqrt=None, vcomp=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create a MixedLMParams object from each parameter component.\\n\\n        Parameters\\n        ----------\\n        fe_params : array_like\\n            The fixed effects parameter (a 1-dimensional array).  If\\n            None, there are no fixed effects.\\n        cov_re : array_like\\n            The random effects covariance matrix (a square, symmetric\\n            2-dimensional array).\\n        cov_re_sqrt : array_like\\n            The Cholesky (lower triangular) square root of the random\\n            effects covariance matrix.\\n        vcomp : array_like\\n            The variance component parameters.  If None, there are no\\n            variance components.\\n\\n        Returns\\n        -------\\n        A MixedLMParams object.\\n        '\n    if vcomp is None:\n        vcomp = np.empty(0)\n    if fe_params is None:\n        fe_params = np.empty(0)\n    if cov_re is None and cov_re_sqrt is None:\n        cov_re = np.empty((0, 0))\n    k_fe = len(fe_params)\n    k_vc = len(vcomp)\n    k_re = cov_re.shape[0] if cov_re is not None else cov_re_sqrt.shape[0]\n    pa = MixedLMParams(k_fe, k_re, k_vc)\n    pa.fe_params = fe_params\n    if cov_re_sqrt is not None:\n        pa.cov_re = np.dot(cov_re_sqrt, cov_re_sqrt.T)\n    elif cov_re is not None:\n        pa.cov_re = cov_re\n    pa.vcomp = vcomp\n    return pa",
            "def from_components(fe_params=None, cov_re=None, cov_re_sqrt=None, vcomp=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create a MixedLMParams object from each parameter component.\\n\\n        Parameters\\n        ----------\\n        fe_params : array_like\\n            The fixed effects parameter (a 1-dimensional array).  If\\n            None, there are no fixed effects.\\n        cov_re : array_like\\n            The random effects covariance matrix (a square, symmetric\\n            2-dimensional array).\\n        cov_re_sqrt : array_like\\n            The Cholesky (lower triangular) square root of the random\\n            effects covariance matrix.\\n        vcomp : array_like\\n            The variance component parameters.  If None, there are no\\n            variance components.\\n\\n        Returns\\n        -------\\n        A MixedLMParams object.\\n        '\n    if vcomp is None:\n        vcomp = np.empty(0)\n    if fe_params is None:\n        fe_params = np.empty(0)\n    if cov_re is None and cov_re_sqrt is None:\n        cov_re = np.empty((0, 0))\n    k_fe = len(fe_params)\n    k_vc = len(vcomp)\n    k_re = cov_re.shape[0] if cov_re is not None else cov_re_sqrt.shape[0]\n    pa = MixedLMParams(k_fe, k_re, k_vc)\n    pa.fe_params = fe_params\n    if cov_re_sqrt is not None:\n        pa.cov_re = np.dot(cov_re_sqrt, cov_re_sqrt.T)\n    elif cov_re is not None:\n        pa.cov_re = cov_re\n    pa.vcomp = vcomp\n    return pa",
            "def from_components(fe_params=None, cov_re=None, cov_re_sqrt=None, vcomp=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create a MixedLMParams object from each parameter component.\\n\\n        Parameters\\n        ----------\\n        fe_params : array_like\\n            The fixed effects parameter (a 1-dimensional array).  If\\n            None, there are no fixed effects.\\n        cov_re : array_like\\n            The random effects covariance matrix (a square, symmetric\\n            2-dimensional array).\\n        cov_re_sqrt : array_like\\n            The Cholesky (lower triangular) square root of the random\\n            effects covariance matrix.\\n        vcomp : array_like\\n            The variance component parameters.  If None, there are no\\n            variance components.\\n\\n        Returns\\n        -------\\n        A MixedLMParams object.\\n        '\n    if vcomp is None:\n        vcomp = np.empty(0)\n    if fe_params is None:\n        fe_params = np.empty(0)\n    if cov_re is None and cov_re_sqrt is None:\n        cov_re = np.empty((0, 0))\n    k_fe = len(fe_params)\n    k_vc = len(vcomp)\n    k_re = cov_re.shape[0] if cov_re is not None else cov_re_sqrt.shape[0]\n    pa = MixedLMParams(k_fe, k_re, k_vc)\n    pa.fe_params = fe_params\n    if cov_re_sqrt is not None:\n        pa.cov_re = np.dot(cov_re_sqrt, cov_re_sqrt.T)\n    elif cov_re is not None:\n        pa.cov_re = cov_re\n    pa.vcomp = vcomp\n    return pa",
            "def from_components(fe_params=None, cov_re=None, cov_re_sqrt=None, vcomp=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create a MixedLMParams object from each parameter component.\\n\\n        Parameters\\n        ----------\\n        fe_params : array_like\\n            The fixed effects parameter (a 1-dimensional array).  If\\n            None, there are no fixed effects.\\n        cov_re : array_like\\n            The random effects covariance matrix (a square, symmetric\\n            2-dimensional array).\\n        cov_re_sqrt : array_like\\n            The Cholesky (lower triangular) square root of the random\\n            effects covariance matrix.\\n        vcomp : array_like\\n            The variance component parameters.  If None, there are no\\n            variance components.\\n\\n        Returns\\n        -------\\n        A MixedLMParams object.\\n        '\n    if vcomp is None:\n        vcomp = np.empty(0)\n    if fe_params is None:\n        fe_params = np.empty(0)\n    if cov_re is None and cov_re_sqrt is None:\n        cov_re = np.empty((0, 0))\n    k_fe = len(fe_params)\n    k_vc = len(vcomp)\n    k_re = cov_re.shape[0] if cov_re is not None else cov_re_sqrt.shape[0]\n    pa = MixedLMParams(k_fe, k_re, k_vc)\n    pa.fe_params = fe_params\n    if cov_re_sqrt is not None:\n        pa.cov_re = np.dot(cov_re_sqrt, cov_re_sqrt.T)\n    elif cov_re is not None:\n        pa.cov_re = cov_re\n    pa.vcomp = vcomp\n    return pa"
        ]
    },
    {
        "func_name": "copy",
        "original": "def copy(self):\n    \"\"\"\n        Returns a copy of the object.\n        \"\"\"\n    obj = MixedLMParams(self.k_fe, self.k_re, self.k_vc)\n    obj.fe_params = self.fe_params.copy()\n    obj.cov_re = self.cov_re.copy()\n    obj.vcomp = self.vcomp.copy()\n    return obj",
        "mutated": [
            "def copy(self):\n    if False:\n        i = 10\n    '\\n        Returns a copy of the object.\\n        '\n    obj = MixedLMParams(self.k_fe, self.k_re, self.k_vc)\n    obj.fe_params = self.fe_params.copy()\n    obj.cov_re = self.cov_re.copy()\n    obj.vcomp = self.vcomp.copy()\n    return obj",
            "def copy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns a copy of the object.\\n        '\n    obj = MixedLMParams(self.k_fe, self.k_re, self.k_vc)\n    obj.fe_params = self.fe_params.copy()\n    obj.cov_re = self.cov_re.copy()\n    obj.vcomp = self.vcomp.copy()\n    return obj",
            "def copy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns a copy of the object.\\n        '\n    obj = MixedLMParams(self.k_fe, self.k_re, self.k_vc)\n    obj.fe_params = self.fe_params.copy()\n    obj.cov_re = self.cov_re.copy()\n    obj.vcomp = self.vcomp.copy()\n    return obj",
            "def copy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns a copy of the object.\\n        '\n    obj = MixedLMParams(self.k_fe, self.k_re, self.k_vc)\n    obj.fe_params = self.fe_params.copy()\n    obj.cov_re = self.cov_re.copy()\n    obj.vcomp = self.vcomp.copy()\n    return obj",
            "def copy(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns a copy of the object.\\n        '\n    obj = MixedLMParams(self.k_fe, self.k_re, self.k_vc)\n    obj.fe_params = self.fe_params.copy()\n    obj.cov_re = self.cov_re.copy()\n    obj.vcomp = self.vcomp.copy()\n    return obj"
        ]
    },
    {
        "func_name": "get_packed",
        "original": "def get_packed(self, use_sqrt, has_fe=False):\n    \"\"\"\n        Return the model parameters packed into a single vector.\n\n        Parameters\n        ----------\n        use_sqrt : bool\n            If True, the Cholesky square root of `cov_re` is\n            included in the packed result.  Otherwise the\n            lower triangle of `cov_re` is included.\n        has_fe : bool\n            If True, the fixed effects parameters are included\n            in the packed result, otherwise they are omitted.\n        \"\"\"\n    if self.k_re > 0:\n        if use_sqrt:\n            try:\n                L = np.linalg.cholesky(self.cov_re)\n            except np.linalg.LinAlgError:\n                L = np.diag(np.sqrt(np.diag(self.cov_re)))\n            cpa = L[self._ix]\n        else:\n            cpa = self.cov_re[self._ix]\n    else:\n        cpa = np.zeros(0)\n    if use_sqrt:\n        vcomp = np.sqrt(self.vcomp)\n    else:\n        vcomp = self.vcomp\n    if has_fe:\n        pa = np.concatenate((self.fe_params, cpa, vcomp))\n    else:\n        pa = np.concatenate((cpa, vcomp))\n    return pa",
        "mutated": [
            "def get_packed(self, use_sqrt, has_fe=False):\n    if False:\n        i = 10\n    '\\n        Return the model parameters packed into a single vector.\\n\\n        Parameters\\n        ----------\\n        use_sqrt : bool\\n            If True, the Cholesky square root of `cov_re` is\\n            included in the packed result.  Otherwise the\\n            lower triangle of `cov_re` is included.\\n        has_fe : bool\\n            If True, the fixed effects parameters are included\\n            in the packed result, otherwise they are omitted.\\n        '\n    if self.k_re > 0:\n        if use_sqrt:\n            try:\n                L = np.linalg.cholesky(self.cov_re)\n            except np.linalg.LinAlgError:\n                L = np.diag(np.sqrt(np.diag(self.cov_re)))\n            cpa = L[self._ix]\n        else:\n            cpa = self.cov_re[self._ix]\n    else:\n        cpa = np.zeros(0)\n    if use_sqrt:\n        vcomp = np.sqrt(self.vcomp)\n    else:\n        vcomp = self.vcomp\n    if has_fe:\n        pa = np.concatenate((self.fe_params, cpa, vcomp))\n    else:\n        pa = np.concatenate((cpa, vcomp))\n    return pa",
            "def get_packed(self, use_sqrt, has_fe=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Return the model parameters packed into a single vector.\\n\\n        Parameters\\n        ----------\\n        use_sqrt : bool\\n            If True, the Cholesky square root of `cov_re` is\\n            included in the packed result.  Otherwise the\\n            lower triangle of `cov_re` is included.\\n        has_fe : bool\\n            If True, the fixed effects parameters are included\\n            in the packed result, otherwise they are omitted.\\n        '\n    if self.k_re > 0:\n        if use_sqrt:\n            try:\n                L = np.linalg.cholesky(self.cov_re)\n            except np.linalg.LinAlgError:\n                L = np.diag(np.sqrt(np.diag(self.cov_re)))\n            cpa = L[self._ix]\n        else:\n            cpa = self.cov_re[self._ix]\n    else:\n        cpa = np.zeros(0)\n    if use_sqrt:\n        vcomp = np.sqrt(self.vcomp)\n    else:\n        vcomp = self.vcomp\n    if has_fe:\n        pa = np.concatenate((self.fe_params, cpa, vcomp))\n    else:\n        pa = np.concatenate((cpa, vcomp))\n    return pa",
            "def get_packed(self, use_sqrt, has_fe=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Return the model parameters packed into a single vector.\\n\\n        Parameters\\n        ----------\\n        use_sqrt : bool\\n            If True, the Cholesky square root of `cov_re` is\\n            included in the packed result.  Otherwise the\\n            lower triangle of `cov_re` is included.\\n        has_fe : bool\\n            If True, the fixed effects parameters are included\\n            in the packed result, otherwise they are omitted.\\n        '\n    if self.k_re > 0:\n        if use_sqrt:\n            try:\n                L = np.linalg.cholesky(self.cov_re)\n            except np.linalg.LinAlgError:\n                L = np.diag(np.sqrt(np.diag(self.cov_re)))\n            cpa = L[self._ix]\n        else:\n            cpa = self.cov_re[self._ix]\n    else:\n        cpa = np.zeros(0)\n    if use_sqrt:\n        vcomp = np.sqrt(self.vcomp)\n    else:\n        vcomp = self.vcomp\n    if has_fe:\n        pa = np.concatenate((self.fe_params, cpa, vcomp))\n    else:\n        pa = np.concatenate((cpa, vcomp))\n    return pa",
            "def get_packed(self, use_sqrt, has_fe=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Return the model parameters packed into a single vector.\\n\\n        Parameters\\n        ----------\\n        use_sqrt : bool\\n            If True, the Cholesky square root of `cov_re` is\\n            included in the packed result.  Otherwise the\\n            lower triangle of `cov_re` is included.\\n        has_fe : bool\\n            If True, the fixed effects parameters are included\\n            in the packed result, otherwise they are omitted.\\n        '\n    if self.k_re > 0:\n        if use_sqrt:\n            try:\n                L = np.linalg.cholesky(self.cov_re)\n            except np.linalg.LinAlgError:\n                L = np.diag(np.sqrt(np.diag(self.cov_re)))\n            cpa = L[self._ix]\n        else:\n            cpa = self.cov_re[self._ix]\n    else:\n        cpa = np.zeros(0)\n    if use_sqrt:\n        vcomp = np.sqrt(self.vcomp)\n    else:\n        vcomp = self.vcomp\n    if has_fe:\n        pa = np.concatenate((self.fe_params, cpa, vcomp))\n    else:\n        pa = np.concatenate((cpa, vcomp))\n    return pa",
            "def get_packed(self, use_sqrt, has_fe=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Return the model parameters packed into a single vector.\\n\\n        Parameters\\n        ----------\\n        use_sqrt : bool\\n            If True, the Cholesky square root of `cov_re` is\\n            included in the packed result.  Otherwise the\\n            lower triangle of `cov_re` is included.\\n        has_fe : bool\\n            If True, the fixed effects parameters are included\\n            in the packed result, otherwise they are omitted.\\n        '\n    if self.k_re > 0:\n        if use_sqrt:\n            try:\n                L = np.linalg.cholesky(self.cov_re)\n            except np.linalg.LinAlgError:\n                L = np.diag(np.sqrt(np.diag(self.cov_re)))\n            cpa = L[self._ix]\n        else:\n            cpa = self.cov_re[self._ix]\n    else:\n        cpa = np.zeros(0)\n    if use_sqrt:\n        vcomp = np.sqrt(self.vcomp)\n    else:\n        vcomp = self.vcomp\n    if has_fe:\n        pa = np.concatenate((self.fe_params, cpa, vcomp))\n    else:\n        pa = np.concatenate((cpa, vcomp))\n    return pa"
        ]
    },
    {
        "func_name": "solver",
        "original": "def solver(rhs):\n    ql = A.T.dot(rhs)\n    ql = sparse.linalg.spsolve(qmat, ql)\n    if ql.ndim < rhs.ndim:\n        ql = ql[:, None]\n    ql = A.dot(ql)\n    return rhs / s - ql / s ** 2",
        "mutated": [
            "def solver(rhs):\n    if False:\n        i = 10\n    ql = A.T.dot(rhs)\n    ql = sparse.linalg.spsolve(qmat, ql)\n    if ql.ndim < rhs.ndim:\n        ql = ql[:, None]\n    ql = A.dot(ql)\n    return rhs / s - ql / s ** 2",
            "def solver(rhs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ql = A.T.dot(rhs)\n    ql = sparse.linalg.spsolve(qmat, ql)\n    if ql.ndim < rhs.ndim:\n        ql = ql[:, None]\n    ql = A.dot(ql)\n    return rhs / s - ql / s ** 2",
            "def solver(rhs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ql = A.T.dot(rhs)\n    ql = sparse.linalg.spsolve(qmat, ql)\n    if ql.ndim < rhs.ndim:\n        ql = ql[:, None]\n    ql = A.dot(ql)\n    return rhs / s - ql / s ** 2",
            "def solver(rhs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ql = A.T.dot(rhs)\n    ql = sparse.linalg.spsolve(qmat, ql)\n    if ql.ndim < rhs.ndim:\n        ql = ql[:, None]\n    ql = A.dot(ql)\n    return rhs / s - ql / s ** 2",
            "def solver(rhs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ql = A.T.dot(rhs)\n    ql = sparse.linalg.spsolve(qmat, ql)\n    if ql.ndim < rhs.ndim:\n        ql = ql[:, None]\n    ql = A.dot(ql)\n    return rhs / s - ql / s ** 2"
        ]
    },
    {
        "func_name": "solver",
        "original": "def solver(rhs):\n    ql = np.dot(qmati, rhs)\n    ql = np.dot(A, ql)\n    return rhs / s - ql / s ** 2",
        "mutated": [
            "def solver(rhs):\n    if False:\n        i = 10\n    ql = np.dot(qmati, rhs)\n    ql = np.dot(A, ql)\n    return rhs / s - ql / s ** 2",
            "def solver(rhs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ql = np.dot(qmati, rhs)\n    ql = np.dot(A, ql)\n    return rhs / s - ql / s ** 2",
            "def solver(rhs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ql = np.dot(qmati, rhs)\n    ql = np.dot(A, ql)\n    return rhs / s - ql / s ** 2",
            "def solver(rhs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ql = np.dot(qmati, rhs)\n    ql = np.dot(A, ql)\n    return rhs / s - ql / s ** 2",
            "def solver(rhs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ql = np.dot(qmati, rhs)\n    ql = np.dot(A, ql)\n    return rhs / s - ql / s ** 2"
        ]
    },
    {
        "func_name": "_smw_solver",
        "original": "def _smw_solver(s, A, AtA, Qi, di):\n    \"\"\"\n    Returns a solver for the linear system:\n\n    .. math::\n\n        (sI + ABA^\\\\prime) y = x\n\n    The returned function f satisfies f(x) = y as defined above.\n\n    B and its inverse matrix are block diagonal.  The upper left block\n    of :math:`B^{-1}` is Qi and its lower right block is diag(di).\n\n    Parameters\n    ----------\n    s : scalar\n        See above for usage\n    A : ndarray\n        p x q matrix, in general q << p, may be sparse.\n    AtA : square ndarray\n        :math:`A^\\\\prime  A`, a q x q matrix.\n    Qi : square symmetric ndarray\n        The matrix `B` is q x q, where q = r + d.  `B` consists of a r\n        x r diagonal block whose inverse is `Qi`, and a d x d diagonal\n        block, whose inverse is diag(di).\n    di : 1d array_like\n        See documentation for Qi.\n\n    Returns\n    -------\n    A function for solving a linear system, as documented above.\n\n    Notes\n    -----\n    Uses Sherman-Morrison-Woodbury identity:\n        https://en.wikipedia.org/wiki/Woodbury_matrix_identity\n    \"\"\"\n    qmat = AtA / s\n    m = Qi.shape[0]\n    qmat[0:m, 0:m] += Qi\n    if sparse.issparse(A):\n        qmat[m:, m:] += sparse.diags(di)\n\n        def solver(rhs):\n            ql = A.T.dot(rhs)\n            ql = sparse.linalg.spsolve(qmat, ql)\n            if ql.ndim < rhs.ndim:\n                ql = ql[:, None]\n            ql = A.dot(ql)\n            return rhs / s - ql / s ** 2\n    else:\n        d = qmat.shape[0]\n        qmat.flat[m * (d + 1)::d + 1] += di\n        qmati = np.linalg.solve(qmat, A.T)\n\n        def solver(rhs):\n            ql = np.dot(qmati, rhs)\n            ql = np.dot(A, ql)\n            return rhs / s - ql / s ** 2\n    return solver",
        "mutated": [
            "def _smw_solver(s, A, AtA, Qi, di):\n    if False:\n        i = 10\n    '\\n    Returns a solver for the linear system:\\n\\n    .. math::\\n\\n        (sI + ABA^\\\\prime) y = x\\n\\n    The returned function f satisfies f(x) = y as defined above.\\n\\n    B and its inverse matrix are block diagonal.  The upper left block\\n    of :math:`B^{-1}` is Qi and its lower right block is diag(di).\\n\\n    Parameters\\n    ----------\\n    s : scalar\\n        See above for usage\\n    A : ndarray\\n        p x q matrix, in general q << p, may be sparse.\\n    AtA : square ndarray\\n        :math:`A^\\\\prime  A`, a q x q matrix.\\n    Qi : square symmetric ndarray\\n        The matrix `B` is q x q, where q = r + d.  `B` consists of a r\\n        x r diagonal block whose inverse is `Qi`, and a d x d diagonal\\n        block, whose inverse is diag(di).\\n    di : 1d array_like\\n        See documentation for Qi.\\n\\n    Returns\\n    -------\\n    A function for solving a linear system, as documented above.\\n\\n    Notes\\n    -----\\n    Uses Sherman-Morrison-Woodbury identity:\\n        https://en.wikipedia.org/wiki/Woodbury_matrix_identity\\n    '\n    qmat = AtA / s\n    m = Qi.shape[0]\n    qmat[0:m, 0:m] += Qi\n    if sparse.issparse(A):\n        qmat[m:, m:] += sparse.diags(di)\n\n        def solver(rhs):\n            ql = A.T.dot(rhs)\n            ql = sparse.linalg.spsolve(qmat, ql)\n            if ql.ndim < rhs.ndim:\n                ql = ql[:, None]\n            ql = A.dot(ql)\n            return rhs / s - ql / s ** 2\n    else:\n        d = qmat.shape[0]\n        qmat.flat[m * (d + 1)::d + 1] += di\n        qmati = np.linalg.solve(qmat, A.T)\n\n        def solver(rhs):\n            ql = np.dot(qmati, rhs)\n            ql = np.dot(A, ql)\n            return rhs / s - ql / s ** 2\n    return solver",
            "def _smw_solver(s, A, AtA, Qi, di):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Returns a solver for the linear system:\\n\\n    .. math::\\n\\n        (sI + ABA^\\\\prime) y = x\\n\\n    The returned function f satisfies f(x) = y as defined above.\\n\\n    B and its inverse matrix are block diagonal.  The upper left block\\n    of :math:`B^{-1}` is Qi and its lower right block is diag(di).\\n\\n    Parameters\\n    ----------\\n    s : scalar\\n        See above for usage\\n    A : ndarray\\n        p x q matrix, in general q << p, may be sparse.\\n    AtA : square ndarray\\n        :math:`A^\\\\prime  A`, a q x q matrix.\\n    Qi : square symmetric ndarray\\n        The matrix `B` is q x q, where q = r + d.  `B` consists of a r\\n        x r diagonal block whose inverse is `Qi`, and a d x d diagonal\\n        block, whose inverse is diag(di).\\n    di : 1d array_like\\n        See documentation for Qi.\\n\\n    Returns\\n    -------\\n    A function for solving a linear system, as documented above.\\n\\n    Notes\\n    -----\\n    Uses Sherman-Morrison-Woodbury identity:\\n        https://en.wikipedia.org/wiki/Woodbury_matrix_identity\\n    '\n    qmat = AtA / s\n    m = Qi.shape[0]\n    qmat[0:m, 0:m] += Qi\n    if sparse.issparse(A):\n        qmat[m:, m:] += sparse.diags(di)\n\n        def solver(rhs):\n            ql = A.T.dot(rhs)\n            ql = sparse.linalg.spsolve(qmat, ql)\n            if ql.ndim < rhs.ndim:\n                ql = ql[:, None]\n            ql = A.dot(ql)\n            return rhs / s - ql / s ** 2\n    else:\n        d = qmat.shape[0]\n        qmat.flat[m * (d + 1)::d + 1] += di\n        qmati = np.linalg.solve(qmat, A.T)\n\n        def solver(rhs):\n            ql = np.dot(qmati, rhs)\n            ql = np.dot(A, ql)\n            return rhs / s - ql / s ** 2\n    return solver",
            "def _smw_solver(s, A, AtA, Qi, di):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Returns a solver for the linear system:\\n\\n    .. math::\\n\\n        (sI + ABA^\\\\prime) y = x\\n\\n    The returned function f satisfies f(x) = y as defined above.\\n\\n    B and its inverse matrix are block diagonal.  The upper left block\\n    of :math:`B^{-1}` is Qi and its lower right block is diag(di).\\n\\n    Parameters\\n    ----------\\n    s : scalar\\n        See above for usage\\n    A : ndarray\\n        p x q matrix, in general q << p, may be sparse.\\n    AtA : square ndarray\\n        :math:`A^\\\\prime  A`, a q x q matrix.\\n    Qi : square symmetric ndarray\\n        The matrix `B` is q x q, where q = r + d.  `B` consists of a r\\n        x r diagonal block whose inverse is `Qi`, and a d x d diagonal\\n        block, whose inverse is diag(di).\\n    di : 1d array_like\\n        See documentation for Qi.\\n\\n    Returns\\n    -------\\n    A function for solving a linear system, as documented above.\\n\\n    Notes\\n    -----\\n    Uses Sherman-Morrison-Woodbury identity:\\n        https://en.wikipedia.org/wiki/Woodbury_matrix_identity\\n    '\n    qmat = AtA / s\n    m = Qi.shape[0]\n    qmat[0:m, 0:m] += Qi\n    if sparse.issparse(A):\n        qmat[m:, m:] += sparse.diags(di)\n\n        def solver(rhs):\n            ql = A.T.dot(rhs)\n            ql = sparse.linalg.spsolve(qmat, ql)\n            if ql.ndim < rhs.ndim:\n                ql = ql[:, None]\n            ql = A.dot(ql)\n            return rhs / s - ql / s ** 2\n    else:\n        d = qmat.shape[0]\n        qmat.flat[m * (d + 1)::d + 1] += di\n        qmati = np.linalg.solve(qmat, A.T)\n\n        def solver(rhs):\n            ql = np.dot(qmati, rhs)\n            ql = np.dot(A, ql)\n            return rhs / s - ql / s ** 2\n    return solver",
            "def _smw_solver(s, A, AtA, Qi, di):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Returns a solver for the linear system:\\n\\n    .. math::\\n\\n        (sI + ABA^\\\\prime) y = x\\n\\n    The returned function f satisfies f(x) = y as defined above.\\n\\n    B and its inverse matrix are block diagonal.  The upper left block\\n    of :math:`B^{-1}` is Qi and its lower right block is diag(di).\\n\\n    Parameters\\n    ----------\\n    s : scalar\\n        See above for usage\\n    A : ndarray\\n        p x q matrix, in general q << p, may be sparse.\\n    AtA : square ndarray\\n        :math:`A^\\\\prime  A`, a q x q matrix.\\n    Qi : square symmetric ndarray\\n        The matrix `B` is q x q, where q = r + d.  `B` consists of a r\\n        x r diagonal block whose inverse is `Qi`, and a d x d diagonal\\n        block, whose inverse is diag(di).\\n    di : 1d array_like\\n        See documentation for Qi.\\n\\n    Returns\\n    -------\\n    A function for solving a linear system, as documented above.\\n\\n    Notes\\n    -----\\n    Uses Sherman-Morrison-Woodbury identity:\\n        https://en.wikipedia.org/wiki/Woodbury_matrix_identity\\n    '\n    qmat = AtA / s\n    m = Qi.shape[0]\n    qmat[0:m, 0:m] += Qi\n    if sparse.issparse(A):\n        qmat[m:, m:] += sparse.diags(di)\n\n        def solver(rhs):\n            ql = A.T.dot(rhs)\n            ql = sparse.linalg.spsolve(qmat, ql)\n            if ql.ndim < rhs.ndim:\n                ql = ql[:, None]\n            ql = A.dot(ql)\n            return rhs / s - ql / s ** 2\n    else:\n        d = qmat.shape[0]\n        qmat.flat[m * (d + 1)::d + 1] += di\n        qmati = np.linalg.solve(qmat, A.T)\n\n        def solver(rhs):\n            ql = np.dot(qmati, rhs)\n            ql = np.dot(A, ql)\n            return rhs / s - ql / s ** 2\n    return solver",
            "def _smw_solver(s, A, AtA, Qi, di):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Returns a solver for the linear system:\\n\\n    .. math::\\n\\n        (sI + ABA^\\\\prime) y = x\\n\\n    The returned function f satisfies f(x) = y as defined above.\\n\\n    B and its inverse matrix are block diagonal.  The upper left block\\n    of :math:`B^{-1}` is Qi and its lower right block is diag(di).\\n\\n    Parameters\\n    ----------\\n    s : scalar\\n        See above for usage\\n    A : ndarray\\n        p x q matrix, in general q << p, may be sparse.\\n    AtA : square ndarray\\n        :math:`A^\\\\prime  A`, a q x q matrix.\\n    Qi : square symmetric ndarray\\n        The matrix `B` is q x q, where q = r + d.  `B` consists of a r\\n        x r diagonal block whose inverse is `Qi`, and a d x d diagonal\\n        block, whose inverse is diag(di).\\n    di : 1d array_like\\n        See documentation for Qi.\\n\\n    Returns\\n    -------\\n    A function for solving a linear system, as documented above.\\n\\n    Notes\\n    -----\\n    Uses Sherman-Morrison-Woodbury identity:\\n        https://en.wikipedia.org/wiki/Woodbury_matrix_identity\\n    '\n    qmat = AtA / s\n    m = Qi.shape[0]\n    qmat[0:m, 0:m] += Qi\n    if sparse.issparse(A):\n        qmat[m:, m:] += sparse.diags(di)\n\n        def solver(rhs):\n            ql = A.T.dot(rhs)\n            ql = sparse.linalg.spsolve(qmat, ql)\n            if ql.ndim < rhs.ndim:\n                ql = ql[:, None]\n            ql = A.dot(ql)\n            return rhs / s - ql / s ** 2\n    else:\n        d = qmat.shape[0]\n        qmat.flat[m * (d + 1)::d + 1] += di\n        qmati = np.linalg.solve(qmat, A.T)\n\n        def solver(rhs):\n            ql = np.dot(qmati, rhs)\n            ql = np.dot(A, ql)\n            return rhs / s - ql / s ** 2\n    return solver"
        ]
    },
    {
        "func_name": "_smw_logdet",
        "original": "def _smw_logdet(s, A, AtA, Qi, di, B_logdet):\n    \"\"\"\n    Returns the log determinant of\n\n    .. math::\n\n        sI + ABA^\\\\prime\n\n    Uses the matrix determinant lemma to accelerate the calculation.\n    B is assumed to be positive definite, and s > 0, therefore the\n    determinant is positive.\n\n    Parameters\n    ----------\n    s : positive scalar\n        See above for usage\n    A : ndarray\n        p x q matrix, in general q << p.\n    AtA : square ndarray\n        :math:`A^\\\\prime  A`, a q x q matrix.\n    Qi : square symmetric ndarray\n        The matrix `B` is q x q, where q = r + d.  `B` consists of a r\n        x r diagonal block whose inverse is `Qi`, and a d x d diagonal\n        block, whose inverse is diag(di).\n    di : 1d array_like\n        See documentation for Qi.\n    B_logdet : real\n        The log determinant of B\n\n    Returns\n    -------\n    The log determinant of s*I + A*B*A'.\n\n    Notes\n    -----\n    Uses the matrix determinant lemma:\n        https://en.wikipedia.org/wiki/Matrix_determinant_lemma\n    \"\"\"\n    p = A.shape[0]\n    ld = p * np.log(s)\n    qmat = AtA / s\n    m = Qi.shape[0]\n    qmat[0:m, 0:m] += Qi\n    if sparse.issparse(qmat):\n        qmat[m:, m:] += sparse.diags(di)\n        lu = sparse.linalg.splu(qmat)\n        dl = lu.L.diagonal().astype(np.complex128)\n        du = lu.U.diagonal().astype(np.complex128)\n        ld1 = np.log(dl).sum() + np.log(du).sum()\n        ld1 = ld1.real\n    else:\n        d = qmat.shape[0]\n        qmat.flat[m * (d + 1)::d + 1] += di\n        (_, ld1) = np.linalg.slogdet(qmat)\n    return B_logdet + ld + ld1",
        "mutated": [
            "def _smw_logdet(s, A, AtA, Qi, di, B_logdet):\n    if False:\n        i = 10\n    \"\\n    Returns the log determinant of\\n\\n    .. math::\\n\\n        sI + ABA^\\\\prime\\n\\n    Uses the matrix determinant lemma to accelerate the calculation.\\n    B is assumed to be positive definite, and s > 0, therefore the\\n    determinant is positive.\\n\\n    Parameters\\n    ----------\\n    s : positive scalar\\n        See above for usage\\n    A : ndarray\\n        p x q matrix, in general q << p.\\n    AtA : square ndarray\\n        :math:`A^\\\\prime  A`, a q x q matrix.\\n    Qi : square symmetric ndarray\\n        The matrix `B` is q x q, where q = r + d.  `B` consists of a r\\n        x r diagonal block whose inverse is `Qi`, and a d x d diagonal\\n        block, whose inverse is diag(di).\\n    di : 1d array_like\\n        See documentation for Qi.\\n    B_logdet : real\\n        The log determinant of B\\n\\n    Returns\\n    -------\\n    The log determinant of s*I + A*B*A'.\\n\\n    Notes\\n    -----\\n    Uses the matrix determinant lemma:\\n        https://en.wikipedia.org/wiki/Matrix_determinant_lemma\\n    \"\n    p = A.shape[0]\n    ld = p * np.log(s)\n    qmat = AtA / s\n    m = Qi.shape[0]\n    qmat[0:m, 0:m] += Qi\n    if sparse.issparse(qmat):\n        qmat[m:, m:] += sparse.diags(di)\n        lu = sparse.linalg.splu(qmat)\n        dl = lu.L.diagonal().astype(np.complex128)\n        du = lu.U.diagonal().astype(np.complex128)\n        ld1 = np.log(dl).sum() + np.log(du).sum()\n        ld1 = ld1.real\n    else:\n        d = qmat.shape[0]\n        qmat.flat[m * (d + 1)::d + 1] += di\n        (_, ld1) = np.linalg.slogdet(qmat)\n    return B_logdet + ld + ld1",
            "def _smw_logdet(s, A, AtA, Qi, di, B_logdet):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Returns the log determinant of\\n\\n    .. math::\\n\\n        sI + ABA^\\\\prime\\n\\n    Uses the matrix determinant lemma to accelerate the calculation.\\n    B is assumed to be positive definite, and s > 0, therefore the\\n    determinant is positive.\\n\\n    Parameters\\n    ----------\\n    s : positive scalar\\n        See above for usage\\n    A : ndarray\\n        p x q matrix, in general q << p.\\n    AtA : square ndarray\\n        :math:`A^\\\\prime  A`, a q x q matrix.\\n    Qi : square symmetric ndarray\\n        The matrix `B` is q x q, where q = r + d.  `B` consists of a r\\n        x r diagonal block whose inverse is `Qi`, and a d x d diagonal\\n        block, whose inverse is diag(di).\\n    di : 1d array_like\\n        See documentation for Qi.\\n    B_logdet : real\\n        The log determinant of B\\n\\n    Returns\\n    -------\\n    The log determinant of s*I + A*B*A'.\\n\\n    Notes\\n    -----\\n    Uses the matrix determinant lemma:\\n        https://en.wikipedia.org/wiki/Matrix_determinant_lemma\\n    \"\n    p = A.shape[0]\n    ld = p * np.log(s)\n    qmat = AtA / s\n    m = Qi.shape[0]\n    qmat[0:m, 0:m] += Qi\n    if sparse.issparse(qmat):\n        qmat[m:, m:] += sparse.diags(di)\n        lu = sparse.linalg.splu(qmat)\n        dl = lu.L.diagonal().astype(np.complex128)\n        du = lu.U.diagonal().astype(np.complex128)\n        ld1 = np.log(dl).sum() + np.log(du).sum()\n        ld1 = ld1.real\n    else:\n        d = qmat.shape[0]\n        qmat.flat[m * (d + 1)::d + 1] += di\n        (_, ld1) = np.linalg.slogdet(qmat)\n    return B_logdet + ld + ld1",
            "def _smw_logdet(s, A, AtA, Qi, di, B_logdet):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Returns the log determinant of\\n\\n    .. math::\\n\\n        sI + ABA^\\\\prime\\n\\n    Uses the matrix determinant lemma to accelerate the calculation.\\n    B is assumed to be positive definite, and s > 0, therefore the\\n    determinant is positive.\\n\\n    Parameters\\n    ----------\\n    s : positive scalar\\n        See above for usage\\n    A : ndarray\\n        p x q matrix, in general q << p.\\n    AtA : square ndarray\\n        :math:`A^\\\\prime  A`, a q x q matrix.\\n    Qi : square symmetric ndarray\\n        The matrix `B` is q x q, where q = r + d.  `B` consists of a r\\n        x r diagonal block whose inverse is `Qi`, and a d x d diagonal\\n        block, whose inverse is diag(di).\\n    di : 1d array_like\\n        See documentation for Qi.\\n    B_logdet : real\\n        The log determinant of B\\n\\n    Returns\\n    -------\\n    The log determinant of s*I + A*B*A'.\\n\\n    Notes\\n    -----\\n    Uses the matrix determinant lemma:\\n        https://en.wikipedia.org/wiki/Matrix_determinant_lemma\\n    \"\n    p = A.shape[0]\n    ld = p * np.log(s)\n    qmat = AtA / s\n    m = Qi.shape[0]\n    qmat[0:m, 0:m] += Qi\n    if sparse.issparse(qmat):\n        qmat[m:, m:] += sparse.diags(di)\n        lu = sparse.linalg.splu(qmat)\n        dl = lu.L.diagonal().astype(np.complex128)\n        du = lu.U.diagonal().astype(np.complex128)\n        ld1 = np.log(dl).sum() + np.log(du).sum()\n        ld1 = ld1.real\n    else:\n        d = qmat.shape[0]\n        qmat.flat[m * (d + 1)::d + 1] += di\n        (_, ld1) = np.linalg.slogdet(qmat)\n    return B_logdet + ld + ld1",
            "def _smw_logdet(s, A, AtA, Qi, di, B_logdet):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Returns the log determinant of\\n\\n    .. math::\\n\\n        sI + ABA^\\\\prime\\n\\n    Uses the matrix determinant lemma to accelerate the calculation.\\n    B is assumed to be positive definite, and s > 0, therefore the\\n    determinant is positive.\\n\\n    Parameters\\n    ----------\\n    s : positive scalar\\n        See above for usage\\n    A : ndarray\\n        p x q matrix, in general q << p.\\n    AtA : square ndarray\\n        :math:`A^\\\\prime  A`, a q x q matrix.\\n    Qi : square symmetric ndarray\\n        The matrix `B` is q x q, where q = r + d.  `B` consists of a r\\n        x r diagonal block whose inverse is `Qi`, and a d x d diagonal\\n        block, whose inverse is diag(di).\\n    di : 1d array_like\\n        See documentation for Qi.\\n    B_logdet : real\\n        The log determinant of B\\n\\n    Returns\\n    -------\\n    The log determinant of s*I + A*B*A'.\\n\\n    Notes\\n    -----\\n    Uses the matrix determinant lemma:\\n        https://en.wikipedia.org/wiki/Matrix_determinant_lemma\\n    \"\n    p = A.shape[0]\n    ld = p * np.log(s)\n    qmat = AtA / s\n    m = Qi.shape[0]\n    qmat[0:m, 0:m] += Qi\n    if sparse.issparse(qmat):\n        qmat[m:, m:] += sparse.diags(di)\n        lu = sparse.linalg.splu(qmat)\n        dl = lu.L.diagonal().astype(np.complex128)\n        du = lu.U.diagonal().astype(np.complex128)\n        ld1 = np.log(dl).sum() + np.log(du).sum()\n        ld1 = ld1.real\n    else:\n        d = qmat.shape[0]\n        qmat.flat[m * (d + 1)::d + 1] += di\n        (_, ld1) = np.linalg.slogdet(qmat)\n    return B_logdet + ld + ld1",
            "def _smw_logdet(s, A, AtA, Qi, di, B_logdet):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Returns the log determinant of\\n\\n    .. math::\\n\\n        sI + ABA^\\\\prime\\n\\n    Uses the matrix determinant lemma to accelerate the calculation.\\n    B is assumed to be positive definite, and s > 0, therefore the\\n    determinant is positive.\\n\\n    Parameters\\n    ----------\\n    s : positive scalar\\n        See above for usage\\n    A : ndarray\\n        p x q matrix, in general q << p.\\n    AtA : square ndarray\\n        :math:`A^\\\\prime  A`, a q x q matrix.\\n    Qi : square symmetric ndarray\\n        The matrix `B` is q x q, where q = r + d.  `B` consists of a r\\n        x r diagonal block whose inverse is `Qi`, and a d x d diagonal\\n        block, whose inverse is diag(di).\\n    di : 1d array_like\\n        See documentation for Qi.\\n    B_logdet : real\\n        The log determinant of B\\n\\n    Returns\\n    -------\\n    The log determinant of s*I + A*B*A'.\\n\\n    Notes\\n    -----\\n    Uses the matrix determinant lemma:\\n        https://en.wikipedia.org/wiki/Matrix_determinant_lemma\\n    \"\n    p = A.shape[0]\n    ld = p * np.log(s)\n    qmat = AtA / s\n    m = Qi.shape[0]\n    qmat[0:m, 0:m] += Qi\n    if sparse.issparse(qmat):\n        qmat[m:, m:] += sparse.diags(di)\n        lu = sparse.linalg.splu(qmat)\n        dl = lu.L.diagonal().astype(np.complex128)\n        du = lu.U.diagonal().astype(np.complex128)\n        ld1 = np.log(dl).sum() + np.log(du).sum()\n        ld1 = ld1.real\n    else:\n        d = qmat.shape[0]\n        qmat.flat[m * (d + 1)::d + 1] += di\n        (_, ld1) = np.linalg.slogdet(qmat)\n    return B_logdet + ld + ld1"
        ]
    },
    {
        "func_name": "_convert_vc",
        "original": "def _convert_vc(exog_vc):\n    vc_names = []\n    vc_colnames = []\n    vc_mats = []\n    groups = set()\n    for (k, v) in exog_vc.items():\n        groups |= set(v.keys())\n    groups = list(groups)\n    groups.sort()\n    for (k, v) in exog_vc.items():\n        vc_names.append(k)\n        (colnames, mats) = ([], [])\n        for g in groups:\n            try:\n                colnames.append(v[g].columns)\n            except AttributeError:\n                colnames.append([str(j) for j in range(v[g].shape[1])])\n            mats.append(v[g])\n        vc_colnames.append(colnames)\n        vc_mats.append(mats)\n    ii = np.argsort(vc_names)\n    vc_names = [vc_names[i] for i in ii]\n    vc_colnames = [vc_colnames[i] for i in ii]\n    vc_mats = [vc_mats[i] for i in ii]\n    return VCSpec(vc_names, vc_colnames, vc_mats)",
        "mutated": [
            "def _convert_vc(exog_vc):\n    if False:\n        i = 10\n    vc_names = []\n    vc_colnames = []\n    vc_mats = []\n    groups = set()\n    for (k, v) in exog_vc.items():\n        groups |= set(v.keys())\n    groups = list(groups)\n    groups.sort()\n    for (k, v) in exog_vc.items():\n        vc_names.append(k)\n        (colnames, mats) = ([], [])\n        for g in groups:\n            try:\n                colnames.append(v[g].columns)\n            except AttributeError:\n                colnames.append([str(j) for j in range(v[g].shape[1])])\n            mats.append(v[g])\n        vc_colnames.append(colnames)\n        vc_mats.append(mats)\n    ii = np.argsort(vc_names)\n    vc_names = [vc_names[i] for i in ii]\n    vc_colnames = [vc_colnames[i] for i in ii]\n    vc_mats = [vc_mats[i] for i in ii]\n    return VCSpec(vc_names, vc_colnames, vc_mats)",
            "def _convert_vc(exog_vc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    vc_names = []\n    vc_colnames = []\n    vc_mats = []\n    groups = set()\n    for (k, v) in exog_vc.items():\n        groups |= set(v.keys())\n    groups = list(groups)\n    groups.sort()\n    for (k, v) in exog_vc.items():\n        vc_names.append(k)\n        (colnames, mats) = ([], [])\n        for g in groups:\n            try:\n                colnames.append(v[g].columns)\n            except AttributeError:\n                colnames.append([str(j) for j in range(v[g].shape[1])])\n            mats.append(v[g])\n        vc_colnames.append(colnames)\n        vc_mats.append(mats)\n    ii = np.argsort(vc_names)\n    vc_names = [vc_names[i] for i in ii]\n    vc_colnames = [vc_colnames[i] for i in ii]\n    vc_mats = [vc_mats[i] for i in ii]\n    return VCSpec(vc_names, vc_colnames, vc_mats)",
            "def _convert_vc(exog_vc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    vc_names = []\n    vc_colnames = []\n    vc_mats = []\n    groups = set()\n    for (k, v) in exog_vc.items():\n        groups |= set(v.keys())\n    groups = list(groups)\n    groups.sort()\n    for (k, v) in exog_vc.items():\n        vc_names.append(k)\n        (colnames, mats) = ([], [])\n        for g in groups:\n            try:\n                colnames.append(v[g].columns)\n            except AttributeError:\n                colnames.append([str(j) for j in range(v[g].shape[1])])\n            mats.append(v[g])\n        vc_colnames.append(colnames)\n        vc_mats.append(mats)\n    ii = np.argsort(vc_names)\n    vc_names = [vc_names[i] for i in ii]\n    vc_colnames = [vc_colnames[i] for i in ii]\n    vc_mats = [vc_mats[i] for i in ii]\n    return VCSpec(vc_names, vc_colnames, vc_mats)",
            "def _convert_vc(exog_vc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    vc_names = []\n    vc_colnames = []\n    vc_mats = []\n    groups = set()\n    for (k, v) in exog_vc.items():\n        groups |= set(v.keys())\n    groups = list(groups)\n    groups.sort()\n    for (k, v) in exog_vc.items():\n        vc_names.append(k)\n        (colnames, mats) = ([], [])\n        for g in groups:\n            try:\n                colnames.append(v[g].columns)\n            except AttributeError:\n                colnames.append([str(j) for j in range(v[g].shape[1])])\n            mats.append(v[g])\n        vc_colnames.append(colnames)\n        vc_mats.append(mats)\n    ii = np.argsort(vc_names)\n    vc_names = [vc_names[i] for i in ii]\n    vc_colnames = [vc_colnames[i] for i in ii]\n    vc_mats = [vc_mats[i] for i in ii]\n    return VCSpec(vc_names, vc_colnames, vc_mats)",
            "def _convert_vc(exog_vc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    vc_names = []\n    vc_colnames = []\n    vc_mats = []\n    groups = set()\n    for (k, v) in exog_vc.items():\n        groups |= set(v.keys())\n    groups = list(groups)\n    groups.sort()\n    for (k, v) in exog_vc.items():\n        vc_names.append(k)\n        (colnames, mats) = ([], [])\n        for g in groups:\n            try:\n                colnames.append(v[g].columns)\n            except AttributeError:\n                colnames.append([str(j) for j in range(v[g].shape[1])])\n            mats.append(v[g])\n        vc_colnames.append(colnames)\n        vc_mats.append(mats)\n    ii = np.argsort(vc_names)\n    vc_names = [vc_names[i] for i in ii]\n    vc_colnames = [vc_colnames[i] for i in ii]\n    vc_mats = [vc_mats[i] for i in ii]\n    return VCSpec(vc_names, vc_colnames, vc_mats)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, endog, exog, groups, exog_re=None, exog_vc=None, use_sqrt=True, missing='none', **kwargs):\n    _allowed_kwargs = ['missing_idx', 'design_info', 'formula']\n    for x in kwargs.keys():\n        if x not in _allowed_kwargs:\n            raise ValueError('argument %s not permitted for MixedLM initialization' % x)\n    self.use_sqrt = use_sqrt\n    self.reml = True\n    self.fe_pen = None\n    self.re_pen = None\n    if isinstance(exog_vc, dict):\n        warnings.warn('Using deprecated variance components format')\n        exog_vc = _convert_vc(exog_vc)\n    if exog_vc is not None:\n        self.k_vc = len(exog_vc.names)\n        self.exog_vc = exog_vc\n    else:\n        self.k_vc = 0\n        self.exog_vc = VCSpec([], [], [])\n    if exog is not None and data_tools._is_using_ndarray_type(exog, None) and (exog.ndim == 1):\n        exog = exog[:, None]\n    if exog_re is not None and data_tools._is_using_ndarray_type(exog_re, None) and (exog_re.ndim == 1):\n        exog_re = exog_re[:, None]\n    super(MixedLM, self).__init__(endog, exog, groups=groups, exog_re=exog_re, missing=missing, **kwargs)\n    self._init_keys.extend(['use_sqrt', 'exog_vc'])\n    self.k_fe = exog.shape[1]\n    if exog_re is None and len(self.exog_vc.names) == 0:\n        self.k_re = 1\n        self.k_re2 = 1\n        self.exog_re = np.ones((len(endog), 1), dtype=np.float64)\n        self.data.exog_re = self.exog_re\n        names = ['Group Var']\n        self.data.param_names = self.exog_names + names\n        self.data.exog_re_names = names\n        self.data.exog_re_names_full = names\n    elif exog_re is not None:\n        self.data.exog_re = exog_re\n        self.exog_re = np.asarray(exog_re)\n        if self.exog_re.ndim == 1:\n            self.exog_re = self.exog_re[:, None]\n        self.k_re = self.exog_re.shape[1]\n        self.k_re2 = self.k_re * (self.k_re + 1) // 2\n    else:\n        self.k_re = 0\n        self.k_re2 = 0\n    if not self.data._param_names:\n        (param_names, exog_re_names, exog_re_names_full) = self._make_param_names(exog_re)\n        self.data.param_names = param_names\n        self.data.exog_re_names = exog_re_names\n        self.data.exog_re_names_full = exog_re_names_full\n    self.k_params = self.k_fe + self.k_re2\n    group_labels = list(set(groups))\n    group_labels.sort()\n    row_indices = dict(((s, []) for s in group_labels))\n    for (i, g) in enumerate(groups):\n        row_indices[g].append(i)\n    self.row_indices = row_indices\n    self.group_labels = group_labels\n    self.n_groups = len(self.group_labels)\n    self.endog_li = self.group_list(self.endog)\n    self.exog_li = self.group_list(self.exog)\n    self.exog_re_li = self.group_list(self.exog_re)\n    if self.exog_re is None:\n        self.exog_re2_li = None\n    else:\n        self.exog_re2_li = [np.dot(x.T, x) for x in self.exog_re_li]\n    self.nobs = len(self.endog)\n    self.n_totobs = self.nobs\n    if self.exog_names is None:\n        self.exog_names = ['FE%d' % (k + 1) for k in range(self.exog.shape[1])]\n    self._aex_r = []\n    self._aex_r2 = []\n    for i in range(self.n_groups):\n        a = self._augment_exog(i)\n        self._aex_r.append(a)\n        ma = _dot(a.T, a)\n        self._aex_r2.append(ma)\n    (self._lin, self._quad) = self._reparam()",
        "mutated": [
            "def __init__(self, endog, exog, groups, exog_re=None, exog_vc=None, use_sqrt=True, missing='none', **kwargs):\n    if False:\n        i = 10\n    _allowed_kwargs = ['missing_idx', 'design_info', 'formula']\n    for x in kwargs.keys():\n        if x not in _allowed_kwargs:\n            raise ValueError('argument %s not permitted for MixedLM initialization' % x)\n    self.use_sqrt = use_sqrt\n    self.reml = True\n    self.fe_pen = None\n    self.re_pen = None\n    if isinstance(exog_vc, dict):\n        warnings.warn('Using deprecated variance components format')\n        exog_vc = _convert_vc(exog_vc)\n    if exog_vc is not None:\n        self.k_vc = len(exog_vc.names)\n        self.exog_vc = exog_vc\n    else:\n        self.k_vc = 0\n        self.exog_vc = VCSpec([], [], [])\n    if exog is not None and data_tools._is_using_ndarray_type(exog, None) and (exog.ndim == 1):\n        exog = exog[:, None]\n    if exog_re is not None and data_tools._is_using_ndarray_type(exog_re, None) and (exog_re.ndim == 1):\n        exog_re = exog_re[:, None]\n    super(MixedLM, self).__init__(endog, exog, groups=groups, exog_re=exog_re, missing=missing, **kwargs)\n    self._init_keys.extend(['use_sqrt', 'exog_vc'])\n    self.k_fe = exog.shape[1]\n    if exog_re is None and len(self.exog_vc.names) == 0:\n        self.k_re = 1\n        self.k_re2 = 1\n        self.exog_re = np.ones((len(endog), 1), dtype=np.float64)\n        self.data.exog_re = self.exog_re\n        names = ['Group Var']\n        self.data.param_names = self.exog_names + names\n        self.data.exog_re_names = names\n        self.data.exog_re_names_full = names\n    elif exog_re is not None:\n        self.data.exog_re = exog_re\n        self.exog_re = np.asarray(exog_re)\n        if self.exog_re.ndim == 1:\n            self.exog_re = self.exog_re[:, None]\n        self.k_re = self.exog_re.shape[1]\n        self.k_re2 = self.k_re * (self.k_re + 1) // 2\n    else:\n        self.k_re = 0\n        self.k_re2 = 0\n    if not self.data._param_names:\n        (param_names, exog_re_names, exog_re_names_full) = self._make_param_names(exog_re)\n        self.data.param_names = param_names\n        self.data.exog_re_names = exog_re_names\n        self.data.exog_re_names_full = exog_re_names_full\n    self.k_params = self.k_fe + self.k_re2\n    group_labels = list(set(groups))\n    group_labels.sort()\n    row_indices = dict(((s, []) for s in group_labels))\n    for (i, g) in enumerate(groups):\n        row_indices[g].append(i)\n    self.row_indices = row_indices\n    self.group_labels = group_labels\n    self.n_groups = len(self.group_labels)\n    self.endog_li = self.group_list(self.endog)\n    self.exog_li = self.group_list(self.exog)\n    self.exog_re_li = self.group_list(self.exog_re)\n    if self.exog_re is None:\n        self.exog_re2_li = None\n    else:\n        self.exog_re2_li = [np.dot(x.T, x) for x in self.exog_re_li]\n    self.nobs = len(self.endog)\n    self.n_totobs = self.nobs\n    if self.exog_names is None:\n        self.exog_names = ['FE%d' % (k + 1) for k in range(self.exog.shape[1])]\n    self._aex_r = []\n    self._aex_r2 = []\n    for i in range(self.n_groups):\n        a = self._augment_exog(i)\n        self._aex_r.append(a)\n        ma = _dot(a.T, a)\n        self._aex_r2.append(ma)\n    (self._lin, self._quad) = self._reparam()",
            "def __init__(self, endog, exog, groups, exog_re=None, exog_vc=None, use_sqrt=True, missing='none', **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    _allowed_kwargs = ['missing_idx', 'design_info', 'formula']\n    for x in kwargs.keys():\n        if x not in _allowed_kwargs:\n            raise ValueError('argument %s not permitted for MixedLM initialization' % x)\n    self.use_sqrt = use_sqrt\n    self.reml = True\n    self.fe_pen = None\n    self.re_pen = None\n    if isinstance(exog_vc, dict):\n        warnings.warn('Using deprecated variance components format')\n        exog_vc = _convert_vc(exog_vc)\n    if exog_vc is not None:\n        self.k_vc = len(exog_vc.names)\n        self.exog_vc = exog_vc\n    else:\n        self.k_vc = 0\n        self.exog_vc = VCSpec([], [], [])\n    if exog is not None and data_tools._is_using_ndarray_type(exog, None) and (exog.ndim == 1):\n        exog = exog[:, None]\n    if exog_re is not None and data_tools._is_using_ndarray_type(exog_re, None) and (exog_re.ndim == 1):\n        exog_re = exog_re[:, None]\n    super(MixedLM, self).__init__(endog, exog, groups=groups, exog_re=exog_re, missing=missing, **kwargs)\n    self._init_keys.extend(['use_sqrt', 'exog_vc'])\n    self.k_fe = exog.shape[1]\n    if exog_re is None and len(self.exog_vc.names) == 0:\n        self.k_re = 1\n        self.k_re2 = 1\n        self.exog_re = np.ones((len(endog), 1), dtype=np.float64)\n        self.data.exog_re = self.exog_re\n        names = ['Group Var']\n        self.data.param_names = self.exog_names + names\n        self.data.exog_re_names = names\n        self.data.exog_re_names_full = names\n    elif exog_re is not None:\n        self.data.exog_re = exog_re\n        self.exog_re = np.asarray(exog_re)\n        if self.exog_re.ndim == 1:\n            self.exog_re = self.exog_re[:, None]\n        self.k_re = self.exog_re.shape[1]\n        self.k_re2 = self.k_re * (self.k_re + 1) // 2\n    else:\n        self.k_re = 0\n        self.k_re2 = 0\n    if not self.data._param_names:\n        (param_names, exog_re_names, exog_re_names_full) = self._make_param_names(exog_re)\n        self.data.param_names = param_names\n        self.data.exog_re_names = exog_re_names\n        self.data.exog_re_names_full = exog_re_names_full\n    self.k_params = self.k_fe + self.k_re2\n    group_labels = list(set(groups))\n    group_labels.sort()\n    row_indices = dict(((s, []) for s in group_labels))\n    for (i, g) in enumerate(groups):\n        row_indices[g].append(i)\n    self.row_indices = row_indices\n    self.group_labels = group_labels\n    self.n_groups = len(self.group_labels)\n    self.endog_li = self.group_list(self.endog)\n    self.exog_li = self.group_list(self.exog)\n    self.exog_re_li = self.group_list(self.exog_re)\n    if self.exog_re is None:\n        self.exog_re2_li = None\n    else:\n        self.exog_re2_li = [np.dot(x.T, x) for x in self.exog_re_li]\n    self.nobs = len(self.endog)\n    self.n_totobs = self.nobs\n    if self.exog_names is None:\n        self.exog_names = ['FE%d' % (k + 1) for k in range(self.exog.shape[1])]\n    self._aex_r = []\n    self._aex_r2 = []\n    for i in range(self.n_groups):\n        a = self._augment_exog(i)\n        self._aex_r.append(a)\n        ma = _dot(a.T, a)\n        self._aex_r2.append(ma)\n    (self._lin, self._quad) = self._reparam()",
            "def __init__(self, endog, exog, groups, exog_re=None, exog_vc=None, use_sqrt=True, missing='none', **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    _allowed_kwargs = ['missing_idx', 'design_info', 'formula']\n    for x in kwargs.keys():\n        if x not in _allowed_kwargs:\n            raise ValueError('argument %s not permitted for MixedLM initialization' % x)\n    self.use_sqrt = use_sqrt\n    self.reml = True\n    self.fe_pen = None\n    self.re_pen = None\n    if isinstance(exog_vc, dict):\n        warnings.warn('Using deprecated variance components format')\n        exog_vc = _convert_vc(exog_vc)\n    if exog_vc is not None:\n        self.k_vc = len(exog_vc.names)\n        self.exog_vc = exog_vc\n    else:\n        self.k_vc = 0\n        self.exog_vc = VCSpec([], [], [])\n    if exog is not None and data_tools._is_using_ndarray_type(exog, None) and (exog.ndim == 1):\n        exog = exog[:, None]\n    if exog_re is not None and data_tools._is_using_ndarray_type(exog_re, None) and (exog_re.ndim == 1):\n        exog_re = exog_re[:, None]\n    super(MixedLM, self).__init__(endog, exog, groups=groups, exog_re=exog_re, missing=missing, **kwargs)\n    self._init_keys.extend(['use_sqrt', 'exog_vc'])\n    self.k_fe = exog.shape[1]\n    if exog_re is None and len(self.exog_vc.names) == 0:\n        self.k_re = 1\n        self.k_re2 = 1\n        self.exog_re = np.ones((len(endog), 1), dtype=np.float64)\n        self.data.exog_re = self.exog_re\n        names = ['Group Var']\n        self.data.param_names = self.exog_names + names\n        self.data.exog_re_names = names\n        self.data.exog_re_names_full = names\n    elif exog_re is not None:\n        self.data.exog_re = exog_re\n        self.exog_re = np.asarray(exog_re)\n        if self.exog_re.ndim == 1:\n            self.exog_re = self.exog_re[:, None]\n        self.k_re = self.exog_re.shape[1]\n        self.k_re2 = self.k_re * (self.k_re + 1) // 2\n    else:\n        self.k_re = 0\n        self.k_re2 = 0\n    if not self.data._param_names:\n        (param_names, exog_re_names, exog_re_names_full) = self._make_param_names(exog_re)\n        self.data.param_names = param_names\n        self.data.exog_re_names = exog_re_names\n        self.data.exog_re_names_full = exog_re_names_full\n    self.k_params = self.k_fe + self.k_re2\n    group_labels = list(set(groups))\n    group_labels.sort()\n    row_indices = dict(((s, []) for s in group_labels))\n    for (i, g) in enumerate(groups):\n        row_indices[g].append(i)\n    self.row_indices = row_indices\n    self.group_labels = group_labels\n    self.n_groups = len(self.group_labels)\n    self.endog_li = self.group_list(self.endog)\n    self.exog_li = self.group_list(self.exog)\n    self.exog_re_li = self.group_list(self.exog_re)\n    if self.exog_re is None:\n        self.exog_re2_li = None\n    else:\n        self.exog_re2_li = [np.dot(x.T, x) for x in self.exog_re_li]\n    self.nobs = len(self.endog)\n    self.n_totobs = self.nobs\n    if self.exog_names is None:\n        self.exog_names = ['FE%d' % (k + 1) for k in range(self.exog.shape[1])]\n    self._aex_r = []\n    self._aex_r2 = []\n    for i in range(self.n_groups):\n        a = self._augment_exog(i)\n        self._aex_r.append(a)\n        ma = _dot(a.T, a)\n        self._aex_r2.append(ma)\n    (self._lin, self._quad) = self._reparam()",
            "def __init__(self, endog, exog, groups, exog_re=None, exog_vc=None, use_sqrt=True, missing='none', **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    _allowed_kwargs = ['missing_idx', 'design_info', 'formula']\n    for x in kwargs.keys():\n        if x not in _allowed_kwargs:\n            raise ValueError('argument %s not permitted for MixedLM initialization' % x)\n    self.use_sqrt = use_sqrt\n    self.reml = True\n    self.fe_pen = None\n    self.re_pen = None\n    if isinstance(exog_vc, dict):\n        warnings.warn('Using deprecated variance components format')\n        exog_vc = _convert_vc(exog_vc)\n    if exog_vc is not None:\n        self.k_vc = len(exog_vc.names)\n        self.exog_vc = exog_vc\n    else:\n        self.k_vc = 0\n        self.exog_vc = VCSpec([], [], [])\n    if exog is not None and data_tools._is_using_ndarray_type(exog, None) and (exog.ndim == 1):\n        exog = exog[:, None]\n    if exog_re is not None and data_tools._is_using_ndarray_type(exog_re, None) and (exog_re.ndim == 1):\n        exog_re = exog_re[:, None]\n    super(MixedLM, self).__init__(endog, exog, groups=groups, exog_re=exog_re, missing=missing, **kwargs)\n    self._init_keys.extend(['use_sqrt', 'exog_vc'])\n    self.k_fe = exog.shape[1]\n    if exog_re is None and len(self.exog_vc.names) == 0:\n        self.k_re = 1\n        self.k_re2 = 1\n        self.exog_re = np.ones((len(endog), 1), dtype=np.float64)\n        self.data.exog_re = self.exog_re\n        names = ['Group Var']\n        self.data.param_names = self.exog_names + names\n        self.data.exog_re_names = names\n        self.data.exog_re_names_full = names\n    elif exog_re is not None:\n        self.data.exog_re = exog_re\n        self.exog_re = np.asarray(exog_re)\n        if self.exog_re.ndim == 1:\n            self.exog_re = self.exog_re[:, None]\n        self.k_re = self.exog_re.shape[1]\n        self.k_re2 = self.k_re * (self.k_re + 1) // 2\n    else:\n        self.k_re = 0\n        self.k_re2 = 0\n    if not self.data._param_names:\n        (param_names, exog_re_names, exog_re_names_full) = self._make_param_names(exog_re)\n        self.data.param_names = param_names\n        self.data.exog_re_names = exog_re_names\n        self.data.exog_re_names_full = exog_re_names_full\n    self.k_params = self.k_fe + self.k_re2\n    group_labels = list(set(groups))\n    group_labels.sort()\n    row_indices = dict(((s, []) for s in group_labels))\n    for (i, g) in enumerate(groups):\n        row_indices[g].append(i)\n    self.row_indices = row_indices\n    self.group_labels = group_labels\n    self.n_groups = len(self.group_labels)\n    self.endog_li = self.group_list(self.endog)\n    self.exog_li = self.group_list(self.exog)\n    self.exog_re_li = self.group_list(self.exog_re)\n    if self.exog_re is None:\n        self.exog_re2_li = None\n    else:\n        self.exog_re2_li = [np.dot(x.T, x) for x in self.exog_re_li]\n    self.nobs = len(self.endog)\n    self.n_totobs = self.nobs\n    if self.exog_names is None:\n        self.exog_names = ['FE%d' % (k + 1) for k in range(self.exog.shape[1])]\n    self._aex_r = []\n    self._aex_r2 = []\n    for i in range(self.n_groups):\n        a = self._augment_exog(i)\n        self._aex_r.append(a)\n        ma = _dot(a.T, a)\n        self._aex_r2.append(ma)\n    (self._lin, self._quad) = self._reparam()",
            "def __init__(self, endog, exog, groups, exog_re=None, exog_vc=None, use_sqrt=True, missing='none', **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    _allowed_kwargs = ['missing_idx', 'design_info', 'formula']\n    for x in kwargs.keys():\n        if x not in _allowed_kwargs:\n            raise ValueError('argument %s not permitted for MixedLM initialization' % x)\n    self.use_sqrt = use_sqrt\n    self.reml = True\n    self.fe_pen = None\n    self.re_pen = None\n    if isinstance(exog_vc, dict):\n        warnings.warn('Using deprecated variance components format')\n        exog_vc = _convert_vc(exog_vc)\n    if exog_vc is not None:\n        self.k_vc = len(exog_vc.names)\n        self.exog_vc = exog_vc\n    else:\n        self.k_vc = 0\n        self.exog_vc = VCSpec([], [], [])\n    if exog is not None and data_tools._is_using_ndarray_type(exog, None) and (exog.ndim == 1):\n        exog = exog[:, None]\n    if exog_re is not None and data_tools._is_using_ndarray_type(exog_re, None) and (exog_re.ndim == 1):\n        exog_re = exog_re[:, None]\n    super(MixedLM, self).__init__(endog, exog, groups=groups, exog_re=exog_re, missing=missing, **kwargs)\n    self._init_keys.extend(['use_sqrt', 'exog_vc'])\n    self.k_fe = exog.shape[1]\n    if exog_re is None and len(self.exog_vc.names) == 0:\n        self.k_re = 1\n        self.k_re2 = 1\n        self.exog_re = np.ones((len(endog), 1), dtype=np.float64)\n        self.data.exog_re = self.exog_re\n        names = ['Group Var']\n        self.data.param_names = self.exog_names + names\n        self.data.exog_re_names = names\n        self.data.exog_re_names_full = names\n    elif exog_re is not None:\n        self.data.exog_re = exog_re\n        self.exog_re = np.asarray(exog_re)\n        if self.exog_re.ndim == 1:\n            self.exog_re = self.exog_re[:, None]\n        self.k_re = self.exog_re.shape[1]\n        self.k_re2 = self.k_re * (self.k_re + 1) // 2\n    else:\n        self.k_re = 0\n        self.k_re2 = 0\n    if not self.data._param_names:\n        (param_names, exog_re_names, exog_re_names_full) = self._make_param_names(exog_re)\n        self.data.param_names = param_names\n        self.data.exog_re_names = exog_re_names\n        self.data.exog_re_names_full = exog_re_names_full\n    self.k_params = self.k_fe + self.k_re2\n    group_labels = list(set(groups))\n    group_labels.sort()\n    row_indices = dict(((s, []) for s in group_labels))\n    for (i, g) in enumerate(groups):\n        row_indices[g].append(i)\n    self.row_indices = row_indices\n    self.group_labels = group_labels\n    self.n_groups = len(self.group_labels)\n    self.endog_li = self.group_list(self.endog)\n    self.exog_li = self.group_list(self.exog)\n    self.exog_re_li = self.group_list(self.exog_re)\n    if self.exog_re is None:\n        self.exog_re2_li = None\n    else:\n        self.exog_re2_li = [np.dot(x.T, x) for x in self.exog_re_li]\n    self.nobs = len(self.endog)\n    self.n_totobs = self.nobs\n    if self.exog_names is None:\n        self.exog_names = ['FE%d' % (k + 1) for k in range(self.exog.shape[1])]\n    self._aex_r = []\n    self._aex_r2 = []\n    for i in range(self.n_groups):\n        a = self._augment_exog(i)\n        self._aex_r.append(a)\n        ma = _dot(a.T, a)\n        self._aex_r2.append(ma)\n    (self._lin, self._quad) = self._reparam()"
        ]
    },
    {
        "func_name": "_make_param_names",
        "original": "def _make_param_names(self, exog_re):\n    \"\"\"\n        Returns the full parameter names list, just the exogenous random\n        effects variables, and the exogenous random effects variables with\n        the interaction terms.\n        \"\"\"\n    exog_names = list(self.exog_names)\n    exog_re_names = _get_exog_re_names(self, exog_re)\n    param_names = []\n    jj = self.k_fe\n    for i in range(len(exog_re_names)):\n        for j in range(i + 1):\n            if i == j:\n                param_names.append(exog_re_names[i] + ' Var')\n            else:\n                param_names.append(exog_re_names[j] + ' x ' + exog_re_names[i] + ' Cov')\n            jj += 1\n    vc_names = [x + ' Var' for x in self.exog_vc.names]\n    return (exog_names + param_names + vc_names, exog_re_names, param_names)",
        "mutated": [
            "def _make_param_names(self, exog_re):\n    if False:\n        i = 10\n    '\\n        Returns the full parameter names list, just the exogenous random\\n        effects variables, and the exogenous random effects variables with\\n        the interaction terms.\\n        '\n    exog_names = list(self.exog_names)\n    exog_re_names = _get_exog_re_names(self, exog_re)\n    param_names = []\n    jj = self.k_fe\n    for i in range(len(exog_re_names)):\n        for j in range(i + 1):\n            if i == j:\n                param_names.append(exog_re_names[i] + ' Var')\n            else:\n                param_names.append(exog_re_names[j] + ' x ' + exog_re_names[i] + ' Cov')\n            jj += 1\n    vc_names = [x + ' Var' for x in self.exog_vc.names]\n    return (exog_names + param_names + vc_names, exog_re_names, param_names)",
            "def _make_param_names(self, exog_re):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the full parameter names list, just the exogenous random\\n        effects variables, and the exogenous random effects variables with\\n        the interaction terms.\\n        '\n    exog_names = list(self.exog_names)\n    exog_re_names = _get_exog_re_names(self, exog_re)\n    param_names = []\n    jj = self.k_fe\n    for i in range(len(exog_re_names)):\n        for j in range(i + 1):\n            if i == j:\n                param_names.append(exog_re_names[i] + ' Var')\n            else:\n                param_names.append(exog_re_names[j] + ' x ' + exog_re_names[i] + ' Cov')\n            jj += 1\n    vc_names = [x + ' Var' for x in self.exog_vc.names]\n    return (exog_names + param_names + vc_names, exog_re_names, param_names)",
            "def _make_param_names(self, exog_re):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the full parameter names list, just the exogenous random\\n        effects variables, and the exogenous random effects variables with\\n        the interaction terms.\\n        '\n    exog_names = list(self.exog_names)\n    exog_re_names = _get_exog_re_names(self, exog_re)\n    param_names = []\n    jj = self.k_fe\n    for i in range(len(exog_re_names)):\n        for j in range(i + 1):\n            if i == j:\n                param_names.append(exog_re_names[i] + ' Var')\n            else:\n                param_names.append(exog_re_names[j] + ' x ' + exog_re_names[i] + ' Cov')\n            jj += 1\n    vc_names = [x + ' Var' for x in self.exog_vc.names]\n    return (exog_names + param_names + vc_names, exog_re_names, param_names)",
            "def _make_param_names(self, exog_re):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the full parameter names list, just the exogenous random\\n        effects variables, and the exogenous random effects variables with\\n        the interaction terms.\\n        '\n    exog_names = list(self.exog_names)\n    exog_re_names = _get_exog_re_names(self, exog_re)\n    param_names = []\n    jj = self.k_fe\n    for i in range(len(exog_re_names)):\n        for j in range(i + 1):\n            if i == j:\n                param_names.append(exog_re_names[i] + ' Var')\n            else:\n                param_names.append(exog_re_names[j] + ' x ' + exog_re_names[i] + ' Cov')\n            jj += 1\n    vc_names = [x + ' Var' for x in self.exog_vc.names]\n    return (exog_names + param_names + vc_names, exog_re_names, param_names)",
            "def _make_param_names(self, exog_re):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the full parameter names list, just the exogenous random\\n        effects variables, and the exogenous random effects variables with\\n        the interaction terms.\\n        '\n    exog_names = list(self.exog_names)\n    exog_re_names = _get_exog_re_names(self, exog_re)\n    param_names = []\n    jj = self.k_fe\n    for i in range(len(exog_re_names)):\n        for j in range(i + 1):\n            if i == j:\n                param_names.append(exog_re_names[i] + ' Var')\n            else:\n                param_names.append(exog_re_names[j] + ' x ' + exog_re_names[i] + ' Cov')\n            jj += 1\n    vc_names = [x + ' Var' for x in self.exog_vc.names]\n    return (exog_names + param_names + vc_names, exog_re_names, param_names)"
        ]
    },
    {
        "func_name": "from_formula",
        "original": "@classmethod\ndef from_formula(cls, formula, data, re_formula=None, vc_formula=None, subset=None, use_sparse=False, missing='none', *args, **kwargs):\n    \"\"\"\n        Create a Model from a formula and dataframe.\n\n        Parameters\n        ----------\n        formula : str or generic Formula object\n            The formula specifying the model\n        data : array_like\n            The data for the model. See Notes.\n        re_formula : str\n            A one-sided formula defining the variance structure of the\n            model.  The default gives a random intercept for each\n            group.\n        vc_formula : dict-like\n            Formulas describing variance components.  `vc_formula[vc]` is\n            the formula for the component with variance parameter named\n            `vc`.  The formula is processed into a matrix, and the columns\n            of this matrix are linearly combined with independent random\n            coefficients having mean zero and a common variance.\n        subset : array_like\n            An array-like object of booleans, integers, or index\n            values that indicate the subset of df to use in the\n            model. Assumes df is a `pandas.DataFrame`\n        missing : str\n            Either 'none' or 'drop'\n        args : extra arguments\n            These are passed to the model\n        kwargs : extra keyword arguments\n            These are passed to the model with one exception. The\n            ``eval_env`` keyword is passed to patsy. It can be either a\n            :class:`patsy:patsy.EvalEnvironment` object or an integer\n            indicating the depth of the namespace to use. For example, the\n            default ``eval_env=0`` uses the calling namespace. If you wish\n            to use a \"clean\" environment set ``eval_env=-1``.\n\n        Returns\n        -------\n        model : Model instance\n\n        Notes\n        -----\n        `data` must define __getitem__ with the keys in the formula\n        terms args and kwargs are passed on to the model\n        instantiation. E.g., a numpy structured or rec array, a\n        dictionary, or a pandas DataFrame.\n\n        If the variance component is intended to produce random\n        intercepts for disjoint subsets of a group, specified by\n        string labels or a categorical data value, always use '0 +' in\n        the formula so that no overall intercept is included.\n\n        If the variance components specify random slopes and you do\n        not also want a random group-level intercept in the model,\n        then use '0 +' in the formula to exclude the intercept.\n\n        The variance components formulas are processed separately for\n        each group.  If a variable is categorical the results will not\n        be affected by whether the group labels are distinct or\n        re-used over the top-level groups.\n\n        Examples\n        --------\n        Suppose we have data from an educational study with students\n        nested in classrooms nested in schools.  The students take a\n        test, and we want to relate the test scores to the students'\n        ages, while accounting for the effects of classrooms and\n        schools.  The school will be the top-level group, and the\n        classroom is a nested group that is specified as a variance\n        component.  Note that the schools may have different number of\n        classrooms, and the classroom labels may (but need not be)\n        different across the schools.\n\n        >>> vc = {'classroom': '0 + C(classroom)'}\n        >>> MixedLM.from_formula('test_score ~ age', vc_formula=vc,                                   re_formula='1', groups='school', data=data)\n\n        Now suppose we also have a previous test score called\n        'pretest'.  If we want the relationship between pretest\n        scores and the current test to vary by classroom, we can\n        specify a random slope for the pretest score\n\n        >>> vc = {'classroom': '0 + C(classroom)', 'pretest': '0 + pretest'}\n        >>> MixedLM.from_formula('test_score ~ age + pretest', vc_formula=vc,                                   re_formula='1', groups='school', data=data)\n\n        The following model is almost equivalent to the previous one,\n        but here the classroom random intercept and pretest slope may\n        be correlated.\n\n        >>> vc = {'classroom': '0 + C(classroom)'}\n        >>> MixedLM.from_formula('test_score ~ age + pretest', vc_formula=vc,                                   re_formula='1 + pretest', groups='school',                                   data=data)\n        \"\"\"\n    if 'groups' not in kwargs.keys():\n        raise AttributeError(\"'groups' is a required keyword argument \" + 'in MixedLM.from_formula')\n    groups = kwargs['groups']\n    group_name = 'Group'\n    if isinstance(groups, str):\n        group_name = groups\n        groups = np.asarray(data[groups])\n    else:\n        groups = np.asarray(groups)\n    del kwargs['groups']\n    if missing == 'drop':\n        (data, groups) = _handle_missing(data, groups, formula, re_formula, vc_formula)\n        missing = 'none'\n    if re_formula is not None:\n        if re_formula.strip() == '1':\n            exog_re = np.ones((data.shape[0], 1))\n            exog_re_names = [group_name]\n        else:\n            eval_env = kwargs.get('eval_env', None)\n            if eval_env is None:\n                eval_env = 1\n            elif eval_env == -1:\n                from patsy import EvalEnvironment\n                eval_env = EvalEnvironment({})\n            exog_re = patsy.dmatrix(re_formula, data, eval_env=eval_env)\n            exog_re_names = exog_re.design_info.column_names\n            exog_re_names = [x.replace('Intercept', group_name) for x in exog_re_names]\n            exog_re = np.asarray(exog_re)\n        if exog_re.ndim == 1:\n            exog_re = exog_re[:, None]\n    else:\n        exog_re = None\n        if vc_formula is None:\n            exog_re_names = [group_name]\n        else:\n            exog_re_names = []\n    if vc_formula is not None:\n        eval_env = kwargs.get('eval_env', None)\n        if eval_env is None:\n            eval_env = 1\n        elif eval_env == -1:\n            from patsy import EvalEnvironment\n            eval_env = EvalEnvironment({})\n        vc_mats = []\n        vc_colnames = []\n        vc_names = []\n        gb = data.groupby(groups)\n        kylist = sorted(gb.groups.keys())\n        vcf = sorted(vc_formula.keys())\n        for vc_name in vcf:\n            md = patsy.ModelDesc.from_formula(vc_formula[vc_name])\n            vc_names.append(vc_name)\n            (evc_mats, evc_colnames) = ([], [])\n            for (group_ix, group) in enumerate(kylist):\n                ii = gb.groups[group]\n                mat = patsy.dmatrix(md, data.loc[ii, :], eval_env=eval_env, return_type='dataframe')\n                evc_colnames.append(mat.columns.tolist())\n                if use_sparse:\n                    evc_mats.append(sparse.csr_matrix(mat))\n                else:\n                    evc_mats.append(np.asarray(mat))\n            vc_mats.append(evc_mats)\n            vc_colnames.append(evc_colnames)\n        exog_vc = VCSpec(vc_names, vc_colnames, vc_mats)\n    else:\n        exog_vc = VCSpec([], [], [])\n    kwargs['subset'] = None\n    kwargs['exog_re'] = exog_re\n    kwargs['exog_vc'] = exog_vc\n    kwargs['groups'] = groups\n    mod = super(MixedLM, cls).from_formula(formula, data, *args, **kwargs)\n    (param_names, exog_re_names, exog_re_names_full) = mod._make_param_names(exog_re_names)\n    mod.data.param_names = param_names\n    mod.data.exog_re_names = exog_re_names\n    mod.data.exog_re_names_full = exog_re_names_full\n    if vc_formula is not None:\n        mod.data.vcomp_names = mod.exog_vc.names\n    return mod",
        "mutated": [
            "@classmethod\ndef from_formula(cls, formula, data, re_formula=None, vc_formula=None, subset=None, use_sparse=False, missing='none', *args, **kwargs):\n    if False:\n        i = 10\n    '\\n        Create a Model from a formula and dataframe.\\n\\n        Parameters\\n        ----------\\n        formula : str or generic Formula object\\n            The formula specifying the model\\n        data : array_like\\n            The data for the model. See Notes.\\n        re_formula : str\\n            A one-sided formula defining the variance structure of the\\n            model.  The default gives a random intercept for each\\n            group.\\n        vc_formula : dict-like\\n            Formulas describing variance components.  `vc_formula[vc]` is\\n            the formula for the component with variance parameter named\\n            `vc`.  The formula is processed into a matrix, and the columns\\n            of this matrix are linearly combined with independent random\\n            coefficients having mean zero and a common variance.\\n        subset : array_like\\n            An array-like object of booleans, integers, or index\\n            values that indicate the subset of df to use in the\\n            model. Assumes df is a `pandas.DataFrame`\\n        missing : str\\n            Either \\'none\\' or \\'drop\\'\\n        args : extra arguments\\n            These are passed to the model\\n        kwargs : extra keyword arguments\\n            These are passed to the model with one exception. The\\n            ``eval_env`` keyword is passed to patsy. It can be either a\\n            :class:`patsy:patsy.EvalEnvironment` object or an integer\\n            indicating the depth of the namespace to use. For example, the\\n            default ``eval_env=0`` uses the calling namespace. If you wish\\n            to use a \"clean\" environment set ``eval_env=-1``.\\n\\n        Returns\\n        -------\\n        model : Model instance\\n\\n        Notes\\n        -----\\n        `data` must define __getitem__ with the keys in the formula\\n        terms args and kwargs are passed on to the model\\n        instantiation. E.g., a numpy structured or rec array, a\\n        dictionary, or a pandas DataFrame.\\n\\n        If the variance component is intended to produce random\\n        intercepts for disjoint subsets of a group, specified by\\n        string labels or a categorical data value, always use \\'0 +\\' in\\n        the formula so that no overall intercept is included.\\n\\n        If the variance components specify random slopes and you do\\n        not also want a random group-level intercept in the model,\\n        then use \\'0 +\\' in the formula to exclude the intercept.\\n\\n        The variance components formulas are processed separately for\\n        each group.  If a variable is categorical the results will not\\n        be affected by whether the group labels are distinct or\\n        re-used over the top-level groups.\\n\\n        Examples\\n        --------\\n        Suppose we have data from an educational study with students\\n        nested in classrooms nested in schools.  The students take a\\n        test, and we want to relate the test scores to the students\\'\\n        ages, while accounting for the effects of classrooms and\\n        schools.  The school will be the top-level group, and the\\n        classroom is a nested group that is specified as a variance\\n        component.  Note that the schools may have different number of\\n        classrooms, and the classroom labels may (but need not be)\\n        different across the schools.\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age\\', vc_formula=vc,                                   re_formula=\\'1\\', groups=\\'school\\', data=data)\\n\\n        Now suppose we also have a previous test score called\\n        \\'pretest\\'.  If we want the relationship between pretest\\n        scores and the current test to vary by classroom, we can\\n        specify a random slope for the pretest score\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\', \\'pretest\\': \\'0 + pretest\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age + pretest\\', vc_formula=vc,                                   re_formula=\\'1\\', groups=\\'school\\', data=data)\\n\\n        The following model is almost equivalent to the previous one,\\n        but here the classroom random intercept and pretest slope may\\n        be correlated.\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age + pretest\\', vc_formula=vc,                                   re_formula=\\'1 + pretest\\', groups=\\'school\\',                                   data=data)\\n        '\n    if 'groups' not in kwargs.keys():\n        raise AttributeError(\"'groups' is a required keyword argument \" + 'in MixedLM.from_formula')\n    groups = kwargs['groups']\n    group_name = 'Group'\n    if isinstance(groups, str):\n        group_name = groups\n        groups = np.asarray(data[groups])\n    else:\n        groups = np.asarray(groups)\n    del kwargs['groups']\n    if missing == 'drop':\n        (data, groups) = _handle_missing(data, groups, formula, re_formula, vc_formula)\n        missing = 'none'\n    if re_formula is not None:\n        if re_formula.strip() == '1':\n            exog_re = np.ones((data.shape[0], 1))\n            exog_re_names = [group_name]\n        else:\n            eval_env = kwargs.get('eval_env', None)\n            if eval_env is None:\n                eval_env = 1\n            elif eval_env == -1:\n                from patsy import EvalEnvironment\n                eval_env = EvalEnvironment({})\n            exog_re = patsy.dmatrix(re_formula, data, eval_env=eval_env)\n            exog_re_names = exog_re.design_info.column_names\n            exog_re_names = [x.replace('Intercept', group_name) for x in exog_re_names]\n            exog_re = np.asarray(exog_re)\n        if exog_re.ndim == 1:\n            exog_re = exog_re[:, None]\n    else:\n        exog_re = None\n        if vc_formula is None:\n            exog_re_names = [group_name]\n        else:\n            exog_re_names = []\n    if vc_formula is not None:\n        eval_env = kwargs.get('eval_env', None)\n        if eval_env is None:\n            eval_env = 1\n        elif eval_env == -1:\n            from patsy import EvalEnvironment\n            eval_env = EvalEnvironment({})\n        vc_mats = []\n        vc_colnames = []\n        vc_names = []\n        gb = data.groupby(groups)\n        kylist = sorted(gb.groups.keys())\n        vcf = sorted(vc_formula.keys())\n        for vc_name in vcf:\n            md = patsy.ModelDesc.from_formula(vc_formula[vc_name])\n            vc_names.append(vc_name)\n            (evc_mats, evc_colnames) = ([], [])\n            for (group_ix, group) in enumerate(kylist):\n                ii = gb.groups[group]\n                mat = patsy.dmatrix(md, data.loc[ii, :], eval_env=eval_env, return_type='dataframe')\n                evc_colnames.append(mat.columns.tolist())\n                if use_sparse:\n                    evc_mats.append(sparse.csr_matrix(mat))\n                else:\n                    evc_mats.append(np.asarray(mat))\n            vc_mats.append(evc_mats)\n            vc_colnames.append(evc_colnames)\n        exog_vc = VCSpec(vc_names, vc_colnames, vc_mats)\n    else:\n        exog_vc = VCSpec([], [], [])\n    kwargs['subset'] = None\n    kwargs['exog_re'] = exog_re\n    kwargs['exog_vc'] = exog_vc\n    kwargs['groups'] = groups\n    mod = super(MixedLM, cls).from_formula(formula, data, *args, **kwargs)\n    (param_names, exog_re_names, exog_re_names_full) = mod._make_param_names(exog_re_names)\n    mod.data.param_names = param_names\n    mod.data.exog_re_names = exog_re_names\n    mod.data.exog_re_names_full = exog_re_names_full\n    if vc_formula is not None:\n        mod.data.vcomp_names = mod.exog_vc.names\n    return mod",
            "@classmethod\ndef from_formula(cls, formula, data, re_formula=None, vc_formula=None, subset=None, use_sparse=False, missing='none', *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create a Model from a formula and dataframe.\\n\\n        Parameters\\n        ----------\\n        formula : str or generic Formula object\\n            The formula specifying the model\\n        data : array_like\\n            The data for the model. See Notes.\\n        re_formula : str\\n            A one-sided formula defining the variance structure of the\\n            model.  The default gives a random intercept for each\\n            group.\\n        vc_formula : dict-like\\n            Formulas describing variance components.  `vc_formula[vc]` is\\n            the formula for the component with variance parameter named\\n            `vc`.  The formula is processed into a matrix, and the columns\\n            of this matrix are linearly combined with independent random\\n            coefficients having mean zero and a common variance.\\n        subset : array_like\\n            An array-like object of booleans, integers, or index\\n            values that indicate the subset of df to use in the\\n            model. Assumes df is a `pandas.DataFrame`\\n        missing : str\\n            Either \\'none\\' or \\'drop\\'\\n        args : extra arguments\\n            These are passed to the model\\n        kwargs : extra keyword arguments\\n            These are passed to the model with one exception. The\\n            ``eval_env`` keyword is passed to patsy. It can be either a\\n            :class:`patsy:patsy.EvalEnvironment` object or an integer\\n            indicating the depth of the namespace to use. For example, the\\n            default ``eval_env=0`` uses the calling namespace. If you wish\\n            to use a \"clean\" environment set ``eval_env=-1``.\\n\\n        Returns\\n        -------\\n        model : Model instance\\n\\n        Notes\\n        -----\\n        `data` must define __getitem__ with the keys in the formula\\n        terms args and kwargs are passed on to the model\\n        instantiation. E.g., a numpy structured or rec array, a\\n        dictionary, or a pandas DataFrame.\\n\\n        If the variance component is intended to produce random\\n        intercepts for disjoint subsets of a group, specified by\\n        string labels or a categorical data value, always use \\'0 +\\' in\\n        the formula so that no overall intercept is included.\\n\\n        If the variance components specify random slopes and you do\\n        not also want a random group-level intercept in the model,\\n        then use \\'0 +\\' in the formula to exclude the intercept.\\n\\n        The variance components formulas are processed separately for\\n        each group.  If a variable is categorical the results will not\\n        be affected by whether the group labels are distinct or\\n        re-used over the top-level groups.\\n\\n        Examples\\n        --------\\n        Suppose we have data from an educational study with students\\n        nested in classrooms nested in schools.  The students take a\\n        test, and we want to relate the test scores to the students\\'\\n        ages, while accounting for the effects of classrooms and\\n        schools.  The school will be the top-level group, and the\\n        classroom is a nested group that is specified as a variance\\n        component.  Note that the schools may have different number of\\n        classrooms, and the classroom labels may (but need not be)\\n        different across the schools.\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age\\', vc_formula=vc,                                   re_formula=\\'1\\', groups=\\'school\\', data=data)\\n\\n        Now suppose we also have a previous test score called\\n        \\'pretest\\'.  If we want the relationship between pretest\\n        scores and the current test to vary by classroom, we can\\n        specify a random slope for the pretest score\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\', \\'pretest\\': \\'0 + pretest\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age + pretest\\', vc_formula=vc,                                   re_formula=\\'1\\', groups=\\'school\\', data=data)\\n\\n        The following model is almost equivalent to the previous one,\\n        but here the classroom random intercept and pretest slope may\\n        be correlated.\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age + pretest\\', vc_formula=vc,                                   re_formula=\\'1 + pretest\\', groups=\\'school\\',                                   data=data)\\n        '\n    if 'groups' not in kwargs.keys():\n        raise AttributeError(\"'groups' is a required keyword argument \" + 'in MixedLM.from_formula')\n    groups = kwargs['groups']\n    group_name = 'Group'\n    if isinstance(groups, str):\n        group_name = groups\n        groups = np.asarray(data[groups])\n    else:\n        groups = np.asarray(groups)\n    del kwargs['groups']\n    if missing == 'drop':\n        (data, groups) = _handle_missing(data, groups, formula, re_formula, vc_formula)\n        missing = 'none'\n    if re_formula is not None:\n        if re_formula.strip() == '1':\n            exog_re = np.ones((data.shape[0], 1))\n            exog_re_names = [group_name]\n        else:\n            eval_env = kwargs.get('eval_env', None)\n            if eval_env is None:\n                eval_env = 1\n            elif eval_env == -1:\n                from patsy import EvalEnvironment\n                eval_env = EvalEnvironment({})\n            exog_re = patsy.dmatrix(re_formula, data, eval_env=eval_env)\n            exog_re_names = exog_re.design_info.column_names\n            exog_re_names = [x.replace('Intercept', group_name) for x in exog_re_names]\n            exog_re = np.asarray(exog_re)\n        if exog_re.ndim == 1:\n            exog_re = exog_re[:, None]\n    else:\n        exog_re = None\n        if vc_formula is None:\n            exog_re_names = [group_name]\n        else:\n            exog_re_names = []\n    if vc_formula is not None:\n        eval_env = kwargs.get('eval_env', None)\n        if eval_env is None:\n            eval_env = 1\n        elif eval_env == -1:\n            from patsy import EvalEnvironment\n            eval_env = EvalEnvironment({})\n        vc_mats = []\n        vc_colnames = []\n        vc_names = []\n        gb = data.groupby(groups)\n        kylist = sorted(gb.groups.keys())\n        vcf = sorted(vc_formula.keys())\n        for vc_name in vcf:\n            md = patsy.ModelDesc.from_formula(vc_formula[vc_name])\n            vc_names.append(vc_name)\n            (evc_mats, evc_colnames) = ([], [])\n            for (group_ix, group) in enumerate(kylist):\n                ii = gb.groups[group]\n                mat = patsy.dmatrix(md, data.loc[ii, :], eval_env=eval_env, return_type='dataframe')\n                evc_colnames.append(mat.columns.tolist())\n                if use_sparse:\n                    evc_mats.append(sparse.csr_matrix(mat))\n                else:\n                    evc_mats.append(np.asarray(mat))\n            vc_mats.append(evc_mats)\n            vc_colnames.append(evc_colnames)\n        exog_vc = VCSpec(vc_names, vc_colnames, vc_mats)\n    else:\n        exog_vc = VCSpec([], [], [])\n    kwargs['subset'] = None\n    kwargs['exog_re'] = exog_re\n    kwargs['exog_vc'] = exog_vc\n    kwargs['groups'] = groups\n    mod = super(MixedLM, cls).from_formula(formula, data, *args, **kwargs)\n    (param_names, exog_re_names, exog_re_names_full) = mod._make_param_names(exog_re_names)\n    mod.data.param_names = param_names\n    mod.data.exog_re_names = exog_re_names\n    mod.data.exog_re_names_full = exog_re_names_full\n    if vc_formula is not None:\n        mod.data.vcomp_names = mod.exog_vc.names\n    return mod",
            "@classmethod\ndef from_formula(cls, formula, data, re_formula=None, vc_formula=None, subset=None, use_sparse=False, missing='none', *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create a Model from a formula and dataframe.\\n\\n        Parameters\\n        ----------\\n        formula : str or generic Formula object\\n            The formula specifying the model\\n        data : array_like\\n            The data for the model. See Notes.\\n        re_formula : str\\n            A one-sided formula defining the variance structure of the\\n            model.  The default gives a random intercept for each\\n            group.\\n        vc_formula : dict-like\\n            Formulas describing variance components.  `vc_formula[vc]` is\\n            the formula for the component with variance parameter named\\n            `vc`.  The formula is processed into a matrix, and the columns\\n            of this matrix are linearly combined with independent random\\n            coefficients having mean zero and a common variance.\\n        subset : array_like\\n            An array-like object of booleans, integers, or index\\n            values that indicate the subset of df to use in the\\n            model. Assumes df is a `pandas.DataFrame`\\n        missing : str\\n            Either \\'none\\' or \\'drop\\'\\n        args : extra arguments\\n            These are passed to the model\\n        kwargs : extra keyword arguments\\n            These are passed to the model with one exception. The\\n            ``eval_env`` keyword is passed to patsy. It can be either a\\n            :class:`patsy:patsy.EvalEnvironment` object or an integer\\n            indicating the depth of the namespace to use. For example, the\\n            default ``eval_env=0`` uses the calling namespace. If you wish\\n            to use a \"clean\" environment set ``eval_env=-1``.\\n\\n        Returns\\n        -------\\n        model : Model instance\\n\\n        Notes\\n        -----\\n        `data` must define __getitem__ with the keys in the formula\\n        terms args and kwargs are passed on to the model\\n        instantiation. E.g., a numpy structured or rec array, a\\n        dictionary, or a pandas DataFrame.\\n\\n        If the variance component is intended to produce random\\n        intercepts for disjoint subsets of a group, specified by\\n        string labels or a categorical data value, always use \\'0 +\\' in\\n        the formula so that no overall intercept is included.\\n\\n        If the variance components specify random slopes and you do\\n        not also want a random group-level intercept in the model,\\n        then use \\'0 +\\' in the formula to exclude the intercept.\\n\\n        The variance components formulas are processed separately for\\n        each group.  If a variable is categorical the results will not\\n        be affected by whether the group labels are distinct or\\n        re-used over the top-level groups.\\n\\n        Examples\\n        --------\\n        Suppose we have data from an educational study with students\\n        nested in classrooms nested in schools.  The students take a\\n        test, and we want to relate the test scores to the students\\'\\n        ages, while accounting for the effects of classrooms and\\n        schools.  The school will be the top-level group, and the\\n        classroom is a nested group that is specified as a variance\\n        component.  Note that the schools may have different number of\\n        classrooms, and the classroom labels may (but need not be)\\n        different across the schools.\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age\\', vc_formula=vc,                                   re_formula=\\'1\\', groups=\\'school\\', data=data)\\n\\n        Now suppose we also have a previous test score called\\n        \\'pretest\\'.  If we want the relationship between pretest\\n        scores and the current test to vary by classroom, we can\\n        specify a random slope for the pretest score\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\', \\'pretest\\': \\'0 + pretest\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age + pretest\\', vc_formula=vc,                                   re_formula=\\'1\\', groups=\\'school\\', data=data)\\n\\n        The following model is almost equivalent to the previous one,\\n        but here the classroom random intercept and pretest slope may\\n        be correlated.\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age + pretest\\', vc_formula=vc,                                   re_formula=\\'1 + pretest\\', groups=\\'school\\',                                   data=data)\\n        '\n    if 'groups' not in kwargs.keys():\n        raise AttributeError(\"'groups' is a required keyword argument \" + 'in MixedLM.from_formula')\n    groups = kwargs['groups']\n    group_name = 'Group'\n    if isinstance(groups, str):\n        group_name = groups\n        groups = np.asarray(data[groups])\n    else:\n        groups = np.asarray(groups)\n    del kwargs['groups']\n    if missing == 'drop':\n        (data, groups) = _handle_missing(data, groups, formula, re_formula, vc_formula)\n        missing = 'none'\n    if re_formula is not None:\n        if re_formula.strip() == '1':\n            exog_re = np.ones((data.shape[0], 1))\n            exog_re_names = [group_name]\n        else:\n            eval_env = kwargs.get('eval_env', None)\n            if eval_env is None:\n                eval_env = 1\n            elif eval_env == -1:\n                from patsy import EvalEnvironment\n                eval_env = EvalEnvironment({})\n            exog_re = patsy.dmatrix(re_formula, data, eval_env=eval_env)\n            exog_re_names = exog_re.design_info.column_names\n            exog_re_names = [x.replace('Intercept', group_name) for x in exog_re_names]\n            exog_re = np.asarray(exog_re)\n        if exog_re.ndim == 1:\n            exog_re = exog_re[:, None]\n    else:\n        exog_re = None\n        if vc_formula is None:\n            exog_re_names = [group_name]\n        else:\n            exog_re_names = []\n    if vc_formula is not None:\n        eval_env = kwargs.get('eval_env', None)\n        if eval_env is None:\n            eval_env = 1\n        elif eval_env == -1:\n            from patsy import EvalEnvironment\n            eval_env = EvalEnvironment({})\n        vc_mats = []\n        vc_colnames = []\n        vc_names = []\n        gb = data.groupby(groups)\n        kylist = sorted(gb.groups.keys())\n        vcf = sorted(vc_formula.keys())\n        for vc_name in vcf:\n            md = patsy.ModelDesc.from_formula(vc_formula[vc_name])\n            vc_names.append(vc_name)\n            (evc_mats, evc_colnames) = ([], [])\n            for (group_ix, group) in enumerate(kylist):\n                ii = gb.groups[group]\n                mat = patsy.dmatrix(md, data.loc[ii, :], eval_env=eval_env, return_type='dataframe')\n                evc_colnames.append(mat.columns.tolist())\n                if use_sparse:\n                    evc_mats.append(sparse.csr_matrix(mat))\n                else:\n                    evc_mats.append(np.asarray(mat))\n            vc_mats.append(evc_mats)\n            vc_colnames.append(evc_colnames)\n        exog_vc = VCSpec(vc_names, vc_colnames, vc_mats)\n    else:\n        exog_vc = VCSpec([], [], [])\n    kwargs['subset'] = None\n    kwargs['exog_re'] = exog_re\n    kwargs['exog_vc'] = exog_vc\n    kwargs['groups'] = groups\n    mod = super(MixedLM, cls).from_formula(formula, data, *args, **kwargs)\n    (param_names, exog_re_names, exog_re_names_full) = mod._make_param_names(exog_re_names)\n    mod.data.param_names = param_names\n    mod.data.exog_re_names = exog_re_names\n    mod.data.exog_re_names_full = exog_re_names_full\n    if vc_formula is not None:\n        mod.data.vcomp_names = mod.exog_vc.names\n    return mod",
            "@classmethod\ndef from_formula(cls, formula, data, re_formula=None, vc_formula=None, subset=None, use_sparse=False, missing='none', *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create a Model from a formula and dataframe.\\n\\n        Parameters\\n        ----------\\n        formula : str or generic Formula object\\n            The formula specifying the model\\n        data : array_like\\n            The data for the model. See Notes.\\n        re_formula : str\\n            A one-sided formula defining the variance structure of the\\n            model.  The default gives a random intercept for each\\n            group.\\n        vc_formula : dict-like\\n            Formulas describing variance components.  `vc_formula[vc]` is\\n            the formula for the component with variance parameter named\\n            `vc`.  The formula is processed into a matrix, and the columns\\n            of this matrix are linearly combined with independent random\\n            coefficients having mean zero and a common variance.\\n        subset : array_like\\n            An array-like object of booleans, integers, or index\\n            values that indicate the subset of df to use in the\\n            model. Assumes df is a `pandas.DataFrame`\\n        missing : str\\n            Either \\'none\\' or \\'drop\\'\\n        args : extra arguments\\n            These are passed to the model\\n        kwargs : extra keyword arguments\\n            These are passed to the model with one exception. The\\n            ``eval_env`` keyword is passed to patsy. It can be either a\\n            :class:`patsy:patsy.EvalEnvironment` object or an integer\\n            indicating the depth of the namespace to use. For example, the\\n            default ``eval_env=0`` uses the calling namespace. If you wish\\n            to use a \"clean\" environment set ``eval_env=-1``.\\n\\n        Returns\\n        -------\\n        model : Model instance\\n\\n        Notes\\n        -----\\n        `data` must define __getitem__ with the keys in the formula\\n        terms args and kwargs are passed on to the model\\n        instantiation. E.g., a numpy structured or rec array, a\\n        dictionary, or a pandas DataFrame.\\n\\n        If the variance component is intended to produce random\\n        intercepts for disjoint subsets of a group, specified by\\n        string labels or a categorical data value, always use \\'0 +\\' in\\n        the formula so that no overall intercept is included.\\n\\n        If the variance components specify random slopes and you do\\n        not also want a random group-level intercept in the model,\\n        then use \\'0 +\\' in the formula to exclude the intercept.\\n\\n        The variance components formulas are processed separately for\\n        each group.  If a variable is categorical the results will not\\n        be affected by whether the group labels are distinct or\\n        re-used over the top-level groups.\\n\\n        Examples\\n        --------\\n        Suppose we have data from an educational study with students\\n        nested in classrooms nested in schools.  The students take a\\n        test, and we want to relate the test scores to the students\\'\\n        ages, while accounting for the effects of classrooms and\\n        schools.  The school will be the top-level group, and the\\n        classroom is a nested group that is specified as a variance\\n        component.  Note that the schools may have different number of\\n        classrooms, and the classroom labels may (but need not be)\\n        different across the schools.\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age\\', vc_formula=vc,                                   re_formula=\\'1\\', groups=\\'school\\', data=data)\\n\\n        Now suppose we also have a previous test score called\\n        \\'pretest\\'.  If we want the relationship between pretest\\n        scores and the current test to vary by classroom, we can\\n        specify a random slope for the pretest score\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\', \\'pretest\\': \\'0 + pretest\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age + pretest\\', vc_formula=vc,                                   re_formula=\\'1\\', groups=\\'school\\', data=data)\\n\\n        The following model is almost equivalent to the previous one,\\n        but here the classroom random intercept and pretest slope may\\n        be correlated.\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age + pretest\\', vc_formula=vc,                                   re_formula=\\'1 + pretest\\', groups=\\'school\\',                                   data=data)\\n        '\n    if 'groups' not in kwargs.keys():\n        raise AttributeError(\"'groups' is a required keyword argument \" + 'in MixedLM.from_formula')\n    groups = kwargs['groups']\n    group_name = 'Group'\n    if isinstance(groups, str):\n        group_name = groups\n        groups = np.asarray(data[groups])\n    else:\n        groups = np.asarray(groups)\n    del kwargs['groups']\n    if missing == 'drop':\n        (data, groups) = _handle_missing(data, groups, formula, re_formula, vc_formula)\n        missing = 'none'\n    if re_formula is not None:\n        if re_formula.strip() == '1':\n            exog_re = np.ones((data.shape[0], 1))\n            exog_re_names = [group_name]\n        else:\n            eval_env = kwargs.get('eval_env', None)\n            if eval_env is None:\n                eval_env = 1\n            elif eval_env == -1:\n                from patsy import EvalEnvironment\n                eval_env = EvalEnvironment({})\n            exog_re = patsy.dmatrix(re_formula, data, eval_env=eval_env)\n            exog_re_names = exog_re.design_info.column_names\n            exog_re_names = [x.replace('Intercept', group_name) for x in exog_re_names]\n            exog_re = np.asarray(exog_re)\n        if exog_re.ndim == 1:\n            exog_re = exog_re[:, None]\n    else:\n        exog_re = None\n        if vc_formula is None:\n            exog_re_names = [group_name]\n        else:\n            exog_re_names = []\n    if vc_formula is not None:\n        eval_env = kwargs.get('eval_env', None)\n        if eval_env is None:\n            eval_env = 1\n        elif eval_env == -1:\n            from patsy import EvalEnvironment\n            eval_env = EvalEnvironment({})\n        vc_mats = []\n        vc_colnames = []\n        vc_names = []\n        gb = data.groupby(groups)\n        kylist = sorted(gb.groups.keys())\n        vcf = sorted(vc_formula.keys())\n        for vc_name in vcf:\n            md = patsy.ModelDesc.from_formula(vc_formula[vc_name])\n            vc_names.append(vc_name)\n            (evc_mats, evc_colnames) = ([], [])\n            for (group_ix, group) in enumerate(kylist):\n                ii = gb.groups[group]\n                mat = patsy.dmatrix(md, data.loc[ii, :], eval_env=eval_env, return_type='dataframe')\n                evc_colnames.append(mat.columns.tolist())\n                if use_sparse:\n                    evc_mats.append(sparse.csr_matrix(mat))\n                else:\n                    evc_mats.append(np.asarray(mat))\n            vc_mats.append(evc_mats)\n            vc_colnames.append(evc_colnames)\n        exog_vc = VCSpec(vc_names, vc_colnames, vc_mats)\n    else:\n        exog_vc = VCSpec([], [], [])\n    kwargs['subset'] = None\n    kwargs['exog_re'] = exog_re\n    kwargs['exog_vc'] = exog_vc\n    kwargs['groups'] = groups\n    mod = super(MixedLM, cls).from_formula(formula, data, *args, **kwargs)\n    (param_names, exog_re_names, exog_re_names_full) = mod._make_param_names(exog_re_names)\n    mod.data.param_names = param_names\n    mod.data.exog_re_names = exog_re_names\n    mod.data.exog_re_names_full = exog_re_names_full\n    if vc_formula is not None:\n        mod.data.vcomp_names = mod.exog_vc.names\n    return mod",
            "@classmethod\ndef from_formula(cls, formula, data, re_formula=None, vc_formula=None, subset=None, use_sparse=False, missing='none', *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create a Model from a formula and dataframe.\\n\\n        Parameters\\n        ----------\\n        formula : str or generic Formula object\\n            The formula specifying the model\\n        data : array_like\\n            The data for the model. See Notes.\\n        re_formula : str\\n            A one-sided formula defining the variance structure of the\\n            model.  The default gives a random intercept for each\\n            group.\\n        vc_formula : dict-like\\n            Formulas describing variance components.  `vc_formula[vc]` is\\n            the formula for the component with variance parameter named\\n            `vc`.  The formula is processed into a matrix, and the columns\\n            of this matrix are linearly combined with independent random\\n            coefficients having mean zero and a common variance.\\n        subset : array_like\\n            An array-like object of booleans, integers, or index\\n            values that indicate the subset of df to use in the\\n            model. Assumes df is a `pandas.DataFrame`\\n        missing : str\\n            Either \\'none\\' or \\'drop\\'\\n        args : extra arguments\\n            These are passed to the model\\n        kwargs : extra keyword arguments\\n            These are passed to the model with one exception. The\\n            ``eval_env`` keyword is passed to patsy. It can be either a\\n            :class:`patsy:patsy.EvalEnvironment` object or an integer\\n            indicating the depth of the namespace to use. For example, the\\n            default ``eval_env=0`` uses the calling namespace. If you wish\\n            to use a \"clean\" environment set ``eval_env=-1``.\\n\\n        Returns\\n        -------\\n        model : Model instance\\n\\n        Notes\\n        -----\\n        `data` must define __getitem__ with the keys in the formula\\n        terms args and kwargs are passed on to the model\\n        instantiation. E.g., a numpy structured or rec array, a\\n        dictionary, or a pandas DataFrame.\\n\\n        If the variance component is intended to produce random\\n        intercepts for disjoint subsets of a group, specified by\\n        string labels or a categorical data value, always use \\'0 +\\' in\\n        the formula so that no overall intercept is included.\\n\\n        If the variance components specify random slopes and you do\\n        not also want a random group-level intercept in the model,\\n        then use \\'0 +\\' in the formula to exclude the intercept.\\n\\n        The variance components formulas are processed separately for\\n        each group.  If a variable is categorical the results will not\\n        be affected by whether the group labels are distinct or\\n        re-used over the top-level groups.\\n\\n        Examples\\n        --------\\n        Suppose we have data from an educational study with students\\n        nested in classrooms nested in schools.  The students take a\\n        test, and we want to relate the test scores to the students\\'\\n        ages, while accounting for the effects of classrooms and\\n        schools.  The school will be the top-level group, and the\\n        classroom is a nested group that is specified as a variance\\n        component.  Note that the schools may have different number of\\n        classrooms, and the classroom labels may (but need not be)\\n        different across the schools.\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age\\', vc_formula=vc,                                   re_formula=\\'1\\', groups=\\'school\\', data=data)\\n\\n        Now suppose we also have a previous test score called\\n        \\'pretest\\'.  If we want the relationship between pretest\\n        scores and the current test to vary by classroom, we can\\n        specify a random slope for the pretest score\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\', \\'pretest\\': \\'0 + pretest\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age + pretest\\', vc_formula=vc,                                   re_formula=\\'1\\', groups=\\'school\\', data=data)\\n\\n        The following model is almost equivalent to the previous one,\\n        but here the classroom random intercept and pretest slope may\\n        be correlated.\\n\\n        >>> vc = {\\'classroom\\': \\'0 + C(classroom)\\'}\\n        >>> MixedLM.from_formula(\\'test_score ~ age + pretest\\', vc_formula=vc,                                   re_formula=\\'1 + pretest\\', groups=\\'school\\',                                   data=data)\\n        '\n    if 'groups' not in kwargs.keys():\n        raise AttributeError(\"'groups' is a required keyword argument \" + 'in MixedLM.from_formula')\n    groups = kwargs['groups']\n    group_name = 'Group'\n    if isinstance(groups, str):\n        group_name = groups\n        groups = np.asarray(data[groups])\n    else:\n        groups = np.asarray(groups)\n    del kwargs['groups']\n    if missing == 'drop':\n        (data, groups) = _handle_missing(data, groups, formula, re_formula, vc_formula)\n        missing = 'none'\n    if re_formula is not None:\n        if re_formula.strip() == '1':\n            exog_re = np.ones((data.shape[0], 1))\n            exog_re_names = [group_name]\n        else:\n            eval_env = kwargs.get('eval_env', None)\n            if eval_env is None:\n                eval_env = 1\n            elif eval_env == -1:\n                from patsy import EvalEnvironment\n                eval_env = EvalEnvironment({})\n            exog_re = patsy.dmatrix(re_formula, data, eval_env=eval_env)\n            exog_re_names = exog_re.design_info.column_names\n            exog_re_names = [x.replace('Intercept', group_name) for x in exog_re_names]\n            exog_re = np.asarray(exog_re)\n        if exog_re.ndim == 1:\n            exog_re = exog_re[:, None]\n    else:\n        exog_re = None\n        if vc_formula is None:\n            exog_re_names = [group_name]\n        else:\n            exog_re_names = []\n    if vc_formula is not None:\n        eval_env = kwargs.get('eval_env', None)\n        if eval_env is None:\n            eval_env = 1\n        elif eval_env == -1:\n            from patsy import EvalEnvironment\n            eval_env = EvalEnvironment({})\n        vc_mats = []\n        vc_colnames = []\n        vc_names = []\n        gb = data.groupby(groups)\n        kylist = sorted(gb.groups.keys())\n        vcf = sorted(vc_formula.keys())\n        for vc_name in vcf:\n            md = patsy.ModelDesc.from_formula(vc_formula[vc_name])\n            vc_names.append(vc_name)\n            (evc_mats, evc_colnames) = ([], [])\n            for (group_ix, group) in enumerate(kylist):\n                ii = gb.groups[group]\n                mat = patsy.dmatrix(md, data.loc[ii, :], eval_env=eval_env, return_type='dataframe')\n                evc_colnames.append(mat.columns.tolist())\n                if use_sparse:\n                    evc_mats.append(sparse.csr_matrix(mat))\n                else:\n                    evc_mats.append(np.asarray(mat))\n            vc_mats.append(evc_mats)\n            vc_colnames.append(evc_colnames)\n        exog_vc = VCSpec(vc_names, vc_colnames, vc_mats)\n    else:\n        exog_vc = VCSpec([], [], [])\n    kwargs['subset'] = None\n    kwargs['exog_re'] = exog_re\n    kwargs['exog_vc'] = exog_vc\n    kwargs['groups'] = groups\n    mod = super(MixedLM, cls).from_formula(formula, data, *args, **kwargs)\n    (param_names, exog_re_names, exog_re_names_full) = mod._make_param_names(exog_re_names)\n    mod.data.param_names = param_names\n    mod.data.exog_re_names = exog_re_names\n    mod.data.exog_re_names_full = exog_re_names_full\n    if vc_formula is not None:\n        mod.data.vcomp_names = mod.exog_vc.names\n    return mod"
        ]
    },
    {
        "func_name": "predict",
        "original": "def predict(self, params, exog=None):\n    \"\"\"\n        Return predicted values from a design matrix.\n\n        Parameters\n        ----------\n        params : array_like\n            Parameters of a mixed linear model.  Can be either a\n            MixedLMParams instance, or a vector containing the packed\n            model parameters in which the fixed effects parameters are\n            at the beginning of the vector, or a vector containing\n            only the fixed effects parameters.\n        exog : array_like, optional\n            Design / exogenous data for the fixed effects. Model exog\n            is used if None.\n\n        Returns\n        -------\n        An array of fitted values.  Note that these predicted values\n        only reflect the fixed effects mean structure of the model.\n        \"\"\"\n    if exog is None:\n        exog = self.exog\n    if isinstance(params, MixedLMParams):\n        params = params.fe_params\n    else:\n        params = params[0:self.k_fe]\n    return np.dot(exog, params)",
        "mutated": [
            "def predict(self, params, exog=None):\n    if False:\n        i = 10\n    '\\n        Return predicted values from a design matrix.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Parameters of a mixed linear model.  Can be either a\\n            MixedLMParams instance, or a vector containing the packed\\n            model parameters in which the fixed effects parameters are\\n            at the beginning of the vector, or a vector containing\\n            only the fixed effects parameters.\\n        exog : array_like, optional\\n            Design / exogenous data for the fixed effects. Model exog\\n            is used if None.\\n\\n        Returns\\n        -------\\n        An array of fitted values.  Note that these predicted values\\n        only reflect the fixed effects mean structure of the model.\\n        '\n    if exog is None:\n        exog = self.exog\n    if isinstance(params, MixedLMParams):\n        params = params.fe_params\n    else:\n        params = params[0:self.k_fe]\n    return np.dot(exog, params)",
            "def predict(self, params, exog=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Return predicted values from a design matrix.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Parameters of a mixed linear model.  Can be either a\\n            MixedLMParams instance, or a vector containing the packed\\n            model parameters in which the fixed effects parameters are\\n            at the beginning of the vector, or a vector containing\\n            only the fixed effects parameters.\\n        exog : array_like, optional\\n            Design / exogenous data for the fixed effects. Model exog\\n            is used if None.\\n\\n        Returns\\n        -------\\n        An array of fitted values.  Note that these predicted values\\n        only reflect the fixed effects mean structure of the model.\\n        '\n    if exog is None:\n        exog = self.exog\n    if isinstance(params, MixedLMParams):\n        params = params.fe_params\n    else:\n        params = params[0:self.k_fe]\n    return np.dot(exog, params)",
            "def predict(self, params, exog=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Return predicted values from a design matrix.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Parameters of a mixed linear model.  Can be either a\\n            MixedLMParams instance, or a vector containing the packed\\n            model parameters in which the fixed effects parameters are\\n            at the beginning of the vector, or a vector containing\\n            only the fixed effects parameters.\\n        exog : array_like, optional\\n            Design / exogenous data for the fixed effects. Model exog\\n            is used if None.\\n\\n        Returns\\n        -------\\n        An array of fitted values.  Note that these predicted values\\n        only reflect the fixed effects mean structure of the model.\\n        '\n    if exog is None:\n        exog = self.exog\n    if isinstance(params, MixedLMParams):\n        params = params.fe_params\n    else:\n        params = params[0:self.k_fe]\n    return np.dot(exog, params)",
            "def predict(self, params, exog=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Return predicted values from a design matrix.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Parameters of a mixed linear model.  Can be either a\\n            MixedLMParams instance, or a vector containing the packed\\n            model parameters in which the fixed effects parameters are\\n            at the beginning of the vector, or a vector containing\\n            only the fixed effects parameters.\\n        exog : array_like, optional\\n            Design / exogenous data for the fixed effects. Model exog\\n            is used if None.\\n\\n        Returns\\n        -------\\n        An array of fitted values.  Note that these predicted values\\n        only reflect the fixed effects mean structure of the model.\\n        '\n    if exog is None:\n        exog = self.exog\n    if isinstance(params, MixedLMParams):\n        params = params.fe_params\n    else:\n        params = params[0:self.k_fe]\n    return np.dot(exog, params)",
            "def predict(self, params, exog=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Return predicted values from a design matrix.\\n\\n        Parameters\\n        ----------\\n        params : array_like\\n            Parameters of a mixed linear model.  Can be either a\\n            MixedLMParams instance, or a vector containing the packed\\n            model parameters in which the fixed effects parameters are\\n            at the beginning of the vector, or a vector containing\\n            only the fixed effects parameters.\\n        exog : array_like, optional\\n            Design / exogenous data for the fixed effects. Model exog\\n            is used if None.\\n\\n        Returns\\n        -------\\n        An array of fitted values.  Note that these predicted values\\n        only reflect the fixed effects mean structure of the model.\\n        '\n    if exog is None:\n        exog = self.exog\n    if isinstance(params, MixedLMParams):\n        params = params.fe_params\n    else:\n        params = params[0:self.k_fe]\n    return np.dot(exog, params)"
        ]
    },
    {
        "func_name": "group_list",
        "original": "def group_list(self, array):\n    \"\"\"\n        Returns `array` split into subarrays corresponding to the\n        grouping structure.\n        \"\"\"\n    if array is None:\n        return None\n    if array.ndim == 1:\n        return [np.array(array[self.row_indices[k]]) for k in self.group_labels]\n    else:\n        return [np.array(array[self.row_indices[k], :]) for k in self.group_labels]",
        "mutated": [
            "def group_list(self, array):\n    if False:\n        i = 10\n    '\\n        Returns `array` split into subarrays corresponding to the\\n        grouping structure.\\n        '\n    if array is None:\n        return None\n    if array.ndim == 1:\n        return [np.array(array[self.row_indices[k]]) for k in self.group_labels]\n    else:\n        return [np.array(array[self.row_indices[k], :]) for k in self.group_labels]",
            "def group_list(self, array):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns `array` split into subarrays corresponding to the\\n        grouping structure.\\n        '\n    if array is None:\n        return None\n    if array.ndim == 1:\n        return [np.array(array[self.row_indices[k]]) for k in self.group_labels]\n    else:\n        return [np.array(array[self.row_indices[k], :]) for k in self.group_labels]",
            "def group_list(self, array):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns `array` split into subarrays corresponding to the\\n        grouping structure.\\n        '\n    if array is None:\n        return None\n    if array.ndim == 1:\n        return [np.array(array[self.row_indices[k]]) for k in self.group_labels]\n    else:\n        return [np.array(array[self.row_indices[k], :]) for k in self.group_labels]",
            "def group_list(self, array):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns `array` split into subarrays corresponding to the\\n        grouping structure.\\n        '\n    if array is None:\n        return None\n    if array.ndim == 1:\n        return [np.array(array[self.row_indices[k]]) for k in self.group_labels]\n    else:\n        return [np.array(array[self.row_indices[k], :]) for k in self.group_labels]",
            "def group_list(self, array):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns `array` split into subarrays corresponding to the\\n        grouping structure.\\n        '\n    if array is None:\n        return None\n    if array.ndim == 1:\n        return [np.array(array[self.row_indices[k]]) for k in self.group_labels]\n    else:\n        return [np.array(array[self.row_indices[k], :]) for k in self.group_labels]"
        ]
    },
    {
        "func_name": "fit_regularized",
        "original": "def fit_regularized(self, start_params=None, method='l1', alpha=0, ceps=0.0001, ptol=1e-06, maxit=200, **fit_kwargs):\n    \"\"\"\n        Fit a model in which the fixed effects parameters are\n        penalized.  The dependence parameters are held fixed at their\n        estimated values in the unpenalized model.\n\n        Parameters\n        ----------\n        method : str of Penalty object\n            Method for regularization.  If a string, must be 'l1'.\n        alpha : array_like\n            Scalar or vector of penalty weights.  If a scalar, the\n            same weight is applied to all coefficients; if a vector,\n            it contains a weight for each coefficient.  If method is a\n            Penalty object, the weights are scaled by alpha.  For L1\n            regularization, the weights are used directly.\n        ceps : positive real scalar\n            Fixed effects parameters smaller than this value\n            in magnitude are treated as being zero.\n        ptol : positive real scalar\n            Convergence occurs when the sup norm difference\n            between successive values of `fe_params` is less than\n            `ptol`.\n        maxit : int\n            The maximum number of iterations.\n        **fit_kwargs\n            Additional keyword arguments passed to fit.\n\n        Returns\n        -------\n        A MixedLMResults instance containing the results.\n\n        Notes\n        -----\n        The covariance structure is not updated as the fixed effects\n        parameters are varied.\n\n        The algorithm used here for L1 regularization is a\"shooting\"\n        or cyclic coordinate descent algorithm.\n\n        If method is 'l1', then `fe_pen` and `cov_pen` are used to\n        obtain the covariance structure, but are ignored during the\n        L1-penalized fitting.\n\n        References\n        ----------\n        Friedman, J. H., Hastie, T. and Tibshirani, R. Regularized\n        Paths for Generalized Linear Models via Coordinate\n        Descent. Journal of Statistical Software, 33(1) (2008)\n        http://www.jstatsoft.org/v33/i01/paper\n\n        http://statweb.stanford.edu/~tibs/stat315a/Supplements/fuse.pdf\n        \"\"\"\n    if isinstance(method, str) and method.lower() != 'l1':\n        raise ValueError('Invalid regularization method')\n    if isinstance(method, Penalty):\n        method.alpha = alpha\n        fit_kwargs.update({'fe_pen': method})\n        return self.fit(**fit_kwargs)\n    if np.isscalar(alpha):\n        alpha = alpha * np.ones(self.k_fe, dtype=np.float64)\n    mdf = self.fit(**fit_kwargs)\n    fe_params = mdf.fe_params\n    cov_re = mdf.cov_re\n    vcomp = mdf.vcomp\n    scale = mdf.scale\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = None\n    for itr in range(maxit):\n        fe_params_s = fe_params.copy()\n        for j in range(self.k_fe):\n            if abs(fe_params[j]) < ceps:\n                continue\n            fe_params[j] = 0.0\n            expval = np.dot(self.exog, fe_params)\n            resid_all = self.endog - expval\n            (a, b) = (0.0, 0.0)\n            for (group_ix, group) in enumerate(self.group_labels):\n                vc_var = self._expand_vcomp(vcomp, group_ix)\n                exog = self.exog_li[group_ix]\n                (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n                resid = resid_all[self.row_indices[group]]\n                solver = _smw_solver(scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n                x = exog[:, j]\n                u = solver(x)\n                a += np.dot(u, x)\n                b -= 2 * np.dot(u, resid)\n            pwt1 = alpha[j]\n            if b > pwt1:\n                fe_params[j] = -(b - pwt1) / (2 * a)\n            elif b < -pwt1:\n                fe_params[j] = -(b + pwt1) / (2 * a)\n        if np.abs(fe_params_s - fe_params).max() < ptol:\n            break\n    params_prof = mdf.params.copy()\n    params_prof[0:self.k_fe] = fe_params\n    scale = self.get_scale(fe_params, mdf.cov_re_unscaled, mdf.vcomp)\n    (hess, sing) = self.hessian(params_prof)\n    if sing:\n        warnings.warn(_warn_cov_sing)\n    pcov = np.nan * np.ones_like(hess)\n    ii = np.abs(params_prof) > ceps\n    ii[self.k_fe:] = True\n    ii = np.flatnonzero(ii)\n    hess1 = hess[ii, :][:, ii]\n    pcov[np.ix_(ii, ii)] = np.linalg.inv(-hess1)\n    params_object = MixedLMParams.from_components(fe_params, cov_re=cov_re)\n    results = MixedLMResults(self, params_prof, pcov / scale)\n    results.params_object = params_object\n    results.fe_params = fe_params\n    results.cov_re = cov_re\n    results.vcomp = vcomp\n    results.scale = scale\n    results.cov_re_unscaled = mdf.cov_re_unscaled\n    results.method = mdf.method\n    results.converged = True\n    results.cov_pen = self.cov_pen\n    results.k_fe = self.k_fe\n    results.k_re = self.k_re\n    results.k_re2 = self.k_re2\n    results.k_vc = self.k_vc\n    return MixedLMResultsWrapper(results)",
        "mutated": [
            "def fit_regularized(self, start_params=None, method='l1', alpha=0, ceps=0.0001, ptol=1e-06, maxit=200, **fit_kwargs):\n    if False:\n        i = 10\n    '\\n        Fit a model in which the fixed effects parameters are\\n        penalized.  The dependence parameters are held fixed at their\\n        estimated values in the unpenalized model.\\n\\n        Parameters\\n        ----------\\n        method : str of Penalty object\\n            Method for regularization.  If a string, must be \\'l1\\'.\\n        alpha : array_like\\n            Scalar or vector of penalty weights.  If a scalar, the\\n            same weight is applied to all coefficients; if a vector,\\n            it contains a weight for each coefficient.  If method is a\\n            Penalty object, the weights are scaled by alpha.  For L1\\n            regularization, the weights are used directly.\\n        ceps : positive real scalar\\n            Fixed effects parameters smaller than this value\\n            in magnitude are treated as being zero.\\n        ptol : positive real scalar\\n            Convergence occurs when the sup norm difference\\n            between successive values of `fe_params` is less than\\n            `ptol`.\\n        maxit : int\\n            The maximum number of iterations.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        A MixedLMResults instance containing the results.\\n\\n        Notes\\n        -----\\n        The covariance structure is not updated as the fixed effects\\n        parameters are varied.\\n\\n        The algorithm used here for L1 regularization is a\"shooting\"\\n        or cyclic coordinate descent algorithm.\\n\\n        If method is \\'l1\\', then `fe_pen` and `cov_pen` are used to\\n        obtain the covariance structure, but are ignored during the\\n        L1-penalized fitting.\\n\\n        References\\n        ----------\\n        Friedman, J. H., Hastie, T. and Tibshirani, R. Regularized\\n        Paths for Generalized Linear Models via Coordinate\\n        Descent. Journal of Statistical Software, 33(1) (2008)\\n        http://www.jstatsoft.org/v33/i01/paper\\n\\n        http://statweb.stanford.edu/~tibs/stat315a/Supplements/fuse.pdf\\n        '\n    if isinstance(method, str) and method.lower() != 'l1':\n        raise ValueError('Invalid regularization method')\n    if isinstance(method, Penalty):\n        method.alpha = alpha\n        fit_kwargs.update({'fe_pen': method})\n        return self.fit(**fit_kwargs)\n    if np.isscalar(alpha):\n        alpha = alpha * np.ones(self.k_fe, dtype=np.float64)\n    mdf = self.fit(**fit_kwargs)\n    fe_params = mdf.fe_params\n    cov_re = mdf.cov_re\n    vcomp = mdf.vcomp\n    scale = mdf.scale\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = None\n    for itr in range(maxit):\n        fe_params_s = fe_params.copy()\n        for j in range(self.k_fe):\n            if abs(fe_params[j]) < ceps:\n                continue\n            fe_params[j] = 0.0\n            expval = np.dot(self.exog, fe_params)\n            resid_all = self.endog - expval\n            (a, b) = (0.0, 0.0)\n            for (group_ix, group) in enumerate(self.group_labels):\n                vc_var = self._expand_vcomp(vcomp, group_ix)\n                exog = self.exog_li[group_ix]\n                (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n                resid = resid_all[self.row_indices[group]]\n                solver = _smw_solver(scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n                x = exog[:, j]\n                u = solver(x)\n                a += np.dot(u, x)\n                b -= 2 * np.dot(u, resid)\n            pwt1 = alpha[j]\n            if b > pwt1:\n                fe_params[j] = -(b - pwt1) / (2 * a)\n            elif b < -pwt1:\n                fe_params[j] = -(b + pwt1) / (2 * a)\n        if np.abs(fe_params_s - fe_params).max() < ptol:\n            break\n    params_prof = mdf.params.copy()\n    params_prof[0:self.k_fe] = fe_params\n    scale = self.get_scale(fe_params, mdf.cov_re_unscaled, mdf.vcomp)\n    (hess, sing) = self.hessian(params_prof)\n    if sing:\n        warnings.warn(_warn_cov_sing)\n    pcov = np.nan * np.ones_like(hess)\n    ii = np.abs(params_prof) > ceps\n    ii[self.k_fe:] = True\n    ii = np.flatnonzero(ii)\n    hess1 = hess[ii, :][:, ii]\n    pcov[np.ix_(ii, ii)] = np.linalg.inv(-hess1)\n    params_object = MixedLMParams.from_components(fe_params, cov_re=cov_re)\n    results = MixedLMResults(self, params_prof, pcov / scale)\n    results.params_object = params_object\n    results.fe_params = fe_params\n    results.cov_re = cov_re\n    results.vcomp = vcomp\n    results.scale = scale\n    results.cov_re_unscaled = mdf.cov_re_unscaled\n    results.method = mdf.method\n    results.converged = True\n    results.cov_pen = self.cov_pen\n    results.k_fe = self.k_fe\n    results.k_re = self.k_re\n    results.k_re2 = self.k_re2\n    results.k_vc = self.k_vc\n    return MixedLMResultsWrapper(results)",
            "def fit_regularized(self, start_params=None, method='l1', alpha=0, ceps=0.0001, ptol=1e-06, maxit=200, **fit_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Fit a model in which the fixed effects parameters are\\n        penalized.  The dependence parameters are held fixed at their\\n        estimated values in the unpenalized model.\\n\\n        Parameters\\n        ----------\\n        method : str of Penalty object\\n            Method for regularization.  If a string, must be \\'l1\\'.\\n        alpha : array_like\\n            Scalar or vector of penalty weights.  If a scalar, the\\n            same weight is applied to all coefficients; if a vector,\\n            it contains a weight for each coefficient.  If method is a\\n            Penalty object, the weights are scaled by alpha.  For L1\\n            regularization, the weights are used directly.\\n        ceps : positive real scalar\\n            Fixed effects parameters smaller than this value\\n            in magnitude are treated as being zero.\\n        ptol : positive real scalar\\n            Convergence occurs when the sup norm difference\\n            between successive values of `fe_params` is less than\\n            `ptol`.\\n        maxit : int\\n            The maximum number of iterations.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        A MixedLMResults instance containing the results.\\n\\n        Notes\\n        -----\\n        The covariance structure is not updated as the fixed effects\\n        parameters are varied.\\n\\n        The algorithm used here for L1 regularization is a\"shooting\"\\n        or cyclic coordinate descent algorithm.\\n\\n        If method is \\'l1\\', then `fe_pen` and `cov_pen` are used to\\n        obtain the covariance structure, but are ignored during the\\n        L1-penalized fitting.\\n\\n        References\\n        ----------\\n        Friedman, J. H., Hastie, T. and Tibshirani, R. Regularized\\n        Paths for Generalized Linear Models via Coordinate\\n        Descent. Journal of Statistical Software, 33(1) (2008)\\n        http://www.jstatsoft.org/v33/i01/paper\\n\\n        http://statweb.stanford.edu/~tibs/stat315a/Supplements/fuse.pdf\\n        '\n    if isinstance(method, str) and method.lower() != 'l1':\n        raise ValueError('Invalid regularization method')\n    if isinstance(method, Penalty):\n        method.alpha = alpha\n        fit_kwargs.update({'fe_pen': method})\n        return self.fit(**fit_kwargs)\n    if np.isscalar(alpha):\n        alpha = alpha * np.ones(self.k_fe, dtype=np.float64)\n    mdf = self.fit(**fit_kwargs)\n    fe_params = mdf.fe_params\n    cov_re = mdf.cov_re\n    vcomp = mdf.vcomp\n    scale = mdf.scale\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = None\n    for itr in range(maxit):\n        fe_params_s = fe_params.copy()\n        for j in range(self.k_fe):\n            if abs(fe_params[j]) < ceps:\n                continue\n            fe_params[j] = 0.0\n            expval = np.dot(self.exog, fe_params)\n            resid_all = self.endog - expval\n            (a, b) = (0.0, 0.0)\n            for (group_ix, group) in enumerate(self.group_labels):\n                vc_var = self._expand_vcomp(vcomp, group_ix)\n                exog = self.exog_li[group_ix]\n                (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n                resid = resid_all[self.row_indices[group]]\n                solver = _smw_solver(scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n                x = exog[:, j]\n                u = solver(x)\n                a += np.dot(u, x)\n                b -= 2 * np.dot(u, resid)\n            pwt1 = alpha[j]\n            if b > pwt1:\n                fe_params[j] = -(b - pwt1) / (2 * a)\n            elif b < -pwt1:\n                fe_params[j] = -(b + pwt1) / (2 * a)\n        if np.abs(fe_params_s - fe_params).max() < ptol:\n            break\n    params_prof = mdf.params.copy()\n    params_prof[0:self.k_fe] = fe_params\n    scale = self.get_scale(fe_params, mdf.cov_re_unscaled, mdf.vcomp)\n    (hess, sing) = self.hessian(params_prof)\n    if sing:\n        warnings.warn(_warn_cov_sing)\n    pcov = np.nan * np.ones_like(hess)\n    ii = np.abs(params_prof) > ceps\n    ii[self.k_fe:] = True\n    ii = np.flatnonzero(ii)\n    hess1 = hess[ii, :][:, ii]\n    pcov[np.ix_(ii, ii)] = np.linalg.inv(-hess1)\n    params_object = MixedLMParams.from_components(fe_params, cov_re=cov_re)\n    results = MixedLMResults(self, params_prof, pcov / scale)\n    results.params_object = params_object\n    results.fe_params = fe_params\n    results.cov_re = cov_re\n    results.vcomp = vcomp\n    results.scale = scale\n    results.cov_re_unscaled = mdf.cov_re_unscaled\n    results.method = mdf.method\n    results.converged = True\n    results.cov_pen = self.cov_pen\n    results.k_fe = self.k_fe\n    results.k_re = self.k_re\n    results.k_re2 = self.k_re2\n    results.k_vc = self.k_vc\n    return MixedLMResultsWrapper(results)",
            "def fit_regularized(self, start_params=None, method='l1', alpha=0, ceps=0.0001, ptol=1e-06, maxit=200, **fit_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Fit a model in which the fixed effects parameters are\\n        penalized.  The dependence parameters are held fixed at their\\n        estimated values in the unpenalized model.\\n\\n        Parameters\\n        ----------\\n        method : str of Penalty object\\n            Method for regularization.  If a string, must be \\'l1\\'.\\n        alpha : array_like\\n            Scalar or vector of penalty weights.  If a scalar, the\\n            same weight is applied to all coefficients; if a vector,\\n            it contains a weight for each coefficient.  If method is a\\n            Penalty object, the weights are scaled by alpha.  For L1\\n            regularization, the weights are used directly.\\n        ceps : positive real scalar\\n            Fixed effects parameters smaller than this value\\n            in magnitude are treated as being zero.\\n        ptol : positive real scalar\\n            Convergence occurs when the sup norm difference\\n            between successive values of `fe_params` is less than\\n            `ptol`.\\n        maxit : int\\n            The maximum number of iterations.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        A MixedLMResults instance containing the results.\\n\\n        Notes\\n        -----\\n        The covariance structure is not updated as the fixed effects\\n        parameters are varied.\\n\\n        The algorithm used here for L1 regularization is a\"shooting\"\\n        or cyclic coordinate descent algorithm.\\n\\n        If method is \\'l1\\', then `fe_pen` and `cov_pen` are used to\\n        obtain the covariance structure, but are ignored during the\\n        L1-penalized fitting.\\n\\n        References\\n        ----------\\n        Friedman, J. H., Hastie, T. and Tibshirani, R. Regularized\\n        Paths for Generalized Linear Models via Coordinate\\n        Descent. Journal of Statistical Software, 33(1) (2008)\\n        http://www.jstatsoft.org/v33/i01/paper\\n\\n        http://statweb.stanford.edu/~tibs/stat315a/Supplements/fuse.pdf\\n        '\n    if isinstance(method, str) and method.lower() != 'l1':\n        raise ValueError('Invalid regularization method')\n    if isinstance(method, Penalty):\n        method.alpha = alpha\n        fit_kwargs.update({'fe_pen': method})\n        return self.fit(**fit_kwargs)\n    if np.isscalar(alpha):\n        alpha = alpha * np.ones(self.k_fe, dtype=np.float64)\n    mdf = self.fit(**fit_kwargs)\n    fe_params = mdf.fe_params\n    cov_re = mdf.cov_re\n    vcomp = mdf.vcomp\n    scale = mdf.scale\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = None\n    for itr in range(maxit):\n        fe_params_s = fe_params.copy()\n        for j in range(self.k_fe):\n            if abs(fe_params[j]) < ceps:\n                continue\n            fe_params[j] = 0.0\n            expval = np.dot(self.exog, fe_params)\n            resid_all = self.endog - expval\n            (a, b) = (0.0, 0.0)\n            for (group_ix, group) in enumerate(self.group_labels):\n                vc_var = self._expand_vcomp(vcomp, group_ix)\n                exog = self.exog_li[group_ix]\n                (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n                resid = resid_all[self.row_indices[group]]\n                solver = _smw_solver(scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n                x = exog[:, j]\n                u = solver(x)\n                a += np.dot(u, x)\n                b -= 2 * np.dot(u, resid)\n            pwt1 = alpha[j]\n            if b > pwt1:\n                fe_params[j] = -(b - pwt1) / (2 * a)\n            elif b < -pwt1:\n                fe_params[j] = -(b + pwt1) / (2 * a)\n        if np.abs(fe_params_s - fe_params).max() < ptol:\n            break\n    params_prof = mdf.params.copy()\n    params_prof[0:self.k_fe] = fe_params\n    scale = self.get_scale(fe_params, mdf.cov_re_unscaled, mdf.vcomp)\n    (hess, sing) = self.hessian(params_prof)\n    if sing:\n        warnings.warn(_warn_cov_sing)\n    pcov = np.nan * np.ones_like(hess)\n    ii = np.abs(params_prof) > ceps\n    ii[self.k_fe:] = True\n    ii = np.flatnonzero(ii)\n    hess1 = hess[ii, :][:, ii]\n    pcov[np.ix_(ii, ii)] = np.linalg.inv(-hess1)\n    params_object = MixedLMParams.from_components(fe_params, cov_re=cov_re)\n    results = MixedLMResults(self, params_prof, pcov / scale)\n    results.params_object = params_object\n    results.fe_params = fe_params\n    results.cov_re = cov_re\n    results.vcomp = vcomp\n    results.scale = scale\n    results.cov_re_unscaled = mdf.cov_re_unscaled\n    results.method = mdf.method\n    results.converged = True\n    results.cov_pen = self.cov_pen\n    results.k_fe = self.k_fe\n    results.k_re = self.k_re\n    results.k_re2 = self.k_re2\n    results.k_vc = self.k_vc\n    return MixedLMResultsWrapper(results)",
            "def fit_regularized(self, start_params=None, method='l1', alpha=0, ceps=0.0001, ptol=1e-06, maxit=200, **fit_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Fit a model in which the fixed effects parameters are\\n        penalized.  The dependence parameters are held fixed at their\\n        estimated values in the unpenalized model.\\n\\n        Parameters\\n        ----------\\n        method : str of Penalty object\\n            Method for regularization.  If a string, must be \\'l1\\'.\\n        alpha : array_like\\n            Scalar or vector of penalty weights.  If a scalar, the\\n            same weight is applied to all coefficients; if a vector,\\n            it contains a weight for each coefficient.  If method is a\\n            Penalty object, the weights are scaled by alpha.  For L1\\n            regularization, the weights are used directly.\\n        ceps : positive real scalar\\n            Fixed effects parameters smaller than this value\\n            in magnitude are treated as being zero.\\n        ptol : positive real scalar\\n            Convergence occurs when the sup norm difference\\n            between successive values of `fe_params` is less than\\n            `ptol`.\\n        maxit : int\\n            The maximum number of iterations.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        A MixedLMResults instance containing the results.\\n\\n        Notes\\n        -----\\n        The covariance structure is not updated as the fixed effects\\n        parameters are varied.\\n\\n        The algorithm used here for L1 regularization is a\"shooting\"\\n        or cyclic coordinate descent algorithm.\\n\\n        If method is \\'l1\\', then `fe_pen` and `cov_pen` are used to\\n        obtain the covariance structure, but are ignored during the\\n        L1-penalized fitting.\\n\\n        References\\n        ----------\\n        Friedman, J. H., Hastie, T. and Tibshirani, R. Regularized\\n        Paths for Generalized Linear Models via Coordinate\\n        Descent. Journal of Statistical Software, 33(1) (2008)\\n        http://www.jstatsoft.org/v33/i01/paper\\n\\n        http://statweb.stanford.edu/~tibs/stat315a/Supplements/fuse.pdf\\n        '\n    if isinstance(method, str) and method.lower() != 'l1':\n        raise ValueError('Invalid regularization method')\n    if isinstance(method, Penalty):\n        method.alpha = alpha\n        fit_kwargs.update({'fe_pen': method})\n        return self.fit(**fit_kwargs)\n    if np.isscalar(alpha):\n        alpha = alpha * np.ones(self.k_fe, dtype=np.float64)\n    mdf = self.fit(**fit_kwargs)\n    fe_params = mdf.fe_params\n    cov_re = mdf.cov_re\n    vcomp = mdf.vcomp\n    scale = mdf.scale\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = None\n    for itr in range(maxit):\n        fe_params_s = fe_params.copy()\n        for j in range(self.k_fe):\n            if abs(fe_params[j]) < ceps:\n                continue\n            fe_params[j] = 0.0\n            expval = np.dot(self.exog, fe_params)\n            resid_all = self.endog - expval\n            (a, b) = (0.0, 0.0)\n            for (group_ix, group) in enumerate(self.group_labels):\n                vc_var = self._expand_vcomp(vcomp, group_ix)\n                exog = self.exog_li[group_ix]\n                (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n                resid = resid_all[self.row_indices[group]]\n                solver = _smw_solver(scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n                x = exog[:, j]\n                u = solver(x)\n                a += np.dot(u, x)\n                b -= 2 * np.dot(u, resid)\n            pwt1 = alpha[j]\n            if b > pwt1:\n                fe_params[j] = -(b - pwt1) / (2 * a)\n            elif b < -pwt1:\n                fe_params[j] = -(b + pwt1) / (2 * a)\n        if np.abs(fe_params_s - fe_params).max() < ptol:\n            break\n    params_prof = mdf.params.copy()\n    params_prof[0:self.k_fe] = fe_params\n    scale = self.get_scale(fe_params, mdf.cov_re_unscaled, mdf.vcomp)\n    (hess, sing) = self.hessian(params_prof)\n    if sing:\n        warnings.warn(_warn_cov_sing)\n    pcov = np.nan * np.ones_like(hess)\n    ii = np.abs(params_prof) > ceps\n    ii[self.k_fe:] = True\n    ii = np.flatnonzero(ii)\n    hess1 = hess[ii, :][:, ii]\n    pcov[np.ix_(ii, ii)] = np.linalg.inv(-hess1)\n    params_object = MixedLMParams.from_components(fe_params, cov_re=cov_re)\n    results = MixedLMResults(self, params_prof, pcov / scale)\n    results.params_object = params_object\n    results.fe_params = fe_params\n    results.cov_re = cov_re\n    results.vcomp = vcomp\n    results.scale = scale\n    results.cov_re_unscaled = mdf.cov_re_unscaled\n    results.method = mdf.method\n    results.converged = True\n    results.cov_pen = self.cov_pen\n    results.k_fe = self.k_fe\n    results.k_re = self.k_re\n    results.k_re2 = self.k_re2\n    results.k_vc = self.k_vc\n    return MixedLMResultsWrapper(results)",
            "def fit_regularized(self, start_params=None, method='l1', alpha=0, ceps=0.0001, ptol=1e-06, maxit=200, **fit_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Fit a model in which the fixed effects parameters are\\n        penalized.  The dependence parameters are held fixed at their\\n        estimated values in the unpenalized model.\\n\\n        Parameters\\n        ----------\\n        method : str of Penalty object\\n            Method for regularization.  If a string, must be \\'l1\\'.\\n        alpha : array_like\\n            Scalar or vector of penalty weights.  If a scalar, the\\n            same weight is applied to all coefficients; if a vector,\\n            it contains a weight for each coefficient.  If method is a\\n            Penalty object, the weights are scaled by alpha.  For L1\\n            regularization, the weights are used directly.\\n        ceps : positive real scalar\\n            Fixed effects parameters smaller than this value\\n            in magnitude are treated as being zero.\\n        ptol : positive real scalar\\n            Convergence occurs when the sup norm difference\\n            between successive values of `fe_params` is less than\\n            `ptol`.\\n        maxit : int\\n            The maximum number of iterations.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        A MixedLMResults instance containing the results.\\n\\n        Notes\\n        -----\\n        The covariance structure is not updated as the fixed effects\\n        parameters are varied.\\n\\n        The algorithm used here for L1 regularization is a\"shooting\"\\n        or cyclic coordinate descent algorithm.\\n\\n        If method is \\'l1\\', then `fe_pen` and `cov_pen` are used to\\n        obtain the covariance structure, but are ignored during the\\n        L1-penalized fitting.\\n\\n        References\\n        ----------\\n        Friedman, J. H., Hastie, T. and Tibshirani, R. Regularized\\n        Paths for Generalized Linear Models via Coordinate\\n        Descent. Journal of Statistical Software, 33(1) (2008)\\n        http://www.jstatsoft.org/v33/i01/paper\\n\\n        http://statweb.stanford.edu/~tibs/stat315a/Supplements/fuse.pdf\\n        '\n    if isinstance(method, str) and method.lower() != 'l1':\n        raise ValueError('Invalid regularization method')\n    if isinstance(method, Penalty):\n        method.alpha = alpha\n        fit_kwargs.update({'fe_pen': method})\n        return self.fit(**fit_kwargs)\n    if np.isscalar(alpha):\n        alpha = alpha * np.ones(self.k_fe, dtype=np.float64)\n    mdf = self.fit(**fit_kwargs)\n    fe_params = mdf.fe_params\n    cov_re = mdf.cov_re\n    vcomp = mdf.vcomp\n    scale = mdf.scale\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = None\n    for itr in range(maxit):\n        fe_params_s = fe_params.copy()\n        for j in range(self.k_fe):\n            if abs(fe_params[j]) < ceps:\n                continue\n            fe_params[j] = 0.0\n            expval = np.dot(self.exog, fe_params)\n            resid_all = self.endog - expval\n            (a, b) = (0.0, 0.0)\n            for (group_ix, group) in enumerate(self.group_labels):\n                vc_var = self._expand_vcomp(vcomp, group_ix)\n                exog = self.exog_li[group_ix]\n                (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n                resid = resid_all[self.row_indices[group]]\n                solver = _smw_solver(scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n                x = exog[:, j]\n                u = solver(x)\n                a += np.dot(u, x)\n                b -= 2 * np.dot(u, resid)\n            pwt1 = alpha[j]\n            if b > pwt1:\n                fe_params[j] = -(b - pwt1) / (2 * a)\n            elif b < -pwt1:\n                fe_params[j] = -(b + pwt1) / (2 * a)\n        if np.abs(fe_params_s - fe_params).max() < ptol:\n            break\n    params_prof = mdf.params.copy()\n    params_prof[0:self.k_fe] = fe_params\n    scale = self.get_scale(fe_params, mdf.cov_re_unscaled, mdf.vcomp)\n    (hess, sing) = self.hessian(params_prof)\n    if sing:\n        warnings.warn(_warn_cov_sing)\n    pcov = np.nan * np.ones_like(hess)\n    ii = np.abs(params_prof) > ceps\n    ii[self.k_fe:] = True\n    ii = np.flatnonzero(ii)\n    hess1 = hess[ii, :][:, ii]\n    pcov[np.ix_(ii, ii)] = np.linalg.inv(-hess1)\n    params_object = MixedLMParams.from_components(fe_params, cov_re=cov_re)\n    results = MixedLMResults(self, params_prof, pcov / scale)\n    results.params_object = params_object\n    results.fe_params = fe_params\n    results.cov_re = cov_re\n    results.vcomp = vcomp\n    results.scale = scale\n    results.cov_re_unscaled = mdf.cov_re_unscaled\n    results.method = mdf.method\n    results.converged = True\n    results.cov_pen = self.cov_pen\n    results.k_fe = self.k_fe\n    results.k_re = self.k_re\n    results.k_re2 = self.k_re2\n    results.k_vc = self.k_vc\n    return MixedLMResultsWrapper(results)"
        ]
    },
    {
        "func_name": "get_fe_params",
        "original": "def get_fe_params(self, cov_re, vcomp, tol=1e-10):\n    \"\"\"\n        Use GLS to update the fixed effects parameter estimates.\n\n        Parameters\n        ----------\n        cov_re : array_like (2d)\n            The covariance matrix of the random effects.\n        vcomp : array_like (1d)\n            The variance components.\n        tol : float\n            A tolerance parameter to determine when covariances\n            are singular.\n\n        Returns\n        -------\n        params : ndarray\n            The GLS estimates of the fixed effects parameters.\n        singular : bool\n            True if the covariance is singular\n        \"\"\"\n    if self.k_fe == 0:\n        return (np.array([]), False)\n    sing = False\n    if self.k_re == 0:\n        cov_re_inv = np.empty((0, 0))\n    else:\n        (w, v) = np.linalg.eigh(cov_re)\n        if w.min() < tol:\n            sing = True\n            ii = np.flatnonzero(w >= tol)\n            if len(ii) == 0:\n                cov_re_inv = np.zeros_like(cov_re)\n            else:\n                vi = v[:, ii]\n                wi = w[ii]\n                cov_re_inv = np.dot(vi / wi, vi.T)\n        else:\n            cov_re_inv = np.linalg.inv(cov_re)\n    if not hasattr(self, '_endex_li'):\n        self._endex_li = []\n        for (group_ix, _) in enumerate(self.group_labels):\n            mat = np.concatenate((self.exog_li[group_ix], self.endog_li[group_ix][:, None]), axis=1)\n            self._endex_li.append(mat)\n    xtxy = 0.0\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        if vc_var.size > 0:\n            if vc_var.min() < tol:\n                sing = True\n                ii = np.flatnonzero(vc_var >= tol)\n                vc_vari = np.zeros_like(vc_var)\n                vc_vari[ii] = 1 / vc_var[ii]\n            else:\n                vc_vari = 1 / vc_var\n        else:\n            vc_vari = np.empty(0)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, vc_vari)\n        u = solver(self._endex_li[group_ix])\n        xtxy += np.dot(exog.T, u)\n    if sing:\n        fe_params = np.dot(np.linalg.pinv(xtxy[:, 0:-1]), xtxy[:, -1])\n    else:\n        fe_params = np.linalg.solve(xtxy[:, 0:-1], xtxy[:, -1])\n    return (fe_params, sing)",
        "mutated": [
            "def get_fe_params(self, cov_re, vcomp, tol=1e-10):\n    if False:\n        i = 10\n    '\\n        Use GLS to update the fixed effects parameter estimates.\\n\\n        Parameters\\n        ----------\\n        cov_re : array_like (2d)\\n            The covariance matrix of the random effects.\\n        vcomp : array_like (1d)\\n            The variance components.\\n        tol : float\\n            A tolerance parameter to determine when covariances\\n            are singular.\\n\\n        Returns\\n        -------\\n        params : ndarray\\n            The GLS estimates of the fixed effects parameters.\\n        singular : bool\\n            True if the covariance is singular\\n        '\n    if self.k_fe == 0:\n        return (np.array([]), False)\n    sing = False\n    if self.k_re == 0:\n        cov_re_inv = np.empty((0, 0))\n    else:\n        (w, v) = np.linalg.eigh(cov_re)\n        if w.min() < tol:\n            sing = True\n            ii = np.flatnonzero(w >= tol)\n            if len(ii) == 0:\n                cov_re_inv = np.zeros_like(cov_re)\n            else:\n                vi = v[:, ii]\n                wi = w[ii]\n                cov_re_inv = np.dot(vi / wi, vi.T)\n        else:\n            cov_re_inv = np.linalg.inv(cov_re)\n    if not hasattr(self, '_endex_li'):\n        self._endex_li = []\n        for (group_ix, _) in enumerate(self.group_labels):\n            mat = np.concatenate((self.exog_li[group_ix], self.endog_li[group_ix][:, None]), axis=1)\n            self._endex_li.append(mat)\n    xtxy = 0.0\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        if vc_var.size > 0:\n            if vc_var.min() < tol:\n                sing = True\n                ii = np.flatnonzero(vc_var >= tol)\n                vc_vari = np.zeros_like(vc_var)\n                vc_vari[ii] = 1 / vc_var[ii]\n            else:\n                vc_vari = 1 / vc_var\n        else:\n            vc_vari = np.empty(0)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, vc_vari)\n        u = solver(self._endex_li[group_ix])\n        xtxy += np.dot(exog.T, u)\n    if sing:\n        fe_params = np.dot(np.linalg.pinv(xtxy[:, 0:-1]), xtxy[:, -1])\n    else:\n        fe_params = np.linalg.solve(xtxy[:, 0:-1], xtxy[:, -1])\n    return (fe_params, sing)",
            "def get_fe_params(self, cov_re, vcomp, tol=1e-10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Use GLS to update the fixed effects parameter estimates.\\n\\n        Parameters\\n        ----------\\n        cov_re : array_like (2d)\\n            The covariance matrix of the random effects.\\n        vcomp : array_like (1d)\\n            The variance components.\\n        tol : float\\n            A tolerance parameter to determine when covariances\\n            are singular.\\n\\n        Returns\\n        -------\\n        params : ndarray\\n            The GLS estimates of the fixed effects parameters.\\n        singular : bool\\n            True if the covariance is singular\\n        '\n    if self.k_fe == 0:\n        return (np.array([]), False)\n    sing = False\n    if self.k_re == 0:\n        cov_re_inv = np.empty((0, 0))\n    else:\n        (w, v) = np.linalg.eigh(cov_re)\n        if w.min() < tol:\n            sing = True\n            ii = np.flatnonzero(w >= tol)\n            if len(ii) == 0:\n                cov_re_inv = np.zeros_like(cov_re)\n            else:\n                vi = v[:, ii]\n                wi = w[ii]\n                cov_re_inv = np.dot(vi / wi, vi.T)\n        else:\n            cov_re_inv = np.linalg.inv(cov_re)\n    if not hasattr(self, '_endex_li'):\n        self._endex_li = []\n        for (group_ix, _) in enumerate(self.group_labels):\n            mat = np.concatenate((self.exog_li[group_ix], self.endog_li[group_ix][:, None]), axis=1)\n            self._endex_li.append(mat)\n    xtxy = 0.0\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        if vc_var.size > 0:\n            if vc_var.min() < tol:\n                sing = True\n                ii = np.flatnonzero(vc_var >= tol)\n                vc_vari = np.zeros_like(vc_var)\n                vc_vari[ii] = 1 / vc_var[ii]\n            else:\n                vc_vari = 1 / vc_var\n        else:\n            vc_vari = np.empty(0)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, vc_vari)\n        u = solver(self._endex_li[group_ix])\n        xtxy += np.dot(exog.T, u)\n    if sing:\n        fe_params = np.dot(np.linalg.pinv(xtxy[:, 0:-1]), xtxy[:, -1])\n    else:\n        fe_params = np.linalg.solve(xtxy[:, 0:-1], xtxy[:, -1])\n    return (fe_params, sing)",
            "def get_fe_params(self, cov_re, vcomp, tol=1e-10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Use GLS to update the fixed effects parameter estimates.\\n\\n        Parameters\\n        ----------\\n        cov_re : array_like (2d)\\n            The covariance matrix of the random effects.\\n        vcomp : array_like (1d)\\n            The variance components.\\n        tol : float\\n            A tolerance parameter to determine when covariances\\n            are singular.\\n\\n        Returns\\n        -------\\n        params : ndarray\\n            The GLS estimates of the fixed effects parameters.\\n        singular : bool\\n            True if the covariance is singular\\n        '\n    if self.k_fe == 0:\n        return (np.array([]), False)\n    sing = False\n    if self.k_re == 0:\n        cov_re_inv = np.empty((0, 0))\n    else:\n        (w, v) = np.linalg.eigh(cov_re)\n        if w.min() < tol:\n            sing = True\n            ii = np.flatnonzero(w >= tol)\n            if len(ii) == 0:\n                cov_re_inv = np.zeros_like(cov_re)\n            else:\n                vi = v[:, ii]\n                wi = w[ii]\n                cov_re_inv = np.dot(vi / wi, vi.T)\n        else:\n            cov_re_inv = np.linalg.inv(cov_re)\n    if not hasattr(self, '_endex_li'):\n        self._endex_li = []\n        for (group_ix, _) in enumerate(self.group_labels):\n            mat = np.concatenate((self.exog_li[group_ix], self.endog_li[group_ix][:, None]), axis=1)\n            self._endex_li.append(mat)\n    xtxy = 0.0\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        if vc_var.size > 0:\n            if vc_var.min() < tol:\n                sing = True\n                ii = np.flatnonzero(vc_var >= tol)\n                vc_vari = np.zeros_like(vc_var)\n                vc_vari[ii] = 1 / vc_var[ii]\n            else:\n                vc_vari = 1 / vc_var\n        else:\n            vc_vari = np.empty(0)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, vc_vari)\n        u = solver(self._endex_li[group_ix])\n        xtxy += np.dot(exog.T, u)\n    if sing:\n        fe_params = np.dot(np.linalg.pinv(xtxy[:, 0:-1]), xtxy[:, -1])\n    else:\n        fe_params = np.linalg.solve(xtxy[:, 0:-1], xtxy[:, -1])\n    return (fe_params, sing)",
            "def get_fe_params(self, cov_re, vcomp, tol=1e-10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Use GLS to update the fixed effects parameter estimates.\\n\\n        Parameters\\n        ----------\\n        cov_re : array_like (2d)\\n            The covariance matrix of the random effects.\\n        vcomp : array_like (1d)\\n            The variance components.\\n        tol : float\\n            A tolerance parameter to determine when covariances\\n            are singular.\\n\\n        Returns\\n        -------\\n        params : ndarray\\n            The GLS estimates of the fixed effects parameters.\\n        singular : bool\\n            True if the covariance is singular\\n        '\n    if self.k_fe == 0:\n        return (np.array([]), False)\n    sing = False\n    if self.k_re == 0:\n        cov_re_inv = np.empty((0, 0))\n    else:\n        (w, v) = np.linalg.eigh(cov_re)\n        if w.min() < tol:\n            sing = True\n            ii = np.flatnonzero(w >= tol)\n            if len(ii) == 0:\n                cov_re_inv = np.zeros_like(cov_re)\n            else:\n                vi = v[:, ii]\n                wi = w[ii]\n                cov_re_inv = np.dot(vi / wi, vi.T)\n        else:\n            cov_re_inv = np.linalg.inv(cov_re)\n    if not hasattr(self, '_endex_li'):\n        self._endex_li = []\n        for (group_ix, _) in enumerate(self.group_labels):\n            mat = np.concatenate((self.exog_li[group_ix], self.endog_li[group_ix][:, None]), axis=1)\n            self._endex_li.append(mat)\n    xtxy = 0.0\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        if vc_var.size > 0:\n            if vc_var.min() < tol:\n                sing = True\n                ii = np.flatnonzero(vc_var >= tol)\n                vc_vari = np.zeros_like(vc_var)\n                vc_vari[ii] = 1 / vc_var[ii]\n            else:\n                vc_vari = 1 / vc_var\n        else:\n            vc_vari = np.empty(0)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, vc_vari)\n        u = solver(self._endex_li[group_ix])\n        xtxy += np.dot(exog.T, u)\n    if sing:\n        fe_params = np.dot(np.linalg.pinv(xtxy[:, 0:-1]), xtxy[:, -1])\n    else:\n        fe_params = np.linalg.solve(xtxy[:, 0:-1], xtxy[:, -1])\n    return (fe_params, sing)",
            "def get_fe_params(self, cov_re, vcomp, tol=1e-10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Use GLS to update the fixed effects parameter estimates.\\n\\n        Parameters\\n        ----------\\n        cov_re : array_like (2d)\\n            The covariance matrix of the random effects.\\n        vcomp : array_like (1d)\\n            The variance components.\\n        tol : float\\n            A tolerance parameter to determine when covariances\\n            are singular.\\n\\n        Returns\\n        -------\\n        params : ndarray\\n            The GLS estimates of the fixed effects parameters.\\n        singular : bool\\n            True if the covariance is singular\\n        '\n    if self.k_fe == 0:\n        return (np.array([]), False)\n    sing = False\n    if self.k_re == 0:\n        cov_re_inv = np.empty((0, 0))\n    else:\n        (w, v) = np.linalg.eigh(cov_re)\n        if w.min() < tol:\n            sing = True\n            ii = np.flatnonzero(w >= tol)\n            if len(ii) == 0:\n                cov_re_inv = np.zeros_like(cov_re)\n            else:\n                vi = v[:, ii]\n                wi = w[ii]\n                cov_re_inv = np.dot(vi / wi, vi.T)\n        else:\n            cov_re_inv = np.linalg.inv(cov_re)\n    if not hasattr(self, '_endex_li'):\n        self._endex_li = []\n        for (group_ix, _) in enumerate(self.group_labels):\n            mat = np.concatenate((self.exog_li[group_ix], self.endog_li[group_ix][:, None]), axis=1)\n            self._endex_li.append(mat)\n    xtxy = 0.0\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        if vc_var.size > 0:\n            if vc_var.min() < tol:\n                sing = True\n                ii = np.flatnonzero(vc_var >= tol)\n                vc_vari = np.zeros_like(vc_var)\n                vc_vari[ii] = 1 / vc_var[ii]\n            else:\n                vc_vari = 1 / vc_var\n        else:\n            vc_vari = np.empty(0)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, vc_vari)\n        u = solver(self._endex_li[group_ix])\n        xtxy += np.dot(exog.T, u)\n    if sing:\n        fe_params = np.dot(np.linalg.pinv(xtxy[:, 0:-1]), xtxy[:, -1])\n    else:\n        fe_params = np.linalg.solve(xtxy[:, 0:-1], xtxy[:, -1])\n    return (fe_params, sing)"
        ]
    },
    {
        "func_name": "_reparam",
        "original": "def _reparam(self):\n    \"\"\"\n        Returns parameters of the map converting parameters from the\n        form used in optimization to the form returned to the user.\n\n        Returns\n        -------\n        lin : list-like\n            Linear terms of the map\n        quad : list-like\n            Quadratic terms of the map\n\n        Notes\n        -----\n        If P are the standard form parameters and R are the\n        transformed parameters (i.e. with the Cholesky square root\n        covariance and square root transformed variance components),\n        then P[i] = lin[i] * R + R' * quad[i] * R\n        \"\"\"\n    (k_fe, k_re, k_re2, k_vc) = (self.k_fe, self.k_re, self.k_re2, self.k_vc)\n    k_tot = k_fe + k_re2 + k_vc\n    ix = np.tril_indices(self.k_re)\n    lin = []\n    for k in range(k_fe):\n        e = np.zeros(k_tot)\n        e[k] = 1\n        lin.append(e)\n    for k in range(k_re2):\n        lin.append(np.zeros(k_tot))\n    for k in range(k_vc):\n        lin.append(np.zeros(k_tot))\n    quad = []\n    for k in range(k_tot):\n        quad.append(np.zeros((k_tot, k_tot)))\n    ii = np.tril_indices(k_re)\n    ix = [(a, b) for (a, b) in zip(ii[0], ii[1])]\n    for i1 in range(k_re2):\n        for i2 in range(k_re2):\n            ix1 = ix[i1]\n            ix2 = ix[i2]\n            if ix1[1] == ix2[1] and ix1[0] <= ix2[0]:\n                ii = (ix2[0], ix1[0])\n                k = ix.index(ii)\n                quad[k_fe + k][k_fe + i2, k_fe + i1] += 1\n    for k in range(k_tot):\n        quad[k] = 0.5 * (quad[k] + quad[k].T)\n    km = k_fe + k_re2\n    for k in range(km, km + k_vc):\n        quad[k][k, k] = 1\n    return (lin, quad)",
        "mutated": [
            "def _reparam(self):\n    if False:\n        i = 10\n    \"\\n        Returns parameters of the map converting parameters from the\\n        form used in optimization to the form returned to the user.\\n\\n        Returns\\n        -------\\n        lin : list-like\\n            Linear terms of the map\\n        quad : list-like\\n            Quadratic terms of the map\\n\\n        Notes\\n        -----\\n        If P are the standard form parameters and R are the\\n        transformed parameters (i.e. with the Cholesky square root\\n        covariance and square root transformed variance components),\\n        then P[i] = lin[i] * R + R' * quad[i] * R\\n        \"\n    (k_fe, k_re, k_re2, k_vc) = (self.k_fe, self.k_re, self.k_re2, self.k_vc)\n    k_tot = k_fe + k_re2 + k_vc\n    ix = np.tril_indices(self.k_re)\n    lin = []\n    for k in range(k_fe):\n        e = np.zeros(k_tot)\n        e[k] = 1\n        lin.append(e)\n    for k in range(k_re2):\n        lin.append(np.zeros(k_tot))\n    for k in range(k_vc):\n        lin.append(np.zeros(k_tot))\n    quad = []\n    for k in range(k_tot):\n        quad.append(np.zeros((k_tot, k_tot)))\n    ii = np.tril_indices(k_re)\n    ix = [(a, b) for (a, b) in zip(ii[0], ii[1])]\n    for i1 in range(k_re2):\n        for i2 in range(k_re2):\n            ix1 = ix[i1]\n            ix2 = ix[i2]\n            if ix1[1] == ix2[1] and ix1[0] <= ix2[0]:\n                ii = (ix2[0], ix1[0])\n                k = ix.index(ii)\n                quad[k_fe + k][k_fe + i2, k_fe + i1] += 1\n    for k in range(k_tot):\n        quad[k] = 0.5 * (quad[k] + quad[k].T)\n    km = k_fe + k_re2\n    for k in range(km, km + k_vc):\n        quad[k][k, k] = 1\n    return (lin, quad)",
            "def _reparam(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Returns parameters of the map converting parameters from the\\n        form used in optimization to the form returned to the user.\\n\\n        Returns\\n        -------\\n        lin : list-like\\n            Linear terms of the map\\n        quad : list-like\\n            Quadratic terms of the map\\n\\n        Notes\\n        -----\\n        If P are the standard form parameters and R are the\\n        transformed parameters (i.e. with the Cholesky square root\\n        covariance and square root transformed variance components),\\n        then P[i] = lin[i] * R + R' * quad[i] * R\\n        \"\n    (k_fe, k_re, k_re2, k_vc) = (self.k_fe, self.k_re, self.k_re2, self.k_vc)\n    k_tot = k_fe + k_re2 + k_vc\n    ix = np.tril_indices(self.k_re)\n    lin = []\n    for k in range(k_fe):\n        e = np.zeros(k_tot)\n        e[k] = 1\n        lin.append(e)\n    for k in range(k_re2):\n        lin.append(np.zeros(k_tot))\n    for k in range(k_vc):\n        lin.append(np.zeros(k_tot))\n    quad = []\n    for k in range(k_tot):\n        quad.append(np.zeros((k_tot, k_tot)))\n    ii = np.tril_indices(k_re)\n    ix = [(a, b) for (a, b) in zip(ii[0], ii[1])]\n    for i1 in range(k_re2):\n        for i2 in range(k_re2):\n            ix1 = ix[i1]\n            ix2 = ix[i2]\n            if ix1[1] == ix2[1] and ix1[0] <= ix2[0]:\n                ii = (ix2[0], ix1[0])\n                k = ix.index(ii)\n                quad[k_fe + k][k_fe + i2, k_fe + i1] += 1\n    for k in range(k_tot):\n        quad[k] = 0.5 * (quad[k] + quad[k].T)\n    km = k_fe + k_re2\n    for k in range(km, km + k_vc):\n        quad[k][k, k] = 1\n    return (lin, quad)",
            "def _reparam(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Returns parameters of the map converting parameters from the\\n        form used in optimization to the form returned to the user.\\n\\n        Returns\\n        -------\\n        lin : list-like\\n            Linear terms of the map\\n        quad : list-like\\n            Quadratic terms of the map\\n\\n        Notes\\n        -----\\n        If P are the standard form parameters and R are the\\n        transformed parameters (i.e. with the Cholesky square root\\n        covariance and square root transformed variance components),\\n        then P[i] = lin[i] * R + R' * quad[i] * R\\n        \"\n    (k_fe, k_re, k_re2, k_vc) = (self.k_fe, self.k_re, self.k_re2, self.k_vc)\n    k_tot = k_fe + k_re2 + k_vc\n    ix = np.tril_indices(self.k_re)\n    lin = []\n    for k in range(k_fe):\n        e = np.zeros(k_tot)\n        e[k] = 1\n        lin.append(e)\n    for k in range(k_re2):\n        lin.append(np.zeros(k_tot))\n    for k in range(k_vc):\n        lin.append(np.zeros(k_tot))\n    quad = []\n    for k in range(k_tot):\n        quad.append(np.zeros((k_tot, k_tot)))\n    ii = np.tril_indices(k_re)\n    ix = [(a, b) for (a, b) in zip(ii[0], ii[1])]\n    for i1 in range(k_re2):\n        for i2 in range(k_re2):\n            ix1 = ix[i1]\n            ix2 = ix[i2]\n            if ix1[1] == ix2[1] and ix1[0] <= ix2[0]:\n                ii = (ix2[0], ix1[0])\n                k = ix.index(ii)\n                quad[k_fe + k][k_fe + i2, k_fe + i1] += 1\n    for k in range(k_tot):\n        quad[k] = 0.5 * (quad[k] + quad[k].T)\n    km = k_fe + k_re2\n    for k in range(km, km + k_vc):\n        quad[k][k, k] = 1\n    return (lin, quad)",
            "def _reparam(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Returns parameters of the map converting parameters from the\\n        form used in optimization to the form returned to the user.\\n\\n        Returns\\n        -------\\n        lin : list-like\\n            Linear terms of the map\\n        quad : list-like\\n            Quadratic terms of the map\\n\\n        Notes\\n        -----\\n        If P are the standard form parameters and R are the\\n        transformed parameters (i.e. with the Cholesky square root\\n        covariance and square root transformed variance components),\\n        then P[i] = lin[i] * R + R' * quad[i] * R\\n        \"\n    (k_fe, k_re, k_re2, k_vc) = (self.k_fe, self.k_re, self.k_re2, self.k_vc)\n    k_tot = k_fe + k_re2 + k_vc\n    ix = np.tril_indices(self.k_re)\n    lin = []\n    for k in range(k_fe):\n        e = np.zeros(k_tot)\n        e[k] = 1\n        lin.append(e)\n    for k in range(k_re2):\n        lin.append(np.zeros(k_tot))\n    for k in range(k_vc):\n        lin.append(np.zeros(k_tot))\n    quad = []\n    for k in range(k_tot):\n        quad.append(np.zeros((k_tot, k_tot)))\n    ii = np.tril_indices(k_re)\n    ix = [(a, b) for (a, b) in zip(ii[0], ii[1])]\n    for i1 in range(k_re2):\n        for i2 in range(k_re2):\n            ix1 = ix[i1]\n            ix2 = ix[i2]\n            if ix1[1] == ix2[1] and ix1[0] <= ix2[0]:\n                ii = (ix2[0], ix1[0])\n                k = ix.index(ii)\n                quad[k_fe + k][k_fe + i2, k_fe + i1] += 1\n    for k in range(k_tot):\n        quad[k] = 0.5 * (quad[k] + quad[k].T)\n    km = k_fe + k_re2\n    for k in range(km, km + k_vc):\n        quad[k][k, k] = 1\n    return (lin, quad)",
            "def _reparam(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Returns parameters of the map converting parameters from the\\n        form used in optimization to the form returned to the user.\\n\\n        Returns\\n        -------\\n        lin : list-like\\n            Linear terms of the map\\n        quad : list-like\\n            Quadratic terms of the map\\n\\n        Notes\\n        -----\\n        If P are the standard form parameters and R are the\\n        transformed parameters (i.e. with the Cholesky square root\\n        covariance and square root transformed variance components),\\n        then P[i] = lin[i] * R + R' * quad[i] * R\\n        \"\n    (k_fe, k_re, k_re2, k_vc) = (self.k_fe, self.k_re, self.k_re2, self.k_vc)\n    k_tot = k_fe + k_re2 + k_vc\n    ix = np.tril_indices(self.k_re)\n    lin = []\n    for k in range(k_fe):\n        e = np.zeros(k_tot)\n        e[k] = 1\n        lin.append(e)\n    for k in range(k_re2):\n        lin.append(np.zeros(k_tot))\n    for k in range(k_vc):\n        lin.append(np.zeros(k_tot))\n    quad = []\n    for k in range(k_tot):\n        quad.append(np.zeros((k_tot, k_tot)))\n    ii = np.tril_indices(k_re)\n    ix = [(a, b) for (a, b) in zip(ii[0], ii[1])]\n    for i1 in range(k_re2):\n        for i2 in range(k_re2):\n            ix1 = ix[i1]\n            ix2 = ix[i2]\n            if ix1[1] == ix2[1] and ix1[0] <= ix2[0]:\n                ii = (ix2[0], ix1[0])\n                k = ix.index(ii)\n                quad[k_fe + k][k_fe + i2, k_fe + i1] += 1\n    for k in range(k_tot):\n        quad[k] = 0.5 * (quad[k] + quad[k].T)\n    km = k_fe + k_re2\n    for k in range(km, km + k_vc):\n        quad[k][k, k] = 1\n    return (lin, quad)"
        ]
    },
    {
        "func_name": "_expand_vcomp",
        "original": "def _expand_vcomp(self, vcomp, group_ix):\n    \"\"\"\n        Replicate variance parameters to match a group's design.\n\n        Parameters\n        ----------\n        vcomp : array_like\n            The variance parameters for the variance components.\n        group_ix : int\n            The group index\n\n        Returns an expanded version of vcomp, in which each variance\n        parameter is copied as many times as there are independent\n        realizations of the variance component in the given group.\n        \"\"\"\n    if len(vcomp) == 0:\n        return np.empty(0)\n    vc_var = []\n    for j in range(len(self.exog_vc.names)):\n        d = self.exog_vc.mats[j][group_ix].shape[1]\n        vc_var.append(vcomp[j] * np.ones(d))\n    if len(vc_var) > 0:\n        return np.concatenate(vc_var)\n    else:\n        return np.empty(0)",
        "mutated": [
            "def _expand_vcomp(self, vcomp, group_ix):\n    if False:\n        i = 10\n    \"\\n        Replicate variance parameters to match a group's design.\\n\\n        Parameters\\n        ----------\\n        vcomp : array_like\\n            The variance parameters for the variance components.\\n        group_ix : int\\n            The group index\\n\\n        Returns an expanded version of vcomp, in which each variance\\n        parameter is copied as many times as there are independent\\n        realizations of the variance component in the given group.\\n        \"\n    if len(vcomp) == 0:\n        return np.empty(0)\n    vc_var = []\n    for j in range(len(self.exog_vc.names)):\n        d = self.exog_vc.mats[j][group_ix].shape[1]\n        vc_var.append(vcomp[j] * np.ones(d))\n    if len(vc_var) > 0:\n        return np.concatenate(vc_var)\n    else:\n        return np.empty(0)",
            "def _expand_vcomp(self, vcomp, group_ix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Replicate variance parameters to match a group's design.\\n\\n        Parameters\\n        ----------\\n        vcomp : array_like\\n            The variance parameters for the variance components.\\n        group_ix : int\\n            The group index\\n\\n        Returns an expanded version of vcomp, in which each variance\\n        parameter is copied as many times as there are independent\\n        realizations of the variance component in the given group.\\n        \"\n    if len(vcomp) == 0:\n        return np.empty(0)\n    vc_var = []\n    for j in range(len(self.exog_vc.names)):\n        d = self.exog_vc.mats[j][group_ix].shape[1]\n        vc_var.append(vcomp[j] * np.ones(d))\n    if len(vc_var) > 0:\n        return np.concatenate(vc_var)\n    else:\n        return np.empty(0)",
            "def _expand_vcomp(self, vcomp, group_ix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Replicate variance parameters to match a group's design.\\n\\n        Parameters\\n        ----------\\n        vcomp : array_like\\n            The variance parameters for the variance components.\\n        group_ix : int\\n            The group index\\n\\n        Returns an expanded version of vcomp, in which each variance\\n        parameter is copied as many times as there are independent\\n        realizations of the variance component in the given group.\\n        \"\n    if len(vcomp) == 0:\n        return np.empty(0)\n    vc_var = []\n    for j in range(len(self.exog_vc.names)):\n        d = self.exog_vc.mats[j][group_ix].shape[1]\n        vc_var.append(vcomp[j] * np.ones(d))\n    if len(vc_var) > 0:\n        return np.concatenate(vc_var)\n    else:\n        return np.empty(0)",
            "def _expand_vcomp(self, vcomp, group_ix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Replicate variance parameters to match a group's design.\\n\\n        Parameters\\n        ----------\\n        vcomp : array_like\\n            The variance parameters for the variance components.\\n        group_ix : int\\n            The group index\\n\\n        Returns an expanded version of vcomp, in which each variance\\n        parameter is copied as many times as there are independent\\n        realizations of the variance component in the given group.\\n        \"\n    if len(vcomp) == 0:\n        return np.empty(0)\n    vc_var = []\n    for j in range(len(self.exog_vc.names)):\n        d = self.exog_vc.mats[j][group_ix].shape[1]\n        vc_var.append(vcomp[j] * np.ones(d))\n    if len(vc_var) > 0:\n        return np.concatenate(vc_var)\n    else:\n        return np.empty(0)",
            "def _expand_vcomp(self, vcomp, group_ix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Replicate variance parameters to match a group's design.\\n\\n        Parameters\\n        ----------\\n        vcomp : array_like\\n            The variance parameters for the variance components.\\n        group_ix : int\\n            The group index\\n\\n        Returns an expanded version of vcomp, in which each variance\\n        parameter is copied as many times as there are independent\\n        realizations of the variance component in the given group.\\n        \"\n    if len(vcomp) == 0:\n        return np.empty(0)\n    vc_var = []\n    for j in range(len(self.exog_vc.names)):\n        d = self.exog_vc.mats[j][group_ix].shape[1]\n        vc_var.append(vcomp[j] * np.ones(d))\n    if len(vc_var) > 0:\n        return np.concatenate(vc_var)\n    else:\n        return np.empty(0)"
        ]
    },
    {
        "func_name": "_augment_exog",
        "original": "def _augment_exog(self, group_ix):\n    \"\"\"\n        Concatenate the columns for variance components to the columns\n        for other random effects to obtain a single random effects\n        exog matrix for a given group.\n        \"\"\"\n    ex_r = self.exog_re_li[group_ix] if self.k_re > 0 else None\n    if self.k_vc == 0:\n        return ex_r\n    ex = [ex_r] if self.k_re > 0 else []\n    any_sparse = False\n    for (j, _) in enumerate(self.exog_vc.names):\n        ex.append(self.exog_vc.mats[j][group_ix])\n        any_sparse |= sparse.issparse(ex[-1])\n    if any_sparse:\n        for (j, x) in enumerate(ex):\n            if not sparse.issparse(x):\n                ex[j] = sparse.csr_matrix(x)\n        ex = sparse.hstack(ex)\n        ex = sparse.csr_matrix(ex)\n    else:\n        ex = np.concatenate(ex, axis=1)\n    return ex",
        "mutated": [
            "def _augment_exog(self, group_ix):\n    if False:\n        i = 10\n    '\\n        Concatenate the columns for variance components to the columns\\n        for other random effects to obtain a single random effects\\n        exog matrix for a given group.\\n        '\n    ex_r = self.exog_re_li[group_ix] if self.k_re > 0 else None\n    if self.k_vc == 0:\n        return ex_r\n    ex = [ex_r] if self.k_re > 0 else []\n    any_sparse = False\n    for (j, _) in enumerate(self.exog_vc.names):\n        ex.append(self.exog_vc.mats[j][group_ix])\n        any_sparse |= sparse.issparse(ex[-1])\n    if any_sparse:\n        for (j, x) in enumerate(ex):\n            if not sparse.issparse(x):\n                ex[j] = sparse.csr_matrix(x)\n        ex = sparse.hstack(ex)\n        ex = sparse.csr_matrix(ex)\n    else:\n        ex = np.concatenate(ex, axis=1)\n    return ex",
            "def _augment_exog(self, group_ix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Concatenate the columns for variance components to the columns\\n        for other random effects to obtain a single random effects\\n        exog matrix for a given group.\\n        '\n    ex_r = self.exog_re_li[group_ix] if self.k_re > 0 else None\n    if self.k_vc == 0:\n        return ex_r\n    ex = [ex_r] if self.k_re > 0 else []\n    any_sparse = False\n    for (j, _) in enumerate(self.exog_vc.names):\n        ex.append(self.exog_vc.mats[j][group_ix])\n        any_sparse |= sparse.issparse(ex[-1])\n    if any_sparse:\n        for (j, x) in enumerate(ex):\n            if not sparse.issparse(x):\n                ex[j] = sparse.csr_matrix(x)\n        ex = sparse.hstack(ex)\n        ex = sparse.csr_matrix(ex)\n    else:\n        ex = np.concatenate(ex, axis=1)\n    return ex",
            "def _augment_exog(self, group_ix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Concatenate the columns for variance components to the columns\\n        for other random effects to obtain a single random effects\\n        exog matrix for a given group.\\n        '\n    ex_r = self.exog_re_li[group_ix] if self.k_re > 0 else None\n    if self.k_vc == 0:\n        return ex_r\n    ex = [ex_r] if self.k_re > 0 else []\n    any_sparse = False\n    for (j, _) in enumerate(self.exog_vc.names):\n        ex.append(self.exog_vc.mats[j][group_ix])\n        any_sparse |= sparse.issparse(ex[-1])\n    if any_sparse:\n        for (j, x) in enumerate(ex):\n            if not sparse.issparse(x):\n                ex[j] = sparse.csr_matrix(x)\n        ex = sparse.hstack(ex)\n        ex = sparse.csr_matrix(ex)\n    else:\n        ex = np.concatenate(ex, axis=1)\n    return ex",
            "def _augment_exog(self, group_ix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Concatenate the columns for variance components to the columns\\n        for other random effects to obtain a single random effects\\n        exog matrix for a given group.\\n        '\n    ex_r = self.exog_re_li[group_ix] if self.k_re > 0 else None\n    if self.k_vc == 0:\n        return ex_r\n    ex = [ex_r] if self.k_re > 0 else []\n    any_sparse = False\n    for (j, _) in enumerate(self.exog_vc.names):\n        ex.append(self.exog_vc.mats[j][group_ix])\n        any_sparse |= sparse.issparse(ex[-1])\n    if any_sparse:\n        for (j, x) in enumerate(ex):\n            if not sparse.issparse(x):\n                ex[j] = sparse.csr_matrix(x)\n        ex = sparse.hstack(ex)\n        ex = sparse.csr_matrix(ex)\n    else:\n        ex = np.concatenate(ex, axis=1)\n    return ex",
            "def _augment_exog(self, group_ix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Concatenate the columns for variance components to the columns\\n        for other random effects to obtain a single random effects\\n        exog matrix for a given group.\\n        '\n    ex_r = self.exog_re_li[group_ix] if self.k_re > 0 else None\n    if self.k_vc == 0:\n        return ex_r\n    ex = [ex_r] if self.k_re > 0 else []\n    any_sparse = False\n    for (j, _) in enumerate(self.exog_vc.names):\n        ex.append(self.exog_vc.mats[j][group_ix])\n        any_sparse |= sparse.issparse(ex[-1])\n    if any_sparse:\n        for (j, x) in enumerate(ex):\n            if not sparse.issparse(x):\n                ex[j] = sparse.csr_matrix(x)\n        ex = sparse.hstack(ex)\n        ex = sparse.csr_matrix(ex)\n    else:\n        ex = np.concatenate(ex, axis=1)\n    return ex"
        ]
    },
    {
        "func_name": "loglike",
        "original": "def loglike(self, params, profile_fe=True):\n    \"\"\"\n        Evaluate the (profile) log-likelihood of the linear mixed\n        effects model.\n\n        Parameters\n        ----------\n        params : MixedLMParams, or array_like.\n            The parameter value.  If array-like, must be a packed\n            parameter vector containing only the covariance\n            parameters.\n        profile_fe : bool\n            If True, replace the provided value of `fe_params` with\n            the GLS estimates.\n\n        Returns\n        -------\n        The log-likelihood value at `params`.\n\n        Notes\n        -----\n        The scale parameter `scale` is always profiled out of the\n        log-likelihood.  In addition, if `profile_fe` is true the\n        fixed effects parameters are also profiled out.\n        \"\"\"\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    cov_re = params.cov_re\n    vcomp = params.vcomp\n    if profile_fe:\n        (fe_params, sing) = self.get_fe_params(cov_re, vcomp)\n        if sing:\n            self._cov_sing += 1\n    else:\n        fe_params = params.fe_params\n    if self.k_re > 0:\n        try:\n            cov_re_inv = np.linalg.inv(cov_re)\n        except np.linalg.LinAlgError:\n            cov_re_inv = np.linalg.pinv(cov_re)\n            self._cov_sing += 1\n        (_, cov_re_logdet) = np.linalg.slogdet(cov_re)\n    else:\n        cov_re_inv = np.zeros((0, 0))\n        cov_re_logdet = 0\n    expval = np.dot(self.exog, fe_params)\n    resid_all = self.endog - expval\n    likeval = 0.0\n    if self.cov_pen is not None and self.k_re > 0:\n        likeval -= self.cov_pen.func(cov_re, cov_re_inv)\n    if self.fe_pen is not None:\n        likeval -= self.fe_pen.func(fe_params)\n    (xvx, qf) = (0.0, 0.0)\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        cov_aug_logdet = cov_re_logdet + np.sum(np.log(vc_var))\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = resid_all[self.row_indices[group]]\n        ld = _smw_logdet(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var, cov_aug_logdet)\n        likeval -= ld / 2.0\n        u = solver(resid)\n        qf += np.dot(resid, u)\n        if self.reml:\n            mat = solver(exog)\n            xvx += np.dot(exog.T, mat)\n    if self.reml:\n        likeval -= (self.n_totobs - self.k_fe) * np.log(qf) / 2.0\n        (_, ld) = np.linalg.slogdet(xvx)\n        likeval -= ld / 2.0\n        likeval -= (self.n_totobs - self.k_fe) * np.log(2 * np.pi) / 2.0\n        likeval += (self.n_totobs - self.k_fe) * np.log(self.n_totobs - self.k_fe) / 2.0\n        likeval -= (self.n_totobs - self.k_fe) / 2.0\n    else:\n        likeval -= self.n_totobs * np.log(qf) / 2.0\n        likeval -= self.n_totobs * np.log(2 * np.pi) / 2.0\n        likeval += self.n_totobs * np.log(self.n_totobs) / 2.0\n        likeval -= self.n_totobs / 2.0\n    return likeval",
        "mutated": [
            "def loglike(self, params, profile_fe=True):\n    if False:\n        i = 10\n    '\\n        Evaluate the (profile) log-likelihood of the linear mixed\\n        effects model.\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams, or array_like.\\n            The parameter value.  If array-like, must be a packed\\n            parameter vector containing only the covariance\\n            parameters.\\n        profile_fe : bool\\n            If True, replace the provided value of `fe_params` with\\n            the GLS estimates.\\n\\n        Returns\\n        -------\\n        The log-likelihood value at `params`.\\n\\n        Notes\\n        -----\\n        The scale parameter `scale` is always profiled out of the\\n        log-likelihood.  In addition, if `profile_fe` is true the\\n        fixed effects parameters are also profiled out.\\n        '\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    cov_re = params.cov_re\n    vcomp = params.vcomp\n    if profile_fe:\n        (fe_params, sing) = self.get_fe_params(cov_re, vcomp)\n        if sing:\n            self._cov_sing += 1\n    else:\n        fe_params = params.fe_params\n    if self.k_re > 0:\n        try:\n            cov_re_inv = np.linalg.inv(cov_re)\n        except np.linalg.LinAlgError:\n            cov_re_inv = np.linalg.pinv(cov_re)\n            self._cov_sing += 1\n        (_, cov_re_logdet) = np.linalg.slogdet(cov_re)\n    else:\n        cov_re_inv = np.zeros((0, 0))\n        cov_re_logdet = 0\n    expval = np.dot(self.exog, fe_params)\n    resid_all = self.endog - expval\n    likeval = 0.0\n    if self.cov_pen is not None and self.k_re > 0:\n        likeval -= self.cov_pen.func(cov_re, cov_re_inv)\n    if self.fe_pen is not None:\n        likeval -= self.fe_pen.func(fe_params)\n    (xvx, qf) = (0.0, 0.0)\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        cov_aug_logdet = cov_re_logdet + np.sum(np.log(vc_var))\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = resid_all[self.row_indices[group]]\n        ld = _smw_logdet(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var, cov_aug_logdet)\n        likeval -= ld / 2.0\n        u = solver(resid)\n        qf += np.dot(resid, u)\n        if self.reml:\n            mat = solver(exog)\n            xvx += np.dot(exog.T, mat)\n    if self.reml:\n        likeval -= (self.n_totobs - self.k_fe) * np.log(qf) / 2.0\n        (_, ld) = np.linalg.slogdet(xvx)\n        likeval -= ld / 2.0\n        likeval -= (self.n_totobs - self.k_fe) * np.log(2 * np.pi) / 2.0\n        likeval += (self.n_totobs - self.k_fe) * np.log(self.n_totobs - self.k_fe) / 2.0\n        likeval -= (self.n_totobs - self.k_fe) / 2.0\n    else:\n        likeval -= self.n_totobs * np.log(qf) / 2.0\n        likeval -= self.n_totobs * np.log(2 * np.pi) / 2.0\n        likeval += self.n_totobs * np.log(self.n_totobs) / 2.0\n        likeval -= self.n_totobs / 2.0\n    return likeval",
            "def loglike(self, params, profile_fe=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Evaluate the (profile) log-likelihood of the linear mixed\\n        effects model.\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams, or array_like.\\n            The parameter value.  If array-like, must be a packed\\n            parameter vector containing only the covariance\\n            parameters.\\n        profile_fe : bool\\n            If True, replace the provided value of `fe_params` with\\n            the GLS estimates.\\n\\n        Returns\\n        -------\\n        The log-likelihood value at `params`.\\n\\n        Notes\\n        -----\\n        The scale parameter `scale` is always profiled out of the\\n        log-likelihood.  In addition, if `profile_fe` is true the\\n        fixed effects parameters are also profiled out.\\n        '\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    cov_re = params.cov_re\n    vcomp = params.vcomp\n    if profile_fe:\n        (fe_params, sing) = self.get_fe_params(cov_re, vcomp)\n        if sing:\n            self._cov_sing += 1\n    else:\n        fe_params = params.fe_params\n    if self.k_re > 0:\n        try:\n            cov_re_inv = np.linalg.inv(cov_re)\n        except np.linalg.LinAlgError:\n            cov_re_inv = np.linalg.pinv(cov_re)\n            self._cov_sing += 1\n        (_, cov_re_logdet) = np.linalg.slogdet(cov_re)\n    else:\n        cov_re_inv = np.zeros((0, 0))\n        cov_re_logdet = 0\n    expval = np.dot(self.exog, fe_params)\n    resid_all = self.endog - expval\n    likeval = 0.0\n    if self.cov_pen is not None and self.k_re > 0:\n        likeval -= self.cov_pen.func(cov_re, cov_re_inv)\n    if self.fe_pen is not None:\n        likeval -= self.fe_pen.func(fe_params)\n    (xvx, qf) = (0.0, 0.0)\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        cov_aug_logdet = cov_re_logdet + np.sum(np.log(vc_var))\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = resid_all[self.row_indices[group]]\n        ld = _smw_logdet(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var, cov_aug_logdet)\n        likeval -= ld / 2.0\n        u = solver(resid)\n        qf += np.dot(resid, u)\n        if self.reml:\n            mat = solver(exog)\n            xvx += np.dot(exog.T, mat)\n    if self.reml:\n        likeval -= (self.n_totobs - self.k_fe) * np.log(qf) / 2.0\n        (_, ld) = np.linalg.slogdet(xvx)\n        likeval -= ld / 2.0\n        likeval -= (self.n_totobs - self.k_fe) * np.log(2 * np.pi) / 2.0\n        likeval += (self.n_totobs - self.k_fe) * np.log(self.n_totobs - self.k_fe) / 2.0\n        likeval -= (self.n_totobs - self.k_fe) / 2.0\n    else:\n        likeval -= self.n_totobs * np.log(qf) / 2.0\n        likeval -= self.n_totobs * np.log(2 * np.pi) / 2.0\n        likeval += self.n_totobs * np.log(self.n_totobs) / 2.0\n        likeval -= self.n_totobs / 2.0\n    return likeval",
            "def loglike(self, params, profile_fe=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Evaluate the (profile) log-likelihood of the linear mixed\\n        effects model.\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams, or array_like.\\n            The parameter value.  If array-like, must be a packed\\n            parameter vector containing only the covariance\\n            parameters.\\n        profile_fe : bool\\n            If True, replace the provided value of `fe_params` with\\n            the GLS estimates.\\n\\n        Returns\\n        -------\\n        The log-likelihood value at `params`.\\n\\n        Notes\\n        -----\\n        The scale parameter `scale` is always profiled out of the\\n        log-likelihood.  In addition, if `profile_fe` is true the\\n        fixed effects parameters are also profiled out.\\n        '\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    cov_re = params.cov_re\n    vcomp = params.vcomp\n    if profile_fe:\n        (fe_params, sing) = self.get_fe_params(cov_re, vcomp)\n        if sing:\n            self._cov_sing += 1\n    else:\n        fe_params = params.fe_params\n    if self.k_re > 0:\n        try:\n            cov_re_inv = np.linalg.inv(cov_re)\n        except np.linalg.LinAlgError:\n            cov_re_inv = np.linalg.pinv(cov_re)\n            self._cov_sing += 1\n        (_, cov_re_logdet) = np.linalg.slogdet(cov_re)\n    else:\n        cov_re_inv = np.zeros((0, 0))\n        cov_re_logdet = 0\n    expval = np.dot(self.exog, fe_params)\n    resid_all = self.endog - expval\n    likeval = 0.0\n    if self.cov_pen is not None and self.k_re > 0:\n        likeval -= self.cov_pen.func(cov_re, cov_re_inv)\n    if self.fe_pen is not None:\n        likeval -= self.fe_pen.func(fe_params)\n    (xvx, qf) = (0.0, 0.0)\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        cov_aug_logdet = cov_re_logdet + np.sum(np.log(vc_var))\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = resid_all[self.row_indices[group]]\n        ld = _smw_logdet(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var, cov_aug_logdet)\n        likeval -= ld / 2.0\n        u = solver(resid)\n        qf += np.dot(resid, u)\n        if self.reml:\n            mat = solver(exog)\n            xvx += np.dot(exog.T, mat)\n    if self.reml:\n        likeval -= (self.n_totobs - self.k_fe) * np.log(qf) / 2.0\n        (_, ld) = np.linalg.slogdet(xvx)\n        likeval -= ld / 2.0\n        likeval -= (self.n_totobs - self.k_fe) * np.log(2 * np.pi) / 2.0\n        likeval += (self.n_totobs - self.k_fe) * np.log(self.n_totobs - self.k_fe) / 2.0\n        likeval -= (self.n_totobs - self.k_fe) / 2.0\n    else:\n        likeval -= self.n_totobs * np.log(qf) / 2.0\n        likeval -= self.n_totobs * np.log(2 * np.pi) / 2.0\n        likeval += self.n_totobs * np.log(self.n_totobs) / 2.0\n        likeval -= self.n_totobs / 2.0\n    return likeval",
            "def loglike(self, params, profile_fe=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Evaluate the (profile) log-likelihood of the linear mixed\\n        effects model.\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams, or array_like.\\n            The parameter value.  If array-like, must be a packed\\n            parameter vector containing only the covariance\\n            parameters.\\n        profile_fe : bool\\n            If True, replace the provided value of `fe_params` with\\n            the GLS estimates.\\n\\n        Returns\\n        -------\\n        The log-likelihood value at `params`.\\n\\n        Notes\\n        -----\\n        The scale parameter `scale` is always profiled out of the\\n        log-likelihood.  In addition, if `profile_fe` is true the\\n        fixed effects parameters are also profiled out.\\n        '\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    cov_re = params.cov_re\n    vcomp = params.vcomp\n    if profile_fe:\n        (fe_params, sing) = self.get_fe_params(cov_re, vcomp)\n        if sing:\n            self._cov_sing += 1\n    else:\n        fe_params = params.fe_params\n    if self.k_re > 0:\n        try:\n            cov_re_inv = np.linalg.inv(cov_re)\n        except np.linalg.LinAlgError:\n            cov_re_inv = np.linalg.pinv(cov_re)\n            self._cov_sing += 1\n        (_, cov_re_logdet) = np.linalg.slogdet(cov_re)\n    else:\n        cov_re_inv = np.zeros((0, 0))\n        cov_re_logdet = 0\n    expval = np.dot(self.exog, fe_params)\n    resid_all = self.endog - expval\n    likeval = 0.0\n    if self.cov_pen is not None and self.k_re > 0:\n        likeval -= self.cov_pen.func(cov_re, cov_re_inv)\n    if self.fe_pen is not None:\n        likeval -= self.fe_pen.func(fe_params)\n    (xvx, qf) = (0.0, 0.0)\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        cov_aug_logdet = cov_re_logdet + np.sum(np.log(vc_var))\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = resid_all[self.row_indices[group]]\n        ld = _smw_logdet(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var, cov_aug_logdet)\n        likeval -= ld / 2.0\n        u = solver(resid)\n        qf += np.dot(resid, u)\n        if self.reml:\n            mat = solver(exog)\n            xvx += np.dot(exog.T, mat)\n    if self.reml:\n        likeval -= (self.n_totobs - self.k_fe) * np.log(qf) / 2.0\n        (_, ld) = np.linalg.slogdet(xvx)\n        likeval -= ld / 2.0\n        likeval -= (self.n_totobs - self.k_fe) * np.log(2 * np.pi) / 2.0\n        likeval += (self.n_totobs - self.k_fe) * np.log(self.n_totobs - self.k_fe) / 2.0\n        likeval -= (self.n_totobs - self.k_fe) / 2.0\n    else:\n        likeval -= self.n_totobs * np.log(qf) / 2.0\n        likeval -= self.n_totobs * np.log(2 * np.pi) / 2.0\n        likeval += self.n_totobs * np.log(self.n_totobs) / 2.0\n        likeval -= self.n_totobs / 2.0\n    return likeval",
            "def loglike(self, params, profile_fe=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Evaluate the (profile) log-likelihood of the linear mixed\\n        effects model.\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams, or array_like.\\n            The parameter value.  If array-like, must be a packed\\n            parameter vector containing only the covariance\\n            parameters.\\n        profile_fe : bool\\n            If True, replace the provided value of `fe_params` with\\n            the GLS estimates.\\n\\n        Returns\\n        -------\\n        The log-likelihood value at `params`.\\n\\n        Notes\\n        -----\\n        The scale parameter `scale` is always profiled out of the\\n        log-likelihood.  In addition, if `profile_fe` is true the\\n        fixed effects parameters are also profiled out.\\n        '\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    cov_re = params.cov_re\n    vcomp = params.vcomp\n    if profile_fe:\n        (fe_params, sing) = self.get_fe_params(cov_re, vcomp)\n        if sing:\n            self._cov_sing += 1\n    else:\n        fe_params = params.fe_params\n    if self.k_re > 0:\n        try:\n            cov_re_inv = np.linalg.inv(cov_re)\n        except np.linalg.LinAlgError:\n            cov_re_inv = np.linalg.pinv(cov_re)\n            self._cov_sing += 1\n        (_, cov_re_logdet) = np.linalg.slogdet(cov_re)\n    else:\n        cov_re_inv = np.zeros((0, 0))\n        cov_re_logdet = 0\n    expval = np.dot(self.exog, fe_params)\n    resid_all = self.endog - expval\n    likeval = 0.0\n    if self.cov_pen is not None and self.k_re > 0:\n        likeval -= self.cov_pen.func(cov_re, cov_re_inv)\n    if self.fe_pen is not None:\n        likeval -= self.fe_pen.func(fe_params)\n    (xvx, qf) = (0.0, 0.0)\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        cov_aug_logdet = cov_re_logdet + np.sum(np.log(vc_var))\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = resid_all[self.row_indices[group]]\n        ld = _smw_logdet(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var, cov_aug_logdet)\n        likeval -= ld / 2.0\n        u = solver(resid)\n        qf += np.dot(resid, u)\n        if self.reml:\n            mat = solver(exog)\n            xvx += np.dot(exog.T, mat)\n    if self.reml:\n        likeval -= (self.n_totobs - self.k_fe) * np.log(qf) / 2.0\n        (_, ld) = np.linalg.slogdet(xvx)\n        likeval -= ld / 2.0\n        likeval -= (self.n_totobs - self.k_fe) * np.log(2 * np.pi) / 2.0\n        likeval += (self.n_totobs - self.k_fe) * np.log(self.n_totobs - self.k_fe) / 2.0\n        likeval -= (self.n_totobs - self.k_fe) / 2.0\n    else:\n        likeval -= self.n_totobs * np.log(qf) / 2.0\n        likeval -= self.n_totobs * np.log(2 * np.pi) / 2.0\n        likeval += self.n_totobs * np.log(self.n_totobs) / 2.0\n        likeval -= self.n_totobs / 2.0\n    return likeval"
        ]
    },
    {
        "func_name": "_gen_dV_dPar",
        "original": "def _gen_dV_dPar(self, ex_r, solver, group_ix, max_ix=None):\n    \"\"\"\n        A generator that yields the element-wise derivative of the\n        marginal covariance matrix with respect to the random effects\n        variance and covariance parameters.\n\n        ex_r : array_like\n            The random effects design matrix\n        solver : function\n            A function that given x returns V^{-1}x, where V\n            is the group's marginal covariance matrix.\n        group_ix : int\n            The group index\n        max_ix : {int, None}\n            If not None, the generator ends when this index\n            is reached.\n        \"\"\"\n    axr = solver(ex_r)\n    jj = 0\n    for j1 in range(self.k_re):\n        for j2 in range(j1 + 1):\n            if max_ix is not None and jj > max_ix:\n                return\n            (mat_l, mat_r) = (ex_r[:, j1:j1 + 1], ex_r[:, j2:j2 + 1])\n            (vsl, vsr) = (axr[:, j1:j1 + 1], axr[:, j2:j2 + 1])\n            yield (jj, mat_l, mat_r, vsl, vsr, j1 == j2)\n            jj += 1\n    for (j, _) in enumerate(self.exog_vc.names):\n        if max_ix is not None and jj > max_ix:\n            return\n        mat = self.exog_vc.mats[j][group_ix]\n        axmat = solver(mat)\n        yield (jj, mat, mat, axmat, axmat, True)\n        jj += 1",
        "mutated": [
            "def _gen_dV_dPar(self, ex_r, solver, group_ix, max_ix=None):\n    if False:\n        i = 10\n    \"\\n        A generator that yields the element-wise derivative of the\\n        marginal covariance matrix with respect to the random effects\\n        variance and covariance parameters.\\n\\n        ex_r : array_like\\n            The random effects design matrix\\n        solver : function\\n            A function that given x returns V^{-1}x, where V\\n            is the group's marginal covariance matrix.\\n        group_ix : int\\n            The group index\\n        max_ix : {int, None}\\n            If not None, the generator ends when this index\\n            is reached.\\n        \"\n    axr = solver(ex_r)\n    jj = 0\n    for j1 in range(self.k_re):\n        for j2 in range(j1 + 1):\n            if max_ix is not None and jj > max_ix:\n                return\n            (mat_l, mat_r) = (ex_r[:, j1:j1 + 1], ex_r[:, j2:j2 + 1])\n            (vsl, vsr) = (axr[:, j1:j1 + 1], axr[:, j2:j2 + 1])\n            yield (jj, mat_l, mat_r, vsl, vsr, j1 == j2)\n            jj += 1\n    for (j, _) in enumerate(self.exog_vc.names):\n        if max_ix is not None and jj > max_ix:\n            return\n        mat = self.exog_vc.mats[j][group_ix]\n        axmat = solver(mat)\n        yield (jj, mat, mat, axmat, axmat, True)\n        jj += 1",
            "def _gen_dV_dPar(self, ex_r, solver, group_ix, max_ix=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        A generator that yields the element-wise derivative of the\\n        marginal covariance matrix with respect to the random effects\\n        variance and covariance parameters.\\n\\n        ex_r : array_like\\n            The random effects design matrix\\n        solver : function\\n            A function that given x returns V^{-1}x, where V\\n            is the group's marginal covariance matrix.\\n        group_ix : int\\n            The group index\\n        max_ix : {int, None}\\n            If not None, the generator ends when this index\\n            is reached.\\n        \"\n    axr = solver(ex_r)\n    jj = 0\n    for j1 in range(self.k_re):\n        for j2 in range(j1 + 1):\n            if max_ix is not None and jj > max_ix:\n                return\n            (mat_l, mat_r) = (ex_r[:, j1:j1 + 1], ex_r[:, j2:j2 + 1])\n            (vsl, vsr) = (axr[:, j1:j1 + 1], axr[:, j2:j2 + 1])\n            yield (jj, mat_l, mat_r, vsl, vsr, j1 == j2)\n            jj += 1\n    for (j, _) in enumerate(self.exog_vc.names):\n        if max_ix is not None and jj > max_ix:\n            return\n        mat = self.exog_vc.mats[j][group_ix]\n        axmat = solver(mat)\n        yield (jj, mat, mat, axmat, axmat, True)\n        jj += 1",
            "def _gen_dV_dPar(self, ex_r, solver, group_ix, max_ix=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        A generator that yields the element-wise derivative of the\\n        marginal covariance matrix with respect to the random effects\\n        variance and covariance parameters.\\n\\n        ex_r : array_like\\n            The random effects design matrix\\n        solver : function\\n            A function that given x returns V^{-1}x, where V\\n            is the group's marginal covariance matrix.\\n        group_ix : int\\n            The group index\\n        max_ix : {int, None}\\n            If not None, the generator ends when this index\\n            is reached.\\n        \"\n    axr = solver(ex_r)\n    jj = 0\n    for j1 in range(self.k_re):\n        for j2 in range(j1 + 1):\n            if max_ix is not None and jj > max_ix:\n                return\n            (mat_l, mat_r) = (ex_r[:, j1:j1 + 1], ex_r[:, j2:j2 + 1])\n            (vsl, vsr) = (axr[:, j1:j1 + 1], axr[:, j2:j2 + 1])\n            yield (jj, mat_l, mat_r, vsl, vsr, j1 == j2)\n            jj += 1\n    for (j, _) in enumerate(self.exog_vc.names):\n        if max_ix is not None and jj > max_ix:\n            return\n        mat = self.exog_vc.mats[j][group_ix]\n        axmat = solver(mat)\n        yield (jj, mat, mat, axmat, axmat, True)\n        jj += 1",
            "def _gen_dV_dPar(self, ex_r, solver, group_ix, max_ix=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        A generator that yields the element-wise derivative of the\\n        marginal covariance matrix with respect to the random effects\\n        variance and covariance parameters.\\n\\n        ex_r : array_like\\n            The random effects design matrix\\n        solver : function\\n            A function that given x returns V^{-1}x, where V\\n            is the group's marginal covariance matrix.\\n        group_ix : int\\n            The group index\\n        max_ix : {int, None}\\n            If not None, the generator ends when this index\\n            is reached.\\n        \"\n    axr = solver(ex_r)\n    jj = 0\n    for j1 in range(self.k_re):\n        for j2 in range(j1 + 1):\n            if max_ix is not None and jj > max_ix:\n                return\n            (mat_l, mat_r) = (ex_r[:, j1:j1 + 1], ex_r[:, j2:j2 + 1])\n            (vsl, vsr) = (axr[:, j1:j1 + 1], axr[:, j2:j2 + 1])\n            yield (jj, mat_l, mat_r, vsl, vsr, j1 == j2)\n            jj += 1\n    for (j, _) in enumerate(self.exog_vc.names):\n        if max_ix is not None and jj > max_ix:\n            return\n        mat = self.exog_vc.mats[j][group_ix]\n        axmat = solver(mat)\n        yield (jj, mat, mat, axmat, axmat, True)\n        jj += 1",
            "def _gen_dV_dPar(self, ex_r, solver, group_ix, max_ix=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        A generator that yields the element-wise derivative of the\\n        marginal covariance matrix with respect to the random effects\\n        variance and covariance parameters.\\n\\n        ex_r : array_like\\n            The random effects design matrix\\n        solver : function\\n            A function that given x returns V^{-1}x, where V\\n            is the group's marginal covariance matrix.\\n        group_ix : int\\n            The group index\\n        max_ix : {int, None}\\n            If not None, the generator ends when this index\\n            is reached.\\n        \"\n    axr = solver(ex_r)\n    jj = 0\n    for j1 in range(self.k_re):\n        for j2 in range(j1 + 1):\n            if max_ix is not None and jj > max_ix:\n                return\n            (mat_l, mat_r) = (ex_r[:, j1:j1 + 1], ex_r[:, j2:j2 + 1])\n            (vsl, vsr) = (axr[:, j1:j1 + 1], axr[:, j2:j2 + 1])\n            yield (jj, mat_l, mat_r, vsl, vsr, j1 == j2)\n            jj += 1\n    for (j, _) in enumerate(self.exog_vc.names):\n        if max_ix is not None and jj > max_ix:\n            return\n        mat = self.exog_vc.mats[j][group_ix]\n        axmat = solver(mat)\n        yield (jj, mat, mat, axmat, axmat, True)\n        jj += 1"
        ]
    },
    {
        "func_name": "score",
        "original": "def score(self, params, profile_fe=True):\n    \"\"\"\n        Returns the score vector of the profile log-likelihood.\n\n        Notes\n        -----\n        The score vector that is returned is computed with respect to\n        the parameterization defined by this model instance's\n        `use_sqrt` attribute.\n        \"\"\"\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    if profile_fe:\n        (params.fe_params, sing) = self.get_fe_params(params.cov_re, params.vcomp)\n        if sing:\n            msg = 'Random effects covariance is singular'\n            warnings.warn(msg)\n    if self.use_sqrt:\n        (score_fe, score_re, score_vc) = self.score_sqrt(params, calc_fe=not profile_fe)\n    else:\n        (score_fe, score_re, score_vc) = self.score_full(params, calc_fe=not profile_fe)\n    if self._freepat is not None:\n        score_fe *= self._freepat.fe_params\n        score_re *= self._freepat.cov_re[self._freepat._ix]\n        score_vc *= self._freepat.vcomp\n    if profile_fe:\n        return np.concatenate((score_re, score_vc))\n    else:\n        return np.concatenate((score_fe, score_re, score_vc))",
        "mutated": [
            "def score(self, params, profile_fe=True):\n    if False:\n        i = 10\n    \"\\n        Returns the score vector of the profile log-likelihood.\\n\\n        Notes\\n        -----\\n        The score vector that is returned is computed with respect to\\n        the parameterization defined by this model instance's\\n        `use_sqrt` attribute.\\n        \"\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    if profile_fe:\n        (params.fe_params, sing) = self.get_fe_params(params.cov_re, params.vcomp)\n        if sing:\n            msg = 'Random effects covariance is singular'\n            warnings.warn(msg)\n    if self.use_sqrt:\n        (score_fe, score_re, score_vc) = self.score_sqrt(params, calc_fe=not profile_fe)\n    else:\n        (score_fe, score_re, score_vc) = self.score_full(params, calc_fe=not profile_fe)\n    if self._freepat is not None:\n        score_fe *= self._freepat.fe_params\n        score_re *= self._freepat.cov_re[self._freepat._ix]\n        score_vc *= self._freepat.vcomp\n    if profile_fe:\n        return np.concatenate((score_re, score_vc))\n    else:\n        return np.concatenate((score_fe, score_re, score_vc))",
            "def score(self, params, profile_fe=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Returns the score vector of the profile log-likelihood.\\n\\n        Notes\\n        -----\\n        The score vector that is returned is computed with respect to\\n        the parameterization defined by this model instance's\\n        `use_sqrt` attribute.\\n        \"\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    if profile_fe:\n        (params.fe_params, sing) = self.get_fe_params(params.cov_re, params.vcomp)\n        if sing:\n            msg = 'Random effects covariance is singular'\n            warnings.warn(msg)\n    if self.use_sqrt:\n        (score_fe, score_re, score_vc) = self.score_sqrt(params, calc_fe=not profile_fe)\n    else:\n        (score_fe, score_re, score_vc) = self.score_full(params, calc_fe=not profile_fe)\n    if self._freepat is not None:\n        score_fe *= self._freepat.fe_params\n        score_re *= self._freepat.cov_re[self._freepat._ix]\n        score_vc *= self._freepat.vcomp\n    if profile_fe:\n        return np.concatenate((score_re, score_vc))\n    else:\n        return np.concatenate((score_fe, score_re, score_vc))",
            "def score(self, params, profile_fe=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Returns the score vector of the profile log-likelihood.\\n\\n        Notes\\n        -----\\n        The score vector that is returned is computed with respect to\\n        the parameterization defined by this model instance's\\n        `use_sqrt` attribute.\\n        \"\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    if profile_fe:\n        (params.fe_params, sing) = self.get_fe_params(params.cov_re, params.vcomp)\n        if sing:\n            msg = 'Random effects covariance is singular'\n            warnings.warn(msg)\n    if self.use_sqrt:\n        (score_fe, score_re, score_vc) = self.score_sqrt(params, calc_fe=not profile_fe)\n    else:\n        (score_fe, score_re, score_vc) = self.score_full(params, calc_fe=not profile_fe)\n    if self._freepat is not None:\n        score_fe *= self._freepat.fe_params\n        score_re *= self._freepat.cov_re[self._freepat._ix]\n        score_vc *= self._freepat.vcomp\n    if profile_fe:\n        return np.concatenate((score_re, score_vc))\n    else:\n        return np.concatenate((score_fe, score_re, score_vc))",
            "def score(self, params, profile_fe=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Returns the score vector of the profile log-likelihood.\\n\\n        Notes\\n        -----\\n        The score vector that is returned is computed with respect to\\n        the parameterization defined by this model instance's\\n        `use_sqrt` attribute.\\n        \"\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    if profile_fe:\n        (params.fe_params, sing) = self.get_fe_params(params.cov_re, params.vcomp)\n        if sing:\n            msg = 'Random effects covariance is singular'\n            warnings.warn(msg)\n    if self.use_sqrt:\n        (score_fe, score_re, score_vc) = self.score_sqrt(params, calc_fe=not profile_fe)\n    else:\n        (score_fe, score_re, score_vc) = self.score_full(params, calc_fe=not profile_fe)\n    if self._freepat is not None:\n        score_fe *= self._freepat.fe_params\n        score_re *= self._freepat.cov_re[self._freepat._ix]\n        score_vc *= self._freepat.vcomp\n    if profile_fe:\n        return np.concatenate((score_re, score_vc))\n    else:\n        return np.concatenate((score_fe, score_re, score_vc))",
            "def score(self, params, profile_fe=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Returns the score vector of the profile log-likelihood.\\n\\n        Notes\\n        -----\\n        The score vector that is returned is computed with respect to\\n        the parameterization defined by this model instance's\\n        `use_sqrt` attribute.\\n        \"\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    if profile_fe:\n        (params.fe_params, sing) = self.get_fe_params(params.cov_re, params.vcomp)\n        if sing:\n            msg = 'Random effects covariance is singular'\n            warnings.warn(msg)\n    if self.use_sqrt:\n        (score_fe, score_re, score_vc) = self.score_sqrt(params, calc_fe=not profile_fe)\n    else:\n        (score_fe, score_re, score_vc) = self.score_full(params, calc_fe=not profile_fe)\n    if self._freepat is not None:\n        score_fe *= self._freepat.fe_params\n        score_re *= self._freepat.cov_re[self._freepat._ix]\n        score_vc *= self._freepat.vcomp\n    if profile_fe:\n        return np.concatenate((score_re, score_vc))\n    else:\n        return np.concatenate((score_fe, score_re, score_vc))"
        ]
    },
    {
        "func_name": "score_full",
        "original": "def score_full(self, params, calc_fe):\n    \"\"\"\n        Returns the score with respect to untransformed parameters.\n\n        Calculates the score vector for the profiled log-likelihood of\n        the mixed effects model with respect to the parameterization\n        in which the random effects covariance matrix is represented\n        in its full form (not using the Cholesky factor).\n\n        Parameters\n        ----------\n        params : MixedLMParams or array_like\n            The parameter at which the score function is evaluated.\n            If array-like, must contain the packed random effects\n            parameters (cov_re and vcomp) without fe_params.\n        calc_fe : bool\n            If True, calculate the score vector for the fixed effects\n            parameters.  If False, this vector is not calculated, and\n            a vector of zeros is returned in its place.\n\n        Returns\n        -------\n        score_fe : array_like\n            The score vector with respect to the fixed effects\n            parameters.\n        score_re : array_like\n            The score vector with respect to the random effects\n            parameters (excluding variance components parameters).\n        score_vc : array_like\n            The score vector with respect to variance components\n            parameters.\n\n        Notes\n        -----\n        `score_re` is taken with respect to the parameterization in\n        which `cov_re` is represented through its lower triangle\n        (without taking the Cholesky square root).\n        \"\"\"\n    fe_params = params.fe_params\n    cov_re = params.cov_re\n    vcomp = params.vcomp\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = np.linalg.pinv(cov_re)\n        self._cov_sing += 1\n    score_fe = np.zeros(self.k_fe)\n    score_re = np.zeros(self.k_re2)\n    score_vc = np.zeros(self.k_vc)\n    if self.cov_pen is not None:\n        score_re -= self.cov_pen.deriv(cov_re, cov_re_inv)\n    if calc_fe and self.fe_pen is not None:\n        score_fe -= self.fe_pen.deriv(fe_params)\n    rvir = 0.0\n    xtvir = 0.0\n    xtvix = 0.0\n    xtax = [0.0] * (self.k_re2 + self.k_vc)\n    dlv = np.zeros(self.k_re2 + self.k_vc)\n    rvavr = np.zeros(self.k_re2 + self.k_vc)\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        if self.reml:\n            viexog = solver(exog)\n            xtvix += np.dot(exog.T, viexog)\n        vir = solver(resid)\n        for (jj, matl, matr, vsl, vsr, sym) in self._gen_dV_dPar(ex_r, solver, group_ix):\n            dlv[jj] = _dotsum(matr, vsl)\n            if not sym:\n                dlv[jj] += _dotsum(matl, vsr)\n            ul = _dot(vir, matl)\n            ur = ul.T if sym else _dot(matr.T, vir)\n            ulr = np.dot(ul, ur)\n            rvavr[jj] += ulr\n            if not sym:\n                rvavr[jj] += ulr.T\n            if self.reml:\n                ul = _dot(viexog.T, matl)\n                ur = ul.T if sym else _dot(matr.T, viexog)\n                ulr = np.dot(ul, ur)\n                xtax[jj] += ulr\n                if not sym:\n                    xtax[jj] += ulr.T\n        if self.k_re > 0:\n            score_re -= 0.5 * dlv[0:self.k_re2]\n        if self.k_vc > 0:\n            score_vc -= 0.5 * dlv[self.k_re2:]\n        rvir += np.dot(resid, vir)\n        if calc_fe:\n            xtvir += np.dot(exog.T, vir)\n    fac = self.n_totobs\n    if self.reml:\n        fac -= self.k_fe\n    if calc_fe and self.k_fe > 0:\n        score_fe += fac * xtvir / rvir\n    if self.k_re > 0:\n        score_re += 0.5 * fac * rvavr[0:self.k_re2] / rvir\n    if self.k_vc > 0:\n        score_vc += 0.5 * fac * rvavr[self.k_re2:] / rvir\n    if self.reml:\n        xtvixi = np.linalg.inv(xtvix)\n        for j in range(self.k_re2):\n            score_re[j] += 0.5 * _dotsum(xtvixi.T, xtax[j])\n        for j in range(self.k_vc):\n            score_vc[j] += 0.5 * _dotsum(xtvixi.T, xtax[self.k_re2 + j])\n    return (score_fe, score_re, score_vc)",
        "mutated": [
            "def score_full(self, params, calc_fe):\n    if False:\n        i = 10\n    '\\n        Returns the score with respect to untransformed parameters.\\n\\n        Calculates the score vector for the profiled log-likelihood of\\n        the mixed effects model with respect to the parameterization\\n        in which the random effects covariance matrix is represented\\n        in its full form (not using the Cholesky factor).\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The parameter at which the score function is evaluated.\\n            If array-like, must contain the packed random effects\\n            parameters (cov_re and vcomp) without fe_params.\\n        calc_fe : bool\\n            If True, calculate the score vector for the fixed effects\\n            parameters.  If False, this vector is not calculated, and\\n            a vector of zeros is returned in its place.\\n\\n        Returns\\n        -------\\n        score_fe : array_like\\n            The score vector with respect to the fixed effects\\n            parameters.\\n        score_re : array_like\\n            The score vector with respect to the random effects\\n            parameters (excluding variance components parameters).\\n        score_vc : array_like\\n            The score vector with respect to variance components\\n            parameters.\\n\\n        Notes\\n        -----\\n        `score_re` is taken with respect to the parameterization in\\n        which `cov_re` is represented through its lower triangle\\n        (without taking the Cholesky square root).\\n        '\n    fe_params = params.fe_params\n    cov_re = params.cov_re\n    vcomp = params.vcomp\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = np.linalg.pinv(cov_re)\n        self._cov_sing += 1\n    score_fe = np.zeros(self.k_fe)\n    score_re = np.zeros(self.k_re2)\n    score_vc = np.zeros(self.k_vc)\n    if self.cov_pen is not None:\n        score_re -= self.cov_pen.deriv(cov_re, cov_re_inv)\n    if calc_fe and self.fe_pen is not None:\n        score_fe -= self.fe_pen.deriv(fe_params)\n    rvir = 0.0\n    xtvir = 0.0\n    xtvix = 0.0\n    xtax = [0.0] * (self.k_re2 + self.k_vc)\n    dlv = np.zeros(self.k_re2 + self.k_vc)\n    rvavr = np.zeros(self.k_re2 + self.k_vc)\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        if self.reml:\n            viexog = solver(exog)\n            xtvix += np.dot(exog.T, viexog)\n        vir = solver(resid)\n        for (jj, matl, matr, vsl, vsr, sym) in self._gen_dV_dPar(ex_r, solver, group_ix):\n            dlv[jj] = _dotsum(matr, vsl)\n            if not sym:\n                dlv[jj] += _dotsum(matl, vsr)\n            ul = _dot(vir, matl)\n            ur = ul.T if sym else _dot(matr.T, vir)\n            ulr = np.dot(ul, ur)\n            rvavr[jj] += ulr\n            if not sym:\n                rvavr[jj] += ulr.T\n            if self.reml:\n                ul = _dot(viexog.T, matl)\n                ur = ul.T if sym else _dot(matr.T, viexog)\n                ulr = np.dot(ul, ur)\n                xtax[jj] += ulr\n                if not sym:\n                    xtax[jj] += ulr.T\n        if self.k_re > 0:\n            score_re -= 0.5 * dlv[0:self.k_re2]\n        if self.k_vc > 0:\n            score_vc -= 0.5 * dlv[self.k_re2:]\n        rvir += np.dot(resid, vir)\n        if calc_fe:\n            xtvir += np.dot(exog.T, vir)\n    fac = self.n_totobs\n    if self.reml:\n        fac -= self.k_fe\n    if calc_fe and self.k_fe > 0:\n        score_fe += fac * xtvir / rvir\n    if self.k_re > 0:\n        score_re += 0.5 * fac * rvavr[0:self.k_re2] / rvir\n    if self.k_vc > 0:\n        score_vc += 0.5 * fac * rvavr[self.k_re2:] / rvir\n    if self.reml:\n        xtvixi = np.linalg.inv(xtvix)\n        for j in range(self.k_re2):\n            score_re[j] += 0.5 * _dotsum(xtvixi.T, xtax[j])\n        for j in range(self.k_vc):\n            score_vc[j] += 0.5 * _dotsum(xtvixi.T, xtax[self.k_re2 + j])\n    return (score_fe, score_re, score_vc)",
            "def score_full(self, params, calc_fe):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the score with respect to untransformed parameters.\\n\\n        Calculates the score vector for the profiled log-likelihood of\\n        the mixed effects model with respect to the parameterization\\n        in which the random effects covariance matrix is represented\\n        in its full form (not using the Cholesky factor).\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The parameter at which the score function is evaluated.\\n            If array-like, must contain the packed random effects\\n            parameters (cov_re and vcomp) without fe_params.\\n        calc_fe : bool\\n            If True, calculate the score vector for the fixed effects\\n            parameters.  If False, this vector is not calculated, and\\n            a vector of zeros is returned in its place.\\n\\n        Returns\\n        -------\\n        score_fe : array_like\\n            The score vector with respect to the fixed effects\\n            parameters.\\n        score_re : array_like\\n            The score vector with respect to the random effects\\n            parameters (excluding variance components parameters).\\n        score_vc : array_like\\n            The score vector with respect to variance components\\n            parameters.\\n\\n        Notes\\n        -----\\n        `score_re` is taken with respect to the parameterization in\\n        which `cov_re` is represented through its lower triangle\\n        (without taking the Cholesky square root).\\n        '\n    fe_params = params.fe_params\n    cov_re = params.cov_re\n    vcomp = params.vcomp\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = np.linalg.pinv(cov_re)\n        self._cov_sing += 1\n    score_fe = np.zeros(self.k_fe)\n    score_re = np.zeros(self.k_re2)\n    score_vc = np.zeros(self.k_vc)\n    if self.cov_pen is not None:\n        score_re -= self.cov_pen.deriv(cov_re, cov_re_inv)\n    if calc_fe and self.fe_pen is not None:\n        score_fe -= self.fe_pen.deriv(fe_params)\n    rvir = 0.0\n    xtvir = 0.0\n    xtvix = 0.0\n    xtax = [0.0] * (self.k_re2 + self.k_vc)\n    dlv = np.zeros(self.k_re2 + self.k_vc)\n    rvavr = np.zeros(self.k_re2 + self.k_vc)\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        if self.reml:\n            viexog = solver(exog)\n            xtvix += np.dot(exog.T, viexog)\n        vir = solver(resid)\n        for (jj, matl, matr, vsl, vsr, sym) in self._gen_dV_dPar(ex_r, solver, group_ix):\n            dlv[jj] = _dotsum(matr, vsl)\n            if not sym:\n                dlv[jj] += _dotsum(matl, vsr)\n            ul = _dot(vir, matl)\n            ur = ul.T if sym else _dot(matr.T, vir)\n            ulr = np.dot(ul, ur)\n            rvavr[jj] += ulr\n            if not sym:\n                rvavr[jj] += ulr.T\n            if self.reml:\n                ul = _dot(viexog.T, matl)\n                ur = ul.T if sym else _dot(matr.T, viexog)\n                ulr = np.dot(ul, ur)\n                xtax[jj] += ulr\n                if not sym:\n                    xtax[jj] += ulr.T\n        if self.k_re > 0:\n            score_re -= 0.5 * dlv[0:self.k_re2]\n        if self.k_vc > 0:\n            score_vc -= 0.5 * dlv[self.k_re2:]\n        rvir += np.dot(resid, vir)\n        if calc_fe:\n            xtvir += np.dot(exog.T, vir)\n    fac = self.n_totobs\n    if self.reml:\n        fac -= self.k_fe\n    if calc_fe and self.k_fe > 0:\n        score_fe += fac * xtvir / rvir\n    if self.k_re > 0:\n        score_re += 0.5 * fac * rvavr[0:self.k_re2] / rvir\n    if self.k_vc > 0:\n        score_vc += 0.5 * fac * rvavr[self.k_re2:] / rvir\n    if self.reml:\n        xtvixi = np.linalg.inv(xtvix)\n        for j in range(self.k_re2):\n            score_re[j] += 0.5 * _dotsum(xtvixi.T, xtax[j])\n        for j in range(self.k_vc):\n            score_vc[j] += 0.5 * _dotsum(xtvixi.T, xtax[self.k_re2 + j])\n    return (score_fe, score_re, score_vc)",
            "def score_full(self, params, calc_fe):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the score with respect to untransformed parameters.\\n\\n        Calculates the score vector for the profiled log-likelihood of\\n        the mixed effects model with respect to the parameterization\\n        in which the random effects covariance matrix is represented\\n        in its full form (not using the Cholesky factor).\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The parameter at which the score function is evaluated.\\n            If array-like, must contain the packed random effects\\n            parameters (cov_re and vcomp) without fe_params.\\n        calc_fe : bool\\n            If True, calculate the score vector for the fixed effects\\n            parameters.  If False, this vector is not calculated, and\\n            a vector of zeros is returned in its place.\\n\\n        Returns\\n        -------\\n        score_fe : array_like\\n            The score vector with respect to the fixed effects\\n            parameters.\\n        score_re : array_like\\n            The score vector with respect to the random effects\\n            parameters (excluding variance components parameters).\\n        score_vc : array_like\\n            The score vector with respect to variance components\\n            parameters.\\n\\n        Notes\\n        -----\\n        `score_re` is taken with respect to the parameterization in\\n        which `cov_re` is represented through its lower triangle\\n        (without taking the Cholesky square root).\\n        '\n    fe_params = params.fe_params\n    cov_re = params.cov_re\n    vcomp = params.vcomp\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = np.linalg.pinv(cov_re)\n        self._cov_sing += 1\n    score_fe = np.zeros(self.k_fe)\n    score_re = np.zeros(self.k_re2)\n    score_vc = np.zeros(self.k_vc)\n    if self.cov_pen is not None:\n        score_re -= self.cov_pen.deriv(cov_re, cov_re_inv)\n    if calc_fe and self.fe_pen is not None:\n        score_fe -= self.fe_pen.deriv(fe_params)\n    rvir = 0.0\n    xtvir = 0.0\n    xtvix = 0.0\n    xtax = [0.0] * (self.k_re2 + self.k_vc)\n    dlv = np.zeros(self.k_re2 + self.k_vc)\n    rvavr = np.zeros(self.k_re2 + self.k_vc)\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        if self.reml:\n            viexog = solver(exog)\n            xtvix += np.dot(exog.T, viexog)\n        vir = solver(resid)\n        for (jj, matl, matr, vsl, vsr, sym) in self._gen_dV_dPar(ex_r, solver, group_ix):\n            dlv[jj] = _dotsum(matr, vsl)\n            if not sym:\n                dlv[jj] += _dotsum(matl, vsr)\n            ul = _dot(vir, matl)\n            ur = ul.T if sym else _dot(matr.T, vir)\n            ulr = np.dot(ul, ur)\n            rvavr[jj] += ulr\n            if not sym:\n                rvavr[jj] += ulr.T\n            if self.reml:\n                ul = _dot(viexog.T, matl)\n                ur = ul.T if sym else _dot(matr.T, viexog)\n                ulr = np.dot(ul, ur)\n                xtax[jj] += ulr\n                if not sym:\n                    xtax[jj] += ulr.T\n        if self.k_re > 0:\n            score_re -= 0.5 * dlv[0:self.k_re2]\n        if self.k_vc > 0:\n            score_vc -= 0.5 * dlv[self.k_re2:]\n        rvir += np.dot(resid, vir)\n        if calc_fe:\n            xtvir += np.dot(exog.T, vir)\n    fac = self.n_totobs\n    if self.reml:\n        fac -= self.k_fe\n    if calc_fe and self.k_fe > 0:\n        score_fe += fac * xtvir / rvir\n    if self.k_re > 0:\n        score_re += 0.5 * fac * rvavr[0:self.k_re2] / rvir\n    if self.k_vc > 0:\n        score_vc += 0.5 * fac * rvavr[self.k_re2:] / rvir\n    if self.reml:\n        xtvixi = np.linalg.inv(xtvix)\n        for j in range(self.k_re2):\n            score_re[j] += 0.5 * _dotsum(xtvixi.T, xtax[j])\n        for j in range(self.k_vc):\n            score_vc[j] += 0.5 * _dotsum(xtvixi.T, xtax[self.k_re2 + j])\n    return (score_fe, score_re, score_vc)",
            "def score_full(self, params, calc_fe):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the score with respect to untransformed parameters.\\n\\n        Calculates the score vector for the profiled log-likelihood of\\n        the mixed effects model with respect to the parameterization\\n        in which the random effects covariance matrix is represented\\n        in its full form (not using the Cholesky factor).\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The parameter at which the score function is evaluated.\\n            If array-like, must contain the packed random effects\\n            parameters (cov_re and vcomp) without fe_params.\\n        calc_fe : bool\\n            If True, calculate the score vector for the fixed effects\\n            parameters.  If False, this vector is not calculated, and\\n            a vector of zeros is returned in its place.\\n\\n        Returns\\n        -------\\n        score_fe : array_like\\n            The score vector with respect to the fixed effects\\n            parameters.\\n        score_re : array_like\\n            The score vector with respect to the random effects\\n            parameters (excluding variance components parameters).\\n        score_vc : array_like\\n            The score vector with respect to variance components\\n            parameters.\\n\\n        Notes\\n        -----\\n        `score_re` is taken with respect to the parameterization in\\n        which `cov_re` is represented through its lower triangle\\n        (without taking the Cholesky square root).\\n        '\n    fe_params = params.fe_params\n    cov_re = params.cov_re\n    vcomp = params.vcomp\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = np.linalg.pinv(cov_re)\n        self._cov_sing += 1\n    score_fe = np.zeros(self.k_fe)\n    score_re = np.zeros(self.k_re2)\n    score_vc = np.zeros(self.k_vc)\n    if self.cov_pen is not None:\n        score_re -= self.cov_pen.deriv(cov_re, cov_re_inv)\n    if calc_fe and self.fe_pen is not None:\n        score_fe -= self.fe_pen.deriv(fe_params)\n    rvir = 0.0\n    xtvir = 0.0\n    xtvix = 0.0\n    xtax = [0.0] * (self.k_re2 + self.k_vc)\n    dlv = np.zeros(self.k_re2 + self.k_vc)\n    rvavr = np.zeros(self.k_re2 + self.k_vc)\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        if self.reml:\n            viexog = solver(exog)\n            xtvix += np.dot(exog.T, viexog)\n        vir = solver(resid)\n        for (jj, matl, matr, vsl, vsr, sym) in self._gen_dV_dPar(ex_r, solver, group_ix):\n            dlv[jj] = _dotsum(matr, vsl)\n            if not sym:\n                dlv[jj] += _dotsum(matl, vsr)\n            ul = _dot(vir, matl)\n            ur = ul.T if sym else _dot(matr.T, vir)\n            ulr = np.dot(ul, ur)\n            rvavr[jj] += ulr\n            if not sym:\n                rvavr[jj] += ulr.T\n            if self.reml:\n                ul = _dot(viexog.T, matl)\n                ur = ul.T if sym else _dot(matr.T, viexog)\n                ulr = np.dot(ul, ur)\n                xtax[jj] += ulr\n                if not sym:\n                    xtax[jj] += ulr.T\n        if self.k_re > 0:\n            score_re -= 0.5 * dlv[0:self.k_re2]\n        if self.k_vc > 0:\n            score_vc -= 0.5 * dlv[self.k_re2:]\n        rvir += np.dot(resid, vir)\n        if calc_fe:\n            xtvir += np.dot(exog.T, vir)\n    fac = self.n_totobs\n    if self.reml:\n        fac -= self.k_fe\n    if calc_fe and self.k_fe > 0:\n        score_fe += fac * xtvir / rvir\n    if self.k_re > 0:\n        score_re += 0.5 * fac * rvavr[0:self.k_re2] / rvir\n    if self.k_vc > 0:\n        score_vc += 0.5 * fac * rvavr[self.k_re2:] / rvir\n    if self.reml:\n        xtvixi = np.linalg.inv(xtvix)\n        for j in range(self.k_re2):\n            score_re[j] += 0.5 * _dotsum(xtvixi.T, xtax[j])\n        for j in range(self.k_vc):\n            score_vc[j] += 0.5 * _dotsum(xtvixi.T, xtax[self.k_re2 + j])\n    return (score_fe, score_re, score_vc)",
            "def score_full(self, params, calc_fe):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the score with respect to untransformed parameters.\\n\\n        Calculates the score vector for the profiled log-likelihood of\\n        the mixed effects model with respect to the parameterization\\n        in which the random effects covariance matrix is represented\\n        in its full form (not using the Cholesky factor).\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The parameter at which the score function is evaluated.\\n            If array-like, must contain the packed random effects\\n            parameters (cov_re and vcomp) without fe_params.\\n        calc_fe : bool\\n            If True, calculate the score vector for the fixed effects\\n            parameters.  If False, this vector is not calculated, and\\n            a vector of zeros is returned in its place.\\n\\n        Returns\\n        -------\\n        score_fe : array_like\\n            The score vector with respect to the fixed effects\\n            parameters.\\n        score_re : array_like\\n            The score vector with respect to the random effects\\n            parameters (excluding variance components parameters).\\n        score_vc : array_like\\n            The score vector with respect to variance components\\n            parameters.\\n\\n        Notes\\n        -----\\n        `score_re` is taken with respect to the parameterization in\\n        which `cov_re` is represented through its lower triangle\\n        (without taking the Cholesky square root).\\n        '\n    fe_params = params.fe_params\n    cov_re = params.cov_re\n    vcomp = params.vcomp\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = np.linalg.pinv(cov_re)\n        self._cov_sing += 1\n    score_fe = np.zeros(self.k_fe)\n    score_re = np.zeros(self.k_re2)\n    score_vc = np.zeros(self.k_vc)\n    if self.cov_pen is not None:\n        score_re -= self.cov_pen.deriv(cov_re, cov_re_inv)\n    if calc_fe and self.fe_pen is not None:\n        score_fe -= self.fe_pen.deriv(fe_params)\n    rvir = 0.0\n    xtvir = 0.0\n    xtvix = 0.0\n    xtax = [0.0] * (self.k_re2 + self.k_vc)\n    dlv = np.zeros(self.k_re2 + self.k_vc)\n    rvavr = np.zeros(self.k_re2 + self.k_vc)\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        if self.reml:\n            viexog = solver(exog)\n            xtvix += np.dot(exog.T, viexog)\n        vir = solver(resid)\n        for (jj, matl, matr, vsl, vsr, sym) in self._gen_dV_dPar(ex_r, solver, group_ix):\n            dlv[jj] = _dotsum(matr, vsl)\n            if not sym:\n                dlv[jj] += _dotsum(matl, vsr)\n            ul = _dot(vir, matl)\n            ur = ul.T if sym else _dot(matr.T, vir)\n            ulr = np.dot(ul, ur)\n            rvavr[jj] += ulr\n            if not sym:\n                rvavr[jj] += ulr.T\n            if self.reml:\n                ul = _dot(viexog.T, matl)\n                ur = ul.T if sym else _dot(matr.T, viexog)\n                ulr = np.dot(ul, ur)\n                xtax[jj] += ulr\n                if not sym:\n                    xtax[jj] += ulr.T\n        if self.k_re > 0:\n            score_re -= 0.5 * dlv[0:self.k_re2]\n        if self.k_vc > 0:\n            score_vc -= 0.5 * dlv[self.k_re2:]\n        rvir += np.dot(resid, vir)\n        if calc_fe:\n            xtvir += np.dot(exog.T, vir)\n    fac = self.n_totobs\n    if self.reml:\n        fac -= self.k_fe\n    if calc_fe and self.k_fe > 0:\n        score_fe += fac * xtvir / rvir\n    if self.k_re > 0:\n        score_re += 0.5 * fac * rvavr[0:self.k_re2] / rvir\n    if self.k_vc > 0:\n        score_vc += 0.5 * fac * rvavr[self.k_re2:] / rvir\n    if self.reml:\n        xtvixi = np.linalg.inv(xtvix)\n        for j in range(self.k_re2):\n            score_re[j] += 0.5 * _dotsum(xtvixi.T, xtax[j])\n        for j in range(self.k_vc):\n            score_vc[j] += 0.5 * _dotsum(xtvixi.T, xtax[self.k_re2 + j])\n    return (score_fe, score_re, score_vc)"
        ]
    },
    {
        "func_name": "score_sqrt",
        "original": "def score_sqrt(self, params, calc_fe=True):\n    \"\"\"\n        Returns the score with respect to transformed parameters.\n\n        Calculates the score vector with respect to the\n        parameterization in which the random effects covariance matrix\n        is represented through its Cholesky square root.\n\n        Parameters\n        ----------\n        params : MixedLMParams or array_like\n            The model parameters.  If array-like must contain packed\n            parameters that are compatible with this model instance.\n        calc_fe : bool\n            If True, calculate the score vector for the fixed effects\n            parameters.  If False, this vector is not calculated, and\n            a vector of zeros is returned in its place.\n\n        Returns\n        -------\n        score_fe : array_like\n            The score vector with respect to the fixed effects\n            parameters.\n        score_re : array_like\n            The score vector with respect to the random effects\n            parameters (excluding variance components parameters).\n        score_vc : array_like\n            The score vector with respect to variance components\n            parameters.\n        \"\"\"\n    (score_fe, score_re, score_vc) = self.score_full(params, calc_fe=calc_fe)\n    params_vec = params.get_packed(use_sqrt=True, has_fe=True)\n    score_full = np.concatenate((score_fe, score_re, score_vc))\n    scr = 0.0\n    for i in range(len(params_vec)):\n        v = self._lin[i] + 2 * np.dot(self._quad[i], params_vec)\n        scr += score_full[i] * v\n    score_fe = scr[0:self.k_fe]\n    score_re = scr[self.k_fe:self.k_fe + self.k_re2]\n    score_vc = scr[self.k_fe + self.k_re2:]\n    return (score_fe, score_re, score_vc)",
        "mutated": [
            "def score_sqrt(self, params, calc_fe=True):\n    if False:\n        i = 10\n    '\\n        Returns the score with respect to transformed parameters.\\n\\n        Calculates the score vector with respect to the\\n        parameterization in which the random effects covariance matrix\\n        is represented through its Cholesky square root.\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The model parameters.  If array-like must contain packed\\n            parameters that are compatible with this model instance.\\n        calc_fe : bool\\n            If True, calculate the score vector for the fixed effects\\n            parameters.  If False, this vector is not calculated, and\\n            a vector of zeros is returned in its place.\\n\\n        Returns\\n        -------\\n        score_fe : array_like\\n            The score vector with respect to the fixed effects\\n            parameters.\\n        score_re : array_like\\n            The score vector with respect to the random effects\\n            parameters (excluding variance components parameters).\\n        score_vc : array_like\\n            The score vector with respect to variance components\\n            parameters.\\n        '\n    (score_fe, score_re, score_vc) = self.score_full(params, calc_fe=calc_fe)\n    params_vec = params.get_packed(use_sqrt=True, has_fe=True)\n    score_full = np.concatenate((score_fe, score_re, score_vc))\n    scr = 0.0\n    for i in range(len(params_vec)):\n        v = self._lin[i] + 2 * np.dot(self._quad[i], params_vec)\n        scr += score_full[i] * v\n    score_fe = scr[0:self.k_fe]\n    score_re = scr[self.k_fe:self.k_fe + self.k_re2]\n    score_vc = scr[self.k_fe + self.k_re2:]\n    return (score_fe, score_re, score_vc)",
            "def score_sqrt(self, params, calc_fe=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the score with respect to transformed parameters.\\n\\n        Calculates the score vector with respect to the\\n        parameterization in which the random effects covariance matrix\\n        is represented through its Cholesky square root.\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The model parameters.  If array-like must contain packed\\n            parameters that are compatible with this model instance.\\n        calc_fe : bool\\n            If True, calculate the score vector for the fixed effects\\n            parameters.  If False, this vector is not calculated, and\\n            a vector of zeros is returned in its place.\\n\\n        Returns\\n        -------\\n        score_fe : array_like\\n            The score vector with respect to the fixed effects\\n            parameters.\\n        score_re : array_like\\n            The score vector with respect to the random effects\\n            parameters (excluding variance components parameters).\\n        score_vc : array_like\\n            The score vector with respect to variance components\\n            parameters.\\n        '\n    (score_fe, score_re, score_vc) = self.score_full(params, calc_fe=calc_fe)\n    params_vec = params.get_packed(use_sqrt=True, has_fe=True)\n    score_full = np.concatenate((score_fe, score_re, score_vc))\n    scr = 0.0\n    for i in range(len(params_vec)):\n        v = self._lin[i] + 2 * np.dot(self._quad[i], params_vec)\n        scr += score_full[i] * v\n    score_fe = scr[0:self.k_fe]\n    score_re = scr[self.k_fe:self.k_fe + self.k_re2]\n    score_vc = scr[self.k_fe + self.k_re2:]\n    return (score_fe, score_re, score_vc)",
            "def score_sqrt(self, params, calc_fe=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the score with respect to transformed parameters.\\n\\n        Calculates the score vector with respect to the\\n        parameterization in which the random effects covariance matrix\\n        is represented through its Cholesky square root.\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The model parameters.  If array-like must contain packed\\n            parameters that are compatible with this model instance.\\n        calc_fe : bool\\n            If True, calculate the score vector for the fixed effects\\n            parameters.  If False, this vector is not calculated, and\\n            a vector of zeros is returned in its place.\\n\\n        Returns\\n        -------\\n        score_fe : array_like\\n            The score vector with respect to the fixed effects\\n            parameters.\\n        score_re : array_like\\n            The score vector with respect to the random effects\\n            parameters (excluding variance components parameters).\\n        score_vc : array_like\\n            The score vector with respect to variance components\\n            parameters.\\n        '\n    (score_fe, score_re, score_vc) = self.score_full(params, calc_fe=calc_fe)\n    params_vec = params.get_packed(use_sqrt=True, has_fe=True)\n    score_full = np.concatenate((score_fe, score_re, score_vc))\n    scr = 0.0\n    for i in range(len(params_vec)):\n        v = self._lin[i] + 2 * np.dot(self._quad[i], params_vec)\n        scr += score_full[i] * v\n    score_fe = scr[0:self.k_fe]\n    score_re = scr[self.k_fe:self.k_fe + self.k_re2]\n    score_vc = scr[self.k_fe + self.k_re2:]\n    return (score_fe, score_re, score_vc)",
            "def score_sqrt(self, params, calc_fe=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the score with respect to transformed parameters.\\n\\n        Calculates the score vector with respect to the\\n        parameterization in which the random effects covariance matrix\\n        is represented through its Cholesky square root.\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The model parameters.  If array-like must contain packed\\n            parameters that are compatible with this model instance.\\n        calc_fe : bool\\n            If True, calculate the score vector for the fixed effects\\n            parameters.  If False, this vector is not calculated, and\\n            a vector of zeros is returned in its place.\\n\\n        Returns\\n        -------\\n        score_fe : array_like\\n            The score vector with respect to the fixed effects\\n            parameters.\\n        score_re : array_like\\n            The score vector with respect to the random effects\\n            parameters (excluding variance components parameters).\\n        score_vc : array_like\\n            The score vector with respect to variance components\\n            parameters.\\n        '\n    (score_fe, score_re, score_vc) = self.score_full(params, calc_fe=calc_fe)\n    params_vec = params.get_packed(use_sqrt=True, has_fe=True)\n    score_full = np.concatenate((score_fe, score_re, score_vc))\n    scr = 0.0\n    for i in range(len(params_vec)):\n        v = self._lin[i] + 2 * np.dot(self._quad[i], params_vec)\n        scr += score_full[i] * v\n    score_fe = scr[0:self.k_fe]\n    score_re = scr[self.k_fe:self.k_fe + self.k_re2]\n    score_vc = scr[self.k_fe + self.k_re2:]\n    return (score_fe, score_re, score_vc)",
            "def score_sqrt(self, params, calc_fe=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the score with respect to transformed parameters.\\n\\n        Calculates the score vector with respect to the\\n        parameterization in which the random effects covariance matrix\\n        is represented through its Cholesky square root.\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The model parameters.  If array-like must contain packed\\n            parameters that are compatible with this model instance.\\n        calc_fe : bool\\n            If True, calculate the score vector for the fixed effects\\n            parameters.  If False, this vector is not calculated, and\\n            a vector of zeros is returned in its place.\\n\\n        Returns\\n        -------\\n        score_fe : array_like\\n            The score vector with respect to the fixed effects\\n            parameters.\\n        score_re : array_like\\n            The score vector with respect to the random effects\\n            parameters (excluding variance components parameters).\\n        score_vc : array_like\\n            The score vector with respect to variance components\\n            parameters.\\n        '\n    (score_fe, score_re, score_vc) = self.score_full(params, calc_fe=calc_fe)\n    params_vec = params.get_packed(use_sqrt=True, has_fe=True)\n    score_full = np.concatenate((score_fe, score_re, score_vc))\n    scr = 0.0\n    for i in range(len(params_vec)):\n        v = self._lin[i] + 2 * np.dot(self._quad[i], params_vec)\n        scr += score_full[i] * v\n    score_fe = scr[0:self.k_fe]\n    score_re = scr[self.k_fe:self.k_fe + self.k_re2]\n    score_vc = scr[self.k_fe + self.k_re2:]\n    return (score_fe, score_re, score_vc)"
        ]
    },
    {
        "func_name": "hessian",
        "original": "def hessian(self, params):\n    \"\"\"\n        Returns the model's Hessian matrix.\n\n        Calculates the Hessian matrix for the linear mixed effects\n        model with respect to the parameterization in which the\n        covariance matrix is represented directly (without square-root\n        transformation).\n\n        Parameters\n        ----------\n        params : MixedLMParams or array_like\n            The model parameters at which the Hessian is calculated.\n            If array-like, must contain the packed parameters in a\n            form that is compatible with this model instance.\n\n        Returns\n        -------\n        hess : 2d ndarray\n            The Hessian matrix, evaluated at `params`.\n        sing : boolean\n            If True, the covariance matrix is singular and a\n            pseudo-inverse is returned.\n        \"\"\"\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, use_sqrt=self.use_sqrt, has_fe=True)\n    fe_params = params.fe_params\n    vcomp = params.vcomp\n    cov_re = params.cov_re\n    sing = False\n    if self.k_re > 0:\n        try:\n            cov_re_inv = np.linalg.inv(cov_re)\n        except np.linalg.LinAlgError:\n            cov_re_inv = np.linalg.pinv(cov_re)\n            sing = True\n    else:\n        cov_re_inv = np.empty((0, 0))\n    hess_fe = 0.0\n    hess_re = np.zeros((self.k_re2 + self.k_vc, self.k_re2 + self.k_vc))\n    hess_fere = np.zeros((self.k_re2 + self.k_vc, self.k_fe))\n    fac = self.n_totobs\n    if self.reml:\n        fac -= self.exog.shape[1]\n    rvir = 0.0\n    xtvix = 0.0\n    xtax = [0.0] * (self.k_re2 + self.k_vc)\n    m = self.k_re2 + self.k_vc\n    B = np.zeros(m)\n    D = np.zeros((m, m))\n    F = [[0.0] * m for k in range(m)]\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        vc_vari = np.zeros_like(vc_var)\n        ii = np.flatnonzero(vc_var >= 1e-10)\n        if len(ii) > 0:\n            vc_vari[ii] = 1 / vc_var[ii]\n        if len(ii) < len(vc_var):\n            sing = True\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, vc_vari)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        viexog = solver(exog)\n        xtvix += np.dot(exog.T, viexog)\n        vir = solver(resid)\n        rvir += np.dot(resid, vir)\n        for (jj1, matl1, matr1, vsl1, vsr1, sym1) in self._gen_dV_dPar(ex_r, solver, group_ix):\n            ul = _dot(viexog.T, matl1)\n            ur = _dot(matr1.T, vir)\n            hess_fere[jj1, :] += np.dot(ul, ur)\n            if not sym1:\n                ul = _dot(viexog.T, matr1)\n                ur = _dot(matl1.T, vir)\n                hess_fere[jj1, :] += np.dot(ul, ur)\n            if self.reml:\n                ul = _dot(viexog.T, matl1)\n                ur = ul if sym1 else np.dot(viexog.T, matr1)\n                ulr = _dot(ul, ur.T)\n                xtax[jj1] += ulr\n                if not sym1:\n                    xtax[jj1] += ulr.T\n            ul = _dot(vir, matl1)\n            ur = ul if sym1 else _dot(vir, matr1)\n            B[jj1] += np.dot(ul, ur) * (1 if sym1 else 2)\n            E = [(vsl1, matr1)]\n            if not sym1:\n                E.append((vsr1, matl1))\n            for (jj2, matl2, matr2, vsl2, vsr2, sym2) in self._gen_dV_dPar(ex_r, solver, group_ix, jj1):\n                re = sum([_multi_dot_three(matr2.T, x[0], x[1].T) for x in E])\n                vt = 2 * _dot(_multi_dot_three(vir[None, :], matl2, re), vir[:, None])\n                if not sym2:\n                    le = sum([_multi_dot_three(matl2.T, x[0], x[1].T) for x in E])\n                    vt += 2 * _dot(_multi_dot_three(vir[None, :], matr2, le), vir[:, None])\n                D[jj1, jj2] += np.squeeze(vt)\n                if jj1 != jj2:\n                    D[jj2, jj1] += np.squeeze(vt)\n                rt = _dotsum(vsl2, re.T) / 2\n                if not sym2:\n                    rt += _dotsum(vsr2, le.T) / 2\n                hess_re[jj1, jj2] += rt\n                if jj1 != jj2:\n                    hess_re[jj2, jj1] += rt\n                if self.reml:\n                    ev = sum([_dot(x[0], _dot(x[1].T, viexog)) for x in E])\n                    u1 = _dot(viexog.T, matl2)\n                    u2 = _dot(matr2.T, ev)\n                    um = np.dot(u1, u2)\n                    F[jj1][jj2] += um + um.T\n                    if not sym2:\n                        u1 = np.dot(viexog.T, matr2)\n                        u2 = np.dot(matl2.T, ev)\n                        um = np.dot(u1, u2)\n                        F[jj1][jj2] += um + um.T\n    hess_fe -= fac * xtvix / rvir\n    hess_re = hess_re - 0.5 * fac * (D / rvir - np.outer(B, B) / rvir ** 2)\n    hess_fere = -fac * hess_fere / rvir\n    if self.reml:\n        QL = [np.linalg.solve(xtvix, x) for x in xtax]\n        for j1 in range(self.k_re2 + self.k_vc):\n            for j2 in range(j1 + 1):\n                a = _dotsum(QL[j1].T, QL[j2])\n                a -= np.trace(np.linalg.solve(xtvix, F[j1][j2]))\n                a *= 0.5\n                hess_re[j1, j2] += a\n                if j1 > j2:\n                    hess_re[j2, j1] += a\n    m = self.k_fe + self.k_re2 + self.k_vc\n    hess = np.zeros((m, m))\n    hess[0:self.k_fe, 0:self.k_fe] = hess_fe\n    hess[0:self.k_fe, self.k_fe:] = hess_fere.T\n    hess[self.k_fe:, 0:self.k_fe] = hess_fere\n    hess[self.k_fe:, self.k_fe:] = hess_re\n    return (hess, sing)",
        "mutated": [
            "def hessian(self, params):\n    if False:\n        i = 10\n    \"\\n        Returns the model's Hessian matrix.\\n\\n        Calculates the Hessian matrix for the linear mixed effects\\n        model with respect to the parameterization in which the\\n        covariance matrix is represented directly (without square-root\\n        transformation).\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The model parameters at which the Hessian is calculated.\\n            If array-like, must contain the packed parameters in a\\n            form that is compatible with this model instance.\\n\\n        Returns\\n        -------\\n        hess : 2d ndarray\\n            The Hessian matrix, evaluated at `params`.\\n        sing : boolean\\n            If True, the covariance matrix is singular and a\\n            pseudo-inverse is returned.\\n        \"\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, use_sqrt=self.use_sqrt, has_fe=True)\n    fe_params = params.fe_params\n    vcomp = params.vcomp\n    cov_re = params.cov_re\n    sing = False\n    if self.k_re > 0:\n        try:\n            cov_re_inv = np.linalg.inv(cov_re)\n        except np.linalg.LinAlgError:\n            cov_re_inv = np.linalg.pinv(cov_re)\n            sing = True\n    else:\n        cov_re_inv = np.empty((0, 0))\n    hess_fe = 0.0\n    hess_re = np.zeros((self.k_re2 + self.k_vc, self.k_re2 + self.k_vc))\n    hess_fere = np.zeros((self.k_re2 + self.k_vc, self.k_fe))\n    fac = self.n_totobs\n    if self.reml:\n        fac -= self.exog.shape[1]\n    rvir = 0.0\n    xtvix = 0.0\n    xtax = [0.0] * (self.k_re2 + self.k_vc)\n    m = self.k_re2 + self.k_vc\n    B = np.zeros(m)\n    D = np.zeros((m, m))\n    F = [[0.0] * m for k in range(m)]\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        vc_vari = np.zeros_like(vc_var)\n        ii = np.flatnonzero(vc_var >= 1e-10)\n        if len(ii) > 0:\n            vc_vari[ii] = 1 / vc_var[ii]\n        if len(ii) < len(vc_var):\n            sing = True\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, vc_vari)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        viexog = solver(exog)\n        xtvix += np.dot(exog.T, viexog)\n        vir = solver(resid)\n        rvir += np.dot(resid, vir)\n        for (jj1, matl1, matr1, vsl1, vsr1, sym1) in self._gen_dV_dPar(ex_r, solver, group_ix):\n            ul = _dot(viexog.T, matl1)\n            ur = _dot(matr1.T, vir)\n            hess_fere[jj1, :] += np.dot(ul, ur)\n            if not sym1:\n                ul = _dot(viexog.T, matr1)\n                ur = _dot(matl1.T, vir)\n                hess_fere[jj1, :] += np.dot(ul, ur)\n            if self.reml:\n                ul = _dot(viexog.T, matl1)\n                ur = ul if sym1 else np.dot(viexog.T, matr1)\n                ulr = _dot(ul, ur.T)\n                xtax[jj1] += ulr\n                if not sym1:\n                    xtax[jj1] += ulr.T\n            ul = _dot(vir, matl1)\n            ur = ul if sym1 else _dot(vir, matr1)\n            B[jj1] += np.dot(ul, ur) * (1 if sym1 else 2)\n            E = [(vsl1, matr1)]\n            if not sym1:\n                E.append((vsr1, matl1))\n            for (jj2, matl2, matr2, vsl2, vsr2, sym2) in self._gen_dV_dPar(ex_r, solver, group_ix, jj1):\n                re = sum([_multi_dot_three(matr2.T, x[0], x[1].T) for x in E])\n                vt = 2 * _dot(_multi_dot_three(vir[None, :], matl2, re), vir[:, None])\n                if not sym2:\n                    le = sum([_multi_dot_three(matl2.T, x[0], x[1].T) for x in E])\n                    vt += 2 * _dot(_multi_dot_three(vir[None, :], matr2, le), vir[:, None])\n                D[jj1, jj2] += np.squeeze(vt)\n                if jj1 != jj2:\n                    D[jj2, jj1] += np.squeeze(vt)\n                rt = _dotsum(vsl2, re.T) / 2\n                if not sym2:\n                    rt += _dotsum(vsr2, le.T) / 2\n                hess_re[jj1, jj2] += rt\n                if jj1 != jj2:\n                    hess_re[jj2, jj1] += rt\n                if self.reml:\n                    ev = sum([_dot(x[0], _dot(x[1].T, viexog)) for x in E])\n                    u1 = _dot(viexog.T, matl2)\n                    u2 = _dot(matr2.T, ev)\n                    um = np.dot(u1, u2)\n                    F[jj1][jj2] += um + um.T\n                    if not sym2:\n                        u1 = np.dot(viexog.T, matr2)\n                        u2 = np.dot(matl2.T, ev)\n                        um = np.dot(u1, u2)\n                        F[jj1][jj2] += um + um.T\n    hess_fe -= fac * xtvix / rvir\n    hess_re = hess_re - 0.5 * fac * (D / rvir - np.outer(B, B) / rvir ** 2)\n    hess_fere = -fac * hess_fere / rvir\n    if self.reml:\n        QL = [np.linalg.solve(xtvix, x) for x in xtax]\n        for j1 in range(self.k_re2 + self.k_vc):\n            for j2 in range(j1 + 1):\n                a = _dotsum(QL[j1].T, QL[j2])\n                a -= np.trace(np.linalg.solve(xtvix, F[j1][j2]))\n                a *= 0.5\n                hess_re[j1, j2] += a\n                if j1 > j2:\n                    hess_re[j2, j1] += a\n    m = self.k_fe + self.k_re2 + self.k_vc\n    hess = np.zeros((m, m))\n    hess[0:self.k_fe, 0:self.k_fe] = hess_fe\n    hess[0:self.k_fe, self.k_fe:] = hess_fere.T\n    hess[self.k_fe:, 0:self.k_fe] = hess_fere\n    hess[self.k_fe:, self.k_fe:] = hess_re\n    return (hess, sing)",
            "def hessian(self, params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Returns the model's Hessian matrix.\\n\\n        Calculates the Hessian matrix for the linear mixed effects\\n        model with respect to the parameterization in which the\\n        covariance matrix is represented directly (without square-root\\n        transformation).\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The model parameters at which the Hessian is calculated.\\n            If array-like, must contain the packed parameters in a\\n            form that is compatible with this model instance.\\n\\n        Returns\\n        -------\\n        hess : 2d ndarray\\n            The Hessian matrix, evaluated at `params`.\\n        sing : boolean\\n            If True, the covariance matrix is singular and a\\n            pseudo-inverse is returned.\\n        \"\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, use_sqrt=self.use_sqrt, has_fe=True)\n    fe_params = params.fe_params\n    vcomp = params.vcomp\n    cov_re = params.cov_re\n    sing = False\n    if self.k_re > 0:\n        try:\n            cov_re_inv = np.linalg.inv(cov_re)\n        except np.linalg.LinAlgError:\n            cov_re_inv = np.linalg.pinv(cov_re)\n            sing = True\n    else:\n        cov_re_inv = np.empty((0, 0))\n    hess_fe = 0.0\n    hess_re = np.zeros((self.k_re2 + self.k_vc, self.k_re2 + self.k_vc))\n    hess_fere = np.zeros((self.k_re2 + self.k_vc, self.k_fe))\n    fac = self.n_totobs\n    if self.reml:\n        fac -= self.exog.shape[1]\n    rvir = 0.0\n    xtvix = 0.0\n    xtax = [0.0] * (self.k_re2 + self.k_vc)\n    m = self.k_re2 + self.k_vc\n    B = np.zeros(m)\n    D = np.zeros((m, m))\n    F = [[0.0] * m for k in range(m)]\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        vc_vari = np.zeros_like(vc_var)\n        ii = np.flatnonzero(vc_var >= 1e-10)\n        if len(ii) > 0:\n            vc_vari[ii] = 1 / vc_var[ii]\n        if len(ii) < len(vc_var):\n            sing = True\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, vc_vari)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        viexog = solver(exog)\n        xtvix += np.dot(exog.T, viexog)\n        vir = solver(resid)\n        rvir += np.dot(resid, vir)\n        for (jj1, matl1, matr1, vsl1, vsr1, sym1) in self._gen_dV_dPar(ex_r, solver, group_ix):\n            ul = _dot(viexog.T, matl1)\n            ur = _dot(matr1.T, vir)\n            hess_fere[jj1, :] += np.dot(ul, ur)\n            if not sym1:\n                ul = _dot(viexog.T, matr1)\n                ur = _dot(matl1.T, vir)\n                hess_fere[jj1, :] += np.dot(ul, ur)\n            if self.reml:\n                ul = _dot(viexog.T, matl1)\n                ur = ul if sym1 else np.dot(viexog.T, matr1)\n                ulr = _dot(ul, ur.T)\n                xtax[jj1] += ulr\n                if not sym1:\n                    xtax[jj1] += ulr.T\n            ul = _dot(vir, matl1)\n            ur = ul if sym1 else _dot(vir, matr1)\n            B[jj1] += np.dot(ul, ur) * (1 if sym1 else 2)\n            E = [(vsl1, matr1)]\n            if not sym1:\n                E.append((vsr1, matl1))\n            for (jj2, matl2, matr2, vsl2, vsr2, sym2) in self._gen_dV_dPar(ex_r, solver, group_ix, jj1):\n                re = sum([_multi_dot_three(matr2.T, x[0], x[1].T) for x in E])\n                vt = 2 * _dot(_multi_dot_three(vir[None, :], matl2, re), vir[:, None])\n                if not sym2:\n                    le = sum([_multi_dot_three(matl2.T, x[0], x[1].T) for x in E])\n                    vt += 2 * _dot(_multi_dot_three(vir[None, :], matr2, le), vir[:, None])\n                D[jj1, jj2] += np.squeeze(vt)\n                if jj1 != jj2:\n                    D[jj2, jj1] += np.squeeze(vt)\n                rt = _dotsum(vsl2, re.T) / 2\n                if not sym2:\n                    rt += _dotsum(vsr2, le.T) / 2\n                hess_re[jj1, jj2] += rt\n                if jj1 != jj2:\n                    hess_re[jj2, jj1] += rt\n                if self.reml:\n                    ev = sum([_dot(x[0], _dot(x[1].T, viexog)) for x in E])\n                    u1 = _dot(viexog.T, matl2)\n                    u2 = _dot(matr2.T, ev)\n                    um = np.dot(u1, u2)\n                    F[jj1][jj2] += um + um.T\n                    if not sym2:\n                        u1 = np.dot(viexog.T, matr2)\n                        u2 = np.dot(matl2.T, ev)\n                        um = np.dot(u1, u2)\n                        F[jj1][jj2] += um + um.T\n    hess_fe -= fac * xtvix / rvir\n    hess_re = hess_re - 0.5 * fac * (D / rvir - np.outer(B, B) / rvir ** 2)\n    hess_fere = -fac * hess_fere / rvir\n    if self.reml:\n        QL = [np.linalg.solve(xtvix, x) for x in xtax]\n        for j1 in range(self.k_re2 + self.k_vc):\n            for j2 in range(j1 + 1):\n                a = _dotsum(QL[j1].T, QL[j2])\n                a -= np.trace(np.linalg.solve(xtvix, F[j1][j2]))\n                a *= 0.5\n                hess_re[j1, j2] += a\n                if j1 > j2:\n                    hess_re[j2, j1] += a\n    m = self.k_fe + self.k_re2 + self.k_vc\n    hess = np.zeros((m, m))\n    hess[0:self.k_fe, 0:self.k_fe] = hess_fe\n    hess[0:self.k_fe, self.k_fe:] = hess_fere.T\n    hess[self.k_fe:, 0:self.k_fe] = hess_fere\n    hess[self.k_fe:, self.k_fe:] = hess_re\n    return (hess, sing)",
            "def hessian(self, params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Returns the model's Hessian matrix.\\n\\n        Calculates the Hessian matrix for the linear mixed effects\\n        model with respect to the parameterization in which the\\n        covariance matrix is represented directly (without square-root\\n        transformation).\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The model parameters at which the Hessian is calculated.\\n            If array-like, must contain the packed parameters in a\\n            form that is compatible with this model instance.\\n\\n        Returns\\n        -------\\n        hess : 2d ndarray\\n            The Hessian matrix, evaluated at `params`.\\n        sing : boolean\\n            If True, the covariance matrix is singular and a\\n            pseudo-inverse is returned.\\n        \"\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, use_sqrt=self.use_sqrt, has_fe=True)\n    fe_params = params.fe_params\n    vcomp = params.vcomp\n    cov_re = params.cov_re\n    sing = False\n    if self.k_re > 0:\n        try:\n            cov_re_inv = np.linalg.inv(cov_re)\n        except np.linalg.LinAlgError:\n            cov_re_inv = np.linalg.pinv(cov_re)\n            sing = True\n    else:\n        cov_re_inv = np.empty((0, 0))\n    hess_fe = 0.0\n    hess_re = np.zeros((self.k_re2 + self.k_vc, self.k_re2 + self.k_vc))\n    hess_fere = np.zeros((self.k_re2 + self.k_vc, self.k_fe))\n    fac = self.n_totobs\n    if self.reml:\n        fac -= self.exog.shape[1]\n    rvir = 0.0\n    xtvix = 0.0\n    xtax = [0.0] * (self.k_re2 + self.k_vc)\n    m = self.k_re2 + self.k_vc\n    B = np.zeros(m)\n    D = np.zeros((m, m))\n    F = [[0.0] * m for k in range(m)]\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        vc_vari = np.zeros_like(vc_var)\n        ii = np.flatnonzero(vc_var >= 1e-10)\n        if len(ii) > 0:\n            vc_vari[ii] = 1 / vc_var[ii]\n        if len(ii) < len(vc_var):\n            sing = True\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, vc_vari)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        viexog = solver(exog)\n        xtvix += np.dot(exog.T, viexog)\n        vir = solver(resid)\n        rvir += np.dot(resid, vir)\n        for (jj1, matl1, matr1, vsl1, vsr1, sym1) in self._gen_dV_dPar(ex_r, solver, group_ix):\n            ul = _dot(viexog.T, matl1)\n            ur = _dot(matr1.T, vir)\n            hess_fere[jj1, :] += np.dot(ul, ur)\n            if not sym1:\n                ul = _dot(viexog.T, matr1)\n                ur = _dot(matl1.T, vir)\n                hess_fere[jj1, :] += np.dot(ul, ur)\n            if self.reml:\n                ul = _dot(viexog.T, matl1)\n                ur = ul if sym1 else np.dot(viexog.T, matr1)\n                ulr = _dot(ul, ur.T)\n                xtax[jj1] += ulr\n                if not sym1:\n                    xtax[jj1] += ulr.T\n            ul = _dot(vir, matl1)\n            ur = ul if sym1 else _dot(vir, matr1)\n            B[jj1] += np.dot(ul, ur) * (1 if sym1 else 2)\n            E = [(vsl1, matr1)]\n            if not sym1:\n                E.append((vsr1, matl1))\n            for (jj2, matl2, matr2, vsl2, vsr2, sym2) in self._gen_dV_dPar(ex_r, solver, group_ix, jj1):\n                re = sum([_multi_dot_three(matr2.T, x[0], x[1].T) for x in E])\n                vt = 2 * _dot(_multi_dot_three(vir[None, :], matl2, re), vir[:, None])\n                if not sym2:\n                    le = sum([_multi_dot_three(matl2.T, x[0], x[1].T) for x in E])\n                    vt += 2 * _dot(_multi_dot_three(vir[None, :], matr2, le), vir[:, None])\n                D[jj1, jj2] += np.squeeze(vt)\n                if jj1 != jj2:\n                    D[jj2, jj1] += np.squeeze(vt)\n                rt = _dotsum(vsl2, re.T) / 2\n                if not sym2:\n                    rt += _dotsum(vsr2, le.T) / 2\n                hess_re[jj1, jj2] += rt\n                if jj1 != jj2:\n                    hess_re[jj2, jj1] += rt\n                if self.reml:\n                    ev = sum([_dot(x[0], _dot(x[1].T, viexog)) for x in E])\n                    u1 = _dot(viexog.T, matl2)\n                    u2 = _dot(matr2.T, ev)\n                    um = np.dot(u1, u2)\n                    F[jj1][jj2] += um + um.T\n                    if not sym2:\n                        u1 = np.dot(viexog.T, matr2)\n                        u2 = np.dot(matl2.T, ev)\n                        um = np.dot(u1, u2)\n                        F[jj1][jj2] += um + um.T\n    hess_fe -= fac * xtvix / rvir\n    hess_re = hess_re - 0.5 * fac * (D / rvir - np.outer(B, B) / rvir ** 2)\n    hess_fere = -fac * hess_fere / rvir\n    if self.reml:\n        QL = [np.linalg.solve(xtvix, x) for x in xtax]\n        for j1 in range(self.k_re2 + self.k_vc):\n            for j2 in range(j1 + 1):\n                a = _dotsum(QL[j1].T, QL[j2])\n                a -= np.trace(np.linalg.solve(xtvix, F[j1][j2]))\n                a *= 0.5\n                hess_re[j1, j2] += a\n                if j1 > j2:\n                    hess_re[j2, j1] += a\n    m = self.k_fe + self.k_re2 + self.k_vc\n    hess = np.zeros((m, m))\n    hess[0:self.k_fe, 0:self.k_fe] = hess_fe\n    hess[0:self.k_fe, self.k_fe:] = hess_fere.T\n    hess[self.k_fe:, 0:self.k_fe] = hess_fere\n    hess[self.k_fe:, self.k_fe:] = hess_re\n    return (hess, sing)",
            "def hessian(self, params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Returns the model's Hessian matrix.\\n\\n        Calculates the Hessian matrix for the linear mixed effects\\n        model with respect to the parameterization in which the\\n        covariance matrix is represented directly (without square-root\\n        transformation).\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The model parameters at which the Hessian is calculated.\\n            If array-like, must contain the packed parameters in a\\n            form that is compatible with this model instance.\\n\\n        Returns\\n        -------\\n        hess : 2d ndarray\\n            The Hessian matrix, evaluated at `params`.\\n        sing : boolean\\n            If True, the covariance matrix is singular and a\\n            pseudo-inverse is returned.\\n        \"\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, use_sqrt=self.use_sqrt, has_fe=True)\n    fe_params = params.fe_params\n    vcomp = params.vcomp\n    cov_re = params.cov_re\n    sing = False\n    if self.k_re > 0:\n        try:\n            cov_re_inv = np.linalg.inv(cov_re)\n        except np.linalg.LinAlgError:\n            cov_re_inv = np.linalg.pinv(cov_re)\n            sing = True\n    else:\n        cov_re_inv = np.empty((0, 0))\n    hess_fe = 0.0\n    hess_re = np.zeros((self.k_re2 + self.k_vc, self.k_re2 + self.k_vc))\n    hess_fere = np.zeros((self.k_re2 + self.k_vc, self.k_fe))\n    fac = self.n_totobs\n    if self.reml:\n        fac -= self.exog.shape[1]\n    rvir = 0.0\n    xtvix = 0.0\n    xtax = [0.0] * (self.k_re2 + self.k_vc)\n    m = self.k_re2 + self.k_vc\n    B = np.zeros(m)\n    D = np.zeros((m, m))\n    F = [[0.0] * m for k in range(m)]\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        vc_vari = np.zeros_like(vc_var)\n        ii = np.flatnonzero(vc_var >= 1e-10)\n        if len(ii) > 0:\n            vc_vari[ii] = 1 / vc_var[ii]\n        if len(ii) < len(vc_var):\n            sing = True\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, vc_vari)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        viexog = solver(exog)\n        xtvix += np.dot(exog.T, viexog)\n        vir = solver(resid)\n        rvir += np.dot(resid, vir)\n        for (jj1, matl1, matr1, vsl1, vsr1, sym1) in self._gen_dV_dPar(ex_r, solver, group_ix):\n            ul = _dot(viexog.T, matl1)\n            ur = _dot(matr1.T, vir)\n            hess_fere[jj1, :] += np.dot(ul, ur)\n            if not sym1:\n                ul = _dot(viexog.T, matr1)\n                ur = _dot(matl1.T, vir)\n                hess_fere[jj1, :] += np.dot(ul, ur)\n            if self.reml:\n                ul = _dot(viexog.T, matl1)\n                ur = ul if sym1 else np.dot(viexog.T, matr1)\n                ulr = _dot(ul, ur.T)\n                xtax[jj1] += ulr\n                if not sym1:\n                    xtax[jj1] += ulr.T\n            ul = _dot(vir, matl1)\n            ur = ul if sym1 else _dot(vir, matr1)\n            B[jj1] += np.dot(ul, ur) * (1 if sym1 else 2)\n            E = [(vsl1, matr1)]\n            if not sym1:\n                E.append((vsr1, matl1))\n            for (jj2, matl2, matr2, vsl2, vsr2, sym2) in self._gen_dV_dPar(ex_r, solver, group_ix, jj1):\n                re = sum([_multi_dot_three(matr2.T, x[0], x[1].T) for x in E])\n                vt = 2 * _dot(_multi_dot_three(vir[None, :], matl2, re), vir[:, None])\n                if not sym2:\n                    le = sum([_multi_dot_three(matl2.T, x[0], x[1].T) for x in E])\n                    vt += 2 * _dot(_multi_dot_three(vir[None, :], matr2, le), vir[:, None])\n                D[jj1, jj2] += np.squeeze(vt)\n                if jj1 != jj2:\n                    D[jj2, jj1] += np.squeeze(vt)\n                rt = _dotsum(vsl2, re.T) / 2\n                if not sym2:\n                    rt += _dotsum(vsr2, le.T) / 2\n                hess_re[jj1, jj2] += rt\n                if jj1 != jj2:\n                    hess_re[jj2, jj1] += rt\n                if self.reml:\n                    ev = sum([_dot(x[0], _dot(x[1].T, viexog)) for x in E])\n                    u1 = _dot(viexog.T, matl2)\n                    u2 = _dot(matr2.T, ev)\n                    um = np.dot(u1, u2)\n                    F[jj1][jj2] += um + um.T\n                    if not sym2:\n                        u1 = np.dot(viexog.T, matr2)\n                        u2 = np.dot(matl2.T, ev)\n                        um = np.dot(u1, u2)\n                        F[jj1][jj2] += um + um.T\n    hess_fe -= fac * xtvix / rvir\n    hess_re = hess_re - 0.5 * fac * (D / rvir - np.outer(B, B) / rvir ** 2)\n    hess_fere = -fac * hess_fere / rvir\n    if self.reml:\n        QL = [np.linalg.solve(xtvix, x) for x in xtax]\n        for j1 in range(self.k_re2 + self.k_vc):\n            for j2 in range(j1 + 1):\n                a = _dotsum(QL[j1].T, QL[j2])\n                a -= np.trace(np.linalg.solve(xtvix, F[j1][j2]))\n                a *= 0.5\n                hess_re[j1, j2] += a\n                if j1 > j2:\n                    hess_re[j2, j1] += a\n    m = self.k_fe + self.k_re2 + self.k_vc\n    hess = np.zeros((m, m))\n    hess[0:self.k_fe, 0:self.k_fe] = hess_fe\n    hess[0:self.k_fe, self.k_fe:] = hess_fere.T\n    hess[self.k_fe:, 0:self.k_fe] = hess_fere\n    hess[self.k_fe:, self.k_fe:] = hess_re\n    return (hess, sing)",
            "def hessian(self, params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Returns the model's Hessian matrix.\\n\\n        Calculates the Hessian matrix for the linear mixed effects\\n        model with respect to the parameterization in which the\\n        covariance matrix is represented directly (without square-root\\n        transformation).\\n\\n        Parameters\\n        ----------\\n        params : MixedLMParams or array_like\\n            The model parameters at which the Hessian is calculated.\\n            If array-like, must contain the packed parameters in a\\n            form that is compatible with this model instance.\\n\\n        Returns\\n        -------\\n        hess : 2d ndarray\\n            The Hessian matrix, evaluated at `params`.\\n        sing : boolean\\n            If True, the covariance matrix is singular and a\\n            pseudo-inverse is returned.\\n        \"\n    if type(params) is not MixedLMParams:\n        params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, use_sqrt=self.use_sqrt, has_fe=True)\n    fe_params = params.fe_params\n    vcomp = params.vcomp\n    cov_re = params.cov_re\n    sing = False\n    if self.k_re > 0:\n        try:\n            cov_re_inv = np.linalg.inv(cov_re)\n        except np.linalg.LinAlgError:\n            cov_re_inv = np.linalg.pinv(cov_re)\n            sing = True\n    else:\n        cov_re_inv = np.empty((0, 0))\n    hess_fe = 0.0\n    hess_re = np.zeros((self.k_re2 + self.k_vc, self.k_re2 + self.k_vc))\n    hess_fere = np.zeros((self.k_re2 + self.k_vc, self.k_fe))\n    fac = self.n_totobs\n    if self.reml:\n        fac -= self.exog.shape[1]\n    rvir = 0.0\n    xtvix = 0.0\n    xtax = [0.0] * (self.k_re2 + self.k_vc)\n    m = self.k_re2 + self.k_vc\n    B = np.zeros(m)\n    D = np.zeros((m, m))\n    F = [[0.0] * m for k in range(m)]\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        vc_vari = np.zeros_like(vc_var)\n        ii = np.flatnonzero(vc_var >= 1e-10)\n        if len(ii) > 0:\n            vc_vari[ii] = 1 / vc_var[ii]\n        if len(ii) < len(vc_var):\n            sing = True\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, vc_vari)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        viexog = solver(exog)\n        xtvix += np.dot(exog.T, viexog)\n        vir = solver(resid)\n        rvir += np.dot(resid, vir)\n        for (jj1, matl1, matr1, vsl1, vsr1, sym1) in self._gen_dV_dPar(ex_r, solver, group_ix):\n            ul = _dot(viexog.T, matl1)\n            ur = _dot(matr1.T, vir)\n            hess_fere[jj1, :] += np.dot(ul, ur)\n            if not sym1:\n                ul = _dot(viexog.T, matr1)\n                ur = _dot(matl1.T, vir)\n                hess_fere[jj1, :] += np.dot(ul, ur)\n            if self.reml:\n                ul = _dot(viexog.T, matl1)\n                ur = ul if sym1 else np.dot(viexog.T, matr1)\n                ulr = _dot(ul, ur.T)\n                xtax[jj1] += ulr\n                if not sym1:\n                    xtax[jj1] += ulr.T\n            ul = _dot(vir, matl1)\n            ur = ul if sym1 else _dot(vir, matr1)\n            B[jj1] += np.dot(ul, ur) * (1 if sym1 else 2)\n            E = [(vsl1, matr1)]\n            if not sym1:\n                E.append((vsr1, matl1))\n            for (jj2, matl2, matr2, vsl2, vsr2, sym2) in self._gen_dV_dPar(ex_r, solver, group_ix, jj1):\n                re = sum([_multi_dot_three(matr2.T, x[0], x[1].T) for x in E])\n                vt = 2 * _dot(_multi_dot_three(vir[None, :], matl2, re), vir[:, None])\n                if not sym2:\n                    le = sum([_multi_dot_three(matl2.T, x[0], x[1].T) for x in E])\n                    vt += 2 * _dot(_multi_dot_three(vir[None, :], matr2, le), vir[:, None])\n                D[jj1, jj2] += np.squeeze(vt)\n                if jj1 != jj2:\n                    D[jj2, jj1] += np.squeeze(vt)\n                rt = _dotsum(vsl2, re.T) / 2\n                if not sym2:\n                    rt += _dotsum(vsr2, le.T) / 2\n                hess_re[jj1, jj2] += rt\n                if jj1 != jj2:\n                    hess_re[jj2, jj1] += rt\n                if self.reml:\n                    ev = sum([_dot(x[0], _dot(x[1].T, viexog)) for x in E])\n                    u1 = _dot(viexog.T, matl2)\n                    u2 = _dot(matr2.T, ev)\n                    um = np.dot(u1, u2)\n                    F[jj1][jj2] += um + um.T\n                    if not sym2:\n                        u1 = np.dot(viexog.T, matr2)\n                        u2 = np.dot(matl2.T, ev)\n                        um = np.dot(u1, u2)\n                        F[jj1][jj2] += um + um.T\n    hess_fe -= fac * xtvix / rvir\n    hess_re = hess_re - 0.5 * fac * (D / rvir - np.outer(B, B) / rvir ** 2)\n    hess_fere = -fac * hess_fere / rvir\n    if self.reml:\n        QL = [np.linalg.solve(xtvix, x) for x in xtax]\n        for j1 in range(self.k_re2 + self.k_vc):\n            for j2 in range(j1 + 1):\n                a = _dotsum(QL[j1].T, QL[j2])\n                a -= np.trace(np.linalg.solve(xtvix, F[j1][j2]))\n                a *= 0.5\n                hess_re[j1, j2] += a\n                if j1 > j2:\n                    hess_re[j2, j1] += a\n    m = self.k_fe + self.k_re2 + self.k_vc\n    hess = np.zeros((m, m))\n    hess[0:self.k_fe, 0:self.k_fe] = hess_fe\n    hess[0:self.k_fe, self.k_fe:] = hess_fere.T\n    hess[self.k_fe:, 0:self.k_fe] = hess_fere\n    hess[self.k_fe:, self.k_fe:] = hess_re\n    return (hess, sing)"
        ]
    },
    {
        "func_name": "get_scale",
        "original": "def get_scale(self, fe_params, cov_re, vcomp):\n    \"\"\"\n        Returns the estimated error variance based on given estimates\n        of the slopes and random effects covariance matrix.\n\n        Parameters\n        ----------\n        fe_params : array_like\n            The regression slope estimates\n        cov_re : 2d array_like\n            Estimate of the random effects covariance matrix\n        vcomp : array_like\n            Estimate of the variance components\n\n        Returns\n        -------\n        scale : float\n            The estimated error variance.\n        \"\"\"\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = np.linalg.pinv(cov_re)\n        warnings.warn(_warn_cov_sing)\n    qf = 0.0\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        mat = solver(resid)\n        qf += np.dot(resid, mat)\n    if self.reml:\n        qf /= self.n_totobs - self.k_fe\n    else:\n        qf /= self.n_totobs\n    return qf",
        "mutated": [
            "def get_scale(self, fe_params, cov_re, vcomp):\n    if False:\n        i = 10\n    '\\n        Returns the estimated error variance based on given estimates\\n        of the slopes and random effects covariance matrix.\\n\\n        Parameters\\n        ----------\\n        fe_params : array_like\\n            The regression slope estimates\\n        cov_re : 2d array_like\\n            Estimate of the random effects covariance matrix\\n        vcomp : array_like\\n            Estimate of the variance components\\n\\n        Returns\\n        -------\\n        scale : float\\n            The estimated error variance.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = np.linalg.pinv(cov_re)\n        warnings.warn(_warn_cov_sing)\n    qf = 0.0\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        mat = solver(resid)\n        qf += np.dot(resid, mat)\n    if self.reml:\n        qf /= self.n_totobs - self.k_fe\n    else:\n        qf /= self.n_totobs\n    return qf",
            "def get_scale(self, fe_params, cov_re, vcomp):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the estimated error variance based on given estimates\\n        of the slopes and random effects covariance matrix.\\n\\n        Parameters\\n        ----------\\n        fe_params : array_like\\n            The regression slope estimates\\n        cov_re : 2d array_like\\n            Estimate of the random effects covariance matrix\\n        vcomp : array_like\\n            Estimate of the variance components\\n\\n        Returns\\n        -------\\n        scale : float\\n            The estimated error variance.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = np.linalg.pinv(cov_re)\n        warnings.warn(_warn_cov_sing)\n    qf = 0.0\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        mat = solver(resid)\n        qf += np.dot(resid, mat)\n    if self.reml:\n        qf /= self.n_totobs - self.k_fe\n    else:\n        qf /= self.n_totobs\n    return qf",
            "def get_scale(self, fe_params, cov_re, vcomp):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the estimated error variance based on given estimates\\n        of the slopes and random effects covariance matrix.\\n\\n        Parameters\\n        ----------\\n        fe_params : array_like\\n            The regression slope estimates\\n        cov_re : 2d array_like\\n            Estimate of the random effects covariance matrix\\n        vcomp : array_like\\n            Estimate of the variance components\\n\\n        Returns\\n        -------\\n        scale : float\\n            The estimated error variance.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = np.linalg.pinv(cov_re)\n        warnings.warn(_warn_cov_sing)\n    qf = 0.0\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        mat = solver(resid)\n        qf += np.dot(resid, mat)\n    if self.reml:\n        qf /= self.n_totobs - self.k_fe\n    else:\n        qf /= self.n_totobs\n    return qf",
            "def get_scale(self, fe_params, cov_re, vcomp):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the estimated error variance based on given estimates\\n        of the slopes and random effects covariance matrix.\\n\\n        Parameters\\n        ----------\\n        fe_params : array_like\\n            The regression slope estimates\\n        cov_re : 2d array_like\\n            Estimate of the random effects covariance matrix\\n        vcomp : array_like\\n            Estimate of the variance components\\n\\n        Returns\\n        -------\\n        scale : float\\n            The estimated error variance.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = np.linalg.pinv(cov_re)\n        warnings.warn(_warn_cov_sing)\n    qf = 0.0\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        mat = solver(resid)\n        qf += np.dot(resid, mat)\n    if self.reml:\n        qf /= self.n_totobs - self.k_fe\n    else:\n        qf /= self.n_totobs\n    return qf",
            "def get_scale(self, fe_params, cov_re, vcomp):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the estimated error variance based on given estimates\\n        of the slopes and random effects covariance matrix.\\n\\n        Parameters\\n        ----------\\n        fe_params : array_like\\n            The regression slope estimates\\n        cov_re : 2d array_like\\n            Estimate of the random effects covariance matrix\\n        vcomp : array_like\\n            Estimate of the variance components\\n\\n        Returns\\n        -------\\n        scale : float\\n            The estimated error variance.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = np.linalg.pinv(cov_re)\n        warnings.warn(_warn_cov_sing)\n    qf = 0.0\n    for (group_ix, group) in enumerate(self.group_labels):\n        vc_var = self._expand_vcomp(vcomp, group_ix)\n        exog = self.exog_li[group_ix]\n        (ex_r, ex2_r) = (self._aex_r[group_ix], self._aex_r2[group_ix])\n        solver = _smw_solver(1.0, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        resid = self.endog_li[group_ix]\n        if self.k_fe > 0:\n            expval = np.dot(exog, fe_params)\n            resid = resid - expval\n        mat = solver(resid)\n        qf += np.dot(resid, mat)\n    if self.reml:\n        qf /= self.n_totobs - self.k_fe\n    else:\n        qf /= self.n_totobs\n    return qf"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self, start_params=None, reml=True, niter_sa=0, do_cg=True, fe_pen=None, cov_pen=None, free=None, full_output=False, method=None, **fit_kwargs):\n    \"\"\"\n        Fit a linear mixed model to the data.\n\n        Parameters\n        ----------\n        start_params : array_like or MixedLMParams\n            Starting values for the profile log-likelihood.  If not a\n            `MixedLMParams` instance, this should be an array\n            containing the packed parameters for the profile\n            log-likelihood, including the fixed effects\n            parameters.\n        reml : bool\n            If true, fit according to the REML likelihood, else\n            fit the standard likelihood using ML.\n        niter_sa : int\n            Currently this argument is ignored and has no effect\n            on the results.\n        cov_pen : CovariancePenalty object\n            A penalty for the random effects covariance matrix\n        do_cg : bool, defaults to True\n            If False, the optimization is skipped and a results\n            object at the given (or default) starting values is\n            returned.\n        fe_pen : Penalty object\n            A penalty on the fixed effects\n        free : MixedLMParams object\n            If not `None`, this is a mask that allows parameters to be\n            held fixed at specified values.  A 1 indicates that the\n            corresponding parameter is estimated, a 0 indicates that\n            it is fixed at its starting value.  Setting the `cov_re`\n            component to the identity matrix fits a model with\n            independent random effects.  Note that some optimization\n            methods do not respect this constraint (bfgs and lbfgs both\n            work).\n        full_output : bool\n            If true, attach iteration history to results\n        method : str\n            Optimization method.  Can be a scipy.optimize method name,\n            or a list of such names to be tried in sequence.\n        **fit_kwargs\n            Additional keyword arguments passed to fit.\n\n        Returns\n        -------\n        A MixedLMResults instance.\n        \"\"\"\n    _allowed_kwargs = ['gtol', 'maxiter', 'eps', 'maxcor', 'ftol', 'tol', 'disp', 'maxls']\n    for x in fit_kwargs.keys():\n        if x not in _allowed_kwargs:\n            warnings.warn('Argument %s not used by MixedLM.fit' % x)\n    if method is None:\n        method = ['bfgs', 'lbfgs', 'cg']\n    elif isinstance(method, str):\n        method = [method]\n    for meth in method:\n        if meth.lower() in ['newton', 'ncg']:\n            raise ValueError('method %s not available for MixedLM' % meth)\n    self.reml = reml\n    self.cov_pen = cov_pen\n    self.fe_pen = fe_pen\n    self._cov_sing = 0\n    self._freepat = free\n    if full_output:\n        hist = []\n    else:\n        hist = None\n    if start_params is None:\n        params = MixedLMParams(self.k_fe, self.k_re, self.k_vc)\n        params.fe_params = np.zeros(self.k_fe)\n        params.cov_re = np.eye(self.k_re)\n        params.vcomp = np.ones(self.k_vc)\n    elif isinstance(start_params, MixedLMParams):\n        params = start_params\n    elif len(start_params) == self.k_fe + self.k_re2 + self.k_vc:\n        params = MixedLMParams.from_packed(start_params, self.k_fe, self.k_re, self.use_sqrt, has_fe=True)\n    elif len(start_params) == self.k_re2 + self.k_vc:\n        params = MixedLMParams.from_packed(start_params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    else:\n        raise ValueError('invalid start_params')\n    if do_cg:\n        fit_kwargs['retall'] = hist is not None\n        if 'disp' not in fit_kwargs:\n            fit_kwargs['disp'] = False\n        packed = params.get_packed(use_sqrt=self.use_sqrt, has_fe=False)\n        if niter_sa > 0:\n            warnings.warn('niter_sa is currently ignored')\n        for j in range(len(method)):\n            rslt = super(MixedLM, self).fit(start_params=packed, skip_hessian=True, method=method[j], **fit_kwargs)\n            if rslt.mle_retvals['converged']:\n                break\n            packed = rslt.params\n            if j + 1 < len(method):\n                next_method = method[j + 1]\n                warnings.warn('Retrying MixedLM optimization with %s' % next_method, ConvergenceWarning)\n            else:\n                msg = 'MixedLM optimization failed, ' + 'trying a different optimizer may help.'\n                warnings.warn(msg, ConvergenceWarning)\n        params = np.atleast_1d(rslt.params)\n        if hist is not None:\n            hist.append(rslt.mle_retvals)\n    converged = rslt.mle_retvals['converged']\n    if not converged:\n        gn = self.score(rslt.params)\n        gn = np.sqrt(np.sum(gn ** 2))\n        msg = 'Gradient optimization failed, |grad| = %f' % gn\n        warnings.warn(msg, ConvergenceWarning)\n    params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, use_sqrt=self.use_sqrt, has_fe=False)\n    cov_re_unscaled = params.cov_re\n    vcomp_unscaled = params.vcomp\n    (fe_params, sing) = self.get_fe_params(cov_re_unscaled, vcomp_unscaled)\n    params.fe_params = fe_params\n    scale = self.get_scale(fe_params, cov_re_unscaled, vcomp_unscaled)\n    cov_re = scale * cov_re_unscaled\n    vcomp = scale * vcomp_unscaled\n    f1 = self.k_re > 0 and np.min(np.abs(np.diag(cov_re))) < 0.01\n    f2 = self.k_vc > 0 and np.min(np.abs(vcomp)) < 0.01\n    if f1 or f2:\n        msg = 'The MLE may be on the boundary of the parameter space.'\n        warnings.warn(msg, ConvergenceWarning)\n    (hess, sing) = self.hessian(params)\n    if sing:\n        warnings.warn(_warn_cov_sing)\n    hess_diag = np.diag(hess)\n    if free is not None:\n        pcov = np.zeros_like(hess)\n        pat = self._freepat.get_packed(use_sqrt=False, has_fe=True)\n        ii = np.flatnonzero(pat)\n        hess_diag = hess_diag[ii]\n        if len(ii) > 0:\n            hess1 = hess[np.ix_(ii, ii)]\n            pcov[np.ix_(ii, ii)] = np.linalg.inv(-hess1)\n    else:\n        pcov = np.linalg.inv(-hess)\n    if np.any(hess_diag >= 0):\n        msg = 'The Hessian matrix at the estimated parameter values ' + 'is not positive definite.'\n        warnings.warn(msg, ConvergenceWarning)\n    params_packed = params.get_packed(use_sqrt=False, has_fe=True)\n    results = MixedLMResults(self, params_packed, pcov / scale)\n    results.params_object = params\n    results.fe_params = fe_params\n    results.cov_re = cov_re\n    results.vcomp = vcomp\n    results.scale = scale\n    results.cov_re_unscaled = cov_re_unscaled\n    results.method = 'REML' if self.reml else 'ML'\n    results.converged = converged\n    results.hist = hist\n    results.reml = self.reml\n    results.cov_pen = self.cov_pen\n    results.k_fe = self.k_fe\n    results.k_re = self.k_re\n    results.k_re2 = self.k_re2\n    results.k_vc = self.k_vc\n    results.use_sqrt = self.use_sqrt\n    results.freepat = self._freepat\n    return MixedLMResultsWrapper(results)",
        "mutated": [
            "def fit(self, start_params=None, reml=True, niter_sa=0, do_cg=True, fe_pen=None, cov_pen=None, free=None, full_output=False, method=None, **fit_kwargs):\n    if False:\n        i = 10\n    '\\n        Fit a linear mixed model to the data.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like or MixedLMParams\\n            Starting values for the profile log-likelihood.  If not a\\n            `MixedLMParams` instance, this should be an array\\n            containing the packed parameters for the profile\\n            log-likelihood, including the fixed effects\\n            parameters.\\n        reml : bool\\n            If true, fit according to the REML likelihood, else\\n            fit the standard likelihood using ML.\\n        niter_sa : int\\n            Currently this argument is ignored and has no effect\\n            on the results.\\n        cov_pen : CovariancePenalty object\\n            A penalty for the random effects covariance matrix\\n        do_cg : bool, defaults to True\\n            If False, the optimization is skipped and a results\\n            object at the given (or default) starting values is\\n            returned.\\n        fe_pen : Penalty object\\n            A penalty on the fixed effects\\n        free : MixedLMParams object\\n            If not `None`, this is a mask that allows parameters to be\\n            held fixed at specified values.  A 1 indicates that the\\n            corresponding parameter is estimated, a 0 indicates that\\n            it is fixed at its starting value.  Setting the `cov_re`\\n            component to the identity matrix fits a model with\\n            independent random effects.  Note that some optimization\\n            methods do not respect this constraint (bfgs and lbfgs both\\n            work).\\n        full_output : bool\\n            If true, attach iteration history to results\\n        method : str\\n            Optimization method.  Can be a scipy.optimize method name,\\n            or a list of such names to be tried in sequence.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        A MixedLMResults instance.\\n        '\n    _allowed_kwargs = ['gtol', 'maxiter', 'eps', 'maxcor', 'ftol', 'tol', 'disp', 'maxls']\n    for x in fit_kwargs.keys():\n        if x not in _allowed_kwargs:\n            warnings.warn('Argument %s not used by MixedLM.fit' % x)\n    if method is None:\n        method = ['bfgs', 'lbfgs', 'cg']\n    elif isinstance(method, str):\n        method = [method]\n    for meth in method:\n        if meth.lower() in ['newton', 'ncg']:\n            raise ValueError('method %s not available for MixedLM' % meth)\n    self.reml = reml\n    self.cov_pen = cov_pen\n    self.fe_pen = fe_pen\n    self._cov_sing = 0\n    self._freepat = free\n    if full_output:\n        hist = []\n    else:\n        hist = None\n    if start_params is None:\n        params = MixedLMParams(self.k_fe, self.k_re, self.k_vc)\n        params.fe_params = np.zeros(self.k_fe)\n        params.cov_re = np.eye(self.k_re)\n        params.vcomp = np.ones(self.k_vc)\n    elif isinstance(start_params, MixedLMParams):\n        params = start_params\n    elif len(start_params) == self.k_fe + self.k_re2 + self.k_vc:\n        params = MixedLMParams.from_packed(start_params, self.k_fe, self.k_re, self.use_sqrt, has_fe=True)\n    elif len(start_params) == self.k_re2 + self.k_vc:\n        params = MixedLMParams.from_packed(start_params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    else:\n        raise ValueError('invalid start_params')\n    if do_cg:\n        fit_kwargs['retall'] = hist is not None\n        if 'disp' not in fit_kwargs:\n            fit_kwargs['disp'] = False\n        packed = params.get_packed(use_sqrt=self.use_sqrt, has_fe=False)\n        if niter_sa > 0:\n            warnings.warn('niter_sa is currently ignored')\n        for j in range(len(method)):\n            rslt = super(MixedLM, self).fit(start_params=packed, skip_hessian=True, method=method[j], **fit_kwargs)\n            if rslt.mle_retvals['converged']:\n                break\n            packed = rslt.params\n            if j + 1 < len(method):\n                next_method = method[j + 1]\n                warnings.warn('Retrying MixedLM optimization with %s' % next_method, ConvergenceWarning)\n            else:\n                msg = 'MixedLM optimization failed, ' + 'trying a different optimizer may help.'\n                warnings.warn(msg, ConvergenceWarning)\n        params = np.atleast_1d(rslt.params)\n        if hist is not None:\n            hist.append(rslt.mle_retvals)\n    converged = rslt.mle_retvals['converged']\n    if not converged:\n        gn = self.score(rslt.params)\n        gn = np.sqrt(np.sum(gn ** 2))\n        msg = 'Gradient optimization failed, |grad| = %f' % gn\n        warnings.warn(msg, ConvergenceWarning)\n    params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, use_sqrt=self.use_sqrt, has_fe=False)\n    cov_re_unscaled = params.cov_re\n    vcomp_unscaled = params.vcomp\n    (fe_params, sing) = self.get_fe_params(cov_re_unscaled, vcomp_unscaled)\n    params.fe_params = fe_params\n    scale = self.get_scale(fe_params, cov_re_unscaled, vcomp_unscaled)\n    cov_re = scale * cov_re_unscaled\n    vcomp = scale * vcomp_unscaled\n    f1 = self.k_re > 0 and np.min(np.abs(np.diag(cov_re))) < 0.01\n    f2 = self.k_vc > 0 and np.min(np.abs(vcomp)) < 0.01\n    if f1 or f2:\n        msg = 'The MLE may be on the boundary of the parameter space.'\n        warnings.warn(msg, ConvergenceWarning)\n    (hess, sing) = self.hessian(params)\n    if sing:\n        warnings.warn(_warn_cov_sing)\n    hess_diag = np.diag(hess)\n    if free is not None:\n        pcov = np.zeros_like(hess)\n        pat = self._freepat.get_packed(use_sqrt=False, has_fe=True)\n        ii = np.flatnonzero(pat)\n        hess_diag = hess_diag[ii]\n        if len(ii) > 0:\n            hess1 = hess[np.ix_(ii, ii)]\n            pcov[np.ix_(ii, ii)] = np.linalg.inv(-hess1)\n    else:\n        pcov = np.linalg.inv(-hess)\n    if np.any(hess_diag >= 0):\n        msg = 'The Hessian matrix at the estimated parameter values ' + 'is not positive definite.'\n        warnings.warn(msg, ConvergenceWarning)\n    params_packed = params.get_packed(use_sqrt=False, has_fe=True)\n    results = MixedLMResults(self, params_packed, pcov / scale)\n    results.params_object = params\n    results.fe_params = fe_params\n    results.cov_re = cov_re\n    results.vcomp = vcomp\n    results.scale = scale\n    results.cov_re_unscaled = cov_re_unscaled\n    results.method = 'REML' if self.reml else 'ML'\n    results.converged = converged\n    results.hist = hist\n    results.reml = self.reml\n    results.cov_pen = self.cov_pen\n    results.k_fe = self.k_fe\n    results.k_re = self.k_re\n    results.k_re2 = self.k_re2\n    results.k_vc = self.k_vc\n    results.use_sqrt = self.use_sqrt\n    results.freepat = self._freepat\n    return MixedLMResultsWrapper(results)",
            "def fit(self, start_params=None, reml=True, niter_sa=0, do_cg=True, fe_pen=None, cov_pen=None, free=None, full_output=False, method=None, **fit_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Fit a linear mixed model to the data.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like or MixedLMParams\\n            Starting values for the profile log-likelihood.  If not a\\n            `MixedLMParams` instance, this should be an array\\n            containing the packed parameters for the profile\\n            log-likelihood, including the fixed effects\\n            parameters.\\n        reml : bool\\n            If true, fit according to the REML likelihood, else\\n            fit the standard likelihood using ML.\\n        niter_sa : int\\n            Currently this argument is ignored and has no effect\\n            on the results.\\n        cov_pen : CovariancePenalty object\\n            A penalty for the random effects covariance matrix\\n        do_cg : bool, defaults to True\\n            If False, the optimization is skipped and a results\\n            object at the given (or default) starting values is\\n            returned.\\n        fe_pen : Penalty object\\n            A penalty on the fixed effects\\n        free : MixedLMParams object\\n            If not `None`, this is a mask that allows parameters to be\\n            held fixed at specified values.  A 1 indicates that the\\n            corresponding parameter is estimated, a 0 indicates that\\n            it is fixed at its starting value.  Setting the `cov_re`\\n            component to the identity matrix fits a model with\\n            independent random effects.  Note that some optimization\\n            methods do not respect this constraint (bfgs and lbfgs both\\n            work).\\n        full_output : bool\\n            If true, attach iteration history to results\\n        method : str\\n            Optimization method.  Can be a scipy.optimize method name,\\n            or a list of such names to be tried in sequence.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        A MixedLMResults instance.\\n        '\n    _allowed_kwargs = ['gtol', 'maxiter', 'eps', 'maxcor', 'ftol', 'tol', 'disp', 'maxls']\n    for x in fit_kwargs.keys():\n        if x not in _allowed_kwargs:\n            warnings.warn('Argument %s not used by MixedLM.fit' % x)\n    if method is None:\n        method = ['bfgs', 'lbfgs', 'cg']\n    elif isinstance(method, str):\n        method = [method]\n    for meth in method:\n        if meth.lower() in ['newton', 'ncg']:\n            raise ValueError('method %s not available for MixedLM' % meth)\n    self.reml = reml\n    self.cov_pen = cov_pen\n    self.fe_pen = fe_pen\n    self._cov_sing = 0\n    self._freepat = free\n    if full_output:\n        hist = []\n    else:\n        hist = None\n    if start_params is None:\n        params = MixedLMParams(self.k_fe, self.k_re, self.k_vc)\n        params.fe_params = np.zeros(self.k_fe)\n        params.cov_re = np.eye(self.k_re)\n        params.vcomp = np.ones(self.k_vc)\n    elif isinstance(start_params, MixedLMParams):\n        params = start_params\n    elif len(start_params) == self.k_fe + self.k_re2 + self.k_vc:\n        params = MixedLMParams.from_packed(start_params, self.k_fe, self.k_re, self.use_sqrt, has_fe=True)\n    elif len(start_params) == self.k_re2 + self.k_vc:\n        params = MixedLMParams.from_packed(start_params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    else:\n        raise ValueError('invalid start_params')\n    if do_cg:\n        fit_kwargs['retall'] = hist is not None\n        if 'disp' not in fit_kwargs:\n            fit_kwargs['disp'] = False\n        packed = params.get_packed(use_sqrt=self.use_sqrt, has_fe=False)\n        if niter_sa > 0:\n            warnings.warn('niter_sa is currently ignored')\n        for j in range(len(method)):\n            rslt = super(MixedLM, self).fit(start_params=packed, skip_hessian=True, method=method[j], **fit_kwargs)\n            if rslt.mle_retvals['converged']:\n                break\n            packed = rslt.params\n            if j + 1 < len(method):\n                next_method = method[j + 1]\n                warnings.warn('Retrying MixedLM optimization with %s' % next_method, ConvergenceWarning)\n            else:\n                msg = 'MixedLM optimization failed, ' + 'trying a different optimizer may help.'\n                warnings.warn(msg, ConvergenceWarning)\n        params = np.atleast_1d(rslt.params)\n        if hist is not None:\n            hist.append(rslt.mle_retvals)\n    converged = rslt.mle_retvals['converged']\n    if not converged:\n        gn = self.score(rslt.params)\n        gn = np.sqrt(np.sum(gn ** 2))\n        msg = 'Gradient optimization failed, |grad| = %f' % gn\n        warnings.warn(msg, ConvergenceWarning)\n    params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, use_sqrt=self.use_sqrt, has_fe=False)\n    cov_re_unscaled = params.cov_re\n    vcomp_unscaled = params.vcomp\n    (fe_params, sing) = self.get_fe_params(cov_re_unscaled, vcomp_unscaled)\n    params.fe_params = fe_params\n    scale = self.get_scale(fe_params, cov_re_unscaled, vcomp_unscaled)\n    cov_re = scale * cov_re_unscaled\n    vcomp = scale * vcomp_unscaled\n    f1 = self.k_re > 0 and np.min(np.abs(np.diag(cov_re))) < 0.01\n    f2 = self.k_vc > 0 and np.min(np.abs(vcomp)) < 0.01\n    if f1 or f2:\n        msg = 'The MLE may be on the boundary of the parameter space.'\n        warnings.warn(msg, ConvergenceWarning)\n    (hess, sing) = self.hessian(params)\n    if sing:\n        warnings.warn(_warn_cov_sing)\n    hess_diag = np.diag(hess)\n    if free is not None:\n        pcov = np.zeros_like(hess)\n        pat = self._freepat.get_packed(use_sqrt=False, has_fe=True)\n        ii = np.flatnonzero(pat)\n        hess_diag = hess_diag[ii]\n        if len(ii) > 0:\n            hess1 = hess[np.ix_(ii, ii)]\n            pcov[np.ix_(ii, ii)] = np.linalg.inv(-hess1)\n    else:\n        pcov = np.linalg.inv(-hess)\n    if np.any(hess_diag >= 0):\n        msg = 'The Hessian matrix at the estimated parameter values ' + 'is not positive definite.'\n        warnings.warn(msg, ConvergenceWarning)\n    params_packed = params.get_packed(use_sqrt=False, has_fe=True)\n    results = MixedLMResults(self, params_packed, pcov / scale)\n    results.params_object = params\n    results.fe_params = fe_params\n    results.cov_re = cov_re\n    results.vcomp = vcomp\n    results.scale = scale\n    results.cov_re_unscaled = cov_re_unscaled\n    results.method = 'REML' if self.reml else 'ML'\n    results.converged = converged\n    results.hist = hist\n    results.reml = self.reml\n    results.cov_pen = self.cov_pen\n    results.k_fe = self.k_fe\n    results.k_re = self.k_re\n    results.k_re2 = self.k_re2\n    results.k_vc = self.k_vc\n    results.use_sqrt = self.use_sqrt\n    results.freepat = self._freepat\n    return MixedLMResultsWrapper(results)",
            "def fit(self, start_params=None, reml=True, niter_sa=0, do_cg=True, fe_pen=None, cov_pen=None, free=None, full_output=False, method=None, **fit_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Fit a linear mixed model to the data.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like or MixedLMParams\\n            Starting values for the profile log-likelihood.  If not a\\n            `MixedLMParams` instance, this should be an array\\n            containing the packed parameters for the profile\\n            log-likelihood, including the fixed effects\\n            parameters.\\n        reml : bool\\n            If true, fit according to the REML likelihood, else\\n            fit the standard likelihood using ML.\\n        niter_sa : int\\n            Currently this argument is ignored and has no effect\\n            on the results.\\n        cov_pen : CovariancePenalty object\\n            A penalty for the random effects covariance matrix\\n        do_cg : bool, defaults to True\\n            If False, the optimization is skipped and a results\\n            object at the given (or default) starting values is\\n            returned.\\n        fe_pen : Penalty object\\n            A penalty on the fixed effects\\n        free : MixedLMParams object\\n            If not `None`, this is a mask that allows parameters to be\\n            held fixed at specified values.  A 1 indicates that the\\n            corresponding parameter is estimated, a 0 indicates that\\n            it is fixed at its starting value.  Setting the `cov_re`\\n            component to the identity matrix fits a model with\\n            independent random effects.  Note that some optimization\\n            methods do not respect this constraint (bfgs and lbfgs both\\n            work).\\n        full_output : bool\\n            If true, attach iteration history to results\\n        method : str\\n            Optimization method.  Can be a scipy.optimize method name,\\n            or a list of such names to be tried in sequence.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        A MixedLMResults instance.\\n        '\n    _allowed_kwargs = ['gtol', 'maxiter', 'eps', 'maxcor', 'ftol', 'tol', 'disp', 'maxls']\n    for x in fit_kwargs.keys():\n        if x not in _allowed_kwargs:\n            warnings.warn('Argument %s not used by MixedLM.fit' % x)\n    if method is None:\n        method = ['bfgs', 'lbfgs', 'cg']\n    elif isinstance(method, str):\n        method = [method]\n    for meth in method:\n        if meth.lower() in ['newton', 'ncg']:\n            raise ValueError('method %s not available for MixedLM' % meth)\n    self.reml = reml\n    self.cov_pen = cov_pen\n    self.fe_pen = fe_pen\n    self._cov_sing = 0\n    self._freepat = free\n    if full_output:\n        hist = []\n    else:\n        hist = None\n    if start_params is None:\n        params = MixedLMParams(self.k_fe, self.k_re, self.k_vc)\n        params.fe_params = np.zeros(self.k_fe)\n        params.cov_re = np.eye(self.k_re)\n        params.vcomp = np.ones(self.k_vc)\n    elif isinstance(start_params, MixedLMParams):\n        params = start_params\n    elif len(start_params) == self.k_fe + self.k_re2 + self.k_vc:\n        params = MixedLMParams.from_packed(start_params, self.k_fe, self.k_re, self.use_sqrt, has_fe=True)\n    elif len(start_params) == self.k_re2 + self.k_vc:\n        params = MixedLMParams.from_packed(start_params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    else:\n        raise ValueError('invalid start_params')\n    if do_cg:\n        fit_kwargs['retall'] = hist is not None\n        if 'disp' not in fit_kwargs:\n            fit_kwargs['disp'] = False\n        packed = params.get_packed(use_sqrt=self.use_sqrt, has_fe=False)\n        if niter_sa > 0:\n            warnings.warn('niter_sa is currently ignored')\n        for j in range(len(method)):\n            rslt = super(MixedLM, self).fit(start_params=packed, skip_hessian=True, method=method[j], **fit_kwargs)\n            if rslt.mle_retvals['converged']:\n                break\n            packed = rslt.params\n            if j + 1 < len(method):\n                next_method = method[j + 1]\n                warnings.warn('Retrying MixedLM optimization with %s' % next_method, ConvergenceWarning)\n            else:\n                msg = 'MixedLM optimization failed, ' + 'trying a different optimizer may help.'\n                warnings.warn(msg, ConvergenceWarning)\n        params = np.atleast_1d(rslt.params)\n        if hist is not None:\n            hist.append(rslt.mle_retvals)\n    converged = rslt.mle_retvals['converged']\n    if not converged:\n        gn = self.score(rslt.params)\n        gn = np.sqrt(np.sum(gn ** 2))\n        msg = 'Gradient optimization failed, |grad| = %f' % gn\n        warnings.warn(msg, ConvergenceWarning)\n    params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, use_sqrt=self.use_sqrt, has_fe=False)\n    cov_re_unscaled = params.cov_re\n    vcomp_unscaled = params.vcomp\n    (fe_params, sing) = self.get_fe_params(cov_re_unscaled, vcomp_unscaled)\n    params.fe_params = fe_params\n    scale = self.get_scale(fe_params, cov_re_unscaled, vcomp_unscaled)\n    cov_re = scale * cov_re_unscaled\n    vcomp = scale * vcomp_unscaled\n    f1 = self.k_re > 0 and np.min(np.abs(np.diag(cov_re))) < 0.01\n    f2 = self.k_vc > 0 and np.min(np.abs(vcomp)) < 0.01\n    if f1 or f2:\n        msg = 'The MLE may be on the boundary of the parameter space.'\n        warnings.warn(msg, ConvergenceWarning)\n    (hess, sing) = self.hessian(params)\n    if sing:\n        warnings.warn(_warn_cov_sing)\n    hess_diag = np.diag(hess)\n    if free is not None:\n        pcov = np.zeros_like(hess)\n        pat = self._freepat.get_packed(use_sqrt=False, has_fe=True)\n        ii = np.flatnonzero(pat)\n        hess_diag = hess_diag[ii]\n        if len(ii) > 0:\n            hess1 = hess[np.ix_(ii, ii)]\n            pcov[np.ix_(ii, ii)] = np.linalg.inv(-hess1)\n    else:\n        pcov = np.linalg.inv(-hess)\n    if np.any(hess_diag >= 0):\n        msg = 'The Hessian matrix at the estimated parameter values ' + 'is not positive definite.'\n        warnings.warn(msg, ConvergenceWarning)\n    params_packed = params.get_packed(use_sqrt=False, has_fe=True)\n    results = MixedLMResults(self, params_packed, pcov / scale)\n    results.params_object = params\n    results.fe_params = fe_params\n    results.cov_re = cov_re\n    results.vcomp = vcomp\n    results.scale = scale\n    results.cov_re_unscaled = cov_re_unscaled\n    results.method = 'REML' if self.reml else 'ML'\n    results.converged = converged\n    results.hist = hist\n    results.reml = self.reml\n    results.cov_pen = self.cov_pen\n    results.k_fe = self.k_fe\n    results.k_re = self.k_re\n    results.k_re2 = self.k_re2\n    results.k_vc = self.k_vc\n    results.use_sqrt = self.use_sqrt\n    results.freepat = self._freepat\n    return MixedLMResultsWrapper(results)",
            "def fit(self, start_params=None, reml=True, niter_sa=0, do_cg=True, fe_pen=None, cov_pen=None, free=None, full_output=False, method=None, **fit_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Fit a linear mixed model to the data.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like or MixedLMParams\\n            Starting values for the profile log-likelihood.  If not a\\n            `MixedLMParams` instance, this should be an array\\n            containing the packed parameters for the profile\\n            log-likelihood, including the fixed effects\\n            parameters.\\n        reml : bool\\n            If true, fit according to the REML likelihood, else\\n            fit the standard likelihood using ML.\\n        niter_sa : int\\n            Currently this argument is ignored and has no effect\\n            on the results.\\n        cov_pen : CovariancePenalty object\\n            A penalty for the random effects covariance matrix\\n        do_cg : bool, defaults to True\\n            If False, the optimization is skipped and a results\\n            object at the given (or default) starting values is\\n            returned.\\n        fe_pen : Penalty object\\n            A penalty on the fixed effects\\n        free : MixedLMParams object\\n            If not `None`, this is a mask that allows parameters to be\\n            held fixed at specified values.  A 1 indicates that the\\n            corresponding parameter is estimated, a 0 indicates that\\n            it is fixed at its starting value.  Setting the `cov_re`\\n            component to the identity matrix fits a model with\\n            independent random effects.  Note that some optimization\\n            methods do not respect this constraint (bfgs and lbfgs both\\n            work).\\n        full_output : bool\\n            If true, attach iteration history to results\\n        method : str\\n            Optimization method.  Can be a scipy.optimize method name,\\n            or a list of such names to be tried in sequence.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        A MixedLMResults instance.\\n        '\n    _allowed_kwargs = ['gtol', 'maxiter', 'eps', 'maxcor', 'ftol', 'tol', 'disp', 'maxls']\n    for x in fit_kwargs.keys():\n        if x not in _allowed_kwargs:\n            warnings.warn('Argument %s not used by MixedLM.fit' % x)\n    if method is None:\n        method = ['bfgs', 'lbfgs', 'cg']\n    elif isinstance(method, str):\n        method = [method]\n    for meth in method:\n        if meth.lower() in ['newton', 'ncg']:\n            raise ValueError('method %s not available for MixedLM' % meth)\n    self.reml = reml\n    self.cov_pen = cov_pen\n    self.fe_pen = fe_pen\n    self._cov_sing = 0\n    self._freepat = free\n    if full_output:\n        hist = []\n    else:\n        hist = None\n    if start_params is None:\n        params = MixedLMParams(self.k_fe, self.k_re, self.k_vc)\n        params.fe_params = np.zeros(self.k_fe)\n        params.cov_re = np.eye(self.k_re)\n        params.vcomp = np.ones(self.k_vc)\n    elif isinstance(start_params, MixedLMParams):\n        params = start_params\n    elif len(start_params) == self.k_fe + self.k_re2 + self.k_vc:\n        params = MixedLMParams.from_packed(start_params, self.k_fe, self.k_re, self.use_sqrt, has_fe=True)\n    elif len(start_params) == self.k_re2 + self.k_vc:\n        params = MixedLMParams.from_packed(start_params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    else:\n        raise ValueError('invalid start_params')\n    if do_cg:\n        fit_kwargs['retall'] = hist is not None\n        if 'disp' not in fit_kwargs:\n            fit_kwargs['disp'] = False\n        packed = params.get_packed(use_sqrt=self.use_sqrt, has_fe=False)\n        if niter_sa > 0:\n            warnings.warn('niter_sa is currently ignored')\n        for j in range(len(method)):\n            rslt = super(MixedLM, self).fit(start_params=packed, skip_hessian=True, method=method[j], **fit_kwargs)\n            if rslt.mle_retvals['converged']:\n                break\n            packed = rslt.params\n            if j + 1 < len(method):\n                next_method = method[j + 1]\n                warnings.warn('Retrying MixedLM optimization with %s' % next_method, ConvergenceWarning)\n            else:\n                msg = 'MixedLM optimization failed, ' + 'trying a different optimizer may help.'\n                warnings.warn(msg, ConvergenceWarning)\n        params = np.atleast_1d(rslt.params)\n        if hist is not None:\n            hist.append(rslt.mle_retvals)\n    converged = rslt.mle_retvals['converged']\n    if not converged:\n        gn = self.score(rslt.params)\n        gn = np.sqrt(np.sum(gn ** 2))\n        msg = 'Gradient optimization failed, |grad| = %f' % gn\n        warnings.warn(msg, ConvergenceWarning)\n    params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, use_sqrt=self.use_sqrt, has_fe=False)\n    cov_re_unscaled = params.cov_re\n    vcomp_unscaled = params.vcomp\n    (fe_params, sing) = self.get_fe_params(cov_re_unscaled, vcomp_unscaled)\n    params.fe_params = fe_params\n    scale = self.get_scale(fe_params, cov_re_unscaled, vcomp_unscaled)\n    cov_re = scale * cov_re_unscaled\n    vcomp = scale * vcomp_unscaled\n    f1 = self.k_re > 0 and np.min(np.abs(np.diag(cov_re))) < 0.01\n    f2 = self.k_vc > 0 and np.min(np.abs(vcomp)) < 0.01\n    if f1 or f2:\n        msg = 'The MLE may be on the boundary of the parameter space.'\n        warnings.warn(msg, ConvergenceWarning)\n    (hess, sing) = self.hessian(params)\n    if sing:\n        warnings.warn(_warn_cov_sing)\n    hess_diag = np.diag(hess)\n    if free is not None:\n        pcov = np.zeros_like(hess)\n        pat = self._freepat.get_packed(use_sqrt=False, has_fe=True)\n        ii = np.flatnonzero(pat)\n        hess_diag = hess_diag[ii]\n        if len(ii) > 0:\n            hess1 = hess[np.ix_(ii, ii)]\n            pcov[np.ix_(ii, ii)] = np.linalg.inv(-hess1)\n    else:\n        pcov = np.linalg.inv(-hess)\n    if np.any(hess_diag >= 0):\n        msg = 'The Hessian matrix at the estimated parameter values ' + 'is not positive definite.'\n        warnings.warn(msg, ConvergenceWarning)\n    params_packed = params.get_packed(use_sqrt=False, has_fe=True)\n    results = MixedLMResults(self, params_packed, pcov / scale)\n    results.params_object = params\n    results.fe_params = fe_params\n    results.cov_re = cov_re\n    results.vcomp = vcomp\n    results.scale = scale\n    results.cov_re_unscaled = cov_re_unscaled\n    results.method = 'REML' if self.reml else 'ML'\n    results.converged = converged\n    results.hist = hist\n    results.reml = self.reml\n    results.cov_pen = self.cov_pen\n    results.k_fe = self.k_fe\n    results.k_re = self.k_re\n    results.k_re2 = self.k_re2\n    results.k_vc = self.k_vc\n    results.use_sqrt = self.use_sqrt\n    results.freepat = self._freepat\n    return MixedLMResultsWrapper(results)",
            "def fit(self, start_params=None, reml=True, niter_sa=0, do_cg=True, fe_pen=None, cov_pen=None, free=None, full_output=False, method=None, **fit_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Fit a linear mixed model to the data.\\n\\n        Parameters\\n        ----------\\n        start_params : array_like or MixedLMParams\\n            Starting values for the profile log-likelihood.  If not a\\n            `MixedLMParams` instance, this should be an array\\n            containing the packed parameters for the profile\\n            log-likelihood, including the fixed effects\\n            parameters.\\n        reml : bool\\n            If true, fit according to the REML likelihood, else\\n            fit the standard likelihood using ML.\\n        niter_sa : int\\n            Currently this argument is ignored and has no effect\\n            on the results.\\n        cov_pen : CovariancePenalty object\\n            A penalty for the random effects covariance matrix\\n        do_cg : bool, defaults to True\\n            If False, the optimization is skipped and a results\\n            object at the given (or default) starting values is\\n            returned.\\n        fe_pen : Penalty object\\n            A penalty on the fixed effects\\n        free : MixedLMParams object\\n            If not `None`, this is a mask that allows parameters to be\\n            held fixed at specified values.  A 1 indicates that the\\n            corresponding parameter is estimated, a 0 indicates that\\n            it is fixed at its starting value.  Setting the `cov_re`\\n            component to the identity matrix fits a model with\\n            independent random effects.  Note that some optimization\\n            methods do not respect this constraint (bfgs and lbfgs both\\n            work).\\n        full_output : bool\\n            If true, attach iteration history to results\\n        method : str\\n            Optimization method.  Can be a scipy.optimize method name,\\n            or a list of such names to be tried in sequence.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        A MixedLMResults instance.\\n        '\n    _allowed_kwargs = ['gtol', 'maxiter', 'eps', 'maxcor', 'ftol', 'tol', 'disp', 'maxls']\n    for x in fit_kwargs.keys():\n        if x not in _allowed_kwargs:\n            warnings.warn('Argument %s not used by MixedLM.fit' % x)\n    if method is None:\n        method = ['bfgs', 'lbfgs', 'cg']\n    elif isinstance(method, str):\n        method = [method]\n    for meth in method:\n        if meth.lower() in ['newton', 'ncg']:\n            raise ValueError('method %s not available for MixedLM' % meth)\n    self.reml = reml\n    self.cov_pen = cov_pen\n    self.fe_pen = fe_pen\n    self._cov_sing = 0\n    self._freepat = free\n    if full_output:\n        hist = []\n    else:\n        hist = None\n    if start_params is None:\n        params = MixedLMParams(self.k_fe, self.k_re, self.k_vc)\n        params.fe_params = np.zeros(self.k_fe)\n        params.cov_re = np.eye(self.k_re)\n        params.vcomp = np.ones(self.k_vc)\n    elif isinstance(start_params, MixedLMParams):\n        params = start_params\n    elif len(start_params) == self.k_fe + self.k_re2 + self.k_vc:\n        params = MixedLMParams.from_packed(start_params, self.k_fe, self.k_re, self.use_sqrt, has_fe=True)\n    elif len(start_params) == self.k_re2 + self.k_vc:\n        params = MixedLMParams.from_packed(start_params, self.k_fe, self.k_re, self.use_sqrt, has_fe=False)\n    else:\n        raise ValueError('invalid start_params')\n    if do_cg:\n        fit_kwargs['retall'] = hist is not None\n        if 'disp' not in fit_kwargs:\n            fit_kwargs['disp'] = False\n        packed = params.get_packed(use_sqrt=self.use_sqrt, has_fe=False)\n        if niter_sa > 0:\n            warnings.warn('niter_sa is currently ignored')\n        for j in range(len(method)):\n            rslt = super(MixedLM, self).fit(start_params=packed, skip_hessian=True, method=method[j], **fit_kwargs)\n            if rslt.mle_retvals['converged']:\n                break\n            packed = rslt.params\n            if j + 1 < len(method):\n                next_method = method[j + 1]\n                warnings.warn('Retrying MixedLM optimization with %s' % next_method, ConvergenceWarning)\n            else:\n                msg = 'MixedLM optimization failed, ' + 'trying a different optimizer may help.'\n                warnings.warn(msg, ConvergenceWarning)\n        params = np.atleast_1d(rslt.params)\n        if hist is not None:\n            hist.append(rslt.mle_retvals)\n    converged = rslt.mle_retvals['converged']\n    if not converged:\n        gn = self.score(rslt.params)\n        gn = np.sqrt(np.sum(gn ** 2))\n        msg = 'Gradient optimization failed, |grad| = %f' % gn\n        warnings.warn(msg, ConvergenceWarning)\n    params = MixedLMParams.from_packed(params, self.k_fe, self.k_re, use_sqrt=self.use_sqrt, has_fe=False)\n    cov_re_unscaled = params.cov_re\n    vcomp_unscaled = params.vcomp\n    (fe_params, sing) = self.get_fe_params(cov_re_unscaled, vcomp_unscaled)\n    params.fe_params = fe_params\n    scale = self.get_scale(fe_params, cov_re_unscaled, vcomp_unscaled)\n    cov_re = scale * cov_re_unscaled\n    vcomp = scale * vcomp_unscaled\n    f1 = self.k_re > 0 and np.min(np.abs(np.diag(cov_re))) < 0.01\n    f2 = self.k_vc > 0 and np.min(np.abs(vcomp)) < 0.01\n    if f1 or f2:\n        msg = 'The MLE may be on the boundary of the parameter space.'\n        warnings.warn(msg, ConvergenceWarning)\n    (hess, sing) = self.hessian(params)\n    if sing:\n        warnings.warn(_warn_cov_sing)\n    hess_diag = np.diag(hess)\n    if free is not None:\n        pcov = np.zeros_like(hess)\n        pat = self._freepat.get_packed(use_sqrt=False, has_fe=True)\n        ii = np.flatnonzero(pat)\n        hess_diag = hess_diag[ii]\n        if len(ii) > 0:\n            hess1 = hess[np.ix_(ii, ii)]\n            pcov[np.ix_(ii, ii)] = np.linalg.inv(-hess1)\n    else:\n        pcov = np.linalg.inv(-hess)\n    if np.any(hess_diag >= 0):\n        msg = 'The Hessian matrix at the estimated parameter values ' + 'is not positive definite.'\n        warnings.warn(msg, ConvergenceWarning)\n    params_packed = params.get_packed(use_sqrt=False, has_fe=True)\n    results = MixedLMResults(self, params_packed, pcov / scale)\n    results.params_object = params\n    results.fe_params = fe_params\n    results.cov_re = cov_re\n    results.vcomp = vcomp\n    results.scale = scale\n    results.cov_re_unscaled = cov_re_unscaled\n    results.method = 'REML' if self.reml else 'ML'\n    results.converged = converged\n    results.hist = hist\n    results.reml = self.reml\n    results.cov_pen = self.cov_pen\n    results.k_fe = self.k_fe\n    results.k_re = self.k_re\n    results.k_re2 = self.k_re2\n    results.k_vc = self.k_vc\n    results.use_sqrt = self.use_sqrt\n    results.freepat = self._freepat\n    return MixedLMResultsWrapper(results)"
        ]
    },
    {
        "func_name": "get_distribution",
        "original": "def get_distribution(self, params, scale, exog):\n    return _mixedlm_distribution(self, params, scale, exog)",
        "mutated": [
            "def get_distribution(self, params, scale, exog):\n    if False:\n        i = 10\n    return _mixedlm_distribution(self, params, scale, exog)",
            "def get_distribution(self, params, scale, exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return _mixedlm_distribution(self, params, scale, exog)",
            "def get_distribution(self, params, scale, exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return _mixedlm_distribution(self, params, scale, exog)",
            "def get_distribution(self, params, scale, exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return _mixedlm_distribution(self, params, scale, exog)",
            "def get_distribution(self, params, scale, exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return _mixedlm_distribution(self, params, scale, exog)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, model, params, scale, exog):\n    self.model = model\n    self.exog = exog if exog is not None else model.exog\n    po = MixedLMParams.from_packed(params, model.k_fe, model.k_re, False, True)\n    self.fe_params = po.fe_params\n    self.cov_re = scale * po.cov_re\n    self.vcomp = scale * po.vcomp\n    self.scale = scale\n    group_idx = np.zeros(model.nobs, dtype=int)\n    for (k, g) in enumerate(model.group_labels):\n        group_idx[model.row_indices[g]] = k\n    self.group_idx = group_idx",
        "mutated": [
            "def __init__(self, model, params, scale, exog):\n    if False:\n        i = 10\n    self.model = model\n    self.exog = exog if exog is not None else model.exog\n    po = MixedLMParams.from_packed(params, model.k_fe, model.k_re, False, True)\n    self.fe_params = po.fe_params\n    self.cov_re = scale * po.cov_re\n    self.vcomp = scale * po.vcomp\n    self.scale = scale\n    group_idx = np.zeros(model.nobs, dtype=int)\n    for (k, g) in enumerate(model.group_labels):\n        group_idx[model.row_indices[g]] = k\n    self.group_idx = group_idx",
            "def __init__(self, model, params, scale, exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.model = model\n    self.exog = exog if exog is not None else model.exog\n    po = MixedLMParams.from_packed(params, model.k_fe, model.k_re, False, True)\n    self.fe_params = po.fe_params\n    self.cov_re = scale * po.cov_re\n    self.vcomp = scale * po.vcomp\n    self.scale = scale\n    group_idx = np.zeros(model.nobs, dtype=int)\n    for (k, g) in enumerate(model.group_labels):\n        group_idx[model.row_indices[g]] = k\n    self.group_idx = group_idx",
            "def __init__(self, model, params, scale, exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.model = model\n    self.exog = exog if exog is not None else model.exog\n    po = MixedLMParams.from_packed(params, model.k_fe, model.k_re, False, True)\n    self.fe_params = po.fe_params\n    self.cov_re = scale * po.cov_re\n    self.vcomp = scale * po.vcomp\n    self.scale = scale\n    group_idx = np.zeros(model.nobs, dtype=int)\n    for (k, g) in enumerate(model.group_labels):\n        group_idx[model.row_indices[g]] = k\n    self.group_idx = group_idx",
            "def __init__(self, model, params, scale, exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.model = model\n    self.exog = exog if exog is not None else model.exog\n    po = MixedLMParams.from_packed(params, model.k_fe, model.k_re, False, True)\n    self.fe_params = po.fe_params\n    self.cov_re = scale * po.cov_re\n    self.vcomp = scale * po.vcomp\n    self.scale = scale\n    group_idx = np.zeros(model.nobs, dtype=int)\n    for (k, g) in enumerate(model.group_labels):\n        group_idx[model.row_indices[g]] = k\n    self.group_idx = group_idx",
            "def __init__(self, model, params, scale, exog):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.model = model\n    self.exog = exog if exog is not None else model.exog\n    po = MixedLMParams.from_packed(params, model.k_fe, model.k_re, False, True)\n    self.fe_params = po.fe_params\n    self.cov_re = scale * po.cov_re\n    self.vcomp = scale * po.vcomp\n    self.scale = scale\n    group_idx = np.zeros(model.nobs, dtype=int)\n    for (k, g) in enumerate(model.group_labels):\n        group_idx[model.row_indices[g]] = k\n    self.group_idx = group_idx"
        ]
    },
    {
        "func_name": "rvs",
        "original": "def rvs(self, n):\n    \"\"\"\n        Return a vector of simulated values from a mixed linear\n        model.\n\n        The parameter n is ignored, but required by the interface\n        \"\"\"\n    model = self.model\n    y = np.dot(self.exog, self.fe_params)\n    u = np.random.normal(size=(model.n_groups, model.k_re))\n    u = np.dot(u, np.linalg.cholesky(self.cov_re).T)\n    y += (u[self.group_idx, :] * model.exog_re).sum(1)\n    for (j, _) in enumerate(model.exog_vc.names):\n        ex = model.exog_vc.mats[j]\n        v = self.vcomp[j]\n        for (i, g) in enumerate(model.group_labels):\n            exg = ex[i]\n            ii = model.row_indices[g]\n            u = np.random.normal(size=exg.shape[1])\n            y[ii] += np.sqrt(v) * np.dot(exg, u)\n    y += np.sqrt(self.scale) * np.random.normal(size=len(y))\n    return y",
        "mutated": [
            "def rvs(self, n):\n    if False:\n        i = 10\n    '\\n        Return a vector of simulated values from a mixed linear\\n        model.\\n\\n        The parameter n is ignored, but required by the interface\\n        '\n    model = self.model\n    y = np.dot(self.exog, self.fe_params)\n    u = np.random.normal(size=(model.n_groups, model.k_re))\n    u = np.dot(u, np.linalg.cholesky(self.cov_re).T)\n    y += (u[self.group_idx, :] * model.exog_re).sum(1)\n    for (j, _) in enumerate(model.exog_vc.names):\n        ex = model.exog_vc.mats[j]\n        v = self.vcomp[j]\n        for (i, g) in enumerate(model.group_labels):\n            exg = ex[i]\n            ii = model.row_indices[g]\n            u = np.random.normal(size=exg.shape[1])\n            y[ii] += np.sqrt(v) * np.dot(exg, u)\n    y += np.sqrt(self.scale) * np.random.normal(size=len(y))\n    return y",
            "def rvs(self, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Return a vector of simulated values from a mixed linear\\n        model.\\n\\n        The parameter n is ignored, but required by the interface\\n        '\n    model = self.model\n    y = np.dot(self.exog, self.fe_params)\n    u = np.random.normal(size=(model.n_groups, model.k_re))\n    u = np.dot(u, np.linalg.cholesky(self.cov_re).T)\n    y += (u[self.group_idx, :] * model.exog_re).sum(1)\n    for (j, _) in enumerate(model.exog_vc.names):\n        ex = model.exog_vc.mats[j]\n        v = self.vcomp[j]\n        for (i, g) in enumerate(model.group_labels):\n            exg = ex[i]\n            ii = model.row_indices[g]\n            u = np.random.normal(size=exg.shape[1])\n            y[ii] += np.sqrt(v) * np.dot(exg, u)\n    y += np.sqrt(self.scale) * np.random.normal(size=len(y))\n    return y",
            "def rvs(self, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Return a vector of simulated values from a mixed linear\\n        model.\\n\\n        The parameter n is ignored, but required by the interface\\n        '\n    model = self.model\n    y = np.dot(self.exog, self.fe_params)\n    u = np.random.normal(size=(model.n_groups, model.k_re))\n    u = np.dot(u, np.linalg.cholesky(self.cov_re).T)\n    y += (u[self.group_idx, :] * model.exog_re).sum(1)\n    for (j, _) in enumerate(model.exog_vc.names):\n        ex = model.exog_vc.mats[j]\n        v = self.vcomp[j]\n        for (i, g) in enumerate(model.group_labels):\n            exg = ex[i]\n            ii = model.row_indices[g]\n            u = np.random.normal(size=exg.shape[1])\n            y[ii] += np.sqrt(v) * np.dot(exg, u)\n    y += np.sqrt(self.scale) * np.random.normal(size=len(y))\n    return y",
            "def rvs(self, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Return a vector of simulated values from a mixed linear\\n        model.\\n\\n        The parameter n is ignored, but required by the interface\\n        '\n    model = self.model\n    y = np.dot(self.exog, self.fe_params)\n    u = np.random.normal(size=(model.n_groups, model.k_re))\n    u = np.dot(u, np.linalg.cholesky(self.cov_re).T)\n    y += (u[self.group_idx, :] * model.exog_re).sum(1)\n    for (j, _) in enumerate(model.exog_vc.names):\n        ex = model.exog_vc.mats[j]\n        v = self.vcomp[j]\n        for (i, g) in enumerate(model.group_labels):\n            exg = ex[i]\n            ii = model.row_indices[g]\n            u = np.random.normal(size=exg.shape[1])\n            y[ii] += np.sqrt(v) * np.dot(exg, u)\n    y += np.sqrt(self.scale) * np.random.normal(size=len(y))\n    return y",
            "def rvs(self, n):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Return a vector of simulated values from a mixed linear\\n        model.\\n\\n        The parameter n is ignored, but required by the interface\\n        '\n    model = self.model\n    y = np.dot(self.exog, self.fe_params)\n    u = np.random.normal(size=(model.n_groups, model.k_re))\n    u = np.dot(u, np.linalg.cholesky(self.cov_re).T)\n    y += (u[self.group_idx, :] * model.exog_re).sum(1)\n    for (j, _) in enumerate(model.exog_vc.names):\n        ex = model.exog_vc.mats[j]\n        v = self.vcomp[j]\n        for (i, g) in enumerate(model.group_labels):\n            exg = ex[i]\n            ii = model.row_indices[g]\n            u = np.random.normal(size=exg.shape[1])\n            y[ii] += np.sqrt(v) * np.dot(exg, u)\n    y += np.sqrt(self.scale) * np.random.normal(size=len(y))\n    return y"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, model, params, cov_params):\n    super(MixedLMResults, self).__init__(model, params, normalized_cov_params=cov_params)\n    self.nobs = self.model.nobs\n    self.df_resid = self.nobs - np.linalg.matrix_rank(self.model.exog)",
        "mutated": [
            "def __init__(self, model, params, cov_params):\n    if False:\n        i = 10\n    super(MixedLMResults, self).__init__(model, params, normalized_cov_params=cov_params)\n    self.nobs = self.model.nobs\n    self.df_resid = self.nobs - np.linalg.matrix_rank(self.model.exog)",
            "def __init__(self, model, params, cov_params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(MixedLMResults, self).__init__(model, params, normalized_cov_params=cov_params)\n    self.nobs = self.model.nobs\n    self.df_resid = self.nobs - np.linalg.matrix_rank(self.model.exog)",
            "def __init__(self, model, params, cov_params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(MixedLMResults, self).__init__(model, params, normalized_cov_params=cov_params)\n    self.nobs = self.model.nobs\n    self.df_resid = self.nobs - np.linalg.matrix_rank(self.model.exog)",
            "def __init__(self, model, params, cov_params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(MixedLMResults, self).__init__(model, params, normalized_cov_params=cov_params)\n    self.nobs = self.model.nobs\n    self.df_resid = self.nobs - np.linalg.matrix_rank(self.model.exog)",
            "def __init__(self, model, params, cov_params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(MixedLMResults, self).__init__(model, params, normalized_cov_params=cov_params)\n    self.nobs = self.model.nobs\n    self.df_resid = self.nobs - np.linalg.matrix_rank(self.model.exog)"
        ]
    },
    {
        "func_name": "fittedvalues",
        "original": "@cache_readonly\ndef fittedvalues(self):\n    \"\"\"\n        Returns the fitted values for the model.\n\n        The fitted values reflect the mean structure specified by the\n        fixed effects and the predicted random effects.\n        \"\"\"\n    fit = np.dot(self.model.exog, self.fe_params)\n    re = self.random_effects\n    for (group_ix, group) in enumerate(self.model.group_labels):\n        ix = self.model.row_indices[group]\n        mat = []\n        if self.model.exog_re_li is not None:\n            mat.append(self.model.exog_re_li[group_ix])\n        for j in range(self.k_vc):\n            mat.append(self.model.exog_vc.mats[j][group_ix])\n        mat = np.concatenate(mat, axis=1)\n        fit[ix] += np.dot(mat, re[group])\n    return fit",
        "mutated": [
            "@cache_readonly\ndef fittedvalues(self):\n    if False:\n        i = 10\n    '\\n        Returns the fitted values for the model.\\n\\n        The fitted values reflect the mean structure specified by the\\n        fixed effects and the predicted random effects.\\n        '\n    fit = np.dot(self.model.exog, self.fe_params)\n    re = self.random_effects\n    for (group_ix, group) in enumerate(self.model.group_labels):\n        ix = self.model.row_indices[group]\n        mat = []\n        if self.model.exog_re_li is not None:\n            mat.append(self.model.exog_re_li[group_ix])\n        for j in range(self.k_vc):\n            mat.append(self.model.exog_vc.mats[j][group_ix])\n        mat = np.concatenate(mat, axis=1)\n        fit[ix] += np.dot(mat, re[group])\n    return fit",
            "@cache_readonly\ndef fittedvalues(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the fitted values for the model.\\n\\n        The fitted values reflect the mean structure specified by the\\n        fixed effects and the predicted random effects.\\n        '\n    fit = np.dot(self.model.exog, self.fe_params)\n    re = self.random_effects\n    for (group_ix, group) in enumerate(self.model.group_labels):\n        ix = self.model.row_indices[group]\n        mat = []\n        if self.model.exog_re_li is not None:\n            mat.append(self.model.exog_re_li[group_ix])\n        for j in range(self.k_vc):\n            mat.append(self.model.exog_vc.mats[j][group_ix])\n        mat = np.concatenate(mat, axis=1)\n        fit[ix] += np.dot(mat, re[group])\n    return fit",
            "@cache_readonly\ndef fittedvalues(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the fitted values for the model.\\n\\n        The fitted values reflect the mean structure specified by the\\n        fixed effects and the predicted random effects.\\n        '\n    fit = np.dot(self.model.exog, self.fe_params)\n    re = self.random_effects\n    for (group_ix, group) in enumerate(self.model.group_labels):\n        ix = self.model.row_indices[group]\n        mat = []\n        if self.model.exog_re_li is not None:\n            mat.append(self.model.exog_re_li[group_ix])\n        for j in range(self.k_vc):\n            mat.append(self.model.exog_vc.mats[j][group_ix])\n        mat = np.concatenate(mat, axis=1)\n        fit[ix] += np.dot(mat, re[group])\n    return fit",
            "@cache_readonly\ndef fittedvalues(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the fitted values for the model.\\n\\n        The fitted values reflect the mean structure specified by the\\n        fixed effects and the predicted random effects.\\n        '\n    fit = np.dot(self.model.exog, self.fe_params)\n    re = self.random_effects\n    for (group_ix, group) in enumerate(self.model.group_labels):\n        ix = self.model.row_indices[group]\n        mat = []\n        if self.model.exog_re_li is not None:\n            mat.append(self.model.exog_re_li[group_ix])\n        for j in range(self.k_vc):\n            mat.append(self.model.exog_vc.mats[j][group_ix])\n        mat = np.concatenate(mat, axis=1)\n        fit[ix] += np.dot(mat, re[group])\n    return fit",
            "@cache_readonly\ndef fittedvalues(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the fitted values for the model.\\n\\n        The fitted values reflect the mean structure specified by the\\n        fixed effects and the predicted random effects.\\n        '\n    fit = np.dot(self.model.exog, self.fe_params)\n    re = self.random_effects\n    for (group_ix, group) in enumerate(self.model.group_labels):\n        ix = self.model.row_indices[group]\n        mat = []\n        if self.model.exog_re_li is not None:\n            mat.append(self.model.exog_re_li[group_ix])\n        for j in range(self.k_vc):\n            mat.append(self.model.exog_vc.mats[j][group_ix])\n        mat = np.concatenate(mat, axis=1)\n        fit[ix] += np.dot(mat, re[group])\n    return fit"
        ]
    },
    {
        "func_name": "resid",
        "original": "@cache_readonly\ndef resid(self):\n    \"\"\"\n        Returns the residuals for the model.\n\n        The residuals reflect the mean structure specified by the\n        fixed effects and the predicted random effects.\n        \"\"\"\n    return self.model.endog - self.fittedvalues",
        "mutated": [
            "@cache_readonly\ndef resid(self):\n    if False:\n        i = 10\n    '\\n        Returns the residuals for the model.\\n\\n        The residuals reflect the mean structure specified by the\\n        fixed effects and the predicted random effects.\\n        '\n    return self.model.endog - self.fittedvalues",
            "@cache_readonly\ndef resid(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the residuals for the model.\\n\\n        The residuals reflect the mean structure specified by the\\n        fixed effects and the predicted random effects.\\n        '\n    return self.model.endog - self.fittedvalues",
            "@cache_readonly\ndef resid(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the residuals for the model.\\n\\n        The residuals reflect the mean structure specified by the\\n        fixed effects and the predicted random effects.\\n        '\n    return self.model.endog - self.fittedvalues",
            "@cache_readonly\ndef resid(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the residuals for the model.\\n\\n        The residuals reflect the mean structure specified by the\\n        fixed effects and the predicted random effects.\\n        '\n    return self.model.endog - self.fittedvalues",
            "@cache_readonly\ndef resid(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the residuals for the model.\\n\\n        The residuals reflect the mean structure specified by the\\n        fixed effects and the predicted random effects.\\n        '\n    return self.model.endog - self.fittedvalues"
        ]
    },
    {
        "func_name": "bse_fe",
        "original": "@cache_readonly\ndef bse_fe(self):\n    \"\"\"\n        Returns the standard errors of the fixed effect regression\n        coefficients.\n        \"\"\"\n    p = self.model.exog.shape[1]\n    return np.sqrt(np.diag(self.cov_params())[0:p])",
        "mutated": [
            "@cache_readonly\ndef bse_fe(self):\n    if False:\n        i = 10\n    '\\n        Returns the standard errors of the fixed effect regression\\n        coefficients.\\n        '\n    p = self.model.exog.shape[1]\n    return np.sqrt(np.diag(self.cov_params())[0:p])",
            "@cache_readonly\ndef bse_fe(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the standard errors of the fixed effect regression\\n        coefficients.\\n        '\n    p = self.model.exog.shape[1]\n    return np.sqrt(np.diag(self.cov_params())[0:p])",
            "@cache_readonly\ndef bse_fe(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the standard errors of the fixed effect regression\\n        coefficients.\\n        '\n    p = self.model.exog.shape[1]\n    return np.sqrt(np.diag(self.cov_params())[0:p])",
            "@cache_readonly\ndef bse_fe(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the standard errors of the fixed effect regression\\n        coefficients.\\n        '\n    p = self.model.exog.shape[1]\n    return np.sqrt(np.diag(self.cov_params())[0:p])",
            "@cache_readonly\ndef bse_fe(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the standard errors of the fixed effect regression\\n        coefficients.\\n        '\n    p = self.model.exog.shape[1]\n    return np.sqrt(np.diag(self.cov_params())[0:p])"
        ]
    },
    {
        "func_name": "bse_re",
        "original": "@cache_readonly\ndef bse_re(self):\n    \"\"\"\n        Returns the standard errors of the variance parameters.\n\n        The first `k_re x (k_re + 1)` elements of the returned array\n        are the standard errors of the lower triangle of `cov_re`.\n        The remaining elements are the standard errors of the variance\n        components.\n\n        Note that the sampling distribution of variance parameters is\n        strongly skewed unless the sample size is large, so these\n        standard errors may not give meaningful confidence intervals\n        or p-values if used in the usual way.\n        \"\"\"\n    p = self.model.exog.shape[1]\n    return np.sqrt(self.scale * np.diag(self.cov_params())[p:])",
        "mutated": [
            "@cache_readonly\ndef bse_re(self):\n    if False:\n        i = 10\n    '\\n        Returns the standard errors of the variance parameters.\\n\\n        The first `k_re x (k_re + 1)` elements of the returned array\\n        are the standard errors of the lower triangle of `cov_re`.\\n        The remaining elements are the standard errors of the variance\\n        components.\\n\\n        Note that the sampling distribution of variance parameters is\\n        strongly skewed unless the sample size is large, so these\\n        standard errors may not give meaningful confidence intervals\\n        or p-values if used in the usual way.\\n        '\n    p = self.model.exog.shape[1]\n    return np.sqrt(self.scale * np.diag(self.cov_params())[p:])",
            "@cache_readonly\ndef bse_re(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the standard errors of the variance parameters.\\n\\n        The first `k_re x (k_re + 1)` elements of the returned array\\n        are the standard errors of the lower triangle of `cov_re`.\\n        The remaining elements are the standard errors of the variance\\n        components.\\n\\n        Note that the sampling distribution of variance parameters is\\n        strongly skewed unless the sample size is large, so these\\n        standard errors may not give meaningful confidence intervals\\n        or p-values if used in the usual way.\\n        '\n    p = self.model.exog.shape[1]\n    return np.sqrt(self.scale * np.diag(self.cov_params())[p:])",
            "@cache_readonly\ndef bse_re(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the standard errors of the variance parameters.\\n\\n        The first `k_re x (k_re + 1)` elements of the returned array\\n        are the standard errors of the lower triangle of `cov_re`.\\n        The remaining elements are the standard errors of the variance\\n        components.\\n\\n        Note that the sampling distribution of variance parameters is\\n        strongly skewed unless the sample size is large, so these\\n        standard errors may not give meaningful confidence intervals\\n        or p-values if used in the usual way.\\n        '\n    p = self.model.exog.shape[1]\n    return np.sqrt(self.scale * np.diag(self.cov_params())[p:])",
            "@cache_readonly\ndef bse_re(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the standard errors of the variance parameters.\\n\\n        The first `k_re x (k_re + 1)` elements of the returned array\\n        are the standard errors of the lower triangle of `cov_re`.\\n        The remaining elements are the standard errors of the variance\\n        components.\\n\\n        Note that the sampling distribution of variance parameters is\\n        strongly skewed unless the sample size is large, so these\\n        standard errors may not give meaningful confidence intervals\\n        or p-values if used in the usual way.\\n        '\n    p = self.model.exog.shape[1]\n    return np.sqrt(self.scale * np.diag(self.cov_params())[p:])",
            "@cache_readonly\ndef bse_re(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the standard errors of the variance parameters.\\n\\n        The first `k_re x (k_re + 1)` elements of the returned array\\n        are the standard errors of the lower triangle of `cov_re`.\\n        The remaining elements are the standard errors of the variance\\n        components.\\n\\n        Note that the sampling distribution of variance parameters is\\n        strongly skewed unless the sample size is large, so these\\n        standard errors may not give meaningful confidence intervals\\n        or p-values if used in the usual way.\\n        '\n    p = self.model.exog.shape[1]\n    return np.sqrt(self.scale * np.diag(self.cov_params())[p:])"
        ]
    },
    {
        "func_name": "_expand_re_names",
        "original": "def _expand_re_names(self, group_ix):\n    names = list(self.model.data.exog_re_names)\n    for (j, v) in enumerate(self.model.exog_vc.names):\n        vg = self.model.exog_vc.colnames[j][group_ix]\n        na = ['%s[%s]' % (v, s) for s in vg]\n        names.extend(na)\n    return names",
        "mutated": [
            "def _expand_re_names(self, group_ix):\n    if False:\n        i = 10\n    names = list(self.model.data.exog_re_names)\n    for (j, v) in enumerate(self.model.exog_vc.names):\n        vg = self.model.exog_vc.colnames[j][group_ix]\n        na = ['%s[%s]' % (v, s) for s in vg]\n        names.extend(na)\n    return names",
            "def _expand_re_names(self, group_ix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    names = list(self.model.data.exog_re_names)\n    for (j, v) in enumerate(self.model.exog_vc.names):\n        vg = self.model.exog_vc.colnames[j][group_ix]\n        na = ['%s[%s]' % (v, s) for s in vg]\n        names.extend(na)\n    return names",
            "def _expand_re_names(self, group_ix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    names = list(self.model.data.exog_re_names)\n    for (j, v) in enumerate(self.model.exog_vc.names):\n        vg = self.model.exog_vc.colnames[j][group_ix]\n        na = ['%s[%s]' % (v, s) for s in vg]\n        names.extend(na)\n    return names",
            "def _expand_re_names(self, group_ix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    names = list(self.model.data.exog_re_names)\n    for (j, v) in enumerate(self.model.exog_vc.names):\n        vg = self.model.exog_vc.colnames[j][group_ix]\n        na = ['%s[%s]' % (v, s) for s in vg]\n        names.extend(na)\n    return names",
            "def _expand_re_names(self, group_ix):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    names = list(self.model.data.exog_re_names)\n    for (j, v) in enumerate(self.model.exog_vc.names):\n        vg = self.model.exog_vc.colnames[j][group_ix]\n        na = ['%s[%s]' % (v, s) for s in vg]\n        names.extend(na)\n    return names"
        ]
    },
    {
        "func_name": "random_effects",
        "original": "@cache_readonly\ndef random_effects(self):\n    \"\"\"\n        The conditional means of random effects given the data.\n\n        Returns\n        -------\n        random_effects : dict\n            A dictionary mapping the distinct `group` values to the\n            conditional means of the random effects for the group\n            given the data.\n        \"\"\"\n    try:\n        cov_re_inv = np.linalg.inv(self.cov_re)\n    except np.linalg.LinAlgError:\n        raise ValueError('Cannot predict random effects from ' + 'singular covariance structure.')\n    vcomp = self.vcomp\n    k_re = self.k_re\n    ranef_dict = {}\n    for (group_ix, group) in enumerate(self.model.group_labels):\n        endog = self.model.endog_li[group_ix]\n        exog = self.model.exog_li[group_ix]\n        ex_r = self.model._aex_r[group_ix]\n        ex2_r = self.model._aex_r2[group_ix]\n        vc_var = self.model._expand_vcomp(vcomp, group_ix)\n        resid = endog\n        if self.k_fe > 0:\n            expval = np.dot(exog, self.fe_params)\n            resid = resid - expval\n        solver = _smw_solver(self.scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        vir = solver(resid)\n        xtvir = _dot(ex_r.T, vir)\n        xtvir[0:k_re] = np.dot(self.cov_re, xtvir[0:k_re])\n        xtvir[k_re:] *= vc_var\n        ranef_dict[group] = pd.Series(xtvir, index=self._expand_re_names(group_ix))\n    return ranef_dict",
        "mutated": [
            "@cache_readonly\ndef random_effects(self):\n    if False:\n        i = 10\n    '\\n        The conditional means of random effects given the data.\\n\\n        Returns\\n        -------\\n        random_effects : dict\\n            A dictionary mapping the distinct `group` values to the\\n            conditional means of the random effects for the group\\n            given the data.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(self.cov_re)\n    except np.linalg.LinAlgError:\n        raise ValueError('Cannot predict random effects from ' + 'singular covariance structure.')\n    vcomp = self.vcomp\n    k_re = self.k_re\n    ranef_dict = {}\n    for (group_ix, group) in enumerate(self.model.group_labels):\n        endog = self.model.endog_li[group_ix]\n        exog = self.model.exog_li[group_ix]\n        ex_r = self.model._aex_r[group_ix]\n        ex2_r = self.model._aex_r2[group_ix]\n        vc_var = self.model._expand_vcomp(vcomp, group_ix)\n        resid = endog\n        if self.k_fe > 0:\n            expval = np.dot(exog, self.fe_params)\n            resid = resid - expval\n        solver = _smw_solver(self.scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        vir = solver(resid)\n        xtvir = _dot(ex_r.T, vir)\n        xtvir[0:k_re] = np.dot(self.cov_re, xtvir[0:k_re])\n        xtvir[k_re:] *= vc_var\n        ranef_dict[group] = pd.Series(xtvir, index=self._expand_re_names(group_ix))\n    return ranef_dict",
            "@cache_readonly\ndef random_effects(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        The conditional means of random effects given the data.\\n\\n        Returns\\n        -------\\n        random_effects : dict\\n            A dictionary mapping the distinct `group` values to the\\n            conditional means of the random effects for the group\\n            given the data.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(self.cov_re)\n    except np.linalg.LinAlgError:\n        raise ValueError('Cannot predict random effects from ' + 'singular covariance structure.')\n    vcomp = self.vcomp\n    k_re = self.k_re\n    ranef_dict = {}\n    for (group_ix, group) in enumerate(self.model.group_labels):\n        endog = self.model.endog_li[group_ix]\n        exog = self.model.exog_li[group_ix]\n        ex_r = self.model._aex_r[group_ix]\n        ex2_r = self.model._aex_r2[group_ix]\n        vc_var = self.model._expand_vcomp(vcomp, group_ix)\n        resid = endog\n        if self.k_fe > 0:\n            expval = np.dot(exog, self.fe_params)\n            resid = resid - expval\n        solver = _smw_solver(self.scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        vir = solver(resid)\n        xtvir = _dot(ex_r.T, vir)\n        xtvir[0:k_re] = np.dot(self.cov_re, xtvir[0:k_re])\n        xtvir[k_re:] *= vc_var\n        ranef_dict[group] = pd.Series(xtvir, index=self._expand_re_names(group_ix))\n    return ranef_dict",
            "@cache_readonly\ndef random_effects(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        The conditional means of random effects given the data.\\n\\n        Returns\\n        -------\\n        random_effects : dict\\n            A dictionary mapping the distinct `group` values to the\\n            conditional means of the random effects for the group\\n            given the data.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(self.cov_re)\n    except np.linalg.LinAlgError:\n        raise ValueError('Cannot predict random effects from ' + 'singular covariance structure.')\n    vcomp = self.vcomp\n    k_re = self.k_re\n    ranef_dict = {}\n    for (group_ix, group) in enumerate(self.model.group_labels):\n        endog = self.model.endog_li[group_ix]\n        exog = self.model.exog_li[group_ix]\n        ex_r = self.model._aex_r[group_ix]\n        ex2_r = self.model._aex_r2[group_ix]\n        vc_var = self.model._expand_vcomp(vcomp, group_ix)\n        resid = endog\n        if self.k_fe > 0:\n            expval = np.dot(exog, self.fe_params)\n            resid = resid - expval\n        solver = _smw_solver(self.scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        vir = solver(resid)\n        xtvir = _dot(ex_r.T, vir)\n        xtvir[0:k_re] = np.dot(self.cov_re, xtvir[0:k_re])\n        xtvir[k_re:] *= vc_var\n        ranef_dict[group] = pd.Series(xtvir, index=self._expand_re_names(group_ix))\n    return ranef_dict",
            "@cache_readonly\ndef random_effects(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        The conditional means of random effects given the data.\\n\\n        Returns\\n        -------\\n        random_effects : dict\\n            A dictionary mapping the distinct `group` values to the\\n            conditional means of the random effects for the group\\n            given the data.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(self.cov_re)\n    except np.linalg.LinAlgError:\n        raise ValueError('Cannot predict random effects from ' + 'singular covariance structure.')\n    vcomp = self.vcomp\n    k_re = self.k_re\n    ranef_dict = {}\n    for (group_ix, group) in enumerate(self.model.group_labels):\n        endog = self.model.endog_li[group_ix]\n        exog = self.model.exog_li[group_ix]\n        ex_r = self.model._aex_r[group_ix]\n        ex2_r = self.model._aex_r2[group_ix]\n        vc_var = self.model._expand_vcomp(vcomp, group_ix)\n        resid = endog\n        if self.k_fe > 0:\n            expval = np.dot(exog, self.fe_params)\n            resid = resid - expval\n        solver = _smw_solver(self.scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        vir = solver(resid)\n        xtvir = _dot(ex_r.T, vir)\n        xtvir[0:k_re] = np.dot(self.cov_re, xtvir[0:k_re])\n        xtvir[k_re:] *= vc_var\n        ranef_dict[group] = pd.Series(xtvir, index=self._expand_re_names(group_ix))\n    return ranef_dict",
            "@cache_readonly\ndef random_effects(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        The conditional means of random effects given the data.\\n\\n        Returns\\n        -------\\n        random_effects : dict\\n            A dictionary mapping the distinct `group` values to the\\n            conditional means of the random effects for the group\\n            given the data.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(self.cov_re)\n    except np.linalg.LinAlgError:\n        raise ValueError('Cannot predict random effects from ' + 'singular covariance structure.')\n    vcomp = self.vcomp\n    k_re = self.k_re\n    ranef_dict = {}\n    for (group_ix, group) in enumerate(self.model.group_labels):\n        endog = self.model.endog_li[group_ix]\n        exog = self.model.exog_li[group_ix]\n        ex_r = self.model._aex_r[group_ix]\n        ex2_r = self.model._aex_r2[group_ix]\n        vc_var = self.model._expand_vcomp(vcomp, group_ix)\n        resid = endog\n        if self.k_fe > 0:\n            expval = np.dot(exog, self.fe_params)\n            resid = resid - expval\n        solver = _smw_solver(self.scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        vir = solver(resid)\n        xtvir = _dot(ex_r.T, vir)\n        xtvir[0:k_re] = np.dot(self.cov_re, xtvir[0:k_re])\n        xtvir[k_re:] *= vc_var\n        ranef_dict[group] = pd.Series(xtvir, index=self._expand_re_names(group_ix))\n    return ranef_dict"
        ]
    },
    {
        "func_name": "random_effects_cov",
        "original": "@cache_readonly\ndef random_effects_cov(self):\n    \"\"\"\n        Returns the conditional covariance matrix of the random\n        effects for each group given the data.\n\n        Returns\n        -------\n        random_effects_cov : dict\n            A dictionary mapping the distinct values of the `group`\n            variable to the conditional covariance matrix of the\n            random effects given the data.\n        \"\"\"\n    try:\n        cov_re_inv = np.linalg.inv(self.cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = None\n    vcomp = self.vcomp\n    ranef_dict = {}\n    for group_ix in range(self.model.n_groups):\n        ex_r = self.model._aex_r[group_ix]\n        ex2_r = self.model._aex_r2[group_ix]\n        label = self.model.group_labels[group_ix]\n        vc_var = self.model._expand_vcomp(vcomp, group_ix)\n        solver = _smw_solver(self.scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        n = ex_r.shape[0]\n        m = self.cov_re.shape[0]\n        mat1 = np.empty((n, m + len(vc_var)))\n        mat1[:, 0:m] = np.dot(ex_r[:, 0:m], self.cov_re)\n        mat1[:, m:] = np.dot(ex_r[:, m:], np.diag(vc_var))\n        mat2 = solver(mat1)\n        mat2 = np.dot(mat1.T, mat2)\n        v = -mat2\n        v[0:m, 0:m] += self.cov_re\n        ix = np.arange(m, v.shape[0])\n        v[ix, ix] += vc_var\n        na = self._expand_re_names(group_ix)\n        v = pd.DataFrame(v, index=na, columns=na)\n        ranef_dict[label] = v\n    return ranef_dict",
        "mutated": [
            "@cache_readonly\ndef random_effects_cov(self):\n    if False:\n        i = 10\n    '\\n        Returns the conditional covariance matrix of the random\\n        effects for each group given the data.\\n\\n        Returns\\n        -------\\n        random_effects_cov : dict\\n            A dictionary mapping the distinct values of the `group`\\n            variable to the conditional covariance matrix of the\\n            random effects given the data.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(self.cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = None\n    vcomp = self.vcomp\n    ranef_dict = {}\n    for group_ix in range(self.model.n_groups):\n        ex_r = self.model._aex_r[group_ix]\n        ex2_r = self.model._aex_r2[group_ix]\n        label = self.model.group_labels[group_ix]\n        vc_var = self.model._expand_vcomp(vcomp, group_ix)\n        solver = _smw_solver(self.scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        n = ex_r.shape[0]\n        m = self.cov_re.shape[0]\n        mat1 = np.empty((n, m + len(vc_var)))\n        mat1[:, 0:m] = np.dot(ex_r[:, 0:m], self.cov_re)\n        mat1[:, m:] = np.dot(ex_r[:, m:], np.diag(vc_var))\n        mat2 = solver(mat1)\n        mat2 = np.dot(mat1.T, mat2)\n        v = -mat2\n        v[0:m, 0:m] += self.cov_re\n        ix = np.arange(m, v.shape[0])\n        v[ix, ix] += vc_var\n        na = self._expand_re_names(group_ix)\n        v = pd.DataFrame(v, index=na, columns=na)\n        ranef_dict[label] = v\n    return ranef_dict",
            "@cache_readonly\ndef random_effects_cov(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the conditional covariance matrix of the random\\n        effects for each group given the data.\\n\\n        Returns\\n        -------\\n        random_effects_cov : dict\\n            A dictionary mapping the distinct values of the `group`\\n            variable to the conditional covariance matrix of the\\n            random effects given the data.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(self.cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = None\n    vcomp = self.vcomp\n    ranef_dict = {}\n    for group_ix in range(self.model.n_groups):\n        ex_r = self.model._aex_r[group_ix]\n        ex2_r = self.model._aex_r2[group_ix]\n        label = self.model.group_labels[group_ix]\n        vc_var = self.model._expand_vcomp(vcomp, group_ix)\n        solver = _smw_solver(self.scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        n = ex_r.shape[0]\n        m = self.cov_re.shape[0]\n        mat1 = np.empty((n, m + len(vc_var)))\n        mat1[:, 0:m] = np.dot(ex_r[:, 0:m], self.cov_re)\n        mat1[:, m:] = np.dot(ex_r[:, m:], np.diag(vc_var))\n        mat2 = solver(mat1)\n        mat2 = np.dot(mat1.T, mat2)\n        v = -mat2\n        v[0:m, 0:m] += self.cov_re\n        ix = np.arange(m, v.shape[0])\n        v[ix, ix] += vc_var\n        na = self._expand_re_names(group_ix)\n        v = pd.DataFrame(v, index=na, columns=na)\n        ranef_dict[label] = v\n    return ranef_dict",
            "@cache_readonly\ndef random_effects_cov(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the conditional covariance matrix of the random\\n        effects for each group given the data.\\n\\n        Returns\\n        -------\\n        random_effects_cov : dict\\n            A dictionary mapping the distinct values of the `group`\\n            variable to the conditional covariance matrix of the\\n            random effects given the data.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(self.cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = None\n    vcomp = self.vcomp\n    ranef_dict = {}\n    for group_ix in range(self.model.n_groups):\n        ex_r = self.model._aex_r[group_ix]\n        ex2_r = self.model._aex_r2[group_ix]\n        label = self.model.group_labels[group_ix]\n        vc_var = self.model._expand_vcomp(vcomp, group_ix)\n        solver = _smw_solver(self.scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        n = ex_r.shape[0]\n        m = self.cov_re.shape[0]\n        mat1 = np.empty((n, m + len(vc_var)))\n        mat1[:, 0:m] = np.dot(ex_r[:, 0:m], self.cov_re)\n        mat1[:, m:] = np.dot(ex_r[:, m:], np.diag(vc_var))\n        mat2 = solver(mat1)\n        mat2 = np.dot(mat1.T, mat2)\n        v = -mat2\n        v[0:m, 0:m] += self.cov_re\n        ix = np.arange(m, v.shape[0])\n        v[ix, ix] += vc_var\n        na = self._expand_re_names(group_ix)\n        v = pd.DataFrame(v, index=na, columns=na)\n        ranef_dict[label] = v\n    return ranef_dict",
            "@cache_readonly\ndef random_effects_cov(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the conditional covariance matrix of the random\\n        effects for each group given the data.\\n\\n        Returns\\n        -------\\n        random_effects_cov : dict\\n            A dictionary mapping the distinct values of the `group`\\n            variable to the conditional covariance matrix of the\\n            random effects given the data.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(self.cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = None\n    vcomp = self.vcomp\n    ranef_dict = {}\n    for group_ix in range(self.model.n_groups):\n        ex_r = self.model._aex_r[group_ix]\n        ex2_r = self.model._aex_r2[group_ix]\n        label = self.model.group_labels[group_ix]\n        vc_var = self.model._expand_vcomp(vcomp, group_ix)\n        solver = _smw_solver(self.scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        n = ex_r.shape[0]\n        m = self.cov_re.shape[0]\n        mat1 = np.empty((n, m + len(vc_var)))\n        mat1[:, 0:m] = np.dot(ex_r[:, 0:m], self.cov_re)\n        mat1[:, m:] = np.dot(ex_r[:, m:], np.diag(vc_var))\n        mat2 = solver(mat1)\n        mat2 = np.dot(mat1.T, mat2)\n        v = -mat2\n        v[0:m, 0:m] += self.cov_re\n        ix = np.arange(m, v.shape[0])\n        v[ix, ix] += vc_var\n        na = self._expand_re_names(group_ix)\n        v = pd.DataFrame(v, index=na, columns=na)\n        ranef_dict[label] = v\n    return ranef_dict",
            "@cache_readonly\ndef random_effects_cov(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the conditional covariance matrix of the random\\n        effects for each group given the data.\\n\\n        Returns\\n        -------\\n        random_effects_cov : dict\\n            A dictionary mapping the distinct values of the `group`\\n            variable to the conditional covariance matrix of the\\n            random effects given the data.\\n        '\n    try:\n        cov_re_inv = np.linalg.inv(self.cov_re)\n    except np.linalg.LinAlgError:\n        cov_re_inv = None\n    vcomp = self.vcomp\n    ranef_dict = {}\n    for group_ix in range(self.model.n_groups):\n        ex_r = self.model._aex_r[group_ix]\n        ex2_r = self.model._aex_r2[group_ix]\n        label = self.model.group_labels[group_ix]\n        vc_var = self.model._expand_vcomp(vcomp, group_ix)\n        solver = _smw_solver(self.scale, ex_r, ex2_r, cov_re_inv, 1 / vc_var)\n        n = ex_r.shape[0]\n        m = self.cov_re.shape[0]\n        mat1 = np.empty((n, m + len(vc_var)))\n        mat1[:, 0:m] = np.dot(ex_r[:, 0:m], self.cov_re)\n        mat1[:, m:] = np.dot(ex_r[:, m:], np.diag(vc_var))\n        mat2 = solver(mat1)\n        mat2 = np.dot(mat1.T, mat2)\n        v = -mat2\n        v[0:m, 0:m] += self.cov_re\n        ix = np.arange(m, v.shape[0])\n        v[ix, ix] += vc_var\n        na = self._expand_re_names(group_ix)\n        v = pd.DataFrame(v, index=na, columns=na)\n        ranef_dict[label] = v\n    return ranef_dict"
        ]
    },
    {
        "func_name": "t_test",
        "original": "def t_test(self, r_matrix, use_t=None):\n    \"\"\"\n        Compute a t-test for a each linear hypothesis of the form Rb = q\n\n        Parameters\n        ----------\n        r_matrix : array_like\n            If an array is given, a p x k 2d array or length k 1d\n            array specifying the linear restrictions. It is assumed\n            that the linear combination is equal to zero.\n        scale : float, optional\n            An optional `scale` to use.  Default is the scale specified\n            by the model fit.\n        use_t : bool, optional\n            If use_t is None, then the default of the model is used.\n            If use_t is True, then the p-values are based on the t\n            distribution.\n            If use_t is False, then the p-values are based on the normal\n            distribution.\n\n        Returns\n        -------\n        res : ContrastResults instance\n            The results for the test are attributes of this results instance.\n            The available results have the same elements as the parameter table\n            in `summary()`.\n        \"\"\"\n    if r_matrix.shape[1] != self.k_fe:\n        raise ValueError('r_matrix for t-test should have %d columns' % self.k_fe)\n    d = self.k_re2 + self.k_vc\n    z0 = np.zeros((r_matrix.shape[0], d))\n    r_matrix = np.concatenate((r_matrix, z0), axis=1)\n    tst_rslt = super(MixedLMResults, self).t_test(r_matrix, use_t=use_t)\n    return tst_rslt",
        "mutated": [
            "def t_test(self, r_matrix, use_t=None):\n    if False:\n        i = 10\n    '\\n        Compute a t-test for a each linear hypothesis of the form Rb = q\\n\\n        Parameters\\n        ----------\\n        r_matrix : array_like\\n            If an array is given, a p x k 2d array or length k 1d\\n            array specifying the linear restrictions. It is assumed\\n            that the linear combination is equal to zero.\\n        scale : float, optional\\n            An optional `scale` to use.  Default is the scale specified\\n            by the model fit.\\n        use_t : bool, optional\\n            If use_t is None, then the default of the model is used.\\n            If use_t is True, then the p-values are based on the t\\n            distribution.\\n            If use_t is False, then the p-values are based on the normal\\n            distribution.\\n\\n        Returns\\n        -------\\n        res : ContrastResults instance\\n            The results for the test are attributes of this results instance.\\n            The available results have the same elements as the parameter table\\n            in `summary()`.\\n        '\n    if r_matrix.shape[1] != self.k_fe:\n        raise ValueError('r_matrix for t-test should have %d columns' % self.k_fe)\n    d = self.k_re2 + self.k_vc\n    z0 = np.zeros((r_matrix.shape[0], d))\n    r_matrix = np.concatenate((r_matrix, z0), axis=1)\n    tst_rslt = super(MixedLMResults, self).t_test(r_matrix, use_t=use_t)\n    return tst_rslt",
            "def t_test(self, r_matrix, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Compute a t-test for a each linear hypothesis of the form Rb = q\\n\\n        Parameters\\n        ----------\\n        r_matrix : array_like\\n            If an array is given, a p x k 2d array or length k 1d\\n            array specifying the linear restrictions. It is assumed\\n            that the linear combination is equal to zero.\\n        scale : float, optional\\n            An optional `scale` to use.  Default is the scale specified\\n            by the model fit.\\n        use_t : bool, optional\\n            If use_t is None, then the default of the model is used.\\n            If use_t is True, then the p-values are based on the t\\n            distribution.\\n            If use_t is False, then the p-values are based on the normal\\n            distribution.\\n\\n        Returns\\n        -------\\n        res : ContrastResults instance\\n            The results for the test are attributes of this results instance.\\n            The available results have the same elements as the parameter table\\n            in `summary()`.\\n        '\n    if r_matrix.shape[1] != self.k_fe:\n        raise ValueError('r_matrix for t-test should have %d columns' % self.k_fe)\n    d = self.k_re2 + self.k_vc\n    z0 = np.zeros((r_matrix.shape[0], d))\n    r_matrix = np.concatenate((r_matrix, z0), axis=1)\n    tst_rslt = super(MixedLMResults, self).t_test(r_matrix, use_t=use_t)\n    return tst_rslt",
            "def t_test(self, r_matrix, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Compute a t-test for a each linear hypothesis of the form Rb = q\\n\\n        Parameters\\n        ----------\\n        r_matrix : array_like\\n            If an array is given, a p x k 2d array or length k 1d\\n            array specifying the linear restrictions. It is assumed\\n            that the linear combination is equal to zero.\\n        scale : float, optional\\n            An optional `scale` to use.  Default is the scale specified\\n            by the model fit.\\n        use_t : bool, optional\\n            If use_t is None, then the default of the model is used.\\n            If use_t is True, then the p-values are based on the t\\n            distribution.\\n            If use_t is False, then the p-values are based on the normal\\n            distribution.\\n\\n        Returns\\n        -------\\n        res : ContrastResults instance\\n            The results for the test are attributes of this results instance.\\n            The available results have the same elements as the parameter table\\n            in `summary()`.\\n        '\n    if r_matrix.shape[1] != self.k_fe:\n        raise ValueError('r_matrix for t-test should have %d columns' % self.k_fe)\n    d = self.k_re2 + self.k_vc\n    z0 = np.zeros((r_matrix.shape[0], d))\n    r_matrix = np.concatenate((r_matrix, z0), axis=1)\n    tst_rslt = super(MixedLMResults, self).t_test(r_matrix, use_t=use_t)\n    return tst_rslt",
            "def t_test(self, r_matrix, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Compute a t-test for a each linear hypothesis of the form Rb = q\\n\\n        Parameters\\n        ----------\\n        r_matrix : array_like\\n            If an array is given, a p x k 2d array or length k 1d\\n            array specifying the linear restrictions. It is assumed\\n            that the linear combination is equal to zero.\\n        scale : float, optional\\n            An optional `scale` to use.  Default is the scale specified\\n            by the model fit.\\n        use_t : bool, optional\\n            If use_t is None, then the default of the model is used.\\n            If use_t is True, then the p-values are based on the t\\n            distribution.\\n            If use_t is False, then the p-values are based on the normal\\n            distribution.\\n\\n        Returns\\n        -------\\n        res : ContrastResults instance\\n            The results for the test are attributes of this results instance.\\n            The available results have the same elements as the parameter table\\n            in `summary()`.\\n        '\n    if r_matrix.shape[1] != self.k_fe:\n        raise ValueError('r_matrix for t-test should have %d columns' % self.k_fe)\n    d = self.k_re2 + self.k_vc\n    z0 = np.zeros((r_matrix.shape[0], d))\n    r_matrix = np.concatenate((r_matrix, z0), axis=1)\n    tst_rslt = super(MixedLMResults, self).t_test(r_matrix, use_t=use_t)\n    return tst_rslt",
            "def t_test(self, r_matrix, use_t=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Compute a t-test for a each linear hypothesis of the form Rb = q\\n\\n        Parameters\\n        ----------\\n        r_matrix : array_like\\n            If an array is given, a p x k 2d array or length k 1d\\n            array specifying the linear restrictions. It is assumed\\n            that the linear combination is equal to zero.\\n        scale : float, optional\\n            An optional `scale` to use.  Default is the scale specified\\n            by the model fit.\\n        use_t : bool, optional\\n            If use_t is None, then the default of the model is used.\\n            If use_t is True, then the p-values are based on the t\\n            distribution.\\n            If use_t is False, then the p-values are based on the normal\\n            distribution.\\n\\n        Returns\\n        -------\\n        res : ContrastResults instance\\n            The results for the test are attributes of this results instance.\\n            The available results have the same elements as the parameter table\\n            in `summary()`.\\n        '\n    if r_matrix.shape[1] != self.k_fe:\n        raise ValueError('r_matrix for t-test should have %d columns' % self.k_fe)\n    d = self.k_re2 + self.k_vc\n    z0 = np.zeros((r_matrix.shape[0], d))\n    r_matrix = np.concatenate((r_matrix, z0), axis=1)\n    tst_rslt = super(MixedLMResults, self).t_test(r_matrix, use_t=use_t)\n    return tst_rslt"
        ]
    },
    {
        "func_name": "summary",
        "original": "def summary(self, yname=None, xname_fe=None, xname_re=None, title=None, alpha=0.05):\n    \"\"\"\n        Summarize the mixed model regression results.\n\n        Parameters\n        ----------\n        yname : str, optional\n            Default is `y`\n        xname_fe : list[str], optional\n            Fixed effects covariate names\n        xname_re : list[str], optional\n            Random effects covariate names\n        title : str, optional\n            Title for the top table. If not None, then this replaces\n            the default title\n        alpha : float\n            significance level for the confidence intervals\n\n        Returns\n        -------\n        smry : Summary instance\n            this holds the summary tables and text, which can be\n            printed or converted to various output formats.\n\n        See Also\n        --------\n        statsmodels.iolib.summary2.Summary : class to hold summary results\n        \"\"\"\n    from statsmodels.iolib import summary2\n    smry = summary2.Summary()\n    info = {}\n    info['Model:'] = 'MixedLM'\n    if yname is None:\n        yname = self.model.endog_names\n    param_names = self.model.data.param_names[:]\n    k_fe_params = len(self.fe_params)\n    k_re_params = len(param_names) - len(self.fe_params)\n    if xname_fe is not None:\n        if len(xname_fe) != k_fe_params:\n            msg = 'xname_fe should be a list of length %d' % k_fe_params\n            raise ValueError(msg)\n        param_names[:k_fe_params] = xname_fe\n    if xname_re is not None:\n        if len(xname_re) != k_re_params:\n            msg = 'xname_re should be a list of length %d' % k_re_params\n            raise ValueError(msg)\n        param_names[k_fe_params:] = xname_re\n    info['No. Observations:'] = str(self.model.n_totobs)\n    info['No. Groups:'] = str(self.model.n_groups)\n    gs = np.array([len(x) for x in self.model.endog_li])\n    info['Min. group size:'] = '%.0f' % min(gs)\n    info['Max. group size:'] = '%.0f' % max(gs)\n    info['Mean group size:'] = '%.1f' % np.mean(gs)\n    info['Dependent Variable:'] = yname\n    info['Method:'] = self.method\n    info['Scale:'] = self.scale\n    info['Log-Likelihood:'] = self.llf\n    info['Converged:'] = 'Yes' if self.converged else 'No'\n    smry.add_dict(info)\n    smry.add_title('Mixed Linear Model Regression Results')\n    float_fmt = '%.3f'\n    sdf = np.nan * np.ones((self.k_fe + self.k_re2 + self.k_vc, 6))\n    sdf[0:self.k_fe, 0] = self.fe_params\n    sdf[0:self.k_fe, 1] = np.sqrt(np.diag(self.cov_params()[0:self.k_fe]))\n    sdf[0:self.k_fe, 2] = sdf[0:self.k_fe, 0] / sdf[0:self.k_fe, 1]\n    sdf[0:self.k_fe, 3] = 2 * norm.cdf(-np.abs(sdf[0:self.k_fe, 2]))\n    qm = -norm.ppf(alpha / 2)\n    sdf[0:self.k_fe, 4] = sdf[0:self.k_fe, 0] - qm * sdf[0:self.k_fe, 1]\n    sdf[0:self.k_fe, 5] = sdf[0:self.k_fe, 0] + qm * sdf[0:self.k_fe, 1]\n    jj = self.k_fe\n    for i in range(self.k_re):\n        for j in range(i + 1):\n            sdf[jj, 0] = self.cov_re[i, j]\n            sdf[jj, 1] = np.sqrt(self.scale) * self.bse[jj]\n            jj += 1\n    for i in range(self.k_vc):\n        sdf[jj, 0] = self.vcomp[i]\n        sdf[jj, 1] = np.sqrt(self.scale) * self.bse[jj]\n        jj += 1\n    sdf = pd.DataFrame(index=param_names, data=sdf)\n    sdf.columns = ['Coef.', 'Std.Err.', 'z', 'P>|z|', '[' + str(alpha / 2), str(1 - alpha / 2) + ']']\n    for col in sdf.columns:\n        sdf[col] = [float_fmt % x if np.isfinite(x) else '' for x in sdf[col]]\n    smry.add_df(sdf, align='r')\n    return smry",
        "mutated": [
            "def summary(self, yname=None, xname_fe=None, xname_re=None, title=None, alpha=0.05):\n    if False:\n        i = 10\n    '\\n        Summarize the mixed model regression results.\\n\\n        Parameters\\n        ----------\\n        yname : str, optional\\n            Default is `y`\\n        xname_fe : list[str], optional\\n            Fixed effects covariate names\\n        xname_re : list[str], optional\\n            Random effects covariate names\\n        title : str, optional\\n            Title for the top table. If not None, then this replaces\\n            the default title\\n        alpha : float\\n            significance level for the confidence intervals\\n\\n        Returns\\n        -------\\n        smry : Summary instance\\n            this holds the summary tables and text, which can be\\n            printed or converted to various output formats.\\n\\n        See Also\\n        --------\\n        statsmodels.iolib.summary2.Summary : class to hold summary results\\n        '\n    from statsmodels.iolib import summary2\n    smry = summary2.Summary()\n    info = {}\n    info['Model:'] = 'MixedLM'\n    if yname is None:\n        yname = self.model.endog_names\n    param_names = self.model.data.param_names[:]\n    k_fe_params = len(self.fe_params)\n    k_re_params = len(param_names) - len(self.fe_params)\n    if xname_fe is not None:\n        if len(xname_fe) != k_fe_params:\n            msg = 'xname_fe should be a list of length %d' % k_fe_params\n            raise ValueError(msg)\n        param_names[:k_fe_params] = xname_fe\n    if xname_re is not None:\n        if len(xname_re) != k_re_params:\n            msg = 'xname_re should be a list of length %d' % k_re_params\n            raise ValueError(msg)\n        param_names[k_fe_params:] = xname_re\n    info['No. Observations:'] = str(self.model.n_totobs)\n    info['No. Groups:'] = str(self.model.n_groups)\n    gs = np.array([len(x) for x in self.model.endog_li])\n    info['Min. group size:'] = '%.0f' % min(gs)\n    info['Max. group size:'] = '%.0f' % max(gs)\n    info['Mean group size:'] = '%.1f' % np.mean(gs)\n    info['Dependent Variable:'] = yname\n    info['Method:'] = self.method\n    info['Scale:'] = self.scale\n    info['Log-Likelihood:'] = self.llf\n    info['Converged:'] = 'Yes' if self.converged else 'No'\n    smry.add_dict(info)\n    smry.add_title('Mixed Linear Model Regression Results')\n    float_fmt = '%.3f'\n    sdf = np.nan * np.ones((self.k_fe + self.k_re2 + self.k_vc, 6))\n    sdf[0:self.k_fe, 0] = self.fe_params\n    sdf[0:self.k_fe, 1] = np.sqrt(np.diag(self.cov_params()[0:self.k_fe]))\n    sdf[0:self.k_fe, 2] = sdf[0:self.k_fe, 0] / sdf[0:self.k_fe, 1]\n    sdf[0:self.k_fe, 3] = 2 * norm.cdf(-np.abs(sdf[0:self.k_fe, 2]))\n    qm = -norm.ppf(alpha / 2)\n    sdf[0:self.k_fe, 4] = sdf[0:self.k_fe, 0] - qm * sdf[0:self.k_fe, 1]\n    sdf[0:self.k_fe, 5] = sdf[0:self.k_fe, 0] + qm * sdf[0:self.k_fe, 1]\n    jj = self.k_fe\n    for i in range(self.k_re):\n        for j in range(i + 1):\n            sdf[jj, 0] = self.cov_re[i, j]\n            sdf[jj, 1] = np.sqrt(self.scale) * self.bse[jj]\n            jj += 1\n    for i in range(self.k_vc):\n        sdf[jj, 0] = self.vcomp[i]\n        sdf[jj, 1] = np.sqrt(self.scale) * self.bse[jj]\n        jj += 1\n    sdf = pd.DataFrame(index=param_names, data=sdf)\n    sdf.columns = ['Coef.', 'Std.Err.', 'z', 'P>|z|', '[' + str(alpha / 2), str(1 - alpha / 2) + ']']\n    for col in sdf.columns:\n        sdf[col] = [float_fmt % x if np.isfinite(x) else '' for x in sdf[col]]\n    smry.add_df(sdf, align='r')\n    return smry",
            "def summary(self, yname=None, xname_fe=None, xname_re=None, title=None, alpha=0.05):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Summarize the mixed model regression results.\\n\\n        Parameters\\n        ----------\\n        yname : str, optional\\n            Default is `y`\\n        xname_fe : list[str], optional\\n            Fixed effects covariate names\\n        xname_re : list[str], optional\\n            Random effects covariate names\\n        title : str, optional\\n            Title for the top table. If not None, then this replaces\\n            the default title\\n        alpha : float\\n            significance level for the confidence intervals\\n\\n        Returns\\n        -------\\n        smry : Summary instance\\n            this holds the summary tables and text, which can be\\n            printed or converted to various output formats.\\n\\n        See Also\\n        --------\\n        statsmodels.iolib.summary2.Summary : class to hold summary results\\n        '\n    from statsmodels.iolib import summary2\n    smry = summary2.Summary()\n    info = {}\n    info['Model:'] = 'MixedLM'\n    if yname is None:\n        yname = self.model.endog_names\n    param_names = self.model.data.param_names[:]\n    k_fe_params = len(self.fe_params)\n    k_re_params = len(param_names) - len(self.fe_params)\n    if xname_fe is not None:\n        if len(xname_fe) != k_fe_params:\n            msg = 'xname_fe should be a list of length %d' % k_fe_params\n            raise ValueError(msg)\n        param_names[:k_fe_params] = xname_fe\n    if xname_re is not None:\n        if len(xname_re) != k_re_params:\n            msg = 'xname_re should be a list of length %d' % k_re_params\n            raise ValueError(msg)\n        param_names[k_fe_params:] = xname_re\n    info['No. Observations:'] = str(self.model.n_totobs)\n    info['No. Groups:'] = str(self.model.n_groups)\n    gs = np.array([len(x) for x in self.model.endog_li])\n    info['Min. group size:'] = '%.0f' % min(gs)\n    info['Max. group size:'] = '%.0f' % max(gs)\n    info['Mean group size:'] = '%.1f' % np.mean(gs)\n    info['Dependent Variable:'] = yname\n    info['Method:'] = self.method\n    info['Scale:'] = self.scale\n    info['Log-Likelihood:'] = self.llf\n    info['Converged:'] = 'Yes' if self.converged else 'No'\n    smry.add_dict(info)\n    smry.add_title('Mixed Linear Model Regression Results')\n    float_fmt = '%.3f'\n    sdf = np.nan * np.ones((self.k_fe + self.k_re2 + self.k_vc, 6))\n    sdf[0:self.k_fe, 0] = self.fe_params\n    sdf[0:self.k_fe, 1] = np.sqrt(np.diag(self.cov_params()[0:self.k_fe]))\n    sdf[0:self.k_fe, 2] = sdf[0:self.k_fe, 0] / sdf[0:self.k_fe, 1]\n    sdf[0:self.k_fe, 3] = 2 * norm.cdf(-np.abs(sdf[0:self.k_fe, 2]))\n    qm = -norm.ppf(alpha / 2)\n    sdf[0:self.k_fe, 4] = sdf[0:self.k_fe, 0] - qm * sdf[0:self.k_fe, 1]\n    sdf[0:self.k_fe, 5] = sdf[0:self.k_fe, 0] + qm * sdf[0:self.k_fe, 1]\n    jj = self.k_fe\n    for i in range(self.k_re):\n        for j in range(i + 1):\n            sdf[jj, 0] = self.cov_re[i, j]\n            sdf[jj, 1] = np.sqrt(self.scale) * self.bse[jj]\n            jj += 1\n    for i in range(self.k_vc):\n        sdf[jj, 0] = self.vcomp[i]\n        sdf[jj, 1] = np.sqrt(self.scale) * self.bse[jj]\n        jj += 1\n    sdf = pd.DataFrame(index=param_names, data=sdf)\n    sdf.columns = ['Coef.', 'Std.Err.', 'z', 'P>|z|', '[' + str(alpha / 2), str(1 - alpha / 2) + ']']\n    for col in sdf.columns:\n        sdf[col] = [float_fmt % x if np.isfinite(x) else '' for x in sdf[col]]\n    smry.add_df(sdf, align='r')\n    return smry",
            "def summary(self, yname=None, xname_fe=None, xname_re=None, title=None, alpha=0.05):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Summarize the mixed model regression results.\\n\\n        Parameters\\n        ----------\\n        yname : str, optional\\n            Default is `y`\\n        xname_fe : list[str], optional\\n            Fixed effects covariate names\\n        xname_re : list[str], optional\\n            Random effects covariate names\\n        title : str, optional\\n            Title for the top table. If not None, then this replaces\\n            the default title\\n        alpha : float\\n            significance level for the confidence intervals\\n\\n        Returns\\n        -------\\n        smry : Summary instance\\n            this holds the summary tables and text, which can be\\n            printed or converted to various output formats.\\n\\n        See Also\\n        --------\\n        statsmodels.iolib.summary2.Summary : class to hold summary results\\n        '\n    from statsmodels.iolib import summary2\n    smry = summary2.Summary()\n    info = {}\n    info['Model:'] = 'MixedLM'\n    if yname is None:\n        yname = self.model.endog_names\n    param_names = self.model.data.param_names[:]\n    k_fe_params = len(self.fe_params)\n    k_re_params = len(param_names) - len(self.fe_params)\n    if xname_fe is not None:\n        if len(xname_fe) != k_fe_params:\n            msg = 'xname_fe should be a list of length %d' % k_fe_params\n            raise ValueError(msg)\n        param_names[:k_fe_params] = xname_fe\n    if xname_re is not None:\n        if len(xname_re) != k_re_params:\n            msg = 'xname_re should be a list of length %d' % k_re_params\n            raise ValueError(msg)\n        param_names[k_fe_params:] = xname_re\n    info['No. Observations:'] = str(self.model.n_totobs)\n    info['No. Groups:'] = str(self.model.n_groups)\n    gs = np.array([len(x) for x in self.model.endog_li])\n    info['Min. group size:'] = '%.0f' % min(gs)\n    info['Max. group size:'] = '%.0f' % max(gs)\n    info['Mean group size:'] = '%.1f' % np.mean(gs)\n    info['Dependent Variable:'] = yname\n    info['Method:'] = self.method\n    info['Scale:'] = self.scale\n    info['Log-Likelihood:'] = self.llf\n    info['Converged:'] = 'Yes' if self.converged else 'No'\n    smry.add_dict(info)\n    smry.add_title('Mixed Linear Model Regression Results')\n    float_fmt = '%.3f'\n    sdf = np.nan * np.ones((self.k_fe + self.k_re2 + self.k_vc, 6))\n    sdf[0:self.k_fe, 0] = self.fe_params\n    sdf[0:self.k_fe, 1] = np.sqrt(np.diag(self.cov_params()[0:self.k_fe]))\n    sdf[0:self.k_fe, 2] = sdf[0:self.k_fe, 0] / sdf[0:self.k_fe, 1]\n    sdf[0:self.k_fe, 3] = 2 * norm.cdf(-np.abs(sdf[0:self.k_fe, 2]))\n    qm = -norm.ppf(alpha / 2)\n    sdf[0:self.k_fe, 4] = sdf[0:self.k_fe, 0] - qm * sdf[0:self.k_fe, 1]\n    sdf[0:self.k_fe, 5] = sdf[0:self.k_fe, 0] + qm * sdf[0:self.k_fe, 1]\n    jj = self.k_fe\n    for i in range(self.k_re):\n        for j in range(i + 1):\n            sdf[jj, 0] = self.cov_re[i, j]\n            sdf[jj, 1] = np.sqrt(self.scale) * self.bse[jj]\n            jj += 1\n    for i in range(self.k_vc):\n        sdf[jj, 0] = self.vcomp[i]\n        sdf[jj, 1] = np.sqrt(self.scale) * self.bse[jj]\n        jj += 1\n    sdf = pd.DataFrame(index=param_names, data=sdf)\n    sdf.columns = ['Coef.', 'Std.Err.', 'z', 'P>|z|', '[' + str(alpha / 2), str(1 - alpha / 2) + ']']\n    for col in sdf.columns:\n        sdf[col] = [float_fmt % x if np.isfinite(x) else '' for x in sdf[col]]\n    smry.add_df(sdf, align='r')\n    return smry",
            "def summary(self, yname=None, xname_fe=None, xname_re=None, title=None, alpha=0.05):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Summarize the mixed model regression results.\\n\\n        Parameters\\n        ----------\\n        yname : str, optional\\n            Default is `y`\\n        xname_fe : list[str], optional\\n            Fixed effects covariate names\\n        xname_re : list[str], optional\\n            Random effects covariate names\\n        title : str, optional\\n            Title for the top table. If not None, then this replaces\\n            the default title\\n        alpha : float\\n            significance level for the confidence intervals\\n\\n        Returns\\n        -------\\n        smry : Summary instance\\n            this holds the summary tables and text, which can be\\n            printed or converted to various output formats.\\n\\n        See Also\\n        --------\\n        statsmodels.iolib.summary2.Summary : class to hold summary results\\n        '\n    from statsmodels.iolib import summary2\n    smry = summary2.Summary()\n    info = {}\n    info['Model:'] = 'MixedLM'\n    if yname is None:\n        yname = self.model.endog_names\n    param_names = self.model.data.param_names[:]\n    k_fe_params = len(self.fe_params)\n    k_re_params = len(param_names) - len(self.fe_params)\n    if xname_fe is not None:\n        if len(xname_fe) != k_fe_params:\n            msg = 'xname_fe should be a list of length %d' % k_fe_params\n            raise ValueError(msg)\n        param_names[:k_fe_params] = xname_fe\n    if xname_re is not None:\n        if len(xname_re) != k_re_params:\n            msg = 'xname_re should be a list of length %d' % k_re_params\n            raise ValueError(msg)\n        param_names[k_fe_params:] = xname_re\n    info['No. Observations:'] = str(self.model.n_totobs)\n    info['No. Groups:'] = str(self.model.n_groups)\n    gs = np.array([len(x) for x in self.model.endog_li])\n    info['Min. group size:'] = '%.0f' % min(gs)\n    info['Max. group size:'] = '%.0f' % max(gs)\n    info['Mean group size:'] = '%.1f' % np.mean(gs)\n    info['Dependent Variable:'] = yname\n    info['Method:'] = self.method\n    info['Scale:'] = self.scale\n    info['Log-Likelihood:'] = self.llf\n    info['Converged:'] = 'Yes' if self.converged else 'No'\n    smry.add_dict(info)\n    smry.add_title('Mixed Linear Model Regression Results')\n    float_fmt = '%.3f'\n    sdf = np.nan * np.ones((self.k_fe + self.k_re2 + self.k_vc, 6))\n    sdf[0:self.k_fe, 0] = self.fe_params\n    sdf[0:self.k_fe, 1] = np.sqrt(np.diag(self.cov_params()[0:self.k_fe]))\n    sdf[0:self.k_fe, 2] = sdf[0:self.k_fe, 0] / sdf[0:self.k_fe, 1]\n    sdf[0:self.k_fe, 3] = 2 * norm.cdf(-np.abs(sdf[0:self.k_fe, 2]))\n    qm = -norm.ppf(alpha / 2)\n    sdf[0:self.k_fe, 4] = sdf[0:self.k_fe, 0] - qm * sdf[0:self.k_fe, 1]\n    sdf[0:self.k_fe, 5] = sdf[0:self.k_fe, 0] + qm * sdf[0:self.k_fe, 1]\n    jj = self.k_fe\n    for i in range(self.k_re):\n        for j in range(i + 1):\n            sdf[jj, 0] = self.cov_re[i, j]\n            sdf[jj, 1] = np.sqrt(self.scale) * self.bse[jj]\n            jj += 1\n    for i in range(self.k_vc):\n        sdf[jj, 0] = self.vcomp[i]\n        sdf[jj, 1] = np.sqrt(self.scale) * self.bse[jj]\n        jj += 1\n    sdf = pd.DataFrame(index=param_names, data=sdf)\n    sdf.columns = ['Coef.', 'Std.Err.', 'z', 'P>|z|', '[' + str(alpha / 2), str(1 - alpha / 2) + ']']\n    for col in sdf.columns:\n        sdf[col] = [float_fmt % x if np.isfinite(x) else '' for x in sdf[col]]\n    smry.add_df(sdf, align='r')\n    return smry",
            "def summary(self, yname=None, xname_fe=None, xname_re=None, title=None, alpha=0.05):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Summarize the mixed model regression results.\\n\\n        Parameters\\n        ----------\\n        yname : str, optional\\n            Default is `y`\\n        xname_fe : list[str], optional\\n            Fixed effects covariate names\\n        xname_re : list[str], optional\\n            Random effects covariate names\\n        title : str, optional\\n            Title for the top table. If not None, then this replaces\\n            the default title\\n        alpha : float\\n            significance level for the confidence intervals\\n\\n        Returns\\n        -------\\n        smry : Summary instance\\n            this holds the summary tables and text, which can be\\n            printed or converted to various output formats.\\n\\n        See Also\\n        --------\\n        statsmodels.iolib.summary2.Summary : class to hold summary results\\n        '\n    from statsmodels.iolib import summary2\n    smry = summary2.Summary()\n    info = {}\n    info['Model:'] = 'MixedLM'\n    if yname is None:\n        yname = self.model.endog_names\n    param_names = self.model.data.param_names[:]\n    k_fe_params = len(self.fe_params)\n    k_re_params = len(param_names) - len(self.fe_params)\n    if xname_fe is not None:\n        if len(xname_fe) != k_fe_params:\n            msg = 'xname_fe should be a list of length %d' % k_fe_params\n            raise ValueError(msg)\n        param_names[:k_fe_params] = xname_fe\n    if xname_re is not None:\n        if len(xname_re) != k_re_params:\n            msg = 'xname_re should be a list of length %d' % k_re_params\n            raise ValueError(msg)\n        param_names[k_fe_params:] = xname_re\n    info['No. Observations:'] = str(self.model.n_totobs)\n    info['No. Groups:'] = str(self.model.n_groups)\n    gs = np.array([len(x) for x in self.model.endog_li])\n    info['Min. group size:'] = '%.0f' % min(gs)\n    info['Max. group size:'] = '%.0f' % max(gs)\n    info['Mean group size:'] = '%.1f' % np.mean(gs)\n    info['Dependent Variable:'] = yname\n    info['Method:'] = self.method\n    info['Scale:'] = self.scale\n    info['Log-Likelihood:'] = self.llf\n    info['Converged:'] = 'Yes' if self.converged else 'No'\n    smry.add_dict(info)\n    smry.add_title('Mixed Linear Model Regression Results')\n    float_fmt = '%.3f'\n    sdf = np.nan * np.ones((self.k_fe + self.k_re2 + self.k_vc, 6))\n    sdf[0:self.k_fe, 0] = self.fe_params\n    sdf[0:self.k_fe, 1] = np.sqrt(np.diag(self.cov_params()[0:self.k_fe]))\n    sdf[0:self.k_fe, 2] = sdf[0:self.k_fe, 0] / sdf[0:self.k_fe, 1]\n    sdf[0:self.k_fe, 3] = 2 * norm.cdf(-np.abs(sdf[0:self.k_fe, 2]))\n    qm = -norm.ppf(alpha / 2)\n    sdf[0:self.k_fe, 4] = sdf[0:self.k_fe, 0] - qm * sdf[0:self.k_fe, 1]\n    sdf[0:self.k_fe, 5] = sdf[0:self.k_fe, 0] + qm * sdf[0:self.k_fe, 1]\n    jj = self.k_fe\n    for i in range(self.k_re):\n        for j in range(i + 1):\n            sdf[jj, 0] = self.cov_re[i, j]\n            sdf[jj, 1] = np.sqrt(self.scale) * self.bse[jj]\n            jj += 1\n    for i in range(self.k_vc):\n        sdf[jj, 0] = self.vcomp[i]\n        sdf[jj, 1] = np.sqrt(self.scale) * self.bse[jj]\n        jj += 1\n    sdf = pd.DataFrame(index=param_names, data=sdf)\n    sdf.columns = ['Coef.', 'Std.Err.', 'z', 'P>|z|', '[' + str(alpha / 2), str(1 - alpha / 2) + ']']\n    for col in sdf.columns:\n        sdf[col] = [float_fmt % x if np.isfinite(x) else '' for x in sdf[col]]\n    smry.add_df(sdf, align='r')\n    return smry"
        ]
    },
    {
        "func_name": "llf",
        "original": "@cache_readonly\ndef llf(self):\n    return self.model.loglike(self.params_object, profile_fe=False)",
        "mutated": [
            "@cache_readonly\ndef llf(self):\n    if False:\n        i = 10\n    return self.model.loglike(self.params_object, profile_fe=False)",
            "@cache_readonly\ndef llf(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self.model.loglike(self.params_object, profile_fe=False)",
            "@cache_readonly\ndef llf(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self.model.loglike(self.params_object, profile_fe=False)",
            "@cache_readonly\ndef llf(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self.model.loglike(self.params_object, profile_fe=False)",
            "@cache_readonly\ndef llf(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self.model.loglike(self.params_object, profile_fe=False)"
        ]
    },
    {
        "func_name": "aic",
        "original": "@cache_readonly\ndef aic(self):\n    \"\"\"Akaike information criterion\"\"\"\n    if self.reml:\n        return np.nan\n    if self.freepat is not None:\n        df = self.freepat.get_packed(use_sqrt=False, has_fe=True).sum() + 1\n    else:\n        df = self.params.size + 1\n    return -2 * (self.llf - df)",
        "mutated": [
            "@cache_readonly\ndef aic(self):\n    if False:\n        i = 10\n    'Akaike information criterion'\n    if self.reml:\n        return np.nan\n    if self.freepat is not None:\n        df = self.freepat.get_packed(use_sqrt=False, has_fe=True).sum() + 1\n    else:\n        df = self.params.size + 1\n    return -2 * (self.llf - df)",
            "@cache_readonly\ndef aic(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Akaike information criterion'\n    if self.reml:\n        return np.nan\n    if self.freepat is not None:\n        df = self.freepat.get_packed(use_sqrt=False, has_fe=True).sum() + 1\n    else:\n        df = self.params.size + 1\n    return -2 * (self.llf - df)",
            "@cache_readonly\ndef aic(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Akaike information criterion'\n    if self.reml:\n        return np.nan\n    if self.freepat is not None:\n        df = self.freepat.get_packed(use_sqrt=False, has_fe=True).sum() + 1\n    else:\n        df = self.params.size + 1\n    return -2 * (self.llf - df)",
            "@cache_readonly\ndef aic(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Akaike information criterion'\n    if self.reml:\n        return np.nan\n    if self.freepat is not None:\n        df = self.freepat.get_packed(use_sqrt=False, has_fe=True).sum() + 1\n    else:\n        df = self.params.size + 1\n    return -2 * (self.llf - df)",
            "@cache_readonly\ndef aic(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Akaike information criterion'\n    if self.reml:\n        return np.nan\n    if self.freepat is not None:\n        df = self.freepat.get_packed(use_sqrt=False, has_fe=True).sum() + 1\n    else:\n        df = self.params.size + 1\n    return -2 * (self.llf - df)"
        ]
    },
    {
        "func_name": "bic",
        "original": "@cache_readonly\ndef bic(self):\n    \"\"\"Bayesian information criterion\"\"\"\n    if self.reml:\n        return np.nan\n    if self.freepat is not None:\n        df = self.freepat.get_packed(use_sqrt=False, has_fe=True).sum() + 1\n    else:\n        df = self.params.size + 1\n    return -2 * self.llf + np.log(self.nobs) * df",
        "mutated": [
            "@cache_readonly\ndef bic(self):\n    if False:\n        i = 10\n    'Bayesian information criterion'\n    if self.reml:\n        return np.nan\n    if self.freepat is not None:\n        df = self.freepat.get_packed(use_sqrt=False, has_fe=True).sum() + 1\n    else:\n        df = self.params.size + 1\n    return -2 * self.llf + np.log(self.nobs) * df",
            "@cache_readonly\ndef bic(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Bayesian information criterion'\n    if self.reml:\n        return np.nan\n    if self.freepat is not None:\n        df = self.freepat.get_packed(use_sqrt=False, has_fe=True).sum() + 1\n    else:\n        df = self.params.size + 1\n    return -2 * self.llf + np.log(self.nobs) * df",
            "@cache_readonly\ndef bic(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Bayesian information criterion'\n    if self.reml:\n        return np.nan\n    if self.freepat is not None:\n        df = self.freepat.get_packed(use_sqrt=False, has_fe=True).sum() + 1\n    else:\n        df = self.params.size + 1\n    return -2 * self.llf + np.log(self.nobs) * df",
            "@cache_readonly\ndef bic(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Bayesian information criterion'\n    if self.reml:\n        return np.nan\n    if self.freepat is not None:\n        df = self.freepat.get_packed(use_sqrt=False, has_fe=True).sum() + 1\n    else:\n        df = self.params.size + 1\n    return -2 * self.llf + np.log(self.nobs) * df",
            "@cache_readonly\ndef bic(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Bayesian information criterion'\n    if self.reml:\n        return np.nan\n    if self.freepat is not None:\n        df = self.freepat.get_packed(use_sqrt=False, has_fe=True).sum() + 1\n    else:\n        df = self.params.size + 1\n    return -2 * self.llf + np.log(self.nobs) * df"
        ]
    },
    {
        "func_name": "profile_re",
        "original": "def profile_re(self, re_ix, vtype, num_low=5, dist_low=1.0, num_high=5, dist_high=1.0, **fit_kwargs):\n    \"\"\"\n        Profile-likelihood inference for variance parameters.\n\n        Parameters\n        ----------\n        re_ix : int\n            If vtype is `re`, this value is the index of the variance\n            parameter for which to construct a profile likelihood.  If\n            `vtype` is 'vc' then `re_ix` is the name of the variance\n            parameter to be profiled.\n        vtype : str\n            Either 're' or 'vc', depending on whether the profile\n            analysis is for a random effect or a variance component.\n        num_low : int\n            The number of points at which to calculate the likelihood\n            below the MLE of the parameter of interest.\n        dist_low : float\n            The distance below the MLE of the parameter of interest to\n            begin calculating points on the profile likelihood.\n        num_high : int\n            The number of points at which to calculate the likelihood\n            above the MLE of the parameter of interest.\n        dist_high : float\n            The distance above the MLE of the parameter of interest to\n            begin calculating points on the profile likelihood.\n        **fit_kwargs\n            Additional keyword arguments passed to fit.\n\n        Returns\n        -------\n        An array with two columns.  The first column contains the\n        values to which the parameter of interest is constrained.  The\n        second column contains the corresponding likelihood values.\n\n        Notes\n        -----\n        Only variance parameters can be profiled.\n        \"\"\"\n    pmodel = self.model\n    k_fe = pmodel.k_fe\n    k_re = pmodel.k_re\n    k_vc = pmodel.k_vc\n    (endog, exog) = (pmodel.endog, pmodel.exog)\n    if vtype == 're':\n        ix = np.arange(k_re)\n        ix[0] = re_ix\n        ix[re_ix] = 0\n        exog_re = pmodel.exog_re.copy()[:, ix]\n        params = self.params_object.copy()\n        cov_re_unscaled = params.cov_re\n        cov_re_unscaled = cov_re_unscaled[np.ix_(ix, ix)]\n        params.cov_re = cov_re_unscaled\n        ru0 = cov_re_unscaled[0, 0]\n        cov_re = self.scale * cov_re_unscaled\n        low = (cov_re[0, 0] - dist_low) / self.scale\n        high = (cov_re[0, 0] + dist_high) / self.scale\n    elif vtype == 'vc':\n        re_ix = self.model.exog_vc.names.index(re_ix)\n        params = self.params_object.copy()\n        vcomp = self.vcomp\n        low = (vcomp[re_ix] - dist_low) / self.scale\n        high = (vcomp[re_ix] + dist_high) / self.scale\n        ru0 = vcomp[re_ix] / self.scale\n    if low <= 0:\n        raise ValueError('dist_low is too large and would result in a negative variance. Try a smaller value.')\n    left = np.linspace(low, ru0, num_low + 1)\n    right = np.linspace(ru0, high, num_high + 1)[1:]\n    rvalues = np.concatenate((left, right))\n    free = MixedLMParams(k_fe, k_re, k_vc)\n    if self.freepat is None:\n        free.fe_params = np.ones(k_fe)\n        vcomp = np.ones(k_vc)\n        mat = np.ones((k_re, k_re))\n    else:\n        free.fe_params = self.freepat.fe_params\n        vcomp = self.freepat.vcomp\n        mat = self.freepat.cov_re\n        if vtype == 're':\n            mat = mat[np.ix_(ix, ix)]\n    if vtype == 're':\n        mat[0, 0] = 0\n    else:\n        vcomp[re_ix] = 0\n    free.cov_re = mat\n    free.vcomp = vcomp\n    klass = self.model.__class__\n    init_kwargs = pmodel._get_init_kwds()\n    if vtype == 're':\n        init_kwargs['exog_re'] = exog_re\n    likev = []\n    for x in rvalues:\n        model = klass(endog, exog, **init_kwargs)\n        if vtype == 're':\n            cov_re = params.cov_re.copy()\n            cov_re[0, 0] = x\n            params.cov_re = cov_re\n        else:\n            params.vcomp[re_ix] = x\n        rslt = model.fit(start_params=params, free=free, reml=self.reml, cov_pen=self.cov_pen, **fit_kwargs)._results\n        likev.append([x * rslt.scale, rslt.llf])\n    likev = np.asarray(likev)\n    return likev",
        "mutated": [
            "def profile_re(self, re_ix, vtype, num_low=5, dist_low=1.0, num_high=5, dist_high=1.0, **fit_kwargs):\n    if False:\n        i = 10\n    \"\\n        Profile-likelihood inference for variance parameters.\\n\\n        Parameters\\n        ----------\\n        re_ix : int\\n            If vtype is `re`, this value is the index of the variance\\n            parameter for which to construct a profile likelihood.  If\\n            `vtype` is 'vc' then `re_ix` is the name of the variance\\n            parameter to be profiled.\\n        vtype : str\\n            Either 're' or 'vc', depending on whether the profile\\n            analysis is for a random effect or a variance component.\\n        num_low : int\\n            The number of points at which to calculate the likelihood\\n            below the MLE of the parameter of interest.\\n        dist_low : float\\n            The distance below the MLE of the parameter of interest to\\n            begin calculating points on the profile likelihood.\\n        num_high : int\\n            The number of points at which to calculate the likelihood\\n            above the MLE of the parameter of interest.\\n        dist_high : float\\n            The distance above the MLE of the parameter of interest to\\n            begin calculating points on the profile likelihood.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        An array with two columns.  The first column contains the\\n        values to which the parameter of interest is constrained.  The\\n        second column contains the corresponding likelihood values.\\n\\n        Notes\\n        -----\\n        Only variance parameters can be profiled.\\n        \"\n    pmodel = self.model\n    k_fe = pmodel.k_fe\n    k_re = pmodel.k_re\n    k_vc = pmodel.k_vc\n    (endog, exog) = (pmodel.endog, pmodel.exog)\n    if vtype == 're':\n        ix = np.arange(k_re)\n        ix[0] = re_ix\n        ix[re_ix] = 0\n        exog_re = pmodel.exog_re.copy()[:, ix]\n        params = self.params_object.copy()\n        cov_re_unscaled = params.cov_re\n        cov_re_unscaled = cov_re_unscaled[np.ix_(ix, ix)]\n        params.cov_re = cov_re_unscaled\n        ru0 = cov_re_unscaled[0, 0]\n        cov_re = self.scale * cov_re_unscaled\n        low = (cov_re[0, 0] - dist_low) / self.scale\n        high = (cov_re[0, 0] + dist_high) / self.scale\n    elif vtype == 'vc':\n        re_ix = self.model.exog_vc.names.index(re_ix)\n        params = self.params_object.copy()\n        vcomp = self.vcomp\n        low = (vcomp[re_ix] - dist_low) / self.scale\n        high = (vcomp[re_ix] + dist_high) / self.scale\n        ru0 = vcomp[re_ix] / self.scale\n    if low <= 0:\n        raise ValueError('dist_low is too large and would result in a negative variance. Try a smaller value.')\n    left = np.linspace(low, ru0, num_low + 1)\n    right = np.linspace(ru0, high, num_high + 1)[1:]\n    rvalues = np.concatenate((left, right))\n    free = MixedLMParams(k_fe, k_re, k_vc)\n    if self.freepat is None:\n        free.fe_params = np.ones(k_fe)\n        vcomp = np.ones(k_vc)\n        mat = np.ones((k_re, k_re))\n    else:\n        free.fe_params = self.freepat.fe_params\n        vcomp = self.freepat.vcomp\n        mat = self.freepat.cov_re\n        if vtype == 're':\n            mat = mat[np.ix_(ix, ix)]\n    if vtype == 're':\n        mat[0, 0] = 0\n    else:\n        vcomp[re_ix] = 0\n    free.cov_re = mat\n    free.vcomp = vcomp\n    klass = self.model.__class__\n    init_kwargs = pmodel._get_init_kwds()\n    if vtype == 're':\n        init_kwargs['exog_re'] = exog_re\n    likev = []\n    for x in rvalues:\n        model = klass(endog, exog, **init_kwargs)\n        if vtype == 're':\n            cov_re = params.cov_re.copy()\n            cov_re[0, 0] = x\n            params.cov_re = cov_re\n        else:\n            params.vcomp[re_ix] = x\n        rslt = model.fit(start_params=params, free=free, reml=self.reml, cov_pen=self.cov_pen, **fit_kwargs)._results\n        likev.append([x * rslt.scale, rslt.llf])\n    likev = np.asarray(likev)\n    return likev",
            "def profile_re(self, re_ix, vtype, num_low=5, dist_low=1.0, num_high=5, dist_high=1.0, **fit_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Profile-likelihood inference for variance parameters.\\n\\n        Parameters\\n        ----------\\n        re_ix : int\\n            If vtype is `re`, this value is the index of the variance\\n            parameter for which to construct a profile likelihood.  If\\n            `vtype` is 'vc' then `re_ix` is the name of the variance\\n            parameter to be profiled.\\n        vtype : str\\n            Either 're' or 'vc', depending on whether the profile\\n            analysis is for a random effect or a variance component.\\n        num_low : int\\n            The number of points at which to calculate the likelihood\\n            below the MLE of the parameter of interest.\\n        dist_low : float\\n            The distance below the MLE of the parameter of interest to\\n            begin calculating points on the profile likelihood.\\n        num_high : int\\n            The number of points at which to calculate the likelihood\\n            above the MLE of the parameter of interest.\\n        dist_high : float\\n            The distance above the MLE of the parameter of interest to\\n            begin calculating points on the profile likelihood.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        An array with two columns.  The first column contains the\\n        values to which the parameter of interest is constrained.  The\\n        second column contains the corresponding likelihood values.\\n\\n        Notes\\n        -----\\n        Only variance parameters can be profiled.\\n        \"\n    pmodel = self.model\n    k_fe = pmodel.k_fe\n    k_re = pmodel.k_re\n    k_vc = pmodel.k_vc\n    (endog, exog) = (pmodel.endog, pmodel.exog)\n    if vtype == 're':\n        ix = np.arange(k_re)\n        ix[0] = re_ix\n        ix[re_ix] = 0\n        exog_re = pmodel.exog_re.copy()[:, ix]\n        params = self.params_object.copy()\n        cov_re_unscaled = params.cov_re\n        cov_re_unscaled = cov_re_unscaled[np.ix_(ix, ix)]\n        params.cov_re = cov_re_unscaled\n        ru0 = cov_re_unscaled[0, 0]\n        cov_re = self.scale * cov_re_unscaled\n        low = (cov_re[0, 0] - dist_low) / self.scale\n        high = (cov_re[0, 0] + dist_high) / self.scale\n    elif vtype == 'vc':\n        re_ix = self.model.exog_vc.names.index(re_ix)\n        params = self.params_object.copy()\n        vcomp = self.vcomp\n        low = (vcomp[re_ix] - dist_low) / self.scale\n        high = (vcomp[re_ix] + dist_high) / self.scale\n        ru0 = vcomp[re_ix] / self.scale\n    if low <= 0:\n        raise ValueError('dist_low is too large and would result in a negative variance. Try a smaller value.')\n    left = np.linspace(low, ru0, num_low + 1)\n    right = np.linspace(ru0, high, num_high + 1)[1:]\n    rvalues = np.concatenate((left, right))\n    free = MixedLMParams(k_fe, k_re, k_vc)\n    if self.freepat is None:\n        free.fe_params = np.ones(k_fe)\n        vcomp = np.ones(k_vc)\n        mat = np.ones((k_re, k_re))\n    else:\n        free.fe_params = self.freepat.fe_params\n        vcomp = self.freepat.vcomp\n        mat = self.freepat.cov_re\n        if vtype == 're':\n            mat = mat[np.ix_(ix, ix)]\n    if vtype == 're':\n        mat[0, 0] = 0\n    else:\n        vcomp[re_ix] = 0\n    free.cov_re = mat\n    free.vcomp = vcomp\n    klass = self.model.__class__\n    init_kwargs = pmodel._get_init_kwds()\n    if vtype == 're':\n        init_kwargs['exog_re'] = exog_re\n    likev = []\n    for x in rvalues:\n        model = klass(endog, exog, **init_kwargs)\n        if vtype == 're':\n            cov_re = params.cov_re.copy()\n            cov_re[0, 0] = x\n            params.cov_re = cov_re\n        else:\n            params.vcomp[re_ix] = x\n        rslt = model.fit(start_params=params, free=free, reml=self.reml, cov_pen=self.cov_pen, **fit_kwargs)._results\n        likev.append([x * rslt.scale, rslt.llf])\n    likev = np.asarray(likev)\n    return likev",
            "def profile_re(self, re_ix, vtype, num_low=5, dist_low=1.0, num_high=5, dist_high=1.0, **fit_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Profile-likelihood inference for variance parameters.\\n\\n        Parameters\\n        ----------\\n        re_ix : int\\n            If vtype is `re`, this value is the index of the variance\\n            parameter for which to construct a profile likelihood.  If\\n            `vtype` is 'vc' then `re_ix` is the name of the variance\\n            parameter to be profiled.\\n        vtype : str\\n            Either 're' or 'vc', depending on whether the profile\\n            analysis is for a random effect or a variance component.\\n        num_low : int\\n            The number of points at which to calculate the likelihood\\n            below the MLE of the parameter of interest.\\n        dist_low : float\\n            The distance below the MLE of the parameter of interest to\\n            begin calculating points on the profile likelihood.\\n        num_high : int\\n            The number of points at which to calculate the likelihood\\n            above the MLE of the parameter of interest.\\n        dist_high : float\\n            The distance above the MLE of the parameter of interest to\\n            begin calculating points on the profile likelihood.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        An array with two columns.  The first column contains the\\n        values to which the parameter of interest is constrained.  The\\n        second column contains the corresponding likelihood values.\\n\\n        Notes\\n        -----\\n        Only variance parameters can be profiled.\\n        \"\n    pmodel = self.model\n    k_fe = pmodel.k_fe\n    k_re = pmodel.k_re\n    k_vc = pmodel.k_vc\n    (endog, exog) = (pmodel.endog, pmodel.exog)\n    if vtype == 're':\n        ix = np.arange(k_re)\n        ix[0] = re_ix\n        ix[re_ix] = 0\n        exog_re = pmodel.exog_re.copy()[:, ix]\n        params = self.params_object.copy()\n        cov_re_unscaled = params.cov_re\n        cov_re_unscaled = cov_re_unscaled[np.ix_(ix, ix)]\n        params.cov_re = cov_re_unscaled\n        ru0 = cov_re_unscaled[0, 0]\n        cov_re = self.scale * cov_re_unscaled\n        low = (cov_re[0, 0] - dist_low) / self.scale\n        high = (cov_re[0, 0] + dist_high) / self.scale\n    elif vtype == 'vc':\n        re_ix = self.model.exog_vc.names.index(re_ix)\n        params = self.params_object.copy()\n        vcomp = self.vcomp\n        low = (vcomp[re_ix] - dist_low) / self.scale\n        high = (vcomp[re_ix] + dist_high) / self.scale\n        ru0 = vcomp[re_ix] / self.scale\n    if low <= 0:\n        raise ValueError('dist_low is too large and would result in a negative variance. Try a smaller value.')\n    left = np.linspace(low, ru0, num_low + 1)\n    right = np.linspace(ru0, high, num_high + 1)[1:]\n    rvalues = np.concatenate((left, right))\n    free = MixedLMParams(k_fe, k_re, k_vc)\n    if self.freepat is None:\n        free.fe_params = np.ones(k_fe)\n        vcomp = np.ones(k_vc)\n        mat = np.ones((k_re, k_re))\n    else:\n        free.fe_params = self.freepat.fe_params\n        vcomp = self.freepat.vcomp\n        mat = self.freepat.cov_re\n        if vtype == 're':\n            mat = mat[np.ix_(ix, ix)]\n    if vtype == 're':\n        mat[0, 0] = 0\n    else:\n        vcomp[re_ix] = 0\n    free.cov_re = mat\n    free.vcomp = vcomp\n    klass = self.model.__class__\n    init_kwargs = pmodel._get_init_kwds()\n    if vtype == 're':\n        init_kwargs['exog_re'] = exog_re\n    likev = []\n    for x in rvalues:\n        model = klass(endog, exog, **init_kwargs)\n        if vtype == 're':\n            cov_re = params.cov_re.copy()\n            cov_re[0, 0] = x\n            params.cov_re = cov_re\n        else:\n            params.vcomp[re_ix] = x\n        rslt = model.fit(start_params=params, free=free, reml=self.reml, cov_pen=self.cov_pen, **fit_kwargs)._results\n        likev.append([x * rslt.scale, rslt.llf])\n    likev = np.asarray(likev)\n    return likev",
            "def profile_re(self, re_ix, vtype, num_low=5, dist_low=1.0, num_high=5, dist_high=1.0, **fit_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Profile-likelihood inference for variance parameters.\\n\\n        Parameters\\n        ----------\\n        re_ix : int\\n            If vtype is `re`, this value is the index of the variance\\n            parameter for which to construct a profile likelihood.  If\\n            `vtype` is 'vc' then `re_ix` is the name of the variance\\n            parameter to be profiled.\\n        vtype : str\\n            Either 're' or 'vc', depending on whether the profile\\n            analysis is for a random effect or a variance component.\\n        num_low : int\\n            The number of points at which to calculate the likelihood\\n            below the MLE of the parameter of interest.\\n        dist_low : float\\n            The distance below the MLE of the parameter of interest to\\n            begin calculating points on the profile likelihood.\\n        num_high : int\\n            The number of points at which to calculate the likelihood\\n            above the MLE of the parameter of interest.\\n        dist_high : float\\n            The distance above the MLE of the parameter of interest to\\n            begin calculating points on the profile likelihood.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        An array with two columns.  The first column contains the\\n        values to which the parameter of interest is constrained.  The\\n        second column contains the corresponding likelihood values.\\n\\n        Notes\\n        -----\\n        Only variance parameters can be profiled.\\n        \"\n    pmodel = self.model\n    k_fe = pmodel.k_fe\n    k_re = pmodel.k_re\n    k_vc = pmodel.k_vc\n    (endog, exog) = (pmodel.endog, pmodel.exog)\n    if vtype == 're':\n        ix = np.arange(k_re)\n        ix[0] = re_ix\n        ix[re_ix] = 0\n        exog_re = pmodel.exog_re.copy()[:, ix]\n        params = self.params_object.copy()\n        cov_re_unscaled = params.cov_re\n        cov_re_unscaled = cov_re_unscaled[np.ix_(ix, ix)]\n        params.cov_re = cov_re_unscaled\n        ru0 = cov_re_unscaled[0, 0]\n        cov_re = self.scale * cov_re_unscaled\n        low = (cov_re[0, 0] - dist_low) / self.scale\n        high = (cov_re[0, 0] + dist_high) / self.scale\n    elif vtype == 'vc':\n        re_ix = self.model.exog_vc.names.index(re_ix)\n        params = self.params_object.copy()\n        vcomp = self.vcomp\n        low = (vcomp[re_ix] - dist_low) / self.scale\n        high = (vcomp[re_ix] + dist_high) / self.scale\n        ru0 = vcomp[re_ix] / self.scale\n    if low <= 0:\n        raise ValueError('dist_low is too large and would result in a negative variance. Try a smaller value.')\n    left = np.linspace(low, ru0, num_low + 1)\n    right = np.linspace(ru0, high, num_high + 1)[1:]\n    rvalues = np.concatenate((left, right))\n    free = MixedLMParams(k_fe, k_re, k_vc)\n    if self.freepat is None:\n        free.fe_params = np.ones(k_fe)\n        vcomp = np.ones(k_vc)\n        mat = np.ones((k_re, k_re))\n    else:\n        free.fe_params = self.freepat.fe_params\n        vcomp = self.freepat.vcomp\n        mat = self.freepat.cov_re\n        if vtype == 're':\n            mat = mat[np.ix_(ix, ix)]\n    if vtype == 're':\n        mat[0, 0] = 0\n    else:\n        vcomp[re_ix] = 0\n    free.cov_re = mat\n    free.vcomp = vcomp\n    klass = self.model.__class__\n    init_kwargs = pmodel._get_init_kwds()\n    if vtype == 're':\n        init_kwargs['exog_re'] = exog_re\n    likev = []\n    for x in rvalues:\n        model = klass(endog, exog, **init_kwargs)\n        if vtype == 're':\n            cov_re = params.cov_re.copy()\n            cov_re[0, 0] = x\n            params.cov_re = cov_re\n        else:\n            params.vcomp[re_ix] = x\n        rslt = model.fit(start_params=params, free=free, reml=self.reml, cov_pen=self.cov_pen, **fit_kwargs)._results\n        likev.append([x * rslt.scale, rslt.llf])\n    likev = np.asarray(likev)\n    return likev",
            "def profile_re(self, re_ix, vtype, num_low=5, dist_low=1.0, num_high=5, dist_high=1.0, **fit_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Profile-likelihood inference for variance parameters.\\n\\n        Parameters\\n        ----------\\n        re_ix : int\\n            If vtype is `re`, this value is the index of the variance\\n            parameter for which to construct a profile likelihood.  If\\n            `vtype` is 'vc' then `re_ix` is the name of the variance\\n            parameter to be profiled.\\n        vtype : str\\n            Either 're' or 'vc', depending on whether the profile\\n            analysis is for a random effect or a variance component.\\n        num_low : int\\n            The number of points at which to calculate the likelihood\\n            below the MLE of the parameter of interest.\\n        dist_low : float\\n            The distance below the MLE of the parameter of interest to\\n            begin calculating points on the profile likelihood.\\n        num_high : int\\n            The number of points at which to calculate the likelihood\\n            above the MLE of the parameter of interest.\\n        dist_high : float\\n            The distance above the MLE of the parameter of interest to\\n            begin calculating points on the profile likelihood.\\n        **fit_kwargs\\n            Additional keyword arguments passed to fit.\\n\\n        Returns\\n        -------\\n        An array with two columns.  The first column contains the\\n        values to which the parameter of interest is constrained.  The\\n        second column contains the corresponding likelihood values.\\n\\n        Notes\\n        -----\\n        Only variance parameters can be profiled.\\n        \"\n    pmodel = self.model\n    k_fe = pmodel.k_fe\n    k_re = pmodel.k_re\n    k_vc = pmodel.k_vc\n    (endog, exog) = (pmodel.endog, pmodel.exog)\n    if vtype == 're':\n        ix = np.arange(k_re)\n        ix[0] = re_ix\n        ix[re_ix] = 0\n        exog_re = pmodel.exog_re.copy()[:, ix]\n        params = self.params_object.copy()\n        cov_re_unscaled = params.cov_re\n        cov_re_unscaled = cov_re_unscaled[np.ix_(ix, ix)]\n        params.cov_re = cov_re_unscaled\n        ru0 = cov_re_unscaled[0, 0]\n        cov_re = self.scale * cov_re_unscaled\n        low = (cov_re[0, 0] - dist_low) / self.scale\n        high = (cov_re[0, 0] + dist_high) / self.scale\n    elif vtype == 'vc':\n        re_ix = self.model.exog_vc.names.index(re_ix)\n        params = self.params_object.copy()\n        vcomp = self.vcomp\n        low = (vcomp[re_ix] - dist_low) / self.scale\n        high = (vcomp[re_ix] + dist_high) / self.scale\n        ru0 = vcomp[re_ix] / self.scale\n    if low <= 0:\n        raise ValueError('dist_low is too large and would result in a negative variance. Try a smaller value.')\n    left = np.linspace(low, ru0, num_low + 1)\n    right = np.linspace(ru0, high, num_high + 1)[1:]\n    rvalues = np.concatenate((left, right))\n    free = MixedLMParams(k_fe, k_re, k_vc)\n    if self.freepat is None:\n        free.fe_params = np.ones(k_fe)\n        vcomp = np.ones(k_vc)\n        mat = np.ones((k_re, k_re))\n    else:\n        free.fe_params = self.freepat.fe_params\n        vcomp = self.freepat.vcomp\n        mat = self.freepat.cov_re\n        if vtype == 're':\n            mat = mat[np.ix_(ix, ix)]\n    if vtype == 're':\n        mat[0, 0] = 0\n    else:\n        vcomp[re_ix] = 0\n    free.cov_re = mat\n    free.vcomp = vcomp\n    klass = self.model.__class__\n    init_kwargs = pmodel._get_init_kwds()\n    if vtype == 're':\n        init_kwargs['exog_re'] = exog_re\n    likev = []\n    for x in rvalues:\n        model = klass(endog, exog, **init_kwargs)\n        if vtype == 're':\n            cov_re = params.cov_re.copy()\n            cov_re[0, 0] = x\n            params.cov_re = cov_re\n        else:\n            params.vcomp[re_ix] = x\n        rslt = model.fit(start_params=params, free=free, reml=self.reml, cov_pen=self.cov_pen, **fit_kwargs)._results\n        likev.append([x * rslt.scale, rslt.llf])\n    likev = np.asarray(likev)\n    return likev"
        ]
    },
    {
        "func_name": "rlu",
        "original": "def rlu():\n    line = rl.readline()\n    return asunicode(line, 'ascii')",
        "mutated": [
            "def rlu():\n    if False:\n        i = 10\n    line = rl.readline()\n    return asunicode(line, 'ascii')",
            "def rlu():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    line = rl.readline()\n    return asunicode(line, 'ascii')",
            "def rlu():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    line = rl.readline()\n    return asunicode(line, 'ascii')",
            "def rlu():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    line = rl.readline()\n    return asunicode(line, 'ascii')",
            "def rlu():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    line = rl.readline()\n    return asunicode(line, 'ascii')"
        ]
    },
    {
        "func_name": "_handle_missing",
        "original": "def _handle_missing(data, groups, formula, re_formula, vc_formula):\n    tokens = set()\n    forms = [formula]\n    if re_formula is not None:\n        forms.append(re_formula)\n    if vc_formula is not None:\n        forms.extend(vc_formula.values())\n    from statsmodels.compat.python import asunicode\n    from io import StringIO\n    import tokenize\n    skiptoks = {'(', ')', '*', ':', '+', '-', '**', '/'}\n    for fml in forms:\n        rl = StringIO(fml)\n\n        def rlu():\n            line = rl.readline()\n            return asunicode(line, 'ascii')\n        g = tokenize.generate_tokens(rlu)\n        for tok in g:\n            if tok not in skiptoks:\n                tokens.add(tok.string)\n    tokens = sorted(tokens & set(data.columns))\n    data = data[tokens]\n    ii = pd.notnull(data).all(1)\n    if type(groups) is not str:\n        ii &= pd.notnull(groups)\n    return (data.loc[ii, :], groups[np.asarray(ii)])",
        "mutated": [
            "def _handle_missing(data, groups, formula, re_formula, vc_formula):\n    if False:\n        i = 10\n    tokens = set()\n    forms = [formula]\n    if re_formula is not None:\n        forms.append(re_formula)\n    if vc_formula is not None:\n        forms.extend(vc_formula.values())\n    from statsmodels.compat.python import asunicode\n    from io import StringIO\n    import tokenize\n    skiptoks = {'(', ')', '*', ':', '+', '-', '**', '/'}\n    for fml in forms:\n        rl = StringIO(fml)\n\n        def rlu():\n            line = rl.readline()\n            return asunicode(line, 'ascii')\n        g = tokenize.generate_tokens(rlu)\n        for tok in g:\n            if tok not in skiptoks:\n                tokens.add(tok.string)\n    tokens = sorted(tokens & set(data.columns))\n    data = data[tokens]\n    ii = pd.notnull(data).all(1)\n    if type(groups) is not str:\n        ii &= pd.notnull(groups)\n    return (data.loc[ii, :], groups[np.asarray(ii)])",
            "def _handle_missing(data, groups, formula, re_formula, vc_formula):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tokens = set()\n    forms = [formula]\n    if re_formula is not None:\n        forms.append(re_formula)\n    if vc_formula is not None:\n        forms.extend(vc_formula.values())\n    from statsmodels.compat.python import asunicode\n    from io import StringIO\n    import tokenize\n    skiptoks = {'(', ')', '*', ':', '+', '-', '**', '/'}\n    for fml in forms:\n        rl = StringIO(fml)\n\n        def rlu():\n            line = rl.readline()\n            return asunicode(line, 'ascii')\n        g = tokenize.generate_tokens(rlu)\n        for tok in g:\n            if tok not in skiptoks:\n                tokens.add(tok.string)\n    tokens = sorted(tokens & set(data.columns))\n    data = data[tokens]\n    ii = pd.notnull(data).all(1)\n    if type(groups) is not str:\n        ii &= pd.notnull(groups)\n    return (data.loc[ii, :], groups[np.asarray(ii)])",
            "def _handle_missing(data, groups, formula, re_formula, vc_formula):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tokens = set()\n    forms = [formula]\n    if re_formula is not None:\n        forms.append(re_formula)\n    if vc_formula is not None:\n        forms.extend(vc_formula.values())\n    from statsmodels.compat.python import asunicode\n    from io import StringIO\n    import tokenize\n    skiptoks = {'(', ')', '*', ':', '+', '-', '**', '/'}\n    for fml in forms:\n        rl = StringIO(fml)\n\n        def rlu():\n            line = rl.readline()\n            return asunicode(line, 'ascii')\n        g = tokenize.generate_tokens(rlu)\n        for tok in g:\n            if tok not in skiptoks:\n                tokens.add(tok.string)\n    tokens = sorted(tokens & set(data.columns))\n    data = data[tokens]\n    ii = pd.notnull(data).all(1)\n    if type(groups) is not str:\n        ii &= pd.notnull(groups)\n    return (data.loc[ii, :], groups[np.asarray(ii)])",
            "def _handle_missing(data, groups, formula, re_formula, vc_formula):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tokens = set()\n    forms = [formula]\n    if re_formula is not None:\n        forms.append(re_formula)\n    if vc_formula is not None:\n        forms.extend(vc_formula.values())\n    from statsmodels.compat.python import asunicode\n    from io import StringIO\n    import tokenize\n    skiptoks = {'(', ')', '*', ':', '+', '-', '**', '/'}\n    for fml in forms:\n        rl = StringIO(fml)\n\n        def rlu():\n            line = rl.readline()\n            return asunicode(line, 'ascii')\n        g = tokenize.generate_tokens(rlu)\n        for tok in g:\n            if tok not in skiptoks:\n                tokens.add(tok.string)\n    tokens = sorted(tokens & set(data.columns))\n    data = data[tokens]\n    ii = pd.notnull(data).all(1)\n    if type(groups) is not str:\n        ii &= pd.notnull(groups)\n    return (data.loc[ii, :], groups[np.asarray(ii)])",
            "def _handle_missing(data, groups, formula, re_formula, vc_formula):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tokens = set()\n    forms = [formula]\n    if re_formula is not None:\n        forms.append(re_formula)\n    if vc_formula is not None:\n        forms.extend(vc_formula.values())\n    from statsmodels.compat.python import asunicode\n    from io import StringIO\n    import tokenize\n    skiptoks = {'(', ')', '*', ':', '+', '-', '**', '/'}\n    for fml in forms:\n        rl = StringIO(fml)\n\n        def rlu():\n            line = rl.readline()\n            return asunicode(line, 'ascii')\n        g = tokenize.generate_tokens(rlu)\n        for tok in g:\n            if tok not in skiptoks:\n                tokens.add(tok.string)\n    tokens = sorted(tokens & set(data.columns))\n    data = data[tokens]\n    ii = pd.notnull(data).all(1)\n    if type(groups) is not str:\n        ii &= pd.notnull(groups)\n    return (data.loc[ii, :], groups[np.asarray(ii)])"
        ]
    }
]