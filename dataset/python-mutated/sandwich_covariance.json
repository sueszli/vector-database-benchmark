[
    {
        "func_name": "_HCCM",
        "original": "def _HCCM(results, scale):\n    \"\"\"\n    sandwich with pinv(x) * diag(scale) * pinv(x).T\n\n    where pinv(x) = (X'X)^(-1) X\n    and scale is (nobs,)\n    \"\"\"\n    H = np.dot(results.model.pinv_wexog, scale[:, None] * results.model.pinv_wexog.T)\n    return H",
        "mutated": [
            "def _HCCM(results, scale):\n    if False:\n        i = 10\n    \"\\n    sandwich with pinv(x) * diag(scale) * pinv(x).T\\n\\n    where pinv(x) = (X'X)^(-1) X\\n    and scale is (nobs,)\\n    \"\n    H = np.dot(results.model.pinv_wexog, scale[:, None] * results.model.pinv_wexog.T)\n    return H",
            "def _HCCM(results, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    sandwich with pinv(x) * diag(scale) * pinv(x).T\\n\\n    where pinv(x) = (X'X)^(-1) X\\n    and scale is (nobs,)\\n    \"\n    H = np.dot(results.model.pinv_wexog, scale[:, None] * results.model.pinv_wexog.T)\n    return H",
            "def _HCCM(results, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    sandwich with pinv(x) * diag(scale) * pinv(x).T\\n\\n    where pinv(x) = (X'X)^(-1) X\\n    and scale is (nobs,)\\n    \"\n    H = np.dot(results.model.pinv_wexog, scale[:, None] * results.model.pinv_wexog.T)\n    return H",
            "def _HCCM(results, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    sandwich with pinv(x) * diag(scale) * pinv(x).T\\n\\n    where pinv(x) = (X'X)^(-1) X\\n    and scale is (nobs,)\\n    \"\n    H = np.dot(results.model.pinv_wexog, scale[:, None] * results.model.pinv_wexog.T)\n    return H",
            "def _HCCM(results, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    sandwich with pinv(x) * diag(scale) * pinv(x).T\\n\\n    where pinv(x) = (X'X)^(-1) X\\n    and scale is (nobs,)\\n    \"\n    H = np.dot(results.model.pinv_wexog, scale[:, None] * results.model.pinv_wexog.T)\n    return H"
        ]
    },
    {
        "func_name": "cov_hc0",
        "original": "def cov_hc0(results):\n    \"\"\"\n    See statsmodels.RegressionResults\n    \"\"\"\n    het_scale = results.resid ** 2\n    cov_hc0 = _HCCM(results, het_scale)\n    return cov_hc0",
        "mutated": [
            "def cov_hc0(results):\n    if False:\n        i = 10\n    '\\n    See statsmodels.RegressionResults\\n    '\n    het_scale = results.resid ** 2\n    cov_hc0 = _HCCM(results, het_scale)\n    return cov_hc0",
            "def cov_hc0(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    See statsmodels.RegressionResults\\n    '\n    het_scale = results.resid ** 2\n    cov_hc0 = _HCCM(results, het_scale)\n    return cov_hc0",
            "def cov_hc0(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    See statsmodels.RegressionResults\\n    '\n    het_scale = results.resid ** 2\n    cov_hc0 = _HCCM(results, het_scale)\n    return cov_hc0",
            "def cov_hc0(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    See statsmodels.RegressionResults\\n    '\n    het_scale = results.resid ** 2\n    cov_hc0 = _HCCM(results, het_scale)\n    return cov_hc0",
            "def cov_hc0(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    See statsmodels.RegressionResults\\n    '\n    het_scale = results.resid ** 2\n    cov_hc0 = _HCCM(results, het_scale)\n    return cov_hc0"
        ]
    },
    {
        "func_name": "cov_hc1",
        "original": "def cov_hc1(results):\n    \"\"\"\n    See statsmodels.RegressionResults\n    \"\"\"\n    het_scale = results.nobs / results.df_resid * results.resid ** 2\n    cov_hc1 = _HCCM(results, het_scale)\n    return cov_hc1",
        "mutated": [
            "def cov_hc1(results):\n    if False:\n        i = 10\n    '\\n    See statsmodels.RegressionResults\\n    '\n    het_scale = results.nobs / results.df_resid * results.resid ** 2\n    cov_hc1 = _HCCM(results, het_scale)\n    return cov_hc1",
            "def cov_hc1(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    See statsmodels.RegressionResults\\n    '\n    het_scale = results.nobs / results.df_resid * results.resid ** 2\n    cov_hc1 = _HCCM(results, het_scale)\n    return cov_hc1",
            "def cov_hc1(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    See statsmodels.RegressionResults\\n    '\n    het_scale = results.nobs / results.df_resid * results.resid ** 2\n    cov_hc1 = _HCCM(results, het_scale)\n    return cov_hc1",
            "def cov_hc1(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    See statsmodels.RegressionResults\\n    '\n    het_scale = results.nobs / results.df_resid * results.resid ** 2\n    cov_hc1 = _HCCM(results, het_scale)\n    return cov_hc1",
            "def cov_hc1(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    See statsmodels.RegressionResults\\n    '\n    het_scale = results.nobs / results.df_resid * results.resid ** 2\n    cov_hc1 = _HCCM(results, het_scale)\n    return cov_hc1"
        ]
    },
    {
        "func_name": "cov_hc2",
        "original": "def cov_hc2(results):\n    \"\"\"\n    See statsmodels.RegressionResults\n    \"\"\"\n    h = np.diag(np.dot(results.model.exog, np.dot(results.normalized_cov_params, results.model.exog.T)))\n    het_scale = results.resid ** 2 / (1 - h)\n    cov_hc2_ = _HCCM(results, het_scale)\n    return cov_hc2_",
        "mutated": [
            "def cov_hc2(results):\n    if False:\n        i = 10\n    '\\n    See statsmodels.RegressionResults\\n    '\n    h = np.diag(np.dot(results.model.exog, np.dot(results.normalized_cov_params, results.model.exog.T)))\n    het_scale = results.resid ** 2 / (1 - h)\n    cov_hc2_ = _HCCM(results, het_scale)\n    return cov_hc2_",
            "def cov_hc2(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    See statsmodels.RegressionResults\\n    '\n    h = np.diag(np.dot(results.model.exog, np.dot(results.normalized_cov_params, results.model.exog.T)))\n    het_scale = results.resid ** 2 / (1 - h)\n    cov_hc2_ = _HCCM(results, het_scale)\n    return cov_hc2_",
            "def cov_hc2(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    See statsmodels.RegressionResults\\n    '\n    h = np.diag(np.dot(results.model.exog, np.dot(results.normalized_cov_params, results.model.exog.T)))\n    het_scale = results.resid ** 2 / (1 - h)\n    cov_hc2_ = _HCCM(results, het_scale)\n    return cov_hc2_",
            "def cov_hc2(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    See statsmodels.RegressionResults\\n    '\n    h = np.diag(np.dot(results.model.exog, np.dot(results.normalized_cov_params, results.model.exog.T)))\n    het_scale = results.resid ** 2 / (1 - h)\n    cov_hc2_ = _HCCM(results, het_scale)\n    return cov_hc2_",
            "def cov_hc2(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    See statsmodels.RegressionResults\\n    '\n    h = np.diag(np.dot(results.model.exog, np.dot(results.normalized_cov_params, results.model.exog.T)))\n    het_scale = results.resid ** 2 / (1 - h)\n    cov_hc2_ = _HCCM(results, het_scale)\n    return cov_hc2_"
        ]
    },
    {
        "func_name": "cov_hc3",
        "original": "def cov_hc3(results):\n    \"\"\"\n    See statsmodels.RegressionResults\n    \"\"\"\n    h = np.diag(np.dot(results.model.exog, np.dot(results.normalized_cov_params, results.model.exog.T)))\n    het_scale = (results.resid / (1 - h)) ** 2\n    cov_hc3_ = _HCCM(results, het_scale)\n    return cov_hc3_",
        "mutated": [
            "def cov_hc3(results):\n    if False:\n        i = 10\n    '\\n    See statsmodels.RegressionResults\\n    '\n    h = np.diag(np.dot(results.model.exog, np.dot(results.normalized_cov_params, results.model.exog.T)))\n    het_scale = (results.resid / (1 - h)) ** 2\n    cov_hc3_ = _HCCM(results, het_scale)\n    return cov_hc3_",
            "def cov_hc3(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    See statsmodels.RegressionResults\\n    '\n    h = np.diag(np.dot(results.model.exog, np.dot(results.normalized_cov_params, results.model.exog.T)))\n    het_scale = (results.resid / (1 - h)) ** 2\n    cov_hc3_ = _HCCM(results, het_scale)\n    return cov_hc3_",
            "def cov_hc3(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    See statsmodels.RegressionResults\\n    '\n    h = np.diag(np.dot(results.model.exog, np.dot(results.normalized_cov_params, results.model.exog.T)))\n    het_scale = (results.resid / (1 - h)) ** 2\n    cov_hc3_ = _HCCM(results, het_scale)\n    return cov_hc3_",
            "def cov_hc3(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    See statsmodels.RegressionResults\\n    '\n    h = np.diag(np.dot(results.model.exog, np.dot(results.normalized_cov_params, results.model.exog.T)))\n    het_scale = (results.resid / (1 - h)) ** 2\n    cov_hc3_ = _HCCM(results, het_scale)\n    return cov_hc3_",
            "def cov_hc3(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    See statsmodels.RegressionResults\\n    '\n    h = np.diag(np.dot(results.model.exog, np.dot(results.normalized_cov_params, results.model.exog.T)))\n    het_scale = (results.resid / (1 - h)) ** 2\n    cov_hc3_ = _HCCM(results, het_scale)\n    return cov_hc3_"
        ]
    },
    {
        "func_name": "_get_sandwich_arrays",
        "original": "def _get_sandwich_arrays(results, cov_type=''):\n    \"\"\"Helper function to get scores from results\n\n    Parameters\n    \"\"\"\n    if isinstance(results, tuple):\n        (jac, hessian_inv) = results\n        xu = jac = np.asarray(jac)\n        hessian_inv = np.asarray(hessian_inv)\n    elif hasattr(results, 'model'):\n        if hasattr(results, '_results'):\n            results = results._results\n        if hasattr(results.model, 'jac'):\n            xu = results.model.jac(results.params)\n            hessian_inv = np.linalg.inv(results.model.hessian(results.params))\n        elif hasattr(results.model, 'score_obs'):\n            xu = results.model.score_obs(results.params)\n            hessian_inv = np.linalg.inv(results.model.hessian(results.params))\n        else:\n            xu = results.model.wexog * results.wresid[:, None]\n            hessian_inv = np.asarray(results.normalized_cov_params)\n        if hasattr(results.model, 'freq_weights') and (not cov_type == 'clu'):\n            xu /= np.sqrt(np.asarray(results.model.freq_weights)[:, None])\n    else:\n        raise ValueError('need either tuple of (jac, hessian_inv) or results' + 'instance')\n    return (xu, hessian_inv)",
        "mutated": [
            "def _get_sandwich_arrays(results, cov_type=''):\n    if False:\n        i = 10\n    'Helper function to get scores from results\\n\\n    Parameters\\n    '\n    if isinstance(results, tuple):\n        (jac, hessian_inv) = results\n        xu = jac = np.asarray(jac)\n        hessian_inv = np.asarray(hessian_inv)\n    elif hasattr(results, 'model'):\n        if hasattr(results, '_results'):\n            results = results._results\n        if hasattr(results.model, 'jac'):\n            xu = results.model.jac(results.params)\n            hessian_inv = np.linalg.inv(results.model.hessian(results.params))\n        elif hasattr(results.model, 'score_obs'):\n            xu = results.model.score_obs(results.params)\n            hessian_inv = np.linalg.inv(results.model.hessian(results.params))\n        else:\n            xu = results.model.wexog * results.wresid[:, None]\n            hessian_inv = np.asarray(results.normalized_cov_params)\n        if hasattr(results.model, 'freq_weights') and (not cov_type == 'clu'):\n            xu /= np.sqrt(np.asarray(results.model.freq_weights)[:, None])\n    else:\n        raise ValueError('need either tuple of (jac, hessian_inv) or results' + 'instance')\n    return (xu, hessian_inv)",
            "def _get_sandwich_arrays(results, cov_type=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Helper function to get scores from results\\n\\n    Parameters\\n    '\n    if isinstance(results, tuple):\n        (jac, hessian_inv) = results\n        xu = jac = np.asarray(jac)\n        hessian_inv = np.asarray(hessian_inv)\n    elif hasattr(results, 'model'):\n        if hasattr(results, '_results'):\n            results = results._results\n        if hasattr(results.model, 'jac'):\n            xu = results.model.jac(results.params)\n            hessian_inv = np.linalg.inv(results.model.hessian(results.params))\n        elif hasattr(results.model, 'score_obs'):\n            xu = results.model.score_obs(results.params)\n            hessian_inv = np.linalg.inv(results.model.hessian(results.params))\n        else:\n            xu = results.model.wexog * results.wresid[:, None]\n            hessian_inv = np.asarray(results.normalized_cov_params)\n        if hasattr(results.model, 'freq_weights') and (not cov_type == 'clu'):\n            xu /= np.sqrt(np.asarray(results.model.freq_weights)[:, None])\n    else:\n        raise ValueError('need either tuple of (jac, hessian_inv) or results' + 'instance')\n    return (xu, hessian_inv)",
            "def _get_sandwich_arrays(results, cov_type=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Helper function to get scores from results\\n\\n    Parameters\\n    '\n    if isinstance(results, tuple):\n        (jac, hessian_inv) = results\n        xu = jac = np.asarray(jac)\n        hessian_inv = np.asarray(hessian_inv)\n    elif hasattr(results, 'model'):\n        if hasattr(results, '_results'):\n            results = results._results\n        if hasattr(results.model, 'jac'):\n            xu = results.model.jac(results.params)\n            hessian_inv = np.linalg.inv(results.model.hessian(results.params))\n        elif hasattr(results.model, 'score_obs'):\n            xu = results.model.score_obs(results.params)\n            hessian_inv = np.linalg.inv(results.model.hessian(results.params))\n        else:\n            xu = results.model.wexog * results.wresid[:, None]\n            hessian_inv = np.asarray(results.normalized_cov_params)\n        if hasattr(results.model, 'freq_weights') and (not cov_type == 'clu'):\n            xu /= np.sqrt(np.asarray(results.model.freq_weights)[:, None])\n    else:\n        raise ValueError('need either tuple of (jac, hessian_inv) or results' + 'instance')\n    return (xu, hessian_inv)",
            "def _get_sandwich_arrays(results, cov_type=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Helper function to get scores from results\\n\\n    Parameters\\n    '\n    if isinstance(results, tuple):\n        (jac, hessian_inv) = results\n        xu = jac = np.asarray(jac)\n        hessian_inv = np.asarray(hessian_inv)\n    elif hasattr(results, 'model'):\n        if hasattr(results, '_results'):\n            results = results._results\n        if hasattr(results.model, 'jac'):\n            xu = results.model.jac(results.params)\n            hessian_inv = np.linalg.inv(results.model.hessian(results.params))\n        elif hasattr(results.model, 'score_obs'):\n            xu = results.model.score_obs(results.params)\n            hessian_inv = np.linalg.inv(results.model.hessian(results.params))\n        else:\n            xu = results.model.wexog * results.wresid[:, None]\n            hessian_inv = np.asarray(results.normalized_cov_params)\n        if hasattr(results.model, 'freq_weights') and (not cov_type == 'clu'):\n            xu /= np.sqrt(np.asarray(results.model.freq_weights)[:, None])\n    else:\n        raise ValueError('need either tuple of (jac, hessian_inv) or results' + 'instance')\n    return (xu, hessian_inv)",
            "def _get_sandwich_arrays(results, cov_type=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Helper function to get scores from results\\n\\n    Parameters\\n    '\n    if isinstance(results, tuple):\n        (jac, hessian_inv) = results\n        xu = jac = np.asarray(jac)\n        hessian_inv = np.asarray(hessian_inv)\n    elif hasattr(results, 'model'):\n        if hasattr(results, '_results'):\n            results = results._results\n        if hasattr(results.model, 'jac'):\n            xu = results.model.jac(results.params)\n            hessian_inv = np.linalg.inv(results.model.hessian(results.params))\n        elif hasattr(results.model, 'score_obs'):\n            xu = results.model.score_obs(results.params)\n            hessian_inv = np.linalg.inv(results.model.hessian(results.params))\n        else:\n            xu = results.model.wexog * results.wresid[:, None]\n            hessian_inv = np.asarray(results.normalized_cov_params)\n        if hasattr(results.model, 'freq_weights') and (not cov_type == 'clu'):\n            xu /= np.sqrt(np.asarray(results.model.freq_weights)[:, None])\n    else:\n        raise ValueError('need either tuple of (jac, hessian_inv) or results' + 'instance')\n    return (xu, hessian_inv)"
        ]
    },
    {
        "func_name": "_HCCM1",
        "original": "def _HCCM1(results, scale):\n    \"\"\"\n    sandwich with pinv(x) * scale * pinv(x).T\n\n    where pinv(x) = (X'X)^(-1) X\n    and scale is (nobs, nobs), or (nobs,) with diagonal matrix diag(scale)\n\n    Parameters\n    ----------\n    results : result instance\n       need to contain regression results, uses results.model.pinv_wexog\n    scale : ndarray (nobs,) or (nobs, nobs)\n       scale matrix, treated as diagonal matrix if scale is one-dimensional\n\n    Returns\n    -------\n    H : ndarray (k_vars, k_vars)\n        robust covariance matrix for the parameter estimates\n\n    \"\"\"\n    if scale.ndim == 1:\n        H = np.dot(results.model.pinv_wexog, scale[:, None] * results.model.pinv_wexog.T)\n    else:\n        H = np.dot(results.model.pinv_wexog, np.dot(scale, results.model.pinv_wexog.T))\n    return H",
        "mutated": [
            "def _HCCM1(results, scale):\n    if False:\n        i = 10\n    \"\\n    sandwich with pinv(x) * scale * pinv(x).T\\n\\n    where pinv(x) = (X'X)^(-1) X\\n    and scale is (nobs, nobs), or (nobs,) with diagonal matrix diag(scale)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       need to contain regression results, uses results.model.pinv_wexog\\n    scale : ndarray (nobs,) or (nobs, nobs)\\n       scale matrix, treated as diagonal matrix if scale is one-dimensional\\n\\n    Returns\\n    -------\\n    H : ndarray (k_vars, k_vars)\\n        robust covariance matrix for the parameter estimates\\n\\n    \"\n    if scale.ndim == 1:\n        H = np.dot(results.model.pinv_wexog, scale[:, None] * results.model.pinv_wexog.T)\n    else:\n        H = np.dot(results.model.pinv_wexog, np.dot(scale, results.model.pinv_wexog.T))\n    return H",
            "def _HCCM1(results, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    sandwich with pinv(x) * scale * pinv(x).T\\n\\n    where pinv(x) = (X'X)^(-1) X\\n    and scale is (nobs, nobs), or (nobs,) with diagonal matrix diag(scale)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       need to contain regression results, uses results.model.pinv_wexog\\n    scale : ndarray (nobs,) or (nobs, nobs)\\n       scale matrix, treated as diagonal matrix if scale is one-dimensional\\n\\n    Returns\\n    -------\\n    H : ndarray (k_vars, k_vars)\\n        robust covariance matrix for the parameter estimates\\n\\n    \"\n    if scale.ndim == 1:\n        H = np.dot(results.model.pinv_wexog, scale[:, None] * results.model.pinv_wexog.T)\n    else:\n        H = np.dot(results.model.pinv_wexog, np.dot(scale, results.model.pinv_wexog.T))\n    return H",
            "def _HCCM1(results, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    sandwich with pinv(x) * scale * pinv(x).T\\n\\n    where pinv(x) = (X'X)^(-1) X\\n    and scale is (nobs, nobs), or (nobs,) with diagonal matrix diag(scale)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       need to contain regression results, uses results.model.pinv_wexog\\n    scale : ndarray (nobs,) or (nobs, nobs)\\n       scale matrix, treated as diagonal matrix if scale is one-dimensional\\n\\n    Returns\\n    -------\\n    H : ndarray (k_vars, k_vars)\\n        robust covariance matrix for the parameter estimates\\n\\n    \"\n    if scale.ndim == 1:\n        H = np.dot(results.model.pinv_wexog, scale[:, None] * results.model.pinv_wexog.T)\n    else:\n        H = np.dot(results.model.pinv_wexog, np.dot(scale, results.model.pinv_wexog.T))\n    return H",
            "def _HCCM1(results, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    sandwich with pinv(x) * scale * pinv(x).T\\n\\n    where pinv(x) = (X'X)^(-1) X\\n    and scale is (nobs, nobs), or (nobs,) with diagonal matrix diag(scale)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       need to contain regression results, uses results.model.pinv_wexog\\n    scale : ndarray (nobs,) or (nobs, nobs)\\n       scale matrix, treated as diagonal matrix if scale is one-dimensional\\n\\n    Returns\\n    -------\\n    H : ndarray (k_vars, k_vars)\\n        robust covariance matrix for the parameter estimates\\n\\n    \"\n    if scale.ndim == 1:\n        H = np.dot(results.model.pinv_wexog, scale[:, None] * results.model.pinv_wexog.T)\n    else:\n        H = np.dot(results.model.pinv_wexog, np.dot(scale, results.model.pinv_wexog.T))\n    return H",
            "def _HCCM1(results, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    sandwich with pinv(x) * scale * pinv(x).T\\n\\n    where pinv(x) = (X'X)^(-1) X\\n    and scale is (nobs, nobs), or (nobs,) with diagonal matrix diag(scale)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       need to contain regression results, uses results.model.pinv_wexog\\n    scale : ndarray (nobs,) or (nobs, nobs)\\n       scale matrix, treated as diagonal matrix if scale is one-dimensional\\n\\n    Returns\\n    -------\\n    H : ndarray (k_vars, k_vars)\\n        robust covariance matrix for the parameter estimates\\n\\n    \"\n    if scale.ndim == 1:\n        H = np.dot(results.model.pinv_wexog, scale[:, None] * results.model.pinv_wexog.T)\n    else:\n        H = np.dot(results.model.pinv_wexog, np.dot(scale, results.model.pinv_wexog.T))\n    return H"
        ]
    },
    {
        "func_name": "_HCCM2",
        "original": "def _HCCM2(hessian_inv, scale):\n    \"\"\"\n    sandwich with (X'X)^(-1) * scale * (X'X)^(-1)\n\n    scale is (kvars, kvars)\n    this uses results.normalized_cov_params for (X'X)^(-1)\n\n    Parameters\n    ----------\n    results : result instance\n       need to contain regression results, uses results.normalized_cov_params\n    scale : ndarray (k_vars, k_vars)\n       scale matrix\n\n    Returns\n    -------\n    H : ndarray (k_vars, k_vars)\n        robust covariance matrix for the parameter estimates\n\n    \"\"\"\n    if scale.ndim == 1:\n        scale = scale[:, None]\n    xxi = hessian_inv\n    H = np.dot(np.dot(xxi, scale), xxi.T)\n    return H",
        "mutated": [
            "def _HCCM2(hessian_inv, scale):\n    if False:\n        i = 10\n    \"\\n    sandwich with (X'X)^(-1) * scale * (X'X)^(-1)\\n\\n    scale is (kvars, kvars)\\n    this uses results.normalized_cov_params for (X'X)^(-1)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       need to contain regression results, uses results.normalized_cov_params\\n    scale : ndarray (k_vars, k_vars)\\n       scale matrix\\n\\n    Returns\\n    -------\\n    H : ndarray (k_vars, k_vars)\\n        robust covariance matrix for the parameter estimates\\n\\n    \"\n    if scale.ndim == 1:\n        scale = scale[:, None]\n    xxi = hessian_inv\n    H = np.dot(np.dot(xxi, scale), xxi.T)\n    return H",
            "def _HCCM2(hessian_inv, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    sandwich with (X'X)^(-1) * scale * (X'X)^(-1)\\n\\n    scale is (kvars, kvars)\\n    this uses results.normalized_cov_params for (X'X)^(-1)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       need to contain regression results, uses results.normalized_cov_params\\n    scale : ndarray (k_vars, k_vars)\\n       scale matrix\\n\\n    Returns\\n    -------\\n    H : ndarray (k_vars, k_vars)\\n        robust covariance matrix for the parameter estimates\\n\\n    \"\n    if scale.ndim == 1:\n        scale = scale[:, None]\n    xxi = hessian_inv\n    H = np.dot(np.dot(xxi, scale), xxi.T)\n    return H",
            "def _HCCM2(hessian_inv, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    sandwich with (X'X)^(-1) * scale * (X'X)^(-1)\\n\\n    scale is (kvars, kvars)\\n    this uses results.normalized_cov_params for (X'X)^(-1)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       need to contain regression results, uses results.normalized_cov_params\\n    scale : ndarray (k_vars, k_vars)\\n       scale matrix\\n\\n    Returns\\n    -------\\n    H : ndarray (k_vars, k_vars)\\n        robust covariance matrix for the parameter estimates\\n\\n    \"\n    if scale.ndim == 1:\n        scale = scale[:, None]\n    xxi = hessian_inv\n    H = np.dot(np.dot(xxi, scale), xxi.T)\n    return H",
            "def _HCCM2(hessian_inv, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    sandwich with (X'X)^(-1) * scale * (X'X)^(-1)\\n\\n    scale is (kvars, kvars)\\n    this uses results.normalized_cov_params for (X'X)^(-1)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       need to contain regression results, uses results.normalized_cov_params\\n    scale : ndarray (k_vars, k_vars)\\n       scale matrix\\n\\n    Returns\\n    -------\\n    H : ndarray (k_vars, k_vars)\\n        robust covariance matrix for the parameter estimates\\n\\n    \"\n    if scale.ndim == 1:\n        scale = scale[:, None]\n    xxi = hessian_inv\n    H = np.dot(np.dot(xxi, scale), xxi.T)\n    return H",
            "def _HCCM2(hessian_inv, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    sandwich with (X'X)^(-1) * scale * (X'X)^(-1)\\n\\n    scale is (kvars, kvars)\\n    this uses results.normalized_cov_params for (X'X)^(-1)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       need to contain regression results, uses results.normalized_cov_params\\n    scale : ndarray (k_vars, k_vars)\\n       scale matrix\\n\\n    Returns\\n    -------\\n    H : ndarray (k_vars, k_vars)\\n        robust covariance matrix for the parameter estimates\\n\\n    \"\n    if scale.ndim == 1:\n        scale = scale[:, None]\n    xxi = hessian_inv\n    H = np.dot(np.dot(xxi, scale), xxi.T)\n    return H"
        ]
    },
    {
        "func_name": "weights_bartlett",
        "original": "def weights_bartlett(nlags):\n    \"\"\"Bartlett weights for HAC\n\n    this will be moved to another module\n\n    Parameters\n    ----------\n    nlags : int\n       highest lag in the kernel window, this does not include the zero lag\n\n    Returns\n    -------\n    kernel : ndarray, (nlags+1,)\n        weights for Bartlett kernel\n\n    \"\"\"\n    return 1 - np.arange(nlags + 1) / (nlags + 1.0)",
        "mutated": [
            "def weights_bartlett(nlags):\n    if False:\n        i = 10\n    'Bartlett weights for HAC\\n\\n    this will be moved to another module\\n\\n    Parameters\\n    ----------\\n    nlags : int\\n       highest lag in the kernel window, this does not include the zero lag\\n\\n    Returns\\n    -------\\n    kernel : ndarray, (nlags+1,)\\n        weights for Bartlett kernel\\n\\n    '\n    return 1 - np.arange(nlags + 1) / (nlags + 1.0)",
            "def weights_bartlett(nlags):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Bartlett weights for HAC\\n\\n    this will be moved to another module\\n\\n    Parameters\\n    ----------\\n    nlags : int\\n       highest lag in the kernel window, this does not include the zero lag\\n\\n    Returns\\n    -------\\n    kernel : ndarray, (nlags+1,)\\n        weights for Bartlett kernel\\n\\n    '\n    return 1 - np.arange(nlags + 1) / (nlags + 1.0)",
            "def weights_bartlett(nlags):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Bartlett weights for HAC\\n\\n    this will be moved to another module\\n\\n    Parameters\\n    ----------\\n    nlags : int\\n       highest lag in the kernel window, this does not include the zero lag\\n\\n    Returns\\n    -------\\n    kernel : ndarray, (nlags+1,)\\n        weights for Bartlett kernel\\n\\n    '\n    return 1 - np.arange(nlags + 1) / (nlags + 1.0)",
            "def weights_bartlett(nlags):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Bartlett weights for HAC\\n\\n    this will be moved to another module\\n\\n    Parameters\\n    ----------\\n    nlags : int\\n       highest lag in the kernel window, this does not include the zero lag\\n\\n    Returns\\n    -------\\n    kernel : ndarray, (nlags+1,)\\n        weights for Bartlett kernel\\n\\n    '\n    return 1 - np.arange(nlags + 1) / (nlags + 1.0)",
            "def weights_bartlett(nlags):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Bartlett weights for HAC\\n\\n    this will be moved to another module\\n\\n    Parameters\\n    ----------\\n    nlags : int\\n       highest lag in the kernel window, this does not include the zero lag\\n\\n    Returns\\n    -------\\n    kernel : ndarray, (nlags+1,)\\n        weights for Bartlett kernel\\n\\n    '\n    return 1 - np.arange(nlags + 1) / (nlags + 1.0)"
        ]
    },
    {
        "func_name": "weights_uniform",
        "original": "def weights_uniform(nlags):\n    \"\"\"uniform weights for HAC\n\n    this will be moved to another module\n\n    Parameters\n    ----------\n    nlags : int\n       highest lag in the kernel window, this does not include the zero lag\n\n    Returns\n    -------\n    kernel : ndarray, (nlags+1,)\n        weights for uniform kernel\n\n    \"\"\"\n    return np.ones(nlags + 1)",
        "mutated": [
            "def weights_uniform(nlags):\n    if False:\n        i = 10\n    'uniform weights for HAC\\n\\n    this will be moved to another module\\n\\n    Parameters\\n    ----------\\n    nlags : int\\n       highest lag in the kernel window, this does not include the zero lag\\n\\n    Returns\\n    -------\\n    kernel : ndarray, (nlags+1,)\\n        weights for uniform kernel\\n\\n    '\n    return np.ones(nlags + 1)",
            "def weights_uniform(nlags):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'uniform weights for HAC\\n\\n    this will be moved to another module\\n\\n    Parameters\\n    ----------\\n    nlags : int\\n       highest lag in the kernel window, this does not include the zero lag\\n\\n    Returns\\n    -------\\n    kernel : ndarray, (nlags+1,)\\n        weights for uniform kernel\\n\\n    '\n    return np.ones(nlags + 1)",
            "def weights_uniform(nlags):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'uniform weights for HAC\\n\\n    this will be moved to another module\\n\\n    Parameters\\n    ----------\\n    nlags : int\\n       highest lag in the kernel window, this does not include the zero lag\\n\\n    Returns\\n    -------\\n    kernel : ndarray, (nlags+1,)\\n        weights for uniform kernel\\n\\n    '\n    return np.ones(nlags + 1)",
            "def weights_uniform(nlags):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'uniform weights for HAC\\n\\n    this will be moved to another module\\n\\n    Parameters\\n    ----------\\n    nlags : int\\n       highest lag in the kernel window, this does not include the zero lag\\n\\n    Returns\\n    -------\\n    kernel : ndarray, (nlags+1,)\\n        weights for uniform kernel\\n\\n    '\n    return np.ones(nlags + 1)",
            "def weights_uniform(nlags):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'uniform weights for HAC\\n\\n    this will be moved to another module\\n\\n    Parameters\\n    ----------\\n    nlags : int\\n       highest lag in the kernel window, this does not include the zero lag\\n\\n    Returns\\n    -------\\n    kernel : ndarray, (nlags+1,)\\n        weights for uniform kernel\\n\\n    '\n    return np.ones(nlags + 1)"
        ]
    },
    {
        "func_name": "S_hac_simple",
        "original": "def S_hac_simple(x, nlags=None, weights_func=weights_bartlett):\n    \"\"\"inner covariance matrix for HAC (Newey, West) sandwich\n\n    assumes we have a single time series with zero axis consecutive, equal\n    spaced time periods\n\n\n    Parameters\n    ----------\n    x : ndarray (nobs,) or (nobs, k_var)\n        data, for HAC this is array of x_i * u_i\n    nlags : int or None\n        highest lag to include in kernel window. If None, then\n        nlags = floor(4(T/100)^(2/9)) is used.\n    weights_func : callable\n        weights_func is called with nlags as argument to get the kernel\n        weights. default are Bartlett weights\n\n    Returns\n    -------\n    S : ndarray, (k_vars, k_vars)\n        inner covariance matrix for sandwich\n\n    Notes\n    -----\n    used by cov_hac_simple\n\n    options might change when other kernels besides Bartlett are available.\n\n    \"\"\"\n    if x.ndim == 1:\n        x = x[:, None]\n    n_periods = x.shape[0]\n    if nlags is None:\n        nlags = int(np.floor(4 * (n_periods / 100.0) ** (2.0 / 9.0)))\n    weights = weights_func(nlags)\n    S = weights[0] * np.dot(x.T, x)\n    for lag in range(1, nlags + 1):\n        s = np.dot(x[lag:].T, x[:-lag])\n        S += weights[lag] * (s + s.T)\n    return S",
        "mutated": [
            "def S_hac_simple(x, nlags=None, weights_func=weights_bartlett):\n    if False:\n        i = 10\n    'inner covariance matrix for HAC (Newey, West) sandwich\\n\\n    assumes we have a single time series with zero axis consecutive, equal\\n    spaced time periods\\n\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor(4(T/100)^(2/9)) is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    Notes\\n    -----\\n    used by cov_hac_simple\\n\\n    options might change when other kernels besides Bartlett are available.\\n\\n    '\n    if x.ndim == 1:\n        x = x[:, None]\n    n_periods = x.shape[0]\n    if nlags is None:\n        nlags = int(np.floor(4 * (n_periods / 100.0) ** (2.0 / 9.0)))\n    weights = weights_func(nlags)\n    S = weights[0] * np.dot(x.T, x)\n    for lag in range(1, nlags + 1):\n        s = np.dot(x[lag:].T, x[:-lag])\n        S += weights[lag] * (s + s.T)\n    return S",
            "def S_hac_simple(x, nlags=None, weights_func=weights_bartlett):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'inner covariance matrix for HAC (Newey, West) sandwich\\n\\n    assumes we have a single time series with zero axis consecutive, equal\\n    spaced time periods\\n\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor(4(T/100)^(2/9)) is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    Notes\\n    -----\\n    used by cov_hac_simple\\n\\n    options might change when other kernels besides Bartlett are available.\\n\\n    '\n    if x.ndim == 1:\n        x = x[:, None]\n    n_periods = x.shape[0]\n    if nlags is None:\n        nlags = int(np.floor(4 * (n_periods / 100.0) ** (2.0 / 9.0)))\n    weights = weights_func(nlags)\n    S = weights[0] * np.dot(x.T, x)\n    for lag in range(1, nlags + 1):\n        s = np.dot(x[lag:].T, x[:-lag])\n        S += weights[lag] * (s + s.T)\n    return S",
            "def S_hac_simple(x, nlags=None, weights_func=weights_bartlett):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'inner covariance matrix for HAC (Newey, West) sandwich\\n\\n    assumes we have a single time series with zero axis consecutive, equal\\n    spaced time periods\\n\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor(4(T/100)^(2/9)) is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    Notes\\n    -----\\n    used by cov_hac_simple\\n\\n    options might change when other kernels besides Bartlett are available.\\n\\n    '\n    if x.ndim == 1:\n        x = x[:, None]\n    n_periods = x.shape[0]\n    if nlags is None:\n        nlags = int(np.floor(4 * (n_periods / 100.0) ** (2.0 / 9.0)))\n    weights = weights_func(nlags)\n    S = weights[0] * np.dot(x.T, x)\n    for lag in range(1, nlags + 1):\n        s = np.dot(x[lag:].T, x[:-lag])\n        S += weights[lag] * (s + s.T)\n    return S",
            "def S_hac_simple(x, nlags=None, weights_func=weights_bartlett):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'inner covariance matrix for HAC (Newey, West) sandwich\\n\\n    assumes we have a single time series with zero axis consecutive, equal\\n    spaced time periods\\n\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor(4(T/100)^(2/9)) is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    Notes\\n    -----\\n    used by cov_hac_simple\\n\\n    options might change when other kernels besides Bartlett are available.\\n\\n    '\n    if x.ndim == 1:\n        x = x[:, None]\n    n_periods = x.shape[0]\n    if nlags is None:\n        nlags = int(np.floor(4 * (n_periods / 100.0) ** (2.0 / 9.0)))\n    weights = weights_func(nlags)\n    S = weights[0] * np.dot(x.T, x)\n    for lag in range(1, nlags + 1):\n        s = np.dot(x[lag:].T, x[:-lag])\n        S += weights[lag] * (s + s.T)\n    return S",
            "def S_hac_simple(x, nlags=None, weights_func=weights_bartlett):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'inner covariance matrix for HAC (Newey, West) sandwich\\n\\n    assumes we have a single time series with zero axis consecutive, equal\\n    spaced time periods\\n\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor(4(T/100)^(2/9)) is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    Notes\\n    -----\\n    used by cov_hac_simple\\n\\n    options might change when other kernels besides Bartlett are available.\\n\\n    '\n    if x.ndim == 1:\n        x = x[:, None]\n    n_periods = x.shape[0]\n    if nlags is None:\n        nlags = int(np.floor(4 * (n_periods / 100.0) ** (2.0 / 9.0)))\n    weights = weights_func(nlags)\n    S = weights[0] * np.dot(x.T, x)\n    for lag in range(1, nlags + 1):\n        s = np.dot(x[lag:].T, x[:-lag])\n        S += weights[lag] * (s + s.T)\n    return S"
        ]
    },
    {
        "func_name": "S_white_simple",
        "original": "def S_white_simple(x):\n    \"\"\"inner covariance matrix for White heteroscedastistity sandwich\n\n\n    Parameters\n    ----------\n    x : ndarray (nobs,) or (nobs, k_var)\n        data, for HAC this is array of x_i * u_i\n\n    Returns\n    -------\n    S : ndarray, (k_vars, k_vars)\n        inner covariance matrix for sandwich\n\n    Notes\n    -----\n    this is just dot(X.T, X)\n\n    \"\"\"\n    if x.ndim == 1:\n        x = x[:, None]\n    return np.dot(x.T, x)",
        "mutated": [
            "def S_white_simple(x):\n    if False:\n        i = 10\n    'inner covariance matrix for White heteroscedastistity sandwich\\n\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    Notes\\n    -----\\n    this is just dot(X.T, X)\\n\\n    '\n    if x.ndim == 1:\n        x = x[:, None]\n    return np.dot(x.T, x)",
            "def S_white_simple(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'inner covariance matrix for White heteroscedastistity sandwich\\n\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    Notes\\n    -----\\n    this is just dot(X.T, X)\\n\\n    '\n    if x.ndim == 1:\n        x = x[:, None]\n    return np.dot(x.T, x)",
            "def S_white_simple(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'inner covariance matrix for White heteroscedastistity sandwich\\n\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    Notes\\n    -----\\n    this is just dot(X.T, X)\\n\\n    '\n    if x.ndim == 1:\n        x = x[:, None]\n    return np.dot(x.T, x)",
            "def S_white_simple(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'inner covariance matrix for White heteroscedastistity sandwich\\n\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    Notes\\n    -----\\n    this is just dot(X.T, X)\\n\\n    '\n    if x.ndim == 1:\n        x = x[:, None]\n    return np.dot(x.T, x)",
            "def S_white_simple(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'inner covariance matrix for White heteroscedastistity sandwich\\n\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    Notes\\n    -----\\n    this is just dot(X.T, X)\\n\\n    '\n    if x.ndim == 1:\n        x = x[:, None]\n    return np.dot(x.T, x)"
        ]
    },
    {
        "func_name": "S_hac_groupsum",
        "original": "def S_hac_groupsum(x, time, nlags=None, weights_func=weights_bartlett):\n    \"\"\"inner covariance matrix for HAC over group sums sandwich\n\n    This assumes we have complete equal spaced time periods.\n    The number of time periods per group need not be the same, but we need\n    at least one observation for each time period\n\n    For a single categorical group only, or a everything else but time\n    dimension. This first aggregates x over groups for each time period, then\n    applies HAC on the sum per period.\n\n    Parameters\n    ----------\n    x : ndarray (nobs,) or (nobs, k_var)\n        data, for HAC this is array of x_i * u_i\n    time : ndarray, (nobs,)\n        timeindes, assumed to be integers range(n_periods)\n    nlags : int or None\n        highest lag to include in kernel window. If None, then\n        nlags = floor[4(T/100)^(2/9)] is used.\n    weights_func : callable\n        weights_func is called with nlags as argument to get the kernel\n        weights. default are Bartlett weights\n\n    Returns\n    -------\n    S : ndarray, (k_vars, k_vars)\n        inner covariance matrix for sandwich\n\n    References\n    ----------\n    Daniel Hoechle, xtscc paper\n    Driscoll and Kraay\n\n    \"\"\"\n    x_group_sums = group_sums(x, time).T\n    return S_hac_simple(x_group_sums, nlags=nlags, weights_func=weights_func)",
        "mutated": [
            "def S_hac_groupsum(x, time, nlags=None, weights_func=weights_bartlett):\n    if False:\n        i = 10\n    'inner covariance matrix for HAC over group sums sandwich\\n\\n    This assumes we have complete equal spaced time periods.\\n    The number of time periods per group need not be the same, but we need\\n    at least one observation for each time period\\n\\n    For a single categorical group only, or a everything else but time\\n    dimension. This first aggregates x over groups for each time period, then\\n    applies HAC on the sum per period.\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n    time : ndarray, (nobs,)\\n        timeindes, assumed to be integers range(n_periods)\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor[4(T/100)^(2/9)] is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    References\\n    ----------\\n    Daniel Hoechle, xtscc paper\\n    Driscoll and Kraay\\n\\n    '\n    x_group_sums = group_sums(x, time).T\n    return S_hac_simple(x_group_sums, nlags=nlags, weights_func=weights_func)",
            "def S_hac_groupsum(x, time, nlags=None, weights_func=weights_bartlett):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'inner covariance matrix for HAC over group sums sandwich\\n\\n    This assumes we have complete equal spaced time periods.\\n    The number of time periods per group need not be the same, but we need\\n    at least one observation for each time period\\n\\n    For a single categorical group only, or a everything else but time\\n    dimension. This first aggregates x over groups for each time period, then\\n    applies HAC on the sum per period.\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n    time : ndarray, (nobs,)\\n        timeindes, assumed to be integers range(n_periods)\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor[4(T/100)^(2/9)] is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    References\\n    ----------\\n    Daniel Hoechle, xtscc paper\\n    Driscoll and Kraay\\n\\n    '\n    x_group_sums = group_sums(x, time).T\n    return S_hac_simple(x_group_sums, nlags=nlags, weights_func=weights_func)",
            "def S_hac_groupsum(x, time, nlags=None, weights_func=weights_bartlett):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'inner covariance matrix for HAC over group sums sandwich\\n\\n    This assumes we have complete equal spaced time periods.\\n    The number of time periods per group need not be the same, but we need\\n    at least one observation for each time period\\n\\n    For a single categorical group only, or a everything else but time\\n    dimension. This first aggregates x over groups for each time period, then\\n    applies HAC on the sum per period.\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n    time : ndarray, (nobs,)\\n        timeindes, assumed to be integers range(n_periods)\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor[4(T/100)^(2/9)] is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    References\\n    ----------\\n    Daniel Hoechle, xtscc paper\\n    Driscoll and Kraay\\n\\n    '\n    x_group_sums = group_sums(x, time).T\n    return S_hac_simple(x_group_sums, nlags=nlags, weights_func=weights_func)",
            "def S_hac_groupsum(x, time, nlags=None, weights_func=weights_bartlett):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'inner covariance matrix for HAC over group sums sandwich\\n\\n    This assumes we have complete equal spaced time periods.\\n    The number of time periods per group need not be the same, but we need\\n    at least one observation for each time period\\n\\n    For a single categorical group only, or a everything else but time\\n    dimension. This first aggregates x over groups for each time period, then\\n    applies HAC on the sum per period.\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n    time : ndarray, (nobs,)\\n        timeindes, assumed to be integers range(n_periods)\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor[4(T/100)^(2/9)] is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    References\\n    ----------\\n    Daniel Hoechle, xtscc paper\\n    Driscoll and Kraay\\n\\n    '\n    x_group_sums = group_sums(x, time).T\n    return S_hac_simple(x_group_sums, nlags=nlags, weights_func=weights_func)",
            "def S_hac_groupsum(x, time, nlags=None, weights_func=weights_bartlett):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'inner covariance matrix for HAC over group sums sandwich\\n\\n    This assumes we have complete equal spaced time periods.\\n    The number of time periods per group need not be the same, but we need\\n    at least one observation for each time period\\n\\n    For a single categorical group only, or a everything else but time\\n    dimension. This first aggregates x over groups for each time period, then\\n    applies HAC on the sum per period.\\n\\n    Parameters\\n    ----------\\n    x : ndarray (nobs,) or (nobs, k_var)\\n        data, for HAC this is array of x_i * u_i\\n    time : ndarray, (nobs,)\\n        timeindes, assumed to be integers range(n_periods)\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor[4(T/100)^(2/9)] is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    S : ndarray, (k_vars, k_vars)\\n        inner covariance matrix for sandwich\\n\\n    References\\n    ----------\\n    Daniel Hoechle, xtscc paper\\n    Driscoll and Kraay\\n\\n    '\n    x_group_sums = group_sums(x, time).T\n    return S_hac_simple(x_group_sums, nlags=nlags, weights_func=weights_func)"
        ]
    },
    {
        "func_name": "S_crosssection",
        "original": "def S_crosssection(x, group):\n    \"\"\"inner covariance matrix for White on group sums sandwich\n\n    I guess for a single categorical group only,\n    categorical group, can also be the product/intersection of groups\n\n    This is used by cov_cluster and indirectly verified\n\n    \"\"\"\n    x_group_sums = group_sums(x, group).T\n    return S_white_simple(x_group_sums)",
        "mutated": [
            "def S_crosssection(x, group):\n    if False:\n        i = 10\n    'inner covariance matrix for White on group sums sandwich\\n\\n    I guess for a single categorical group only,\\n    categorical group, can also be the product/intersection of groups\\n\\n    This is used by cov_cluster and indirectly verified\\n\\n    '\n    x_group_sums = group_sums(x, group).T\n    return S_white_simple(x_group_sums)",
            "def S_crosssection(x, group):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'inner covariance matrix for White on group sums sandwich\\n\\n    I guess for a single categorical group only,\\n    categorical group, can also be the product/intersection of groups\\n\\n    This is used by cov_cluster and indirectly verified\\n\\n    '\n    x_group_sums = group_sums(x, group).T\n    return S_white_simple(x_group_sums)",
            "def S_crosssection(x, group):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'inner covariance matrix for White on group sums sandwich\\n\\n    I guess for a single categorical group only,\\n    categorical group, can also be the product/intersection of groups\\n\\n    This is used by cov_cluster and indirectly verified\\n\\n    '\n    x_group_sums = group_sums(x, group).T\n    return S_white_simple(x_group_sums)",
            "def S_crosssection(x, group):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'inner covariance matrix for White on group sums sandwich\\n\\n    I guess for a single categorical group only,\\n    categorical group, can also be the product/intersection of groups\\n\\n    This is used by cov_cluster and indirectly verified\\n\\n    '\n    x_group_sums = group_sums(x, group).T\n    return S_white_simple(x_group_sums)",
            "def S_crosssection(x, group):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'inner covariance matrix for White on group sums sandwich\\n\\n    I guess for a single categorical group only,\\n    categorical group, can also be the product/intersection of groups\\n\\n    This is used by cov_cluster and indirectly verified\\n\\n    '\n    x_group_sums = group_sums(x, group).T\n    return S_white_simple(x_group_sums)"
        ]
    },
    {
        "func_name": "cov_crosssection_0",
        "original": "def cov_crosssection_0(results, group):\n    \"\"\"this one is still wrong, use cov_cluster instead\"\"\"\n    scale = S_crosssection(results.resid[:, None], group)\n    scale = np.squeeze(scale)\n    cov = _HCCM1(results, scale)\n    return cov",
        "mutated": [
            "def cov_crosssection_0(results, group):\n    if False:\n        i = 10\n    'this one is still wrong, use cov_cluster instead'\n    scale = S_crosssection(results.resid[:, None], group)\n    scale = np.squeeze(scale)\n    cov = _HCCM1(results, scale)\n    return cov",
            "def cov_crosssection_0(results, group):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'this one is still wrong, use cov_cluster instead'\n    scale = S_crosssection(results.resid[:, None], group)\n    scale = np.squeeze(scale)\n    cov = _HCCM1(results, scale)\n    return cov",
            "def cov_crosssection_0(results, group):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'this one is still wrong, use cov_cluster instead'\n    scale = S_crosssection(results.resid[:, None], group)\n    scale = np.squeeze(scale)\n    cov = _HCCM1(results, scale)\n    return cov",
            "def cov_crosssection_0(results, group):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'this one is still wrong, use cov_cluster instead'\n    scale = S_crosssection(results.resid[:, None], group)\n    scale = np.squeeze(scale)\n    cov = _HCCM1(results, scale)\n    return cov",
            "def cov_crosssection_0(results, group):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'this one is still wrong, use cov_cluster instead'\n    scale = S_crosssection(results.resid[:, None], group)\n    scale = np.squeeze(scale)\n    cov = _HCCM1(results, scale)\n    return cov"
        ]
    },
    {
        "func_name": "cov_cluster",
        "original": "def cov_cluster(results, group, use_correction=True):\n    \"\"\"cluster robust covariance matrix\n\n    Calculates sandwich covariance matrix for a single cluster, i.e. grouped\n    variables.\n\n    Parameters\n    ----------\n    results : result instance\n       result of a regression, uses results.model.exog and results.resid\n       TODO: this should use wexog instead\n    use_correction : bool\n       If true (default), then the small sample correction factor is used.\n\n    Returns\n    -------\n    cov : ndarray, (k_vars, k_vars)\n        cluster robust covariance matrix for parameter estimates\n\n    Notes\n    -----\n    same result as Stata in UCLA example and same as Peterson\n\n    \"\"\"\n    (xu, hessian_inv) = _get_sandwich_arrays(results, cov_type='clu')\n    if not hasattr(group, 'dtype') or group.dtype != np.dtype('int'):\n        (clusters, group) = np.unique(group, return_inverse=True)\n    else:\n        clusters = np.unique(group)\n    scale = S_crosssection(xu, group)\n    (nobs, k_params) = xu.shape\n    n_groups = len(clusters)\n    cov_c = _HCCM2(hessian_inv, scale)\n    if use_correction:\n        cov_c *= n_groups / (n_groups - 1.0) * ((nobs - 1.0) / float(nobs - k_params))\n    return cov_c",
        "mutated": [
            "def cov_cluster(results, group, use_correction=True):\n    if False:\n        i = 10\n    'cluster robust covariance matrix\\n\\n    Calculates sandwich covariance matrix for a single cluster, i.e. grouped\\n    variables.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    use_correction : bool\\n       If true (default), then the small sample correction factor is used.\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    same result as Stata in UCLA example and same as Peterson\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results, cov_type='clu')\n    if not hasattr(group, 'dtype') or group.dtype != np.dtype('int'):\n        (clusters, group) = np.unique(group, return_inverse=True)\n    else:\n        clusters = np.unique(group)\n    scale = S_crosssection(xu, group)\n    (nobs, k_params) = xu.shape\n    n_groups = len(clusters)\n    cov_c = _HCCM2(hessian_inv, scale)\n    if use_correction:\n        cov_c *= n_groups / (n_groups - 1.0) * ((nobs - 1.0) / float(nobs - k_params))\n    return cov_c",
            "def cov_cluster(results, group, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'cluster robust covariance matrix\\n\\n    Calculates sandwich covariance matrix for a single cluster, i.e. grouped\\n    variables.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    use_correction : bool\\n       If true (default), then the small sample correction factor is used.\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    same result as Stata in UCLA example and same as Peterson\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results, cov_type='clu')\n    if not hasattr(group, 'dtype') or group.dtype != np.dtype('int'):\n        (clusters, group) = np.unique(group, return_inverse=True)\n    else:\n        clusters = np.unique(group)\n    scale = S_crosssection(xu, group)\n    (nobs, k_params) = xu.shape\n    n_groups = len(clusters)\n    cov_c = _HCCM2(hessian_inv, scale)\n    if use_correction:\n        cov_c *= n_groups / (n_groups - 1.0) * ((nobs - 1.0) / float(nobs - k_params))\n    return cov_c",
            "def cov_cluster(results, group, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'cluster robust covariance matrix\\n\\n    Calculates sandwich covariance matrix for a single cluster, i.e. grouped\\n    variables.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    use_correction : bool\\n       If true (default), then the small sample correction factor is used.\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    same result as Stata in UCLA example and same as Peterson\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results, cov_type='clu')\n    if not hasattr(group, 'dtype') or group.dtype != np.dtype('int'):\n        (clusters, group) = np.unique(group, return_inverse=True)\n    else:\n        clusters = np.unique(group)\n    scale = S_crosssection(xu, group)\n    (nobs, k_params) = xu.shape\n    n_groups = len(clusters)\n    cov_c = _HCCM2(hessian_inv, scale)\n    if use_correction:\n        cov_c *= n_groups / (n_groups - 1.0) * ((nobs - 1.0) / float(nobs - k_params))\n    return cov_c",
            "def cov_cluster(results, group, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'cluster robust covariance matrix\\n\\n    Calculates sandwich covariance matrix for a single cluster, i.e. grouped\\n    variables.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    use_correction : bool\\n       If true (default), then the small sample correction factor is used.\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    same result as Stata in UCLA example and same as Peterson\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results, cov_type='clu')\n    if not hasattr(group, 'dtype') or group.dtype != np.dtype('int'):\n        (clusters, group) = np.unique(group, return_inverse=True)\n    else:\n        clusters = np.unique(group)\n    scale = S_crosssection(xu, group)\n    (nobs, k_params) = xu.shape\n    n_groups = len(clusters)\n    cov_c = _HCCM2(hessian_inv, scale)\n    if use_correction:\n        cov_c *= n_groups / (n_groups - 1.0) * ((nobs - 1.0) / float(nobs - k_params))\n    return cov_c",
            "def cov_cluster(results, group, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'cluster robust covariance matrix\\n\\n    Calculates sandwich covariance matrix for a single cluster, i.e. grouped\\n    variables.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    use_correction : bool\\n       If true (default), then the small sample correction factor is used.\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    same result as Stata in UCLA example and same as Peterson\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results, cov_type='clu')\n    if not hasattr(group, 'dtype') or group.dtype != np.dtype('int'):\n        (clusters, group) = np.unique(group, return_inverse=True)\n    else:\n        clusters = np.unique(group)\n    scale = S_crosssection(xu, group)\n    (nobs, k_params) = xu.shape\n    n_groups = len(clusters)\n    cov_c = _HCCM2(hessian_inv, scale)\n    if use_correction:\n        cov_c *= n_groups / (n_groups - 1.0) * ((nobs - 1.0) / float(nobs - k_params))\n    return cov_c"
        ]
    },
    {
        "func_name": "cov_cluster_2groups",
        "original": "def cov_cluster_2groups(results, group, group2=None, use_correction=True):\n    \"\"\"cluster robust covariance matrix for two groups/clusters\n\n    Parameters\n    ----------\n    results : result instance\n       result of a regression, uses results.model.exog and results.resid\n       TODO: this should use wexog instead\n    use_correction : bool\n       If true (default), then the small sample correction factor is used.\n\n    Returns\n    -------\n    cov_both : ndarray, (k_vars, k_vars)\n        cluster robust covariance matrix for parameter estimates, for both\n        clusters\n    cov_0 : ndarray, (k_vars, k_vars)\n        cluster robust covariance matrix for parameter estimates for first\n        cluster\n    cov_1 : ndarray, (k_vars, k_vars)\n        cluster robust covariance matrix for parameter estimates for second\n        cluster\n\n    Notes\n    -----\n\n    verified against Peterson's table, (4 decimal print precision)\n    \"\"\"\n    if group2 is None:\n        if group.ndim != 2 or group.shape[1] != 2:\n            raise ValueError('if group2 is not given, then groups needs to be ' + 'an array with two columns')\n        group0 = group[:, 0]\n        group1 = group[:, 1]\n    else:\n        group0 = group\n        group1 = group2\n        group = (group0, group1)\n    cov0 = cov_cluster(results, group0, use_correction=use_correction)\n    cov1 = cov_cluster(results, group1, use_correction=use_correction)\n    cov01 = cov_cluster(results, combine_indices(group)[0], use_correction=use_correction)\n    cov_both = cov0 + cov1 - cov01\n    return (cov_both, cov0, cov1)",
        "mutated": [
            "def cov_cluster_2groups(results, group, group2=None, use_correction=True):\n    if False:\n        i = 10\n    \"cluster robust covariance matrix for two groups/clusters\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    use_correction : bool\\n       If true (default), then the small sample correction factor is used.\\n\\n    Returns\\n    -------\\n    cov_both : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates, for both\\n        clusters\\n    cov_0 : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates for first\\n        cluster\\n    cov_1 : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates for second\\n        cluster\\n\\n    Notes\\n    -----\\n\\n    verified against Peterson's table, (4 decimal print precision)\\n    \"\n    if group2 is None:\n        if group.ndim != 2 or group.shape[1] != 2:\n            raise ValueError('if group2 is not given, then groups needs to be ' + 'an array with two columns')\n        group0 = group[:, 0]\n        group1 = group[:, 1]\n    else:\n        group0 = group\n        group1 = group2\n        group = (group0, group1)\n    cov0 = cov_cluster(results, group0, use_correction=use_correction)\n    cov1 = cov_cluster(results, group1, use_correction=use_correction)\n    cov01 = cov_cluster(results, combine_indices(group)[0], use_correction=use_correction)\n    cov_both = cov0 + cov1 - cov01\n    return (cov_both, cov0, cov1)",
            "def cov_cluster_2groups(results, group, group2=None, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"cluster robust covariance matrix for two groups/clusters\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    use_correction : bool\\n       If true (default), then the small sample correction factor is used.\\n\\n    Returns\\n    -------\\n    cov_both : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates, for both\\n        clusters\\n    cov_0 : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates for first\\n        cluster\\n    cov_1 : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates for second\\n        cluster\\n\\n    Notes\\n    -----\\n\\n    verified against Peterson's table, (4 decimal print precision)\\n    \"\n    if group2 is None:\n        if group.ndim != 2 or group.shape[1] != 2:\n            raise ValueError('if group2 is not given, then groups needs to be ' + 'an array with two columns')\n        group0 = group[:, 0]\n        group1 = group[:, 1]\n    else:\n        group0 = group\n        group1 = group2\n        group = (group0, group1)\n    cov0 = cov_cluster(results, group0, use_correction=use_correction)\n    cov1 = cov_cluster(results, group1, use_correction=use_correction)\n    cov01 = cov_cluster(results, combine_indices(group)[0], use_correction=use_correction)\n    cov_both = cov0 + cov1 - cov01\n    return (cov_both, cov0, cov1)",
            "def cov_cluster_2groups(results, group, group2=None, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"cluster robust covariance matrix for two groups/clusters\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    use_correction : bool\\n       If true (default), then the small sample correction factor is used.\\n\\n    Returns\\n    -------\\n    cov_both : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates, for both\\n        clusters\\n    cov_0 : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates for first\\n        cluster\\n    cov_1 : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates for second\\n        cluster\\n\\n    Notes\\n    -----\\n\\n    verified against Peterson's table, (4 decimal print precision)\\n    \"\n    if group2 is None:\n        if group.ndim != 2 or group.shape[1] != 2:\n            raise ValueError('if group2 is not given, then groups needs to be ' + 'an array with two columns')\n        group0 = group[:, 0]\n        group1 = group[:, 1]\n    else:\n        group0 = group\n        group1 = group2\n        group = (group0, group1)\n    cov0 = cov_cluster(results, group0, use_correction=use_correction)\n    cov1 = cov_cluster(results, group1, use_correction=use_correction)\n    cov01 = cov_cluster(results, combine_indices(group)[0], use_correction=use_correction)\n    cov_both = cov0 + cov1 - cov01\n    return (cov_both, cov0, cov1)",
            "def cov_cluster_2groups(results, group, group2=None, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"cluster robust covariance matrix for two groups/clusters\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    use_correction : bool\\n       If true (default), then the small sample correction factor is used.\\n\\n    Returns\\n    -------\\n    cov_both : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates, for both\\n        clusters\\n    cov_0 : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates for first\\n        cluster\\n    cov_1 : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates for second\\n        cluster\\n\\n    Notes\\n    -----\\n\\n    verified against Peterson's table, (4 decimal print precision)\\n    \"\n    if group2 is None:\n        if group.ndim != 2 or group.shape[1] != 2:\n            raise ValueError('if group2 is not given, then groups needs to be ' + 'an array with two columns')\n        group0 = group[:, 0]\n        group1 = group[:, 1]\n    else:\n        group0 = group\n        group1 = group2\n        group = (group0, group1)\n    cov0 = cov_cluster(results, group0, use_correction=use_correction)\n    cov1 = cov_cluster(results, group1, use_correction=use_correction)\n    cov01 = cov_cluster(results, combine_indices(group)[0], use_correction=use_correction)\n    cov_both = cov0 + cov1 - cov01\n    return (cov_both, cov0, cov1)",
            "def cov_cluster_2groups(results, group, group2=None, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"cluster robust covariance matrix for two groups/clusters\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    use_correction : bool\\n       If true (default), then the small sample correction factor is used.\\n\\n    Returns\\n    -------\\n    cov_both : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates, for both\\n        clusters\\n    cov_0 : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates for first\\n        cluster\\n    cov_1 : ndarray, (k_vars, k_vars)\\n        cluster robust covariance matrix for parameter estimates for second\\n        cluster\\n\\n    Notes\\n    -----\\n\\n    verified against Peterson's table, (4 decimal print precision)\\n    \"\n    if group2 is None:\n        if group.ndim != 2 or group.shape[1] != 2:\n            raise ValueError('if group2 is not given, then groups needs to be ' + 'an array with two columns')\n        group0 = group[:, 0]\n        group1 = group[:, 1]\n    else:\n        group0 = group\n        group1 = group2\n        group = (group0, group1)\n    cov0 = cov_cluster(results, group0, use_correction=use_correction)\n    cov1 = cov_cluster(results, group1, use_correction=use_correction)\n    cov01 = cov_cluster(results, combine_indices(group)[0], use_correction=use_correction)\n    cov_both = cov0 + cov1 - cov01\n    return (cov_both, cov0, cov1)"
        ]
    },
    {
        "func_name": "cov_white_simple",
        "original": "def cov_white_simple(results, use_correction=True):\n    \"\"\"\n    heteroscedasticity robust covariance matrix (White)\n\n    Parameters\n    ----------\n    results : result instance\n       result of a regression, uses results.model.exog and results.resid\n       TODO: this should use wexog instead\n\n    Returns\n    -------\n    cov : ndarray, (k_vars, k_vars)\n        heteroscedasticity robust covariance matrix for parameter estimates\n\n    Notes\n    -----\n    This produces the same result as cov_hc0, and does not include any small\n    sample correction.\n\n    verified (against LinearRegressionResults and Peterson)\n\n    See Also\n    --------\n    cov_hc1, cov_hc2, cov_hc3 : heteroscedasticity robust covariance matrices\n        with small sample corrections\n\n    \"\"\"\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    sigma = S_white_simple(xu)\n    cov_w = _HCCM2(hessian_inv, sigma)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        cov_w *= nobs / float(nobs - k_params)\n    return cov_w",
        "mutated": [
            "def cov_white_simple(results, use_correction=True):\n    if False:\n        i = 10\n    '\\n    heteroscedasticity robust covariance matrix (White)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        heteroscedasticity robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    This produces the same result as cov_hc0, and does not include any small\\n    sample correction.\\n\\n    verified (against LinearRegressionResults and Peterson)\\n\\n    See Also\\n    --------\\n    cov_hc1, cov_hc2, cov_hc3 : heteroscedasticity robust covariance matrices\\n        with small sample corrections\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    sigma = S_white_simple(xu)\n    cov_w = _HCCM2(hessian_inv, sigma)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        cov_w *= nobs / float(nobs - k_params)\n    return cov_w",
            "def cov_white_simple(results, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    heteroscedasticity robust covariance matrix (White)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        heteroscedasticity robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    This produces the same result as cov_hc0, and does not include any small\\n    sample correction.\\n\\n    verified (against LinearRegressionResults and Peterson)\\n\\n    See Also\\n    --------\\n    cov_hc1, cov_hc2, cov_hc3 : heteroscedasticity robust covariance matrices\\n        with small sample corrections\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    sigma = S_white_simple(xu)\n    cov_w = _HCCM2(hessian_inv, sigma)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        cov_w *= nobs / float(nobs - k_params)\n    return cov_w",
            "def cov_white_simple(results, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    heteroscedasticity robust covariance matrix (White)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        heteroscedasticity robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    This produces the same result as cov_hc0, and does not include any small\\n    sample correction.\\n\\n    verified (against LinearRegressionResults and Peterson)\\n\\n    See Also\\n    --------\\n    cov_hc1, cov_hc2, cov_hc3 : heteroscedasticity robust covariance matrices\\n        with small sample corrections\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    sigma = S_white_simple(xu)\n    cov_w = _HCCM2(hessian_inv, sigma)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        cov_w *= nobs / float(nobs - k_params)\n    return cov_w",
            "def cov_white_simple(results, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    heteroscedasticity robust covariance matrix (White)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        heteroscedasticity robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    This produces the same result as cov_hc0, and does not include any small\\n    sample correction.\\n\\n    verified (against LinearRegressionResults and Peterson)\\n\\n    See Also\\n    --------\\n    cov_hc1, cov_hc2, cov_hc3 : heteroscedasticity robust covariance matrices\\n        with small sample corrections\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    sigma = S_white_simple(xu)\n    cov_w = _HCCM2(hessian_inv, sigma)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        cov_w *= nobs / float(nobs - k_params)\n    return cov_w",
            "def cov_white_simple(results, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    heteroscedasticity robust covariance matrix (White)\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        heteroscedasticity robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    This produces the same result as cov_hc0, and does not include any small\\n    sample correction.\\n\\n    verified (against LinearRegressionResults and Peterson)\\n\\n    See Also\\n    --------\\n    cov_hc1, cov_hc2, cov_hc3 : heteroscedasticity robust covariance matrices\\n        with small sample corrections\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    sigma = S_white_simple(xu)\n    cov_w = _HCCM2(hessian_inv, sigma)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        cov_w *= nobs / float(nobs - k_params)\n    return cov_w"
        ]
    },
    {
        "func_name": "cov_hac_simple",
        "original": "def cov_hac_simple(results, nlags=None, weights_func=weights_bartlett, use_correction=True):\n    \"\"\"\n    heteroscedasticity and autocorrelation robust covariance matrix (Newey-West)\n\n    Assumes we have a single time series with zero axis consecutive, equal\n    spaced time periods\n\n\n    Parameters\n    ----------\n    results : result instance\n       result of a regression, uses results.model.exog and results.resid\n       TODO: this should use wexog instead\n    nlags : int or None\n        highest lag to include in kernel window. If None, then\n        nlags = floor[4(T/100)^(2/9)] is used.\n    weights_func : callable\n        weights_func is called with nlags as argument to get the kernel\n        weights. default are Bartlett weights\n\n    Returns\n    -------\n    cov : ndarray, (k_vars, k_vars)\n        HAC robust covariance matrix for parameter estimates\n\n    Notes\n    -----\n    verified only for nlags=0, which is just White\n    just guessing on correction factor, need reference\n\n    options might change when other kernels besides Bartlett are available.\n\n    \"\"\"\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    sigma = S_hac_simple(xu, nlags=nlags, weights_func=weights_func)\n    cov_hac = _HCCM2(hessian_inv, sigma)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        cov_hac *= nobs / float(nobs - k_params)\n    return cov_hac",
        "mutated": [
            "def cov_hac_simple(results, nlags=None, weights_func=weights_bartlett, use_correction=True):\n    if False:\n        i = 10\n    '\\n    heteroscedasticity and autocorrelation robust covariance matrix (Newey-West)\\n\\n    Assumes we have a single time series with zero axis consecutive, equal\\n    spaced time periods\\n\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor[4(T/100)^(2/9)] is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    verified only for nlags=0, which is just White\\n    just guessing on correction factor, need reference\\n\\n    options might change when other kernels besides Bartlett are available.\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    sigma = S_hac_simple(xu, nlags=nlags, weights_func=weights_func)\n    cov_hac = _HCCM2(hessian_inv, sigma)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        cov_hac *= nobs / float(nobs - k_params)\n    return cov_hac",
            "def cov_hac_simple(results, nlags=None, weights_func=weights_bartlett, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    heteroscedasticity and autocorrelation robust covariance matrix (Newey-West)\\n\\n    Assumes we have a single time series with zero axis consecutive, equal\\n    spaced time periods\\n\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor[4(T/100)^(2/9)] is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    verified only for nlags=0, which is just White\\n    just guessing on correction factor, need reference\\n\\n    options might change when other kernels besides Bartlett are available.\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    sigma = S_hac_simple(xu, nlags=nlags, weights_func=weights_func)\n    cov_hac = _HCCM2(hessian_inv, sigma)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        cov_hac *= nobs / float(nobs - k_params)\n    return cov_hac",
            "def cov_hac_simple(results, nlags=None, weights_func=weights_bartlett, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    heteroscedasticity and autocorrelation robust covariance matrix (Newey-West)\\n\\n    Assumes we have a single time series with zero axis consecutive, equal\\n    spaced time periods\\n\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor[4(T/100)^(2/9)] is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    verified only for nlags=0, which is just White\\n    just guessing on correction factor, need reference\\n\\n    options might change when other kernels besides Bartlett are available.\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    sigma = S_hac_simple(xu, nlags=nlags, weights_func=weights_func)\n    cov_hac = _HCCM2(hessian_inv, sigma)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        cov_hac *= nobs / float(nobs - k_params)\n    return cov_hac",
            "def cov_hac_simple(results, nlags=None, weights_func=weights_bartlett, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    heteroscedasticity and autocorrelation robust covariance matrix (Newey-West)\\n\\n    Assumes we have a single time series with zero axis consecutive, equal\\n    spaced time periods\\n\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor[4(T/100)^(2/9)] is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    verified only for nlags=0, which is just White\\n    just guessing on correction factor, need reference\\n\\n    options might change when other kernels besides Bartlett are available.\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    sigma = S_hac_simple(xu, nlags=nlags, weights_func=weights_func)\n    cov_hac = _HCCM2(hessian_inv, sigma)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        cov_hac *= nobs / float(nobs - k_params)\n    return cov_hac",
            "def cov_hac_simple(results, nlags=None, weights_func=weights_bartlett, use_correction=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    heteroscedasticity and autocorrelation robust covariance matrix (Newey-West)\\n\\n    Assumes we have a single time series with zero axis consecutive, equal\\n    spaced time periods\\n\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        highest lag to include in kernel window. If None, then\\n        nlags = floor[4(T/100)^(2/9)] is used.\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    verified only for nlags=0, which is just White\\n    just guessing on correction factor, need reference\\n\\n    options might change when other kernels besides Bartlett are available.\\n\\n    '\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    sigma = S_hac_simple(xu, nlags=nlags, weights_func=weights_func)\n    cov_hac = _HCCM2(hessian_inv, sigma)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        cov_hac *= nobs / float(nobs - k_params)\n    return cov_hac"
        ]
    },
    {
        "func_name": "lagged_groups",
        "original": "def lagged_groups(x, lag, groupidx):\n    \"\"\"\n    assumes sorted by time, groupidx is tuple of start and end values\n    not optimized, just to get a working version, loop over groups\n    \"\"\"\n    out0 = []\n    out_lagged = []\n    for (l, u) in groupidx:\n        if l + lag < u:\n            out0.append(x[l + lag:u])\n            out_lagged.append(x[l:u - lag])\n    if out0 == []:\n        raise ValueError('all groups are empty taking lags')\n    return (np.vstack(out0), np.vstack(out_lagged))",
        "mutated": [
            "def lagged_groups(x, lag, groupidx):\n    if False:\n        i = 10\n    '\\n    assumes sorted by time, groupidx is tuple of start and end values\\n    not optimized, just to get a working version, loop over groups\\n    '\n    out0 = []\n    out_lagged = []\n    for (l, u) in groupidx:\n        if l + lag < u:\n            out0.append(x[l + lag:u])\n            out_lagged.append(x[l:u - lag])\n    if out0 == []:\n        raise ValueError('all groups are empty taking lags')\n    return (np.vstack(out0), np.vstack(out_lagged))",
            "def lagged_groups(x, lag, groupidx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    assumes sorted by time, groupidx is tuple of start and end values\\n    not optimized, just to get a working version, loop over groups\\n    '\n    out0 = []\n    out_lagged = []\n    for (l, u) in groupidx:\n        if l + lag < u:\n            out0.append(x[l + lag:u])\n            out_lagged.append(x[l:u - lag])\n    if out0 == []:\n        raise ValueError('all groups are empty taking lags')\n    return (np.vstack(out0), np.vstack(out_lagged))",
            "def lagged_groups(x, lag, groupidx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    assumes sorted by time, groupidx is tuple of start and end values\\n    not optimized, just to get a working version, loop over groups\\n    '\n    out0 = []\n    out_lagged = []\n    for (l, u) in groupidx:\n        if l + lag < u:\n            out0.append(x[l + lag:u])\n            out_lagged.append(x[l:u - lag])\n    if out0 == []:\n        raise ValueError('all groups are empty taking lags')\n    return (np.vstack(out0), np.vstack(out_lagged))",
            "def lagged_groups(x, lag, groupidx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    assumes sorted by time, groupidx is tuple of start and end values\\n    not optimized, just to get a working version, loop over groups\\n    '\n    out0 = []\n    out_lagged = []\n    for (l, u) in groupidx:\n        if l + lag < u:\n            out0.append(x[l + lag:u])\n            out_lagged.append(x[l:u - lag])\n    if out0 == []:\n        raise ValueError('all groups are empty taking lags')\n    return (np.vstack(out0), np.vstack(out_lagged))",
            "def lagged_groups(x, lag, groupidx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    assumes sorted by time, groupidx is tuple of start and end values\\n    not optimized, just to get a working version, loop over groups\\n    '\n    out0 = []\n    out_lagged = []\n    for (l, u) in groupidx:\n        if l + lag < u:\n            out0.append(x[l + lag:u])\n            out_lagged.append(x[l:u - lag])\n    if out0 == []:\n        raise ValueError('all groups are empty taking lags')\n    return (np.vstack(out0), np.vstack(out_lagged))"
        ]
    },
    {
        "func_name": "S_nw_panel",
        "original": "def S_nw_panel(xw, weights, groupidx):\n    \"\"\"inner covariance matrix for HAC for panel data\n\n    no denominator nobs used\n\n    no reference for this, just accounting for time indices\n    \"\"\"\n    nlags = len(weights) - 1\n    S = weights[0] * np.dot(xw.T, xw)\n    for lag in range(1, nlags + 1):\n        (xw0, xwlag) = lagged_groups(xw, lag, groupidx)\n        s = np.dot(xw0.T, xwlag)\n        S += weights[lag] * (s + s.T)\n    return S",
        "mutated": [
            "def S_nw_panel(xw, weights, groupidx):\n    if False:\n        i = 10\n    'inner covariance matrix for HAC for panel data\\n\\n    no denominator nobs used\\n\\n    no reference for this, just accounting for time indices\\n    '\n    nlags = len(weights) - 1\n    S = weights[0] * np.dot(xw.T, xw)\n    for lag in range(1, nlags + 1):\n        (xw0, xwlag) = lagged_groups(xw, lag, groupidx)\n        s = np.dot(xw0.T, xwlag)\n        S += weights[lag] * (s + s.T)\n    return S",
            "def S_nw_panel(xw, weights, groupidx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'inner covariance matrix for HAC for panel data\\n\\n    no denominator nobs used\\n\\n    no reference for this, just accounting for time indices\\n    '\n    nlags = len(weights) - 1\n    S = weights[0] * np.dot(xw.T, xw)\n    for lag in range(1, nlags + 1):\n        (xw0, xwlag) = lagged_groups(xw, lag, groupidx)\n        s = np.dot(xw0.T, xwlag)\n        S += weights[lag] * (s + s.T)\n    return S",
            "def S_nw_panel(xw, weights, groupidx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'inner covariance matrix for HAC for panel data\\n\\n    no denominator nobs used\\n\\n    no reference for this, just accounting for time indices\\n    '\n    nlags = len(weights) - 1\n    S = weights[0] * np.dot(xw.T, xw)\n    for lag in range(1, nlags + 1):\n        (xw0, xwlag) = lagged_groups(xw, lag, groupidx)\n        s = np.dot(xw0.T, xwlag)\n        S += weights[lag] * (s + s.T)\n    return S",
            "def S_nw_panel(xw, weights, groupidx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'inner covariance matrix for HAC for panel data\\n\\n    no denominator nobs used\\n\\n    no reference for this, just accounting for time indices\\n    '\n    nlags = len(weights) - 1\n    S = weights[0] * np.dot(xw.T, xw)\n    for lag in range(1, nlags + 1):\n        (xw0, xwlag) = lagged_groups(xw, lag, groupidx)\n        s = np.dot(xw0.T, xwlag)\n        S += weights[lag] * (s + s.T)\n    return S",
            "def S_nw_panel(xw, weights, groupidx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'inner covariance matrix for HAC for panel data\\n\\n    no denominator nobs used\\n\\n    no reference for this, just accounting for time indices\\n    '\n    nlags = len(weights) - 1\n    S = weights[0] * np.dot(xw.T, xw)\n    for lag in range(1, nlags + 1):\n        (xw0, xwlag) = lagged_groups(xw, lag, groupidx)\n        s = np.dot(xw0.T, xwlag)\n        S += weights[lag] * (s + s.T)\n    return S"
        ]
    },
    {
        "func_name": "cov_nw_panel",
        "original": "def cov_nw_panel(results, nlags, groupidx, weights_func=weights_bartlett, use_correction='hac'):\n    \"\"\"Panel HAC robust covariance matrix\n\n    Assumes we have a panel of time series with consecutive, equal spaced time\n    periods. Data is assumed to be in long format with time series of each\n    individual stacked into one array. Panel can be unbalanced.\n\n    Parameters\n    ----------\n    results : result instance\n       result of a regression, uses results.model.exog and results.resid\n       TODO: this should use wexog instead\n    nlags : int or None\n        Highest lag to include in kernel window. Currently, no default\n        because the optimal length will depend on the number of observations\n        per cross-sectional unit.\n    groupidx : list of tuple\n        each tuple should contain the start and end index for an individual.\n        (groupidx might change in future).\n    weights_func : callable\n        weights_func is called with nlags as argument to get the kernel\n        weights. default are Bartlett weights\n    use_correction : 'cluster' or 'hac' or False\n        If False, then no small sample correction is used.\n        If 'cluster' (default), then the same correction as in cov_cluster is\n        used.\n        If 'hac', then the same correction as in single time series, cov_hac\n        is used.\n\n\n    Returns\n    -------\n    cov : ndarray, (k_vars, k_vars)\n        HAC robust covariance matrix for parameter estimates\n\n    Notes\n    -----\n    For nlags=0, this is just White covariance, cov_white.\n    If kernel is uniform, `weights_uniform`, with nlags equal to the number\n    of observations per unit in a balance panel, then cov_cluster and\n    cov_hac_panel are identical.\n\n    Tested against STATA `newey` command with same defaults.\n\n    Options might change when other kernels besides Bartlett and uniform are\n    available.\n\n    \"\"\"\n    if nlags == 0:\n        weights = [1, 0]\n    else:\n        weights = weights_func(nlags)\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    S_hac = S_nw_panel(xu, weights, groupidx)\n    cov_hac = _HCCM2(hessian_inv, S_hac)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        if use_correction == 'hac':\n            cov_hac *= nobs / float(nobs - k_params)\n        elif use_correction in ['c', 'clu', 'cluster']:\n            n_groups = len(groupidx)\n            cov_hac *= n_groups / (n_groups - 1.0)\n            cov_hac *= (nobs - 1.0) / float(nobs - k_params)\n    return cov_hac",
        "mutated": [
            "def cov_nw_panel(results, nlags, groupidx, weights_func=weights_bartlett, use_correction='hac'):\n    if False:\n        i = 10\n    \"Panel HAC robust covariance matrix\\n\\n    Assumes we have a panel of time series with consecutive, equal spaced time\\n    periods. Data is assumed to be in long format with time series of each\\n    individual stacked into one array. Panel can be unbalanced.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        Highest lag to include in kernel window. Currently, no default\\n        because the optimal length will depend on the number of observations\\n        per cross-sectional unit.\\n    groupidx : list of tuple\\n        each tuple should contain the start and end index for an individual.\\n        (groupidx might change in future).\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n    use_correction : 'cluster' or 'hac' or False\\n        If False, then no small sample correction is used.\\n        If 'cluster' (default), then the same correction as in cov_cluster is\\n        used.\\n        If 'hac', then the same correction as in single time series, cov_hac\\n        is used.\\n\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    For nlags=0, this is just White covariance, cov_white.\\n    If kernel is uniform, `weights_uniform`, with nlags equal to the number\\n    of observations per unit in a balance panel, then cov_cluster and\\n    cov_hac_panel are identical.\\n\\n    Tested against STATA `newey` command with same defaults.\\n\\n    Options might change when other kernels besides Bartlett and uniform are\\n    available.\\n\\n    \"\n    if nlags == 0:\n        weights = [1, 0]\n    else:\n        weights = weights_func(nlags)\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    S_hac = S_nw_panel(xu, weights, groupidx)\n    cov_hac = _HCCM2(hessian_inv, S_hac)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        if use_correction == 'hac':\n            cov_hac *= nobs / float(nobs - k_params)\n        elif use_correction in ['c', 'clu', 'cluster']:\n            n_groups = len(groupidx)\n            cov_hac *= n_groups / (n_groups - 1.0)\n            cov_hac *= (nobs - 1.0) / float(nobs - k_params)\n    return cov_hac",
            "def cov_nw_panel(results, nlags, groupidx, weights_func=weights_bartlett, use_correction='hac'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Panel HAC robust covariance matrix\\n\\n    Assumes we have a panel of time series with consecutive, equal spaced time\\n    periods. Data is assumed to be in long format with time series of each\\n    individual stacked into one array. Panel can be unbalanced.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        Highest lag to include in kernel window. Currently, no default\\n        because the optimal length will depend on the number of observations\\n        per cross-sectional unit.\\n    groupidx : list of tuple\\n        each tuple should contain the start and end index for an individual.\\n        (groupidx might change in future).\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n    use_correction : 'cluster' or 'hac' or False\\n        If False, then no small sample correction is used.\\n        If 'cluster' (default), then the same correction as in cov_cluster is\\n        used.\\n        If 'hac', then the same correction as in single time series, cov_hac\\n        is used.\\n\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    For nlags=0, this is just White covariance, cov_white.\\n    If kernel is uniform, `weights_uniform`, with nlags equal to the number\\n    of observations per unit in a balance panel, then cov_cluster and\\n    cov_hac_panel are identical.\\n\\n    Tested against STATA `newey` command with same defaults.\\n\\n    Options might change when other kernels besides Bartlett and uniform are\\n    available.\\n\\n    \"\n    if nlags == 0:\n        weights = [1, 0]\n    else:\n        weights = weights_func(nlags)\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    S_hac = S_nw_panel(xu, weights, groupidx)\n    cov_hac = _HCCM2(hessian_inv, S_hac)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        if use_correction == 'hac':\n            cov_hac *= nobs / float(nobs - k_params)\n        elif use_correction in ['c', 'clu', 'cluster']:\n            n_groups = len(groupidx)\n            cov_hac *= n_groups / (n_groups - 1.0)\n            cov_hac *= (nobs - 1.0) / float(nobs - k_params)\n    return cov_hac",
            "def cov_nw_panel(results, nlags, groupidx, weights_func=weights_bartlett, use_correction='hac'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Panel HAC robust covariance matrix\\n\\n    Assumes we have a panel of time series with consecutive, equal spaced time\\n    periods. Data is assumed to be in long format with time series of each\\n    individual stacked into one array. Panel can be unbalanced.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        Highest lag to include in kernel window. Currently, no default\\n        because the optimal length will depend on the number of observations\\n        per cross-sectional unit.\\n    groupidx : list of tuple\\n        each tuple should contain the start and end index for an individual.\\n        (groupidx might change in future).\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n    use_correction : 'cluster' or 'hac' or False\\n        If False, then no small sample correction is used.\\n        If 'cluster' (default), then the same correction as in cov_cluster is\\n        used.\\n        If 'hac', then the same correction as in single time series, cov_hac\\n        is used.\\n\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    For nlags=0, this is just White covariance, cov_white.\\n    If kernel is uniform, `weights_uniform`, with nlags equal to the number\\n    of observations per unit in a balance panel, then cov_cluster and\\n    cov_hac_panel are identical.\\n\\n    Tested against STATA `newey` command with same defaults.\\n\\n    Options might change when other kernels besides Bartlett and uniform are\\n    available.\\n\\n    \"\n    if nlags == 0:\n        weights = [1, 0]\n    else:\n        weights = weights_func(nlags)\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    S_hac = S_nw_panel(xu, weights, groupidx)\n    cov_hac = _HCCM2(hessian_inv, S_hac)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        if use_correction == 'hac':\n            cov_hac *= nobs / float(nobs - k_params)\n        elif use_correction in ['c', 'clu', 'cluster']:\n            n_groups = len(groupidx)\n            cov_hac *= n_groups / (n_groups - 1.0)\n            cov_hac *= (nobs - 1.0) / float(nobs - k_params)\n    return cov_hac",
            "def cov_nw_panel(results, nlags, groupidx, weights_func=weights_bartlett, use_correction='hac'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Panel HAC robust covariance matrix\\n\\n    Assumes we have a panel of time series with consecutive, equal spaced time\\n    periods. Data is assumed to be in long format with time series of each\\n    individual stacked into one array. Panel can be unbalanced.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        Highest lag to include in kernel window. Currently, no default\\n        because the optimal length will depend on the number of observations\\n        per cross-sectional unit.\\n    groupidx : list of tuple\\n        each tuple should contain the start and end index for an individual.\\n        (groupidx might change in future).\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n    use_correction : 'cluster' or 'hac' or False\\n        If False, then no small sample correction is used.\\n        If 'cluster' (default), then the same correction as in cov_cluster is\\n        used.\\n        If 'hac', then the same correction as in single time series, cov_hac\\n        is used.\\n\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    For nlags=0, this is just White covariance, cov_white.\\n    If kernel is uniform, `weights_uniform`, with nlags equal to the number\\n    of observations per unit in a balance panel, then cov_cluster and\\n    cov_hac_panel are identical.\\n\\n    Tested against STATA `newey` command with same defaults.\\n\\n    Options might change when other kernels besides Bartlett and uniform are\\n    available.\\n\\n    \"\n    if nlags == 0:\n        weights = [1, 0]\n    else:\n        weights = weights_func(nlags)\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    S_hac = S_nw_panel(xu, weights, groupidx)\n    cov_hac = _HCCM2(hessian_inv, S_hac)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        if use_correction == 'hac':\n            cov_hac *= nobs / float(nobs - k_params)\n        elif use_correction in ['c', 'clu', 'cluster']:\n            n_groups = len(groupidx)\n            cov_hac *= n_groups / (n_groups - 1.0)\n            cov_hac *= (nobs - 1.0) / float(nobs - k_params)\n    return cov_hac",
            "def cov_nw_panel(results, nlags, groupidx, weights_func=weights_bartlett, use_correction='hac'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Panel HAC robust covariance matrix\\n\\n    Assumes we have a panel of time series with consecutive, equal spaced time\\n    periods. Data is assumed to be in long format with time series of each\\n    individual stacked into one array. Panel can be unbalanced.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        Highest lag to include in kernel window. Currently, no default\\n        because the optimal length will depend on the number of observations\\n        per cross-sectional unit.\\n    groupidx : list of tuple\\n        each tuple should contain the start and end index for an individual.\\n        (groupidx might change in future).\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n    use_correction : 'cluster' or 'hac' or False\\n        If False, then no small sample correction is used.\\n        If 'cluster' (default), then the same correction as in cov_cluster is\\n        used.\\n        If 'hac', then the same correction as in single time series, cov_hac\\n        is used.\\n\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    For nlags=0, this is just White covariance, cov_white.\\n    If kernel is uniform, `weights_uniform`, with nlags equal to the number\\n    of observations per unit in a balance panel, then cov_cluster and\\n    cov_hac_panel are identical.\\n\\n    Tested against STATA `newey` command with same defaults.\\n\\n    Options might change when other kernels besides Bartlett and uniform are\\n    available.\\n\\n    \"\n    if nlags == 0:\n        weights = [1, 0]\n    else:\n        weights = weights_func(nlags)\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    S_hac = S_nw_panel(xu, weights, groupidx)\n    cov_hac = _HCCM2(hessian_inv, S_hac)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        if use_correction == 'hac':\n            cov_hac *= nobs / float(nobs - k_params)\n        elif use_correction in ['c', 'clu', 'cluster']:\n            n_groups = len(groupidx)\n            cov_hac *= n_groups / (n_groups - 1.0)\n            cov_hac *= (nobs - 1.0) / float(nobs - k_params)\n    return cov_hac"
        ]
    },
    {
        "func_name": "cov_nw_groupsum",
        "original": "def cov_nw_groupsum(results, nlags, time, weights_func=weights_bartlett, use_correction=0):\n    \"\"\"Driscoll and Kraay Panel robust covariance matrix\n\n    Robust covariance matrix for panel data of Driscoll and Kraay.\n\n    Assumes we have a panel of time series where the time index is available.\n    The time index is assumed to represent equal spaced periods. At least one\n    observation per period is required.\n\n    Parameters\n    ----------\n    results : result instance\n       result of a regression, uses results.model.exog and results.resid\n       TODO: this should use wexog instead\n    nlags : int or None\n        Highest lag to include in kernel window. Currently, no default\n        because the optimal length will depend on the number of observations\n        per cross-sectional unit.\n    time : ndarray of int\n        this should contain the coding for the time period of each observation.\n        time periods should be integers in range(maxT) where maxT is obs of i\n    weights_func : callable\n        weights_func is called with nlags as argument to get the kernel\n        weights. default are Bartlett weights\n    use_correction : 'cluster' or 'hac' or False\n        If False, then no small sample correction is used.\n        If 'hac' (default), then the same correction as in single time series, cov_hac\n        is used.\n        If 'cluster', then the same correction as in cov_cluster is\n        used.\n\n    Returns\n    -------\n    cov : ndarray, (k_vars, k_vars)\n        HAC robust covariance matrix for parameter estimates\n\n    Notes\n    -----\n    Tested against STATA xtscc package, which uses no small sample correction\n\n    This first averages relevant variables for each time period over all\n    individuals/groups, and then applies the same kernel weighted averaging\n    over time as in HAC.\n\n    Warning:\n    In the example with a short panel (few time periods and many individuals)\n    with mainly across individual variation this estimator did not produce\n    reasonable results.\n\n    Options might change when other kernels besides Bartlett and uniform are\n    available.\n\n    References\n    ----------\n    Daniel Hoechle, xtscc paper\n    Driscoll and Kraay\n\n    \"\"\"\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    S_hac = S_hac_groupsum(xu, time, nlags=nlags, weights_func=weights_func)\n    cov_hac = _HCCM2(hessian_inv, S_hac)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        if use_correction == 'hac':\n            cov_hac *= nobs / float(nobs - k_params)\n        elif use_correction in ['c', 'cluster']:\n            n_groups = len(np.unique(time))\n            cov_hac *= n_groups / (n_groups - 1.0)\n            cov_hac *= (nobs - 1.0) / float(nobs - k_params)\n    return cov_hac",
        "mutated": [
            "def cov_nw_groupsum(results, nlags, time, weights_func=weights_bartlett, use_correction=0):\n    if False:\n        i = 10\n    \"Driscoll and Kraay Panel robust covariance matrix\\n\\n    Robust covariance matrix for panel data of Driscoll and Kraay.\\n\\n    Assumes we have a panel of time series where the time index is available.\\n    The time index is assumed to represent equal spaced periods. At least one\\n    observation per period is required.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        Highest lag to include in kernel window. Currently, no default\\n        because the optimal length will depend on the number of observations\\n        per cross-sectional unit.\\n    time : ndarray of int\\n        this should contain the coding for the time period of each observation.\\n        time periods should be integers in range(maxT) where maxT is obs of i\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n    use_correction : 'cluster' or 'hac' or False\\n        If False, then no small sample correction is used.\\n        If 'hac' (default), then the same correction as in single time series, cov_hac\\n        is used.\\n        If 'cluster', then the same correction as in cov_cluster is\\n        used.\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    Tested against STATA xtscc package, which uses no small sample correction\\n\\n    This first averages relevant variables for each time period over all\\n    individuals/groups, and then applies the same kernel weighted averaging\\n    over time as in HAC.\\n\\n    Warning:\\n    In the example with a short panel (few time periods and many individuals)\\n    with mainly across individual variation this estimator did not produce\\n    reasonable results.\\n\\n    Options might change when other kernels besides Bartlett and uniform are\\n    available.\\n\\n    References\\n    ----------\\n    Daniel Hoechle, xtscc paper\\n    Driscoll and Kraay\\n\\n    \"\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    S_hac = S_hac_groupsum(xu, time, nlags=nlags, weights_func=weights_func)\n    cov_hac = _HCCM2(hessian_inv, S_hac)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        if use_correction == 'hac':\n            cov_hac *= nobs / float(nobs - k_params)\n        elif use_correction in ['c', 'cluster']:\n            n_groups = len(np.unique(time))\n            cov_hac *= n_groups / (n_groups - 1.0)\n            cov_hac *= (nobs - 1.0) / float(nobs - k_params)\n    return cov_hac",
            "def cov_nw_groupsum(results, nlags, time, weights_func=weights_bartlett, use_correction=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Driscoll and Kraay Panel robust covariance matrix\\n\\n    Robust covariance matrix for panel data of Driscoll and Kraay.\\n\\n    Assumes we have a panel of time series where the time index is available.\\n    The time index is assumed to represent equal spaced periods. At least one\\n    observation per period is required.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        Highest lag to include in kernel window. Currently, no default\\n        because the optimal length will depend on the number of observations\\n        per cross-sectional unit.\\n    time : ndarray of int\\n        this should contain the coding for the time period of each observation.\\n        time periods should be integers in range(maxT) where maxT is obs of i\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n    use_correction : 'cluster' or 'hac' or False\\n        If False, then no small sample correction is used.\\n        If 'hac' (default), then the same correction as in single time series, cov_hac\\n        is used.\\n        If 'cluster', then the same correction as in cov_cluster is\\n        used.\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    Tested against STATA xtscc package, which uses no small sample correction\\n\\n    This first averages relevant variables for each time period over all\\n    individuals/groups, and then applies the same kernel weighted averaging\\n    over time as in HAC.\\n\\n    Warning:\\n    In the example with a short panel (few time periods and many individuals)\\n    with mainly across individual variation this estimator did not produce\\n    reasonable results.\\n\\n    Options might change when other kernels besides Bartlett and uniform are\\n    available.\\n\\n    References\\n    ----------\\n    Daniel Hoechle, xtscc paper\\n    Driscoll and Kraay\\n\\n    \"\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    S_hac = S_hac_groupsum(xu, time, nlags=nlags, weights_func=weights_func)\n    cov_hac = _HCCM2(hessian_inv, S_hac)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        if use_correction == 'hac':\n            cov_hac *= nobs / float(nobs - k_params)\n        elif use_correction in ['c', 'cluster']:\n            n_groups = len(np.unique(time))\n            cov_hac *= n_groups / (n_groups - 1.0)\n            cov_hac *= (nobs - 1.0) / float(nobs - k_params)\n    return cov_hac",
            "def cov_nw_groupsum(results, nlags, time, weights_func=weights_bartlett, use_correction=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Driscoll and Kraay Panel robust covariance matrix\\n\\n    Robust covariance matrix for panel data of Driscoll and Kraay.\\n\\n    Assumes we have a panel of time series where the time index is available.\\n    The time index is assumed to represent equal spaced periods. At least one\\n    observation per period is required.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        Highest lag to include in kernel window. Currently, no default\\n        because the optimal length will depend on the number of observations\\n        per cross-sectional unit.\\n    time : ndarray of int\\n        this should contain the coding for the time period of each observation.\\n        time periods should be integers in range(maxT) where maxT is obs of i\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n    use_correction : 'cluster' or 'hac' or False\\n        If False, then no small sample correction is used.\\n        If 'hac' (default), then the same correction as in single time series, cov_hac\\n        is used.\\n        If 'cluster', then the same correction as in cov_cluster is\\n        used.\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    Tested against STATA xtscc package, which uses no small sample correction\\n\\n    This first averages relevant variables for each time period over all\\n    individuals/groups, and then applies the same kernel weighted averaging\\n    over time as in HAC.\\n\\n    Warning:\\n    In the example with a short panel (few time periods and many individuals)\\n    with mainly across individual variation this estimator did not produce\\n    reasonable results.\\n\\n    Options might change when other kernels besides Bartlett and uniform are\\n    available.\\n\\n    References\\n    ----------\\n    Daniel Hoechle, xtscc paper\\n    Driscoll and Kraay\\n\\n    \"\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    S_hac = S_hac_groupsum(xu, time, nlags=nlags, weights_func=weights_func)\n    cov_hac = _HCCM2(hessian_inv, S_hac)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        if use_correction == 'hac':\n            cov_hac *= nobs / float(nobs - k_params)\n        elif use_correction in ['c', 'cluster']:\n            n_groups = len(np.unique(time))\n            cov_hac *= n_groups / (n_groups - 1.0)\n            cov_hac *= (nobs - 1.0) / float(nobs - k_params)\n    return cov_hac",
            "def cov_nw_groupsum(results, nlags, time, weights_func=weights_bartlett, use_correction=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Driscoll and Kraay Panel robust covariance matrix\\n\\n    Robust covariance matrix for panel data of Driscoll and Kraay.\\n\\n    Assumes we have a panel of time series where the time index is available.\\n    The time index is assumed to represent equal spaced periods. At least one\\n    observation per period is required.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        Highest lag to include in kernel window. Currently, no default\\n        because the optimal length will depend on the number of observations\\n        per cross-sectional unit.\\n    time : ndarray of int\\n        this should contain the coding for the time period of each observation.\\n        time periods should be integers in range(maxT) where maxT is obs of i\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n    use_correction : 'cluster' or 'hac' or False\\n        If False, then no small sample correction is used.\\n        If 'hac' (default), then the same correction as in single time series, cov_hac\\n        is used.\\n        If 'cluster', then the same correction as in cov_cluster is\\n        used.\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    Tested against STATA xtscc package, which uses no small sample correction\\n\\n    This first averages relevant variables for each time period over all\\n    individuals/groups, and then applies the same kernel weighted averaging\\n    over time as in HAC.\\n\\n    Warning:\\n    In the example with a short panel (few time periods and many individuals)\\n    with mainly across individual variation this estimator did not produce\\n    reasonable results.\\n\\n    Options might change when other kernels besides Bartlett and uniform are\\n    available.\\n\\n    References\\n    ----------\\n    Daniel Hoechle, xtscc paper\\n    Driscoll and Kraay\\n\\n    \"\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    S_hac = S_hac_groupsum(xu, time, nlags=nlags, weights_func=weights_func)\n    cov_hac = _HCCM2(hessian_inv, S_hac)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        if use_correction == 'hac':\n            cov_hac *= nobs / float(nobs - k_params)\n        elif use_correction in ['c', 'cluster']:\n            n_groups = len(np.unique(time))\n            cov_hac *= n_groups / (n_groups - 1.0)\n            cov_hac *= (nobs - 1.0) / float(nobs - k_params)\n    return cov_hac",
            "def cov_nw_groupsum(results, nlags, time, weights_func=weights_bartlett, use_correction=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Driscoll and Kraay Panel robust covariance matrix\\n\\n    Robust covariance matrix for panel data of Driscoll and Kraay.\\n\\n    Assumes we have a panel of time series where the time index is available.\\n    The time index is assumed to represent equal spaced periods. At least one\\n    observation per period is required.\\n\\n    Parameters\\n    ----------\\n    results : result instance\\n       result of a regression, uses results.model.exog and results.resid\\n       TODO: this should use wexog instead\\n    nlags : int or None\\n        Highest lag to include in kernel window. Currently, no default\\n        because the optimal length will depend on the number of observations\\n        per cross-sectional unit.\\n    time : ndarray of int\\n        this should contain the coding for the time period of each observation.\\n        time periods should be integers in range(maxT) where maxT is obs of i\\n    weights_func : callable\\n        weights_func is called with nlags as argument to get the kernel\\n        weights. default are Bartlett weights\\n    use_correction : 'cluster' or 'hac' or False\\n        If False, then no small sample correction is used.\\n        If 'hac' (default), then the same correction as in single time series, cov_hac\\n        is used.\\n        If 'cluster', then the same correction as in cov_cluster is\\n        used.\\n\\n    Returns\\n    -------\\n    cov : ndarray, (k_vars, k_vars)\\n        HAC robust covariance matrix for parameter estimates\\n\\n    Notes\\n    -----\\n    Tested against STATA xtscc package, which uses no small sample correction\\n\\n    This first averages relevant variables for each time period over all\\n    individuals/groups, and then applies the same kernel weighted averaging\\n    over time as in HAC.\\n\\n    Warning:\\n    In the example with a short panel (few time periods and many individuals)\\n    with mainly across individual variation this estimator did not produce\\n    reasonable results.\\n\\n    Options might change when other kernels besides Bartlett and uniform are\\n    available.\\n\\n    References\\n    ----------\\n    Daniel Hoechle, xtscc paper\\n    Driscoll and Kraay\\n\\n    \"\n    (xu, hessian_inv) = _get_sandwich_arrays(results)\n    S_hac = S_hac_groupsum(xu, time, nlags=nlags, weights_func=weights_func)\n    cov_hac = _HCCM2(hessian_inv, S_hac)\n    if use_correction:\n        (nobs, k_params) = xu.shape\n        if use_correction == 'hac':\n            cov_hac *= nobs / float(nobs - k_params)\n        elif use_correction in ['c', 'cluster']:\n            n_groups = len(np.unique(time))\n            cov_hac *= n_groups / (n_groups - 1.0)\n            cov_hac *= (nobs - 1.0) / float(nobs - k_params)\n    return cov_hac"
        ]
    }
]