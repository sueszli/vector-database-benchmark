[
    {
        "func_name": "replace_keys",
        "original": "def replace_keys(state_dict):\n    model_state_dict = {}\n    state_dict.pop('pixel_mean', None)\n    state_dict.pop('pixel_std', None)\n    output_hypernetworks_mlps_pattern = '.*.output_hypernetworks_mlps.(\\\\d+).layers.(\\\\d+).*'\n    for (key, value) in state_dict.items():\n        for (key_to_modify, new_key) in KEYS_TO_MODIFY_MAPPING.items():\n            if key_to_modify in key:\n                key = key.replace(key_to_modify, new_key)\n        if re.match(output_hypernetworks_mlps_pattern, key):\n            layer_nb = int(re.match(output_hypernetworks_mlps_pattern, key).group(2))\n            if layer_nb == 0:\n                key = key.replace('layers.0', 'proj_in')\n            elif layer_nb == 1:\n                key = key.replace('layers.1', 'layers.0')\n            elif layer_nb == 2:\n                key = key.replace('layers.2', 'proj_out')\n        model_state_dict[key] = value\n    model_state_dict['shared_image_embedding.positional_embedding'] = model_state_dict['prompt_encoder.shared_embedding.positional_embedding']\n    return model_state_dict",
        "mutated": [
            "def replace_keys(state_dict):\n    if False:\n        i = 10\n    model_state_dict = {}\n    state_dict.pop('pixel_mean', None)\n    state_dict.pop('pixel_std', None)\n    output_hypernetworks_mlps_pattern = '.*.output_hypernetworks_mlps.(\\\\d+).layers.(\\\\d+).*'\n    for (key, value) in state_dict.items():\n        for (key_to_modify, new_key) in KEYS_TO_MODIFY_MAPPING.items():\n            if key_to_modify in key:\n                key = key.replace(key_to_modify, new_key)\n        if re.match(output_hypernetworks_mlps_pattern, key):\n            layer_nb = int(re.match(output_hypernetworks_mlps_pattern, key).group(2))\n            if layer_nb == 0:\n                key = key.replace('layers.0', 'proj_in')\n            elif layer_nb == 1:\n                key = key.replace('layers.1', 'layers.0')\n            elif layer_nb == 2:\n                key = key.replace('layers.2', 'proj_out')\n        model_state_dict[key] = value\n    model_state_dict['shared_image_embedding.positional_embedding'] = model_state_dict['prompt_encoder.shared_embedding.positional_embedding']\n    return model_state_dict",
            "def replace_keys(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    model_state_dict = {}\n    state_dict.pop('pixel_mean', None)\n    state_dict.pop('pixel_std', None)\n    output_hypernetworks_mlps_pattern = '.*.output_hypernetworks_mlps.(\\\\d+).layers.(\\\\d+).*'\n    for (key, value) in state_dict.items():\n        for (key_to_modify, new_key) in KEYS_TO_MODIFY_MAPPING.items():\n            if key_to_modify in key:\n                key = key.replace(key_to_modify, new_key)\n        if re.match(output_hypernetworks_mlps_pattern, key):\n            layer_nb = int(re.match(output_hypernetworks_mlps_pattern, key).group(2))\n            if layer_nb == 0:\n                key = key.replace('layers.0', 'proj_in')\n            elif layer_nb == 1:\n                key = key.replace('layers.1', 'layers.0')\n            elif layer_nb == 2:\n                key = key.replace('layers.2', 'proj_out')\n        model_state_dict[key] = value\n    model_state_dict['shared_image_embedding.positional_embedding'] = model_state_dict['prompt_encoder.shared_embedding.positional_embedding']\n    return model_state_dict",
            "def replace_keys(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    model_state_dict = {}\n    state_dict.pop('pixel_mean', None)\n    state_dict.pop('pixel_std', None)\n    output_hypernetworks_mlps_pattern = '.*.output_hypernetworks_mlps.(\\\\d+).layers.(\\\\d+).*'\n    for (key, value) in state_dict.items():\n        for (key_to_modify, new_key) in KEYS_TO_MODIFY_MAPPING.items():\n            if key_to_modify in key:\n                key = key.replace(key_to_modify, new_key)\n        if re.match(output_hypernetworks_mlps_pattern, key):\n            layer_nb = int(re.match(output_hypernetworks_mlps_pattern, key).group(2))\n            if layer_nb == 0:\n                key = key.replace('layers.0', 'proj_in')\n            elif layer_nb == 1:\n                key = key.replace('layers.1', 'layers.0')\n            elif layer_nb == 2:\n                key = key.replace('layers.2', 'proj_out')\n        model_state_dict[key] = value\n    model_state_dict['shared_image_embedding.positional_embedding'] = model_state_dict['prompt_encoder.shared_embedding.positional_embedding']\n    return model_state_dict",
            "def replace_keys(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    model_state_dict = {}\n    state_dict.pop('pixel_mean', None)\n    state_dict.pop('pixel_std', None)\n    output_hypernetworks_mlps_pattern = '.*.output_hypernetworks_mlps.(\\\\d+).layers.(\\\\d+).*'\n    for (key, value) in state_dict.items():\n        for (key_to_modify, new_key) in KEYS_TO_MODIFY_MAPPING.items():\n            if key_to_modify in key:\n                key = key.replace(key_to_modify, new_key)\n        if re.match(output_hypernetworks_mlps_pattern, key):\n            layer_nb = int(re.match(output_hypernetworks_mlps_pattern, key).group(2))\n            if layer_nb == 0:\n                key = key.replace('layers.0', 'proj_in')\n            elif layer_nb == 1:\n                key = key.replace('layers.1', 'layers.0')\n            elif layer_nb == 2:\n                key = key.replace('layers.2', 'proj_out')\n        model_state_dict[key] = value\n    model_state_dict['shared_image_embedding.positional_embedding'] = model_state_dict['prompt_encoder.shared_embedding.positional_embedding']\n    return model_state_dict",
            "def replace_keys(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    model_state_dict = {}\n    state_dict.pop('pixel_mean', None)\n    state_dict.pop('pixel_std', None)\n    output_hypernetworks_mlps_pattern = '.*.output_hypernetworks_mlps.(\\\\d+).layers.(\\\\d+).*'\n    for (key, value) in state_dict.items():\n        for (key_to_modify, new_key) in KEYS_TO_MODIFY_MAPPING.items():\n            if key_to_modify in key:\n                key = key.replace(key_to_modify, new_key)\n        if re.match(output_hypernetworks_mlps_pattern, key):\n            layer_nb = int(re.match(output_hypernetworks_mlps_pattern, key).group(2))\n            if layer_nb == 0:\n                key = key.replace('layers.0', 'proj_in')\n            elif layer_nb == 1:\n                key = key.replace('layers.1', 'layers.0')\n            elif layer_nb == 2:\n                key = key.replace('layers.2', 'proj_out')\n        model_state_dict[key] = value\n    model_state_dict['shared_image_embedding.positional_embedding'] = model_state_dict['prompt_encoder.shared_embedding.positional_embedding']\n    return model_state_dict"
        ]
    },
    {
        "func_name": "convert_sam_checkpoint",
        "original": "def convert_sam_checkpoint(model_name, pytorch_dump_folder, push_to_hub, model_hub_id='ybelkada/segment-anything'):\n    checkpoint_path = hf_hub_download(model_hub_id, f'checkpoints/{model_name}.pth')\n    if 'sam_vit_b' in model_name:\n        config = SamConfig()\n    elif 'sam_vit_l' in model_name:\n        vision_config = SamVisionConfig(hidden_size=1024, num_hidden_layers=24, num_attention_heads=16, global_attn_indexes=[5, 11, 17, 23])\n        config = SamConfig(vision_config=vision_config)\n    elif 'sam_vit_h' in model_name:\n        vision_config = SamVisionConfig(hidden_size=1280, num_hidden_layers=32, num_attention_heads=16, global_attn_indexes=[7, 15, 23, 31])\n        config = SamConfig(vision_config=vision_config)\n    state_dict = torch.load(checkpoint_path, map_location='cpu')\n    state_dict = replace_keys(state_dict)\n    image_processor = SamImageProcessor()\n    processor = SamProcessor(image_processor=image_processor)\n    hf_model = SamModel(config)\n    hf_model.load_state_dict(state_dict)\n    hf_model = hf_model.to('cuda')\n    img_url = 'https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png'\n    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n    input_points = [[[400, 650]]]\n    input_labels = [[1]]\n    inputs = processor(images=np.array(raw_image), return_tensors='pt').to('cuda')\n    with torch.no_grad():\n        output = hf_model(**inputs)\n    scores = output.iou_scores.squeeze()\n    if model_name == 'sam_vit_h_4b8939':\n        assert scores[-1].item() == 0.579890251159668\n        inputs = processor(images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.9712603092193604\n        input_boxes = ((75, 275, 1725, 850),)\n        inputs = processor(images=np.array(raw_image), input_boxes=input_boxes, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.8686015605926514\n        input_points = [[[400, 650], [800, 650]]]\n        input_labels = [[1, 1]]\n        inputs = processor(images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.9936047792434692",
        "mutated": [
            "def convert_sam_checkpoint(model_name, pytorch_dump_folder, push_to_hub, model_hub_id='ybelkada/segment-anything'):\n    if False:\n        i = 10\n    checkpoint_path = hf_hub_download(model_hub_id, f'checkpoints/{model_name}.pth')\n    if 'sam_vit_b' in model_name:\n        config = SamConfig()\n    elif 'sam_vit_l' in model_name:\n        vision_config = SamVisionConfig(hidden_size=1024, num_hidden_layers=24, num_attention_heads=16, global_attn_indexes=[5, 11, 17, 23])\n        config = SamConfig(vision_config=vision_config)\n    elif 'sam_vit_h' in model_name:\n        vision_config = SamVisionConfig(hidden_size=1280, num_hidden_layers=32, num_attention_heads=16, global_attn_indexes=[7, 15, 23, 31])\n        config = SamConfig(vision_config=vision_config)\n    state_dict = torch.load(checkpoint_path, map_location='cpu')\n    state_dict = replace_keys(state_dict)\n    image_processor = SamImageProcessor()\n    processor = SamProcessor(image_processor=image_processor)\n    hf_model = SamModel(config)\n    hf_model.load_state_dict(state_dict)\n    hf_model = hf_model.to('cuda')\n    img_url = 'https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png'\n    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n    input_points = [[[400, 650]]]\n    input_labels = [[1]]\n    inputs = processor(images=np.array(raw_image), return_tensors='pt').to('cuda')\n    with torch.no_grad():\n        output = hf_model(**inputs)\n    scores = output.iou_scores.squeeze()\n    if model_name == 'sam_vit_h_4b8939':\n        assert scores[-1].item() == 0.579890251159668\n        inputs = processor(images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.9712603092193604\n        input_boxes = ((75, 275, 1725, 850),)\n        inputs = processor(images=np.array(raw_image), input_boxes=input_boxes, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.8686015605926514\n        input_points = [[[400, 650], [800, 650]]]\n        input_labels = [[1, 1]]\n        inputs = processor(images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.9936047792434692",
            "def convert_sam_checkpoint(model_name, pytorch_dump_folder, push_to_hub, model_hub_id='ybelkada/segment-anything'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    checkpoint_path = hf_hub_download(model_hub_id, f'checkpoints/{model_name}.pth')\n    if 'sam_vit_b' in model_name:\n        config = SamConfig()\n    elif 'sam_vit_l' in model_name:\n        vision_config = SamVisionConfig(hidden_size=1024, num_hidden_layers=24, num_attention_heads=16, global_attn_indexes=[5, 11, 17, 23])\n        config = SamConfig(vision_config=vision_config)\n    elif 'sam_vit_h' in model_name:\n        vision_config = SamVisionConfig(hidden_size=1280, num_hidden_layers=32, num_attention_heads=16, global_attn_indexes=[7, 15, 23, 31])\n        config = SamConfig(vision_config=vision_config)\n    state_dict = torch.load(checkpoint_path, map_location='cpu')\n    state_dict = replace_keys(state_dict)\n    image_processor = SamImageProcessor()\n    processor = SamProcessor(image_processor=image_processor)\n    hf_model = SamModel(config)\n    hf_model.load_state_dict(state_dict)\n    hf_model = hf_model.to('cuda')\n    img_url = 'https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png'\n    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n    input_points = [[[400, 650]]]\n    input_labels = [[1]]\n    inputs = processor(images=np.array(raw_image), return_tensors='pt').to('cuda')\n    with torch.no_grad():\n        output = hf_model(**inputs)\n    scores = output.iou_scores.squeeze()\n    if model_name == 'sam_vit_h_4b8939':\n        assert scores[-1].item() == 0.579890251159668\n        inputs = processor(images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.9712603092193604\n        input_boxes = ((75, 275, 1725, 850),)\n        inputs = processor(images=np.array(raw_image), input_boxes=input_boxes, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.8686015605926514\n        input_points = [[[400, 650], [800, 650]]]\n        input_labels = [[1, 1]]\n        inputs = processor(images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.9936047792434692",
            "def convert_sam_checkpoint(model_name, pytorch_dump_folder, push_to_hub, model_hub_id='ybelkada/segment-anything'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    checkpoint_path = hf_hub_download(model_hub_id, f'checkpoints/{model_name}.pth')\n    if 'sam_vit_b' in model_name:\n        config = SamConfig()\n    elif 'sam_vit_l' in model_name:\n        vision_config = SamVisionConfig(hidden_size=1024, num_hidden_layers=24, num_attention_heads=16, global_attn_indexes=[5, 11, 17, 23])\n        config = SamConfig(vision_config=vision_config)\n    elif 'sam_vit_h' in model_name:\n        vision_config = SamVisionConfig(hidden_size=1280, num_hidden_layers=32, num_attention_heads=16, global_attn_indexes=[7, 15, 23, 31])\n        config = SamConfig(vision_config=vision_config)\n    state_dict = torch.load(checkpoint_path, map_location='cpu')\n    state_dict = replace_keys(state_dict)\n    image_processor = SamImageProcessor()\n    processor = SamProcessor(image_processor=image_processor)\n    hf_model = SamModel(config)\n    hf_model.load_state_dict(state_dict)\n    hf_model = hf_model.to('cuda')\n    img_url = 'https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png'\n    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n    input_points = [[[400, 650]]]\n    input_labels = [[1]]\n    inputs = processor(images=np.array(raw_image), return_tensors='pt').to('cuda')\n    with torch.no_grad():\n        output = hf_model(**inputs)\n    scores = output.iou_scores.squeeze()\n    if model_name == 'sam_vit_h_4b8939':\n        assert scores[-1].item() == 0.579890251159668\n        inputs = processor(images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.9712603092193604\n        input_boxes = ((75, 275, 1725, 850),)\n        inputs = processor(images=np.array(raw_image), input_boxes=input_boxes, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.8686015605926514\n        input_points = [[[400, 650], [800, 650]]]\n        input_labels = [[1, 1]]\n        inputs = processor(images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.9936047792434692",
            "def convert_sam_checkpoint(model_name, pytorch_dump_folder, push_to_hub, model_hub_id='ybelkada/segment-anything'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    checkpoint_path = hf_hub_download(model_hub_id, f'checkpoints/{model_name}.pth')\n    if 'sam_vit_b' in model_name:\n        config = SamConfig()\n    elif 'sam_vit_l' in model_name:\n        vision_config = SamVisionConfig(hidden_size=1024, num_hidden_layers=24, num_attention_heads=16, global_attn_indexes=[5, 11, 17, 23])\n        config = SamConfig(vision_config=vision_config)\n    elif 'sam_vit_h' in model_name:\n        vision_config = SamVisionConfig(hidden_size=1280, num_hidden_layers=32, num_attention_heads=16, global_attn_indexes=[7, 15, 23, 31])\n        config = SamConfig(vision_config=vision_config)\n    state_dict = torch.load(checkpoint_path, map_location='cpu')\n    state_dict = replace_keys(state_dict)\n    image_processor = SamImageProcessor()\n    processor = SamProcessor(image_processor=image_processor)\n    hf_model = SamModel(config)\n    hf_model.load_state_dict(state_dict)\n    hf_model = hf_model.to('cuda')\n    img_url = 'https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png'\n    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n    input_points = [[[400, 650]]]\n    input_labels = [[1]]\n    inputs = processor(images=np.array(raw_image), return_tensors='pt').to('cuda')\n    with torch.no_grad():\n        output = hf_model(**inputs)\n    scores = output.iou_scores.squeeze()\n    if model_name == 'sam_vit_h_4b8939':\n        assert scores[-1].item() == 0.579890251159668\n        inputs = processor(images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.9712603092193604\n        input_boxes = ((75, 275, 1725, 850),)\n        inputs = processor(images=np.array(raw_image), input_boxes=input_boxes, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.8686015605926514\n        input_points = [[[400, 650], [800, 650]]]\n        input_labels = [[1, 1]]\n        inputs = processor(images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.9936047792434692",
            "def convert_sam_checkpoint(model_name, pytorch_dump_folder, push_to_hub, model_hub_id='ybelkada/segment-anything'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    checkpoint_path = hf_hub_download(model_hub_id, f'checkpoints/{model_name}.pth')\n    if 'sam_vit_b' in model_name:\n        config = SamConfig()\n    elif 'sam_vit_l' in model_name:\n        vision_config = SamVisionConfig(hidden_size=1024, num_hidden_layers=24, num_attention_heads=16, global_attn_indexes=[5, 11, 17, 23])\n        config = SamConfig(vision_config=vision_config)\n    elif 'sam_vit_h' in model_name:\n        vision_config = SamVisionConfig(hidden_size=1280, num_hidden_layers=32, num_attention_heads=16, global_attn_indexes=[7, 15, 23, 31])\n        config = SamConfig(vision_config=vision_config)\n    state_dict = torch.load(checkpoint_path, map_location='cpu')\n    state_dict = replace_keys(state_dict)\n    image_processor = SamImageProcessor()\n    processor = SamProcessor(image_processor=image_processor)\n    hf_model = SamModel(config)\n    hf_model.load_state_dict(state_dict)\n    hf_model = hf_model.to('cuda')\n    img_url = 'https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png'\n    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n    input_points = [[[400, 650]]]\n    input_labels = [[1]]\n    inputs = processor(images=np.array(raw_image), return_tensors='pt').to('cuda')\n    with torch.no_grad():\n        output = hf_model(**inputs)\n    scores = output.iou_scores.squeeze()\n    if model_name == 'sam_vit_h_4b8939':\n        assert scores[-1].item() == 0.579890251159668\n        inputs = processor(images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.9712603092193604\n        input_boxes = ((75, 275, 1725, 850),)\n        inputs = processor(images=np.array(raw_image), input_boxes=input_boxes, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.8686015605926514\n        input_points = [[[400, 650], [800, 650]]]\n        input_labels = [[1, 1]]\n        inputs = processor(images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors='pt').to('cuda')\n        with torch.no_grad():\n            output = hf_model(**inputs)\n        scores = output.iou_scores.squeeze()\n        assert scores[-1].item() == 0.9936047792434692"
        ]
    }
]