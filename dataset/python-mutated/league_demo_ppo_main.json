[
    {
        "func_name": "__init__",
        "original": "def __init__(self, optimal_policy: list) -> None:\n    assert len(optimal_policy) == 2\n    self.optimal_policy = optimal_policy",
        "mutated": [
            "def __init__(self, optimal_policy: list) -> None:\n    if False:\n        i = 10\n    assert len(optimal_policy) == 2\n    self.optimal_policy = optimal_policy",
            "def __init__(self, optimal_policy: list) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert len(optimal_policy) == 2\n    self.optimal_policy = optimal_policy",
            "def __init__(self, optimal_policy: list) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert len(optimal_policy) == 2\n    self.optimal_policy = optimal_policy",
            "def __init__(self, optimal_policy: list) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert len(optimal_policy) == 2\n    self.optimal_policy = optimal_policy",
            "def __init__(self, optimal_policy: list) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert len(optimal_policy) == 2\n    self.optimal_policy = optimal_policy"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, data: dict) -> dict:\n    return {env_id: {'action': torch.from_numpy(np.random.choice([0, 1], p=self.optimal_policy, size=(1,)))} for env_id in data.keys()}",
        "mutated": [
            "def forward(self, data: dict) -> dict:\n    if False:\n        i = 10\n    return {env_id: {'action': torch.from_numpy(np.random.choice([0, 1], p=self.optimal_policy, size=(1,)))} for env_id in data.keys()}",
            "def forward(self, data: dict) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {env_id: {'action': torch.from_numpy(np.random.choice([0, 1], p=self.optimal_policy, size=(1,)))} for env_id in data.keys()}",
            "def forward(self, data: dict) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {env_id: {'action': torch.from_numpy(np.random.choice([0, 1], p=self.optimal_policy, size=(1,)))} for env_id in data.keys()}",
            "def forward(self, data: dict) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {env_id: {'action': torch.from_numpy(np.random.choice([0, 1], p=self.optimal_policy, size=(1,)))} for env_id in data.keys()}",
            "def forward(self, data: dict) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {env_id: {'action': torch.from_numpy(np.random.choice([0, 1], p=self.optimal_policy, size=(1,)))} for env_id in data.keys()}"
        ]
    },
    {
        "func_name": "reset",
        "original": "def reset(self, data_id: list=[]) -> None:\n    pass",
        "mutated": [
            "def reset(self, data_id: list=[]) -> None:\n    if False:\n        i = 10\n    pass",
            "def reset(self, data_id: list=[]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "def reset(self, data_id: list=[]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "def reset(self, data_id: list=[]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "def reset(self, data_id: list=[]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, data: dict) -> dict:\n    return {env_id: {'action': torch.from_numpy(np.random.choice([0, 1], p=[0.5, 0.5], size=(1,)))} for env_id in data.keys()}",
        "mutated": [
            "def forward(self, data: dict) -> dict:\n    if False:\n        i = 10\n    return {env_id: {'action': torch.from_numpy(np.random.choice([0, 1], p=[0.5, 0.5], size=(1,)))} for env_id in data.keys()}",
            "def forward(self, data: dict) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {env_id: {'action': torch.from_numpy(np.random.choice([0, 1], p=[0.5, 0.5], size=(1,)))} for env_id in data.keys()}",
            "def forward(self, data: dict) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {env_id: {'action': torch.from_numpy(np.random.choice([0, 1], p=[0.5, 0.5], size=(1,)))} for env_id in data.keys()}",
            "def forward(self, data: dict) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {env_id: {'action': torch.from_numpy(np.random.choice([0, 1], p=[0.5, 0.5], size=(1,)))} for env_id in data.keys()}",
            "def forward(self, data: dict) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {env_id: {'action': torch.from_numpy(np.random.choice([0, 1], p=[0.5, 0.5], size=(1,)))} for env_id in data.keys()}"
        ]
    },
    {
        "func_name": "reset",
        "original": "def reset(self, data_id: list=[]) -> None:\n    pass",
        "mutated": [
            "def reset(self, data_id: list=[]) -> None:\n    if False:\n        i = 10\n    pass",
            "def reset(self, data_id: list=[]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "def reset(self, data_id: list=[]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "def reset(self, data_id: list=[]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "def reset(self, data_id: list=[]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "load_checkpoint_fn",
        "original": "def load_checkpoint_fn(player_id: str, ckpt_path: str):\n    state_dict = torch.load(ckpt_path)\n    policies[player_id].learn_mode.load_state_dict(state_dict)",
        "mutated": [
            "def load_checkpoint_fn(player_id: str, ckpt_path: str):\n    if False:\n        i = 10\n    state_dict = torch.load(ckpt_path)\n    policies[player_id].learn_mode.load_state_dict(state_dict)",
            "def load_checkpoint_fn(player_id: str, ckpt_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    state_dict = torch.load(ckpt_path)\n    policies[player_id].learn_mode.load_state_dict(state_dict)",
            "def load_checkpoint_fn(player_id: str, ckpt_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    state_dict = torch.load(ckpt_path)\n    policies[player_id].learn_mode.load_state_dict(state_dict)",
            "def load_checkpoint_fn(player_id: str, ckpt_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    state_dict = torch.load(ckpt_path)\n    policies[player_id].learn_mode.load_state_dict(state_dict)",
            "def load_checkpoint_fn(player_id: str, ckpt_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    state_dict = torch.load(ckpt_path)\n    policies[player_id].learn_mode.load_state_dict(state_dict)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(cfg, seed=0, max_train_iter=int(100000000.0), max_env_step=int(100000000.0)):\n    cfg = compile_config(cfg, BaseEnvManager, PPOPolicy, BaseLearner, LeagueDemoCollector, BattleInteractionSerialEvaluator, NaiveReplayBuffer, save_cfg=True)\n    env_type = cfg.env.env_type\n    (collector_env_num, evaluator_env_num) = (cfg.env.collector_env_num, cfg.env.evaluator_env_num)\n    evaluator_env1 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env2 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env3 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env1.seed(seed, dynamic_seed=False)\n    evaluator_env2.seed(seed, dynamic_seed=False)\n    evaluator_env3.seed(seed, dynamic_seed=False)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n    league = DemoLeague(cfg.policy.other.league)\n    eval_policy1 = EvalPolicy1(evaluator_env1._env_ref.optimal_policy)\n    eval_policy2 = EvalPolicy2()\n    policies = {}\n    learners = {}\n    collectors = {}\n    for player_id in league.active_players_ids:\n        model = VAC(**cfg.policy.model)\n        policy = PPOPolicy(cfg.policy, model=model)\n        policies[player_id] = policy\n        collector_env = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(collector_env_num)], cfg=cfg.env.manager)\n        collector_env.seed(seed)\n        learners[player_id] = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger=tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_learner')\n        collectors[player_id] = LeagueDemoCollector(cfg.policy.collect.collector, collector_env, tb_logger=tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_collector')\n    model = VAC(**cfg.policy.model)\n    policy = PPOPolicy(cfg.policy, model=model)\n    policies['historical'] = policy\n    eval_policy3 = PPOPolicy(cfg.policy, model=copy.deepcopy(model)).collect_mode\n    main_key = [k for k in learners.keys() if k.startswith('main_player')][0]\n    main_player = league.get_player_by_id(main_key)\n    main_learner = learners[main_key]\n    main_collector = collectors[main_key]\n    evaluator1_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator1_cfg.stop_value = cfg.env.stop_value[0]\n    evaluator1 = BattleInteractionSerialEvaluator(evaluator1_cfg, evaluator_env1, [policies[main_key].collect_mode, eval_policy1], tb_logger, exp_name=cfg.exp_name, instance_name='fixed_evaluator')\n    evaluator2_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator2_cfg.stop_value = cfg.env.stop_value[1]\n    evaluator2 = BattleInteractionSerialEvaluator(evaluator2_cfg, evaluator_env2, [policies[main_key].collect_mode, eval_policy2], tb_logger, exp_name=cfg.exp_name, instance_name='uniform_evaluator')\n    evaluator3_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator3_cfg.stop_value = 99999999\n    evaluator3 = BattleInteractionSerialEvaluator(evaluator3_cfg, evaluator_env3, [policies[main_key].collect_mode, eval_policy3], tb_logger, exp_name=cfg.exp_name, instance_name='init_evaluator')\n\n    def load_checkpoint_fn(player_id: str, ckpt_path: str):\n        state_dict = torch.load(ckpt_path)\n        policies[player_id].learn_mode.load_state_dict(state_dict)\n    torch.save(policies['historical'].learn_mode.state_dict(), league.reset_checkpoint_path)\n    league.load_checkpoint = load_checkpoint_fn\n    for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n        torch.save(policies[player_id].collect_mode.state_dict(), player_ckpt_path)\n        league.judge_snapshot(player_id, force=True)\n    init_main_player_rating = league.metric_env.create_rating(mu=0)\n    count = 0\n    while True:\n        if evaluator1.should_eval(main_learner.train_iter):\n            (stop_flag1, episode_info) = evaluator1.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=10, sigma=1e-08), win_loss_result)\n        if evaluator2.should_eval(main_learner.train_iter):\n            (stop_flag2, episode_info) = evaluator2.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=0, sigma=1e-08), win_loss_result)\n        if evaluator3.should_eval(main_learner.train_iter):\n            (_, episode_info) = evaluator3.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            (main_player.rating, init_main_player_rating) = league.metric_env.rate_1vs1(main_player.rating, init_main_player_rating, win_loss_result)\n            tb_logger.add_scalar('league/init_main_player_trueskill', init_main_player_rating.exposure, main_collector.envstep)\n        if stop_flag1 and stop_flag2:\n            break\n        for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n            tb_logger.add_scalar('league/{}_trueskill'.format(player_id), league.get_player_by_id(player_id).rating.exposure, main_collector.envstep)\n            (collector, learner) = (collectors[player_id], learners[player_id])\n            job = league.get_job_info(player_id)\n            opponent_player_id = job['player_id'][1]\n            if 'historical' in opponent_player_id:\n                opponent_policy = policies['historical'].collect_mode\n                opponent_path = job['checkpoint_path'][1]\n                opponent_policy.load_state_dict(torch.load(opponent_path, map_location='cpu'))\n            else:\n                opponent_policy = policies[opponent_player_id].collect_mode\n            collector.reset_policy([policies[player_id].collect_mode, opponent_policy])\n            (train_data, episode_info) = collector.collect(train_iter=learner.train_iter)\n            (train_data, episode_info) = (train_data[0], episode_info[0])\n            for d in train_data:\n                d['adv'] = d['reward']\n            for i in range(cfg.policy.learn.update_per_collect):\n                learner.train(train_data, collector.envstep)\n            torch.save(learner.policy.state_dict(), player_ckpt_path)\n            player_info = learner.learn_info\n            player_info['player_id'] = player_id\n            league.update_active_player(player_info)\n            league.judge_snapshot(player_id)\n            job_finish_info = {'eval_flag': True, 'launch_player': job['launch_player'], 'player_id': job['player_id'], 'result': [e['result'] for e in episode_info]}\n            league.finish_job(job_finish_info)\n        if main_collector.envstep >= max_env_step or main_learner.train_iter >= max_train_iter:\n            break\n        if count % 100 == 0:\n            print(repr(league.payoff))\n        count += 1",
        "mutated": [
            "def main(cfg, seed=0, max_train_iter=int(100000000.0), max_env_step=int(100000000.0)):\n    if False:\n        i = 10\n    cfg = compile_config(cfg, BaseEnvManager, PPOPolicy, BaseLearner, LeagueDemoCollector, BattleInteractionSerialEvaluator, NaiveReplayBuffer, save_cfg=True)\n    env_type = cfg.env.env_type\n    (collector_env_num, evaluator_env_num) = (cfg.env.collector_env_num, cfg.env.evaluator_env_num)\n    evaluator_env1 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env2 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env3 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env1.seed(seed, dynamic_seed=False)\n    evaluator_env2.seed(seed, dynamic_seed=False)\n    evaluator_env3.seed(seed, dynamic_seed=False)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n    league = DemoLeague(cfg.policy.other.league)\n    eval_policy1 = EvalPolicy1(evaluator_env1._env_ref.optimal_policy)\n    eval_policy2 = EvalPolicy2()\n    policies = {}\n    learners = {}\n    collectors = {}\n    for player_id in league.active_players_ids:\n        model = VAC(**cfg.policy.model)\n        policy = PPOPolicy(cfg.policy, model=model)\n        policies[player_id] = policy\n        collector_env = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(collector_env_num)], cfg=cfg.env.manager)\n        collector_env.seed(seed)\n        learners[player_id] = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger=tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_learner')\n        collectors[player_id] = LeagueDemoCollector(cfg.policy.collect.collector, collector_env, tb_logger=tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_collector')\n    model = VAC(**cfg.policy.model)\n    policy = PPOPolicy(cfg.policy, model=model)\n    policies['historical'] = policy\n    eval_policy3 = PPOPolicy(cfg.policy, model=copy.deepcopy(model)).collect_mode\n    main_key = [k for k in learners.keys() if k.startswith('main_player')][0]\n    main_player = league.get_player_by_id(main_key)\n    main_learner = learners[main_key]\n    main_collector = collectors[main_key]\n    evaluator1_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator1_cfg.stop_value = cfg.env.stop_value[0]\n    evaluator1 = BattleInteractionSerialEvaluator(evaluator1_cfg, evaluator_env1, [policies[main_key].collect_mode, eval_policy1], tb_logger, exp_name=cfg.exp_name, instance_name='fixed_evaluator')\n    evaluator2_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator2_cfg.stop_value = cfg.env.stop_value[1]\n    evaluator2 = BattleInteractionSerialEvaluator(evaluator2_cfg, evaluator_env2, [policies[main_key].collect_mode, eval_policy2], tb_logger, exp_name=cfg.exp_name, instance_name='uniform_evaluator')\n    evaluator3_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator3_cfg.stop_value = 99999999\n    evaluator3 = BattleInteractionSerialEvaluator(evaluator3_cfg, evaluator_env3, [policies[main_key].collect_mode, eval_policy3], tb_logger, exp_name=cfg.exp_name, instance_name='init_evaluator')\n\n    def load_checkpoint_fn(player_id: str, ckpt_path: str):\n        state_dict = torch.load(ckpt_path)\n        policies[player_id].learn_mode.load_state_dict(state_dict)\n    torch.save(policies['historical'].learn_mode.state_dict(), league.reset_checkpoint_path)\n    league.load_checkpoint = load_checkpoint_fn\n    for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n        torch.save(policies[player_id].collect_mode.state_dict(), player_ckpt_path)\n        league.judge_snapshot(player_id, force=True)\n    init_main_player_rating = league.metric_env.create_rating(mu=0)\n    count = 0\n    while True:\n        if evaluator1.should_eval(main_learner.train_iter):\n            (stop_flag1, episode_info) = evaluator1.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=10, sigma=1e-08), win_loss_result)\n        if evaluator2.should_eval(main_learner.train_iter):\n            (stop_flag2, episode_info) = evaluator2.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=0, sigma=1e-08), win_loss_result)\n        if evaluator3.should_eval(main_learner.train_iter):\n            (_, episode_info) = evaluator3.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            (main_player.rating, init_main_player_rating) = league.metric_env.rate_1vs1(main_player.rating, init_main_player_rating, win_loss_result)\n            tb_logger.add_scalar('league/init_main_player_trueskill', init_main_player_rating.exposure, main_collector.envstep)\n        if stop_flag1 and stop_flag2:\n            break\n        for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n            tb_logger.add_scalar('league/{}_trueskill'.format(player_id), league.get_player_by_id(player_id).rating.exposure, main_collector.envstep)\n            (collector, learner) = (collectors[player_id], learners[player_id])\n            job = league.get_job_info(player_id)\n            opponent_player_id = job['player_id'][1]\n            if 'historical' in opponent_player_id:\n                opponent_policy = policies['historical'].collect_mode\n                opponent_path = job['checkpoint_path'][1]\n                opponent_policy.load_state_dict(torch.load(opponent_path, map_location='cpu'))\n            else:\n                opponent_policy = policies[opponent_player_id].collect_mode\n            collector.reset_policy([policies[player_id].collect_mode, opponent_policy])\n            (train_data, episode_info) = collector.collect(train_iter=learner.train_iter)\n            (train_data, episode_info) = (train_data[0], episode_info[0])\n            for d in train_data:\n                d['adv'] = d['reward']\n            for i in range(cfg.policy.learn.update_per_collect):\n                learner.train(train_data, collector.envstep)\n            torch.save(learner.policy.state_dict(), player_ckpt_path)\n            player_info = learner.learn_info\n            player_info['player_id'] = player_id\n            league.update_active_player(player_info)\n            league.judge_snapshot(player_id)\n            job_finish_info = {'eval_flag': True, 'launch_player': job['launch_player'], 'player_id': job['player_id'], 'result': [e['result'] for e in episode_info]}\n            league.finish_job(job_finish_info)\n        if main_collector.envstep >= max_env_step or main_learner.train_iter >= max_train_iter:\n            break\n        if count % 100 == 0:\n            print(repr(league.payoff))\n        count += 1",
            "def main(cfg, seed=0, max_train_iter=int(100000000.0), max_env_step=int(100000000.0)):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    cfg = compile_config(cfg, BaseEnvManager, PPOPolicy, BaseLearner, LeagueDemoCollector, BattleInteractionSerialEvaluator, NaiveReplayBuffer, save_cfg=True)\n    env_type = cfg.env.env_type\n    (collector_env_num, evaluator_env_num) = (cfg.env.collector_env_num, cfg.env.evaluator_env_num)\n    evaluator_env1 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env2 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env3 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env1.seed(seed, dynamic_seed=False)\n    evaluator_env2.seed(seed, dynamic_seed=False)\n    evaluator_env3.seed(seed, dynamic_seed=False)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n    league = DemoLeague(cfg.policy.other.league)\n    eval_policy1 = EvalPolicy1(evaluator_env1._env_ref.optimal_policy)\n    eval_policy2 = EvalPolicy2()\n    policies = {}\n    learners = {}\n    collectors = {}\n    for player_id in league.active_players_ids:\n        model = VAC(**cfg.policy.model)\n        policy = PPOPolicy(cfg.policy, model=model)\n        policies[player_id] = policy\n        collector_env = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(collector_env_num)], cfg=cfg.env.manager)\n        collector_env.seed(seed)\n        learners[player_id] = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger=tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_learner')\n        collectors[player_id] = LeagueDemoCollector(cfg.policy.collect.collector, collector_env, tb_logger=tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_collector')\n    model = VAC(**cfg.policy.model)\n    policy = PPOPolicy(cfg.policy, model=model)\n    policies['historical'] = policy\n    eval_policy3 = PPOPolicy(cfg.policy, model=copy.deepcopy(model)).collect_mode\n    main_key = [k for k in learners.keys() if k.startswith('main_player')][0]\n    main_player = league.get_player_by_id(main_key)\n    main_learner = learners[main_key]\n    main_collector = collectors[main_key]\n    evaluator1_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator1_cfg.stop_value = cfg.env.stop_value[0]\n    evaluator1 = BattleInteractionSerialEvaluator(evaluator1_cfg, evaluator_env1, [policies[main_key].collect_mode, eval_policy1], tb_logger, exp_name=cfg.exp_name, instance_name='fixed_evaluator')\n    evaluator2_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator2_cfg.stop_value = cfg.env.stop_value[1]\n    evaluator2 = BattleInteractionSerialEvaluator(evaluator2_cfg, evaluator_env2, [policies[main_key].collect_mode, eval_policy2], tb_logger, exp_name=cfg.exp_name, instance_name='uniform_evaluator')\n    evaluator3_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator3_cfg.stop_value = 99999999\n    evaluator3 = BattleInteractionSerialEvaluator(evaluator3_cfg, evaluator_env3, [policies[main_key].collect_mode, eval_policy3], tb_logger, exp_name=cfg.exp_name, instance_name='init_evaluator')\n\n    def load_checkpoint_fn(player_id: str, ckpt_path: str):\n        state_dict = torch.load(ckpt_path)\n        policies[player_id].learn_mode.load_state_dict(state_dict)\n    torch.save(policies['historical'].learn_mode.state_dict(), league.reset_checkpoint_path)\n    league.load_checkpoint = load_checkpoint_fn\n    for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n        torch.save(policies[player_id].collect_mode.state_dict(), player_ckpt_path)\n        league.judge_snapshot(player_id, force=True)\n    init_main_player_rating = league.metric_env.create_rating(mu=0)\n    count = 0\n    while True:\n        if evaluator1.should_eval(main_learner.train_iter):\n            (stop_flag1, episode_info) = evaluator1.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=10, sigma=1e-08), win_loss_result)\n        if evaluator2.should_eval(main_learner.train_iter):\n            (stop_flag2, episode_info) = evaluator2.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=0, sigma=1e-08), win_loss_result)\n        if evaluator3.should_eval(main_learner.train_iter):\n            (_, episode_info) = evaluator3.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            (main_player.rating, init_main_player_rating) = league.metric_env.rate_1vs1(main_player.rating, init_main_player_rating, win_loss_result)\n            tb_logger.add_scalar('league/init_main_player_trueskill', init_main_player_rating.exposure, main_collector.envstep)\n        if stop_flag1 and stop_flag2:\n            break\n        for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n            tb_logger.add_scalar('league/{}_trueskill'.format(player_id), league.get_player_by_id(player_id).rating.exposure, main_collector.envstep)\n            (collector, learner) = (collectors[player_id], learners[player_id])\n            job = league.get_job_info(player_id)\n            opponent_player_id = job['player_id'][1]\n            if 'historical' in opponent_player_id:\n                opponent_policy = policies['historical'].collect_mode\n                opponent_path = job['checkpoint_path'][1]\n                opponent_policy.load_state_dict(torch.load(opponent_path, map_location='cpu'))\n            else:\n                opponent_policy = policies[opponent_player_id].collect_mode\n            collector.reset_policy([policies[player_id].collect_mode, opponent_policy])\n            (train_data, episode_info) = collector.collect(train_iter=learner.train_iter)\n            (train_data, episode_info) = (train_data[0], episode_info[0])\n            for d in train_data:\n                d['adv'] = d['reward']\n            for i in range(cfg.policy.learn.update_per_collect):\n                learner.train(train_data, collector.envstep)\n            torch.save(learner.policy.state_dict(), player_ckpt_path)\n            player_info = learner.learn_info\n            player_info['player_id'] = player_id\n            league.update_active_player(player_info)\n            league.judge_snapshot(player_id)\n            job_finish_info = {'eval_flag': True, 'launch_player': job['launch_player'], 'player_id': job['player_id'], 'result': [e['result'] for e in episode_info]}\n            league.finish_job(job_finish_info)\n        if main_collector.envstep >= max_env_step or main_learner.train_iter >= max_train_iter:\n            break\n        if count % 100 == 0:\n            print(repr(league.payoff))\n        count += 1",
            "def main(cfg, seed=0, max_train_iter=int(100000000.0), max_env_step=int(100000000.0)):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    cfg = compile_config(cfg, BaseEnvManager, PPOPolicy, BaseLearner, LeagueDemoCollector, BattleInteractionSerialEvaluator, NaiveReplayBuffer, save_cfg=True)\n    env_type = cfg.env.env_type\n    (collector_env_num, evaluator_env_num) = (cfg.env.collector_env_num, cfg.env.evaluator_env_num)\n    evaluator_env1 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env2 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env3 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env1.seed(seed, dynamic_seed=False)\n    evaluator_env2.seed(seed, dynamic_seed=False)\n    evaluator_env3.seed(seed, dynamic_seed=False)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n    league = DemoLeague(cfg.policy.other.league)\n    eval_policy1 = EvalPolicy1(evaluator_env1._env_ref.optimal_policy)\n    eval_policy2 = EvalPolicy2()\n    policies = {}\n    learners = {}\n    collectors = {}\n    for player_id in league.active_players_ids:\n        model = VAC(**cfg.policy.model)\n        policy = PPOPolicy(cfg.policy, model=model)\n        policies[player_id] = policy\n        collector_env = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(collector_env_num)], cfg=cfg.env.manager)\n        collector_env.seed(seed)\n        learners[player_id] = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger=tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_learner')\n        collectors[player_id] = LeagueDemoCollector(cfg.policy.collect.collector, collector_env, tb_logger=tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_collector')\n    model = VAC(**cfg.policy.model)\n    policy = PPOPolicy(cfg.policy, model=model)\n    policies['historical'] = policy\n    eval_policy3 = PPOPolicy(cfg.policy, model=copy.deepcopy(model)).collect_mode\n    main_key = [k for k in learners.keys() if k.startswith('main_player')][0]\n    main_player = league.get_player_by_id(main_key)\n    main_learner = learners[main_key]\n    main_collector = collectors[main_key]\n    evaluator1_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator1_cfg.stop_value = cfg.env.stop_value[0]\n    evaluator1 = BattleInteractionSerialEvaluator(evaluator1_cfg, evaluator_env1, [policies[main_key].collect_mode, eval_policy1], tb_logger, exp_name=cfg.exp_name, instance_name='fixed_evaluator')\n    evaluator2_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator2_cfg.stop_value = cfg.env.stop_value[1]\n    evaluator2 = BattleInteractionSerialEvaluator(evaluator2_cfg, evaluator_env2, [policies[main_key].collect_mode, eval_policy2], tb_logger, exp_name=cfg.exp_name, instance_name='uniform_evaluator')\n    evaluator3_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator3_cfg.stop_value = 99999999\n    evaluator3 = BattleInteractionSerialEvaluator(evaluator3_cfg, evaluator_env3, [policies[main_key].collect_mode, eval_policy3], tb_logger, exp_name=cfg.exp_name, instance_name='init_evaluator')\n\n    def load_checkpoint_fn(player_id: str, ckpt_path: str):\n        state_dict = torch.load(ckpt_path)\n        policies[player_id].learn_mode.load_state_dict(state_dict)\n    torch.save(policies['historical'].learn_mode.state_dict(), league.reset_checkpoint_path)\n    league.load_checkpoint = load_checkpoint_fn\n    for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n        torch.save(policies[player_id].collect_mode.state_dict(), player_ckpt_path)\n        league.judge_snapshot(player_id, force=True)\n    init_main_player_rating = league.metric_env.create_rating(mu=0)\n    count = 0\n    while True:\n        if evaluator1.should_eval(main_learner.train_iter):\n            (stop_flag1, episode_info) = evaluator1.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=10, sigma=1e-08), win_loss_result)\n        if evaluator2.should_eval(main_learner.train_iter):\n            (stop_flag2, episode_info) = evaluator2.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=0, sigma=1e-08), win_loss_result)\n        if evaluator3.should_eval(main_learner.train_iter):\n            (_, episode_info) = evaluator3.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            (main_player.rating, init_main_player_rating) = league.metric_env.rate_1vs1(main_player.rating, init_main_player_rating, win_loss_result)\n            tb_logger.add_scalar('league/init_main_player_trueskill', init_main_player_rating.exposure, main_collector.envstep)\n        if stop_flag1 and stop_flag2:\n            break\n        for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n            tb_logger.add_scalar('league/{}_trueskill'.format(player_id), league.get_player_by_id(player_id).rating.exposure, main_collector.envstep)\n            (collector, learner) = (collectors[player_id], learners[player_id])\n            job = league.get_job_info(player_id)\n            opponent_player_id = job['player_id'][1]\n            if 'historical' in opponent_player_id:\n                opponent_policy = policies['historical'].collect_mode\n                opponent_path = job['checkpoint_path'][1]\n                opponent_policy.load_state_dict(torch.load(opponent_path, map_location='cpu'))\n            else:\n                opponent_policy = policies[opponent_player_id].collect_mode\n            collector.reset_policy([policies[player_id].collect_mode, opponent_policy])\n            (train_data, episode_info) = collector.collect(train_iter=learner.train_iter)\n            (train_data, episode_info) = (train_data[0], episode_info[0])\n            for d in train_data:\n                d['adv'] = d['reward']\n            for i in range(cfg.policy.learn.update_per_collect):\n                learner.train(train_data, collector.envstep)\n            torch.save(learner.policy.state_dict(), player_ckpt_path)\n            player_info = learner.learn_info\n            player_info['player_id'] = player_id\n            league.update_active_player(player_info)\n            league.judge_snapshot(player_id)\n            job_finish_info = {'eval_flag': True, 'launch_player': job['launch_player'], 'player_id': job['player_id'], 'result': [e['result'] for e in episode_info]}\n            league.finish_job(job_finish_info)\n        if main_collector.envstep >= max_env_step or main_learner.train_iter >= max_train_iter:\n            break\n        if count % 100 == 0:\n            print(repr(league.payoff))\n        count += 1",
            "def main(cfg, seed=0, max_train_iter=int(100000000.0), max_env_step=int(100000000.0)):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    cfg = compile_config(cfg, BaseEnvManager, PPOPolicy, BaseLearner, LeagueDemoCollector, BattleInteractionSerialEvaluator, NaiveReplayBuffer, save_cfg=True)\n    env_type = cfg.env.env_type\n    (collector_env_num, evaluator_env_num) = (cfg.env.collector_env_num, cfg.env.evaluator_env_num)\n    evaluator_env1 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env2 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env3 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env1.seed(seed, dynamic_seed=False)\n    evaluator_env2.seed(seed, dynamic_seed=False)\n    evaluator_env3.seed(seed, dynamic_seed=False)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n    league = DemoLeague(cfg.policy.other.league)\n    eval_policy1 = EvalPolicy1(evaluator_env1._env_ref.optimal_policy)\n    eval_policy2 = EvalPolicy2()\n    policies = {}\n    learners = {}\n    collectors = {}\n    for player_id in league.active_players_ids:\n        model = VAC(**cfg.policy.model)\n        policy = PPOPolicy(cfg.policy, model=model)\n        policies[player_id] = policy\n        collector_env = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(collector_env_num)], cfg=cfg.env.manager)\n        collector_env.seed(seed)\n        learners[player_id] = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger=tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_learner')\n        collectors[player_id] = LeagueDemoCollector(cfg.policy.collect.collector, collector_env, tb_logger=tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_collector')\n    model = VAC(**cfg.policy.model)\n    policy = PPOPolicy(cfg.policy, model=model)\n    policies['historical'] = policy\n    eval_policy3 = PPOPolicy(cfg.policy, model=copy.deepcopy(model)).collect_mode\n    main_key = [k for k in learners.keys() if k.startswith('main_player')][0]\n    main_player = league.get_player_by_id(main_key)\n    main_learner = learners[main_key]\n    main_collector = collectors[main_key]\n    evaluator1_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator1_cfg.stop_value = cfg.env.stop_value[0]\n    evaluator1 = BattleInteractionSerialEvaluator(evaluator1_cfg, evaluator_env1, [policies[main_key].collect_mode, eval_policy1], tb_logger, exp_name=cfg.exp_name, instance_name='fixed_evaluator')\n    evaluator2_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator2_cfg.stop_value = cfg.env.stop_value[1]\n    evaluator2 = BattleInteractionSerialEvaluator(evaluator2_cfg, evaluator_env2, [policies[main_key].collect_mode, eval_policy2], tb_logger, exp_name=cfg.exp_name, instance_name='uniform_evaluator')\n    evaluator3_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator3_cfg.stop_value = 99999999\n    evaluator3 = BattleInteractionSerialEvaluator(evaluator3_cfg, evaluator_env3, [policies[main_key].collect_mode, eval_policy3], tb_logger, exp_name=cfg.exp_name, instance_name='init_evaluator')\n\n    def load_checkpoint_fn(player_id: str, ckpt_path: str):\n        state_dict = torch.load(ckpt_path)\n        policies[player_id].learn_mode.load_state_dict(state_dict)\n    torch.save(policies['historical'].learn_mode.state_dict(), league.reset_checkpoint_path)\n    league.load_checkpoint = load_checkpoint_fn\n    for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n        torch.save(policies[player_id].collect_mode.state_dict(), player_ckpt_path)\n        league.judge_snapshot(player_id, force=True)\n    init_main_player_rating = league.metric_env.create_rating(mu=0)\n    count = 0\n    while True:\n        if evaluator1.should_eval(main_learner.train_iter):\n            (stop_flag1, episode_info) = evaluator1.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=10, sigma=1e-08), win_loss_result)\n        if evaluator2.should_eval(main_learner.train_iter):\n            (stop_flag2, episode_info) = evaluator2.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=0, sigma=1e-08), win_loss_result)\n        if evaluator3.should_eval(main_learner.train_iter):\n            (_, episode_info) = evaluator3.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            (main_player.rating, init_main_player_rating) = league.metric_env.rate_1vs1(main_player.rating, init_main_player_rating, win_loss_result)\n            tb_logger.add_scalar('league/init_main_player_trueskill', init_main_player_rating.exposure, main_collector.envstep)\n        if stop_flag1 and stop_flag2:\n            break\n        for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n            tb_logger.add_scalar('league/{}_trueskill'.format(player_id), league.get_player_by_id(player_id).rating.exposure, main_collector.envstep)\n            (collector, learner) = (collectors[player_id], learners[player_id])\n            job = league.get_job_info(player_id)\n            opponent_player_id = job['player_id'][1]\n            if 'historical' in opponent_player_id:\n                opponent_policy = policies['historical'].collect_mode\n                opponent_path = job['checkpoint_path'][1]\n                opponent_policy.load_state_dict(torch.load(opponent_path, map_location='cpu'))\n            else:\n                opponent_policy = policies[opponent_player_id].collect_mode\n            collector.reset_policy([policies[player_id].collect_mode, opponent_policy])\n            (train_data, episode_info) = collector.collect(train_iter=learner.train_iter)\n            (train_data, episode_info) = (train_data[0], episode_info[0])\n            for d in train_data:\n                d['adv'] = d['reward']\n            for i in range(cfg.policy.learn.update_per_collect):\n                learner.train(train_data, collector.envstep)\n            torch.save(learner.policy.state_dict(), player_ckpt_path)\n            player_info = learner.learn_info\n            player_info['player_id'] = player_id\n            league.update_active_player(player_info)\n            league.judge_snapshot(player_id)\n            job_finish_info = {'eval_flag': True, 'launch_player': job['launch_player'], 'player_id': job['player_id'], 'result': [e['result'] for e in episode_info]}\n            league.finish_job(job_finish_info)\n        if main_collector.envstep >= max_env_step or main_learner.train_iter >= max_train_iter:\n            break\n        if count % 100 == 0:\n            print(repr(league.payoff))\n        count += 1",
            "def main(cfg, seed=0, max_train_iter=int(100000000.0), max_env_step=int(100000000.0)):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    cfg = compile_config(cfg, BaseEnvManager, PPOPolicy, BaseLearner, LeagueDemoCollector, BattleInteractionSerialEvaluator, NaiveReplayBuffer, save_cfg=True)\n    env_type = cfg.env.env_type\n    (collector_env_num, evaluator_env_num) = (cfg.env.collector_env_num, cfg.env.evaluator_env_num)\n    evaluator_env1 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env2 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env3 = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(evaluator_env_num)], cfg=cfg.env.manager)\n    evaluator_env1.seed(seed, dynamic_seed=False)\n    evaluator_env2.seed(seed, dynamic_seed=False)\n    evaluator_env3.seed(seed, dynamic_seed=False)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n    league = DemoLeague(cfg.policy.other.league)\n    eval_policy1 = EvalPolicy1(evaluator_env1._env_ref.optimal_policy)\n    eval_policy2 = EvalPolicy2()\n    policies = {}\n    learners = {}\n    collectors = {}\n    for player_id in league.active_players_ids:\n        model = VAC(**cfg.policy.model)\n        policy = PPOPolicy(cfg.policy, model=model)\n        policies[player_id] = policy\n        collector_env = BaseEnvManager(env_fn=[lambda : GameEnv(env_type) for _ in range(collector_env_num)], cfg=cfg.env.manager)\n        collector_env.seed(seed)\n        learners[player_id] = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger=tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_learner')\n        collectors[player_id] = LeagueDemoCollector(cfg.policy.collect.collector, collector_env, tb_logger=tb_logger, exp_name=cfg.exp_name, instance_name=player_id + '_collector')\n    model = VAC(**cfg.policy.model)\n    policy = PPOPolicy(cfg.policy, model=model)\n    policies['historical'] = policy\n    eval_policy3 = PPOPolicy(cfg.policy, model=copy.deepcopy(model)).collect_mode\n    main_key = [k for k in learners.keys() if k.startswith('main_player')][0]\n    main_player = league.get_player_by_id(main_key)\n    main_learner = learners[main_key]\n    main_collector = collectors[main_key]\n    evaluator1_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator1_cfg.stop_value = cfg.env.stop_value[0]\n    evaluator1 = BattleInteractionSerialEvaluator(evaluator1_cfg, evaluator_env1, [policies[main_key].collect_mode, eval_policy1], tb_logger, exp_name=cfg.exp_name, instance_name='fixed_evaluator')\n    evaluator2_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator2_cfg.stop_value = cfg.env.stop_value[1]\n    evaluator2 = BattleInteractionSerialEvaluator(evaluator2_cfg, evaluator_env2, [policies[main_key].collect_mode, eval_policy2], tb_logger, exp_name=cfg.exp_name, instance_name='uniform_evaluator')\n    evaluator3_cfg = copy.deepcopy(cfg.policy.eval.evaluator)\n    evaluator3_cfg.stop_value = 99999999\n    evaluator3 = BattleInteractionSerialEvaluator(evaluator3_cfg, evaluator_env3, [policies[main_key].collect_mode, eval_policy3], tb_logger, exp_name=cfg.exp_name, instance_name='init_evaluator')\n\n    def load_checkpoint_fn(player_id: str, ckpt_path: str):\n        state_dict = torch.load(ckpt_path)\n        policies[player_id].learn_mode.load_state_dict(state_dict)\n    torch.save(policies['historical'].learn_mode.state_dict(), league.reset_checkpoint_path)\n    league.load_checkpoint = load_checkpoint_fn\n    for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n        torch.save(policies[player_id].collect_mode.state_dict(), player_ckpt_path)\n        league.judge_snapshot(player_id, force=True)\n    init_main_player_rating = league.metric_env.create_rating(mu=0)\n    count = 0\n    while True:\n        if evaluator1.should_eval(main_learner.train_iter):\n            (stop_flag1, episode_info) = evaluator1.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=10, sigma=1e-08), win_loss_result)\n        if evaluator2.should_eval(main_learner.train_iter):\n            (stop_flag2, episode_info) = evaluator2.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            main_player.rating = league.metric_env.rate_1vsC(main_player.rating, league.metric_env.create_rating(mu=0, sigma=1e-08), win_loss_result)\n        if evaluator3.should_eval(main_learner.train_iter):\n            (_, episode_info) = evaluator3.eval(main_learner.save_checkpoint, main_learner.train_iter, main_collector.envstep)\n            win_loss_result = [e['result'] for e in episode_info[0]]\n            (main_player.rating, init_main_player_rating) = league.metric_env.rate_1vs1(main_player.rating, init_main_player_rating, win_loss_result)\n            tb_logger.add_scalar('league/init_main_player_trueskill', init_main_player_rating.exposure, main_collector.envstep)\n        if stop_flag1 and stop_flag2:\n            break\n        for (player_id, player_ckpt_path) in zip(league.active_players_ids, league.active_players_ckpts):\n            tb_logger.add_scalar('league/{}_trueskill'.format(player_id), league.get_player_by_id(player_id).rating.exposure, main_collector.envstep)\n            (collector, learner) = (collectors[player_id], learners[player_id])\n            job = league.get_job_info(player_id)\n            opponent_player_id = job['player_id'][1]\n            if 'historical' in opponent_player_id:\n                opponent_policy = policies['historical'].collect_mode\n                opponent_path = job['checkpoint_path'][1]\n                opponent_policy.load_state_dict(torch.load(opponent_path, map_location='cpu'))\n            else:\n                opponent_policy = policies[opponent_player_id].collect_mode\n            collector.reset_policy([policies[player_id].collect_mode, opponent_policy])\n            (train_data, episode_info) = collector.collect(train_iter=learner.train_iter)\n            (train_data, episode_info) = (train_data[0], episode_info[0])\n            for d in train_data:\n                d['adv'] = d['reward']\n            for i in range(cfg.policy.learn.update_per_collect):\n                learner.train(train_data, collector.envstep)\n            torch.save(learner.policy.state_dict(), player_ckpt_path)\n            player_info = learner.learn_info\n            player_info['player_id'] = player_id\n            league.update_active_player(player_info)\n            league.judge_snapshot(player_id)\n            job_finish_info = {'eval_flag': True, 'launch_player': job['launch_player'], 'player_id': job['player_id'], 'result': [e['result'] for e in episode_info]}\n            league.finish_job(job_finish_info)\n        if main_collector.envstep >= max_env_step or main_learner.train_iter >= max_train_iter:\n            break\n        if count % 100 == 0:\n            print(repr(league.payoff))\n        count += 1"
        ]
    }
]