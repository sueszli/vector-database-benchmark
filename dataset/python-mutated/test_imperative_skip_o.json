[
    {
        "func_name": "test_out_scale_acc",
        "original": "def test_out_scale_acc(self):\n    paddle.disable_static()\n    seed = 1000\n    lr = 0.1\n    qat = ImperativeQuantAware()\n    np.random.seed(seed)\n    reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=512, drop_last=True)\n    lenet = ImperativeLenetWithSkipQuant()\n    lenet = fix_model_dict(lenet)\n    qat.quantize(lenet)\n    adam = Adam(learning_rate=lr, parameters=lenet.parameters())\n    dynamic_loss_rec = []\n    lenet.train()\n    loss_list = train_lenet(lenet, reader, adam)\n    lenet.eval()\n    path = './save_dynamic_quant_infer_model/lenet'\n    save_dir = './save_dynamic_quant_infer_model'\n    qat.save_quantized_model(layer=lenet, path=path, input_spec=[paddle.static.InputSpec(shape=[None, 1, 28, 28], dtype='float32')])\n    paddle.enable_static()\n    if core.is_compiled_with_cuda():\n        place = core.CUDAPlace(0)\n    else:\n        place = core.CPUPlace()\n    exe = paddle.static.Executor(place)\n    [inference_program, feed_target_names, fetch_targets] = paddle.static.load_inference_model(save_dir, executor=exe, model_filename='lenet' + INFER_MODEL_SUFFIX, params_filename='lenet' + INFER_PARAMS_SUFFIX)\n    model_ops = inference_program.global_block().ops\n    (conv2d_count, matmul_count) = (0, 0)\n    (conv2d_skip_count, matmul_skip_count) = (0, 0)\n    find_conv2d = False\n    find_matmul = False\n    for (i, op) in enumerate(model_ops):\n        if op.type == 'conv2d':\n            find_conv2d = True\n            if op.has_attr('skip_quant'):\n                conv2d_skip_count += 1\n            if conv2d_count > 0:\n                self.assertTrue('fake_quantize_dequantize' in model_ops[i - 1].type)\n            else:\n                self.assertTrue('fake_quantize_dequantize' not in model_ops[i - 1].type)\n            conv2d_count += 1\n        if op.type == 'matmul':\n            find_matmul = True\n            if op.has_attr('skip_quant'):\n                matmul_skip_count += 1\n            if matmul_count > 0:\n                self.assertTrue('fake_quantize_dequantize' in model_ops[i - 1].type)\n            else:\n                self.assertTrue('fake_quantize_dequantize' not in model_ops[i - 1].type)\n            matmul_count += 1\n    if find_conv2d:\n        self.assertTrue(conv2d_skip_count == 1)\n    if find_matmul:\n        self.assertTrue(matmul_skip_count == 1)",
        "mutated": [
            "def test_out_scale_acc(self):\n    if False:\n        i = 10\n    paddle.disable_static()\n    seed = 1000\n    lr = 0.1\n    qat = ImperativeQuantAware()\n    np.random.seed(seed)\n    reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=512, drop_last=True)\n    lenet = ImperativeLenetWithSkipQuant()\n    lenet = fix_model_dict(lenet)\n    qat.quantize(lenet)\n    adam = Adam(learning_rate=lr, parameters=lenet.parameters())\n    dynamic_loss_rec = []\n    lenet.train()\n    loss_list = train_lenet(lenet, reader, adam)\n    lenet.eval()\n    path = './save_dynamic_quant_infer_model/lenet'\n    save_dir = './save_dynamic_quant_infer_model'\n    qat.save_quantized_model(layer=lenet, path=path, input_spec=[paddle.static.InputSpec(shape=[None, 1, 28, 28], dtype='float32')])\n    paddle.enable_static()\n    if core.is_compiled_with_cuda():\n        place = core.CUDAPlace(0)\n    else:\n        place = core.CPUPlace()\n    exe = paddle.static.Executor(place)\n    [inference_program, feed_target_names, fetch_targets] = paddle.static.load_inference_model(save_dir, executor=exe, model_filename='lenet' + INFER_MODEL_SUFFIX, params_filename='lenet' + INFER_PARAMS_SUFFIX)\n    model_ops = inference_program.global_block().ops\n    (conv2d_count, matmul_count) = (0, 0)\n    (conv2d_skip_count, matmul_skip_count) = (0, 0)\n    find_conv2d = False\n    find_matmul = False\n    for (i, op) in enumerate(model_ops):\n        if op.type == 'conv2d':\n            find_conv2d = True\n            if op.has_attr('skip_quant'):\n                conv2d_skip_count += 1\n            if conv2d_count > 0:\n                self.assertTrue('fake_quantize_dequantize' in model_ops[i - 1].type)\n            else:\n                self.assertTrue('fake_quantize_dequantize' not in model_ops[i - 1].type)\n            conv2d_count += 1\n        if op.type == 'matmul':\n            find_matmul = True\n            if op.has_attr('skip_quant'):\n                matmul_skip_count += 1\n            if matmul_count > 0:\n                self.assertTrue('fake_quantize_dequantize' in model_ops[i - 1].type)\n            else:\n                self.assertTrue('fake_quantize_dequantize' not in model_ops[i - 1].type)\n            matmul_count += 1\n    if find_conv2d:\n        self.assertTrue(conv2d_skip_count == 1)\n    if find_matmul:\n        self.assertTrue(matmul_skip_count == 1)",
            "def test_out_scale_acc(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    paddle.disable_static()\n    seed = 1000\n    lr = 0.1\n    qat = ImperativeQuantAware()\n    np.random.seed(seed)\n    reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=512, drop_last=True)\n    lenet = ImperativeLenetWithSkipQuant()\n    lenet = fix_model_dict(lenet)\n    qat.quantize(lenet)\n    adam = Adam(learning_rate=lr, parameters=lenet.parameters())\n    dynamic_loss_rec = []\n    lenet.train()\n    loss_list = train_lenet(lenet, reader, adam)\n    lenet.eval()\n    path = './save_dynamic_quant_infer_model/lenet'\n    save_dir = './save_dynamic_quant_infer_model'\n    qat.save_quantized_model(layer=lenet, path=path, input_spec=[paddle.static.InputSpec(shape=[None, 1, 28, 28], dtype='float32')])\n    paddle.enable_static()\n    if core.is_compiled_with_cuda():\n        place = core.CUDAPlace(0)\n    else:\n        place = core.CPUPlace()\n    exe = paddle.static.Executor(place)\n    [inference_program, feed_target_names, fetch_targets] = paddle.static.load_inference_model(save_dir, executor=exe, model_filename='lenet' + INFER_MODEL_SUFFIX, params_filename='lenet' + INFER_PARAMS_SUFFIX)\n    model_ops = inference_program.global_block().ops\n    (conv2d_count, matmul_count) = (0, 0)\n    (conv2d_skip_count, matmul_skip_count) = (0, 0)\n    find_conv2d = False\n    find_matmul = False\n    for (i, op) in enumerate(model_ops):\n        if op.type == 'conv2d':\n            find_conv2d = True\n            if op.has_attr('skip_quant'):\n                conv2d_skip_count += 1\n            if conv2d_count > 0:\n                self.assertTrue('fake_quantize_dequantize' in model_ops[i - 1].type)\n            else:\n                self.assertTrue('fake_quantize_dequantize' not in model_ops[i - 1].type)\n            conv2d_count += 1\n        if op.type == 'matmul':\n            find_matmul = True\n            if op.has_attr('skip_quant'):\n                matmul_skip_count += 1\n            if matmul_count > 0:\n                self.assertTrue('fake_quantize_dequantize' in model_ops[i - 1].type)\n            else:\n                self.assertTrue('fake_quantize_dequantize' not in model_ops[i - 1].type)\n            matmul_count += 1\n    if find_conv2d:\n        self.assertTrue(conv2d_skip_count == 1)\n    if find_matmul:\n        self.assertTrue(matmul_skip_count == 1)",
            "def test_out_scale_acc(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    paddle.disable_static()\n    seed = 1000\n    lr = 0.1\n    qat = ImperativeQuantAware()\n    np.random.seed(seed)\n    reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=512, drop_last=True)\n    lenet = ImperativeLenetWithSkipQuant()\n    lenet = fix_model_dict(lenet)\n    qat.quantize(lenet)\n    adam = Adam(learning_rate=lr, parameters=lenet.parameters())\n    dynamic_loss_rec = []\n    lenet.train()\n    loss_list = train_lenet(lenet, reader, adam)\n    lenet.eval()\n    path = './save_dynamic_quant_infer_model/lenet'\n    save_dir = './save_dynamic_quant_infer_model'\n    qat.save_quantized_model(layer=lenet, path=path, input_spec=[paddle.static.InputSpec(shape=[None, 1, 28, 28], dtype='float32')])\n    paddle.enable_static()\n    if core.is_compiled_with_cuda():\n        place = core.CUDAPlace(0)\n    else:\n        place = core.CPUPlace()\n    exe = paddle.static.Executor(place)\n    [inference_program, feed_target_names, fetch_targets] = paddle.static.load_inference_model(save_dir, executor=exe, model_filename='lenet' + INFER_MODEL_SUFFIX, params_filename='lenet' + INFER_PARAMS_SUFFIX)\n    model_ops = inference_program.global_block().ops\n    (conv2d_count, matmul_count) = (0, 0)\n    (conv2d_skip_count, matmul_skip_count) = (0, 0)\n    find_conv2d = False\n    find_matmul = False\n    for (i, op) in enumerate(model_ops):\n        if op.type == 'conv2d':\n            find_conv2d = True\n            if op.has_attr('skip_quant'):\n                conv2d_skip_count += 1\n            if conv2d_count > 0:\n                self.assertTrue('fake_quantize_dequantize' in model_ops[i - 1].type)\n            else:\n                self.assertTrue('fake_quantize_dequantize' not in model_ops[i - 1].type)\n            conv2d_count += 1\n        if op.type == 'matmul':\n            find_matmul = True\n            if op.has_attr('skip_quant'):\n                matmul_skip_count += 1\n            if matmul_count > 0:\n                self.assertTrue('fake_quantize_dequantize' in model_ops[i - 1].type)\n            else:\n                self.assertTrue('fake_quantize_dequantize' not in model_ops[i - 1].type)\n            matmul_count += 1\n    if find_conv2d:\n        self.assertTrue(conv2d_skip_count == 1)\n    if find_matmul:\n        self.assertTrue(matmul_skip_count == 1)",
            "def test_out_scale_acc(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    paddle.disable_static()\n    seed = 1000\n    lr = 0.1\n    qat = ImperativeQuantAware()\n    np.random.seed(seed)\n    reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=512, drop_last=True)\n    lenet = ImperativeLenetWithSkipQuant()\n    lenet = fix_model_dict(lenet)\n    qat.quantize(lenet)\n    adam = Adam(learning_rate=lr, parameters=lenet.parameters())\n    dynamic_loss_rec = []\n    lenet.train()\n    loss_list = train_lenet(lenet, reader, adam)\n    lenet.eval()\n    path = './save_dynamic_quant_infer_model/lenet'\n    save_dir = './save_dynamic_quant_infer_model'\n    qat.save_quantized_model(layer=lenet, path=path, input_spec=[paddle.static.InputSpec(shape=[None, 1, 28, 28], dtype='float32')])\n    paddle.enable_static()\n    if core.is_compiled_with_cuda():\n        place = core.CUDAPlace(0)\n    else:\n        place = core.CPUPlace()\n    exe = paddle.static.Executor(place)\n    [inference_program, feed_target_names, fetch_targets] = paddle.static.load_inference_model(save_dir, executor=exe, model_filename='lenet' + INFER_MODEL_SUFFIX, params_filename='lenet' + INFER_PARAMS_SUFFIX)\n    model_ops = inference_program.global_block().ops\n    (conv2d_count, matmul_count) = (0, 0)\n    (conv2d_skip_count, matmul_skip_count) = (0, 0)\n    find_conv2d = False\n    find_matmul = False\n    for (i, op) in enumerate(model_ops):\n        if op.type == 'conv2d':\n            find_conv2d = True\n            if op.has_attr('skip_quant'):\n                conv2d_skip_count += 1\n            if conv2d_count > 0:\n                self.assertTrue('fake_quantize_dequantize' in model_ops[i - 1].type)\n            else:\n                self.assertTrue('fake_quantize_dequantize' not in model_ops[i - 1].type)\n            conv2d_count += 1\n        if op.type == 'matmul':\n            find_matmul = True\n            if op.has_attr('skip_quant'):\n                matmul_skip_count += 1\n            if matmul_count > 0:\n                self.assertTrue('fake_quantize_dequantize' in model_ops[i - 1].type)\n            else:\n                self.assertTrue('fake_quantize_dequantize' not in model_ops[i - 1].type)\n            matmul_count += 1\n    if find_conv2d:\n        self.assertTrue(conv2d_skip_count == 1)\n    if find_matmul:\n        self.assertTrue(matmul_skip_count == 1)",
            "def test_out_scale_acc(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    paddle.disable_static()\n    seed = 1000\n    lr = 0.1\n    qat = ImperativeQuantAware()\n    np.random.seed(seed)\n    reader = paddle.batch(paddle.dataset.mnist.test(), batch_size=512, drop_last=True)\n    lenet = ImperativeLenetWithSkipQuant()\n    lenet = fix_model_dict(lenet)\n    qat.quantize(lenet)\n    adam = Adam(learning_rate=lr, parameters=lenet.parameters())\n    dynamic_loss_rec = []\n    lenet.train()\n    loss_list = train_lenet(lenet, reader, adam)\n    lenet.eval()\n    path = './save_dynamic_quant_infer_model/lenet'\n    save_dir = './save_dynamic_quant_infer_model'\n    qat.save_quantized_model(layer=lenet, path=path, input_spec=[paddle.static.InputSpec(shape=[None, 1, 28, 28], dtype='float32')])\n    paddle.enable_static()\n    if core.is_compiled_with_cuda():\n        place = core.CUDAPlace(0)\n    else:\n        place = core.CPUPlace()\n    exe = paddle.static.Executor(place)\n    [inference_program, feed_target_names, fetch_targets] = paddle.static.load_inference_model(save_dir, executor=exe, model_filename='lenet' + INFER_MODEL_SUFFIX, params_filename='lenet' + INFER_PARAMS_SUFFIX)\n    model_ops = inference_program.global_block().ops\n    (conv2d_count, matmul_count) = (0, 0)\n    (conv2d_skip_count, matmul_skip_count) = (0, 0)\n    find_conv2d = False\n    find_matmul = False\n    for (i, op) in enumerate(model_ops):\n        if op.type == 'conv2d':\n            find_conv2d = True\n            if op.has_attr('skip_quant'):\n                conv2d_skip_count += 1\n            if conv2d_count > 0:\n                self.assertTrue('fake_quantize_dequantize' in model_ops[i - 1].type)\n            else:\n                self.assertTrue('fake_quantize_dequantize' not in model_ops[i - 1].type)\n            conv2d_count += 1\n        if op.type == 'matmul':\n            find_matmul = True\n            if op.has_attr('skip_quant'):\n                matmul_skip_count += 1\n            if matmul_count > 0:\n                self.assertTrue('fake_quantize_dequantize' in model_ops[i - 1].type)\n            else:\n                self.assertTrue('fake_quantize_dequantize' not in model_ops[i - 1].type)\n            matmul_count += 1\n    if find_conv2d:\n        self.assertTrue(conv2d_skip_count == 1)\n    if find_matmul:\n        self.assertTrue(matmul_skip_count == 1)"
        ]
    }
]