[
    {
        "func_name": "_is_qat_saved_model",
        "original": "def _is_qat_saved_model(saved_model_path: str):\n    \"\"\"Checks if the SavedModel is QAT-enabled by looking for 'FakeQuant' ops.\"\"\"\n    saved_model_proto = saved_model_loader.parse_saved_model(saved_model_path)\n    for meta_graph in saved_model_proto.meta_graphs:\n        if any((node.op.startswith('FakeQuant') for node in meta_graph.graph_def.node)):\n            return True\n        for function in meta_graph.graph_def.library.function:\n            if any((node.op.startswith('FakeQuant') for node in function.node_def)):\n                return True\n    return False",
        "mutated": [
            "def _is_qat_saved_model(saved_model_path: str):\n    if False:\n        i = 10\n    \"Checks if the SavedModel is QAT-enabled by looking for 'FakeQuant' ops.\"\n    saved_model_proto = saved_model_loader.parse_saved_model(saved_model_path)\n    for meta_graph in saved_model_proto.meta_graphs:\n        if any((node.op.startswith('FakeQuant') for node in meta_graph.graph_def.node)):\n            return True\n        for function in meta_graph.graph_def.library.function:\n            if any((node.op.startswith('FakeQuant') for node in function.node_def)):\n                return True\n    return False",
            "def _is_qat_saved_model(saved_model_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Checks if the SavedModel is QAT-enabled by looking for 'FakeQuant' ops.\"\n    saved_model_proto = saved_model_loader.parse_saved_model(saved_model_path)\n    for meta_graph in saved_model_proto.meta_graphs:\n        if any((node.op.startswith('FakeQuant') for node in meta_graph.graph_def.node)):\n            return True\n        for function in meta_graph.graph_def.library.function:\n            if any((node.op.startswith('FakeQuant') for node in function.node_def)):\n                return True\n    return False",
            "def _is_qat_saved_model(saved_model_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Checks if the SavedModel is QAT-enabled by looking for 'FakeQuant' ops.\"\n    saved_model_proto = saved_model_loader.parse_saved_model(saved_model_path)\n    for meta_graph in saved_model_proto.meta_graphs:\n        if any((node.op.startswith('FakeQuant') for node in meta_graph.graph_def.node)):\n            return True\n        for function in meta_graph.graph_def.library.function:\n            if any((node.op.startswith('FakeQuant') for node in function.node_def)):\n                return True\n    return False",
            "def _is_qat_saved_model(saved_model_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Checks if the SavedModel is QAT-enabled by looking for 'FakeQuant' ops.\"\n    saved_model_proto = saved_model_loader.parse_saved_model(saved_model_path)\n    for meta_graph in saved_model_proto.meta_graphs:\n        if any((node.op.startswith('FakeQuant') for node in meta_graph.graph_def.node)):\n            return True\n        for function in meta_graph.graph_def.library.function:\n            if any((node.op.startswith('FakeQuant') for node in function.node_def)):\n                return True\n    return False",
            "def _is_qat_saved_model(saved_model_path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Checks if the SavedModel is QAT-enabled by looking for 'FakeQuant' ops.\"\n    saved_model_proto = saved_model_loader.parse_saved_model(saved_model_path)\n    for meta_graph in saved_model_proto.meta_graphs:\n        if any((node.op.startswith('FakeQuant') for node in meta_graph.graph_def.node)):\n            return True\n        for function in meta_graph.graph_def.library.function:\n            if any((node.op.startswith('FakeQuant') for node in function.node_def)):\n                return True\n    return False"
        ]
    },
    {
        "func_name": "_serialize_signature_def_map",
        "original": "def _serialize_signature_def_map(signature_def_map: _SignatureDefMap) -> dict[str, bytes]:\n    \"\"\"Serializes SignatureDef values in `signature_def_map`.\n\n  Args:\n    signature_def_map: Signature key -> SignatureDef mapping.\n\n  Returns:\n    Signature def map where the values (`SignatureDef`) are serialized.\n  \"\"\"\n    signature_def_map_serialized = {}\n    for (key, signature_def) in signature_def_map.items():\n        signature_def_map_serialized[key] = signature_def.SerializeToString()\n    return signature_def_map_serialized",
        "mutated": [
            "def _serialize_signature_def_map(signature_def_map: _SignatureDefMap) -> dict[str, bytes]:\n    if False:\n        i = 10\n    'Serializes SignatureDef values in `signature_def_map`.\\n\\n  Args:\\n    signature_def_map: Signature key -> SignatureDef mapping.\\n\\n  Returns:\\n    Signature def map where the values (`SignatureDef`) are serialized.\\n  '\n    signature_def_map_serialized = {}\n    for (key, signature_def) in signature_def_map.items():\n        signature_def_map_serialized[key] = signature_def.SerializeToString()\n    return signature_def_map_serialized",
            "def _serialize_signature_def_map(signature_def_map: _SignatureDefMap) -> dict[str, bytes]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Serializes SignatureDef values in `signature_def_map`.\\n\\n  Args:\\n    signature_def_map: Signature key -> SignatureDef mapping.\\n\\n  Returns:\\n    Signature def map where the values (`SignatureDef`) are serialized.\\n  '\n    signature_def_map_serialized = {}\n    for (key, signature_def) in signature_def_map.items():\n        signature_def_map_serialized[key] = signature_def.SerializeToString()\n    return signature_def_map_serialized",
            "def _serialize_signature_def_map(signature_def_map: _SignatureDefMap) -> dict[str, bytes]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Serializes SignatureDef values in `signature_def_map`.\\n\\n  Args:\\n    signature_def_map: Signature key -> SignatureDef mapping.\\n\\n  Returns:\\n    Signature def map where the values (`SignatureDef`) are serialized.\\n  '\n    signature_def_map_serialized = {}\n    for (key, signature_def) in signature_def_map.items():\n        signature_def_map_serialized[key] = signature_def.SerializeToString()\n    return signature_def_map_serialized",
            "def _serialize_signature_def_map(signature_def_map: _SignatureDefMap) -> dict[str, bytes]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Serializes SignatureDef values in `signature_def_map`.\\n\\n  Args:\\n    signature_def_map: Signature key -> SignatureDef mapping.\\n\\n  Returns:\\n    Signature def map where the values (`SignatureDef`) are serialized.\\n  '\n    signature_def_map_serialized = {}\n    for (key, signature_def) in signature_def_map.items():\n        signature_def_map_serialized[key] = signature_def.SerializeToString()\n    return signature_def_map_serialized",
            "def _serialize_signature_def_map(signature_def_map: _SignatureDefMap) -> dict[str, bytes]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Serializes SignatureDef values in `signature_def_map`.\\n\\n  Args:\\n    signature_def_map: Signature key -> SignatureDef mapping.\\n\\n  Returns:\\n    Signature def map where the values (`SignatureDef`) are serialized.\\n  '\n    signature_def_map_serialized = {}\n    for (key, signature_def) in signature_def_map.items():\n        signature_def_map_serialized[key] = signature_def.SerializeToString()\n    return signature_def_map_serialized"
        ]
    },
    {
        "func_name": "_run_static_range_qat",
        "original": "def _run_static_range_qat(src_saved_model_path: str, dst_saved_model_path: str, quant_opts: _QuantizationOptions, signature_def_map: _SignatureDefMap) -> None:\n    \"\"\"Runs static-range quantization for a Quantization-Aware Trained model.\n\n  Runs the quantization for a model trained using QAT.\n\n  Args:\n    src_saved_model_path: Path to the source SavedModel directory.\n    dst_saved_model_path: Path to the destination SavedModel directory.\n    quant_opts: Quantization options.\n    signature_def_map: Signature def key -> SignatureDef mapping.\n  \"\"\"\n    logging.info('Running static-range quantization for QAT model.')\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quant_opts.tags).meta_info_def.function_aliases\n    pywrap_quantize_model.quantize_qat_model(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())",
        "mutated": [
            "def _run_static_range_qat(src_saved_model_path: str, dst_saved_model_path: str, quant_opts: _QuantizationOptions, signature_def_map: _SignatureDefMap) -> None:\n    if False:\n        i = 10\n    'Runs static-range quantization for a Quantization-Aware Trained model.\\n\\n  Runs the quantization for a model trained using QAT.\\n\\n  Args:\\n    src_saved_model_path: Path to the source SavedModel directory.\\n    dst_saved_model_path: Path to the destination SavedModel directory.\\n    quant_opts: Quantization options.\\n    signature_def_map: Signature def key -> SignatureDef mapping.\\n  '\n    logging.info('Running static-range quantization for QAT model.')\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quant_opts.tags).meta_info_def.function_aliases\n    pywrap_quantize_model.quantize_qat_model(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())",
            "def _run_static_range_qat(src_saved_model_path: str, dst_saved_model_path: str, quant_opts: _QuantizationOptions, signature_def_map: _SignatureDefMap) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Runs static-range quantization for a Quantization-Aware Trained model.\\n\\n  Runs the quantization for a model trained using QAT.\\n\\n  Args:\\n    src_saved_model_path: Path to the source SavedModel directory.\\n    dst_saved_model_path: Path to the destination SavedModel directory.\\n    quant_opts: Quantization options.\\n    signature_def_map: Signature def key -> SignatureDef mapping.\\n  '\n    logging.info('Running static-range quantization for QAT model.')\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quant_opts.tags).meta_info_def.function_aliases\n    pywrap_quantize_model.quantize_qat_model(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())",
            "def _run_static_range_qat(src_saved_model_path: str, dst_saved_model_path: str, quant_opts: _QuantizationOptions, signature_def_map: _SignatureDefMap) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Runs static-range quantization for a Quantization-Aware Trained model.\\n\\n  Runs the quantization for a model trained using QAT.\\n\\n  Args:\\n    src_saved_model_path: Path to the source SavedModel directory.\\n    dst_saved_model_path: Path to the destination SavedModel directory.\\n    quant_opts: Quantization options.\\n    signature_def_map: Signature def key -> SignatureDef mapping.\\n  '\n    logging.info('Running static-range quantization for QAT model.')\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quant_opts.tags).meta_info_def.function_aliases\n    pywrap_quantize_model.quantize_qat_model(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())",
            "def _run_static_range_qat(src_saved_model_path: str, dst_saved_model_path: str, quant_opts: _QuantizationOptions, signature_def_map: _SignatureDefMap) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Runs static-range quantization for a Quantization-Aware Trained model.\\n\\n  Runs the quantization for a model trained using QAT.\\n\\n  Args:\\n    src_saved_model_path: Path to the source SavedModel directory.\\n    dst_saved_model_path: Path to the destination SavedModel directory.\\n    quant_opts: Quantization options.\\n    signature_def_map: Signature def key -> SignatureDef mapping.\\n  '\n    logging.info('Running static-range quantization for QAT model.')\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quant_opts.tags).meta_info_def.function_aliases\n    pywrap_quantize_model.quantize_qat_model(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())",
            "def _run_static_range_qat(src_saved_model_path: str, dst_saved_model_path: str, quant_opts: _QuantizationOptions, signature_def_map: _SignatureDefMap) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Runs static-range quantization for a Quantization-Aware Trained model.\\n\\n  Runs the quantization for a model trained using QAT.\\n\\n  Args:\\n    src_saved_model_path: Path to the source SavedModel directory.\\n    dst_saved_model_path: Path to the destination SavedModel directory.\\n    quant_opts: Quantization options.\\n    signature_def_map: Signature def key -> SignatureDef mapping.\\n  '\n    logging.info('Running static-range quantization for QAT model.')\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quant_opts.tags).meta_info_def.function_aliases\n    pywrap_quantize_model.quantize_qat_model(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())"
        ]
    },
    {
        "func_name": "_get_min_max_from_calibrator",
        "original": "def _get_min_max_from_calibrator(node_id: bytes, calib_opts: quant_opts_pb2.CalibrationOptions) -> tuple[float, float]:\n    \"\"\"Calculate min and max from statistics using calibration options.\n\n  Args:\n    node_id: bytes of node id.\n    calib_opts: Calibration options used for calculating min and max.\n\n  Returns:\n    (min_value, max_value): Min and max calculated using calib_opts.\n\n  Raises:\n    ValueError: Unsupported calibration method is given.\n  \"\"\"\n    statistics: calib_stats_pb2.CalibrationStatistics = pywrap_calibration.get_statistics_from_calibrator(node_id)\n    (min_value, max_value) = calibration_algorithm.get_min_max_value(statistics, calib_opts)\n    return (min_value, max_value)",
        "mutated": [
            "def _get_min_max_from_calibrator(node_id: bytes, calib_opts: quant_opts_pb2.CalibrationOptions) -> tuple[float, float]:\n    if False:\n        i = 10\n    'Calculate min and max from statistics using calibration options.\\n\\n  Args:\\n    node_id: bytes of node id.\\n    calib_opts: Calibration options used for calculating min and max.\\n\\n  Returns:\\n    (min_value, max_value): Min and max calculated using calib_opts.\\n\\n  Raises:\\n    ValueError: Unsupported calibration method is given.\\n  '\n    statistics: calib_stats_pb2.CalibrationStatistics = pywrap_calibration.get_statistics_from_calibrator(node_id)\n    (min_value, max_value) = calibration_algorithm.get_min_max_value(statistics, calib_opts)\n    return (min_value, max_value)",
            "def _get_min_max_from_calibrator(node_id: bytes, calib_opts: quant_opts_pb2.CalibrationOptions) -> tuple[float, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calculate min and max from statistics using calibration options.\\n\\n  Args:\\n    node_id: bytes of node id.\\n    calib_opts: Calibration options used for calculating min and max.\\n\\n  Returns:\\n    (min_value, max_value): Min and max calculated using calib_opts.\\n\\n  Raises:\\n    ValueError: Unsupported calibration method is given.\\n  '\n    statistics: calib_stats_pb2.CalibrationStatistics = pywrap_calibration.get_statistics_from_calibrator(node_id)\n    (min_value, max_value) = calibration_algorithm.get_min_max_value(statistics, calib_opts)\n    return (min_value, max_value)",
            "def _get_min_max_from_calibrator(node_id: bytes, calib_opts: quant_opts_pb2.CalibrationOptions) -> tuple[float, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calculate min and max from statistics using calibration options.\\n\\n  Args:\\n    node_id: bytes of node id.\\n    calib_opts: Calibration options used for calculating min and max.\\n\\n  Returns:\\n    (min_value, max_value): Min and max calculated using calib_opts.\\n\\n  Raises:\\n    ValueError: Unsupported calibration method is given.\\n  '\n    statistics: calib_stats_pb2.CalibrationStatistics = pywrap_calibration.get_statistics_from_calibrator(node_id)\n    (min_value, max_value) = calibration_algorithm.get_min_max_value(statistics, calib_opts)\n    return (min_value, max_value)",
            "def _get_min_max_from_calibrator(node_id: bytes, calib_opts: quant_opts_pb2.CalibrationOptions) -> tuple[float, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calculate min and max from statistics using calibration options.\\n\\n  Args:\\n    node_id: bytes of node id.\\n    calib_opts: Calibration options used for calculating min and max.\\n\\n  Returns:\\n    (min_value, max_value): Min and max calculated using calib_opts.\\n\\n  Raises:\\n    ValueError: Unsupported calibration method is given.\\n  '\n    statistics: calib_stats_pb2.CalibrationStatistics = pywrap_calibration.get_statistics_from_calibrator(node_id)\n    (min_value, max_value) = calibration_algorithm.get_min_max_value(statistics, calib_opts)\n    return (min_value, max_value)",
            "def _get_min_max_from_calibrator(node_id: bytes, calib_opts: quant_opts_pb2.CalibrationOptions) -> tuple[float, float]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calculate min and max from statistics using calibration options.\\n\\n  Args:\\n    node_id: bytes of node id.\\n    calib_opts: Calibration options used for calculating min and max.\\n\\n  Returns:\\n    (min_value, max_value): Min and max calculated using calib_opts.\\n\\n  Raises:\\n    ValueError: Unsupported calibration method is given.\\n  '\n    statistics: calib_stats_pb2.CalibrationStatistics = pywrap_calibration.get_statistics_from_calibrator(node_id)\n    (min_value, max_value) = calibration_algorithm.get_min_max_value(statistics, calib_opts)\n    return (min_value, max_value)"
        ]
    },
    {
        "func_name": "_enable_dump_tensor",
        "original": "def _enable_dump_tensor(graph_def: graph_pb2.GraphDef) -> None:\n    \"\"\"Enable DumpTensor in the graph def.\n\n  DumpTensor is disabled by default to avoid logging data during calibration.\n  This function is called after calibration to enable DumpTensor.\n\n  Args:\n    graph_def: GraphDef to enable DumpTensor\n  \"\"\"\n    for function_def in graph_def.library.function:\n        for node_def in function_def.node_def:\n            if node_def.op != 'DumpTensor':\n                continue\n            node_def.attr['enabled'].b = True",
        "mutated": [
            "def _enable_dump_tensor(graph_def: graph_pb2.GraphDef) -> None:\n    if False:\n        i = 10\n    'Enable DumpTensor in the graph def.\\n\\n  DumpTensor is disabled by default to avoid logging data during calibration.\\n  This function is called after calibration to enable DumpTensor.\\n\\n  Args:\\n    graph_def: GraphDef to enable DumpTensor\\n  '\n    for function_def in graph_def.library.function:\n        for node_def in function_def.node_def:\n            if node_def.op != 'DumpTensor':\n                continue\n            node_def.attr['enabled'].b = True",
            "def _enable_dump_tensor(graph_def: graph_pb2.GraphDef) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Enable DumpTensor in the graph def.\\n\\n  DumpTensor is disabled by default to avoid logging data during calibration.\\n  This function is called after calibration to enable DumpTensor.\\n\\n  Args:\\n    graph_def: GraphDef to enable DumpTensor\\n  '\n    for function_def in graph_def.library.function:\n        for node_def in function_def.node_def:\n            if node_def.op != 'DumpTensor':\n                continue\n            node_def.attr['enabled'].b = True",
            "def _enable_dump_tensor(graph_def: graph_pb2.GraphDef) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Enable DumpTensor in the graph def.\\n\\n  DumpTensor is disabled by default to avoid logging data during calibration.\\n  This function is called after calibration to enable DumpTensor.\\n\\n  Args:\\n    graph_def: GraphDef to enable DumpTensor\\n  '\n    for function_def in graph_def.library.function:\n        for node_def in function_def.node_def:\n            if node_def.op != 'DumpTensor':\n                continue\n            node_def.attr['enabled'].b = True",
            "def _enable_dump_tensor(graph_def: graph_pb2.GraphDef) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Enable DumpTensor in the graph def.\\n\\n  DumpTensor is disabled by default to avoid logging data during calibration.\\n  This function is called after calibration to enable DumpTensor.\\n\\n  Args:\\n    graph_def: GraphDef to enable DumpTensor\\n  '\n    for function_def in graph_def.library.function:\n        for node_def in function_def.node_def:\n            if node_def.op != 'DumpTensor':\n                continue\n            node_def.attr['enabled'].b = True",
            "def _enable_dump_tensor(graph_def: graph_pb2.GraphDef) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Enable DumpTensor in the graph def.\\n\\n  DumpTensor is disabled by default to avoid logging data during calibration.\\n  This function is called after calibration to enable DumpTensor.\\n\\n  Args:\\n    graph_def: GraphDef to enable DumpTensor\\n  '\n    for function_def in graph_def.library.function:\n        for node_def in function_def.node_def:\n            if node_def.op != 'DumpTensor':\n                continue\n            node_def.attr['enabled'].b = True"
        ]
    },
    {
        "func_name": "_change_dump_tensor_file_name",
        "original": "def _change_dump_tensor_file_name(graph_def: graph_pb2.GraphDef) -> None:\n    \"\"\"Change file_name used by DumpTensor to quantized_tensor_data.pb.\n\n  In whole model verify, DumpTensor in unquantized model uses file_name\n  unquantized_tensor_data.pb.\n  After unquantized dump model is created, this function allows quantized dump\n  model to use quantized_tensor_data.pb as file_name.\n\n  Args:\n    graph_def: GraphDef to change file_name of DumpTensor\n  \"\"\"\n    for function_def in graph_def.library.function:\n        for node_def in function_def.node_def:\n            if node_def.op != 'DumpTensor':\n                continue\n            node_def.attr['file_name'].s = 'quantized_tensor_data.pb'.encode('utf-8')",
        "mutated": [
            "def _change_dump_tensor_file_name(graph_def: graph_pb2.GraphDef) -> None:\n    if False:\n        i = 10\n    'Change file_name used by DumpTensor to quantized_tensor_data.pb.\\n\\n  In whole model verify, DumpTensor in unquantized model uses file_name\\n  unquantized_tensor_data.pb.\\n  After unquantized dump model is created, this function allows quantized dump\\n  model to use quantized_tensor_data.pb as file_name.\\n\\n  Args:\\n    graph_def: GraphDef to change file_name of DumpTensor\\n  '\n    for function_def in graph_def.library.function:\n        for node_def in function_def.node_def:\n            if node_def.op != 'DumpTensor':\n                continue\n            node_def.attr['file_name'].s = 'quantized_tensor_data.pb'.encode('utf-8')",
            "def _change_dump_tensor_file_name(graph_def: graph_pb2.GraphDef) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Change file_name used by DumpTensor to quantized_tensor_data.pb.\\n\\n  In whole model verify, DumpTensor in unquantized model uses file_name\\n  unquantized_tensor_data.pb.\\n  After unquantized dump model is created, this function allows quantized dump\\n  model to use quantized_tensor_data.pb as file_name.\\n\\n  Args:\\n    graph_def: GraphDef to change file_name of DumpTensor\\n  '\n    for function_def in graph_def.library.function:\n        for node_def in function_def.node_def:\n            if node_def.op != 'DumpTensor':\n                continue\n            node_def.attr['file_name'].s = 'quantized_tensor_data.pb'.encode('utf-8')",
            "def _change_dump_tensor_file_name(graph_def: graph_pb2.GraphDef) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Change file_name used by DumpTensor to quantized_tensor_data.pb.\\n\\n  In whole model verify, DumpTensor in unquantized model uses file_name\\n  unquantized_tensor_data.pb.\\n  After unquantized dump model is created, this function allows quantized dump\\n  model to use quantized_tensor_data.pb as file_name.\\n\\n  Args:\\n    graph_def: GraphDef to change file_name of DumpTensor\\n  '\n    for function_def in graph_def.library.function:\n        for node_def in function_def.node_def:\n            if node_def.op != 'DumpTensor':\n                continue\n            node_def.attr['file_name'].s = 'quantized_tensor_data.pb'.encode('utf-8')",
            "def _change_dump_tensor_file_name(graph_def: graph_pb2.GraphDef) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Change file_name used by DumpTensor to quantized_tensor_data.pb.\\n\\n  In whole model verify, DumpTensor in unquantized model uses file_name\\n  unquantized_tensor_data.pb.\\n  After unquantized dump model is created, this function allows quantized dump\\n  model to use quantized_tensor_data.pb as file_name.\\n\\n  Args:\\n    graph_def: GraphDef to change file_name of DumpTensor\\n  '\n    for function_def in graph_def.library.function:\n        for node_def in function_def.node_def:\n            if node_def.op != 'DumpTensor':\n                continue\n            node_def.attr['file_name'].s = 'quantized_tensor_data.pb'.encode('utf-8')",
            "def _change_dump_tensor_file_name(graph_def: graph_pb2.GraphDef) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Change file_name used by DumpTensor to quantized_tensor_data.pb.\\n\\n  In whole model verify, DumpTensor in unquantized model uses file_name\\n  unquantized_tensor_data.pb.\\n  After unquantized dump model is created, this function allows quantized dump\\n  model to use quantized_tensor_data.pb as file_name.\\n\\n  Args:\\n    graph_def: GraphDef to change file_name of DumpTensor\\n  '\n    for function_def in graph_def.library.function:\n        for node_def in function_def.node_def:\n            if node_def.op != 'DumpTensor':\n                continue\n            node_def.attr['file_name'].s = 'quantized_tensor_data.pb'.encode('utf-8')"
        ]
    },
    {
        "func_name": "_run_static_range_ptq",
        "original": "def _run_static_range_ptq(src_saved_model_path: str, dst_saved_model_path: str, quant_opts: _QuantizationOptions, representative_dataset: repr_dataset.RepresentativeDatasetOrMapping, signature_def_map: _SignatureDefMap) -> None:\n    \"\"\"Runs static-range Post-Training Quantization.\n\n  Runs static-range PTQ for the model. Runs the calibration step with\n  `representative_dataset` to collect statistics required for quantization. This\n  produces the quantized GraphDef along with the SignatureDefs which might have\n  been modified according to the changes in the graph.\n\n  Args:\n    src_saved_model_path: Path to the source SavedModel directory.\n    dst_saved_model_path: Path to the destination SavedModel directory.\n    quant_opts: Quantization options.\n    representative_dataset: Representative dataset used for the calibration\n      step. Representative datasets should exist for each signature def key in\n      `signature_def_keys`.\n    signature_def_map: Signature def key -> SignatureDef mapping.\n\n  Raises:\n    ValueError if the graph doesn't contain a valid signature.\n  \"\"\"\n    logging.info('Running post-training quantization pre-calibration step.')\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quant_opts.tags).meta_info_def.function_aliases\n    signature_def_map_serialized = _serialize_signature_def_map(signature_def_map)\n    (exported_model_serialized, pre_calib_output_model_path) = pywrap_quantize_model.quantize_ptq_model_pre_calibration(src_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=signature_def_map_serialized, function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary(), representative_dataset=representative_dataset)\n    exported_model = exported_model_pb2.ExportedModel.FromString(exported_model_serialized)\n    graph_def = exported_model.graph_def\n    py_function_library = py_function_lib.PyFunctionLibrary()\n    if quant_opts.HasField('debugger_options'):\n        _enable_dump_tensor(graph_def)\n        if quant_opts.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_WHOLE_MODEL:\n            py_function_library.save_exported_model(dst_saved_model_path=quant_opts.debugger_options.unquantized_dump_model_path, exported_model_serialized=exported_model.SerializeToString(), src_saved_model_path=src_saved_model_path, tags=quant_opts.tags, serialized_signature_def_map=signature_def_map_serialized)\n            _change_dump_tensor_file_name(graph_def)\n    calibrated_model_path = tempfile.mkdtemp()\n    py_function_library.save_exported_model(dst_saved_model_path=calibrated_model_path, exported_model_serialized=exported_model.SerializeToString(), src_saved_model_path=pre_calib_output_model_path, tags=quant_opts.tags, serialized_signature_def_map=signature_def_map_serialized)\n    logging.info('Running post-training quantization post-calibration step.')\n    pywrap_quantize_model.quantize_ptq_model_post_calibration(src_saved_model_path=calibrated_model_path, dst_saved_model_path=dst_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=signature_def_map_serialized, function_aliases=dict(exported_model.function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())",
        "mutated": [
            "def _run_static_range_ptq(src_saved_model_path: str, dst_saved_model_path: str, quant_opts: _QuantizationOptions, representative_dataset: repr_dataset.RepresentativeDatasetOrMapping, signature_def_map: _SignatureDefMap) -> None:\n    if False:\n        i = 10\n    \"Runs static-range Post-Training Quantization.\\n\\n  Runs static-range PTQ for the model. Runs the calibration step with\\n  `representative_dataset` to collect statistics required for quantization. This\\n  produces the quantized GraphDef along with the SignatureDefs which might have\\n  been modified according to the changes in the graph.\\n\\n  Args:\\n    src_saved_model_path: Path to the source SavedModel directory.\\n    dst_saved_model_path: Path to the destination SavedModel directory.\\n    quant_opts: Quantization options.\\n    representative_dataset: Representative dataset used for the calibration\\n      step. Representative datasets should exist for each signature def key in\\n      `signature_def_keys`.\\n    signature_def_map: Signature def key -> SignatureDef mapping.\\n\\n  Raises:\\n    ValueError if the graph doesn't contain a valid signature.\\n  \"\n    logging.info('Running post-training quantization pre-calibration step.')\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quant_opts.tags).meta_info_def.function_aliases\n    signature_def_map_serialized = _serialize_signature_def_map(signature_def_map)\n    (exported_model_serialized, pre_calib_output_model_path) = pywrap_quantize_model.quantize_ptq_model_pre_calibration(src_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=signature_def_map_serialized, function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary(), representative_dataset=representative_dataset)\n    exported_model = exported_model_pb2.ExportedModel.FromString(exported_model_serialized)\n    graph_def = exported_model.graph_def\n    py_function_library = py_function_lib.PyFunctionLibrary()\n    if quant_opts.HasField('debugger_options'):\n        _enable_dump_tensor(graph_def)\n        if quant_opts.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_WHOLE_MODEL:\n            py_function_library.save_exported_model(dst_saved_model_path=quant_opts.debugger_options.unquantized_dump_model_path, exported_model_serialized=exported_model.SerializeToString(), src_saved_model_path=src_saved_model_path, tags=quant_opts.tags, serialized_signature_def_map=signature_def_map_serialized)\n            _change_dump_tensor_file_name(graph_def)\n    calibrated_model_path = tempfile.mkdtemp()\n    py_function_library.save_exported_model(dst_saved_model_path=calibrated_model_path, exported_model_serialized=exported_model.SerializeToString(), src_saved_model_path=pre_calib_output_model_path, tags=quant_opts.tags, serialized_signature_def_map=signature_def_map_serialized)\n    logging.info('Running post-training quantization post-calibration step.')\n    pywrap_quantize_model.quantize_ptq_model_post_calibration(src_saved_model_path=calibrated_model_path, dst_saved_model_path=dst_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=signature_def_map_serialized, function_aliases=dict(exported_model.function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())",
            "def _run_static_range_ptq(src_saved_model_path: str, dst_saved_model_path: str, quant_opts: _QuantizationOptions, representative_dataset: repr_dataset.RepresentativeDatasetOrMapping, signature_def_map: _SignatureDefMap) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Runs static-range Post-Training Quantization.\\n\\n  Runs static-range PTQ for the model. Runs the calibration step with\\n  `representative_dataset` to collect statistics required for quantization. This\\n  produces the quantized GraphDef along with the SignatureDefs which might have\\n  been modified according to the changes in the graph.\\n\\n  Args:\\n    src_saved_model_path: Path to the source SavedModel directory.\\n    dst_saved_model_path: Path to the destination SavedModel directory.\\n    quant_opts: Quantization options.\\n    representative_dataset: Representative dataset used for the calibration\\n      step. Representative datasets should exist for each signature def key in\\n      `signature_def_keys`.\\n    signature_def_map: Signature def key -> SignatureDef mapping.\\n\\n  Raises:\\n    ValueError if the graph doesn't contain a valid signature.\\n  \"\n    logging.info('Running post-training quantization pre-calibration step.')\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quant_opts.tags).meta_info_def.function_aliases\n    signature_def_map_serialized = _serialize_signature_def_map(signature_def_map)\n    (exported_model_serialized, pre_calib_output_model_path) = pywrap_quantize_model.quantize_ptq_model_pre_calibration(src_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=signature_def_map_serialized, function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary(), representative_dataset=representative_dataset)\n    exported_model = exported_model_pb2.ExportedModel.FromString(exported_model_serialized)\n    graph_def = exported_model.graph_def\n    py_function_library = py_function_lib.PyFunctionLibrary()\n    if quant_opts.HasField('debugger_options'):\n        _enable_dump_tensor(graph_def)\n        if quant_opts.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_WHOLE_MODEL:\n            py_function_library.save_exported_model(dst_saved_model_path=quant_opts.debugger_options.unquantized_dump_model_path, exported_model_serialized=exported_model.SerializeToString(), src_saved_model_path=src_saved_model_path, tags=quant_opts.tags, serialized_signature_def_map=signature_def_map_serialized)\n            _change_dump_tensor_file_name(graph_def)\n    calibrated_model_path = tempfile.mkdtemp()\n    py_function_library.save_exported_model(dst_saved_model_path=calibrated_model_path, exported_model_serialized=exported_model.SerializeToString(), src_saved_model_path=pre_calib_output_model_path, tags=quant_opts.tags, serialized_signature_def_map=signature_def_map_serialized)\n    logging.info('Running post-training quantization post-calibration step.')\n    pywrap_quantize_model.quantize_ptq_model_post_calibration(src_saved_model_path=calibrated_model_path, dst_saved_model_path=dst_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=signature_def_map_serialized, function_aliases=dict(exported_model.function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())",
            "def _run_static_range_ptq(src_saved_model_path: str, dst_saved_model_path: str, quant_opts: _QuantizationOptions, representative_dataset: repr_dataset.RepresentativeDatasetOrMapping, signature_def_map: _SignatureDefMap) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Runs static-range Post-Training Quantization.\\n\\n  Runs static-range PTQ for the model. Runs the calibration step with\\n  `representative_dataset` to collect statistics required for quantization. This\\n  produces the quantized GraphDef along with the SignatureDefs which might have\\n  been modified according to the changes in the graph.\\n\\n  Args:\\n    src_saved_model_path: Path to the source SavedModel directory.\\n    dst_saved_model_path: Path to the destination SavedModel directory.\\n    quant_opts: Quantization options.\\n    representative_dataset: Representative dataset used for the calibration\\n      step. Representative datasets should exist for each signature def key in\\n      `signature_def_keys`.\\n    signature_def_map: Signature def key -> SignatureDef mapping.\\n\\n  Raises:\\n    ValueError if the graph doesn't contain a valid signature.\\n  \"\n    logging.info('Running post-training quantization pre-calibration step.')\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quant_opts.tags).meta_info_def.function_aliases\n    signature_def_map_serialized = _serialize_signature_def_map(signature_def_map)\n    (exported_model_serialized, pre_calib_output_model_path) = pywrap_quantize_model.quantize_ptq_model_pre_calibration(src_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=signature_def_map_serialized, function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary(), representative_dataset=representative_dataset)\n    exported_model = exported_model_pb2.ExportedModel.FromString(exported_model_serialized)\n    graph_def = exported_model.graph_def\n    py_function_library = py_function_lib.PyFunctionLibrary()\n    if quant_opts.HasField('debugger_options'):\n        _enable_dump_tensor(graph_def)\n        if quant_opts.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_WHOLE_MODEL:\n            py_function_library.save_exported_model(dst_saved_model_path=quant_opts.debugger_options.unquantized_dump_model_path, exported_model_serialized=exported_model.SerializeToString(), src_saved_model_path=src_saved_model_path, tags=quant_opts.tags, serialized_signature_def_map=signature_def_map_serialized)\n            _change_dump_tensor_file_name(graph_def)\n    calibrated_model_path = tempfile.mkdtemp()\n    py_function_library.save_exported_model(dst_saved_model_path=calibrated_model_path, exported_model_serialized=exported_model.SerializeToString(), src_saved_model_path=pre_calib_output_model_path, tags=quant_opts.tags, serialized_signature_def_map=signature_def_map_serialized)\n    logging.info('Running post-training quantization post-calibration step.')\n    pywrap_quantize_model.quantize_ptq_model_post_calibration(src_saved_model_path=calibrated_model_path, dst_saved_model_path=dst_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=signature_def_map_serialized, function_aliases=dict(exported_model.function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())",
            "def _run_static_range_ptq(src_saved_model_path: str, dst_saved_model_path: str, quant_opts: _QuantizationOptions, representative_dataset: repr_dataset.RepresentativeDatasetOrMapping, signature_def_map: _SignatureDefMap) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Runs static-range Post-Training Quantization.\\n\\n  Runs static-range PTQ for the model. Runs the calibration step with\\n  `representative_dataset` to collect statistics required for quantization. This\\n  produces the quantized GraphDef along with the SignatureDefs which might have\\n  been modified according to the changes in the graph.\\n\\n  Args:\\n    src_saved_model_path: Path to the source SavedModel directory.\\n    dst_saved_model_path: Path to the destination SavedModel directory.\\n    quant_opts: Quantization options.\\n    representative_dataset: Representative dataset used for the calibration\\n      step. Representative datasets should exist for each signature def key in\\n      `signature_def_keys`.\\n    signature_def_map: Signature def key -> SignatureDef mapping.\\n\\n  Raises:\\n    ValueError if the graph doesn't contain a valid signature.\\n  \"\n    logging.info('Running post-training quantization pre-calibration step.')\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quant_opts.tags).meta_info_def.function_aliases\n    signature_def_map_serialized = _serialize_signature_def_map(signature_def_map)\n    (exported_model_serialized, pre_calib_output_model_path) = pywrap_quantize_model.quantize_ptq_model_pre_calibration(src_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=signature_def_map_serialized, function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary(), representative_dataset=representative_dataset)\n    exported_model = exported_model_pb2.ExportedModel.FromString(exported_model_serialized)\n    graph_def = exported_model.graph_def\n    py_function_library = py_function_lib.PyFunctionLibrary()\n    if quant_opts.HasField('debugger_options'):\n        _enable_dump_tensor(graph_def)\n        if quant_opts.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_WHOLE_MODEL:\n            py_function_library.save_exported_model(dst_saved_model_path=quant_opts.debugger_options.unquantized_dump_model_path, exported_model_serialized=exported_model.SerializeToString(), src_saved_model_path=src_saved_model_path, tags=quant_opts.tags, serialized_signature_def_map=signature_def_map_serialized)\n            _change_dump_tensor_file_name(graph_def)\n    calibrated_model_path = tempfile.mkdtemp()\n    py_function_library.save_exported_model(dst_saved_model_path=calibrated_model_path, exported_model_serialized=exported_model.SerializeToString(), src_saved_model_path=pre_calib_output_model_path, tags=quant_opts.tags, serialized_signature_def_map=signature_def_map_serialized)\n    logging.info('Running post-training quantization post-calibration step.')\n    pywrap_quantize_model.quantize_ptq_model_post_calibration(src_saved_model_path=calibrated_model_path, dst_saved_model_path=dst_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=signature_def_map_serialized, function_aliases=dict(exported_model.function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())",
            "def _run_static_range_ptq(src_saved_model_path: str, dst_saved_model_path: str, quant_opts: _QuantizationOptions, representative_dataset: repr_dataset.RepresentativeDatasetOrMapping, signature_def_map: _SignatureDefMap) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Runs static-range Post-Training Quantization.\\n\\n  Runs static-range PTQ for the model. Runs the calibration step with\\n  `representative_dataset` to collect statistics required for quantization. This\\n  produces the quantized GraphDef along with the SignatureDefs which might have\\n  been modified according to the changes in the graph.\\n\\n  Args:\\n    src_saved_model_path: Path to the source SavedModel directory.\\n    dst_saved_model_path: Path to the destination SavedModel directory.\\n    quant_opts: Quantization options.\\n    representative_dataset: Representative dataset used for the calibration\\n      step. Representative datasets should exist for each signature def key in\\n      `signature_def_keys`.\\n    signature_def_map: Signature def key -> SignatureDef mapping.\\n\\n  Raises:\\n    ValueError if the graph doesn't contain a valid signature.\\n  \"\n    logging.info('Running post-training quantization pre-calibration step.')\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quant_opts.tags).meta_info_def.function_aliases\n    signature_def_map_serialized = _serialize_signature_def_map(signature_def_map)\n    (exported_model_serialized, pre_calib_output_model_path) = pywrap_quantize_model.quantize_ptq_model_pre_calibration(src_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=signature_def_map_serialized, function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary(), representative_dataset=representative_dataset)\n    exported_model = exported_model_pb2.ExportedModel.FromString(exported_model_serialized)\n    graph_def = exported_model.graph_def\n    py_function_library = py_function_lib.PyFunctionLibrary()\n    if quant_opts.HasField('debugger_options'):\n        _enable_dump_tensor(graph_def)\n        if quant_opts.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_WHOLE_MODEL:\n            py_function_library.save_exported_model(dst_saved_model_path=quant_opts.debugger_options.unquantized_dump_model_path, exported_model_serialized=exported_model.SerializeToString(), src_saved_model_path=src_saved_model_path, tags=quant_opts.tags, serialized_signature_def_map=signature_def_map_serialized)\n            _change_dump_tensor_file_name(graph_def)\n    calibrated_model_path = tempfile.mkdtemp()\n    py_function_library.save_exported_model(dst_saved_model_path=calibrated_model_path, exported_model_serialized=exported_model.SerializeToString(), src_saved_model_path=pre_calib_output_model_path, tags=quant_opts.tags, serialized_signature_def_map=signature_def_map_serialized)\n    logging.info('Running post-training quantization post-calibration step.')\n    pywrap_quantize_model.quantize_ptq_model_post_calibration(src_saved_model_path=calibrated_model_path, dst_saved_model_path=dst_saved_model_path, quantization_options_serialized=quant_opts.SerializeToString(), signature_keys=list(quant_opts.signature_keys), signature_def_map_serialized=signature_def_map_serialized, function_aliases=dict(exported_model.function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())"
        ]
    },
    {
        "func_name": "_static_range_quantize",
        "original": "def _static_range_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: _QuantizationOptions, representative_dataset: Optional[repr_dataset.RepresentativeDatasetOrMapping]=None) -> autotrackable.AutoTrackable:\n    \"\"\"Quantizes the given SavedModel via static range quantization.\n\n  If the model is not trained with Quantization-Aware Training (QAT) technique,\n  it requires `representative_dataset` to collect statistics required for\n  quantization. If non-None `representative_dataset` is provided with a QAT\n  model input, `representative_dataset` will be ignored.\n\n  Args:\n    src_saved_model_path: Path to the saved model. When representative_dataset\n      is not provided, this should be a model trained with QAT.\n    dst_saved_model_path: The path to save the output SavedModel. The directory\n      will be overwritten if not empty.\n    quantization_options: QuantizationOptions proto describing quantization\n      related config.\n    representative_dataset: a generator that returns a dictionary in {input_key:\n      input_value} format or a tuple with signature key and a dictionary in\n      {input_key: input_value} format that feeds calibration data for quantizing\n      model. This should be provided when the model is not a QAT model.\n\n  Returns:\n    A SavedModel object with TF quantization applied.\n\n  Raises:\n    ValueError: when representative_dataset is not provided for non-QAT model.\n    RuntimeError: When a MetaGraphDef could not be found associated with `tags`\n      in the SavedModel.\n  \"\"\"\n    logging.info('Running static range quantization on model: %s', src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    is_qat_saved_model_or_method_no_quantize = _is_qat_saved_model(src_saved_model_path) or quantization_options.quantization_method.preset_method == _QuantizationMethod.METHOD_NO_QUANTIZE\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, quantization_options.signature_keys, set(quantization_options.tags))\n    if representative_dataset is None and (not is_qat_saved_model_or_method_no_quantize):\n        raise ValueError('When `representative_dataset` is not provided, the model should be trained with quantization-aware training (QAT).')\n    if quantization_options.min_num_elements_for_weights > 0:\n        logging.warn('min_num_elements_for_weights is set but is not supported for the Post-training static range quantization. The flag is ignored.')\n    if is_qat_saved_model_or_method_no_quantize:\n        _run_static_range_qat(src_saved_model_path, dst_saved_model_path, quantization_options, signature_def_map)\n    else:\n        _run_static_range_ptq(src_saved_model_path, dst_saved_model_path, quantization_options, representative_dataset, signature_def_map)\n    return saved_model_load.load(dst_saved_model_path)",
        "mutated": [
            "def _static_range_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: _QuantizationOptions, representative_dataset: Optional[repr_dataset.RepresentativeDatasetOrMapping]=None) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n    'Quantizes the given SavedModel via static range quantization.\\n\\n  If the model is not trained with Quantization-Aware Training (QAT) technique,\\n  it requires `representative_dataset` to collect statistics required for\\n  quantization. If non-None `representative_dataset` is provided with a QAT\\n  model input, `representative_dataset` will be ignored.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model. When representative_dataset\\n      is not provided, this should be a model trained with QAT.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n    representative_dataset: a generator that returns a dictionary in {input_key:\\n      input_value} format or a tuple with signature key and a dictionary in\\n      {input_key: input_value} format that feeds calibration data for quantizing\\n      model. This should be provided when the model is not a QAT model.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when representative_dataset is not provided for non-QAT model.\\n    RuntimeError: When a MetaGraphDef could not be found associated with `tags`\\n      in the SavedModel.\\n  '\n    logging.info('Running static range quantization on model: %s', src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    is_qat_saved_model_or_method_no_quantize = _is_qat_saved_model(src_saved_model_path) or quantization_options.quantization_method.preset_method == _QuantizationMethod.METHOD_NO_QUANTIZE\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, quantization_options.signature_keys, set(quantization_options.tags))\n    if representative_dataset is None and (not is_qat_saved_model_or_method_no_quantize):\n        raise ValueError('When `representative_dataset` is not provided, the model should be trained with quantization-aware training (QAT).')\n    if quantization_options.min_num_elements_for_weights > 0:\n        logging.warn('min_num_elements_for_weights is set but is not supported for the Post-training static range quantization. The flag is ignored.')\n    if is_qat_saved_model_or_method_no_quantize:\n        _run_static_range_qat(src_saved_model_path, dst_saved_model_path, quantization_options, signature_def_map)\n    else:\n        _run_static_range_ptq(src_saved_model_path, dst_saved_model_path, quantization_options, representative_dataset, signature_def_map)\n    return saved_model_load.load(dst_saved_model_path)",
            "def _static_range_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: _QuantizationOptions, representative_dataset: Optional[repr_dataset.RepresentativeDatasetOrMapping]=None) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Quantizes the given SavedModel via static range quantization.\\n\\n  If the model is not trained with Quantization-Aware Training (QAT) technique,\\n  it requires `representative_dataset` to collect statistics required for\\n  quantization. If non-None `representative_dataset` is provided with a QAT\\n  model input, `representative_dataset` will be ignored.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model. When representative_dataset\\n      is not provided, this should be a model trained with QAT.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n    representative_dataset: a generator that returns a dictionary in {input_key:\\n      input_value} format or a tuple with signature key and a dictionary in\\n      {input_key: input_value} format that feeds calibration data for quantizing\\n      model. This should be provided when the model is not a QAT model.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when representative_dataset is not provided for non-QAT model.\\n    RuntimeError: When a MetaGraphDef could not be found associated with `tags`\\n      in the SavedModel.\\n  '\n    logging.info('Running static range quantization on model: %s', src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    is_qat_saved_model_or_method_no_quantize = _is_qat_saved_model(src_saved_model_path) or quantization_options.quantization_method.preset_method == _QuantizationMethod.METHOD_NO_QUANTIZE\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, quantization_options.signature_keys, set(quantization_options.tags))\n    if representative_dataset is None and (not is_qat_saved_model_or_method_no_quantize):\n        raise ValueError('When `representative_dataset` is not provided, the model should be trained with quantization-aware training (QAT).')\n    if quantization_options.min_num_elements_for_weights > 0:\n        logging.warn('min_num_elements_for_weights is set but is not supported for the Post-training static range quantization. The flag is ignored.')\n    if is_qat_saved_model_or_method_no_quantize:\n        _run_static_range_qat(src_saved_model_path, dst_saved_model_path, quantization_options, signature_def_map)\n    else:\n        _run_static_range_ptq(src_saved_model_path, dst_saved_model_path, quantization_options, representative_dataset, signature_def_map)\n    return saved_model_load.load(dst_saved_model_path)",
            "def _static_range_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: _QuantizationOptions, representative_dataset: Optional[repr_dataset.RepresentativeDatasetOrMapping]=None) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Quantizes the given SavedModel via static range quantization.\\n\\n  If the model is not trained with Quantization-Aware Training (QAT) technique,\\n  it requires `representative_dataset` to collect statistics required for\\n  quantization. If non-None `representative_dataset` is provided with a QAT\\n  model input, `representative_dataset` will be ignored.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model. When representative_dataset\\n      is not provided, this should be a model trained with QAT.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n    representative_dataset: a generator that returns a dictionary in {input_key:\\n      input_value} format or a tuple with signature key and a dictionary in\\n      {input_key: input_value} format that feeds calibration data for quantizing\\n      model. This should be provided when the model is not a QAT model.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when representative_dataset is not provided for non-QAT model.\\n    RuntimeError: When a MetaGraphDef could not be found associated with `tags`\\n      in the SavedModel.\\n  '\n    logging.info('Running static range quantization on model: %s', src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    is_qat_saved_model_or_method_no_quantize = _is_qat_saved_model(src_saved_model_path) or quantization_options.quantization_method.preset_method == _QuantizationMethod.METHOD_NO_QUANTIZE\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, quantization_options.signature_keys, set(quantization_options.tags))\n    if representative_dataset is None and (not is_qat_saved_model_or_method_no_quantize):\n        raise ValueError('When `representative_dataset` is not provided, the model should be trained with quantization-aware training (QAT).')\n    if quantization_options.min_num_elements_for_weights > 0:\n        logging.warn('min_num_elements_for_weights is set but is not supported for the Post-training static range quantization. The flag is ignored.')\n    if is_qat_saved_model_or_method_no_quantize:\n        _run_static_range_qat(src_saved_model_path, dst_saved_model_path, quantization_options, signature_def_map)\n    else:\n        _run_static_range_ptq(src_saved_model_path, dst_saved_model_path, quantization_options, representative_dataset, signature_def_map)\n    return saved_model_load.load(dst_saved_model_path)",
            "def _static_range_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: _QuantizationOptions, representative_dataset: Optional[repr_dataset.RepresentativeDatasetOrMapping]=None) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Quantizes the given SavedModel via static range quantization.\\n\\n  If the model is not trained with Quantization-Aware Training (QAT) technique,\\n  it requires `representative_dataset` to collect statistics required for\\n  quantization. If non-None `representative_dataset` is provided with a QAT\\n  model input, `representative_dataset` will be ignored.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model. When representative_dataset\\n      is not provided, this should be a model trained with QAT.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n    representative_dataset: a generator that returns a dictionary in {input_key:\\n      input_value} format or a tuple with signature key and a dictionary in\\n      {input_key: input_value} format that feeds calibration data for quantizing\\n      model. This should be provided when the model is not a QAT model.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when representative_dataset is not provided for non-QAT model.\\n    RuntimeError: When a MetaGraphDef could not be found associated with `tags`\\n      in the SavedModel.\\n  '\n    logging.info('Running static range quantization on model: %s', src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    is_qat_saved_model_or_method_no_quantize = _is_qat_saved_model(src_saved_model_path) or quantization_options.quantization_method.preset_method == _QuantizationMethod.METHOD_NO_QUANTIZE\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, quantization_options.signature_keys, set(quantization_options.tags))\n    if representative_dataset is None and (not is_qat_saved_model_or_method_no_quantize):\n        raise ValueError('When `representative_dataset` is not provided, the model should be trained with quantization-aware training (QAT).')\n    if quantization_options.min_num_elements_for_weights > 0:\n        logging.warn('min_num_elements_for_weights is set but is not supported for the Post-training static range quantization. The flag is ignored.')\n    if is_qat_saved_model_or_method_no_quantize:\n        _run_static_range_qat(src_saved_model_path, dst_saved_model_path, quantization_options, signature_def_map)\n    else:\n        _run_static_range_ptq(src_saved_model_path, dst_saved_model_path, quantization_options, representative_dataset, signature_def_map)\n    return saved_model_load.load(dst_saved_model_path)",
            "def _static_range_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: _QuantizationOptions, representative_dataset: Optional[repr_dataset.RepresentativeDatasetOrMapping]=None) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Quantizes the given SavedModel via static range quantization.\\n\\n  If the model is not trained with Quantization-Aware Training (QAT) technique,\\n  it requires `representative_dataset` to collect statistics required for\\n  quantization. If non-None `representative_dataset` is provided with a QAT\\n  model input, `representative_dataset` will be ignored.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model. When representative_dataset\\n      is not provided, this should be a model trained with QAT.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n    representative_dataset: a generator that returns a dictionary in {input_key:\\n      input_value} format or a tuple with signature key and a dictionary in\\n      {input_key: input_value} format that feeds calibration data for quantizing\\n      model. This should be provided when the model is not a QAT model.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when representative_dataset is not provided for non-QAT model.\\n    RuntimeError: When a MetaGraphDef could not be found associated with `tags`\\n      in the SavedModel.\\n  '\n    logging.info('Running static range quantization on model: %s', src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    is_qat_saved_model_or_method_no_quantize = _is_qat_saved_model(src_saved_model_path) or quantization_options.quantization_method.preset_method == _QuantizationMethod.METHOD_NO_QUANTIZE\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, quantization_options.signature_keys, set(quantization_options.tags))\n    if representative_dataset is None and (not is_qat_saved_model_or_method_no_quantize):\n        raise ValueError('When `representative_dataset` is not provided, the model should be trained with quantization-aware training (QAT).')\n    if quantization_options.min_num_elements_for_weights > 0:\n        logging.warn('min_num_elements_for_weights is set but is not supported for the Post-training static range quantization. The flag is ignored.')\n    if is_qat_saved_model_or_method_no_quantize:\n        _run_static_range_qat(src_saved_model_path, dst_saved_model_path, quantization_options, signature_def_map)\n    else:\n        _run_static_range_ptq(src_saved_model_path, dst_saved_model_path, quantization_options, representative_dataset, signature_def_map)\n    return saved_model_load.load(dst_saved_model_path)"
        ]
    },
    {
        "func_name": "_dynamic_range_quantize",
        "original": "def _dynamic_range_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: _QuantizationOptions) -> autotrackable.AutoTrackable:\n    \"\"\"Quantizes the given SavedModel via post-training dynamic range quantization.\n\n  Args:\n    src_saved_model_path: Path to the saved model.\n    dst_saved_model_path: The path to save the output SavedModel. The directory\n      will be overwritten if not empty.\n    quantization_options: QuantizationOptions proto describing quantization\n      related config.\n\n  Returns:\n    A SavedModel object with TF quantization applied.\n\n  Raises:\n    ValueError: when the model is QAT model.\n  \"\"\"\n    mode_str = 'dynamic-range quantization'\n    if _is_qat_saved_model(src_saved_model_path):\n        raise ValueError('The models trained with quantization-aware training (QAT) is not supported for %s.' % mode_str)\n    logging.info('Running post-training %s on model: %s', mode_str, src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quantization_options.tags).meta_info_def.function_aliases\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, quantization_options.signature_keys, quantization_options.tags)\n    pywrap_quantize_model.quantize_ptq_dynamic_range(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quantization_options.SerializeToString(), signature_keys=list(quantization_options.signature_keys), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())\n    return saved_model_load.load(dst_saved_model_path)",
        "mutated": [
            "def _dynamic_range_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: _QuantizationOptions) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n    'Quantizes the given SavedModel via post-training dynamic range quantization.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when the model is QAT model.\\n  '\n    mode_str = 'dynamic-range quantization'\n    if _is_qat_saved_model(src_saved_model_path):\n        raise ValueError('The models trained with quantization-aware training (QAT) is not supported for %s.' % mode_str)\n    logging.info('Running post-training %s on model: %s', mode_str, src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quantization_options.tags).meta_info_def.function_aliases\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, quantization_options.signature_keys, quantization_options.tags)\n    pywrap_quantize_model.quantize_ptq_dynamic_range(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quantization_options.SerializeToString(), signature_keys=list(quantization_options.signature_keys), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())\n    return saved_model_load.load(dst_saved_model_path)",
            "def _dynamic_range_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: _QuantizationOptions) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Quantizes the given SavedModel via post-training dynamic range quantization.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when the model is QAT model.\\n  '\n    mode_str = 'dynamic-range quantization'\n    if _is_qat_saved_model(src_saved_model_path):\n        raise ValueError('The models trained with quantization-aware training (QAT) is not supported for %s.' % mode_str)\n    logging.info('Running post-training %s on model: %s', mode_str, src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quantization_options.tags).meta_info_def.function_aliases\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, quantization_options.signature_keys, quantization_options.tags)\n    pywrap_quantize_model.quantize_ptq_dynamic_range(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quantization_options.SerializeToString(), signature_keys=list(quantization_options.signature_keys), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())\n    return saved_model_load.load(dst_saved_model_path)",
            "def _dynamic_range_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: _QuantizationOptions) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Quantizes the given SavedModel via post-training dynamic range quantization.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when the model is QAT model.\\n  '\n    mode_str = 'dynamic-range quantization'\n    if _is_qat_saved_model(src_saved_model_path):\n        raise ValueError('The models trained with quantization-aware training (QAT) is not supported for %s.' % mode_str)\n    logging.info('Running post-training %s on model: %s', mode_str, src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quantization_options.tags).meta_info_def.function_aliases\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, quantization_options.signature_keys, quantization_options.tags)\n    pywrap_quantize_model.quantize_ptq_dynamic_range(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quantization_options.SerializeToString(), signature_keys=list(quantization_options.signature_keys), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())\n    return saved_model_load.load(dst_saved_model_path)",
            "def _dynamic_range_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: _QuantizationOptions) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Quantizes the given SavedModel via post-training dynamic range quantization.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when the model is QAT model.\\n  '\n    mode_str = 'dynamic-range quantization'\n    if _is_qat_saved_model(src_saved_model_path):\n        raise ValueError('The models trained with quantization-aware training (QAT) is not supported for %s.' % mode_str)\n    logging.info('Running post-training %s on model: %s', mode_str, src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quantization_options.tags).meta_info_def.function_aliases\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, quantization_options.signature_keys, quantization_options.tags)\n    pywrap_quantize_model.quantize_ptq_dynamic_range(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quantization_options.SerializeToString(), signature_keys=list(quantization_options.signature_keys), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())\n    return saved_model_load.load(dst_saved_model_path)",
            "def _dynamic_range_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: _QuantizationOptions) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Quantizes the given SavedModel via post-training dynamic range quantization.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when the model is QAT model.\\n  '\n    mode_str = 'dynamic-range quantization'\n    if _is_qat_saved_model(src_saved_model_path):\n        raise ValueError('The models trained with quantization-aware training (QAT) is not supported for %s.' % mode_str)\n    logging.info('Running post-training %s on model: %s', mode_str, src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quantization_options.tags).meta_info_def.function_aliases\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, quantization_options.signature_keys, quantization_options.tags)\n    pywrap_quantize_model.quantize_ptq_dynamic_range(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quantization_options.SerializeToString(), signature_keys=list(quantization_options.signature_keys), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())\n    return saved_model_load.load(dst_saved_model_path)"
        ]
    },
    {
        "func_name": "_weight_only_quantize",
        "original": "def _weight_only_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: quant_opts_pb2.QuantizationOptions) -> autotrackable.AutoTrackable:\n    \"\"\"Quantizes the given SavedModel via weight-only quantization.\n\n  Args:\n    src_saved_model_path: Path to the saved model.\n    dst_saved_model_path: The path to save the output SavedModel. The directory\n      will be overwritten if not empty.\n    quantization_options: QuantizationOptions proto describing quantization\n      related config.\n\n  Returns:\n    A SavedModel object with TF quantization applied.\n\n  Raises:\n    ValueError: when the model is QAT model.\n  \"\"\"\n    mode_str = 'weight-only quantization'\n    if _is_qat_saved_model(src_saved_model_path):\n        raise ValueError('The models trained with quantization-aware training (QAT) is not supported for %s.' % mode_str)\n    logging.info('Running post-training %s on model: %s', mode_str, src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quantization_options.tags).meta_info_def.function_aliases\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, list(quantization_options.signature_keys), set(quantization_options.tags))\n    pywrap_quantize_model.quantize_weight_only(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quantization_options.SerializeToString(), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())\n    return saved_model_load.load(dst_saved_model_path)",
        "mutated": [
            "def _weight_only_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: quant_opts_pb2.QuantizationOptions) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n    'Quantizes the given SavedModel via weight-only quantization.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when the model is QAT model.\\n  '\n    mode_str = 'weight-only quantization'\n    if _is_qat_saved_model(src_saved_model_path):\n        raise ValueError('The models trained with quantization-aware training (QAT) is not supported for %s.' % mode_str)\n    logging.info('Running post-training %s on model: %s', mode_str, src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quantization_options.tags).meta_info_def.function_aliases\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, list(quantization_options.signature_keys), set(quantization_options.tags))\n    pywrap_quantize_model.quantize_weight_only(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quantization_options.SerializeToString(), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())\n    return saved_model_load.load(dst_saved_model_path)",
            "def _weight_only_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: quant_opts_pb2.QuantizationOptions) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Quantizes the given SavedModel via weight-only quantization.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when the model is QAT model.\\n  '\n    mode_str = 'weight-only quantization'\n    if _is_qat_saved_model(src_saved_model_path):\n        raise ValueError('The models trained with quantization-aware training (QAT) is not supported for %s.' % mode_str)\n    logging.info('Running post-training %s on model: %s', mode_str, src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quantization_options.tags).meta_info_def.function_aliases\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, list(quantization_options.signature_keys), set(quantization_options.tags))\n    pywrap_quantize_model.quantize_weight_only(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quantization_options.SerializeToString(), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())\n    return saved_model_load.load(dst_saved_model_path)",
            "def _weight_only_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: quant_opts_pb2.QuantizationOptions) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Quantizes the given SavedModel via weight-only quantization.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when the model is QAT model.\\n  '\n    mode_str = 'weight-only quantization'\n    if _is_qat_saved_model(src_saved_model_path):\n        raise ValueError('The models trained with quantization-aware training (QAT) is not supported for %s.' % mode_str)\n    logging.info('Running post-training %s on model: %s', mode_str, src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quantization_options.tags).meta_info_def.function_aliases\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, list(quantization_options.signature_keys), set(quantization_options.tags))\n    pywrap_quantize_model.quantize_weight_only(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quantization_options.SerializeToString(), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())\n    return saved_model_load.load(dst_saved_model_path)",
            "def _weight_only_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: quant_opts_pb2.QuantizationOptions) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Quantizes the given SavedModel via weight-only quantization.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when the model is QAT model.\\n  '\n    mode_str = 'weight-only quantization'\n    if _is_qat_saved_model(src_saved_model_path):\n        raise ValueError('The models trained with quantization-aware training (QAT) is not supported for %s.' % mode_str)\n    logging.info('Running post-training %s on model: %s', mode_str, src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quantization_options.tags).meta_info_def.function_aliases\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, list(quantization_options.signature_keys), set(quantization_options.tags))\n    pywrap_quantize_model.quantize_weight_only(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quantization_options.SerializeToString(), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())\n    return saved_model_load.load(dst_saved_model_path)",
            "def _weight_only_quantize(src_saved_model_path: str, dst_saved_model_path: str, quantization_options: quant_opts_pb2.QuantizationOptions) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Quantizes the given SavedModel via weight-only quantization.\\n\\n  Args:\\n    src_saved_model_path: Path to the saved model.\\n    dst_saved_model_path: The path to save the output SavedModel. The directory\\n      will be overwritten if not empty.\\n    quantization_options: QuantizationOptions proto describing quantization\\n      related config.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied.\\n\\n  Raises:\\n    ValueError: when the model is QAT model.\\n  '\n    mode_str = 'weight-only quantization'\n    if _is_qat_saved_model(src_saved_model_path):\n        raise ValueError('The models trained with quantization-aware training (QAT) is not supported for %s.' % mode_str)\n    logging.info('Running post-training %s on model: %s', mode_str, src_saved_model_path)\n    logging.info('QuantizationOptions: \\n%s', quantization_options)\n    loader = saved_model_loader.SavedModelLoader(src_saved_model_path)\n    function_aliases = loader.get_meta_graph_def_from_tags(quantization_options.tags).meta_info_def.function_aliases\n    signature_def_map = save_model.get_signatures_from_saved_model(src_saved_model_path, list(quantization_options.signature_keys), set(quantization_options.tags))\n    pywrap_quantize_model.quantize_weight_only(src_saved_model_path, dst_saved_model_path, quantization_options_serialized=quantization_options.SerializeToString(), signature_def_map_serialized=_serialize_signature_def_map(signature_def_map), function_aliases=dict(function_aliases), py_function_library=py_function_lib.PyFunctionLibrary())\n    return saved_model_load.load(dst_saved_model_path)"
        ]
    },
    {
        "func_name": "_verify_output_dir",
        "original": "def _verify_output_dir(output_dir: Optional[str], overwrite: bool) -> None:\n    \"\"\"Verifies the output directory.\n\n  Raises an error if `output_dir` is not suitable for writing the output saved\n  model.\n\n  Args:\n    output_dir: Output directory.\n    overwrite: An option allowing to overwrite the existing output directory if\n      set to true. Does not actually create or modify the `output_dir` in this\n      function.\n\n  Raises:\n    FileExistsError: Iff `output_dir` is not empty and `overwrite` is false.\n  \"\"\"\n    dir_not_empty = output_dir is not None and file_io.file_exists_v2(output_dir) and file_io.list_directory_v2(output_dir)\n    if dir_not_empty and (not overwrite):\n        raise FileExistsError(f'Output directory already exists: {output_dir} . Please set overwrite_output_directory to true to overwrite the existing directory.')",
        "mutated": [
            "def _verify_output_dir(output_dir: Optional[str], overwrite: bool) -> None:\n    if False:\n        i = 10\n    'Verifies the output directory.\\n\\n  Raises an error if `output_dir` is not suitable for writing the output saved\\n  model.\\n\\n  Args:\\n    output_dir: Output directory.\\n    overwrite: An option allowing to overwrite the existing output directory if\\n      set to true. Does not actually create or modify the `output_dir` in this\\n      function.\\n\\n  Raises:\\n    FileExistsError: Iff `output_dir` is not empty and `overwrite` is false.\\n  '\n    dir_not_empty = output_dir is not None and file_io.file_exists_v2(output_dir) and file_io.list_directory_v2(output_dir)\n    if dir_not_empty and (not overwrite):\n        raise FileExistsError(f'Output directory already exists: {output_dir} . Please set overwrite_output_directory to true to overwrite the existing directory.')",
            "def _verify_output_dir(output_dir: Optional[str], overwrite: bool) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Verifies the output directory.\\n\\n  Raises an error if `output_dir` is not suitable for writing the output saved\\n  model.\\n\\n  Args:\\n    output_dir: Output directory.\\n    overwrite: An option allowing to overwrite the existing output directory if\\n      set to true. Does not actually create or modify the `output_dir` in this\\n      function.\\n\\n  Raises:\\n    FileExistsError: Iff `output_dir` is not empty and `overwrite` is false.\\n  '\n    dir_not_empty = output_dir is not None and file_io.file_exists_v2(output_dir) and file_io.list_directory_v2(output_dir)\n    if dir_not_empty and (not overwrite):\n        raise FileExistsError(f'Output directory already exists: {output_dir} . Please set overwrite_output_directory to true to overwrite the existing directory.')",
            "def _verify_output_dir(output_dir: Optional[str], overwrite: bool) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Verifies the output directory.\\n\\n  Raises an error if `output_dir` is not suitable for writing the output saved\\n  model.\\n\\n  Args:\\n    output_dir: Output directory.\\n    overwrite: An option allowing to overwrite the existing output directory if\\n      set to true. Does not actually create or modify the `output_dir` in this\\n      function.\\n\\n  Raises:\\n    FileExistsError: Iff `output_dir` is not empty and `overwrite` is false.\\n  '\n    dir_not_empty = output_dir is not None and file_io.file_exists_v2(output_dir) and file_io.list_directory_v2(output_dir)\n    if dir_not_empty and (not overwrite):\n        raise FileExistsError(f'Output directory already exists: {output_dir} . Please set overwrite_output_directory to true to overwrite the existing directory.')",
            "def _verify_output_dir(output_dir: Optional[str], overwrite: bool) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Verifies the output directory.\\n\\n  Raises an error if `output_dir` is not suitable for writing the output saved\\n  model.\\n\\n  Args:\\n    output_dir: Output directory.\\n    overwrite: An option allowing to overwrite the existing output directory if\\n      set to true. Does not actually create or modify the `output_dir` in this\\n      function.\\n\\n  Raises:\\n    FileExistsError: Iff `output_dir` is not empty and `overwrite` is false.\\n  '\n    dir_not_empty = output_dir is not None and file_io.file_exists_v2(output_dir) and file_io.list_directory_v2(output_dir)\n    if dir_not_empty and (not overwrite):\n        raise FileExistsError(f'Output directory already exists: {output_dir} . Please set overwrite_output_directory to true to overwrite the existing directory.')",
            "def _verify_output_dir(output_dir: Optional[str], overwrite: bool) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Verifies the output directory.\\n\\n  Raises an error if `output_dir` is not suitable for writing the output saved\\n  model.\\n\\n  Args:\\n    output_dir: Output directory.\\n    overwrite: An option allowing to overwrite the existing output directory if\\n      set to true. Does not actually create or modify the `output_dir` in this\\n      function.\\n\\n  Raises:\\n    FileExistsError: Iff `output_dir` is not empty and `overwrite` is false.\\n  '\n    dir_not_empty = output_dir is not None and file_io.file_exists_v2(output_dir) and file_io.list_directory_v2(output_dir)\n    if dir_not_empty and (not overwrite):\n        raise FileExistsError(f'Output directory already exists: {output_dir} . Please set overwrite_output_directory to true to overwrite the existing directory.')"
        ]
    },
    {
        "func_name": "_populate_quantization_component_spec",
        "original": "def _populate_quantization_component_spec(quant_method: _QuantizationMethod) -> None:\n    \"\"\"Populates default values for QuantizationComponentSpec.\n\n  Args:\n    quant_method: The quantization method to be updated.\n  \"\"\"\n    updated_component_spec = dict()\n    if quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or quant_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        updated_component_spec[_QuantizationComponent.COMPONENT_ACTIVATION] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_ACTIVATION, tensor_type=_TensorType.TENSORTYPE_INT_8)\n        updated_component_spec[_QuantizationComponent.COMPONENT_WEIGHT] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_WEIGHT, tensor_type=_TensorType.TENSORTYPE_INT_8)\n        updated_component_spec[_QuantizationComponent.COMPONENT_BIAS] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_BIAS, tensor_type=_TensorType.TENSORTYPE_INT_32)\n    elif quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8:\n        updated_component_spec[_QuantizationComponent.COMPONENT_WEIGHT] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_WEIGHT, tensor_type=_TensorType.TENSORTYPE_INT_8)\n    if quant_method.quantization_component_specs:\n        for component_spec in quant_method.quantization_component_specs:\n            if component_spec.quantization_component in [_QuantizationComponent.COMPONENT_WEIGHT, _QuantizationComponent.COMPONENT_ACTIVATION]:\n                if component_spec.tensor_type != _TensorType.TENSORTYPE_INT_8:\n                    raise ValueError('Only int8 precision is supported for input operands.')\n            elif component_spec.tensor_type != _TensorType.TENSORTYPE_INT_32:\n                raise ValueError('Only int32 precision is supported for bias.')\n            updated_component_spec[component_spec.quantization_component] = component_spec\n    del quant_method.quantization_component_specs[:]\n    quant_method.quantization_component_specs.extend(updated_component_spec.values())\n    if (quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or quant_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8) and len(quant_method.quantization_component_specs) != 3:\n        raise ValueError('Only 3 components are needed for', quant_method)\n    elif quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 and len(quant_method.quantization_component_specs) != 1:\n        raise ValueError('At least one component spec needs to be specified.')",
        "mutated": [
            "def _populate_quantization_component_spec(quant_method: _QuantizationMethod) -> None:\n    if False:\n        i = 10\n    'Populates default values for QuantizationComponentSpec.\\n\\n  Args:\\n    quant_method: The quantization method to be updated.\\n  '\n    updated_component_spec = dict()\n    if quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or quant_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        updated_component_spec[_QuantizationComponent.COMPONENT_ACTIVATION] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_ACTIVATION, tensor_type=_TensorType.TENSORTYPE_INT_8)\n        updated_component_spec[_QuantizationComponent.COMPONENT_WEIGHT] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_WEIGHT, tensor_type=_TensorType.TENSORTYPE_INT_8)\n        updated_component_spec[_QuantizationComponent.COMPONENT_BIAS] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_BIAS, tensor_type=_TensorType.TENSORTYPE_INT_32)\n    elif quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8:\n        updated_component_spec[_QuantizationComponent.COMPONENT_WEIGHT] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_WEIGHT, tensor_type=_TensorType.TENSORTYPE_INT_8)\n    if quant_method.quantization_component_specs:\n        for component_spec in quant_method.quantization_component_specs:\n            if component_spec.quantization_component in [_QuantizationComponent.COMPONENT_WEIGHT, _QuantizationComponent.COMPONENT_ACTIVATION]:\n                if component_spec.tensor_type != _TensorType.TENSORTYPE_INT_8:\n                    raise ValueError('Only int8 precision is supported for input operands.')\n            elif component_spec.tensor_type != _TensorType.TENSORTYPE_INT_32:\n                raise ValueError('Only int32 precision is supported for bias.')\n            updated_component_spec[component_spec.quantization_component] = component_spec\n    del quant_method.quantization_component_specs[:]\n    quant_method.quantization_component_specs.extend(updated_component_spec.values())\n    if (quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or quant_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8) and len(quant_method.quantization_component_specs) != 3:\n        raise ValueError('Only 3 components are needed for', quant_method)\n    elif quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 and len(quant_method.quantization_component_specs) != 1:\n        raise ValueError('At least one component spec needs to be specified.')",
            "def _populate_quantization_component_spec(quant_method: _QuantizationMethod) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Populates default values for QuantizationComponentSpec.\\n\\n  Args:\\n    quant_method: The quantization method to be updated.\\n  '\n    updated_component_spec = dict()\n    if quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or quant_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        updated_component_spec[_QuantizationComponent.COMPONENT_ACTIVATION] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_ACTIVATION, tensor_type=_TensorType.TENSORTYPE_INT_8)\n        updated_component_spec[_QuantizationComponent.COMPONENT_WEIGHT] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_WEIGHT, tensor_type=_TensorType.TENSORTYPE_INT_8)\n        updated_component_spec[_QuantizationComponent.COMPONENT_BIAS] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_BIAS, tensor_type=_TensorType.TENSORTYPE_INT_32)\n    elif quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8:\n        updated_component_spec[_QuantizationComponent.COMPONENT_WEIGHT] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_WEIGHT, tensor_type=_TensorType.TENSORTYPE_INT_8)\n    if quant_method.quantization_component_specs:\n        for component_spec in quant_method.quantization_component_specs:\n            if component_spec.quantization_component in [_QuantizationComponent.COMPONENT_WEIGHT, _QuantizationComponent.COMPONENT_ACTIVATION]:\n                if component_spec.tensor_type != _TensorType.TENSORTYPE_INT_8:\n                    raise ValueError('Only int8 precision is supported for input operands.')\n            elif component_spec.tensor_type != _TensorType.TENSORTYPE_INT_32:\n                raise ValueError('Only int32 precision is supported for bias.')\n            updated_component_spec[component_spec.quantization_component] = component_spec\n    del quant_method.quantization_component_specs[:]\n    quant_method.quantization_component_specs.extend(updated_component_spec.values())\n    if (quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or quant_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8) and len(quant_method.quantization_component_specs) != 3:\n        raise ValueError('Only 3 components are needed for', quant_method)\n    elif quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 and len(quant_method.quantization_component_specs) != 1:\n        raise ValueError('At least one component spec needs to be specified.')",
            "def _populate_quantization_component_spec(quant_method: _QuantizationMethod) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Populates default values for QuantizationComponentSpec.\\n\\n  Args:\\n    quant_method: The quantization method to be updated.\\n  '\n    updated_component_spec = dict()\n    if quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or quant_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        updated_component_spec[_QuantizationComponent.COMPONENT_ACTIVATION] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_ACTIVATION, tensor_type=_TensorType.TENSORTYPE_INT_8)\n        updated_component_spec[_QuantizationComponent.COMPONENT_WEIGHT] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_WEIGHT, tensor_type=_TensorType.TENSORTYPE_INT_8)\n        updated_component_spec[_QuantizationComponent.COMPONENT_BIAS] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_BIAS, tensor_type=_TensorType.TENSORTYPE_INT_32)\n    elif quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8:\n        updated_component_spec[_QuantizationComponent.COMPONENT_WEIGHT] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_WEIGHT, tensor_type=_TensorType.TENSORTYPE_INT_8)\n    if quant_method.quantization_component_specs:\n        for component_spec in quant_method.quantization_component_specs:\n            if component_spec.quantization_component in [_QuantizationComponent.COMPONENT_WEIGHT, _QuantizationComponent.COMPONENT_ACTIVATION]:\n                if component_spec.tensor_type != _TensorType.TENSORTYPE_INT_8:\n                    raise ValueError('Only int8 precision is supported for input operands.')\n            elif component_spec.tensor_type != _TensorType.TENSORTYPE_INT_32:\n                raise ValueError('Only int32 precision is supported for bias.')\n            updated_component_spec[component_spec.quantization_component] = component_spec\n    del quant_method.quantization_component_specs[:]\n    quant_method.quantization_component_specs.extend(updated_component_spec.values())\n    if (quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or quant_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8) and len(quant_method.quantization_component_specs) != 3:\n        raise ValueError('Only 3 components are needed for', quant_method)\n    elif quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 and len(quant_method.quantization_component_specs) != 1:\n        raise ValueError('At least one component spec needs to be specified.')",
            "def _populate_quantization_component_spec(quant_method: _QuantizationMethod) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Populates default values for QuantizationComponentSpec.\\n\\n  Args:\\n    quant_method: The quantization method to be updated.\\n  '\n    updated_component_spec = dict()\n    if quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or quant_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        updated_component_spec[_QuantizationComponent.COMPONENT_ACTIVATION] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_ACTIVATION, tensor_type=_TensorType.TENSORTYPE_INT_8)\n        updated_component_spec[_QuantizationComponent.COMPONENT_WEIGHT] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_WEIGHT, tensor_type=_TensorType.TENSORTYPE_INT_8)\n        updated_component_spec[_QuantizationComponent.COMPONENT_BIAS] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_BIAS, tensor_type=_TensorType.TENSORTYPE_INT_32)\n    elif quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8:\n        updated_component_spec[_QuantizationComponent.COMPONENT_WEIGHT] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_WEIGHT, tensor_type=_TensorType.TENSORTYPE_INT_8)\n    if quant_method.quantization_component_specs:\n        for component_spec in quant_method.quantization_component_specs:\n            if component_spec.quantization_component in [_QuantizationComponent.COMPONENT_WEIGHT, _QuantizationComponent.COMPONENT_ACTIVATION]:\n                if component_spec.tensor_type != _TensorType.TENSORTYPE_INT_8:\n                    raise ValueError('Only int8 precision is supported for input operands.')\n            elif component_spec.tensor_type != _TensorType.TENSORTYPE_INT_32:\n                raise ValueError('Only int32 precision is supported for bias.')\n            updated_component_spec[component_spec.quantization_component] = component_spec\n    del quant_method.quantization_component_specs[:]\n    quant_method.quantization_component_specs.extend(updated_component_spec.values())\n    if (quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or quant_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8) and len(quant_method.quantization_component_specs) != 3:\n        raise ValueError('Only 3 components are needed for', quant_method)\n    elif quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 and len(quant_method.quantization_component_specs) != 1:\n        raise ValueError('At least one component spec needs to be specified.')",
            "def _populate_quantization_component_spec(quant_method: _QuantizationMethod) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Populates default values for QuantizationComponentSpec.\\n\\n  Args:\\n    quant_method: The quantization method to be updated.\\n  '\n    updated_component_spec = dict()\n    if quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or quant_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        updated_component_spec[_QuantizationComponent.COMPONENT_ACTIVATION] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_ACTIVATION, tensor_type=_TensorType.TENSORTYPE_INT_8)\n        updated_component_spec[_QuantizationComponent.COMPONENT_WEIGHT] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_WEIGHT, tensor_type=_TensorType.TENSORTYPE_INT_8)\n        updated_component_spec[_QuantizationComponent.COMPONENT_BIAS] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_BIAS, tensor_type=_TensorType.TENSORTYPE_INT_32)\n    elif quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8:\n        updated_component_spec[_QuantizationComponent.COMPONENT_WEIGHT] = _QuantizationComponentSpec(quantization_component=_QuantizationComponent.COMPONENT_WEIGHT, tensor_type=_TensorType.TENSORTYPE_INT_8)\n    if quant_method.quantization_component_specs:\n        for component_spec in quant_method.quantization_component_specs:\n            if component_spec.quantization_component in [_QuantizationComponent.COMPONENT_WEIGHT, _QuantizationComponent.COMPONENT_ACTIVATION]:\n                if component_spec.tensor_type != _TensorType.TENSORTYPE_INT_8:\n                    raise ValueError('Only int8 precision is supported for input operands.')\n            elif component_spec.tensor_type != _TensorType.TENSORTYPE_INT_32:\n                raise ValueError('Only int32 precision is supported for bias.')\n            updated_component_spec[component_spec.quantization_component] = component_spec\n    del quant_method.quantization_component_specs[:]\n    quant_method.quantization_component_specs.extend(updated_component_spec.values())\n    if (quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or quant_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8) and len(quant_method.quantization_component_specs) != 3:\n        raise ValueError('Only 3 components are needed for', quant_method)\n    elif quant_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 and len(quant_method.quantization_component_specs) != 1:\n        raise ValueError('At least one component spec needs to be specified.')"
        ]
    },
    {
        "func_name": "_populate_unitwise_quantization_specs",
        "original": "def _populate_unitwise_quantization_specs(quantization_options: _QuantizationOptions) -> None:\n    \"\"\"Verifies and pupulates unitwise quantization specs.\"\"\"\n    if not quantization_options.unit_wise_quantization_specs:\n        return\n    sorted_top_level_component_specs = sorted(quantization_options.quantization_method.quantization_component_specs, key=lambda x: x.quantization_component)\n    for unitwise_spec in quantization_options.unit_wise_quantization_specs:\n        if not unitwise_spec.unit:\n            raise ValueError('UnitWiseQuantizationSpec must contain at least one unit.')\n        for unit in unitwise_spec.unit:\n            if not unit.op_type and (not unit.node_name):\n                raise ValueError('Either `op_type` or `node_name` must be specified.')\n        _populate_quantization_component_spec(unitwise_spec.quantization_method)\n        component_specs = unitwise_spec.quantization_method.quantization_component_specs\n        if component_specs and sorted_top_level_component_specs != sorted(component_specs, key=lambda x: x.quantization_component):\n            raise ValueError('Currently unit-wise quantization spec only supports NO_QUANTIZE and same quantization method as the top-level `quantization_method`')",
        "mutated": [
            "def _populate_unitwise_quantization_specs(quantization_options: _QuantizationOptions) -> None:\n    if False:\n        i = 10\n    'Verifies and pupulates unitwise quantization specs.'\n    if not quantization_options.unit_wise_quantization_specs:\n        return\n    sorted_top_level_component_specs = sorted(quantization_options.quantization_method.quantization_component_specs, key=lambda x: x.quantization_component)\n    for unitwise_spec in quantization_options.unit_wise_quantization_specs:\n        if not unitwise_spec.unit:\n            raise ValueError('UnitWiseQuantizationSpec must contain at least one unit.')\n        for unit in unitwise_spec.unit:\n            if not unit.op_type and (not unit.node_name):\n                raise ValueError('Either `op_type` or `node_name` must be specified.')\n        _populate_quantization_component_spec(unitwise_spec.quantization_method)\n        component_specs = unitwise_spec.quantization_method.quantization_component_specs\n        if component_specs and sorted_top_level_component_specs != sorted(component_specs, key=lambda x: x.quantization_component):\n            raise ValueError('Currently unit-wise quantization spec only supports NO_QUANTIZE and same quantization method as the top-level `quantization_method`')",
            "def _populate_unitwise_quantization_specs(quantization_options: _QuantizationOptions) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Verifies and pupulates unitwise quantization specs.'\n    if not quantization_options.unit_wise_quantization_specs:\n        return\n    sorted_top_level_component_specs = sorted(quantization_options.quantization_method.quantization_component_specs, key=lambda x: x.quantization_component)\n    for unitwise_spec in quantization_options.unit_wise_quantization_specs:\n        if not unitwise_spec.unit:\n            raise ValueError('UnitWiseQuantizationSpec must contain at least one unit.')\n        for unit in unitwise_spec.unit:\n            if not unit.op_type and (not unit.node_name):\n                raise ValueError('Either `op_type` or `node_name` must be specified.')\n        _populate_quantization_component_spec(unitwise_spec.quantization_method)\n        component_specs = unitwise_spec.quantization_method.quantization_component_specs\n        if component_specs and sorted_top_level_component_specs != sorted(component_specs, key=lambda x: x.quantization_component):\n            raise ValueError('Currently unit-wise quantization spec only supports NO_QUANTIZE and same quantization method as the top-level `quantization_method`')",
            "def _populate_unitwise_quantization_specs(quantization_options: _QuantizationOptions) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Verifies and pupulates unitwise quantization specs.'\n    if not quantization_options.unit_wise_quantization_specs:\n        return\n    sorted_top_level_component_specs = sorted(quantization_options.quantization_method.quantization_component_specs, key=lambda x: x.quantization_component)\n    for unitwise_spec in quantization_options.unit_wise_quantization_specs:\n        if not unitwise_spec.unit:\n            raise ValueError('UnitWiseQuantizationSpec must contain at least one unit.')\n        for unit in unitwise_spec.unit:\n            if not unit.op_type and (not unit.node_name):\n                raise ValueError('Either `op_type` or `node_name` must be specified.')\n        _populate_quantization_component_spec(unitwise_spec.quantization_method)\n        component_specs = unitwise_spec.quantization_method.quantization_component_specs\n        if component_specs and sorted_top_level_component_specs != sorted(component_specs, key=lambda x: x.quantization_component):\n            raise ValueError('Currently unit-wise quantization spec only supports NO_QUANTIZE and same quantization method as the top-level `quantization_method`')",
            "def _populate_unitwise_quantization_specs(quantization_options: _QuantizationOptions) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Verifies and pupulates unitwise quantization specs.'\n    if not quantization_options.unit_wise_quantization_specs:\n        return\n    sorted_top_level_component_specs = sorted(quantization_options.quantization_method.quantization_component_specs, key=lambda x: x.quantization_component)\n    for unitwise_spec in quantization_options.unit_wise_quantization_specs:\n        if not unitwise_spec.unit:\n            raise ValueError('UnitWiseQuantizationSpec must contain at least one unit.')\n        for unit in unitwise_spec.unit:\n            if not unit.op_type and (not unit.node_name):\n                raise ValueError('Either `op_type` or `node_name` must be specified.')\n        _populate_quantization_component_spec(unitwise_spec.quantization_method)\n        component_specs = unitwise_spec.quantization_method.quantization_component_specs\n        if component_specs and sorted_top_level_component_specs != sorted(component_specs, key=lambda x: x.quantization_component):\n            raise ValueError('Currently unit-wise quantization spec only supports NO_QUANTIZE and same quantization method as the top-level `quantization_method`')",
            "def _populate_unitwise_quantization_specs(quantization_options: _QuantizationOptions) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Verifies and pupulates unitwise quantization specs.'\n    if not quantization_options.unit_wise_quantization_specs:\n        return\n    sorted_top_level_component_specs = sorted(quantization_options.quantization_method.quantization_component_specs, key=lambda x: x.quantization_component)\n    for unitwise_spec in quantization_options.unit_wise_quantization_specs:\n        if not unitwise_spec.unit:\n            raise ValueError('UnitWiseQuantizationSpec must contain at least one unit.')\n        for unit in unitwise_spec.unit:\n            if not unit.op_type and (not unit.node_name):\n                raise ValueError('Either `op_type` or `node_name` must be specified.')\n        _populate_quantization_component_spec(unitwise_spec.quantization_method)\n        component_specs = unitwise_spec.quantization_method.quantization_component_specs\n        if component_specs and sorted_top_level_component_specs != sorted(component_specs, key=lambda x: x.quantization_component):\n            raise ValueError('Currently unit-wise quantization spec only supports NO_QUANTIZE and same quantization method as the top-level `quantization_method`')"
        ]
    },
    {
        "func_name": "_populate_calibration_options",
        "original": "def _populate_calibration_options(quantization_options: quant_opts_pb2.QuantizationOptions):\n    \"\"\"Populates default values for CalibrationOptions.\n\n  Args:\n    quantization_options: An instance of QuantizationOptions with a field\n      specifying CalibrationOptions\n  \"\"\"\n    calib_opts = quantization_options.calibration_options\n    if calib_opts.calibration_method == _CalibrationMethod.CALIBRATION_METHOD_UNSPECIFIED:\n        calib_opts.calibration_method = _CalibrationMethod.CALIBRATION_METHOD_MIN_MAX\n    elif calib_opts.calibration_method == _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_PERCENTILE:\n        if not calib_opts.calibration_parameters.initial_num_bins:\n            calib_opts.calibration_parameters.initial_num_bins = 256\n        if not calib_opts.calibration_parameters.min_percentile:\n            calib_opts.calibration_parameters.min_percentile = 0.001\n        if not calib_opts.calibration_parameters.max_percentile:\n            calib_opts.calibration_parameters.max_percentile = 99.999\n    elif calib_opts.calibration_method in [_CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_BRUTEFORCE, _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_MAX_FREQUENCY, _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_SYMMETRIC]:\n        activation_tensor_type = quantization_options.quantization_method.quantization_component_specs[_QuantizationComponent.COMPONENT_ACTIVATION].tensor_type\n        if activation_tensor_type != _TensorType.TENSORTYPE_INT_8:\n            raise ValueError(f'Only TENSORTYPE_INT_8 is supported for HISTOGRAM_MSE calibration methods. calibration_method={calib_opts.calibration_method}')\n        if not calib_opts.calibration_parameters.initial_num_bins:\n            calib_opts.calibration_parameters.initial_num_bins = 256",
        "mutated": [
            "def _populate_calibration_options(quantization_options: quant_opts_pb2.QuantizationOptions):\n    if False:\n        i = 10\n    'Populates default values for CalibrationOptions.\\n\\n  Args:\\n    quantization_options: An instance of QuantizationOptions with a field\\n      specifying CalibrationOptions\\n  '\n    calib_opts = quantization_options.calibration_options\n    if calib_opts.calibration_method == _CalibrationMethod.CALIBRATION_METHOD_UNSPECIFIED:\n        calib_opts.calibration_method = _CalibrationMethod.CALIBRATION_METHOD_MIN_MAX\n    elif calib_opts.calibration_method == _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_PERCENTILE:\n        if not calib_opts.calibration_parameters.initial_num_bins:\n            calib_opts.calibration_parameters.initial_num_bins = 256\n        if not calib_opts.calibration_parameters.min_percentile:\n            calib_opts.calibration_parameters.min_percentile = 0.001\n        if not calib_opts.calibration_parameters.max_percentile:\n            calib_opts.calibration_parameters.max_percentile = 99.999\n    elif calib_opts.calibration_method in [_CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_BRUTEFORCE, _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_MAX_FREQUENCY, _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_SYMMETRIC]:\n        activation_tensor_type = quantization_options.quantization_method.quantization_component_specs[_QuantizationComponent.COMPONENT_ACTIVATION].tensor_type\n        if activation_tensor_type != _TensorType.TENSORTYPE_INT_8:\n            raise ValueError(f'Only TENSORTYPE_INT_8 is supported for HISTOGRAM_MSE calibration methods. calibration_method={calib_opts.calibration_method}')\n        if not calib_opts.calibration_parameters.initial_num_bins:\n            calib_opts.calibration_parameters.initial_num_bins = 256",
            "def _populate_calibration_options(quantization_options: quant_opts_pb2.QuantizationOptions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Populates default values for CalibrationOptions.\\n\\n  Args:\\n    quantization_options: An instance of QuantizationOptions with a field\\n      specifying CalibrationOptions\\n  '\n    calib_opts = quantization_options.calibration_options\n    if calib_opts.calibration_method == _CalibrationMethod.CALIBRATION_METHOD_UNSPECIFIED:\n        calib_opts.calibration_method = _CalibrationMethod.CALIBRATION_METHOD_MIN_MAX\n    elif calib_opts.calibration_method == _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_PERCENTILE:\n        if not calib_opts.calibration_parameters.initial_num_bins:\n            calib_opts.calibration_parameters.initial_num_bins = 256\n        if not calib_opts.calibration_parameters.min_percentile:\n            calib_opts.calibration_parameters.min_percentile = 0.001\n        if not calib_opts.calibration_parameters.max_percentile:\n            calib_opts.calibration_parameters.max_percentile = 99.999\n    elif calib_opts.calibration_method in [_CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_BRUTEFORCE, _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_MAX_FREQUENCY, _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_SYMMETRIC]:\n        activation_tensor_type = quantization_options.quantization_method.quantization_component_specs[_QuantizationComponent.COMPONENT_ACTIVATION].tensor_type\n        if activation_tensor_type != _TensorType.TENSORTYPE_INT_8:\n            raise ValueError(f'Only TENSORTYPE_INT_8 is supported for HISTOGRAM_MSE calibration methods. calibration_method={calib_opts.calibration_method}')\n        if not calib_opts.calibration_parameters.initial_num_bins:\n            calib_opts.calibration_parameters.initial_num_bins = 256",
            "def _populate_calibration_options(quantization_options: quant_opts_pb2.QuantizationOptions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Populates default values for CalibrationOptions.\\n\\n  Args:\\n    quantization_options: An instance of QuantizationOptions with a field\\n      specifying CalibrationOptions\\n  '\n    calib_opts = quantization_options.calibration_options\n    if calib_opts.calibration_method == _CalibrationMethod.CALIBRATION_METHOD_UNSPECIFIED:\n        calib_opts.calibration_method = _CalibrationMethod.CALIBRATION_METHOD_MIN_MAX\n    elif calib_opts.calibration_method == _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_PERCENTILE:\n        if not calib_opts.calibration_parameters.initial_num_bins:\n            calib_opts.calibration_parameters.initial_num_bins = 256\n        if not calib_opts.calibration_parameters.min_percentile:\n            calib_opts.calibration_parameters.min_percentile = 0.001\n        if not calib_opts.calibration_parameters.max_percentile:\n            calib_opts.calibration_parameters.max_percentile = 99.999\n    elif calib_opts.calibration_method in [_CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_BRUTEFORCE, _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_MAX_FREQUENCY, _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_SYMMETRIC]:\n        activation_tensor_type = quantization_options.quantization_method.quantization_component_specs[_QuantizationComponent.COMPONENT_ACTIVATION].tensor_type\n        if activation_tensor_type != _TensorType.TENSORTYPE_INT_8:\n            raise ValueError(f'Only TENSORTYPE_INT_8 is supported for HISTOGRAM_MSE calibration methods. calibration_method={calib_opts.calibration_method}')\n        if not calib_opts.calibration_parameters.initial_num_bins:\n            calib_opts.calibration_parameters.initial_num_bins = 256",
            "def _populate_calibration_options(quantization_options: quant_opts_pb2.QuantizationOptions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Populates default values for CalibrationOptions.\\n\\n  Args:\\n    quantization_options: An instance of QuantizationOptions with a field\\n      specifying CalibrationOptions\\n  '\n    calib_opts = quantization_options.calibration_options\n    if calib_opts.calibration_method == _CalibrationMethod.CALIBRATION_METHOD_UNSPECIFIED:\n        calib_opts.calibration_method = _CalibrationMethod.CALIBRATION_METHOD_MIN_MAX\n    elif calib_opts.calibration_method == _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_PERCENTILE:\n        if not calib_opts.calibration_parameters.initial_num_bins:\n            calib_opts.calibration_parameters.initial_num_bins = 256\n        if not calib_opts.calibration_parameters.min_percentile:\n            calib_opts.calibration_parameters.min_percentile = 0.001\n        if not calib_opts.calibration_parameters.max_percentile:\n            calib_opts.calibration_parameters.max_percentile = 99.999\n    elif calib_opts.calibration_method in [_CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_BRUTEFORCE, _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_MAX_FREQUENCY, _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_SYMMETRIC]:\n        activation_tensor_type = quantization_options.quantization_method.quantization_component_specs[_QuantizationComponent.COMPONENT_ACTIVATION].tensor_type\n        if activation_tensor_type != _TensorType.TENSORTYPE_INT_8:\n            raise ValueError(f'Only TENSORTYPE_INT_8 is supported for HISTOGRAM_MSE calibration methods. calibration_method={calib_opts.calibration_method}')\n        if not calib_opts.calibration_parameters.initial_num_bins:\n            calib_opts.calibration_parameters.initial_num_bins = 256",
            "def _populate_calibration_options(quantization_options: quant_opts_pb2.QuantizationOptions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Populates default values for CalibrationOptions.\\n\\n  Args:\\n    quantization_options: An instance of QuantizationOptions with a field\\n      specifying CalibrationOptions\\n  '\n    calib_opts = quantization_options.calibration_options\n    if calib_opts.calibration_method == _CalibrationMethod.CALIBRATION_METHOD_UNSPECIFIED:\n        calib_opts.calibration_method = _CalibrationMethod.CALIBRATION_METHOD_MIN_MAX\n    elif calib_opts.calibration_method == _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_PERCENTILE:\n        if not calib_opts.calibration_parameters.initial_num_bins:\n            calib_opts.calibration_parameters.initial_num_bins = 256\n        if not calib_opts.calibration_parameters.min_percentile:\n            calib_opts.calibration_parameters.min_percentile = 0.001\n        if not calib_opts.calibration_parameters.max_percentile:\n            calib_opts.calibration_parameters.max_percentile = 99.999\n    elif calib_opts.calibration_method in [_CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_BRUTEFORCE, _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_MAX_FREQUENCY, _CalibrationMethod.CALIBRATION_METHOD_HISTOGRAM_MSE_SYMMETRIC]:\n        activation_tensor_type = quantization_options.quantization_method.quantization_component_specs[_QuantizationComponent.COMPONENT_ACTIVATION].tensor_type\n        if activation_tensor_type != _TensorType.TENSORTYPE_INT_8:\n            raise ValueError(f'Only TENSORTYPE_INT_8 is supported for HISTOGRAM_MSE calibration methods. calibration_method={calib_opts.calibration_method}')\n        if not calib_opts.calibration_parameters.initial_num_bins:\n            calib_opts.calibration_parameters.initial_num_bins = 256"
        ]
    },
    {
        "func_name": "_populate_quantization_options_default_values",
        "original": "def _populate_quantization_options_default_values(quantization_options: _QuantizationOptions) -> None:\n    \"\"\"Populates default values for QuantizationOptions.\n\n  Populates unspecified or unset fields of QuantizationOptions with the default\n  values.\n\n  * If `op_set` is unspecified, it defaults to `OpSet.XLA`.\n  * If `freeze_all_variables` is not set, it defaults to `True`.\n  * Check if configurations are set correctly:\n    - Per-channel quantization is supported for Uniform Quantized opset only.\n\n  Args:\n    quantization_options: An instance of QuantizationOptions.\n  \"\"\"\n    if quantization_options.op_set == quant_opts_pb2.OpSet.OP_SET_UNSPECIFIED:\n        quantization_options.op_set = quant_opts_pb2.OpSet.XLA\n    if not quantization_options.tags:\n        quantization_options.tags.append(tag_constants.SERVING)\n    if not quantization_options.signature_keys:\n        quantization_options.signature_keys.append(signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)\n    if not quantization_options.HasField('freeze_all_variables'):\n        quantization_options.freeze_all_variables = True\n    if quantization_options.enable_legacy_weight_only:\n        raise ValueError('Legacy weight-only is deprecated. Use weight-only quantization method.')\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_UNSPECIFIED:\n        logging.debug('\"preset_method\" for QuantizationMethod is not specified.Static range quantization is used by default.')\n        quantization_options.quantization_method.preset_method = _PresetMethod.METHOD_STATIC_RANGE_INT8\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 or quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        if quantization_options.min_num_elements_for_weights == 0:\n            quantization_options.min_num_elements_for_weights = _DYNAMIC_RANGE_DEFAULT_MIN_NUM_ELEMENTS_FOR_WEIGHTS\n            logging.warning('QuantizationOptions.min_num_elements_for_weights is not set (0). Setting to the default value: %d.', _DYNAMIC_RANGE_DEFAULT_MIN_NUM_ELEMENTS_FOR_WEIGHTS)\n    if quantization_options.enable_per_channel_quantization and (not ((quantization_options.op_set == quant_opts_pb2.OpSet.UNIFORM_QUANTIZED or quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8) or (quantization_options.op_set == quant_opts_pb2.OpSet.XLA and quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8))):\n        raise ValueError('Currently, per-channel quantization is supported for Uniform Quantized opset, weight only quantization, or XLA opset with static range quantization.')\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 and (quantization_options.op_set == quant_opts_pb2.OpSet.UNIFORM_QUANTIZED or quantization_options.op_set == quant_opts_pb2.OpSet.TF):\n        raise ValueError('TF/Uniform quantized opset does not support weight-only.')\n    if quantization_options.op_set == quant_opts_pb2.OpSet.STABLEHLO and quantization_options.quantization_method.preset_method != _PresetMethod.METHOD_STATIC_RANGE_INT8:\n        raise ValueError('StableHLO quantized opset currently only supports static range quantization via TF Quantizer.')\n    if quantization_options.HasField('debugger_options'):\n        logging.debug('Setting `force_graph_mode_calibration = True` to ensure the debugging model is executed in graph mode during calibration, rather than eager mode.')\n        quantization_options.force_graph_mode_calibration = True\n        if not quantization_options.debugger_options.log_dir_path:\n            quantization_options.debugger_options.log_dir_path = '/tmp/dumps'\n        if quantization_options.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_UNSPECIFIED:\n            raise ValueError('Debugger is enabled but debugger type was not specified.')\n        if quantization_options.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_WHOLE_MODEL and (not quantization_options.debugger_options.unquantized_dump_model_path):\n            raise ValueError('Debugger type whole model verify was used but unquantized_dump_model_path was not specified.')\n    _populate_quantization_component_spec(quantization_options.quantization_method)\n    _populate_unitwise_quantization_specs(quantization_options)\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8:\n        _populate_calibration_options(quantization_options)",
        "mutated": [
            "def _populate_quantization_options_default_values(quantization_options: _QuantizationOptions) -> None:\n    if False:\n        i = 10\n    'Populates default values for QuantizationOptions.\\n\\n  Populates unspecified or unset fields of QuantizationOptions with the default\\n  values.\\n\\n  * If `op_set` is unspecified, it defaults to `OpSet.XLA`.\\n  * If `freeze_all_variables` is not set, it defaults to `True`.\\n  * Check if configurations are set correctly:\\n    - Per-channel quantization is supported for Uniform Quantized opset only.\\n\\n  Args:\\n    quantization_options: An instance of QuantizationOptions.\\n  '\n    if quantization_options.op_set == quant_opts_pb2.OpSet.OP_SET_UNSPECIFIED:\n        quantization_options.op_set = quant_opts_pb2.OpSet.XLA\n    if not quantization_options.tags:\n        quantization_options.tags.append(tag_constants.SERVING)\n    if not quantization_options.signature_keys:\n        quantization_options.signature_keys.append(signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)\n    if not quantization_options.HasField('freeze_all_variables'):\n        quantization_options.freeze_all_variables = True\n    if quantization_options.enable_legacy_weight_only:\n        raise ValueError('Legacy weight-only is deprecated. Use weight-only quantization method.')\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_UNSPECIFIED:\n        logging.debug('\"preset_method\" for QuantizationMethod is not specified.Static range quantization is used by default.')\n        quantization_options.quantization_method.preset_method = _PresetMethod.METHOD_STATIC_RANGE_INT8\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 or quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        if quantization_options.min_num_elements_for_weights == 0:\n            quantization_options.min_num_elements_for_weights = _DYNAMIC_RANGE_DEFAULT_MIN_NUM_ELEMENTS_FOR_WEIGHTS\n            logging.warning('QuantizationOptions.min_num_elements_for_weights is not set (0). Setting to the default value: %d.', _DYNAMIC_RANGE_DEFAULT_MIN_NUM_ELEMENTS_FOR_WEIGHTS)\n    if quantization_options.enable_per_channel_quantization and (not ((quantization_options.op_set == quant_opts_pb2.OpSet.UNIFORM_QUANTIZED or quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8) or (quantization_options.op_set == quant_opts_pb2.OpSet.XLA and quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8))):\n        raise ValueError('Currently, per-channel quantization is supported for Uniform Quantized opset, weight only quantization, or XLA opset with static range quantization.')\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 and (quantization_options.op_set == quant_opts_pb2.OpSet.UNIFORM_QUANTIZED or quantization_options.op_set == quant_opts_pb2.OpSet.TF):\n        raise ValueError('TF/Uniform quantized opset does not support weight-only.')\n    if quantization_options.op_set == quant_opts_pb2.OpSet.STABLEHLO and quantization_options.quantization_method.preset_method != _PresetMethod.METHOD_STATIC_RANGE_INT8:\n        raise ValueError('StableHLO quantized opset currently only supports static range quantization via TF Quantizer.')\n    if quantization_options.HasField('debugger_options'):\n        logging.debug('Setting `force_graph_mode_calibration = True` to ensure the debugging model is executed in graph mode during calibration, rather than eager mode.')\n        quantization_options.force_graph_mode_calibration = True\n        if not quantization_options.debugger_options.log_dir_path:\n            quantization_options.debugger_options.log_dir_path = '/tmp/dumps'\n        if quantization_options.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_UNSPECIFIED:\n            raise ValueError('Debugger is enabled but debugger type was not specified.')\n        if quantization_options.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_WHOLE_MODEL and (not quantization_options.debugger_options.unquantized_dump_model_path):\n            raise ValueError('Debugger type whole model verify was used but unquantized_dump_model_path was not specified.')\n    _populate_quantization_component_spec(quantization_options.quantization_method)\n    _populate_unitwise_quantization_specs(quantization_options)\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8:\n        _populate_calibration_options(quantization_options)",
            "def _populate_quantization_options_default_values(quantization_options: _QuantizationOptions) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Populates default values for QuantizationOptions.\\n\\n  Populates unspecified or unset fields of QuantizationOptions with the default\\n  values.\\n\\n  * If `op_set` is unspecified, it defaults to `OpSet.XLA`.\\n  * If `freeze_all_variables` is not set, it defaults to `True`.\\n  * Check if configurations are set correctly:\\n    - Per-channel quantization is supported for Uniform Quantized opset only.\\n\\n  Args:\\n    quantization_options: An instance of QuantizationOptions.\\n  '\n    if quantization_options.op_set == quant_opts_pb2.OpSet.OP_SET_UNSPECIFIED:\n        quantization_options.op_set = quant_opts_pb2.OpSet.XLA\n    if not quantization_options.tags:\n        quantization_options.tags.append(tag_constants.SERVING)\n    if not quantization_options.signature_keys:\n        quantization_options.signature_keys.append(signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)\n    if not quantization_options.HasField('freeze_all_variables'):\n        quantization_options.freeze_all_variables = True\n    if quantization_options.enable_legacy_weight_only:\n        raise ValueError('Legacy weight-only is deprecated. Use weight-only quantization method.')\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_UNSPECIFIED:\n        logging.debug('\"preset_method\" for QuantizationMethod is not specified.Static range quantization is used by default.')\n        quantization_options.quantization_method.preset_method = _PresetMethod.METHOD_STATIC_RANGE_INT8\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 or quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        if quantization_options.min_num_elements_for_weights == 0:\n            quantization_options.min_num_elements_for_weights = _DYNAMIC_RANGE_DEFAULT_MIN_NUM_ELEMENTS_FOR_WEIGHTS\n            logging.warning('QuantizationOptions.min_num_elements_for_weights is not set (0). Setting to the default value: %d.', _DYNAMIC_RANGE_DEFAULT_MIN_NUM_ELEMENTS_FOR_WEIGHTS)\n    if quantization_options.enable_per_channel_quantization and (not ((quantization_options.op_set == quant_opts_pb2.OpSet.UNIFORM_QUANTIZED or quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8) or (quantization_options.op_set == quant_opts_pb2.OpSet.XLA and quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8))):\n        raise ValueError('Currently, per-channel quantization is supported for Uniform Quantized opset, weight only quantization, or XLA opset with static range quantization.')\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 and (quantization_options.op_set == quant_opts_pb2.OpSet.UNIFORM_QUANTIZED or quantization_options.op_set == quant_opts_pb2.OpSet.TF):\n        raise ValueError('TF/Uniform quantized opset does not support weight-only.')\n    if quantization_options.op_set == quant_opts_pb2.OpSet.STABLEHLO and quantization_options.quantization_method.preset_method != _PresetMethod.METHOD_STATIC_RANGE_INT8:\n        raise ValueError('StableHLO quantized opset currently only supports static range quantization via TF Quantizer.')\n    if quantization_options.HasField('debugger_options'):\n        logging.debug('Setting `force_graph_mode_calibration = True` to ensure the debugging model is executed in graph mode during calibration, rather than eager mode.')\n        quantization_options.force_graph_mode_calibration = True\n        if not quantization_options.debugger_options.log_dir_path:\n            quantization_options.debugger_options.log_dir_path = '/tmp/dumps'\n        if quantization_options.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_UNSPECIFIED:\n            raise ValueError('Debugger is enabled but debugger type was not specified.')\n        if quantization_options.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_WHOLE_MODEL and (not quantization_options.debugger_options.unquantized_dump_model_path):\n            raise ValueError('Debugger type whole model verify was used but unquantized_dump_model_path was not specified.')\n    _populate_quantization_component_spec(quantization_options.quantization_method)\n    _populate_unitwise_quantization_specs(quantization_options)\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8:\n        _populate_calibration_options(quantization_options)",
            "def _populate_quantization_options_default_values(quantization_options: _QuantizationOptions) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Populates default values for QuantizationOptions.\\n\\n  Populates unspecified or unset fields of QuantizationOptions with the default\\n  values.\\n\\n  * If `op_set` is unspecified, it defaults to `OpSet.XLA`.\\n  * If `freeze_all_variables` is not set, it defaults to `True`.\\n  * Check if configurations are set correctly:\\n    - Per-channel quantization is supported for Uniform Quantized opset only.\\n\\n  Args:\\n    quantization_options: An instance of QuantizationOptions.\\n  '\n    if quantization_options.op_set == quant_opts_pb2.OpSet.OP_SET_UNSPECIFIED:\n        quantization_options.op_set = quant_opts_pb2.OpSet.XLA\n    if not quantization_options.tags:\n        quantization_options.tags.append(tag_constants.SERVING)\n    if not quantization_options.signature_keys:\n        quantization_options.signature_keys.append(signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)\n    if not quantization_options.HasField('freeze_all_variables'):\n        quantization_options.freeze_all_variables = True\n    if quantization_options.enable_legacy_weight_only:\n        raise ValueError('Legacy weight-only is deprecated. Use weight-only quantization method.')\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_UNSPECIFIED:\n        logging.debug('\"preset_method\" for QuantizationMethod is not specified.Static range quantization is used by default.')\n        quantization_options.quantization_method.preset_method = _PresetMethod.METHOD_STATIC_RANGE_INT8\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 or quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        if quantization_options.min_num_elements_for_weights == 0:\n            quantization_options.min_num_elements_for_weights = _DYNAMIC_RANGE_DEFAULT_MIN_NUM_ELEMENTS_FOR_WEIGHTS\n            logging.warning('QuantizationOptions.min_num_elements_for_weights is not set (0). Setting to the default value: %d.', _DYNAMIC_RANGE_DEFAULT_MIN_NUM_ELEMENTS_FOR_WEIGHTS)\n    if quantization_options.enable_per_channel_quantization and (not ((quantization_options.op_set == quant_opts_pb2.OpSet.UNIFORM_QUANTIZED or quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8) or (quantization_options.op_set == quant_opts_pb2.OpSet.XLA and quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8))):\n        raise ValueError('Currently, per-channel quantization is supported for Uniform Quantized opset, weight only quantization, or XLA opset with static range quantization.')\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 and (quantization_options.op_set == quant_opts_pb2.OpSet.UNIFORM_QUANTIZED or quantization_options.op_set == quant_opts_pb2.OpSet.TF):\n        raise ValueError('TF/Uniform quantized opset does not support weight-only.')\n    if quantization_options.op_set == quant_opts_pb2.OpSet.STABLEHLO and quantization_options.quantization_method.preset_method != _PresetMethod.METHOD_STATIC_RANGE_INT8:\n        raise ValueError('StableHLO quantized opset currently only supports static range quantization via TF Quantizer.')\n    if quantization_options.HasField('debugger_options'):\n        logging.debug('Setting `force_graph_mode_calibration = True` to ensure the debugging model is executed in graph mode during calibration, rather than eager mode.')\n        quantization_options.force_graph_mode_calibration = True\n        if not quantization_options.debugger_options.log_dir_path:\n            quantization_options.debugger_options.log_dir_path = '/tmp/dumps'\n        if quantization_options.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_UNSPECIFIED:\n            raise ValueError('Debugger is enabled but debugger type was not specified.')\n        if quantization_options.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_WHOLE_MODEL and (not quantization_options.debugger_options.unquantized_dump_model_path):\n            raise ValueError('Debugger type whole model verify was used but unquantized_dump_model_path was not specified.')\n    _populate_quantization_component_spec(quantization_options.quantization_method)\n    _populate_unitwise_quantization_specs(quantization_options)\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8:\n        _populate_calibration_options(quantization_options)",
            "def _populate_quantization_options_default_values(quantization_options: _QuantizationOptions) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Populates default values for QuantizationOptions.\\n\\n  Populates unspecified or unset fields of QuantizationOptions with the default\\n  values.\\n\\n  * If `op_set` is unspecified, it defaults to `OpSet.XLA`.\\n  * If `freeze_all_variables` is not set, it defaults to `True`.\\n  * Check if configurations are set correctly:\\n    - Per-channel quantization is supported for Uniform Quantized opset only.\\n\\n  Args:\\n    quantization_options: An instance of QuantizationOptions.\\n  '\n    if quantization_options.op_set == quant_opts_pb2.OpSet.OP_SET_UNSPECIFIED:\n        quantization_options.op_set = quant_opts_pb2.OpSet.XLA\n    if not quantization_options.tags:\n        quantization_options.tags.append(tag_constants.SERVING)\n    if not quantization_options.signature_keys:\n        quantization_options.signature_keys.append(signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)\n    if not quantization_options.HasField('freeze_all_variables'):\n        quantization_options.freeze_all_variables = True\n    if quantization_options.enable_legacy_weight_only:\n        raise ValueError('Legacy weight-only is deprecated. Use weight-only quantization method.')\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_UNSPECIFIED:\n        logging.debug('\"preset_method\" for QuantizationMethod is not specified.Static range quantization is used by default.')\n        quantization_options.quantization_method.preset_method = _PresetMethod.METHOD_STATIC_RANGE_INT8\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 or quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        if quantization_options.min_num_elements_for_weights == 0:\n            quantization_options.min_num_elements_for_weights = _DYNAMIC_RANGE_DEFAULT_MIN_NUM_ELEMENTS_FOR_WEIGHTS\n            logging.warning('QuantizationOptions.min_num_elements_for_weights is not set (0). Setting to the default value: %d.', _DYNAMIC_RANGE_DEFAULT_MIN_NUM_ELEMENTS_FOR_WEIGHTS)\n    if quantization_options.enable_per_channel_quantization and (not ((quantization_options.op_set == quant_opts_pb2.OpSet.UNIFORM_QUANTIZED or quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8) or (quantization_options.op_set == quant_opts_pb2.OpSet.XLA and quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8))):\n        raise ValueError('Currently, per-channel quantization is supported for Uniform Quantized opset, weight only quantization, or XLA opset with static range quantization.')\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 and (quantization_options.op_set == quant_opts_pb2.OpSet.UNIFORM_QUANTIZED or quantization_options.op_set == quant_opts_pb2.OpSet.TF):\n        raise ValueError('TF/Uniform quantized opset does not support weight-only.')\n    if quantization_options.op_set == quant_opts_pb2.OpSet.STABLEHLO and quantization_options.quantization_method.preset_method != _PresetMethod.METHOD_STATIC_RANGE_INT8:\n        raise ValueError('StableHLO quantized opset currently only supports static range quantization via TF Quantizer.')\n    if quantization_options.HasField('debugger_options'):\n        logging.debug('Setting `force_graph_mode_calibration = True` to ensure the debugging model is executed in graph mode during calibration, rather than eager mode.')\n        quantization_options.force_graph_mode_calibration = True\n        if not quantization_options.debugger_options.log_dir_path:\n            quantization_options.debugger_options.log_dir_path = '/tmp/dumps'\n        if quantization_options.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_UNSPECIFIED:\n            raise ValueError('Debugger is enabled but debugger type was not specified.')\n        if quantization_options.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_WHOLE_MODEL and (not quantization_options.debugger_options.unquantized_dump_model_path):\n            raise ValueError('Debugger type whole model verify was used but unquantized_dump_model_path was not specified.')\n    _populate_quantization_component_spec(quantization_options.quantization_method)\n    _populate_unitwise_quantization_specs(quantization_options)\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8:\n        _populate_calibration_options(quantization_options)",
            "def _populate_quantization_options_default_values(quantization_options: _QuantizationOptions) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Populates default values for QuantizationOptions.\\n\\n  Populates unspecified or unset fields of QuantizationOptions with the default\\n  values.\\n\\n  * If `op_set` is unspecified, it defaults to `OpSet.XLA`.\\n  * If `freeze_all_variables` is not set, it defaults to `True`.\\n  * Check if configurations are set correctly:\\n    - Per-channel quantization is supported for Uniform Quantized opset only.\\n\\n  Args:\\n    quantization_options: An instance of QuantizationOptions.\\n  '\n    if quantization_options.op_set == quant_opts_pb2.OpSet.OP_SET_UNSPECIFIED:\n        quantization_options.op_set = quant_opts_pb2.OpSet.XLA\n    if not quantization_options.tags:\n        quantization_options.tags.append(tag_constants.SERVING)\n    if not quantization_options.signature_keys:\n        quantization_options.signature_keys.append(signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)\n    if not quantization_options.HasField('freeze_all_variables'):\n        quantization_options.freeze_all_variables = True\n    if quantization_options.enable_legacy_weight_only:\n        raise ValueError('Legacy weight-only is deprecated. Use weight-only quantization method.')\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_UNSPECIFIED:\n        logging.debug('\"preset_method\" for QuantizationMethod is not specified.Static range quantization is used by default.')\n        quantization_options.quantization_method.preset_method = _PresetMethod.METHOD_STATIC_RANGE_INT8\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 or quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        if quantization_options.min_num_elements_for_weights == 0:\n            quantization_options.min_num_elements_for_weights = _DYNAMIC_RANGE_DEFAULT_MIN_NUM_ELEMENTS_FOR_WEIGHTS\n            logging.warning('QuantizationOptions.min_num_elements_for_weights is not set (0). Setting to the default value: %d.', _DYNAMIC_RANGE_DEFAULT_MIN_NUM_ELEMENTS_FOR_WEIGHTS)\n    if quantization_options.enable_per_channel_quantization and (not ((quantization_options.op_set == quant_opts_pb2.OpSet.UNIFORM_QUANTIZED or quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8) or (quantization_options.op_set == quant_opts_pb2.OpSet.XLA and quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8))):\n        raise ValueError('Currently, per-channel quantization is supported for Uniform Quantized opset, weight only quantization, or XLA opset with static range quantization.')\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8 and (quantization_options.op_set == quant_opts_pb2.OpSet.UNIFORM_QUANTIZED or quantization_options.op_set == quant_opts_pb2.OpSet.TF):\n        raise ValueError('TF/Uniform quantized opset does not support weight-only.')\n    if quantization_options.op_set == quant_opts_pb2.OpSet.STABLEHLO and quantization_options.quantization_method.preset_method != _PresetMethod.METHOD_STATIC_RANGE_INT8:\n        raise ValueError('StableHLO quantized opset currently only supports static range quantization via TF Quantizer.')\n    if quantization_options.HasField('debugger_options'):\n        logging.debug('Setting `force_graph_mode_calibration = True` to ensure the debugging model is executed in graph mode during calibration, rather than eager mode.')\n        quantization_options.force_graph_mode_calibration = True\n        if not quantization_options.debugger_options.log_dir_path:\n            quantization_options.debugger_options.log_dir_path = '/tmp/dumps'\n        if quantization_options.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_UNSPECIFIED:\n            raise ValueError('Debugger is enabled but debugger type was not specified.')\n        if quantization_options.debugger_options.debugger_type == quant_opts_pb2.DebuggerOptions.DebuggerType.DEBUGGER_TYPE_WHOLE_MODEL and (not quantization_options.debugger_options.unquantized_dump_model_path):\n            raise ValueError('Debugger type whole model verify was used but unquantized_dump_model_path was not specified.')\n    _populate_quantization_component_spec(quantization_options.quantization_method)\n    _populate_unitwise_quantization_specs(quantization_options)\n    if quantization_options.quantization_method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8:\n        _populate_calibration_options(quantization_options)"
        ]
    },
    {
        "func_name": "quantize",
        "original": "@tf_export.tf_export('quantization.experimental.quantize_saved_model')\ndef quantize(saved_model_path: str, output_directory: Optional[str]=None, quantization_options: Optional[_QuantizationOptions]=None, representative_dataset: Optional[repr_dataset.RepresentativeDatasetOrMapping]=None, *, overwrite_output_directory: bool=False) -> autotrackable.AutoTrackable:\n    \"\"\"Quantizes the SavedModel with the given quantization options.\n\n  Example usage:\n  ```python\n  # Quantizing a model trained with QAT.\n  quantization_options = tf.quantization.experimental.QuantizationOptions(\n      signature_keys=['your_signature_key'],\n  )\n  tf.quantization.experimental.quantize_saved_model(\n      '/tmp/input_model',\n      '/tmp/output_model',\n      quantization_options=quantization_options,\n  )\n\n  # When quantizing a model trained without QAT (Post-Training Quantization),\n  # a representative dataset is required.\n  representative_dataset = [{\"input\": tf.random.uniform(shape=(3, 3))}\n                        for _ in range(256)]\n  tf.quantization.experimental.quantize_saved_model(\n      '/tmp/input_model',\n      '/tmp/output_model',\n      quantization_options=quantization_options,\n      representative_dataset={'your_signature_key': representative_dataset},\n    )\n\n  # In addition to preset quantization methods, fine-grained control of\n  # quantization for each component is also supported.\n  _QuantizationComponentSpec = (\n      tf.quantization.experimental.QuantizationComponentSpec\n  )\n  quantization_options = tf.quantization.experimental.QuantizationOptions(\n      signature_keys=['your_signature_key'],\n      quantization_method=tf.quantization.experimental.QuantizationMethod(\n          quantization_component_specs=[\n              _QuantizationComponentSpec(\n                  quantization_component=(\n                      _QuantizationComponentSpec.COMPONENT_ACTIVATION\n                  ),\n                  tensor_type=_QuantizationComponentSpec.TENSORTYPE_INT_8,\n              )\n          ]\n      )\n  )\n  tf.quantization.experimental.quantize_saved_model(\n      '/tmp/input_model',\n      '/tmp/output_model',\n      quantization_options=quantization_options,\n  )\n  ```\n\n  Args:\n    saved_model_path: Path to the saved model. When representative_dataset is\n      not provided, this should be a model trained with QAT.\n    output_directory: The path to save the output SavedModel. Set\n      `overwrite_output_directory` to `True` to overwrite any existing contents\n      in the directory if not empty.\n    quantization_options: A set of options for quantization. If None, it uses\n      post-training static range quantization with XLA opset by default.\n    representative_dataset: an iterator that returns a dictionary of {input_key:\n      input_value} or a map from signature key to a dictionary of {input_key:\n      input_value} that feeds calibration data for quantizing model. The\n      representative should be provided when the model is a PTQ model. It can be\n      provided either via this parameter or via the `representative_datasets`\n      field in `QuantizationOptions`.\n    overwrite_output_directory: If set to true, overwrites the output directory\n      iff it isn't empty. The default value is false.\n\n  Returns:\n    A SavedModel object with TF quantization applied, or None if no quantization\n    is performed.\n\n  Raises:\n    ValueError: When 1) representative_dataset is not provided for non QAT model\n      for enabling static range quantization, 2) invalid value is provided as\n      a quantization method, or 3) provide representative dataset via both\n      argument and QuantizationOptions.\n    ValueError: When the specified quantization method is not yet supported.\n  \"\"\"\n    _verify_output_dir(output_directory, overwrite_output_directory)\n    if output_directory is None:\n        output_directory = tempfile.mkdtemp()\n    if quantization_options is None:\n        quantization_options = _QuantizationOptions()\n    _populate_quantization_options_default_values(quantization_options)\n    if representative_dataset is not None and quantization_options.representative_datasets:\n        raise ValueError('Do not specify both the `representative_dataset` argument and the `representative_datasets` field in `QuantizationOptions`.')\n    if quantization_options.representative_datasets:\n        representative_dataset = repr_dataset.RepresentativeDatasetLoader(quantization_options.representative_datasets).load()\n    method: _QuantizationMethod = quantization_options.quantization_method\n    if method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or method.preset_method == _PresetMethod.METHOD_NO_QUANTIZE:\n        return _static_range_quantize(saved_model_path, output_directory, quantization_options, representative_dataset)\n    elif method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        return _dynamic_range_quantize(saved_model_path, output_directory, quantization_options)\n    elif method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8:\n        return _weight_only_quantize(saved_model_path, output_directory, quantization_options)\n    else:\n        raise ValueError('Quantization method {method.preset_method} is not supported.')",
        "mutated": [
            "@tf_export.tf_export('quantization.experimental.quantize_saved_model')\ndef quantize(saved_model_path: str, output_directory: Optional[str]=None, quantization_options: Optional[_QuantizationOptions]=None, representative_dataset: Optional[repr_dataset.RepresentativeDatasetOrMapping]=None, *, overwrite_output_directory: bool=False) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n    'Quantizes the SavedModel with the given quantization options.\\n\\n  Example usage:\\n  ```python\\n  # Quantizing a model trained with QAT.\\n  quantization_options = tf.quantization.experimental.QuantizationOptions(\\n      signature_keys=[\\'your_signature_key\\'],\\n  )\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n  )\\n\\n  # When quantizing a model trained without QAT (Post-Training Quantization),\\n  # a representative dataset is required.\\n  representative_dataset = [{\"input\": tf.random.uniform(shape=(3, 3))}\\n                        for _ in range(256)]\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n      representative_dataset={\\'your_signature_key\\': representative_dataset},\\n    )\\n\\n  # In addition to preset quantization methods, fine-grained control of\\n  # quantization for each component is also supported.\\n  _QuantizationComponentSpec = (\\n      tf.quantization.experimental.QuantizationComponentSpec\\n  )\\n  quantization_options = tf.quantization.experimental.QuantizationOptions(\\n      signature_keys=[\\'your_signature_key\\'],\\n      quantization_method=tf.quantization.experimental.QuantizationMethod(\\n          quantization_component_specs=[\\n              _QuantizationComponentSpec(\\n                  quantization_component=(\\n                      _QuantizationComponentSpec.COMPONENT_ACTIVATION\\n                  ),\\n                  tensor_type=_QuantizationComponentSpec.TENSORTYPE_INT_8,\\n              )\\n          ]\\n      )\\n  )\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n  )\\n  ```\\n\\n  Args:\\n    saved_model_path: Path to the saved model. When representative_dataset is\\n      not provided, this should be a model trained with QAT.\\n    output_directory: The path to save the output SavedModel. Set\\n      `overwrite_output_directory` to `True` to overwrite any existing contents\\n      in the directory if not empty.\\n    quantization_options: A set of options for quantization. If None, it uses\\n      post-training static range quantization with XLA opset by default.\\n    representative_dataset: an iterator that returns a dictionary of {input_key:\\n      input_value} or a map from signature key to a dictionary of {input_key:\\n      input_value} that feeds calibration data for quantizing model. The\\n      representative should be provided when the model is a PTQ model. It can be\\n      provided either via this parameter or via the `representative_datasets`\\n      field in `QuantizationOptions`.\\n    overwrite_output_directory: If set to true, overwrites the output directory\\n      iff it isn\\'t empty. The default value is false.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied, or None if no quantization\\n    is performed.\\n\\n  Raises:\\n    ValueError: When 1) representative_dataset is not provided for non QAT model\\n      for enabling static range quantization, 2) invalid value is provided as\\n      a quantization method, or 3) provide representative dataset via both\\n      argument and QuantizationOptions.\\n    ValueError: When the specified quantization method is not yet supported.\\n  '\n    _verify_output_dir(output_directory, overwrite_output_directory)\n    if output_directory is None:\n        output_directory = tempfile.mkdtemp()\n    if quantization_options is None:\n        quantization_options = _QuantizationOptions()\n    _populate_quantization_options_default_values(quantization_options)\n    if representative_dataset is not None and quantization_options.representative_datasets:\n        raise ValueError('Do not specify both the `representative_dataset` argument and the `representative_datasets` field in `QuantizationOptions`.')\n    if quantization_options.representative_datasets:\n        representative_dataset = repr_dataset.RepresentativeDatasetLoader(quantization_options.representative_datasets).load()\n    method: _QuantizationMethod = quantization_options.quantization_method\n    if method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or method.preset_method == _PresetMethod.METHOD_NO_QUANTIZE:\n        return _static_range_quantize(saved_model_path, output_directory, quantization_options, representative_dataset)\n    elif method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        return _dynamic_range_quantize(saved_model_path, output_directory, quantization_options)\n    elif method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8:\n        return _weight_only_quantize(saved_model_path, output_directory, quantization_options)\n    else:\n        raise ValueError('Quantization method {method.preset_method} is not supported.')",
            "@tf_export.tf_export('quantization.experimental.quantize_saved_model')\ndef quantize(saved_model_path: str, output_directory: Optional[str]=None, quantization_options: Optional[_QuantizationOptions]=None, representative_dataset: Optional[repr_dataset.RepresentativeDatasetOrMapping]=None, *, overwrite_output_directory: bool=False) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Quantizes the SavedModel with the given quantization options.\\n\\n  Example usage:\\n  ```python\\n  # Quantizing a model trained with QAT.\\n  quantization_options = tf.quantization.experimental.QuantizationOptions(\\n      signature_keys=[\\'your_signature_key\\'],\\n  )\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n  )\\n\\n  # When quantizing a model trained without QAT (Post-Training Quantization),\\n  # a representative dataset is required.\\n  representative_dataset = [{\"input\": tf.random.uniform(shape=(3, 3))}\\n                        for _ in range(256)]\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n      representative_dataset={\\'your_signature_key\\': representative_dataset},\\n    )\\n\\n  # In addition to preset quantization methods, fine-grained control of\\n  # quantization for each component is also supported.\\n  _QuantizationComponentSpec = (\\n      tf.quantization.experimental.QuantizationComponentSpec\\n  )\\n  quantization_options = tf.quantization.experimental.QuantizationOptions(\\n      signature_keys=[\\'your_signature_key\\'],\\n      quantization_method=tf.quantization.experimental.QuantizationMethod(\\n          quantization_component_specs=[\\n              _QuantizationComponentSpec(\\n                  quantization_component=(\\n                      _QuantizationComponentSpec.COMPONENT_ACTIVATION\\n                  ),\\n                  tensor_type=_QuantizationComponentSpec.TENSORTYPE_INT_8,\\n              )\\n          ]\\n      )\\n  )\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n  )\\n  ```\\n\\n  Args:\\n    saved_model_path: Path to the saved model. When representative_dataset is\\n      not provided, this should be a model trained with QAT.\\n    output_directory: The path to save the output SavedModel. Set\\n      `overwrite_output_directory` to `True` to overwrite any existing contents\\n      in the directory if not empty.\\n    quantization_options: A set of options for quantization. If None, it uses\\n      post-training static range quantization with XLA opset by default.\\n    representative_dataset: an iterator that returns a dictionary of {input_key:\\n      input_value} or a map from signature key to a dictionary of {input_key:\\n      input_value} that feeds calibration data for quantizing model. The\\n      representative should be provided when the model is a PTQ model. It can be\\n      provided either via this parameter or via the `representative_datasets`\\n      field in `QuantizationOptions`.\\n    overwrite_output_directory: If set to true, overwrites the output directory\\n      iff it isn\\'t empty. The default value is false.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied, or None if no quantization\\n    is performed.\\n\\n  Raises:\\n    ValueError: When 1) representative_dataset is not provided for non QAT model\\n      for enabling static range quantization, 2) invalid value is provided as\\n      a quantization method, or 3) provide representative dataset via both\\n      argument and QuantizationOptions.\\n    ValueError: When the specified quantization method is not yet supported.\\n  '\n    _verify_output_dir(output_directory, overwrite_output_directory)\n    if output_directory is None:\n        output_directory = tempfile.mkdtemp()\n    if quantization_options is None:\n        quantization_options = _QuantizationOptions()\n    _populate_quantization_options_default_values(quantization_options)\n    if representative_dataset is not None and quantization_options.representative_datasets:\n        raise ValueError('Do not specify both the `representative_dataset` argument and the `representative_datasets` field in `QuantizationOptions`.')\n    if quantization_options.representative_datasets:\n        representative_dataset = repr_dataset.RepresentativeDatasetLoader(quantization_options.representative_datasets).load()\n    method: _QuantizationMethod = quantization_options.quantization_method\n    if method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or method.preset_method == _PresetMethod.METHOD_NO_QUANTIZE:\n        return _static_range_quantize(saved_model_path, output_directory, quantization_options, representative_dataset)\n    elif method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        return _dynamic_range_quantize(saved_model_path, output_directory, quantization_options)\n    elif method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8:\n        return _weight_only_quantize(saved_model_path, output_directory, quantization_options)\n    else:\n        raise ValueError('Quantization method {method.preset_method} is not supported.')",
            "@tf_export.tf_export('quantization.experimental.quantize_saved_model')\ndef quantize(saved_model_path: str, output_directory: Optional[str]=None, quantization_options: Optional[_QuantizationOptions]=None, representative_dataset: Optional[repr_dataset.RepresentativeDatasetOrMapping]=None, *, overwrite_output_directory: bool=False) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Quantizes the SavedModel with the given quantization options.\\n\\n  Example usage:\\n  ```python\\n  # Quantizing a model trained with QAT.\\n  quantization_options = tf.quantization.experimental.QuantizationOptions(\\n      signature_keys=[\\'your_signature_key\\'],\\n  )\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n  )\\n\\n  # When quantizing a model trained without QAT (Post-Training Quantization),\\n  # a representative dataset is required.\\n  representative_dataset = [{\"input\": tf.random.uniform(shape=(3, 3))}\\n                        for _ in range(256)]\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n      representative_dataset={\\'your_signature_key\\': representative_dataset},\\n    )\\n\\n  # In addition to preset quantization methods, fine-grained control of\\n  # quantization for each component is also supported.\\n  _QuantizationComponentSpec = (\\n      tf.quantization.experimental.QuantizationComponentSpec\\n  )\\n  quantization_options = tf.quantization.experimental.QuantizationOptions(\\n      signature_keys=[\\'your_signature_key\\'],\\n      quantization_method=tf.quantization.experimental.QuantizationMethod(\\n          quantization_component_specs=[\\n              _QuantizationComponentSpec(\\n                  quantization_component=(\\n                      _QuantizationComponentSpec.COMPONENT_ACTIVATION\\n                  ),\\n                  tensor_type=_QuantizationComponentSpec.TENSORTYPE_INT_8,\\n              )\\n          ]\\n      )\\n  )\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n  )\\n  ```\\n\\n  Args:\\n    saved_model_path: Path to the saved model. When representative_dataset is\\n      not provided, this should be a model trained with QAT.\\n    output_directory: The path to save the output SavedModel. Set\\n      `overwrite_output_directory` to `True` to overwrite any existing contents\\n      in the directory if not empty.\\n    quantization_options: A set of options for quantization. If None, it uses\\n      post-training static range quantization with XLA opset by default.\\n    representative_dataset: an iterator that returns a dictionary of {input_key:\\n      input_value} or a map from signature key to a dictionary of {input_key:\\n      input_value} that feeds calibration data for quantizing model. The\\n      representative should be provided when the model is a PTQ model. It can be\\n      provided either via this parameter or via the `representative_datasets`\\n      field in `QuantizationOptions`.\\n    overwrite_output_directory: If set to true, overwrites the output directory\\n      iff it isn\\'t empty. The default value is false.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied, or None if no quantization\\n    is performed.\\n\\n  Raises:\\n    ValueError: When 1) representative_dataset is not provided for non QAT model\\n      for enabling static range quantization, 2) invalid value is provided as\\n      a quantization method, or 3) provide representative dataset via both\\n      argument and QuantizationOptions.\\n    ValueError: When the specified quantization method is not yet supported.\\n  '\n    _verify_output_dir(output_directory, overwrite_output_directory)\n    if output_directory is None:\n        output_directory = tempfile.mkdtemp()\n    if quantization_options is None:\n        quantization_options = _QuantizationOptions()\n    _populate_quantization_options_default_values(quantization_options)\n    if representative_dataset is not None and quantization_options.representative_datasets:\n        raise ValueError('Do not specify both the `representative_dataset` argument and the `representative_datasets` field in `QuantizationOptions`.')\n    if quantization_options.representative_datasets:\n        representative_dataset = repr_dataset.RepresentativeDatasetLoader(quantization_options.representative_datasets).load()\n    method: _QuantizationMethod = quantization_options.quantization_method\n    if method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or method.preset_method == _PresetMethod.METHOD_NO_QUANTIZE:\n        return _static_range_quantize(saved_model_path, output_directory, quantization_options, representative_dataset)\n    elif method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        return _dynamic_range_quantize(saved_model_path, output_directory, quantization_options)\n    elif method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8:\n        return _weight_only_quantize(saved_model_path, output_directory, quantization_options)\n    else:\n        raise ValueError('Quantization method {method.preset_method} is not supported.')",
            "@tf_export.tf_export('quantization.experimental.quantize_saved_model')\ndef quantize(saved_model_path: str, output_directory: Optional[str]=None, quantization_options: Optional[_QuantizationOptions]=None, representative_dataset: Optional[repr_dataset.RepresentativeDatasetOrMapping]=None, *, overwrite_output_directory: bool=False) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Quantizes the SavedModel with the given quantization options.\\n\\n  Example usage:\\n  ```python\\n  # Quantizing a model trained with QAT.\\n  quantization_options = tf.quantization.experimental.QuantizationOptions(\\n      signature_keys=[\\'your_signature_key\\'],\\n  )\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n  )\\n\\n  # When quantizing a model trained without QAT (Post-Training Quantization),\\n  # a representative dataset is required.\\n  representative_dataset = [{\"input\": tf.random.uniform(shape=(3, 3))}\\n                        for _ in range(256)]\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n      representative_dataset={\\'your_signature_key\\': representative_dataset},\\n    )\\n\\n  # In addition to preset quantization methods, fine-grained control of\\n  # quantization for each component is also supported.\\n  _QuantizationComponentSpec = (\\n      tf.quantization.experimental.QuantizationComponentSpec\\n  )\\n  quantization_options = tf.quantization.experimental.QuantizationOptions(\\n      signature_keys=[\\'your_signature_key\\'],\\n      quantization_method=tf.quantization.experimental.QuantizationMethod(\\n          quantization_component_specs=[\\n              _QuantizationComponentSpec(\\n                  quantization_component=(\\n                      _QuantizationComponentSpec.COMPONENT_ACTIVATION\\n                  ),\\n                  tensor_type=_QuantizationComponentSpec.TENSORTYPE_INT_8,\\n              )\\n          ]\\n      )\\n  )\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n  )\\n  ```\\n\\n  Args:\\n    saved_model_path: Path to the saved model. When representative_dataset is\\n      not provided, this should be a model trained with QAT.\\n    output_directory: The path to save the output SavedModel. Set\\n      `overwrite_output_directory` to `True` to overwrite any existing contents\\n      in the directory if not empty.\\n    quantization_options: A set of options for quantization. If None, it uses\\n      post-training static range quantization with XLA opset by default.\\n    representative_dataset: an iterator that returns a dictionary of {input_key:\\n      input_value} or a map from signature key to a dictionary of {input_key:\\n      input_value} that feeds calibration data for quantizing model. The\\n      representative should be provided when the model is a PTQ model. It can be\\n      provided either via this parameter or via the `representative_datasets`\\n      field in `QuantizationOptions`.\\n    overwrite_output_directory: If set to true, overwrites the output directory\\n      iff it isn\\'t empty. The default value is false.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied, or None if no quantization\\n    is performed.\\n\\n  Raises:\\n    ValueError: When 1) representative_dataset is not provided for non QAT model\\n      for enabling static range quantization, 2) invalid value is provided as\\n      a quantization method, or 3) provide representative dataset via both\\n      argument and QuantizationOptions.\\n    ValueError: When the specified quantization method is not yet supported.\\n  '\n    _verify_output_dir(output_directory, overwrite_output_directory)\n    if output_directory is None:\n        output_directory = tempfile.mkdtemp()\n    if quantization_options is None:\n        quantization_options = _QuantizationOptions()\n    _populate_quantization_options_default_values(quantization_options)\n    if representative_dataset is not None and quantization_options.representative_datasets:\n        raise ValueError('Do not specify both the `representative_dataset` argument and the `representative_datasets` field in `QuantizationOptions`.')\n    if quantization_options.representative_datasets:\n        representative_dataset = repr_dataset.RepresentativeDatasetLoader(quantization_options.representative_datasets).load()\n    method: _QuantizationMethod = quantization_options.quantization_method\n    if method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or method.preset_method == _PresetMethod.METHOD_NO_QUANTIZE:\n        return _static_range_quantize(saved_model_path, output_directory, quantization_options, representative_dataset)\n    elif method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        return _dynamic_range_quantize(saved_model_path, output_directory, quantization_options)\n    elif method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8:\n        return _weight_only_quantize(saved_model_path, output_directory, quantization_options)\n    else:\n        raise ValueError('Quantization method {method.preset_method} is not supported.')",
            "@tf_export.tf_export('quantization.experimental.quantize_saved_model')\ndef quantize(saved_model_path: str, output_directory: Optional[str]=None, quantization_options: Optional[_QuantizationOptions]=None, representative_dataset: Optional[repr_dataset.RepresentativeDatasetOrMapping]=None, *, overwrite_output_directory: bool=False) -> autotrackable.AutoTrackable:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Quantizes the SavedModel with the given quantization options.\\n\\n  Example usage:\\n  ```python\\n  # Quantizing a model trained with QAT.\\n  quantization_options = tf.quantization.experimental.QuantizationOptions(\\n      signature_keys=[\\'your_signature_key\\'],\\n  )\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n  )\\n\\n  # When quantizing a model trained without QAT (Post-Training Quantization),\\n  # a representative dataset is required.\\n  representative_dataset = [{\"input\": tf.random.uniform(shape=(3, 3))}\\n                        for _ in range(256)]\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n      representative_dataset={\\'your_signature_key\\': representative_dataset},\\n    )\\n\\n  # In addition to preset quantization methods, fine-grained control of\\n  # quantization for each component is also supported.\\n  _QuantizationComponentSpec = (\\n      tf.quantization.experimental.QuantizationComponentSpec\\n  )\\n  quantization_options = tf.quantization.experimental.QuantizationOptions(\\n      signature_keys=[\\'your_signature_key\\'],\\n      quantization_method=tf.quantization.experimental.QuantizationMethod(\\n          quantization_component_specs=[\\n              _QuantizationComponentSpec(\\n                  quantization_component=(\\n                      _QuantizationComponentSpec.COMPONENT_ACTIVATION\\n                  ),\\n                  tensor_type=_QuantizationComponentSpec.TENSORTYPE_INT_8,\\n              )\\n          ]\\n      )\\n  )\\n  tf.quantization.experimental.quantize_saved_model(\\n      \\'/tmp/input_model\\',\\n      \\'/tmp/output_model\\',\\n      quantization_options=quantization_options,\\n  )\\n  ```\\n\\n  Args:\\n    saved_model_path: Path to the saved model. When representative_dataset is\\n      not provided, this should be a model trained with QAT.\\n    output_directory: The path to save the output SavedModel. Set\\n      `overwrite_output_directory` to `True` to overwrite any existing contents\\n      in the directory if not empty.\\n    quantization_options: A set of options for quantization. If None, it uses\\n      post-training static range quantization with XLA opset by default.\\n    representative_dataset: an iterator that returns a dictionary of {input_key:\\n      input_value} or a map from signature key to a dictionary of {input_key:\\n      input_value} that feeds calibration data for quantizing model. The\\n      representative should be provided when the model is a PTQ model. It can be\\n      provided either via this parameter or via the `representative_datasets`\\n      field in `QuantizationOptions`.\\n    overwrite_output_directory: If set to true, overwrites the output directory\\n      iff it isn\\'t empty. The default value is false.\\n\\n  Returns:\\n    A SavedModel object with TF quantization applied, or None if no quantization\\n    is performed.\\n\\n  Raises:\\n    ValueError: When 1) representative_dataset is not provided for non QAT model\\n      for enabling static range quantization, 2) invalid value is provided as\\n      a quantization method, or 3) provide representative dataset via both\\n      argument and QuantizationOptions.\\n    ValueError: When the specified quantization method is not yet supported.\\n  '\n    _verify_output_dir(output_directory, overwrite_output_directory)\n    if output_directory is None:\n        output_directory = tempfile.mkdtemp()\n    if quantization_options is None:\n        quantization_options = _QuantizationOptions()\n    _populate_quantization_options_default_values(quantization_options)\n    if representative_dataset is not None and quantization_options.representative_datasets:\n        raise ValueError('Do not specify both the `representative_dataset` argument and the `representative_datasets` field in `QuantizationOptions`.')\n    if quantization_options.representative_datasets:\n        representative_dataset = repr_dataset.RepresentativeDatasetLoader(quantization_options.representative_datasets).load()\n    method: _QuantizationMethod = quantization_options.quantization_method\n    if method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_INT8 or method.preset_method == _PresetMethod.METHOD_NO_QUANTIZE:\n        return _static_range_quantize(saved_model_path, output_directory, quantization_options, representative_dataset)\n    elif method.preset_method == _PresetMethod.METHOD_DYNAMIC_RANGE_INT8:\n        return _dynamic_range_quantize(saved_model_path, output_directory, quantization_options)\n    elif method.preset_method == _PresetMethod.METHOD_STATIC_RANGE_WEIGHT_ONLY_INT8:\n        return _weight_only_quantize(saved_model_path, output_directory, quantization_options)\n    else:\n        raise ValueError('Quantization method {method.preset_method} is not supported.')"
        ]
    }
]