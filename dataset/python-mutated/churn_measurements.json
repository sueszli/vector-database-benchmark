[
    {
        "func_name": "calibration",
        "original": "def calibration(prob, outcome, n_bins=10):\n    \"\"\"Calibration measurement for a set of predictions.\n\n    When predicting events at a given probability, how far is frequency\n    of positive outcomes from that probability?\n    NOTE: Lower scores are better\n\n    prob: array_like, float\n        Probability estimates for a set of events\n\n    outcome: array_like, bool\n        If event predicted occurred\n\n    n_bins: int\n        Number of judgement categories to prefrom calculation over.\n        Prediction are binned based on probability, since \"descrete\" \n        probabilities aren't required. \n\n    \"\"\"\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n    c = 0.0\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    bin_num = np.digitize(prob, judgement_bins)\n    for j_bin in np.unique(bin_num):\n        in_bin = bin_num == j_bin\n        predicted_prob = np.mean(prob[in_bin])\n        true_bin_prob = np.mean(outcome[in_bin])\n        c += np.sum(in_bin) * (predicted_prob - true_bin_prob) ** 2\n    return c / len(prob)",
        "mutated": [
            "def calibration(prob, outcome, n_bins=10):\n    if False:\n        i = 10\n    'Calibration measurement for a set of predictions.\\n\\n    When predicting events at a given probability, how far is frequency\\n    of positive outcomes from that probability?\\n    NOTE: Lower scores are better\\n\\n    prob: array_like, float\\n        Probability estimates for a set of events\\n\\n    outcome: array_like, bool\\n        If event predicted occurred\\n\\n    n_bins: int\\n        Number of judgement categories to prefrom calculation over.\\n        Prediction are binned based on probability, since \"descrete\" \\n        probabilities aren\\'t required. \\n\\n    '\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n    c = 0.0\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    bin_num = np.digitize(prob, judgement_bins)\n    for j_bin in np.unique(bin_num):\n        in_bin = bin_num == j_bin\n        predicted_prob = np.mean(prob[in_bin])\n        true_bin_prob = np.mean(outcome[in_bin])\n        c += np.sum(in_bin) * (predicted_prob - true_bin_prob) ** 2\n    return c / len(prob)",
            "def calibration(prob, outcome, n_bins=10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calibration measurement for a set of predictions.\\n\\n    When predicting events at a given probability, how far is frequency\\n    of positive outcomes from that probability?\\n    NOTE: Lower scores are better\\n\\n    prob: array_like, float\\n        Probability estimates for a set of events\\n\\n    outcome: array_like, bool\\n        If event predicted occurred\\n\\n    n_bins: int\\n        Number of judgement categories to prefrom calculation over.\\n        Prediction are binned based on probability, since \"descrete\" \\n        probabilities aren\\'t required. \\n\\n    '\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n    c = 0.0\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    bin_num = np.digitize(prob, judgement_bins)\n    for j_bin in np.unique(bin_num):\n        in_bin = bin_num == j_bin\n        predicted_prob = np.mean(prob[in_bin])\n        true_bin_prob = np.mean(outcome[in_bin])\n        c += np.sum(in_bin) * (predicted_prob - true_bin_prob) ** 2\n    return c / len(prob)",
            "def calibration(prob, outcome, n_bins=10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calibration measurement for a set of predictions.\\n\\n    When predicting events at a given probability, how far is frequency\\n    of positive outcomes from that probability?\\n    NOTE: Lower scores are better\\n\\n    prob: array_like, float\\n        Probability estimates for a set of events\\n\\n    outcome: array_like, bool\\n        If event predicted occurred\\n\\n    n_bins: int\\n        Number of judgement categories to prefrom calculation over.\\n        Prediction are binned based on probability, since \"descrete\" \\n        probabilities aren\\'t required. \\n\\n    '\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n    c = 0.0\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    bin_num = np.digitize(prob, judgement_bins)\n    for j_bin in np.unique(bin_num):\n        in_bin = bin_num == j_bin\n        predicted_prob = np.mean(prob[in_bin])\n        true_bin_prob = np.mean(outcome[in_bin])\n        c += np.sum(in_bin) * (predicted_prob - true_bin_prob) ** 2\n    return c / len(prob)",
            "def calibration(prob, outcome, n_bins=10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calibration measurement for a set of predictions.\\n\\n    When predicting events at a given probability, how far is frequency\\n    of positive outcomes from that probability?\\n    NOTE: Lower scores are better\\n\\n    prob: array_like, float\\n        Probability estimates for a set of events\\n\\n    outcome: array_like, bool\\n        If event predicted occurred\\n\\n    n_bins: int\\n        Number of judgement categories to prefrom calculation over.\\n        Prediction are binned based on probability, since \"descrete\" \\n        probabilities aren\\'t required. \\n\\n    '\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n    c = 0.0\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    bin_num = np.digitize(prob, judgement_bins)\n    for j_bin in np.unique(bin_num):\n        in_bin = bin_num == j_bin\n        predicted_prob = np.mean(prob[in_bin])\n        true_bin_prob = np.mean(outcome[in_bin])\n        c += np.sum(in_bin) * (predicted_prob - true_bin_prob) ** 2\n    return c / len(prob)",
            "def calibration(prob, outcome, n_bins=10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calibration measurement for a set of predictions.\\n\\n    When predicting events at a given probability, how far is frequency\\n    of positive outcomes from that probability?\\n    NOTE: Lower scores are better\\n\\n    prob: array_like, float\\n        Probability estimates for a set of events\\n\\n    outcome: array_like, bool\\n        If event predicted occurred\\n\\n    n_bins: int\\n        Number of judgement categories to prefrom calculation over.\\n        Prediction are binned based on probability, since \"descrete\" \\n        probabilities aren\\'t required. \\n\\n    '\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n    c = 0.0\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    bin_num = np.digitize(prob, judgement_bins)\n    for j_bin in np.unique(bin_num):\n        in_bin = bin_num == j_bin\n        predicted_prob = np.mean(prob[in_bin])\n        true_bin_prob = np.mean(outcome[in_bin])\n        c += np.sum(in_bin) * (predicted_prob - true_bin_prob) ** 2\n    return c / len(prob)"
        ]
    },
    {
        "func_name": "discrimination",
        "original": "def discrimination(prob, outcome, n_bins=10):\n    \"\"\"Discrimination measurement for a set of predictions.\n\n    For each judgement category, how far from the base probability\n    is the true frequency of that bin?\n    NOTE: High scores are better\n\n    prob: array_like, float\n        Probability estimates for a set of events\n\n    outcome: array_like, bool\n        If event predicted occurred\n\n    n_bins: int\n        Number of judgement categories to prefrom calculation over.\n        Prediction are binned based on probability, since \"descrete\" \n        probabilities aren't required. \n\n    \"\"\"\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n    d = 0.0\n    base_prob = np.mean(outcome)\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    bin_num = np.digitize(prob, judgement_bins)\n    for j_bin in np.unique(bin_num):\n        in_bin = bin_num == j_bin\n        true_bin_prob = np.mean(outcome[in_bin])\n        d += np.sum(in_bin) * (true_bin_prob - base_prob) ** 2\n    return d / len(prob)",
        "mutated": [
            "def discrimination(prob, outcome, n_bins=10):\n    if False:\n        i = 10\n    'Discrimination measurement for a set of predictions.\\n\\n    For each judgement category, how far from the base probability\\n    is the true frequency of that bin?\\n    NOTE: High scores are better\\n\\n    prob: array_like, float\\n        Probability estimates for a set of events\\n\\n    outcome: array_like, bool\\n        If event predicted occurred\\n\\n    n_bins: int\\n        Number of judgement categories to prefrom calculation over.\\n        Prediction are binned based on probability, since \"descrete\" \\n        probabilities aren\\'t required. \\n\\n    '\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n    d = 0.0\n    base_prob = np.mean(outcome)\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    bin_num = np.digitize(prob, judgement_bins)\n    for j_bin in np.unique(bin_num):\n        in_bin = bin_num == j_bin\n        true_bin_prob = np.mean(outcome[in_bin])\n        d += np.sum(in_bin) * (true_bin_prob - base_prob) ** 2\n    return d / len(prob)",
            "def discrimination(prob, outcome, n_bins=10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Discrimination measurement for a set of predictions.\\n\\n    For each judgement category, how far from the base probability\\n    is the true frequency of that bin?\\n    NOTE: High scores are better\\n\\n    prob: array_like, float\\n        Probability estimates for a set of events\\n\\n    outcome: array_like, bool\\n        If event predicted occurred\\n\\n    n_bins: int\\n        Number of judgement categories to prefrom calculation over.\\n        Prediction are binned based on probability, since \"descrete\" \\n        probabilities aren\\'t required. \\n\\n    '\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n    d = 0.0\n    base_prob = np.mean(outcome)\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    bin_num = np.digitize(prob, judgement_bins)\n    for j_bin in np.unique(bin_num):\n        in_bin = bin_num == j_bin\n        true_bin_prob = np.mean(outcome[in_bin])\n        d += np.sum(in_bin) * (true_bin_prob - base_prob) ** 2\n    return d / len(prob)",
            "def discrimination(prob, outcome, n_bins=10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Discrimination measurement for a set of predictions.\\n\\n    For each judgement category, how far from the base probability\\n    is the true frequency of that bin?\\n    NOTE: High scores are better\\n\\n    prob: array_like, float\\n        Probability estimates for a set of events\\n\\n    outcome: array_like, bool\\n        If event predicted occurred\\n\\n    n_bins: int\\n        Number of judgement categories to prefrom calculation over.\\n        Prediction are binned based on probability, since \"descrete\" \\n        probabilities aren\\'t required. \\n\\n    '\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n    d = 0.0\n    base_prob = np.mean(outcome)\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    bin_num = np.digitize(prob, judgement_bins)\n    for j_bin in np.unique(bin_num):\n        in_bin = bin_num == j_bin\n        true_bin_prob = np.mean(outcome[in_bin])\n        d += np.sum(in_bin) * (true_bin_prob - base_prob) ** 2\n    return d / len(prob)",
            "def discrimination(prob, outcome, n_bins=10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Discrimination measurement for a set of predictions.\\n\\n    For each judgement category, how far from the base probability\\n    is the true frequency of that bin?\\n    NOTE: High scores are better\\n\\n    prob: array_like, float\\n        Probability estimates for a set of events\\n\\n    outcome: array_like, bool\\n        If event predicted occurred\\n\\n    n_bins: int\\n        Number of judgement categories to prefrom calculation over.\\n        Prediction are binned based on probability, since \"descrete\" \\n        probabilities aren\\'t required. \\n\\n    '\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n    d = 0.0\n    base_prob = np.mean(outcome)\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    bin_num = np.digitize(prob, judgement_bins)\n    for j_bin in np.unique(bin_num):\n        in_bin = bin_num == j_bin\n        true_bin_prob = np.mean(outcome[in_bin])\n        d += np.sum(in_bin) * (true_bin_prob - base_prob) ** 2\n    return d / len(prob)",
            "def discrimination(prob, outcome, n_bins=10):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Discrimination measurement for a set of predictions.\\n\\n    For each judgement category, how far from the base probability\\n    is the true frequency of that bin?\\n    NOTE: High scores are better\\n\\n    prob: array_like, float\\n        Probability estimates for a set of events\\n\\n    outcome: array_like, bool\\n        If event predicted occurred\\n\\n    n_bins: int\\n        Number of judgement categories to prefrom calculation over.\\n        Prediction are binned based on probability, since \"descrete\" \\n        probabilities aren\\'t required. \\n\\n    '\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n    d = 0.0\n    base_prob = np.mean(outcome)\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    bin_num = np.digitize(prob, judgement_bins)\n    for j_bin in np.unique(bin_num):\n        in_bin = bin_num == j_bin\n        true_bin_prob = np.mean(outcome[in_bin])\n        d += np.sum(in_bin) * (true_bin_prob - base_prob) ** 2\n    return d / len(prob)"
        ]
    }
]