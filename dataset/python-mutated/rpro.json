[
    {
        "func_name": "__init__",
        "original": "def __init__(self, params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50), *, foreach: Optional[bool]=None, maximize: bool=False, differentiable: bool=False):\n    if not 0.0 <= lr:\n        raise ValueError(f'Invalid learning rate: {lr}')\n    if not 0.0 < etas[0] < 1.0 < etas[1]:\n        raise ValueError(f'Invalid eta values: {etas[0]}, {etas[1]}')\n    defaults = dict(lr=lr, etas=etas, step_sizes=step_sizes, foreach=foreach, maximize=maximize, differentiable=differentiable)\n    super().__init__(params, defaults)",
        "mutated": [
            "def __init__(self, params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50), *, foreach: Optional[bool]=None, maximize: bool=False, differentiable: bool=False):\n    if False:\n        i = 10\n    if not 0.0 <= lr:\n        raise ValueError(f'Invalid learning rate: {lr}')\n    if not 0.0 < etas[0] < 1.0 < etas[1]:\n        raise ValueError(f'Invalid eta values: {etas[0]}, {etas[1]}')\n    defaults = dict(lr=lr, etas=etas, step_sizes=step_sizes, foreach=foreach, maximize=maximize, differentiable=differentiable)\n    super().__init__(params, defaults)",
            "def __init__(self, params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50), *, foreach: Optional[bool]=None, maximize: bool=False, differentiable: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not 0.0 <= lr:\n        raise ValueError(f'Invalid learning rate: {lr}')\n    if not 0.0 < etas[0] < 1.0 < etas[1]:\n        raise ValueError(f'Invalid eta values: {etas[0]}, {etas[1]}')\n    defaults = dict(lr=lr, etas=etas, step_sizes=step_sizes, foreach=foreach, maximize=maximize, differentiable=differentiable)\n    super().__init__(params, defaults)",
            "def __init__(self, params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50), *, foreach: Optional[bool]=None, maximize: bool=False, differentiable: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not 0.0 <= lr:\n        raise ValueError(f'Invalid learning rate: {lr}')\n    if not 0.0 < etas[0] < 1.0 < etas[1]:\n        raise ValueError(f'Invalid eta values: {etas[0]}, {etas[1]}')\n    defaults = dict(lr=lr, etas=etas, step_sizes=step_sizes, foreach=foreach, maximize=maximize, differentiable=differentiable)\n    super().__init__(params, defaults)",
            "def __init__(self, params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50), *, foreach: Optional[bool]=None, maximize: bool=False, differentiable: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not 0.0 <= lr:\n        raise ValueError(f'Invalid learning rate: {lr}')\n    if not 0.0 < etas[0] < 1.0 < etas[1]:\n        raise ValueError(f'Invalid eta values: {etas[0]}, {etas[1]}')\n    defaults = dict(lr=lr, etas=etas, step_sizes=step_sizes, foreach=foreach, maximize=maximize, differentiable=differentiable)\n    super().__init__(params, defaults)",
            "def __init__(self, params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50), *, foreach: Optional[bool]=None, maximize: bool=False, differentiable: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not 0.0 <= lr:\n        raise ValueError(f'Invalid learning rate: {lr}')\n    if not 0.0 < etas[0] < 1.0 < etas[1]:\n        raise ValueError(f'Invalid eta values: {etas[0]}, {etas[1]}')\n    defaults = dict(lr=lr, etas=etas, step_sizes=step_sizes, foreach=foreach, maximize=maximize, differentiable=differentiable)\n    super().__init__(params, defaults)"
        ]
    },
    {
        "func_name": "__setstate__",
        "original": "def __setstate__(self, state):\n    super().__setstate__(state)\n    for group in self.param_groups:\n        group.setdefault('foreach', None)\n        group.setdefault('maximize', False)\n        group.setdefault('differentiable', False)",
        "mutated": [
            "def __setstate__(self, state):\n    if False:\n        i = 10\n    super().__setstate__(state)\n    for group in self.param_groups:\n        group.setdefault('foreach', None)\n        group.setdefault('maximize', False)\n        group.setdefault('differentiable', False)",
            "def __setstate__(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__setstate__(state)\n    for group in self.param_groups:\n        group.setdefault('foreach', None)\n        group.setdefault('maximize', False)\n        group.setdefault('differentiable', False)",
            "def __setstate__(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__setstate__(state)\n    for group in self.param_groups:\n        group.setdefault('foreach', None)\n        group.setdefault('maximize', False)\n        group.setdefault('differentiable', False)",
            "def __setstate__(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__setstate__(state)\n    for group in self.param_groups:\n        group.setdefault('foreach', None)\n        group.setdefault('maximize', False)\n        group.setdefault('differentiable', False)",
            "def __setstate__(self, state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__setstate__(state)\n    for group in self.param_groups:\n        group.setdefault('foreach', None)\n        group.setdefault('maximize', False)\n        group.setdefault('differentiable', False)"
        ]
    },
    {
        "func_name": "_init_group",
        "original": "def _init_group(self, group, params, grads, prevs, step_sizes):\n    has_complex = False\n    for p in group['params']:\n        if p.grad is None:\n            continue\n        has_complex |= torch.is_complex(p)\n        params.append(p)\n        grad = p.grad\n        if grad.is_sparse:\n            raise RuntimeError('Rprop does not support sparse gradients')\n        grads.append(grad)\n        state = self.state[p]\n        if len(state) == 0:\n            state['step'] = 0\n            state['prev'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n            if p.dtype.is_complex:\n                state['step_size'] = grad.new().resize_as_(grad).fill_(complex(group['lr'], group['lr']))\n            else:\n                state['step_size'] = grad.new().resize_as_(grad).fill_(group['lr'])\n        prevs.append(state['prev'])\n        step_sizes.append(state['step_size'])\n        state['step'] += 1\n    return has_complex",
        "mutated": [
            "def _init_group(self, group, params, grads, prevs, step_sizes):\n    if False:\n        i = 10\n    has_complex = False\n    for p in group['params']:\n        if p.grad is None:\n            continue\n        has_complex |= torch.is_complex(p)\n        params.append(p)\n        grad = p.grad\n        if grad.is_sparse:\n            raise RuntimeError('Rprop does not support sparse gradients')\n        grads.append(grad)\n        state = self.state[p]\n        if len(state) == 0:\n            state['step'] = 0\n            state['prev'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n            if p.dtype.is_complex:\n                state['step_size'] = grad.new().resize_as_(grad).fill_(complex(group['lr'], group['lr']))\n            else:\n                state['step_size'] = grad.new().resize_as_(grad).fill_(group['lr'])\n        prevs.append(state['prev'])\n        step_sizes.append(state['step_size'])\n        state['step'] += 1\n    return has_complex",
            "def _init_group(self, group, params, grads, prevs, step_sizes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    has_complex = False\n    for p in group['params']:\n        if p.grad is None:\n            continue\n        has_complex |= torch.is_complex(p)\n        params.append(p)\n        grad = p.grad\n        if grad.is_sparse:\n            raise RuntimeError('Rprop does not support sparse gradients')\n        grads.append(grad)\n        state = self.state[p]\n        if len(state) == 0:\n            state['step'] = 0\n            state['prev'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n            if p.dtype.is_complex:\n                state['step_size'] = grad.new().resize_as_(grad).fill_(complex(group['lr'], group['lr']))\n            else:\n                state['step_size'] = grad.new().resize_as_(grad).fill_(group['lr'])\n        prevs.append(state['prev'])\n        step_sizes.append(state['step_size'])\n        state['step'] += 1\n    return has_complex",
            "def _init_group(self, group, params, grads, prevs, step_sizes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    has_complex = False\n    for p in group['params']:\n        if p.grad is None:\n            continue\n        has_complex |= torch.is_complex(p)\n        params.append(p)\n        grad = p.grad\n        if grad.is_sparse:\n            raise RuntimeError('Rprop does not support sparse gradients')\n        grads.append(grad)\n        state = self.state[p]\n        if len(state) == 0:\n            state['step'] = 0\n            state['prev'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n            if p.dtype.is_complex:\n                state['step_size'] = grad.new().resize_as_(grad).fill_(complex(group['lr'], group['lr']))\n            else:\n                state['step_size'] = grad.new().resize_as_(grad).fill_(group['lr'])\n        prevs.append(state['prev'])\n        step_sizes.append(state['step_size'])\n        state['step'] += 1\n    return has_complex",
            "def _init_group(self, group, params, grads, prevs, step_sizes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    has_complex = False\n    for p in group['params']:\n        if p.grad is None:\n            continue\n        has_complex |= torch.is_complex(p)\n        params.append(p)\n        grad = p.grad\n        if grad.is_sparse:\n            raise RuntimeError('Rprop does not support sparse gradients')\n        grads.append(grad)\n        state = self.state[p]\n        if len(state) == 0:\n            state['step'] = 0\n            state['prev'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n            if p.dtype.is_complex:\n                state['step_size'] = grad.new().resize_as_(grad).fill_(complex(group['lr'], group['lr']))\n            else:\n                state['step_size'] = grad.new().resize_as_(grad).fill_(group['lr'])\n        prevs.append(state['prev'])\n        step_sizes.append(state['step_size'])\n        state['step'] += 1\n    return has_complex",
            "def _init_group(self, group, params, grads, prevs, step_sizes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    has_complex = False\n    for p in group['params']:\n        if p.grad is None:\n            continue\n        has_complex |= torch.is_complex(p)\n        params.append(p)\n        grad = p.grad\n        if grad.is_sparse:\n            raise RuntimeError('Rprop does not support sparse gradients')\n        grads.append(grad)\n        state = self.state[p]\n        if len(state) == 0:\n            state['step'] = 0\n            state['prev'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n            if p.dtype.is_complex:\n                state['step_size'] = grad.new().resize_as_(grad).fill_(complex(group['lr'], group['lr']))\n            else:\n                state['step_size'] = grad.new().resize_as_(grad).fill_(group['lr'])\n        prevs.append(state['prev'])\n        step_sizes.append(state['step_size'])\n        state['step'] += 1\n    return has_complex"
        ]
    },
    {
        "func_name": "step",
        "original": "@_use_grad_for_differentiable\ndef step(self, closure=None):\n    \"\"\"Performs a single optimization step.\n\n        Args:\n            closure (Callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n    for group in self.param_groups:\n        params = []\n        grads = []\n        prevs = []\n        step_sizes = []\n        (etaminus, etaplus) = group['etas']\n        (step_size_min, step_size_max) = group['step_sizes']\n        foreach = group['foreach']\n        maximize = group['maximize']\n        has_complex = self._init_group(group, params, grads, prevs, step_sizes)\n        rprop(params, grads, prevs, step_sizes, step_size_min=step_size_min, step_size_max=step_size_max, etaminus=etaminus, etaplus=etaplus, foreach=foreach, maximize=maximize, differentiable=group['differentiable'], has_complex=has_complex)\n    return loss",
        "mutated": [
            "@_use_grad_for_differentiable\ndef step(self, closure=None):\n    if False:\n        i = 10\n    'Performs a single optimization step.\\n\\n        Args:\\n            closure (Callable, optional): A closure that reevaluates the model\\n                and returns the loss.\\n        '\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n    for group in self.param_groups:\n        params = []\n        grads = []\n        prevs = []\n        step_sizes = []\n        (etaminus, etaplus) = group['etas']\n        (step_size_min, step_size_max) = group['step_sizes']\n        foreach = group['foreach']\n        maximize = group['maximize']\n        has_complex = self._init_group(group, params, grads, prevs, step_sizes)\n        rprop(params, grads, prevs, step_sizes, step_size_min=step_size_min, step_size_max=step_size_max, etaminus=etaminus, etaplus=etaplus, foreach=foreach, maximize=maximize, differentiable=group['differentiable'], has_complex=has_complex)\n    return loss",
            "@_use_grad_for_differentiable\ndef step(self, closure=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Performs a single optimization step.\\n\\n        Args:\\n            closure (Callable, optional): A closure that reevaluates the model\\n                and returns the loss.\\n        '\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n    for group in self.param_groups:\n        params = []\n        grads = []\n        prevs = []\n        step_sizes = []\n        (etaminus, etaplus) = group['etas']\n        (step_size_min, step_size_max) = group['step_sizes']\n        foreach = group['foreach']\n        maximize = group['maximize']\n        has_complex = self._init_group(group, params, grads, prevs, step_sizes)\n        rprop(params, grads, prevs, step_sizes, step_size_min=step_size_min, step_size_max=step_size_max, etaminus=etaminus, etaplus=etaplus, foreach=foreach, maximize=maximize, differentiable=group['differentiable'], has_complex=has_complex)\n    return loss",
            "@_use_grad_for_differentiable\ndef step(self, closure=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Performs a single optimization step.\\n\\n        Args:\\n            closure (Callable, optional): A closure that reevaluates the model\\n                and returns the loss.\\n        '\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n    for group in self.param_groups:\n        params = []\n        grads = []\n        prevs = []\n        step_sizes = []\n        (etaminus, etaplus) = group['etas']\n        (step_size_min, step_size_max) = group['step_sizes']\n        foreach = group['foreach']\n        maximize = group['maximize']\n        has_complex = self._init_group(group, params, grads, prevs, step_sizes)\n        rprop(params, grads, prevs, step_sizes, step_size_min=step_size_min, step_size_max=step_size_max, etaminus=etaminus, etaplus=etaplus, foreach=foreach, maximize=maximize, differentiable=group['differentiable'], has_complex=has_complex)\n    return loss",
            "@_use_grad_for_differentiable\ndef step(self, closure=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Performs a single optimization step.\\n\\n        Args:\\n            closure (Callable, optional): A closure that reevaluates the model\\n                and returns the loss.\\n        '\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n    for group in self.param_groups:\n        params = []\n        grads = []\n        prevs = []\n        step_sizes = []\n        (etaminus, etaplus) = group['etas']\n        (step_size_min, step_size_max) = group['step_sizes']\n        foreach = group['foreach']\n        maximize = group['maximize']\n        has_complex = self._init_group(group, params, grads, prevs, step_sizes)\n        rprop(params, grads, prevs, step_sizes, step_size_min=step_size_min, step_size_max=step_size_max, etaminus=etaminus, etaplus=etaplus, foreach=foreach, maximize=maximize, differentiable=group['differentiable'], has_complex=has_complex)\n    return loss",
            "@_use_grad_for_differentiable\ndef step(self, closure=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Performs a single optimization step.\\n\\n        Args:\\n            closure (Callable, optional): A closure that reevaluates the model\\n                and returns the loss.\\n        '\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n    for group in self.param_groups:\n        params = []\n        grads = []\n        prevs = []\n        step_sizes = []\n        (etaminus, etaplus) = group['etas']\n        (step_size_min, step_size_max) = group['step_sizes']\n        foreach = group['foreach']\n        maximize = group['maximize']\n        has_complex = self._init_group(group, params, grads, prevs, step_sizes)\n        rprop(params, grads, prevs, step_sizes, step_size_min=step_size_min, step_size_max=step_size_max, etaminus=etaminus, etaplus=etaplus, foreach=foreach, maximize=maximize, differentiable=group['differentiable'], has_complex=has_complex)\n    return loss"
        ]
    },
    {
        "func_name": "rprop",
        "original": "def rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], foreach: Optional[bool]=None, maximize: bool=False, differentiable: bool=False, has_complex: bool=False, *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float):\n    \"\"\"Functional API that performs rprop algorithm computation.\n\n    See :class:`~torch.optim.Rprop` for details.\n    \"\"\"\n    if foreach is None:\n        (_, foreach) = _default_to_fused_or_foreach(params, differentiable, use_fused=False)\n    if foreach and torch.jit.is_scripting():\n        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n    if foreach and (not torch.jit.is_scripting()):\n        func = _multi_tensor_rprop\n    else:\n        func = _single_tensor_rprop\n    func(params, grads, prevs, step_sizes, step_size_min=step_size_min, step_size_max=step_size_max, etaminus=etaminus, etaplus=etaplus, maximize=maximize, differentiable=differentiable, has_complex=has_complex)",
        "mutated": [
            "def rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], foreach: Optional[bool]=None, maximize: bool=False, differentiable: bool=False, has_complex: bool=False, *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float):\n    if False:\n        i = 10\n    'Functional API that performs rprop algorithm computation.\\n\\n    See :class:`~torch.optim.Rprop` for details.\\n    '\n    if foreach is None:\n        (_, foreach) = _default_to_fused_or_foreach(params, differentiable, use_fused=False)\n    if foreach and torch.jit.is_scripting():\n        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n    if foreach and (not torch.jit.is_scripting()):\n        func = _multi_tensor_rprop\n    else:\n        func = _single_tensor_rprop\n    func(params, grads, prevs, step_sizes, step_size_min=step_size_min, step_size_max=step_size_max, etaminus=etaminus, etaplus=etaplus, maximize=maximize, differentiable=differentiable, has_complex=has_complex)",
            "def rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], foreach: Optional[bool]=None, maximize: bool=False, differentiable: bool=False, has_complex: bool=False, *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Functional API that performs rprop algorithm computation.\\n\\n    See :class:`~torch.optim.Rprop` for details.\\n    '\n    if foreach is None:\n        (_, foreach) = _default_to_fused_or_foreach(params, differentiable, use_fused=False)\n    if foreach and torch.jit.is_scripting():\n        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n    if foreach and (not torch.jit.is_scripting()):\n        func = _multi_tensor_rprop\n    else:\n        func = _single_tensor_rprop\n    func(params, grads, prevs, step_sizes, step_size_min=step_size_min, step_size_max=step_size_max, etaminus=etaminus, etaplus=etaplus, maximize=maximize, differentiable=differentiable, has_complex=has_complex)",
            "def rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], foreach: Optional[bool]=None, maximize: bool=False, differentiable: bool=False, has_complex: bool=False, *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Functional API that performs rprop algorithm computation.\\n\\n    See :class:`~torch.optim.Rprop` for details.\\n    '\n    if foreach is None:\n        (_, foreach) = _default_to_fused_or_foreach(params, differentiable, use_fused=False)\n    if foreach and torch.jit.is_scripting():\n        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n    if foreach and (not torch.jit.is_scripting()):\n        func = _multi_tensor_rprop\n    else:\n        func = _single_tensor_rprop\n    func(params, grads, prevs, step_sizes, step_size_min=step_size_min, step_size_max=step_size_max, etaminus=etaminus, etaplus=etaplus, maximize=maximize, differentiable=differentiable, has_complex=has_complex)",
            "def rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], foreach: Optional[bool]=None, maximize: bool=False, differentiable: bool=False, has_complex: bool=False, *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Functional API that performs rprop algorithm computation.\\n\\n    See :class:`~torch.optim.Rprop` for details.\\n    '\n    if foreach is None:\n        (_, foreach) = _default_to_fused_or_foreach(params, differentiable, use_fused=False)\n    if foreach and torch.jit.is_scripting():\n        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n    if foreach and (not torch.jit.is_scripting()):\n        func = _multi_tensor_rprop\n    else:\n        func = _single_tensor_rprop\n    func(params, grads, prevs, step_sizes, step_size_min=step_size_min, step_size_max=step_size_max, etaminus=etaminus, etaplus=etaplus, maximize=maximize, differentiable=differentiable, has_complex=has_complex)",
            "def rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], foreach: Optional[bool]=None, maximize: bool=False, differentiable: bool=False, has_complex: bool=False, *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Functional API that performs rprop algorithm computation.\\n\\n    See :class:`~torch.optim.Rprop` for details.\\n    '\n    if foreach is None:\n        (_, foreach) = _default_to_fused_or_foreach(params, differentiable, use_fused=False)\n    if foreach and torch.jit.is_scripting():\n        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n    if foreach and (not torch.jit.is_scripting()):\n        func = _multi_tensor_rprop\n    else:\n        func = _single_tensor_rprop\n    func(params, grads, prevs, step_sizes, step_size_min=step_size_min, step_size_max=step_size_max, etaminus=etaminus, etaplus=etaplus, maximize=maximize, differentiable=differentiable, has_complex=has_complex)"
        ]
    },
    {
        "func_name": "_single_tensor_rprop",
        "original": "def _single_tensor_rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float, maximize: bool, differentiable: bool, has_complex: bool):\n    for (i, param) in enumerate(params):\n        grad = grads[i]\n        grad = grad if not maximize else -grad\n        prev = prevs[i]\n        step_size = step_sizes[i]\n        if torch.is_complex(param):\n            grad = torch.view_as_real(grad)\n            prev = torch.view_as_real(prev)\n            param = torch.view_as_real(param)\n            step_size = torch.view_as_real(step_size)\n        if differentiable:\n            sign = grad.mul(prev.clone()).sign()\n        else:\n            sign = grad.mul(prev).sign()\n        sign[sign.gt(0)] = etaplus\n        sign[sign.lt(0)] = etaminus\n        sign[sign.eq(0)] = 1\n        step_size.mul_(sign).clamp_(step_size_min, step_size_max)\n        grad = grad.clone(memory_format=torch.preserve_format)\n        grad[sign.eq(etaminus)] = 0\n        param.addcmul_(grad.sign(), step_size, value=-1)\n        prev.copy_(grad)",
        "mutated": [
            "def _single_tensor_rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float, maximize: bool, differentiable: bool, has_complex: bool):\n    if False:\n        i = 10\n    for (i, param) in enumerate(params):\n        grad = grads[i]\n        grad = grad if not maximize else -grad\n        prev = prevs[i]\n        step_size = step_sizes[i]\n        if torch.is_complex(param):\n            grad = torch.view_as_real(grad)\n            prev = torch.view_as_real(prev)\n            param = torch.view_as_real(param)\n            step_size = torch.view_as_real(step_size)\n        if differentiable:\n            sign = grad.mul(prev.clone()).sign()\n        else:\n            sign = grad.mul(prev).sign()\n        sign[sign.gt(0)] = etaplus\n        sign[sign.lt(0)] = etaminus\n        sign[sign.eq(0)] = 1\n        step_size.mul_(sign).clamp_(step_size_min, step_size_max)\n        grad = grad.clone(memory_format=torch.preserve_format)\n        grad[sign.eq(etaminus)] = 0\n        param.addcmul_(grad.sign(), step_size, value=-1)\n        prev.copy_(grad)",
            "def _single_tensor_rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float, maximize: bool, differentiable: bool, has_complex: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for (i, param) in enumerate(params):\n        grad = grads[i]\n        grad = grad if not maximize else -grad\n        prev = prevs[i]\n        step_size = step_sizes[i]\n        if torch.is_complex(param):\n            grad = torch.view_as_real(grad)\n            prev = torch.view_as_real(prev)\n            param = torch.view_as_real(param)\n            step_size = torch.view_as_real(step_size)\n        if differentiable:\n            sign = grad.mul(prev.clone()).sign()\n        else:\n            sign = grad.mul(prev).sign()\n        sign[sign.gt(0)] = etaplus\n        sign[sign.lt(0)] = etaminus\n        sign[sign.eq(0)] = 1\n        step_size.mul_(sign).clamp_(step_size_min, step_size_max)\n        grad = grad.clone(memory_format=torch.preserve_format)\n        grad[sign.eq(etaminus)] = 0\n        param.addcmul_(grad.sign(), step_size, value=-1)\n        prev.copy_(grad)",
            "def _single_tensor_rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float, maximize: bool, differentiable: bool, has_complex: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for (i, param) in enumerate(params):\n        grad = grads[i]\n        grad = grad if not maximize else -grad\n        prev = prevs[i]\n        step_size = step_sizes[i]\n        if torch.is_complex(param):\n            grad = torch.view_as_real(grad)\n            prev = torch.view_as_real(prev)\n            param = torch.view_as_real(param)\n            step_size = torch.view_as_real(step_size)\n        if differentiable:\n            sign = grad.mul(prev.clone()).sign()\n        else:\n            sign = grad.mul(prev).sign()\n        sign[sign.gt(0)] = etaplus\n        sign[sign.lt(0)] = etaminus\n        sign[sign.eq(0)] = 1\n        step_size.mul_(sign).clamp_(step_size_min, step_size_max)\n        grad = grad.clone(memory_format=torch.preserve_format)\n        grad[sign.eq(etaminus)] = 0\n        param.addcmul_(grad.sign(), step_size, value=-1)\n        prev.copy_(grad)",
            "def _single_tensor_rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float, maximize: bool, differentiable: bool, has_complex: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for (i, param) in enumerate(params):\n        grad = grads[i]\n        grad = grad if not maximize else -grad\n        prev = prevs[i]\n        step_size = step_sizes[i]\n        if torch.is_complex(param):\n            grad = torch.view_as_real(grad)\n            prev = torch.view_as_real(prev)\n            param = torch.view_as_real(param)\n            step_size = torch.view_as_real(step_size)\n        if differentiable:\n            sign = grad.mul(prev.clone()).sign()\n        else:\n            sign = grad.mul(prev).sign()\n        sign[sign.gt(0)] = etaplus\n        sign[sign.lt(0)] = etaminus\n        sign[sign.eq(0)] = 1\n        step_size.mul_(sign).clamp_(step_size_min, step_size_max)\n        grad = grad.clone(memory_format=torch.preserve_format)\n        grad[sign.eq(etaminus)] = 0\n        param.addcmul_(grad.sign(), step_size, value=-1)\n        prev.copy_(grad)",
            "def _single_tensor_rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float, maximize: bool, differentiable: bool, has_complex: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for (i, param) in enumerate(params):\n        grad = grads[i]\n        grad = grad if not maximize else -grad\n        prev = prevs[i]\n        step_size = step_sizes[i]\n        if torch.is_complex(param):\n            grad = torch.view_as_real(grad)\n            prev = torch.view_as_real(prev)\n            param = torch.view_as_real(param)\n            step_size = torch.view_as_real(step_size)\n        if differentiable:\n            sign = grad.mul(prev.clone()).sign()\n        else:\n            sign = grad.mul(prev).sign()\n        sign[sign.gt(0)] = etaplus\n        sign[sign.lt(0)] = etaminus\n        sign[sign.eq(0)] = 1\n        step_size.mul_(sign).clamp_(step_size_min, step_size_max)\n        grad = grad.clone(memory_format=torch.preserve_format)\n        grad[sign.eq(etaminus)] = 0\n        param.addcmul_(grad.sign(), step_size, value=-1)\n        prev.copy_(grad)"
        ]
    },
    {
        "func_name": "_multi_tensor_rprop",
        "original": "def _multi_tensor_rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float, maximize: bool, differentiable: bool, has_complex: bool):\n    if len(params) == 0:\n        return\n    assert not differentiable, \"_foreach ops don't support autograd\"\n    grouped_tensors = Optimizer._group_tensors_by_device_and_dtype([params, grads, prevs, step_sizes])\n    for ((grouped_params, grouped_grads, grouped_prevs, grouped_step_sizes), _) in grouped_tensors.values():\n        if has_complex:\n            _view_as_real(grouped_params, grouped_grads, grouped_prevs, grouped_step_sizes)\n        signs = torch._foreach_mul(grouped_grads, grouped_prevs)\n        if maximize:\n            torch._foreach_neg_(signs)\n        torch._foreach_copy_(grouped_prevs, grouped_grads)\n        if maximize:\n            torch._foreach_neg_(grouped_prevs)\n        grouped_grads = grouped_prevs\n        torch._foreach_sign_(signs)\n        for sign in signs:\n            sign[sign.gt(0)] = etaplus\n            sign[sign.lt(0)] = etaminus\n            sign[sign.eq(0)] = 1\n        torch._foreach_mul_(grouped_step_sizes, signs)\n        for step_size in grouped_step_sizes:\n            step_size.clamp_(step_size_min, step_size_max)\n        grouped_grads = list(grouped_grads)\n        for i in range(len(grouped_grads)):\n            grouped_grads[i][signs[i].eq(etaminus)] = 0\n        del signs\n        grad_signs = [grad.sign() for grad in grouped_grads]\n        torch._foreach_addcmul_(grouped_params, grad_signs, grouped_step_sizes, value=-1)",
        "mutated": [
            "def _multi_tensor_rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float, maximize: bool, differentiable: bool, has_complex: bool):\n    if False:\n        i = 10\n    if len(params) == 0:\n        return\n    assert not differentiable, \"_foreach ops don't support autograd\"\n    grouped_tensors = Optimizer._group_tensors_by_device_and_dtype([params, grads, prevs, step_sizes])\n    for ((grouped_params, grouped_grads, grouped_prevs, grouped_step_sizes), _) in grouped_tensors.values():\n        if has_complex:\n            _view_as_real(grouped_params, grouped_grads, grouped_prevs, grouped_step_sizes)\n        signs = torch._foreach_mul(grouped_grads, grouped_prevs)\n        if maximize:\n            torch._foreach_neg_(signs)\n        torch._foreach_copy_(grouped_prevs, grouped_grads)\n        if maximize:\n            torch._foreach_neg_(grouped_prevs)\n        grouped_grads = grouped_prevs\n        torch._foreach_sign_(signs)\n        for sign in signs:\n            sign[sign.gt(0)] = etaplus\n            sign[sign.lt(0)] = etaminus\n            sign[sign.eq(0)] = 1\n        torch._foreach_mul_(grouped_step_sizes, signs)\n        for step_size in grouped_step_sizes:\n            step_size.clamp_(step_size_min, step_size_max)\n        grouped_grads = list(grouped_grads)\n        for i in range(len(grouped_grads)):\n            grouped_grads[i][signs[i].eq(etaminus)] = 0\n        del signs\n        grad_signs = [grad.sign() for grad in grouped_grads]\n        torch._foreach_addcmul_(grouped_params, grad_signs, grouped_step_sizes, value=-1)",
            "def _multi_tensor_rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float, maximize: bool, differentiable: bool, has_complex: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if len(params) == 0:\n        return\n    assert not differentiable, \"_foreach ops don't support autograd\"\n    grouped_tensors = Optimizer._group_tensors_by_device_and_dtype([params, grads, prevs, step_sizes])\n    for ((grouped_params, grouped_grads, grouped_prevs, grouped_step_sizes), _) in grouped_tensors.values():\n        if has_complex:\n            _view_as_real(grouped_params, grouped_grads, grouped_prevs, grouped_step_sizes)\n        signs = torch._foreach_mul(grouped_grads, grouped_prevs)\n        if maximize:\n            torch._foreach_neg_(signs)\n        torch._foreach_copy_(grouped_prevs, grouped_grads)\n        if maximize:\n            torch._foreach_neg_(grouped_prevs)\n        grouped_grads = grouped_prevs\n        torch._foreach_sign_(signs)\n        for sign in signs:\n            sign[sign.gt(0)] = etaplus\n            sign[sign.lt(0)] = etaminus\n            sign[sign.eq(0)] = 1\n        torch._foreach_mul_(grouped_step_sizes, signs)\n        for step_size in grouped_step_sizes:\n            step_size.clamp_(step_size_min, step_size_max)\n        grouped_grads = list(grouped_grads)\n        for i in range(len(grouped_grads)):\n            grouped_grads[i][signs[i].eq(etaminus)] = 0\n        del signs\n        grad_signs = [grad.sign() for grad in grouped_grads]\n        torch._foreach_addcmul_(grouped_params, grad_signs, grouped_step_sizes, value=-1)",
            "def _multi_tensor_rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float, maximize: bool, differentiable: bool, has_complex: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if len(params) == 0:\n        return\n    assert not differentiable, \"_foreach ops don't support autograd\"\n    grouped_tensors = Optimizer._group_tensors_by_device_and_dtype([params, grads, prevs, step_sizes])\n    for ((grouped_params, grouped_grads, grouped_prevs, grouped_step_sizes), _) in grouped_tensors.values():\n        if has_complex:\n            _view_as_real(grouped_params, grouped_grads, grouped_prevs, grouped_step_sizes)\n        signs = torch._foreach_mul(grouped_grads, grouped_prevs)\n        if maximize:\n            torch._foreach_neg_(signs)\n        torch._foreach_copy_(grouped_prevs, grouped_grads)\n        if maximize:\n            torch._foreach_neg_(grouped_prevs)\n        grouped_grads = grouped_prevs\n        torch._foreach_sign_(signs)\n        for sign in signs:\n            sign[sign.gt(0)] = etaplus\n            sign[sign.lt(0)] = etaminus\n            sign[sign.eq(0)] = 1\n        torch._foreach_mul_(grouped_step_sizes, signs)\n        for step_size in grouped_step_sizes:\n            step_size.clamp_(step_size_min, step_size_max)\n        grouped_grads = list(grouped_grads)\n        for i in range(len(grouped_grads)):\n            grouped_grads[i][signs[i].eq(etaminus)] = 0\n        del signs\n        grad_signs = [grad.sign() for grad in grouped_grads]\n        torch._foreach_addcmul_(grouped_params, grad_signs, grouped_step_sizes, value=-1)",
            "def _multi_tensor_rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float, maximize: bool, differentiable: bool, has_complex: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if len(params) == 0:\n        return\n    assert not differentiable, \"_foreach ops don't support autograd\"\n    grouped_tensors = Optimizer._group_tensors_by_device_and_dtype([params, grads, prevs, step_sizes])\n    for ((grouped_params, grouped_grads, grouped_prevs, grouped_step_sizes), _) in grouped_tensors.values():\n        if has_complex:\n            _view_as_real(grouped_params, grouped_grads, grouped_prevs, grouped_step_sizes)\n        signs = torch._foreach_mul(grouped_grads, grouped_prevs)\n        if maximize:\n            torch._foreach_neg_(signs)\n        torch._foreach_copy_(grouped_prevs, grouped_grads)\n        if maximize:\n            torch._foreach_neg_(grouped_prevs)\n        grouped_grads = grouped_prevs\n        torch._foreach_sign_(signs)\n        for sign in signs:\n            sign[sign.gt(0)] = etaplus\n            sign[sign.lt(0)] = etaminus\n            sign[sign.eq(0)] = 1\n        torch._foreach_mul_(grouped_step_sizes, signs)\n        for step_size in grouped_step_sizes:\n            step_size.clamp_(step_size_min, step_size_max)\n        grouped_grads = list(grouped_grads)\n        for i in range(len(grouped_grads)):\n            grouped_grads[i][signs[i].eq(etaminus)] = 0\n        del signs\n        grad_signs = [grad.sign() for grad in grouped_grads]\n        torch._foreach_addcmul_(grouped_params, grad_signs, grouped_step_sizes, value=-1)",
            "def _multi_tensor_rprop(params: List[Tensor], grads: List[Tensor], prevs: List[Tensor], step_sizes: List[Tensor], *, step_size_min: float, step_size_max: float, etaminus: float, etaplus: float, maximize: bool, differentiable: bool, has_complex: bool):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if len(params) == 0:\n        return\n    assert not differentiable, \"_foreach ops don't support autograd\"\n    grouped_tensors = Optimizer._group_tensors_by_device_and_dtype([params, grads, prevs, step_sizes])\n    for ((grouped_params, grouped_grads, grouped_prevs, grouped_step_sizes), _) in grouped_tensors.values():\n        if has_complex:\n            _view_as_real(grouped_params, grouped_grads, grouped_prevs, grouped_step_sizes)\n        signs = torch._foreach_mul(grouped_grads, grouped_prevs)\n        if maximize:\n            torch._foreach_neg_(signs)\n        torch._foreach_copy_(grouped_prevs, grouped_grads)\n        if maximize:\n            torch._foreach_neg_(grouped_prevs)\n        grouped_grads = grouped_prevs\n        torch._foreach_sign_(signs)\n        for sign in signs:\n            sign[sign.gt(0)] = etaplus\n            sign[sign.lt(0)] = etaminus\n            sign[sign.eq(0)] = 1\n        torch._foreach_mul_(grouped_step_sizes, signs)\n        for step_size in grouped_step_sizes:\n            step_size.clamp_(step_size_min, step_size_max)\n        grouped_grads = list(grouped_grads)\n        for i in range(len(grouped_grads)):\n            grouped_grads[i][signs[i].eq(etaminus)] = 0\n        del signs\n        grad_signs = [grad.sign() for grad in grouped_grads]\n        torch._foreach_addcmul_(grouped_params, grad_signs, grouped_step_sizes, value=-1)"
        ]
    }
]