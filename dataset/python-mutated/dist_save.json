[
    {
        "func_name": "save",
        "original": "@dygraph_only\ndef save(state_dict, path, **configs):\n    \"\"\"\n    Save a state dict to the specified path in both distributed and single-card environment.\n\n    Note:\n        Now supports saving ``state_dict`` of Layer/Optimizer, Tensor and nested structure containing Tensor, Program.\n\n    Note:\n        Different from ``paddle.jit.save``, since the save result of ``paddle.save`` is a single file,\n        there is no need to distinguish multiple saved files by adding a suffix. The argument ``path``\n        of ``paddle.save`` will be directly used as the saved file name instead of a prefix.\n        In order to unify the saved file name format, we recommend using the paddle standard suffix:\n        1. for ``Layer.state_dict`` , recommend to use ``.pdparams`` ;\n        2. for ``Optimizer.state_dict`` , recommend to use ``.pdopt`` .\n        For specific examples, please refer to API code examples.\n\n    Args:\n        obj(Object) : The object to be saved.\n        path(str|BytesIO) : The path/buffer of the object to be saved.\n          If saved in the current directory, the input path string will be used as the file name.\n        protocol(int, optional): The protocol version of pickle module must be greater than 1 and less than 5.\n                                 Default: 4\n        **configs(dict, optional): optional keyword arguments. The following options are currently supported:\n          (1)use_binary_format(bool):\n            To be used in paddle.save. When the saved object is static graph variable, you can specify ``use_binary_for_var``.\n            If True, save the file in the c++ binary format when saving a single static graph variable; otherwise, save it in pickle format.\n            Default: False\n          (2)gather_to(int|list|tuple|None):\n            To specify which global rank to save in.Defalut is None.\n            None value means distributed saving with no gathering to a single card.\n          (3)state_type(str):\n            Value can be 'params' or 'opt', specifying to save parametres or optimizer state.\n          (4)max_grouped_size(str|int):\n            To limit the max size(how many bits) a object group to be transfered a time.\n            If str, the format must be as num+'G/M/K', for example, 3G, 2K, 10M, etc. Default is 3G.\n    Returns:\n        None\n    Examples:\n        import paddle\n        paddle.distributed.init_process_group(backend='nccl')\n        paddle.distributed.fleet.init(is_collective=True)\n\n        model = build_model()\n        optimizer = build_optimizer(model)\n\n        dist_optimizer = paddle.distributed_optimizer(optimizer)\n        dist_model = paddle.distributed_optimizer(model)\n\n        # gather params to rank 0 and then save\n        paddle.incubate.distributed.utils.io.save(model.state_dict(), path=\"path/to/save.pdparams\", gather_to=[0], state_type=\"params\")\n\n        # save whoe params on all ranks\n        paddle.incubate.distributed.utils.io.save(model.state_dict(), path=\"path/to/save.pdparams\", gather_to=[0,1], state_type=\"params\")\n\n        # save optimizer state dict on rank 0\n        paddle.incubate.distributed.utils.io.save(optimizer.state_dict(), path=\"path/to/save.pdopt\", gather=0, state_type=\"opt\")\n\n    \"\"\"\n    gather_to = configs.get('gather_to', None)\n    if dist.get_world_size() == 1 or gather_to is None:\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    state_type = configs.get('state_type', None)\n    assert isinstance(state_type, str), \"must pass an arg state_type='params' or state_type='opt' to specify whether to save model state_dict or optimizer state_dict\"\n    assert state_type in ['params', 'opt'], \"must pass an arg state_type='params' or state_type='opt'\"\n    if re.search(f'{state_type}$', path) is None:\n        logger.warning(f'You are saving {state_type}, while the path({path} does not end with {state_type})')\n    hcg = fleet.get_hybrid_communicate_group()\n    assert hcg.get_model_parallel_world_size() == 1 and hcg.get_pipe_parallel_world_size() == 1, f'Only DP and Sharding is supported now. However, current MP={hcg.get_model_parallel_world_size()} , PP={hcg.get_pipe_parallel_world_size()}'\n    sharding_group = hcg.get_sharding_parallel_group()\n    dp_group = hcg.get_data_parallel_group()\n    if state_type == 'params':\n        if dp_group.nranks > 1:\n            assert _same_keys(state_dict, dp_group), 'only sharding stage 1/2 and DP are supported now'\n        if sharding_group.nranks > 1:\n            assert _same_keys(state_dict, sharding_group), 'only sharding stage 1/2 and DP are supported now'\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    if sharding_group.nranks == 1:\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    if _same_keys(state_dict, sharding_group):\n        return paddle.save(state_dict, path, **configs)\n    assert isinstance(gather_to, (list, tuple, int))\n    if isinstance(gather_to, int):\n        gather_to = [gather_to]\n    max_size = configs.get('max_grouped_size', '3G')\n    try:\n        logger.info('state_dict_keys:' + str(state_dict.keys()))\n        gathered_state_dict = _gather_state_dict(state_dict, gather_to, sharding_group, max_size=max_size)\n        logger.info('gathered_state_dict_keys:' + str(state_dict.keys()))\n        if dist.get_rank() in gather_to:\n            configs = _remove_not_supported_conf(configs)\n            paddle.save(gathered_state_dict, path, **configs)\n    except:\n        raise RuntimeError(f'Saving failed. Follwing are some suggestions:\\n    1) pass the param max_grouped_size to turn the grouped size smaller (current value of max_grouped_size is {max_size})\\n    2) if sharding stage is 1, use paddle.save rather than paddle.distributed.save\\n    3) Concat the developers\\n')",
        "mutated": [
            "@dygraph_only\ndef save(state_dict, path, **configs):\n    if False:\n        i = 10\n    '\\n    Save a state dict to the specified path in both distributed and single-card environment.\\n\\n    Note:\\n        Now supports saving ``state_dict`` of Layer/Optimizer, Tensor and nested structure containing Tensor, Program.\\n\\n    Note:\\n        Different from ``paddle.jit.save``, since the save result of ``paddle.save`` is a single file,\\n        there is no need to distinguish multiple saved files by adding a suffix. The argument ``path``\\n        of ``paddle.save`` will be directly used as the saved file name instead of a prefix.\\n        In order to unify the saved file name format, we recommend using the paddle standard suffix:\\n        1. for ``Layer.state_dict`` , recommend to use ``.pdparams`` ;\\n        2. for ``Optimizer.state_dict`` , recommend to use ``.pdopt`` .\\n        For specific examples, please refer to API code examples.\\n\\n    Args:\\n        obj(Object) : The object to be saved.\\n        path(str|BytesIO) : The path/buffer of the object to be saved.\\n          If saved in the current directory, the input path string will be used as the file name.\\n        protocol(int, optional): The protocol version of pickle module must be greater than 1 and less than 5.\\n                                 Default: 4\\n        **configs(dict, optional): optional keyword arguments. The following options are currently supported:\\n          (1)use_binary_format(bool):\\n            To be used in paddle.save. When the saved object is static graph variable, you can specify ``use_binary_for_var``.\\n            If True, save the file in the c++ binary format when saving a single static graph variable; otherwise, save it in pickle format.\\n            Default: False\\n          (2)gather_to(int|list|tuple|None):\\n            To specify which global rank to save in.Defalut is None.\\n            None value means distributed saving with no gathering to a single card.\\n          (3)state_type(str):\\n            Value can be \\'params\\' or \\'opt\\', specifying to save parametres or optimizer state.\\n          (4)max_grouped_size(str|int):\\n            To limit the max size(how many bits) a object group to be transfered a time.\\n            If str, the format must be as num+\\'G/M/K\\', for example, 3G, 2K, 10M, etc. Default is 3G.\\n    Returns:\\n        None\\n    Examples:\\n        import paddle\\n        paddle.distributed.init_process_group(backend=\\'nccl\\')\\n        paddle.distributed.fleet.init(is_collective=True)\\n\\n        model = build_model()\\n        optimizer = build_optimizer(model)\\n\\n        dist_optimizer = paddle.distributed_optimizer(optimizer)\\n        dist_model = paddle.distributed_optimizer(model)\\n\\n        # gather params to rank 0 and then save\\n        paddle.incubate.distributed.utils.io.save(model.state_dict(), path=\"path/to/save.pdparams\", gather_to=[0], state_type=\"params\")\\n\\n        # save whoe params on all ranks\\n        paddle.incubate.distributed.utils.io.save(model.state_dict(), path=\"path/to/save.pdparams\", gather_to=[0,1], state_type=\"params\")\\n\\n        # save optimizer state dict on rank 0\\n        paddle.incubate.distributed.utils.io.save(optimizer.state_dict(), path=\"path/to/save.pdopt\", gather=0, state_type=\"opt\")\\n\\n    '\n    gather_to = configs.get('gather_to', None)\n    if dist.get_world_size() == 1 or gather_to is None:\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    state_type = configs.get('state_type', None)\n    assert isinstance(state_type, str), \"must pass an arg state_type='params' or state_type='opt' to specify whether to save model state_dict or optimizer state_dict\"\n    assert state_type in ['params', 'opt'], \"must pass an arg state_type='params' or state_type='opt'\"\n    if re.search(f'{state_type}$', path) is None:\n        logger.warning(f'You are saving {state_type}, while the path({path} does not end with {state_type})')\n    hcg = fleet.get_hybrid_communicate_group()\n    assert hcg.get_model_parallel_world_size() == 1 and hcg.get_pipe_parallel_world_size() == 1, f'Only DP and Sharding is supported now. However, current MP={hcg.get_model_parallel_world_size()} , PP={hcg.get_pipe_parallel_world_size()}'\n    sharding_group = hcg.get_sharding_parallel_group()\n    dp_group = hcg.get_data_parallel_group()\n    if state_type == 'params':\n        if dp_group.nranks > 1:\n            assert _same_keys(state_dict, dp_group), 'only sharding stage 1/2 and DP are supported now'\n        if sharding_group.nranks > 1:\n            assert _same_keys(state_dict, sharding_group), 'only sharding stage 1/2 and DP are supported now'\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    if sharding_group.nranks == 1:\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    if _same_keys(state_dict, sharding_group):\n        return paddle.save(state_dict, path, **configs)\n    assert isinstance(gather_to, (list, tuple, int))\n    if isinstance(gather_to, int):\n        gather_to = [gather_to]\n    max_size = configs.get('max_grouped_size', '3G')\n    try:\n        logger.info('state_dict_keys:' + str(state_dict.keys()))\n        gathered_state_dict = _gather_state_dict(state_dict, gather_to, sharding_group, max_size=max_size)\n        logger.info('gathered_state_dict_keys:' + str(state_dict.keys()))\n        if dist.get_rank() in gather_to:\n            configs = _remove_not_supported_conf(configs)\n            paddle.save(gathered_state_dict, path, **configs)\n    except:\n        raise RuntimeError(f'Saving failed. Follwing are some suggestions:\\n    1) pass the param max_grouped_size to turn the grouped size smaller (current value of max_grouped_size is {max_size})\\n    2) if sharding stage is 1, use paddle.save rather than paddle.distributed.save\\n    3) Concat the developers\\n')",
            "@dygraph_only\ndef save(state_dict, path, **configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Save a state dict to the specified path in both distributed and single-card environment.\\n\\n    Note:\\n        Now supports saving ``state_dict`` of Layer/Optimizer, Tensor and nested structure containing Tensor, Program.\\n\\n    Note:\\n        Different from ``paddle.jit.save``, since the save result of ``paddle.save`` is a single file,\\n        there is no need to distinguish multiple saved files by adding a suffix. The argument ``path``\\n        of ``paddle.save`` will be directly used as the saved file name instead of a prefix.\\n        In order to unify the saved file name format, we recommend using the paddle standard suffix:\\n        1. for ``Layer.state_dict`` , recommend to use ``.pdparams`` ;\\n        2. for ``Optimizer.state_dict`` , recommend to use ``.pdopt`` .\\n        For specific examples, please refer to API code examples.\\n\\n    Args:\\n        obj(Object) : The object to be saved.\\n        path(str|BytesIO) : The path/buffer of the object to be saved.\\n          If saved in the current directory, the input path string will be used as the file name.\\n        protocol(int, optional): The protocol version of pickle module must be greater than 1 and less than 5.\\n                                 Default: 4\\n        **configs(dict, optional): optional keyword arguments. The following options are currently supported:\\n          (1)use_binary_format(bool):\\n            To be used in paddle.save. When the saved object is static graph variable, you can specify ``use_binary_for_var``.\\n            If True, save the file in the c++ binary format when saving a single static graph variable; otherwise, save it in pickle format.\\n            Default: False\\n          (2)gather_to(int|list|tuple|None):\\n            To specify which global rank to save in.Defalut is None.\\n            None value means distributed saving with no gathering to a single card.\\n          (3)state_type(str):\\n            Value can be \\'params\\' or \\'opt\\', specifying to save parametres or optimizer state.\\n          (4)max_grouped_size(str|int):\\n            To limit the max size(how many bits) a object group to be transfered a time.\\n            If str, the format must be as num+\\'G/M/K\\', for example, 3G, 2K, 10M, etc. Default is 3G.\\n    Returns:\\n        None\\n    Examples:\\n        import paddle\\n        paddle.distributed.init_process_group(backend=\\'nccl\\')\\n        paddle.distributed.fleet.init(is_collective=True)\\n\\n        model = build_model()\\n        optimizer = build_optimizer(model)\\n\\n        dist_optimizer = paddle.distributed_optimizer(optimizer)\\n        dist_model = paddle.distributed_optimizer(model)\\n\\n        # gather params to rank 0 and then save\\n        paddle.incubate.distributed.utils.io.save(model.state_dict(), path=\"path/to/save.pdparams\", gather_to=[0], state_type=\"params\")\\n\\n        # save whoe params on all ranks\\n        paddle.incubate.distributed.utils.io.save(model.state_dict(), path=\"path/to/save.pdparams\", gather_to=[0,1], state_type=\"params\")\\n\\n        # save optimizer state dict on rank 0\\n        paddle.incubate.distributed.utils.io.save(optimizer.state_dict(), path=\"path/to/save.pdopt\", gather=0, state_type=\"opt\")\\n\\n    '\n    gather_to = configs.get('gather_to', None)\n    if dist.get_world_size() == 1 or gather_to is None:\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    state_type = configs.get('state_type', None)\n    assert isinstance(state_type, str), \"must pass an arg state_type='params' or state_type='opt' to specify whether to save model state_dict or optimizer state_dict\"\n    assert state_type in ['params', 'opt'], \"must pass an arg state_type='params' or state_type='opt'\"\n    if re.search(f'{state_type}$', path) is None:\n        logger.warning(f'You are saving {state_type}, while the path({path} does not end with {state_type})')\n    hcg = fleet.get_hybrid_communicate_group()\n    assert hcg.get_model_parallel_world_size() == 1 and hcg.get_pipe_parallel_world_size() == 1, f'Only DP and Sharding is supported now. However, current MP={hcg.get_model_parallel_world_size()} , PP={hcg.get_pipe_parallel_world_size()}'\n    sharding_group = hcg.get_sharding_parallel_group()\n    dp_group = hcg.get_data_parallel_group()\n    if state_type == 'params':\n        if dp_group.nranks > 1:\n            assert _same_keys(state_dict, dp_group), 'only sharding stage 1/2 and DP are supported now'\n        if sharding_group.nranks > 1:\n            assert _same_keys(state_dict, sharding_group), 'only sharding stage 1/2 and DP are supported now'\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    if sharding_group.nranks == 1:\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    if _same_keys(state_dict, sharding_group):\n        return paddle.save(state_dict, path, **configs)\n    assert isinstance(gather_to, (list, tuple, int))\n    if isinstance(gather_to, int):\n        gather_to = [gather_to]\n    max_size = configs.get('max_grouped_size', '3G')\n    try:\n        logger.info('state_dict_keys:' + str(state_dict.keys()))\n        gathered_state_dict = _gather_state_dict(state_dict, gather_to, sharding_group, max_size=max_size)\n        logger.info('gathered_state_dict_keys:' + str(state_dict.keys()))\n        if dist.get_rank() in gather_to:\n            configs = _remove_not_supported_conf(configs)\n            paddle.save(gathered_state_dict, path, **configs)\n    except:\n        raise RuntimeError(f'Saving failed. Follwing are some suggestions:\\n    1) pass the param max_grouped_size to turn the grouped size smaller (current value of max_grouped_size is {max_size})\\n    2) if sharding stage is 1, use paddle.save rather than paddle.distributed.save\\n    3) Concat the developers\\n')",
            "@dygraph_only\ndef save(state_dict, path, **configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Save a state dict to the specified path in both distributed and single-card environment.\\n\\n    Note:\\n        Now supports saving ``state_dict`` of Layer/Optimizer, Tensor and nested structure containing Tensor, Program.\\n\\n    Note:\\n        Different from ``paddle.jit.save``, since the save result of ``paddle.save`` is a single file,\\n        there is no need to distinguish multiple saved files by adding a suffix. The argument ``path``\\n        of ``paddle.save`` will be directly used as the saved file name instead of a prefix.\\n        In order to unify the saved file name format, we recommend using the paddle standard suffix:\\n        1. for ``Layer.state_dict`` , recommend to use ``.pdparams`` ;\\n        2. for ``Optimizer.state_dict`` , recommend to use ``.pdopt`` .\\n        For specific examples, please refer to API code examples.\\n\\n    Args:\\n        obj(Object) : The object to be saved.\\n        path(str|BytesIO) : The path/buffer of the object to be saved.\\n          If saved in the current directory, the input path string will be used as the file name.\\n        protocol(int, optional): The protocol version of pickle module must be greater than 1 and less than 5.\\n                                 Default: 4\\n        **configs(dict, optional): optional keyword arguments. The following options are currently supported:\\n          (1)use_binary_format(bool):\\n            To be used in paddle.save. When the saved object is static graph variable, you can specify ``use_binary_for_var``.\\n            If True, save the file in the c++ binary format when saving a single static graph variable; otherwise, save it in pickle format.\\n            Default: False\\n          (2)gather_to(int|list|tuple|None):\\n            To specify which global rank to save in.Defalut is None.\\n            None value means distributed saving with no gathering to a single card.\\n          (3)state_type(str):\\n            Value can be \\'params\\' or \\'opt\\', specifying to save parametres or optimizer state.\\n          (4)max_grouped_size(str|int):\\n            To limit the max size(how many bits) a object group to be transfered a time.\\n            If str, the format must be as num+\\'G/M/K\\', for example, 3G, 2K, 10M, etc. Default is 3G.\\n    Returns:\\n        None\\n    Examples:\\n        import paddle\\n        paddle.distributed.init_process_group(backend=\\'nccl\\')\\n        paddle.distributed.fleet.init(is_collective=True)\\n\\n        model = build_model()\\n        optimizer = build_optimizer(model)\\n\\n        dist_optimizer = paddle.distributed_optimizer(optimizer)\\n        dist_model = paddle.distributed_optimizer(model)\\n\\n        # gather params to rank 0 and then save\\n        paddle.incubate.distributed.utils.io.save(model.state_dict(), path=\"path/to/save.pdparams\", gather_to=[0], state_type=\"params\")\\n\\n        # save whoe params on all ranks\\n        paddle.incubate.distributed.utils.io.save(model.state_dict(), path=\"path/to/save.pdparams\", gather_to=[0,1], state_type=\"params\")\\n\\n        # save optimizer state dict on rank 0\\n        paddle.incubate.distributed.utils.io.save(optimizer.state_dict(), path=\"path/to/save.pdopt\", gather=0, state_type=\"opt\")\\n\\n    '\n    gather_to = configs.get('gather_to', None)\n    if dist.get_world_size() == 1 or gather_to is None:\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    state_type = configs.get('state_type', None)\n    assert isinstance(state_type, str), \"must pass an arg state_type='params' or state_type='opt' to specify whether to save model state_dict or optimizer state_dict\"\n    assert state_type in ['params', 'opt'], \"must pass an arg state_type='params' or state_type='opt'\"\n    if re.search(f'{state_type}$', path) is None:\n        logger.warning(f'You are saving {state_type}, while the path({path} does not end with {state_type})')\n    hcg = fleet.get_hybrid_communicate_group()\n    assert hcg.get_model_parallel_world_size() == 1 and hcg.get_pipe_parallel_world_size() == 1, f'Only DP and Sharding is supported now. However, current MP={hcg.get_model_parallel_world_size()} , PP={hcg.get_pipe_parallel_world_size()}'\n    sharding_group = hcg.get_sharding_parallel_group()\n    dp_group = hcg.get_data_parallel_group()\n    if state_type == 'params':\n        if dp_group.nranks > 1:\n            assert _same_keys(state_dict, dp_group), 'only sharding stage 1/2 and DP are supported now'\n        if sharding_group.nranks > 1:\n            assert _same_keys(state_dict, sharding_group), 'only sharding stage 1/2 and DP are supported now'\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    if sharding_group.nranks == 1:\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    if _same_keys(state_dict, sharding_group):\n        return paddle.save(state_dict, path, **configs)\n    assert isinstance(gather_to, (list, tuple, int))\n    if isinstance(gather_to, int):\n        gather_to = [gather_to]\n    max_size = configs.get('max_grouped_size', '3G')\n    try:\n        logger.info('state_dict_keys:' + str(state_dict.keys()))\n        gathered_state_dict = _gather_state_dict(state_dict, gather_to, sharding_group, max_size=max_size)\n        logger.info('gathered_state_dict_keys:' + str(state_dict.keys()))\n        if dist.get_rank() in gather_to:\n            configs = _remove_not_supported_conf(configs)\n            paddle.save(gathered_state_dict, path, **configs)\n    except:\n        raise RuntimeError(f'Saving failed. Follwing are some suggestions:\\n    1) pass the param max_grouped_size to turn the grouped size smaller (current value of max_grouped_size is {max_size})\\n    2) if sharding stage is 1, use paddle.save rather than paddle.distributed.save\\n    3) Concat the developers\\n')",
            "@dygraph_only\ndef save(state_dict, path, **configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Save a state dict to the specified path in both distributed and single-card environment.\\n\\n    Note:\\n        Now supports saving ``state_dict`` of Layer/Optimizer, Tensor and nested structure containing Tensor, Program.\\n\\n    Note:\\n        Different from ``paddle.jit.save``, since the save result of ``paddle.save`` is a single file,\\n        there is no need to distinguish multiple saved files by adding a suffix. The argument ``path``\\n        of ``paddle.save`` will be directly used as the saved file name instead of a prefix.\\n        In order to unify the saved file name format, we recommend using the paddle standard suffix:\\n        1. for ``Layer.state_dict`` , recommend to use ``.pdparams`` ;\\n        2. for ``Optimizer.state_dict`` , recommend to use ``.pdopt`` .\\n        For specific examples, please refer to API code examples.\\n\\n    Args:\\n        obj(Object) : The object to be saved.\\n        path(str|BytesIO) : The path/buffer of the object to be saved.\\n          If saved in the current directory, the input path string will be used as the file name.\\n        protocol(int, optional): The protocol version of pickle module must be greater than 1 and less than 5.\\n                                 Default: 4\\n        **configs(dict, optional): optional keyword arguments. The following options are currently supported:\\n          (1)use_binary_format(bool):\\n            To be used in paddle.save. When the saved object is static graph variable, you can specify ``use_binary_for_var``.\\n            If True, save the file in the c++ binary format when saving a single static graph variable; otherwise, save it in pickle format.\\n            Default: False\\n          (2)gather_to(int|list|tuple|None):\\n            To specify which global rank to save in.Defalut is None.\\n            None value means distributed saving with no gathering to a single card.\\n          (3)state_type(str):\\n            Value can be \\'params\\' or \\'opt\\', specifying to save parametres or optimizer state.\\n          (4)max_grouped_size(str|int):\\n            To limit the max size(how many bits) a object group to be transfered a time.\\n            If str, the format must be as num+\\'G/M/K\\', for example, 3G, 2K, 10M, etc. Default is 3G.\\n    Returns:\\n        None\\n    Examples:\\n        import paddle\\n        paddle.distributed.init_process_group(backend=\\'nccl\\')\\n        paddle.distributed.fleet.init(is_collective=True)\\n\\n        model = build_model()\\n        optimizer = build_optimizer(model)\\n\\n        dist_optimizer = paddle.distributed_optimizer(optimizer)\\n        dist_model = paddle.distributed_optimizer(model)\\n\\n        # gather params to rank 0 and then save\\n        paddle.incubate.distributed.utils.io.save(model.state_dict(), path=\"path/to/save.pdparams\", gather_to=[0], state_type=\"params\")\\n\\n        # save whoe params on all ranks\\n        paddle.incubate.distributed.utils.io.save(model.state_dict(), path=\"path/to/save.pdparams\", gather_to=[0,1], state_type=\"params\")\\n\\n        # save optimizer state dict on rank 0\\n        paddle.incubate.distributed.utils.io.save(optimizer.state_dict(), path=\"path/to/save.pdopt\", gather=0, state_type=\"opt\")\\n\\n    '\n    gather_to = configs.get('gather_to', None)\n    if dist.get_world_size() == 1 or gather_to is None:\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    state_type = configs.get('state_type', None)\n    assert isinstance(state_type, str), \"must pass an arg state_type='params' or state_type='opt' to specify whether to save model state_dict or optimizer state_dict\"\n    assert state_type in ['params', 'opt'], \"must pass an arg state_type='params' or state_type='opt'\"\n    if re.search(f'{state_type}$', path) is None:\n        logger.warning(f'You are saving {state_type}, while the path({path} does not end with {state_type})')\n    hcg = fleet.get_hybrid_communicate_group()\n    assert hcg.get_model_parallel_world_size() == 1 and hcg.get_pipe_parallel_world_size() == 1, f'Only DP and Sharding is supported now. However, current MP={hcg.get_model_parallel_world_size()} , PP={hcg.get_pipe_parallel_world_size()}'\n    sharding_group = hcg.get_sharding_parallel_group()\n    dp_group = hcg.get_data_parallel_group()\n    if state_type == 'params':\n        if dp_group.nranks > 1:\n            assert _same_keys(state_dict, dp_group), 'only sharding stage 1/2 and DP are supported now'\n        if sharding_group.nranks > 1:\n            assert _same_keys(state_dict, sharding_group), 'only sharding stage 1/2 and DP are supported now'\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    if sharding_group.nranks == 1:\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    if _same_keys(state_dict, sharding_group):\n        return paddle.save(state_dict, path, **configs)\n    assert isinstance(gather_to, (list, tuple, int))\n    if isinstance(gather_to, int):\n        gather_to = [gather_to]\n    max_size = configs.get('max_grouped_size', '3G')\n    try:\n        logger.info('state_dict_keys:' + str(state_dict.keys()))\n        gathered_state_dict = _gather_state_dict(state_dict, gather_to, sharding_group, max_size=max_size)\n        logger.info('gathered_state_dict_keys:' + str(state_dict.keys()))\n        if dist.get_rank() in gather_to:\n            configs = _remove_not_supported_conf(configs)\n            paddle.save(gathered_state_dict, path, **configs)\n    except:\n        raise RuntimeError(f'Saving failed. Follwing are some suggestions:\\n    1) pass the param max_grouped_size to turn the grouped size smaller (current value of max_grouped_size is {max_size})\\n    2) if sharding stage is 1, use paddle.save rather than paddle.distributed.save\\n    3) Concat the developers\\n')",
            "@dygraph_only\ndef save(state_dict, path, **configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Save a state dict to the specified path in both distributed and single-card environment.\\n\\n    Note:\\n        Now supports saving ``state_dict`` of Layer/Optimizer, Tensor and nested structure containing Tensor, Program.\\n\\n    Note:\\n        Different from ``paddle.jit.save``, since the save result of ``paddle.save`` is a single file,\\n        there is no need to distinguish multiple saved files by adding a suffix. The argument ``path``\\n        of ``paddle.save`` will be directly used as the saved file name instead of a prefix.\\n        In order to unify the saved file name format, we recommend using the paddle standard suffix:\\n        1. for ``Layer.state_dict`` , recommend to use ``.pdparams`` ;\\n        2. for ``Optimizer.state_dict`` , recommend to use ``.pdopt`` .\\n        For specific examples, please refer to API code examples.\\n\\n    Args:\\n        obj(Object) : The object to be saved.\\n        path(str|BytesIO) : The path/buffer of the object to be saved.\\n          If saved in the current directory, the input path string will be used as the file name.\\n        protocol(int, optional): The protocol version of pickle module must be greater than 1 and less than 5.\\n                                 Default: 4\\n        **configs(dict, optional): optional keyword arguments. The following options are currently supported:\\n          (1)use_binary_format(bool):\\n            To be used in paddle.save. When the saved object is static graph variable, you can specify ``use_binary_for_var``.\\n            If True, save the file in the c++ binary format when saving a single static graph variable; otherwise, save it in pickle format.\\n            Default: False\\n          (2)gather_to(int|list|tuple|None):\\n            To specify which global rank to save in.Defalut is None.\\n            None value means distributed saving with no gathering to a single card.\\n          (3)state_type(str):\\n            Value can be \\'params\\' or \\'opt\\', specifying to save parametres or optimizer state.\\n          (4)max_grouped_size(str|int):\\n            To limit the max size(how many bits) a object group to be transfered a time.\\n            If str, the format must be as num+\\'G/M/K\\', for example, 3G, 2K, 10M, etc. Default is 3G.\\n    Returns:\\n        None\\n    Examples:\\n        import paddle\\n        paddle.distributed.init_process_group(backend=\\'nccl\\')\\n        paddle.distributed.fleet.init(is_collective=True)\\n\\n        model = build_model()\\n        optimizer = build_optimizer(model)\\n\\n        dist_optimizer = paddle.distributed_optimizer(optimizer)\\n        dist_model = paddle.distributed_optimizer(model)\\n\\n        # gather params to rank 0 and then save\\n        paddle.incubate.distributed.utils.io.save(model.state_dict(), path=\"path/to/save.pdparams\", gather_to=[0], state_type=\"params\")\\n\\n        # save whoe params on all ranks\\n        paddle.incubate.distributed.utils.io.save(model.state_dict(), path=\"path/to/save.pdparams\", gather_to=[0,1], state_type=\"params\")\\n\\n        # save optimizer state dict on rank 0\\n        paddle.incubate.distributed.utils.io.save(optimizer.state_dict(), path=\"path/to/save.pdopt\", gather=0, state_type=\"opt\")\\n\\n    '\n    gather_to = configs.get('gather_to', None)\n    if dist.get_world_size() == 1 or gather_to is None:\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    state_type = configs.get('state_type', None)\n    assert isinstance(state_type, str), \"must pass an arg state_type='params' or state_type='opt' to specify whether to save model state_dict or optimizer state_dict\"\n    assert state_type in ['params', 'opt'], \"must pass an arg state_type='params' or state_type='opt'\"\n    if re.search(f'{state_type}$', path) is None:\n        logger.warning(f'You are saving {state_type}, while the path({path} does not end with {state_type})')\n    hcg = fleet.get_hybrid_communicate_group()\n    assert hcg.get_model_parallel_world_size() == 1 and hcg.get_pipe_parallel_world_size() == 1, f'Only DP and Sharding is supported now. However, current MP={hcg.get_model_parallel_world_size()} , PP={hcg.get_pipe_parallel_world_size()}'\n    sharding_group = hcg.get_sharding_parallel_group()\n    dp_group = hcg.get_data_parallel_group()\n    if state_type == 'params':\n        if dp_group.nranks > 1:\n            assert _same_keys(state_dict, dp_group), 'only sharding stage 1/2 and DP are supported now'\n        if sharding_group.nranks > 1:\n            assert _same_keys(state_dict, sharding_group), 'only sharding stage 1/2 and DP are supported now'\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    if sharding_group.nranks == 1:\n        configs = _remove_not_supported_conf(configs)\n        return paddle.save(state_dict, path, **configs)\n    if _same_keys(state_dict, sharding_group):\n        return paddle.save(state_dict, path, **configs)\n    assert isinstance(gather_to, (list, tuple, int))\n    if isinstance(gather_to, int):\n        gather_to = [gather_to]\n    max_size = configs.get('max_grouped_size', '3G')\n    try:\n        logger.info('state_dict_keys:' + str(state_dict.keys()))\n        gathered_state_dict = _gather_state_dict(state_dict, gather_to, sharding_group, max_size=max_size)\n        logger.info('gathered_state_dict_keys:' + str(state_dict.keys()))\n        if dist.get_rank() in gather_to:\n            configs = _remove_not_supported_conf(configs)\n            paddle.save(gathered_state_dict, path, **configs)\n    except:\n        raise RuntimeError(f'Saving failed. Follwing are some suggestions:\\n    1) pass the param max_grouped_size to turn the grouped size smaller (current value of max_grouped_size is {max_size})\\n    2) if sharding stage is 1, use paddle.save rather than paddle.distributed.save\\n    3) Concat the developers\\n')"
        ]
    },
    {
        "func_name": "_state_dict_groups",
        "original": "def _state_dict_groups(state_dict, max_size):\n    \"\"\"\n    Description:\n        Generator of state dict groups to transfer.the size of each group is less than max_size.\n    \"\"\"\n    max_tensor_size = 0\n    for (k, v) in state_dict.items():\n        if max_tensor_size < sys.getsizeof(v) + sys.getsizeof(k):\n            max_tensor_size = sys.getsizeof(v) + sys.getsizeof(k)\n    max_size = max(max_size, max_tensor_size)\n    logger.debug(f'max tensor size: {max_size}')\n    state_group = {}\n    k_list = list(state_dict.keys())\n    index = 0\n    bits = 0\n    while index < len(k_list):\n        bsize = sys.getsizeof(state_dict[k_list[index]]) + sys.getsizeof(k_list[index])\n        if bits + bsize >= max_size:\n            yield state_group\n            state_group = {}\n            bits = 0\n        state_group[k_list[index]] = state_dict[k_list[index]]\n        index += 1\n        bits += bsize\n        if index == len(k_list) and bits > 0:\n            yield state_group",
        "mutated": [
            "def _state_dict_groups(state_dict, max_size):\n    if False:\n        i = 10\n    '\\n    Description:\\n        Generator of state dict groups to transfer.the size of each group is less than max_size.\\n    '\n    max_tensor_size = 0\n    for (k, v) in state_dict.items():\n        if max_tensor_size < sys.getsizeof(v) + sys.getsizeof(k):\n            max_tensor_size = sys.getsizeof(v) + sys.getsizeof(k)\n    max_size = max(max_size, max_tensor_size)\n    logger.debug(f'max tensor size: {max_size}')\n    state_group = {}\n    k_list = list(state_dict.keys())\n    index = 0\n    bits = 0\n    while index < len(k_list):\n        bsize = sys.getsizeof(state_dict[k_list[index]]) + sys.getsizeof(k_list[index])\n        if bits + bsize >= max_size:\n            yield state_group\n            state_group = {}\n            bits = 0\n        state_group[k_list[index]] = state_dict[k_list[index]]\n        index += 1\n        bits += bsize\n        if index == len(k_list) and bits > 0:\n            yield state_group",
            "def _state_dict_groups(state_dict, max_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Description:\\n        Generator of state dict groups to transfer.the size of each group is less than max_size.\\n    '\n    max_tensor_size = 0\n    for (k, v) in state_dict.items():\n        if max_tensor_size < sys.getsizeof(v) + sys.getsizeof(k):\n            max_tensor_size = sys.getsizeof(v) + sys.getsizeof(k)\n    max_size = max(max_size, max_tensor_size)\n    logger.debug(f'max tensor size: {max_size}')\n    state_group = {}\n    k_list = list(state_dict.keys())\n    index = 0\n    bits = 0\n    while index < len(k_list):\n        bsize = sys.getsizeof(state_dict[k_list[index]]) + sys.getsizeof(k_list[index])\n        if bits + bsize >= max_size:\n            yield state_group\n            state_group = {}\n            bits = 0\n        state_group[k_list[index]] = state_dict[k_list[index]]\n        index += 1\n        bits += bsize\n        if index == len(k_list) and bits > 0:\n            yield state_group",
            "def _state_dict_groups(state_dict, max_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Description:\\n        Generator of state dict groups to transfer.the size of each group is less than max_size.\\n    '\n    max_tensor_size = 0\n    for (k, v) in state_dict.items():\n        if max_tensor_size < sys.getsizeof(v) + sys.getsizeof(k):\n            max_tensor_size = sys.getsizeof(v) + sys.getsizeof(k)\n    max_size = max(max_size, max_tensor_size)\n    logger.debug(f'max tensor size: {max_size}')\n    state_group = {}\n    k_list = list(state_dict.keys())\n    index = 0\n    bits = 0\n    while index < len(k_list):\n        bsize = sys.getsizeof(state_dict[k_list[index]]) + sys.getsizeof(k_list[index])\n        if bits + bsize >= max_size:\n            yield state_group\n            state_group = {}\n            bits = 0\n        state_group[k_list[index]] = state_dict[k_list[index]]\n        index += 1\n        bits += bsize\n        if index == len(k_list) and bits > 0:\n            yield state_group",
            "def _state_dict_groups(state_dict, max_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Description:\\n        Generator of state dict groups to transfer.the size of each group is less than max_size.\\n    '\n    max_tensor_size = 0\n    for (k, v) in state_dict.items():\n        if max_tensor_size < sys.getsizeof(v) + sys.getsizeof(k):\n            max_tensor_size = sys.getsizeof(v) + sys.getsizeof(k)\n    max_size = max(max_size, max_tensor_size)\n    logger.debug(f'max tensor size: {max_size}')\n    state_group = {}\n    k_list = list(state_dict.keys())\n    index = 0\n    bits = 0\n    while index < len(k_list):\n        bsize = sys.getsizeof(state_dict[k_list[index]]) + sys.getsizeof(k_list[index])\n        if bits + bsize >= max_size:\n            yield state_group\n            state_group = {}\n            bits = 0\n        state_group[k_list[index]] = state_dict[k_list[index]]\n        index += 1\n        bits += bsize\n        if index == len(k_list) and bits > 0:\n            yield state_group",
            "def _state_dict_groups(state_dict, max_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Description:\\n        Generator of state dict groups to transfer.the size of each group is less than max_size.\\n    '\n    max_tensor_size = 0\n    for (k, v) in state_dict.items():\n        if max_tensor_size < sys.getsizeof(v) + sys.getsizeof(k):\n            max_tensor_size = sys.getsizeof(v) + sys.getsizeof(k)\n    max_size = max(max_size, max_tensor_size)\n    logger.debug(f'max tensor size: {max_size}')\n    state_group = {}\n    k_list = list(state_dict.keys())\n    index = 0\n    bits = 0\n    while index < len(k_list):\n        bsize = sys.getsizeof(state_dict[k_list[index]]) + sys.getsizeof(k_list[index])\n        if bits + bsize >= max_size:\n            yield state_group\n            state_group = {}\n            bits = 0\n        state_group[k_list[index]] = state_dict[k_list[index]]\n        index += 1\n        bits += bsize\n        if index == len(k_list) and bits > 0:\n            yield state_group"
        ]
    },
    {
        "func_name": "all_empty",
        "original": "def all_empty(dict_list):\n    \"\"\"\n    Check if all items are empty\n    \"\"\"\n    for v in dict_list:\n        if len(v) > 0:\n            return False\n    return True",
        "mutated": [
            "def all_empty(dict_list):\n    if False:\n        i = 10\n    '\\n    Check if all items are empty\\n    '\n    for v in dict_list:\n        if len(v) > 0:\n            return False\n    return True",
            "def all_empty(dict_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Check if all items are empty\\n    '\n    for v in dict_list:\n        if len(v) > 0:\n            return False\n    return True",
            "def all_empty(dict_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Check if all items are empty\\n    '\n    for v in dict_list:\n        if len(v) > 0:\n            return False\n    return True",
            "def all_empty(dict_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Check if all items are empty\\n    '\n    for v in dict_list:\n        if len(v) > 0:\n            return False\n    return True",
            "def all_empty(dict_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Check if all items are empty\\n    '\n    for v in dict_list:\n        if len(v) > 0:\n            return False\n    return True"
        ]
    },
    {
        "func_name": "_parse_mem_size_to_bits",
        "original": "def _parse_mem_size_to_bits(max_size):\n    \"\"\"\n    Parse an integer or a mem size str to an integer\n    convert xxxG to xxx * 1024^3\n    convert xxxM to xxx * 1024^2\n    convert xxxK to xxx * 1024^1\n    \"\"\"\n    assert isinstance(max_size, (int, str))\n    if isinstance(max_size, str):\n        assert re.search('^[0-9]*[GMK]$', max_size), f\"Wrong max_size 's format, the format ust be like 10K, 9M, 200G , etc, or an integer. However this is {max_size}\"\n        num = int(max_size[:-1])\n        if max_size[-1] == 'G':\n            max_size = num * 1024 ** 3\n        elif max_size[-1] == 'M':\n            max_size = num * 1024 ** 2\n        else:\n            max_size = num * 1024\n    return max_size",
        "mutated": [
            "def _parse_mem_size_to_bits(max_size):\n    if False:\n        i = 10\n    '\\n    Parse an integer or a mem size str to an integer\\n    convert xxxG to xxx * 1024^3\\n    convert xxxM to xxx * 1024^2\\n    convert xxxK to xxx * 1024^1\\n    '\n    assert isinstance(max_size, (int, str))\n    if isinstance(max_size, str):\n        assert re.search('^[0-9]*[GMK]$', max_size), f\"Wrong max_size 's format, the format ust be like 10K, 9M, 200G , etc, or an integer. However this is {max_size}\"\n        num = int(max_size[:-1])\n        if max_size[-1] == 'G':\n            max_size = num * 1024 ** 3\n        elif max_size[-1] == 'M':\n            max_size = num * 1024 ** 2\n        else:\n            max_size = num * 1024\n    return max_size",
            "def _parse_mem_size_to_bits(max_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Parse an integer or a mem size str to an integer\\n    convert xxxG to xxx * 1024^3\\n    convert xxxM to xxx * 1024^2\\n    convert xxxK to xxx * 1024^1\\n    '\n    assert isinstance(max_size, (int, str))\n    if isinstance(max_size, str):\n        assert re.search('^[0-9]*[GMK]$', max_size), f\"Wrong max_size 's format, the format ust be like 10K, 9M, 200G , etc, or an integer. However this is {max_size}\"\n        num = int(max_size[:-1])\n        if max_size[-1] == 'G':\n            max_size = num * 1024 ** 3\n        elif max_size[-1] == 'M':\n            max_size = num * 1024 ** 2\n        else:\n            max_size = num * 1024\n    return max_size",
            "def _parse_mem_size_to_bits(max_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Parse an integer or a mem size str to an integer\\n    convert xxxG to xxx * 1024^3\\n    convert xxxM to xxx * 1024^2\\n    convert xxxK to xxx * 1024^1\\n    '\n    assert isinstance(max_size, (int, str))\n    if isinstance(max_size, str):\n        assert re.search('^[0-9]*[GMK]$', max_size), f\"Wrong max_size 's format, the format ust be like 10K, 9M, 200G , etc, or an integer. However this is {max_size}\"\n        num = int(max_size[:-1])\n        if max_size[-1] == 'G':\n            max_size = num * 1024 ** 3\n        elif max_size[-1] == 'M':\n            max_size = num * 1024 ** 2\n        else:\n            max_size = num * 1024\n    return max_size",
            "def _parse_mem_size_to_bits(max_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Parse an integer or a mem size str to an integer\\n    convert xxxG to xxx * 1024^3\\n    convert xxxM to xxx * 1024^2\\n    convert xxxK to xxx * 1024^1\\n    '\n    assert isinstance(max_size, (int, str))\n    if isinstance(max_size, str):\n        assert re.search('^[0-9]*[GMK]$', max_size), f\"Wrong max_size 's format, the format ust be like 10K, 9M, 200G , etc, or an integer. However this is {max_size}\"\n        num = int(max_size[:-1])\n        if max_size[-1] == 'G':\n            max_size = num * 1024 ** 3\n        elif max_size[-1] == 'M':\n            max_size = num * 1024 ** 2\n        else:\n            max_size = num * 1024\n    return max_size",
            "def _parse_mem_size_to_bits(max_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Parse an integer or a mem size str to an integer\\n    convert xxxG to xxx * 1024^3\\n    convert xxxM to xxx * 1024^2\\n    convert xxxK to xxx * 1024^1\\n    '\n    assert isinstance(max_size, (int, str))\n    if isinstance(max_size, str):\n        assert re.search('^[0-9]*[GMK]$', max_size), f\"Wrong max_size 's format, the format ust be like 10K, 9M, 200G , etc, or an integer. However this is {max_size}\"\n        num = int(max_size[:-1])\n        if max_size[-1] == 'G':\n            max_size = num * 1024 ** 3\n        elif max_size[-1] == 'M':\n            max_size = num * 1024 ** 2\n        else:\n            max_size = num * 1024\n    return max_size"
        ]
    },
    {
        "func_name": "_gather_state_dict",
        "original": "def _gather_state_dict(state_dict, dst, group, max_size='3G'):\n    \"\"\"\n    Description:\n        Gather state dicts across all group ranks to dst, Depiring the same elements. including LR_Scheduler.\n    Args:\n        state_dict(dict):\n            local state dict\n        dst(int|list|tuple):\n            ranks the state dicts are gathered to\n        group(ProcessGroup):\n            group across which the state dicts are gathered\n        max_size(int|str):\n            The max limitation of the gathered tensor group size transformered a time. Default is 3G bits.\n            Each rank 's max tensor group before gathering is max_size // group.size\n    Returns:\n        Gathered state dict\n    \"\"\"\n    assert isinstance(dst, (list, tuple, int)), \"dst' type must be one of int, list and tuple\"\n    if isinstance(dst, int):\n        dst = [dst]\n    max_size = _parse_mem_size_to_bits(max_size)\n    max_size //= dist.get_world_size(group)\n    logger.debug('len state_dict: len(state_dict)')\n    state_dict_ = copy.copy(state_dict)\n    mw = None\n    has_mw = False\n    has_lr = False\n    if 'master_weights' in state_dict_:\n        mw = state_dict_.pop('master_weights', None)\n        has_mw = True\n    if 'LR_Scheduler' in state_dict_:\n        lr = state_dict_.pop('LR_Scheduler', None)\n        has_lr = True\n    output = _grouped_gather_data_dict(state_dict_, dst, group, max_size)\n    if isinstance(mw, dict):\n        masters = _grouped_gather_data_dict(mw, dst, group, max_size)\n    else:\n        assert mw is None, f'Wrong type of master weights . type: {type(mw)}'\n    if has_mw:\n        output['master_weights'] = masters\n    if has_lr:\n        output['LR_Scheduler'] = lr\n    return output",
        "mutated": [
            "def _gather_state_dict(state_dict, dst, group, max_size='3G'):\n    if False:\n        i = 10\n    \"\\n    Description:\\n        Gather state dicts across all group ranks to dst, Depiring the same elements. including LR_Scheduler.\\n    Args:\\n        state_dict(dict):\\n            local state dict\\n        dst(int|list|tuple):\\n            ranks the state dicts are gathered to\\n        group(ProcessGroup):\\n            group across which the state dicts are gathered\\n        max_size(int|str):\\n            The max limitation of the gathered tensor group size transformered a time. Default is 3G bits.\\n            Each rank 's max tensor group before gathering is max_size // group.size\\n    Returns:\\n        Gathered state dict\\n    \"\n    assert isinstance(dst, (list, tuple, int)), \"dst' type must be one of int, list and tuple\"\n    if isinstance(dst, int):\n        dst = [dst]\n    max_size = _parse_mem_size_to_bits(max_size)\n    max_size //= dist.get_world_size(group)\n    logger.debug('len state_dict: len(state_dict)')\n    state_dict_ = copy.copy(state_dict)\n    mw = None\n    has_mw = False\n    has_lr = False\n    if 'master_weights' in state_dict_:\n        mw = state_dict_.pop('master_weights', None)\n        has_mw = True\n    if 'LR_Scheduler' in state_dict_:\n        lr = state_dict_.pop('LR_Scheduler', None)\n        has_lr = True\n    output = _grouped_gather_data_dict(state_dict_, dst, group, max_size)\n    if isinstance(mw, dict):\n        masters = _grouped_gather_data_dict(mw, dst, group, max_size)\n    else:\n        assert mw is None, f'Wrong type of master weights . type: {type(mw)}'\n    if has_mw:\n        output['master_weights'] = masters\n    if has_lr:\n        output['LR_Scheduler'] = lr\n    return output",
            "def _gather_state_dict(state_dict, dst, group, max_size='3G'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Description:\\n        Gather state dicts across all group ranks to dst, Depiring the same elements. including LR_Scheduler.\\n    Args:\\n        state_dict(dict):\\n            local state dict\\n        dst(int|list|tuple):\\n            ranks the state dicts are gathered to\\n        group(ProcessGroup):\\n            group across which the state dicts are gathered\\n        max_size(int|str):\\n            The max limitation of the gathered tensor group size transformered a time. Default is 3G bits.\\n            Each rank 's max tensor group before gathering is max_size // group.size\\n    Returns:\\n        Gathered state dict\\n    \"\n    assert isinstance(dst, (list, tuple, int)), \"dst' type must be one of int, list and tuple\"\n    if isinstance(dst, int):\n        dst = [dst]\n    max_size = _parse_mem_size_to_bits(max_size)\n    max_size //= dist.get_world_size(group)\n    logger.debug('len state_dict: len(state_dict)')\n    state_dict_ = copy.copy(state_dict)\n    mw = None\n    has_mw = False\n    has_lr = False\n    if 'master_weights' in state_dict_:\n        mw = state_dict_.pop('master_weights', None)\n        has_mw = True\n    if 'LR_Scheduler' in state_dict_:\n        lr = state_dict_.pop('LR_Scheduler', None)\n        has_lr = True\n    output = _grouped_gather_data_dict(state_dict_, dst, group, max_size)\n    if isinstance(mw, dict):\n        masters = _grouped_gather_data_dict(mw, dst, group, max_size)\n    else:\n        assert mw is None, f'Wrong type of master weights . type: {type(mw)}'\n    if has_mw:\n        output['master_weights'] = masters\n    if has_lr:\n        output['LR_Scheduler'] = lr\n    return output",
            "def _gather_state_dict(state_dict, dst, group, max_size='3G'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Description:\\n        Gather state dicts across all group ranks to dst, Depiring the same elements. including LR_Scheduler.\\n    Args:\\n        state_dict(dict):\\n            local state dict\\n        dst(int|list|tuple):\\n            ranks the state dicts are gathered to\\n        group(ProcessGroup):\\n            group across which the state dicts are gathered\\n        max_size(int|str):\\n            The max limitation of the gathered tensor group size transformered a time. Default is 3G bits.\\n            Each rank 's max tensor group before gathering is max_size // group.size\\n    Returns:\\n        Gathered state dict\\n    \"\n    assert isinstance(dst, (list, tuple, int)), \"dst' type must be one of int, list and tuple\"\n    if isinstance(dst, int):\n        dst = [dst]\n    max_size = _parse_mem_size_to_bits(max_size)\n    max_size //= dist.get_world_size(group)\n    logger.debug('len state_dict: len(state_dict)')\n    state_dict_ = copy.copy(state_dict)\n    mw = None\n    has_mw = False\n    has_lr = False\n    if 'master_weights' in state_dict_:\n        mw = state_dict_.pop('master_weights', None)\n        has_mw = True\n    if 'LR_Scheduler' in state_dict_:\n        lr = state_dict_.pop('LR_Scheduler', None)\n        has_lr = True\n    output = _grouped_gather_data_dict(state_dict_, dst, group, max_size)\n    if isinstance(mw, dict):\n        masters = _grouped_gather_data_dict(mw, dst, group, max_size)\n    else:\n        assert mw is None, f'Wrong type of master weights . type: {type(mw)}'\n    if has_mw:\n        output['master_weights'] = masters\n    if has_lr:\n        output['LR_Scheduler'] = lr\n    return output",
            "def _gather_state_dict(state_dict, dst, group, max_size='3G'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Description:\\n        Gather state dicts across all group ranks to dst, Depiring the same elements. including LR_Scheduler.\\n    Args:\\n        state_dict(dict):\\n            local state dict\\n        dst(int|list|tuple):\\n            ranks the state dicts are gathered to\\n        group(ProcessGroup):\\n            group across which the state dicts are gathered\\n        max_size(int|str):\\n            The max limitation of the gathered tensor group size transformered a time. Default is 3G bits.\\n            Each rank 's max tensor group before gathering is max_size // group.size\\n    Returns:\\n        Gathered state dict\\n    \"\n    assert isinstance(dst, (list, tuple, int)), \"dst' type must be one of int, list and tuple\"\n    if isinstance(dst, int):\n        dst = [dst]\n    max_size = _parse_mem_size_to_bits(max_size)\n    max_size //= dist.get_world_size(group)\n    logger.debug('len state_dict: len(state_dict)')\n    state_dict_ = copy.copy(state_dict)\n    mw = None\n    has_mw = False\n    has_lr = False\n    if 'master_weights' in state_dict_:\n        mw = state_dict_.pop('master_weights', None)\n        has_mw = True\n    if 'LR_Scheduler' in state_dict_:\n        lr = state_dict_.pop('LR_Scheduler', None)\n        has_lr = True\n    output = _grouped_gather_data_dict(state_dict_, dst, group, max_size)\n    if isinstance(mw, dict):\n        masters = _grouped_gather_data_dict(mw, dst, group, max_size)\n    else:\n        assert mw is None, f'Wrong type of master weights . type: {type(mw)}'\n    if has_mw:\n        output['master_weights'] = masters\n    if has_lr:\n        output['LR_Scheduler'] = lr\n    return output",
            "def _gather_state_dict(state_dict, dst, group, max_size='3G'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Description:\\n        Gather state dicts across all group ranks to dst, Depiring the same elements. including LR_Scheduler.\\n    Args:\\n        state_dict(dict):\\n            local state dict\\n        dst(int|list|tuple):\\n            ranks the state dicts are gathered to\\n        group(ProcessGroup):\\n            group across which the state dicts are gathered\\n        max_size(int|str):\\n            The max limitation of the gathered tensor group size transformered a time. Default is 3G bits.\\n            Each rank 's max tensor group before gathering is max_size // group.size\\n    Returns:\\n        Gathered state dict\\n    \"\n    assert isinstance(dst, (list, tuple, int)), \"dst' type must be one of int, list and tuple\"\n    if isinstance(dst, int):\n        dst = [dst]\n    max_size = _parse_mem_size_to_bits(max_size)\n    max_size //= dist.get_world_size(group)\n    logger.debug('len state_dict: len(state_dict)')\n    state_dict_ = copy.copy(state_dict)\n    mw = None\n    has_mw = False\n    has_lr = False\n    if 'master_weights' in state_dict_:\n        mw = state_dict_.pop('master_weights', None)\n        has_mw = True\n    if 'LR_Scheduler' in state_dict_:\n        lr = state_dict_.pop('LR_Scheduler', None)\n        has_lr = True\n    output = _grouped_gather_data_dict(state_dict_, dst, group, max_size)\n    if isinstance(mw, dict):\n        masters = _grouped_gather_data_dict(mw, dst, group, max_size)\n    else:\n        assert mw is None, f'Wrong type of master weights . type: {type(mw)}'\n    if has_mw:\n        output['master_weights'] = masters\n    if has_lr:\n        output['LR_Scheduler'] = lr\n    return output"
        ]
    },
    {
        "func_name": "_grouped_gather_data_dict",
        "original": "def _grouped_gather_data_dict(state_data_dict, dst, group, max_size):\n    \"\"\"\n    Description:\n        Gather state data dict by groups.\n    Args:\n        state__data_dict(dict):\n            local dict to transfer.The state_data_dict only contains the mapping: str->paddle.Tensor\n        dst(int|list|tuple):\n            ranks the state dicts are gathered to\n        group(ProcessGroup):\n            group across which the state dicts are gathered\n        max_size(int|str):\n            The max limitation of the gathered tensor group size transformered a time. Default is 3G bits.\n            Each rank 's max tensor group before gathering is max_size // group.size\n    Returns:\n        Gatherd state_data_dict\n\n    \"\"\"\n    numpy_dict = {}\n    logger.debug(f'len state_tict_ : {len(state_data_dict)}')\n    for (k, v) in state_data_dict.items():\n        try:\n            numpy_dict[k] = v.numpy()\n        except:\n            raise TypeError(f\"the object (type of {type(v)}) of '{k}' is neither tensor nor parameter\")\n    total = 0\n    output_state = {}\n    logger.info('start all gather ...')\n    for state in _state_dict_groups(numpy_dict, max_size):\n        s_list = []\n        total += len(state)\n        logger.info(f'gen to gather: {total} / {len(numpy_dict)}')\n        dist.all_gather_object(s_list, state, group)\n        if dist.get_rank() in dst:\n            for s in s_list:\n                for (k, v) in s.items():\n                    logger.debug(f'gathered: {k}, {v.shape}')\n                output_state.update(s)\n        logger.debug(f's list size: {sum((len(s) for s in s_list))} output: {len(output_state)}')\n    while True:\n        s_list = []\n        state = {}\n        logger.debug('while True')\n        dist.all_gather_object(s_list, state, group)\n        if all_empty(s_list):\n            break\n        if dist.get_rank() in dst:\n            for s in s_list:\n                for (k, v) in s.items():\n                    logger.debug(f'gathered: {k}, {v.shape}')\n                output_state.update(s)\n        logger.debug(f's list size: {sum((len(s) for s in s_list))} output: {len(output_state)}')\n    logger.debug('all gathered ...')\n    if dist.get_rank() in dst:\n        place = paddle.CPUPlace()\n        for k in output_state.keys():\n            output_state[k] = paddle.to_tensor(output_state[k], place=place)\n            output_state[k].name = k\n        return output_state\n    return {}",
        "mutated": [
            "def _grouped_gather_data_dict(state_data_dict, dst, group, max_size):\n    if False:\n        i = 10\n    \"\\n    Description:\\n        Gather state data dict by groups.\\n    Args:\\n        state__data_dict(dict):\\n            local dict to transfer.The state_data_dict only contains the mapping: str->paddle.Tensor\\n        dst(int|list|tuple):\\n            ranks the state dicts are gathered to\\n        group(ProcessGroup):\\n            group across which the state dicts are gathered\\n        max_size(int|str):\\n            The max limitation of the gathered tensor group size transformered a time. Default is 3G bits.\\n            Each rank 's max tensor group before gathering is max_size // group.size\\n    Returns:\\n        Gatherd state_data_dict\\n\\n    \"\n    numpy_dict = {}\n    logger.debug(f'len state_tict_ : {len(state_data_dict)}')\n    for (k, v) in state_data_dict.items():\n        try:\n            numpy_dict[k] = v.numpy()\n        except:\n            raise TypeError(f\"the object (type of {type(v)}) of '{k}' is neither tensor nor parameter\")\n    total = 0\n    output_state = {}\n    logger.info('start all gather ...')\n    for state in _state_dict_groups(numpy_dict, max_size):\n        s_list = []\n        total += len(state)\n        logger.info(f'gen to gather: {total} / {len(numpy_dict)}')\n        dist.all_gather_object(s_list, state, group)\n        if dist.get_rank() in dst:\n            for s in s_list:\n                for (k, v) in s.items():\n                    logger.debug(f'gathered: {k}, {v.shape}')\n                output_state.update(s)\n        logger.debug(f's list size: {sum((len(s) for s in s_list))} output: {len(output_state)}')\n    while True:\n        s_list = []\n        state = {}\n        logger.debug('while True')\n        dist.all_gather_object(s_list, state, group)\n        if all_empty(s_list):\n            break\n        if dist.get_rank() in dst:\n            for s in s_list:\n                for (k, v) in s.items():\n                    logger.debug(f'gathered: {k}, {v.shape}')\n                output_state.update(s)\n        logger.debug(f's list size: {sum((len(s) for s in s_list))} output: {len(output_state)}')\n    logger.debug('all gathered ...')\n    if dist.get_rank() in dst:\n        place = paddle.CPUPlace()\n        for k in output_state.keys():\n            output_state[k] = paddle.to_tensor(output_state[k], place=place)\n            output_state[k].name = k\n        return output_state\n    return {}",
            "def _grouped_gather_data_dict(state_data_dict, dst, group, max_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Description:\\n        Gather state data dict by groups.\\n    Args:\\n        state__data_dict(dict):\\n            local dict to transfer.The state_data_dict only contains the mapping: str->paddle.Tensor\\n        dst(int|list|tuple):\\n            ranks the state dicts are gathered to\\n        group(ProcessGroup):\\n            group across which the state dicts are gathered\\n        max_size(int|str):\\n            The max limitation of the gathered tensor group size transformered a time. Default is 3G bits.\\n            Each rank 's max tensor group before gathering is max_size // group.size\\n    Returns:\\n        Gatherd state_data_dict\\n\\n    \"\n    numpy_dict = {}\n    logger.debug(f'len state_tict_ : {len(state_data_dict)}')\n    for (k, v) in state_data_dict.items():\n        try:\n            numpy_dict[k] = v.numpy()\n        except:\n            raise TypeError(f\"the object (type of {type(v)}) of '{k}' is neither tensor nor parameter\")\n    total = 0\n    output_state = {}\n    logger.info('start all gather ...')\n    for state in _state_dict_groups(numpy_dict, max_size):\n        s_list = []\n        total += len(state)\n        logger.info(f'gen to gather: {total} / {len(numpy_dict)}')\n        dist.all_gather_object(s_list, state, group)\n        if dist.get_rank() in dst:\n            for s in s_list:\n                for (k, v) in s.items():\n                    logger.debug(f'gathered: {k}, {v.shape}')\n                output_state.update(s)\n        logger.debug(f's list size: {sum((len(s) for s in s_list))} output: {len(output_state)}')\n    while True:\n        s_list = []\n        state = {}\n        logger.debug('while True')\n        dist.all_gather_object(s_list, state, group)\n        if all_empty(s_list):\n            break\n        if dist.get_rank() in dst:\n            for s in s_list:\n                for (k, v) in s.items():\n                    logger.debug(f'gathered: {k}, {v.shape}')\n                output_state.update(s)\n        logger.debug(f's list size: {sum((len(s) for s in s_list))} output: {len(output_state)}')\n    logger.debug('all gathered ...')\n    if dist.get_rank() in dst:\n        place = paddle.CPUPlace()\n        for k in output_state.keys():\n            output_state[k] = paddle.to_tensor(output_state[k], place=place)\n            output_state[k].name = k\n        return output_state\n    return {}",
            "def _grouped_gather_data_dict(state_data_dict, dst, group, max_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Description:\\n        Gather state data dict by groups.\\n    Args:\\n        state__data_dict(dict):\\n            local dict to transfer.The state_data_dict only contains the mapping: str->paddle.Tensor\\n        dst(int|list|tuple):\\n            ranks the state dicts are gathered to\\n        group(ProcessGroup):\\n            group across which the state dicts are gathered\\n        max_size(int|str):\\n            The max limitation of the gathered tensor group size transformered a time. Default is 3G bits.\\n            Each rank 's max tensor group before gathering is max_size // group.size\\n    Returns:\\n        Gatherd state_data_dict\\n\\n    \"\n    numpy_dict = {}\n    logger.debug(f'len state_tict_ : {len(state_data_dict)}')\n    for (k, v) in state_data_dict.items():\n        try:\n            numpy_dict[k] = v.numpy()\n        except:\n            raise TypeError(f\"the object (type of {type(v)}) of '{k}' is neither tensor nor parameter\")\n    total = 0\n    output_state = {}\n    logger.info('start all gather ...')\n    for state in _state_dict_groups(numpy_dict, max_size):\n        s_list = []\n        total += len(state)\n        logger.info(f'gen to gather: {total} / {len(numpy_dict)}')\n        dist.all_gather_object(s_list, state, group)\n        if dist.get_rank() in dst:\n            for s in s_list:\n                for (k, v) in s.items():\n                    logger.debug(f'gathered: {k}, {v.shape}')\n                output_state.update(s)\n        logger.debug(f's list size: {sum((len(s) for s in s_list))} output: {len(output_state)}')\n    while True:\n        s_list = []\n        state = {}\n        logger.debug('while True')\n        dist.all_gather_object(s_list, state, group)\n        if all_empty(s_list):\n            break\n        if dist.get_rank() in dst:\n            for s in s_list:\n                for (k, v) in s.items():\n                    logger.debug(f'gathered: {k}, {v.shape}')\n                output_state.update(s)\n        logger.debug(f's list size: {sum((len(s) for s in s_list))} output: {len(output_state)}')\n    logger.debug('all gathered ...')\n    if dist.get_rank() in dst:\n        place = paddle.CPUPlace()\n        for k in output_state.keys():\n            output_state[k] = paddle.to_tensor(output_state[k], place=place)\n            output_state[k].name = k\n        return output_state\n    return {}",
            "def _grouped_gather_data_dict(state_data_dict, dst, group, max_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Description:\\n        Gather state data dict by groups.\\n    Args:\\n        state__data_dict(dict):\\n            local dict to transfer.The state_data_dict only contains the mapping: str->paddle.Tensor\\n        dst(int|list|tuple):\\n            ranks the state dicts are gathered to\\n        group(ProcessGroup):\\n            group across which the state dicts are gathered\\n        max_size(int|str):\\n            The max limitation of the gathered tensor group size transformered a time. Default is 3G bits.\\n            Each rank 's max tensor group before gathering is max_size // group.size\\n    Returns:\\n        Gatherd state_data_dict\\n\\n    \"\n    numpy_dict = {}\n    logger.debug(f'len state_tict_ : {len(state_data_dict)}')\n    for (k, v) in state_data_dict.items():\n        try:\n            numpy_dict[k] = v.numpy()\n        except:\n            raise TypeError(f\"the object (type of {type(v)}) of '{k}' is neither tensor nor parameter\")\n    total = 0\n    output_state = {}\n    logger.info('start all gather ...')\n    for state in _state_dict_groups(numpy_dict, max_size):\n        s_list = []\n        total += len(state)\n        logger.info(f'gen to gather: {total} / {len(numpy_dict)}')\n        dist.all_gather_object(s_list, state, group)\n        if dist.get_rank() in dst:\n            for s in s_list:\n                for (k, v) in s.items():\n                    logger.debug(f'gathered: {k}, {v.shape}')\n                output_state.update(s)\n        logger.debug(f's list size: {sum((len(s) for s in s_list))} output: {len(output_state)}')\n    while True:\n        s_list = []\n        state = {}\n        logger.debug('while True')\n        dist.all_gather_object(s_list, state, group)\n        if all_empty(s_list):\n            break\n        if dist.get_rank() in dst:\n            for s in s_list:\n                for (k, v) in s.items():\n                    logger.debug(f'gathered: {k}, {v.shape}')\n                output_state.update(s)\n        logger.debug(f's list size: {sum((len(s) for s in s_list))} output: {len(output_state)}')\n    logger.debug('all gathered ...')\n    if dist.get_rank() in dst:\n        place = paddle.CPUPlace()\n        for k in output_state.keys():\n            output_state[k] = paddle.to_tensor(output_state[k], place=place)\n            output_state[k].name = k\n        return output_state\n    return {}",
            "def _grouped_gather_data_dict(state_data_dict, dst, group, max_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Description:\\n        Gather state data dict by groups.\\n    Args:\\n        state__data_dict(dict):\\n            local dict to transfer.The state_data_dict only contains the mapping: str->paddle.Tensor\\n        dst(int|list|tuple):\\n            ranks the state dicts are gathered to\\n        group(ProcessGroup):\\n            group across which the state dicts are gathered\\n        max_size(int|str):\\n            The max limitation of the gathered tensor group size transformered a time. Default is 3G bits.\\n            Each rank 's max tensor group before gathering is max_size // group.size\\n    Returns:\\n        Gatherd state_data_dict\\n\\n    \"\n    numpy_dict = {}\n    logger.debug(f'len state_tict_ : {len(state_data_dict)}')\n    for (k, v) in state_data_dict.items():\n        try:\n            numpy_dict[k] = v.numpy()\n        except:\n            raise TypeError(f\"the object (type of {type(v)}) of '{k}' is neither tensor nor parameter\")\n    total = 0\n    output_state = {}\n    logger.info('start all gather ...')\n    for state in _state_dict_groups(numpy_dict, max_size):\n        s_list = []\n        total += len(state)\n        logger.info(f'gen to gather: {total} / {len(numpy_dict)}')\n        dist.all_gather_object(s_list, state, group)\n        if dist.get_rank() in dst:\n            for s in s_list:\n                for (k, v) in s.items():\n                    logger.debug(f'gathered: {k}, {v.shape}')\n                output_state.update(s)\n        logger.debug(f's list size: {sum((len(s) for s in s_list))} output: {len(output_state)}')\n    while True:\n        s_list = []\n        state = {}\n        logger.debug('while True')\n        dist.all_gather_object(s_list, state, group)\n        if all_empty(s_list):\n            break\n        if dist.get_rank() in dst:\n            for s in s_list:\n                for (k, v) in s.items():\n                    logger.debug(f'gathered: {k}, {v.shape}')\n                output_state.update(s)\n        logger.debug(f's list size: {sum((len(s) for s in s_list))} output: {len(output_state)}')\n    logger.debug('all gathered ...')\n    if dist.get_rank() in dst:\n        place = paddle.CPUPlace()\n        for k in output_state.keys():\n            output_state[k] = paddle.to_tensor(output_state[k], place=place)\n            output_state[k].name = k\n        return output_state\n    return {}"
        ]
    },
    {
        "func_name": "_same_keys",
        "original": "def _same_keys(state_dict, group):\n    \"\"\"\n    Check whther all keys in each dict in the group are the same.\n    Used in sharding strategy to determine whether a dict needs to be gathered.\n    \"\"\"\n    keys = list(state_dict.keys())\n    key_list = []\n    logger.info(keys)\n    dist.all_gather_object(key_list, keys, group=group)\n    for k in key_list:\n        if not k == keys:\n            return False\n    return True",
        "mutated": [
            "def _same_keys(state_dict, group):\n    if False:\n        i = 10\n    '\\n    Check whther all keys in each dict in the group are the same.\\n    Used in sharding strategy to determine whether a dict needs to be gathered.\\n    '\n    keys = list(state_dict.keys())\n    key_list = []\n    logger.info(keys)\n    dist.all_gather_object(key_list, keys, group=group)\n    for k in key_list:\n        if not k == keys:\n            return False\n    return True",
            "def _same_keys(state_dict, group):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Check whther all keys in each dict in the group are the same.\\n    Used in sharding strategy to determine whether a dict needs to be gathered.\\n    '\n    keys = list(state_dict.keys())\n    key_list = []\n    logger.info(keys)\n    dist.all_gather_object(key_list, keys, group=group)\n    for k in key_list:\n        if not k == keys:\n            return False\n    return True",
            "def _same_keys(state_dict, group):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Check whther all keys in each dict in the group are the same.\\n    Used in sharding strategy to determine whether a dict needs to be gathered.\\n    '\n    keys = list(state_dict.keys())\n    key_list = []\n    logger.info(keys)\n    dist.all_gather_object(key_list, keys, group=group)\n    for k in key_list:\n        if not k == keys:\n            return False\n    return True",
            "def _same_keys(state_dict, group):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Check whther all keys in each dict in the group are the same.\\n    Used in sharding strategy to determine whether a dict needs to be gathered.\\n    '\n    keys = list(state_dict.keys())\n    key_list = []\n    logger.info(keys)\n    dist.all_gather_object(key_list, keys, group=group)\n    for k in key_list:\n        if not k == keys:\n            return False\n    return True",
            "def _same_keys(state_dict, group):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Check whther all keys in each dict in the group are the same.\\n    Used in sharding strategy to determine whether a dict needs to be gathered.\\n    '\n    keys = list(state_dict.keys())\n    key_list = []\n    logger.info(keys)\n    dist.all_gather_object(key_list, keys, group=group)\n    for k in key_list:\n        if not k == keys:\n            return False\n    return True"
        ]
    },
    {
        "func_name": "_remove_not_supported_conf",
        "original": "def _remove_not_supported_conf(configs):\n    \"\"\"\n    Remove the config values not supported by paddle.save\n    \"\"\"\n    __supported_by_save__ = ['use_binary_format']\n    configs_ = copy.copy(configs)\n    for k in configs.keys():\n        if k not in __supported_by_save__:\n            configs_.pop(k, None)\n    return configs_",
        "mutated": [
            "def _remove_not_supported_conf(configs):\n    if False:\n        i = 10\n    '\\n    Remove the config values not supported by paddle.save\\n    '\n    __supported_by_save__ = ['use_binary_format']\n    configs_ = copy.copy(configs)\n    for k in configs.keys():\n        if k not in __supported_by_save__:\n            configs_.pop(k, None)\n    return configs_",
            "def _remove_not_supported_conf(configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Remove the config values not supported by paddle.save\\n    '\n    __supported_by_save__ = ['use_binary_format']\n    configs_ = copy.copy(configs)\n    for k in configs.keys():\n        if k not in __supported_by_save__:\n            configs_.pop(k, None)\n    return configs_",
            "def _remove_not_supported_conf(configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Remove the config values not supported by paddle.save\\n    '\n    __supported_by_save__ = ['use_binary_format']\n    configs_ = copy.copy(configs)\n    for k in configs.keys():\n        if k not in __supported_by_save__:\n            configs_.pop(k, None)\n    return configs_",
            "def _remove_not_supported_conf(configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Remove the config values not supported by paddle.save\\n    '\n    __supported_by_save__ = ['use_binary_format']\n    configs_ = copy.copy(configs)\n    for k in configs.keys():\n        if k not in __supported_by_save__:\n            configs_.pop(k, None)\n    return configs_",
            "def _remove_not_supported_conf(configs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Remove the config values not supported by paddle.save\\n    '\n    __supported_by_save__ = ['use_binary_format']\n    configs_ = copy.copy(configs)\n    for k in configs.keys():\n        if k not in __supported_by_save__:\n            configs_.pop(k, None)\n    return configs_"
        ]
    }
]