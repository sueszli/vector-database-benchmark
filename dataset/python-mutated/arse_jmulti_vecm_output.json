[
    {
        "func_name": "print_debug_output",
        "original": "def print_debug_output(results, dt):\n    print('\\n\\n\\nDETERMINISTIC TERMS: ' + dt)\n    alpha = results['est']['alpha']\n    print('alpha:')\n    print(str(type(alpha)) + str(alpha.shape))\n    print(alpha)\n    print('se: ')\n    print(results['se']['alpha'])\n    print('t: ')\n    print(results['t']['alpha'])\n    print('p: ')\n    print(results['p']['alpha'])\n    beta = results['est']['beta']\n    print('beta:')\n    print(str(type(beta)) + str(beta.shape))\n    print(beta)\n    gamma = results['est']['Gamma']\n    print('Gamma:')\n    print(str(type(gamma)) + str(gamma.shape))\n    print(gamma)\n    if 'co' in dt or 's' in dt or 'lo' in dt:\n        c = results['est']['C']\n        print('C:')\n        print(str(type(c)) + str(c.shape))\n        print(c)\n        print('se: ')\n        print(results['se']['C'])",
        "mutated": [
            "def print_debug_output(results, dt):\n    if False:\n        i = 10\n    print('\\n\\n\\nDETERMINISTIC TERMS: ' + dt)\n    alpha = results['est']['alpha']\n    print('alpha:')\n    print(str(type(alpha)) + str(alpha.shape))\n    print(alpha)\n    print('se: ')\n    print(results['se']['alpha'])\n    print('t: ')\n    print(results['t']['alpha'])\n    print('p: ')\n    print(results['p']['alpha'])\n    beta = results['est']['beta']\n    print('beta:')\n    print(str(type(beta)) + str(beta.shape))\n    print(beta)\n    gamma = results['est']['Gamma']\n    print('Gamma:')\n    print(str(type(gamma)) + str(gamma.shape))\n    print(gamma)\n    if 'co' in dt or 's' in dt or 'lo' in dt:\n        c = results['est']['C']\n        print('C:')\n        print(str(type(c)) + str(c.shape))\n        print(c)\n        print('se: ')\n        print(results['se']['C'])",
            "def print_debug_output(results, dt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    print('\\n\\n\\nDETERMINISTIC TERMS: ' + dt)\n    alpha = results['est']['alpha']\n    print('alpha:')\n    print(str(type(alpha)) + str(alpha.shape))\n    print(alpha)\n    print('se: ')\n    print(results['se']['alpha'])\n    print('t: ')\n    print(results['t']['alpha'])\n    print('p: ')\n    print(results['p']['alpha'])\n    beta = results['est']['beta']\n    print('beta:')\n    print(str(type(beta)) + str(beta.shape))\n    print(beta)\n    gamma = results['est']['Gamma']\n    print('Gamma:')\n    print(str(type(gamma)) + str(gamma.shape))\n    print(gamma)\n    if 'co' in dt or 's' in dt or 'lo' in dt:\n        c = results['est']['C']\n        print('C:')\n        print(str(type(c)) + str(c.shape))\n        print(c)\n        print('se: ')\n        print(results['se']['C'])",
            "def print_debug_output(results, dt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    print('\\n\\n\\nDETERMINISTIC TERMS: ' + dt)\n    alpha = results['est']['alpha']\n    print('alpha:')\n    print(str(type(alpha)) + str(alpha.shape))\n    print(alpha)\n    print('se: ')\n    print(results['se']['alpha'])\n    print('t: ')\n    print(results['t']['alpha'])\n    print('p: ')\n    print(results['p']['alpha'])\n    beta = results['est']['beta']\n    print('beta:')\n    print(str(type(beta)) + str(beta.shape))\n    print(beta)\n    gamma = results['est']['Gamma']\n    print('Gamma:')\n    print(str(type(gamma)) + str(gamma.shape))\n    print(gamma)\n    if 'co' in dt or 's' in dt or 'lo' in dt:\n        c = results['est']['C']\n        print('C:')\n        print(str(type(c)) + str(c.shape))\n        print(c)\n        print('se: ')\n        print(results['se']['C'])",
            "def print_debug_output(results, dt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    print('\\n\\n\\nDETERMINISTIC TERMS: ' + dt)\n    alpha = results['est']['alpha']\n    print('alpha:')\n    print(str(type(alpha)) + str(alpha.shape))\n    print(alpha)\n    print('se: ')\n    print(results['se']['alpha'])\n    print('t: ')\n    print(results['t']['alpha'])\n    print('p: ')\n    print(results['p']['alpha'])\n    beta = results['est']['beta']\n    print('beta:')\n    print(str(type(beta)) + str(beta.shape))\n    print(beta)\n    gamma = results['est']['Gamma']\n    print('Gamma:')\n    print(str(type(gamma)) + str(gamma.shape))\n    print(gamma)\n    if 'co' in dt or 's' in dt or 'lo' in dt:\n        c = results['est']['C']\n        print('C:')\n        print(str(type(c)) + str(c.shape))\n        print(c)\n        print('se: ')\n        print(results['se']['C'])",
            "def print_debug_output(results, dt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    print('\\n\\n\\nDETERMINISTIC TERMS: ' + dt)\n    alpha = results['est']['alpha']\n    print('alpha:')\n    print(str(type(alpha)) + str(alpha.shape))\n    print(alpha)\n    print('se: ')\n    print(results['se']['alpha'])\n    print('t: ')\n    print(results['t']['alpha'])\n    print('p: ')\n    print(results['p']['alpha'])\n    beta = results['est']['beta']\n    print('beta:')\n    print(str(type(beta)) + str(beta.shape))\n    print(beta)\n    gamma = results['est']['Gamma']\n    print('Gamma:')\n    print(str(type(gamma)) + str(gamma.shape))\n    print(gamma)\n    if 'co' in dt or 's' in dt or 'lo' in dt:\n        c = results['est']['C']\n        print('C:')\n        print(str(type(c)) + str(c.shape))\n        print(c)\n        print('se: ')\n        print(results['se']['C'])"
        ]
    },
    {
        "func_name": "dt_s_tup_to_string",
        "original": "def dt_s_tup_to_string(dt_s_tup):\n    dt_string = dt_s_tup[0]\n    if dt_s_tup[1] > 0:\n        if 'co' in dt_string or 'ci' in dt_string or 'nc' in dt_string:\n            dt_string = dt_string[:2] + 's' + dt_string[2:]\n        else:\n            dt_string = 's' + dt_string\n    return dt_string",
        "mutated": [
            "def dt_s_tup_to_string(dt_s_tup):\n    if False:\n        i = 10\n    dt_string = dt_s_tup[0]\n    if dt_s_tup[1] > 0:\n        if 'co' in dt_string or 'ci' in dt_string or 'nc' in dt_string:\n            dt_string = dt_string[:2] + 's' + dt_string[2:]\n        else:\n            dt_string = 's' + dt_string\n    return dt_string",
            "def dt_s_tup_to_string(dt_s_tup):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dt_string = dt_s_tup[0]\n    if dt_s_tup[1] > 0:\n        if 'co' in dt_string or 'ci' in dt_string or 'nc' in dt_string:\n            dt_string = dt_string[:2] + 's' + dt_string[2:]\n        else:\n            dt_string = 's' + dt_string\n    return dt_string",
            "def dt_s_tup_to_string(dt_s_tup):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dt_string = dt_s_tup[0]\n    if dt_s_tup[1] > 0:\n        if 'co' in dt_string or 'ci' in dt_string or 'nc' in dt_string:\n            dt_string = dt_string[:2] + 's' + dt_string[2:]\n        else:\n            dt_string = 's' + dt_string\n    return dt_string",
            "def dt_s_tup_to_string(dt_s_tup):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dt_string = dt_s_tup[0]\n    if dt_s_tup[1] > 0:\n        if 'co' in dt_string or 'ci' in dt_string or 'nc' in dt_string:\n            dt_string = dt_string[:2] + 's' + dt_string[2:]\n        else:\n            dt_string = 's' + dt_string\n    return dt_string",
            "def dt_s_tup_to_string(dt_s_tup):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dt_string = dt_s_tup[0]\n    if dt_s_tup[1] > 0:\n        if 'co' in dt_string or 'ci' in dt_string or 'nc' in dt_string:\n            dt_string = dt_string[:2] + 's' + dt_string[2:]\n        else:\n            dt_string = 's' + dt_string\n    return dt_string"
        ]
    },
    {
        "func_name": "sublists",
        "original": "def sublists(lst, min_elmts=0, max_elmts=None):\n    \"\"\"Build a list of all possible sublists of a given list. Restrictions\n    on the length of the sublists can be posed via the min_elmts and max_elmts\n    parameters.\n    All sublists\n    have will have at least min_elmts elements and not more than max_elmts\n    elements.\n\n    Parameters\n    ----------\n    lst : list\n        Original list from which sublists are generated.\n    min_elmts : int\n        Lower bound for the length of sublists.\n    max_elmts : int or None\n        If int, then max_elmts are the upper bound for the length of sublists.\n        If None, sublists' length is not restricted. In this case the longest\n        sublist will be of the same length as the original list lst.\n\n    Returns\n    -------\n    result : list\n        A list of all sublists of lst fulfilling the length restrictions.\n    \"\"\"\n    if max_elmts is None:\n        max_elmts = len(lst)\n    result = itertools.chain.from_iterable((itertools.combinations(lst, sublist_len) for sublist_len in range(min_elmts, max_elmts + 1)))\n    if type(result) is not list:\n        result = list(result)\n    return result",
        "mutated": [
            "def sublists(lst, min_elmts=0, max_elmts=None):\n    if False:\n        i = 10\n    \"Build a list of all possible sublists of a given list. Restrictions\\n    on the length of the sublists can be posed via the min_elmts and max_elmts\\n    parameters.\\n    All sublists\\n    have will have at least min_elmts elements and not more than max_elmts\\n    elements.\\n\\n    Parameters\\n    ----------\\n    lst : list\\n        Original list from which sublists are generated.\\n    min_elmts : int\\n        Lower bound for the length of sublists.\\n    max_elmts : int or None\\n        If int, then max_elmts are the upper bound for the length of sublists.\\n        If None, sublists' length is not restricted. In this case the longest\\n        sublist will be of the same length as the original list lst.\\n\\n    Returns\\n    -------\\n    result : list\\n        A list of all sublists of lst fulfilling the length restrictions.\\n    \"\n    if max_elmts is None:\n        max_elmts = len(lst)\n    result = itertools.chain.from_iterable((itertools.combinations(lst, sublist_len) for sublist_len in range(min_elmts, max_elmts + 1)))\n    if type(result) is not list:\n        result = list(result)\n    return result",
            "def sublists(lst, min_elmts=0, max_elmts=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Build a list of all possible sublists of a given list. Restrictions\\n    on the length of the sublists can be posed via the min_elmts and max_elmts\\n    parameters.\\n    All sublists\\n    have will have at least min_elmts elements and not more than max_elmts\\n    elements.\\n\\n    Parameters\\n    ----------\\n    lst : list\\n        Original list from which sublists are generated.\\n    min_elmts : int\\n        Lower bound for the length of sublists.\\n    max_elmts : int or None\\n        If int, then max_elmts are the upper bound for the length of sublists.\\n        If None, sublists' length is not restricted. In this case the longest\\n        sublist will be of the same length as the original list lst.\\n\\n    Returns\\n    -------\\n    result : list\\n        A list of all sublists of lst fulfilling the length restrictions.\\n    \"\n    if max_elmts is None:\n        max_elmts = len(lst)\n    result = itertools.chain.from_iterable((itertools.combinations(lst, sublist_len) for sublist_len in range(min_elmts, max_elmts + 1)))\n    if type(result) is not list:\n        result = list(result)\n    return result",
            "def sublists(lst, min_elmts=0, max_elmts=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Build a list of all possible sublists of a given list. Restrictions\\n    on the length of the sublists can be posed via the min_elmts and max_elmts\\n    parameters.\\n    All sublists\\n    have will have at least min_elmts elements and not more than max_elmts\\n    elements.\\n\\n    Parameters\\n    ----------\\n    lst : list\\n        Original list from which sublists are generated.\\n    min_elmts : int\\n        Lower bound for the length of sublists.\\n    max_elmts : int or None\\n        If int, then max_elmts are the upper bound for the length of sublists.\\n        If None, sublists' length is not restricted. In this case the longest\\n        sublist will be of the same length as the original list lst.\\n\\n    Returns\\n    -------\\n    result : list\\n        A list of all sublists of lst fulfilling the length restrictions.\\n    \"\n    if max_elmts is None:\n        max_elmts = len(lst)\n    result = itertools.chain.from_iterable((itertools.combinations(lst, sublist_len) for sublist_len in range(min_elmts, max_elmts + 1)))\n    if type(result) is not list:\n        result = list(result)\n    return result",
            "def sublists(lst, min_elmts=0, max_elmts=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Build a list of all possible sublists of a given list. Restrictions\\n    on the length of the sublists can be posed via the min_elmts and max_elmts\\n    parameters.\\n    All sublists\\n    have will have at least min_elmts elements and not more than max_elmts\\n    elements.\\n\\n    Parameters\\n    ----------\\n    lst : list\\n        Original list from which sublists are generated.\\n    min_elmts : int\\n        Lower bound for the length of sublists.\\n    max_elmts : int or None\\n        If int, then max_elmts are the upper bound for the length of sublists.\\n        If None, sublists' length is not restricted. In this case the longest\\n        sublist will be of the same length as the original list lst.\\n\\n    Returns\\n    -------\\n    result : list\\n        A list of all sublists of lst fulfilling the length restrictions.\\n    \"\n    if max_elmts is None:\n        max_elmts = len(lst)\n    result = itertools.chain.from_iterable((itertools.combinations(lst, sublist_len) for sublist_len in range(min_elmts, max_elmts + 1)))\n    if type(result) is not list:\n        result = list(result)\n    return result",
            "def sublists(lst, min_elmts=0, max_elmts=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Build a list of all possible sublists of a given list. Restrictions\\n    on the length of the sublists can be posed via the min_elmts and max_elmts\\n    parameters.\\n    All sublists\\n    have will have at least min_elmts elements and not more than max_elmts\\n    elements.\\n\\n    Parameters\\n    ----------\\n    lst : list\\n        Original list from which sublists are generated.\\n    min_elmts : int\\n        Lower bound for the length of sublists.\\n    max_elmts : int or None\\n        If int, then max_elmts are the upper bound for the length of sublists.\\n        If None, sublists' length is not restricted. In this case the longest\\n        sublist will be of the same length as the original list lst.\\n\\n    Returns\\n    -------\\n    result : list\\n        A list of all sublists of lst fulfilling the length restrictions.\\n    \"\n    if max_elmts is None:\n        max_elmts = len(lst)\n    result = itertools.chain.from_iterable((itertools.combinations(lst, sublist_len) for sublist_len in range(min_elmts, max_elmts + 1)))\n    if type(result) is not list:\n        result = list(result)\n    return result"
        ]
    },
    {
        "func_name": "stringify_var_names",
        "original": "def stringify_var_names(var_list, delimiter=''):\n    \"\"\"\n\n    Parameters\n    ----------\n    var_list : list[str]\n        Each list element is the name of a variable.\n\n    Returns\n    -------\n    result : str\n        Concatenated variable names.\n    \"\"\"\n    result = var_list[0]\n    for var_name in var_list[1:]:\n        result += delimiter + var_name\n    return result.lower()",
        "mutated": [
            "def stringify_var_names(var_list, delimiter=''):\n    if False:\n        i = 10\n    '\\n\\n    Parameters\\n    ----------\\n    var_list : list[str]\\n        Each list element is the name of a variable.\\n\\n    Returns\\n    -------\\n    result : str\\n        Concatenated variable names.\\n    '\n    result = var_list[0]\n    for var_name in var_list[1:]:\n        result += delimiter + var_name\n    return result.lower()",
            "def stringify_var_names(var_list, delimiter=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n\\n    Parameters\\n    ----------\\n    var_list : list[str]\\n        Each list element is the name of a variable.\\n\\n    Returns\\n    -------\\n    result : str\\n        Concatenated variable names.\\n    '\n    result = var_list[0]\n    for var_name in var_list[1:]:\n        result += delimiter + var_name\n    return result.lower()",
            "def stringify_var_names(var_list, delimiter=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n\\n    Parameters\\n    ----------\\n    var_list : list[str]\\n        Each list element is the name of a variable.\\n\\n    Returns\\n    -------\\n    result : str\\n        Concatenated variable names.\\n    '\n    result = var_list[0]\n    for var_name in var_list[1:]:\n        result += delimiter + var_name\n    return result.lower()",
            "def stringify_var_names(var_list, delimiter=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n\\n    Parameters\\n    ----------\\n    var_list : list[str]\\n        Each list element is the name of a variable.\\n\\n    Returns\\n    -------\\n    result : str\\n        Concatenated variable names.\\n    '\n    result = var_list[0]\n    for var_name in var_list[1:]:\n        result += delimiter + var_name\n    return result.lower()",
            "def stringify_var_names(var_list, delimiter=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n\\n    Parameters\\n    ----------\\n    var_list : list[str]\\n        Each list element is the name of a variable.\\n\\n    Returns\\n    -------\\n    result : str\\n        Concatenated variable names.\\n    '\n    result = var_list[0]\n    for var_name in var_list[1:]:\n        result += delimiter + var_name\n    return result.lower()"
        ]
    },
    {
        "func_name": "load_results_jmulti",
        "original": "def load_results_jmulti(dataset):\n    \"\"\"\n\n    Parameters\n    ----------\n    dataset : module\n        A data module in the statsmodels/datasets directory that defines a\n        __str__() method returning the dataset's name.\n    dt_s_list : list\n        A list of strings where each string represents a combination of\n        deterministic terms.\n\n    Returns\n    -------\n    result : dict\n        A dict (keys: tuples of deterministic terms and seasonal terms)\n        of dicts (keys: strings \"est\" (for estimators),\n                              \"se\" (for standard errors),\n                              \"t\" (for t-values),\n                              \"p\" (for p-values))\n        of dicts (keys: strings \"alpha\", \"beta\", \"Gamma\" and other results)\n    \"\"\"\n    source = 'jmulti'\n    results_dict_per_det_terms = dict.fromkeys(dataset.dt_s_list)\n    for dt_s in dataset.dt_s_list:\n        dt_string = dt_s_tup_to_string(dt_s)\n        params_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '.txt'\n        params_file = os.path.join(here, params_file)\n        section_header = ['Lagged endogenous term', 'Deterministic term', 'Loading coefficients', 'Estimated cointegration relation', 'Legend', 'Lagged endogenous term', 'Deterministic term']\n        sections = ['Gamma', 'C', 'alpha', 'beta', 'Legend', 'VAR A', 'VAR deterministic']\n        if 'co' not in dt_string and 'lo' not in dt_string and ('s' not in dt_string):\n            del section_header[1]\n            del sections[1]\n            if 'ci' not in dt_string and 'li' not in dt_string:\n                del section_header[-1]\n                del sections[-1]\n        results = dict()\n        results['est'] = dict.fromkeys(sections)\n        results['se'] = dict.fromkeys(sections)\n        results['t'] = dict.fromkeys(sections)\n        results['p'] = dict.fromkeys(sections)\n        section = -1\n        result = []\n        result_se = []\n        result_t = []\n        result_p = []\n        rows = 0\n        started_reading_section = False\n        start_end_mark = '-----'\n        params_file = open(params_file, encoding='latin_1')\n        for line in params_file:\n            if section == -1 and section_header[section + 1] not in line:\n                continue\n            if section < len(section_header) - 1 and section_header[section + 1] in line:\n                section += 1\n                continue\n            if not started_reading_section:\n                if line.startswith(start_end_mark):\n                    started_reading_section = True\n                continue\n            if started_reading_section:\n                if line.startswith(start_end_mark):\n                    if result == []:\n                        started_reading_section = False\n                        continue\n                    results['est'][sections[section]] = np.column_stack(result)\n                    result = []\n                    results['se'][sections[section]] = np.column_stack(result_se)\n                    result_se = []\n                    results['t'][sections[section]] = np.column_stack(result_t)\n                    result_t = []\n                    results['p'][sections[section]] = np.column_stack(result_p)\n                    result_p = []\n                    started_reading_section = False\n                    continue\n                str_number = '-?\\\\d+\\\\.\\\\d{3}'\n                regex_est = re.compile(str_number + '[^\\\\)\\\\]\\\\}]')\n                est_col = re.findall(regex_est, line)\n                regex_se = re.compile('\\\\(' + str_number + '\\\\)')\n                se_col = re.findall(regex_se, line)\n                regex_t_value = re.compile('\\\\[' + str_number + '\\\\]')\n                t_col = re.findall(regex_t_value, line)\n                regex_p_value = re.compile('\\\\{' + str_number + '\\\\}')\n                p_col = re.findall(regex_p_value, line)\n                if result == [] and est_col != []:\n                    rows = len(est_col)\n                if est_col != []:\n                    est_col = [float(el) for el in est_col]\n                    result.append(est_col)\n                elif se_col != []:\n                    for i in range(rows):\n                        se_col[i] = se_col[i].replace('(', '').replace(')', '')\n                    se_col = [float(el) for el in se_col]\n                    result_se.append(se_col)\n                elif t_col != []:\n                    for i in range(rows):\n                        t_col[i] = t_col[i].replace('[', '').replace(']', '')\n                    t_col = [float(el) for el in t_col]\n                    result_t.append(t_col)\n                elif p_col != []:\n                    for i in range(rows):\n                        p_col[i] = p_col[i].replace('{', '').replace('}', '')\n                    p_col = [float(el) for el in p_col]\n                    result_p.append(p_col)\n        params_file.close()\n        del results['est']['Legend']\n        del results['se']['Legend']\n        del results['t']['Legend']\n        del results['p']['Legend']\n        results['est']['beta'] = results['est']['beta'].T\n        results['se']['beta'] = results['se']['beta'].T\n        results['t']['beta'] = results['t']['beta'].T\n        results['p']['beta'] = results['p']['beta'].T\n        alpha = results['est']['alpha']\n        beta = results['est']['beta']\n        alpha_rows = alpha.shape[0]\n        if beta.shape[0] > alpha_rows:\n            (results['est']['beta'], results['est']['det_coint']) = np.vsplit(results['est']['beta'], [alpha_rows])\n            (results['se']['beta'], results['se']['det_coint']) = np.vsplit(results['se']['beta'], [alpha_rows])\n            (results['t']['beta'], results['t']['det_coint']) = np.vsplit(results['t']['beta'], [alpha_rows])\n            (results['p']['beta'], results['p']['det_coint']) = np.vsplit(results['p']['beta'], [alpha_rows])\n        sigmau_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_Sigmau' + '.txt'\n        sigmau_file = os.path.join(here, sigmau_file)\n        rows_to_parse = 0\n        regex_est = re.compile('\\\\s+\\\\S+e\\\\S+')\n        sigmau_section_reached = False\n        sigmau_file = open(sigmau_file, encoding='latin_1')\n        for line in sigmau_file:\n            if line.startswith('Log Likelihood:'):\n                line = line.split('Log Likelihood:')[1]\n                results['log_like'] = float(re.findall(regex_est, line)[0])\n            if not sigmau_section_reached and 'Covariance:' not in line:\n                continue\n            if 'Covariance:' in line:\n                sigmau_section_reached = True\n                row = re.findall(regex_est, line)\n                rows_to_parse = len(row)\n                sigma_u = np.empty((rows_to_parse, rows_to_parse))\n            row = re.findall(regex_est, line)\n            rows_to_parse -= 1\n            sigma_u[rows_to_parse] = row\n            if rows_to_parse == 0:\n                break\n        sigmau_file.close()\n        results['est']['Sigma_u'] = sigma_u[::-1]\n        fc_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_fc5' + '.txt'\n        fc_file = os.path.join(here, fc_file)\n        (fc, lower, upper, plu_min) = ([], [], [], [])\n        fc_file = open(fc_file, encoding='latin_1')\n        for line in fc_file:\n            str_number = '(\\\\s+-?\\\\d+\\\\.\\\\d{4}\\\\s*?)'\n            regex_number = re.compile(str_number)\n            numbers = re.findall(regex_number, line)\n            if numbers == []:\n                continue\n            fc.append(float(numbers[0]))\n            lower.append(float(numbers[1]))\n            upper.append(float(numbers[2]))\n            plu_min.append(float(numbers[3]))\n        fc_file.close()\n        variables = alpha.shape[0]\n        fc = np.hstack(np.vsplit(np.array(fc)[:, None], variables))\n        lower = np.hstack(np.vsplit(np.array(lower)[:, None], variables))\n        upper = np.hstack(np.vsplit(np.array(upper)[:, None], variables))\n        results['fc'] = dict.fromkeys(['fc', 'lower', 'upper'])\n        results['fc']['fc'] = fc\n        results['fc']['lower'] = lower\n        results['fc']['upper'] = upper\n        results['granger_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['granger_caus']['p'] = dict()\n        results['granger_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        for causing in var_combs:\n            caused = tuple((el for el in vn if el not in causing))\n            granger_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_granger_causality_' + stringify_var_names(causing) + '_' + stringify_var_names(caused) + '.txt'\n            granger_file = os.path.join(here, granger_file)\n            granger_file = open(granger_file, encoding='latin_1')\n            granger_results = []\n            for line in granger_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                granger_results.append(number)\n            granger_file.close()\n            results['granger_caus']['test_stat'][causing, caused] = granger_results[0]\n            results['granger_caus']['p'][causing, caused] = granger_results[1]\n        results['inst_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['inst_caus']['p'] = dict()\n        results['inst_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        for causing in var_combs:\n            caused = tuple((el for el in vn if el not in causing))\n            inst_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_inst_causality_' + stringify_var_names(causing) + '_' + stringify_var_names(caused) + '.txt'\n            inst_file = os.path.join(here, inst_file)\n            inst_file = open(inst_file, encoding='latin_1')\n            inst_results = []\n            for line in inst_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                inst_results.append(number)\n            inst_file.close()\n            results['inst_caus']['test_stat'][causing, caused] = inst_results[2]\n            results['inst_caus']['p'][causing, caused] = inst_results[3]\n        ir_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_ir' + '.txt'\n        ir_file = os.path.join(here, ir_file)\n        ir_file = open(ir_file, encoding='latin_1')\n        causing = None\n        caused = None\n        data = None\n        regex_vars = re.compile('\\\\w+')\n        regex_vals = re.compile('-?\\\\d+\\\\.\\\\d{4}')\n        line_start_causing = 'time'\n        data_line_indicator = 'point estimate'\n        data_rows_read = 0\n        for line in ir_file:\n            if causing is None and (not line.startswith(line_start_causing)):\n                continue\n            if line.startswith(line_start_causing):\n                line = line[4:]\n                causing = re.findall(regex_vars, line)\n                data = np.empty((21, len(causing)))\n                continue\n            if caused is None:\n                caused = re.findall(regex_vars, line)\n                continue\n            if data_line_indicator not in line:\n                continue\n            start = line.find(data_line_indicator) + len(data_line_indicator)\n            line = line[start:]\n            data[data_rows_read] = re.findall(regex_vals, line)\n            data_rows_read += 1\n        ir_file.close()\n        results['ir'] = data\n        lagorder_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_lagorder' + '.txt'\n        lagorder_file = os.path.join(here, lagorder_file)\n        lagorder_file = open(lagorder_file, encoding='latin_1')\n        results['lagorder'] = dict()\n        aic_start = 'Akaike Info Criterion:'\n        fpe_start = 'Final Prediction Error:'\n        hqic_start = 'Hannan-Quinn Criterion:'\n        bic_start = 'Schwarz Criterion:'\n        for line in lagorder_file:\n            if line.startswith(aic_start):\n                results['lagorder']['aic'] = int(line[len(aic_start):])\n            elif line.startswith(fpe_start):\n                results['lagorder']['fpe'] = int(line[len(fpe_start):])\n            elif line.startswith(hqic_start):\n                results['lagorder']['hqic'] = int(line[len(hqic_start):])\n            elif line.startswith(bic_start):\n                results['lagorder']['bic'] = int(line[len(bic_start):])\n        lagorder_file.close()\n        test_norm_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        test_norm_file = os.path.join(here, test_norm_file)\n        test_norm_file = open(test_norm_file, encoding='latin_1')\n        results['test_norm'] = dict()\n        reading_values = False\n        line_start_statistic = 'joint test statistic:'\n        line_start_pvalue = ' p-value:'\n        for line in test_norm_file:\n            if not reading_values:\n                if 'Introduction to Multiple Time Series Analysis' in line:\n                    reading_values = True\n                continue\n            if 'joint_pvalue' in results['test_norm'].keys():\n                break\n            if line.startswith(line_start_statistic):\n                line_end = line[len(line_start_statistic):]\n                results['test_norm']['joint_test_statistic'] = float(line_end)\n            if line.startswith(line_start_pvalue):\n                line_end = line[len(line_start_pvalue):]\n                results['test_norm']['joint_pvalue'] = float(line_end)\n        test_norm_file.close()\n        whiteness_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        whiteness_file = os.path.join(here, whiteness_file)\n        whiteness_file = open(whiteness_file, encoding='latin_1')\n        results['whiteness'] = dict()\n        section_start_marker = 'PORTMANTEAU TEST'\n        order_start = 'tested order:'\n        statistic_start = 'test statistic:'\n        p_start = ' p-value:'\n        adj_statistic_start = 'adjusted test statistic:'\n        unadjusted_finished = False\n        in_section = False\n        for line in whiteness_file:\n            if not in_section and section_start_marker not in line:\n                continue\n            if not in_section and section_start_marker in line:\n                in_section = True\n                continue\n            if line.startswith(order_start):\n                results['whiteness']['tested order'] = int(line[len(order_start):])\n                continue\n            if line.startswith(statistic_start):\n                results['whiteness']['test statistic'] = float(line[len(statistic_start):])\n                continue\n            if line.startswith(adj_statistic_start):\n                results['whiteness']['test statistic adj.'] = float(line[len(adj_statistic_start):])\n                continue\n            if line.startswith(p_start):\n                if not unadjusted_finished:\n                    results['whiteness']['p-value'] = float(line[len(p_start):])\n                    unadjusted_finished = True\n                else:\n                    results['whiteness']['p-value adjusted'] = float(line[len(p_start):])\n                    break\n        whiteness_file.close()\n        if debug_mode:\n            print_debug_output(results, dt_string)\n        results_dict_per_det_terms[dt_s] = results\n    return results_dict_per_det_terms",
        "mutated": [
            "def load_results_jmulti(dataset):\n    if False:\n        i = 10\n    '\\n\\n    Parameters\\n    ----------\\n    dataset : module\\n        A data module in the statsmodels/datasets directory that defines a\\n        __str__() method returning the dataset\\'s name.\\n    dt_s_list : list\\n        A list of strings where each string represents a combination of\\n        deterministic terms.\\n\\n    Returns\\n    -------\\n    result : dict\\n        A dict (keys: tuples of deterministic terms and seasonal terms)\\n        of dicts (keys: strings \"est\" (for estimators),\\n                              \"se\" (for standard errors),\\n                              \"t\" (for t-values),\\n                              \"p\" (for p-values))\\n        of dicts (keys: strings \"alpha\", \"beta\", \"Gamma\" and other results)\\n    '\n    source = 'jmulti'\n    results_dict_per_det_terms = dict.fromkeys(dataset.dt_s_list)\n    for dt_s in dataset.dt_s_list:\n        dt_string = dt_s_tup_to_string(dt_s)\n        params_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '.txt'\n        params_file = os.path.join(here, params_file)\n        section_header = ['Lagged endogenous term', 'Deterministic term', 'Loading coefficients', 'Estimated cointegration relation', 'Legend', 'Lagged endogenous term', 'Deterministic term']\n        sections = ['Gamma', 'C', 'alpha', 'beta', 'Legend', 'VAR A', 'VAR deterministic']\n        if 'co' not in dt_string and 'lo' not in dt_string and ('s' not in dt_string):\n            del section_header[1]\n            del sections[1]\n            if 'ci' not in dt_string and 'li' not in dt_string:\n                del section_header[-1]\n                del sections[-1]\n        results = dict()\n        results['est'] = dict.fromkeys(sections)\n        results['se'] = dict.fromkeys(sections)\n        results['t'] = dict.fromkeys(sections)\n        results['p'] = dict.fromkeys(sections)\n        section = -1\n        result = []\n        result_se = []\n        result_t = []\n        result_p = []\n        rows = 0\n        started_reading_section = False\n        start_end_mark = '-----'\n        params_file = open(params_file, encoding='latin_1')\n        for line in params_file:\n            if section == -1 and section_header[section + 1] not in line:\n                continue\n            if section < len(section_header) - 1 and section_header[section + 1] in line:\n                section += 1\n                continue\n            if not started_reading_section:\n                if line.startswith(start_end_mark):\n                    started_reading_section = True\n                continue\n            if started_reading_section:\n                if line.startswith(start_end_mark):\n                    if result == []:\n                        started_reading_section = False\n                        continue\n                    results['est'][sections[section]] = np.column_stack(result)\n                    result = []\n                    results['se'][sections[section]] = np.column_stack(result_se)\n                    result_se = []\n                    results['t'][sections[section]] = np.column_stack(result_t)\n                    result_t = []\n                    results['p'][sections[section]] = np.column_stack(result_p)\n                    result_p = []\n                    started_reading_section = False\n                    continue\n                str_number = '-?\\\\d+\\\\.\\\\d{3}'\n                regex_est = re.compile(str_number + '[^\\\\)\\\\]\\\\}]')\n                est_col = re.findall(regex_est, line)\n                regex_se = re.compile('\\\\(' + str_number + '\\\\)')\n                se_col = re.findall(regex_se, line)\n                regex_t_value = re.compile('\\\\[' + str_number + '\\\\]')\n                t_col = re.findall(regex_t_value, line)\n                regex_p_value = re.compile('\\\\{' + str_number + '\\\\}')\n                p_col = re.findall(regex_p_value, line)\n                if result == [] and est_col != []:\n                    rows = len(est_col)\n                if est_col != []:\n                    est_col = [float(el) for el in est_col]\n                    result.append(est_col)\n                elif se_col != []:\n                    for i in range(rows):\n                        se_col[i] = se_col[i].replace('(', '').replace(')', '')\n                    se_col = [float(el) for el in se_col]\n                    result_se.append(se_col)\n                elif t_col != []:\n                    for i in range(rows):\n                        t_col[i] = t_col[i].replace('[', '').replace(']', '')\n                    t_col = [float(el) for el in t_col]\n                    result_t.append(t_col)\n                elif p_col != []:\n                    for i in range(rows):\n                        p_col[i] = p_col[i].replace('{', '').replace('}', '')\n                    p_col = [float(el) for el in p_col]\n                    result_p.append(p_col)\n        params_file.close()\n        del results['est']['Legend']\n        del results['se']['Legend']\n        del results['t']['Legend']\n        del results['p']['Legend']\n        results['est']['beta'] = results['est']['beta'].T\n        results['se']['beta'] = results['se']['beta'].T\n        results['t']['beta'] = results['t']['beta'].T\n        results['p']['beta'] = results['p']['beta'].T\n        alpha = results['est']['alpha']\n        beta = results['est']['beta']\n        alpha_rows = alpha.shape[0]\n        if beta.shape[0] > alpha_rows:\n            (results['est']['beta'], results['est']['det_coint']) = np.vsplit(results['est']['beta'], [alpha_rows])\n            (results['se']['beta'], results['se']['det_coint']) = np.vsplit(results['se']['beta'], [alpha_rows])\n            (results['t']['beta'], results['t']['det_coint']) = np.vsplit(results['t']['beta'], [alpha_rows])\n            (results['p']['beta'], results['p']['det_coint']) = np.vsplit(results['p']['beta'], [alpha_rows])\n        sigmau_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_Sigmau' + '.txt'\n        sigmau_file = os.path.join(here, sigmau_file)\n        rows_to_parse = 0\n        regex_est = re.compile('\\\\s+\\\\S+e\\\\S+')\n        sigmau_section_reached = False\n        sigmau_file = open(sigmau_file, encoding='latin_1')\n        for line in sigmau_file:\n            if line.startswith('Log Likelihood:'):\n                line = line.split('Log Likelihood:')[1]\n                results['log_like'] = float(re.findall(regex_est, line)[0])\n            if not sigmau_section_reached and 'Covariance:' not in line:\n                continue\n            if 'Covariance:' in line:\n                sigmau_section_reached = True\n                row = re.findall(regex_est, line)\n                rows_to_parse = len(row)\n                sigma_u = np.empty((rows_to_parse, rows_to_parse))\n            row = re.findall(regex_est, line)\n            rows_to_parse -= 1\n            sigma_u[rows_to_parse] = row\n            if rows_to_parse == 0:\n                break\n        sigmau_file.close()\n        results['est']['Sigma_u'] = sigma_u[::-1]\n        fc_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_fc5' + '.txt'\n        fc_file = os.path.join(here, fc_file)\n        (fc, lower, upper, plu_min) = ([], [], [], [])\n        fc_file = open(fc_file, encoding='latin_1')\n        for line in fc_file:\n            str_number = '(\\\\s+-?\\\\d+\\\\.\\\\d{4}\\\\s*?)'\n            regex_number = re.compile(str_number)\n            numbers = re.findall(regex_number, line)\n            if numbers == []:\n                continue\n            fc.append(float(numbers[0]))\n            lower.append(float(numbers[1]))\n            upper.append(float(numbers[2]))\n            plu_min.append(float(numbers[3]))\n        fc_file.close()\n        variables = alpha.shape[0]\n        fc = np.hstack(np.vsplit(np.array(fc)[:, None], variables))\n        lower = np.hstack(np.vsplit(np.array(lower)[:, None], variables))\n        upper = np.hstack(np.vsplit(np.array(upper)[:, None], variables))\n        results['fc'] = dict.fromkeys(['fc', 'lower', 'upper'])\n        results['fc']['fc'] = fc\n        results['fc']['lower'] = lower\n        results['fc']['upper'] = upper\n        results['granger_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['granger_caus']['p'] = dict()\n        results['granger_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        for causing in var_combs:\n            caused = tuple((el for el in vn if el not in causing))\n            granger_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_granger_causality_' + stringify_var_names(causing) + '_' + stringify_var_names(caused) + '.txt'\n            granger_file = os.path.join(here, granger_file)\n            granger_file = open(granger_file, encoding='latin_1')\n            granger_results = []\n            for line in granger_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                granger_results.append(number)\n            granger_file.close()\n            results['granger_caus']['test_stat'][causing, caused] = granger_results[0]\n            results['granger_caus']['p'][causing, caused] = granger_results[1]\n        results['inst_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['inst_caus']['p'] = dict()\n        results['inst_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        for causing in var_combs:\n            caused = tuple((el for el in vn if el not in causing))\n            inst_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_inst_causality_' + stringify_var_names(causing) + '_' + stringify_var_names(caused) + '.txt'\n            inst_file = os.path.join(here, inst_file)\n            inst_file = open(inst_file, encoding='latin_1')\n            inst_results = []\n            for line in inst_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                inst_results.append(number)\n            inst_file.close()\n            results['inst_caus']['test_stat'][causing, caused] = inst_results[2]\n            results['inst_caus']['p'][causing, caused] = inst_results[3]\n        ir_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_ir' + '.txt'\n        ir_file = os.path.join(here, ir_file)\n        ir_file = open(ir_file, encoding='latin_1')\n        causing = None\n        caused = None\n        data = None\n        regex_vars = re.compile('\\\\w+')\n        regex_vals = re.compile('-?\\\\d+\\\\.\\\\d{4}')\n        line_start_causing = 'time'\n        data_line_indicator = 'point estimate'\n        data_rows_read = 0\n        for line in ir_file:\n            if causing is None and (not line.startswith(line_start_causing)):\n                continue\n            if line.startswith(line_start_causing):\n                line = line[4:]\n                causing = re.findall(regex_vars, line)\n                data = np.empty((21, len(causing)))\n                continue\n            if caused is None:\n                caused = re.findall(regex_vars, line)\n                continue\n            if data_line_indicator not in line:\n                continue\n            start = line.find(data_line_indicator) + len(data_line_indicator)\n            line = line[start:]\n            data[data_rows_read] = re.findall(regex_vals, line)\n            data_rows_read += 1\n        ir_file.close()\n        results['ir'] = data\n        lagorder_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_lagorder' + '.txt'\n        lagorder_file = os.path.join(here, lagorder_file)\n        lagorder_file = open(lagorder_file, encoding='latin_1')\n        results['lagorder'] = dict()\n        aic_start = 'Akaike Info Criterion:'\n        fpe_start = 'Final Prediction Error:'\n        hqic_start = 'Hannan-Quinn Criterion:'\n        bic_start = 'Schwarz Criterion:'\n        for line in lagorder_file:\n            if line.startswith(aic_start):\n                results['lagorder']['aic'] = int(line[len(aic_start):])\n            elif line.startswith(fpe_start):\n                results['lagorder']['fpe'] = int(line[len(fpe_start):])\n            elif line.startswith(hqic_start):\n                results['lagorder']['hqic'] = int(line[len(hqic_start):])\n            elif line.startswith(bic_start):\n                results['lagorder']['bic'] = int(line[len(bic_start):])\n        lagorder_file.close()\n        test_norm_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        test_norm_file = os.path.join(here, test_norm_file)\n        test_norm_file = open(test_norm_file, encoding='latin_1')\n        results['test_norm'] = dict()\n        reading_values = False\n        line_start_statistic = 'joint test statistic:'\n        line_start_pvalue = ' p-value:'\n        for line in test_norm_file:\n            if not reading_values:\n                if 'Introduction to Multiple Time Series Analysis' in line:\n                    reading_values = True\n                continue\n            if 'joint_pvalue' in results['test_norm'].keys():\n                break\n            if line.startswith(line_start_statistic):\n                line_end = line[len(line_start_statistic):]\n                results['test_norm']['joint_test_statistic'] = float(line_end)\n            if line.startswith(line_start_pvalue):\n                line_end = line[len(line_start_pvalue):]\n                results['test_norm']['joint_pvalue'] = float(line_end)\n        test_norm_file.close()\n        whiteness_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        whiteness_file = os.path.join(here, whiteness_file)\n        whiteness_file = open(whiteness_file, encoding='latin_1')\n        results['whiteness'] = dict()\n        section_start_marker = 'PORTMANTEAU TEST'\n        order_start = 'tested order:'\n        statistic_start = 'test statistic:'\n        p_start = ' p-value:'\n        adj_statistic_start = 'adjusted test statistic:'\n        unadjusted_finished = False\n        in_section = False\n        for line in whiteness_file:\n            if not in_section and section_start_marker not in line:\n                continue\n            if not in_section and section_start_marker in line:\n                in_section = True\n                continue\n            if line.startswith(order_start):\n                results['whiteness']['tested order'] = int(line[len(order_start):])\n                continue\n            if line.startswith(statistic_start):\n                results['whiteness']['test statistic'] = float(line[len(statistic_start):])\n                continue\n            if line.startswith(adj_statistic_start):\n                results['whiteness']['test statistic adj.'] = float(line[len(adj_statistic_start):])\n                continue\n            if line.startswith(p_start):\n                if not unadjusted_finished:\n                    results['whiteness']['p-value'] = float(line[len(p_start):])\n                    unadjusted_finished = True\n                else:\n                    results['whiteness']['p-value adjusted'] = float(line[len(p_start):])\n                    break\n        whiteness_file.close()\n        if debug_mode:\n            print_debug_output(results, dt_string)\n        results_dict_per_det_terms[dt_s] = results\n    return results_dict_per_det_terms",
            "def load_results_jmulti(dataset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n\\n    Parameters\\n    ----------\\n    dataset : module\\n        A data module in the statsmodels/datasets directory that defines a\\n        __str__() method returning the dataset\\'s name.\\n    dt_s_list : list\\n        A list of strings where each string represents a combination of\\n        deterministic terms.\\n\\n    Returns\\n    -------\\n    result : dict\\n        A dict (keys: tuples of deterministic terms and seasonal terms)\\n        of dicts (keys: strings \"est\" (for estimators),\\n                              \"se\" (for standard errors),\\n                              \"t\" (for t-values),\\n                              \"p\" (for p-values))\\n        of dicts (keys: strings \"alpha\", \"beta\", \"Gamma\" and other results)\\n    '\n    source = 'jmulti'\n    results_dict_per_det_terms = dict.fromkeys(dataset.dt_s_list)\n    for dt_s in dataset.dt_s_list:\n        dt_string = dt_s_tup_to_string(dt_s)\n        params_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '.txt'\n        params_file = os.path.join(here, params_file)\n        section_header = ['Lagged endogenous term', 'Deterministic term', 'Loading coefficients', 'Estimated cointegration relation', 'Legend', 'Lagged endogenous term', 'Deterministic term']\n        sections = ['Gamma', 'C', 'alpha', 'beta', 'Legend', 'VAR A', 'VAR deterministic']\n        if 'co' not in dt_string and 'lo' not in dt_string and ('s' not in dt_string):\n            del section_header[1]\n            del sections[1]\n            if 'ci' not in dt_string and 'li' not in dt_string:\n                del section_header[-1]\n                del sections[-1]\n        results = dict()\n        results['est'] = dict.fromkeys(sections)\n        results['se'] = dict.fromkeys(sections)\n        results['t'] = dict.fromkeys(sections)\n        results['p'] = dict.fromkeys(sections)\n        section = -1\n        result = []\n        result_se = []\n        result_t = []\n        result_p = []\n        rows = 0\n        started_reading_section = False\n        start_end_mark = '-----'\n        params_file = open(params_file, encoding='latin_1')\n        for line in params_file:\n            if section == -1 and section_header[section + 1] not in line:\n                continue\n            if section < len(section_header) - 1 and section_header[section + 1] in line:\n                section += 1\n                continue\n            if not started_reading_section:\n                if line.startswith(start_end_mark):\n                    started_reading_section = True\n                continue\n            if started_reading_section:\n                if line.startswith(start_end_mark):\n                    if result == []:\n                        started_reading_section = False\n                        continue\n                    results['est'][sections[section]] = np.column_stack(result)\n                    result = []\n                    results['se'][sections[section]] = np.column_stack(result_se)\n                    result_se = []\n                    results['t'][sections[section]] = np.column_stack(result_t)\n                    result_t = []\n                    results['p'][sections[section]] = np.column_stack(result_p)\n                    result_p = []\n                    started_reading_section = False\n                    continue\n                str_number = '-?\\\\d+\\\\.\\\\d{3}'\n                regex_est = re.compile(str_number + '[^\\\\)\\\\]\\\\}]')\n                est_col = re.findall(regex_est, line)\n                regex_se = re.compile('\\\\(' + str_number + '\\\\)')\n                se_col = re.findall(regex_se, line)\n                regex_t_value = re.compile('\\\\[' + str_number + '\\\\]')\n                t_col = re.findall(regex_t_value, line)\n                regex_p_value = re.compile('\\\\{' + str_number + '\\\\}')\n                p_col = re.findall(regex_p_value, line)\n                if result == [] and est_col != []:\n                    rows = len(est_col)\n                if est_col != []:\n                    est_col = [float(el) for el in est_col]\n                    result.append(est_col)\n                elif se_col != []:\n                    for i in range(rows):\n                        se_col[i] = se_col[i].replace('(', '').replace(')', '')\n                    se_col = [float(el) for el in se_col]\n                    result_se.append(se_col)\n                elif t_col != []:\n                    for i in range(rows):\n                        t_col[i] = t_col[i].replace('[', '').replace(']', '')\n                    t_col = [float(el) for el in t_col]\n                    result_t.append(t_col)\n                elif p_col != []:\n                    for i in range(rows):\n                        p_col[i] = p_col[i].replace('{', '').replace('}', '')\n                    p_col = [float(el) for el in p_col]\n                    result_p.append(p_col)\n        params_file.close()\n        del results['est']['Legend']\n        del results['se']['Legend']\n        del results['t']['Legend']\n        del results['p']['Legend']\n        results['est']['beta'] = results['est']['beta'].T\n        results['se']['beta'] = results['se']['beta'].T\n        results['t']['beta'] = results['t']['beta'].T\n        results['p']['beta'] = results['p']['beta'].T\n        alpha = results['est']['alpha']\n        beta = results['est']['beta']\n        alpha_rows = alpha.shape[0]\n        if beta.shape[0] > alpha_rows:\n            (results['est']['beta'], results['est']['det_coint']) = np.vsplit(results['est']['beta'], [alpha_rows])\n            (results['se']['beta'], results['se']['det_coint']) = np.vsplit(results['se']['beta'], [alpha_rows])\n            (results['t']['beta'], results['t']['det_coint']) = np.vsplit(results['t']['beta'], [alpha_rows])\n            (results['p']['beta'], results['p']['det_coint']) = np.vsplit(results['p']['beta'], [alpha_rows])\n        sigmau_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_Sigmau' + '.txt'\n        sigmau_file = os.path.join(here, sigmau_file)\n        rows_to_parse = 0\n        regex_est = re.compile('\\\\s+\\\\S+e\\\\S+')\n        sigmau_section_reached = False\n        sigmau_file = open(sigmau_file, encoding='latin_1')\n        for line in sigmau_file:\n            if line.startswith('Log Likelihood:'):\n                line = line.split('Log Likelihood:')[1]\n                results['log_like'] = float(re.findall(regex_est, line)[0])\n            if not sigmau_section_reached and 'Covariance:' not in line:\n                continue\n            if 'Covariance:' in line:\n                sigmau_section_reached = True\n                row = re.findall(regex_est, line)\n                rows_to_parse = len(row)\n                sigma_u = np.empty((rows_to_parse, rows_to_parse))\n            row = re.findall(regex_est, line)\n            rows_to_parse -= 1\n            sigma_u[rows_to_parse] = row\n            if rows_to_parse == 0:\n                break\n        sigmau_file.close()\n        results['est']['Sigma_u'] = sigma_u[::-1]\n        fc_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_fc5' + '.txt'\n        fc_file = os.path.join(here, fc_file)\n        (fc, lower, upper, plu_min) = ([], [], [], [])\n        fc_file = open(fc_file, encoding='latin_1')\n        for line in fc_file:\n            str_number = '(\\\\s+-?\\\\d+\\\\.\\\\d{4}\\\\s*?)'\n            regex_number = re.compile(str_number)\n            numbers = re.findall(regex_number, line)\n            if numbers == []:\n                continue\n            fc.append(float(numbers[0]))\n            lower.append(float(numbers[1]))\n            upper.append(float(numbers[2]))\n            plu_min.append(float(numbers[3]))\n        fc_file.close()\n        variables = alpha.shape[0]\n        fc = np.hstack(np.vsplit(np.array(fc)[:, None], variables))\n        lower = np.hstack(np.vsplit(np.array(lower)[:, None], variables))\n        upper = np.hstack(np.vsplit(np.array(upper)[:, None], variables))\n        results['fc'] = dict.fromkeys(['fc', 'lower', 'upper'])\n        results['fc']['fc'] = fc\n        results['fc']['lower'] = lower\n        results['fc']['upper'] = upper\n        results['granger_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['granger_caus']['p'] = dict()\n        results['granger_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        for causing in var_combs:\n            caused = tuple((el for el in vn if el not in causing))\n            granger_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_granger_causality_' + stringify_var_names(causing) + '_' + stringify_var_names(caused) + '.txt'\n            granger_file = os.path.join(here, granger_file)\n            granger_file = open(granger_file, encoding='latin_1')\n            granger_results = []\n            for line in granger_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                granger_results.append(number)\n            granger_file.close()\n            results['granger_caus']['test_stat'][causing, caused] = granger_results[0]\n            results['granger_caus']['p'][causing, caused] = granger_results[1]\n        results['inst_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['inst_caus']['p'] = dict()\n        results['inst_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        for causing in var_combs:\n            caused = tuple((el for el in vn if el not in causing))\n            inst_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_inst_causality_' + stringify_var_names(causing) + '_' + stringify_var_names(caused) + '.txt'\n            inst_file = os.path.join(here, inst_file)\n            inst_file = open(inst_file, encoding='latin_1')\n            inst_results = []\n            for line in inst_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                inst_results.append(number)\n            inst_file.close()\n            results['inst_caus']['test_stat'][causing, caused] = inst_results[2]\n            results['inst_caus']['p'][causing, caused] = inst_results[3]\n        ir_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_ir' + '.txt'\n        ir_file = os.path.join(here, ir_file)\n        ir_file = open(ir_file, encoding='latin_1')\n        causing = None\n        caused = None\n        data = None\n        regex_vars = re.compile('\\\\w+')\n        regex_vals = re.compile('-?\\\\d+\\\\.\\\\d{4}')\n        line_start_causing = 'time'\n        data_line_indicator = 'point estimate'\n        data_rows_read = 0\n        for line in ir_file:\n            if causing is None and (not line.startswith(line_start_causing)):\n                continue\n            if line.startswith(line_start_causing):\n                line = line[4:]\n                causing = re.findall(regex_vars, line)\n                data = np.empty((21, len(causing)))\n                continue\n            if caused is None:\n                caused = re.findall(regex_vars, line)\n                continue\n            if data_line_indicator not in line:\n                continue\n            start = line.find(data_line_indicator) + len(data_line_indicator)\n            line = line[start:]\n            data[data_rows_read] = re.findall(regex_vals, line)\n            data_rows_read += 1\n        ir_file.close()\n        results['ir'] = data\n        lagorder_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_lagorder' + '.txt'\n        lagorder_file = os.path.join(here, lagorder_file)\n        lagorder_file = open(lagorder_file, encoding='latin_1')\n        results['lagorder'] = dict()\n        aic_start = 'Akaike Info Criterion:'\n        fpe_start = 'Final Prediction Error:'\n        hqic_start = 'Hannan-Quinn Criterion:'\n        bic_start = 'Schwarz Criterion:'\n        for line in lagorder_file:\n            if line.startswith(aic_start):\n                results['lagorder']['aic'] = int(line[len(aic_start):])\n            elif line.startswith(fpe_start):\n                results['lagorder']['fpe'] = int(line[len(fpe_start):])\n            elif line.startswith(hqic_start):\n                results['lagorder']['hqic'] = int(line[len(hqic_start):])\n            elif line.startswith(bic_start):\n                results['lagorder']['bic'] = int(line[len(bic_start):])\n        lagorder_file.close()\n        test_norm_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        test_norm_file = os.path.join(here, test_norm_file)\n        test_norm_file = open(test_norm_file, encoding='latin_1')\n        results['test_norm'] = dict()\n        reading_values = False\n        line_start_statistic = 'joint test statistic:'\n        line_start_pvalue = ' p-value:'\n        for line in test_norm_file:\n            if not reading_values:\n                if 'Introduction to Multiple Time Series Analysis' in line:\n                    reading_values = True\n                continue\n            if 'joint_pvalue' in results['test_norm'].keys():\n                break\n            if line.startswith(line_start_statistic):\n                line_end = line[len(line_start_statistic):]\n                results['test_norm']['joint_test_statistic'] = float(line_end)\n            if line.startswith(line_start_pvalue):\n                line_end = line[len(line_start_pvalue):]\n                results['test_norm']['joint_pvalue'] = float(line_end)\n        test_norm_file.close()\n        whiteness_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        whiteness_file = os.path.join(here, whiteness_file)\n        whiteness_file = open(whiteness_file, encoding='latin_1')\n        results['whiteness'] = dict()\n        section_start_marker = 'PORTMANTEAU TEST'\n        order_start = 'tested order:'\n        statistic_start = 'test statistic:'\n        p_start = ' p-value:'\n        adj_statistic_start = 'adjusted test statistic:'\n        unadjusted_finished = False\n        in_section = False\n        for line in whiteness_file:\n            if not in_section and section_start_marker not in line:\n                continue\n            if not in_section and section_start_marker in line:\n                in_section = True\n                continue\n            if line.startswith(order_start):\n                results['whiteness']['tested order'] = int(line[len(order_start):])\n                continue\n            if line.startswith(statistic_start):\n                results['whiteness']['test statistic'] = float(line[len(statistic_start):])\n                continue\n            if line.startswith(adj_statistic_start):\n                results['whiteness']['test statistic adj.'] = float(line[len(adj_statistic_start):])\n                continue\n            if line.startswith(p_start):\n                if not unadjusted_finished:\n                    results['whiteness']['p-value'] = float(line[len(p_start):])\n                    unadjusted_finished = True\n                else:\n                    results['whiteness']['p-value adjusted'] = float(line[len(p_start):])\n                    break\n        whiteness_file.close()\n        if debug_mode:\n            print_debug_output(results, dt_string)\n        results_dict_per_det_terms[dt_s] = results\n    return results_dict_per_det_terms",
            "def load_results_jmulti(dataset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n\\n    Parameters\\n    ----------\\n    dataset : module\\n        A data module in the statsmodels/datasets directory that defines a\\n        __str__() method returning the dataset\\'s name.\\n    dt_s_list : list\\n        A list of strings where each string represents a combination of\\n        deterministic terms.\\n\\n    Returns\\n    -------\\n    result : dict\\n        A dict (keys: tuples of deterministic terms and seasonal terms)\\n        of dicts (keys: strings \"est\" (for estimators),\\n                              \"se\" (for standard errors),\\n                              \"t\" (for t-values),\\n                              \"p\" (for p-values))\\n        of dicts (keys: strings \"alpha\", \"beta\", \"Gamma\" and other results)\\n    '\n    source = 'jmulti'\n    results_dict_per_det_terms = dict.fromkeys(dataset.dt_s_list)\n    for dt_s in dataset.dt_s_list:\n        dt_string = dt_s_tup_to_string(dt_s)\n        params_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '.txt'\n        params_file = os.path.join(here, params_file)\n        section_header = ['Lagged endogenous term', 'Deterministic term', 'Loading coefficients', 'Estimated cointegration relation', 'Legend', 'Lagged endogenous term', 'Deterministic term']\n        sections = ['Gamma', 'C', 'alpha', 'beta', 'Legend', 'VAR A', 'VAR deterministic']\n        if 'co' not in dt_string and 'lo' not in dt_string and ('s' not in dt_string):\n            del section_header[1]\n            del sections[1]\n            if 'ci' not in dt_string and 'li' not in dt_string:\n                del section_header[-1]\n                del sections[-1]\n        results = dict()\n        results['est'] = dict.fromkeys(sections)\n        results['se'] = dict.fromkeys(sections)\n        results['t'] = dict.fromkeys(sections)\n        results['p'] = dict.fromkeys(sections)\n        section = -1\n        result = []\n        result_se = []\n        result_t = []\n        result_p = []\n        rows = 0\n        started_reading_section = False\n        start_end_mark = '-----'\n        params_file = open(params_file, encoding='latin_1')\n        for line in params_file:\n            if section == -1 and section_header[section + 1] not in line:\n                continue\n            if section < len(section_header) - 1 and section_header[section + 1] in line:\n                section += 1\n                continue\n            if not started_reading_section:\n                if line.startswith(start_end_mark):\n                    started_reading_section = True\n                continue\n            if started_reading_section:\n                if line.startswith(start_end_mark):\n                    if result == []:\n                        started_reading_section = False\n                        continue\n                    results['est'][sections[section]] = np.column_stack(result)\n                    result = []\n                    results['se'][sections[section]] = np.column_stack(result_se)\n                    result_se = []\n                    results['t'][sections[section]] = np.column_stack(result_t)\n                    result_t = []\n                    results['p'][sections[section]] = np.column_stack(result_p)\n                    result_p = []\n                    started_reading_section = False\n                    continue\n                str_number = '-?\\\\d+\\\\.\\\\d{3}'\n                regex_est = re.compile(str_number + '[^\\\\)\\\\]\\\\}]')\n                est_col = re.findall(regex_est, line)\n                regex_se = re.compile('\\\\(' + str_number + '\\\\)')\n                se_col = re.findall(regex_se, line)\n                regex_t_value = re.compile('\\\\[' + str_number + '\\\\]')\n                t_col = re.findall(regex_t_value, line)\n                regex_p_value = re.compile('\\\\{' + str_number + '\\\\}')\n                p_col = re.findall(regex_p_value, line)\n                if result == [] and est_col != []:\n                    rows = len(est_col)\n                if est_col != []:\n                    est_col = [float(el) for el in est_col]\n                    result.append(est_col)\n                elif se_col != []:\n                    for i in range(rows):\n                        se_col[i] = se_col[i].replace('(', '').replace(')', '')\n                    se_col = [float(el) for el in se_col]\n                    result_se.append(se_col)\n                elif t_col != []:\n                    for i in range(rows):\n                        t_col[i] = t_col[i].replace('[', '').replace(']', '')\n                    t_col = [float(el) for el in t_col]\n                    result_t.append(t_col)\n                elif p_col != []:\n                    for i in range(rows):\n                        p_col[i] = p_col[i].replace('{', '').replace('}', '')\n                    p_col = [float(el) for el in p_col]\n                    result_p.append(p_col)\n        params_file.close()\n        del results['est']['Legend']\n        del results['se']['Legend']\n        del results['t']['Legend']\n        del results['p']['Legend']\n        results['est']['beta'] = results['est']['beta'].T\n        results['se']['beta'] = results['se']['beta'].T\n        results['t']['beta'] = results['t']['beta'].T\n        results['p']['beta'] = results['p']['beta'].T\n        alpha = results['est']['alpha']\n        beta = results['est']['beta']\n        alpha_rows = alpha.shape[0]\n        if beta.shape[0] > alpha_rows:\n            (results['est']['beta'], results['est']['det_coint']) = np.vsplit(results['est']['beta'], [alpha_rows])\n            (results['se']['beta'], results['se']['det_coint']) = np.vsplit(results['se']['beta'], [alpha_rows])\n            (results['t']['beta'], results['t']['det_coint']) = np.vsplit(results['t']['beta'], [alpha_rows])\n            (results['p']['beta'], results['p']['det_coint']) = np.vsplit(results['p']['beta'], [alpha_rows])\n        sigmau_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_Sigmau' + '.txt'\n        sigmau_file = os.path.join(here, sigmau_file)\n        rows_to_parse = 0\n        regex_est = re.compile('\\\\s+\\\\S+e\\\\S+')\n        sigmau_section_reached = False\n        sigmau_file = open(sigmau_file, encoding='latin_1')\n        for line in sigmau_file:\n            if line.startswith('Log Likelihood:'):\n                line = line.split('Log Likelihood:')[1]\n                results['log_like'] = float(re.findall(regex_est, line)[0])\n            if not sigmau_section_reached and 'Covariance:' not in line:\n                continue\n            if 'Covariance:' in line:\n                sigmau_section_reached = True\n                row = re.findall(regex_est, line)\n                rows_to_parse = len(row)\n                sigma_u = np.empty((rows_to_parse, rows_to_parse))\n            row = re.findall(regex_est, line)\n            rows_to_parse -= 1\n            sigma_u[rows_to_parse] = row\n            if rows_to_parse == 0:\n                break\n        sigmau_file.close()\n        results['est']['Sigma_u'] = sigma_u[::-1]\n        fc_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_fc5' + '.txt'\n        fc_file = os.path.join(here, fc_file)\n        (fc, lower, upper, plu_min) = ([], [], [], [])\n        fc_file = open(fc_file, encoding='latin_1')\n        for line in fc_file:\n            str_number = '(\\\\s+-?\\\\d+\\\\.\\\\d{4}\\\\s*?)'\n            regex_number = re.compile(str_number)\n            numbers = re.findall(regex_number, line)\n            if numbers == []:\n                continue\n            fc.append(float(numbers[0]))\n            lower.append(float(numbers[1]))\n            upper.append(float(numbers[2]))\n            plu_min.append(float(numbers[3]))\n        fc_file.close()\n        variables = alpha.shape[0]\n        fc = np.hstack(np.vsplit(np.array(fc)[:, None], variables))\n        lower = np.hstack(np.vsplit(np.array(lower)[:, None], variables))\n        upper = np.hstack(np.vsplit(np.array(upper)[:, None], variables))\n        results['fc'] = dict.fromkeys(['fc', 'lower', 'upper'])\n        results['fc']['fc'] = fc\n        results['fc']['lower'] = lower\n        results['fc']['upper'] = upper\n        results['granger_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['granger_caus']['p'] = dict()\n        results['granger_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        for causing in var_combs:\n            caused = tuple((el for el in vn if el not in causing))\n            granger_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_granger_causality_' + stringify_var_names(causing) + '_' + stringify_var_names(caused) + '.txt'\n            granger_file = os.path.join(here, granger_file)\n            granger_file = open(granger_file, encoding='latin_1')\n            granger_results = []\n            for line in granger_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                granger_results.append(number)\n            granger_file.close()\n            results['granger_caus']['test_stat'][causing, caused] = granger_results[0]\n            results['granger_caus']['p'][causing, caused] = granger_results[1]\n        results['inst_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['inst_caus']['p'] = dict()\n        results['inst_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        for causing in var_combs:\n            caused = tuple((el for el in vn if el not in causing))\n            inst_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_inst_causality_' + stringify_var_names(causing) + '_' + stringify_var_names(caused) + '.txt'\n            inst_file = os.path.join(here, inst_file)\n            inst_file = open(inst_file, encoding='latin_1')\n            inst_results = []\n            for line in inst_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                inst_results.append(number)\n            inst_file.close()\n            results['inst_caus']['test_stat'][causing, caused] = inst_results[2]\n            results['inst_caus']['p'][causing, caused] = inst_results[3]\n        ir_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_ir' + '.txt'\n        ir_file = os.path.join(here, ir_file)\n        ir_file = open(ir_file, encoding='latin_1')\n        causing = None\n        caused = None\n        data = None\n        regex_vars = re.compile('\\\\w+')\n        regex_vals = re.compile('-?\\\\d+\\\\.\\\\d{4}')\n        line_start_causing = 'time'\n        data_line_indicator = 'point estimate'\n        data_rows_read = 0\n        for line in ir_file:\n            if causing is None and (not line.startswith(line_start_causing)):\n                continue\n            if line.startswith(line_start_causing):\n                line = line[4:]\n                causing = re.findall(regex_vars, line)\n                data = np.empty((21, len(causing)))\n                continue\n            if caused is None:\n                caused = re.findall(regex_vars, line)\n                continue\n            if data_line_indicator not in line:\n                continue\n            start = line.find(data_line_indicator) + len(data_line_indicator)\n            line = line[start:]\n            data[data_rows_read] = re.findall(regex_vals, line)\n            data_rows_read += 1\n        ir_file.close()\n        results['ir'] = data\n        lagorder_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_lagorder' + '.txt'\n        lagorder_file = os.path.join(here, lagorder_file)\n        lagorder_file = open(lagorder_file, encoding='latin_1')\n        results['lagorder'] = dict()\n        aic_start = 'Akaike Info Criterion:'\n        fpe_start = 'Final Prediction Error:'\n        hqic_start = 'Hannan-Quinn Criterion:'\n        bic_start = 'Schwarz Criterion:'\n        for line in lagorder_file:\n            if line.startswith(aic_start):\n                results['lagorder']['aic'] = int(line[len(aic_start):])\n            elif line.startswith(fpe_start):\n                results['lagorder']['fpe'] = int(line[len(fpe_start):])\n            elif line.startswith(hqic_start):\n                results['lagorder']['hqic'] = int(line[len(hqic_start):])\n            elif line.startswith(bic_start):\n                results['lagorder']['bic'] = int(line[len(bic_start):])\n        lagorder_file.close()\n        test_norm_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        test_norm_file = os.path.join(here, test_norm_file)\n        test_norm_file = open(test_norm_file, encoding='latin_1')\n        results['test_norm'] = dict()\n        reading_values = False\n        line_start_statistic = 'joint test statistic:'\n        line_start_pvalue = ' p-value:'\n        for line in test_norm_file:\n            if not reading_values:\n                if 'Introduction to Multiple Time Series Analysis' in line:\n                    reading_values = True\n                continue\n            if 'joint_pvalue' in results['test_norm'].keys():\n                break\n            if line.startswith(line_start_statistic):\n                line_end = line[len(line_start_statistic):]\n                results['test_norm']['joint_test_statistic'] = float(line_end)\n            if line.startswith(line_start_pvalue):\n                line_end = line[len(line_start_pvalue):]\n                results['test_norm']['joint_pvalue'] = float(line_end)\n        test_norm_file.close()\n        whiteness_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        whiteness_file = os.path.join(here, whiteness_file)\n        whiteness_file = open(whiteness_file, encoding='latin_1')\n        results['whiteness'] = dict()\n        section_start_marker = 'PORTMANTEAU TEST'\n        order_start = 'tested order:'\n        statistic_start = 'test statistic:'\n        p_start = ' p-value:'\n        adj_statistic_start = 'adjusted test statistic:'\n        unadjusted_finished = False\n        in_section = False\n        for line in whiteness_file:\n            if not in_section and section_start_marker not in line:\n                continue\n            if not in_section and section_start_marker in line:\n                in_section = True\n                continue\n            if line.startswith(order_start):\n                results['whiteness']['tested order'] = int(line[len(order_start):])\n                continue\n            if line.startswith(statistic_start):\n                results['whiteness']['test statistic'] = float(line[len(statistic_start):])\n                continue\n            if line.startswith(adj_statistic_start):\n                results['whiteness']['test statistic adj.'] = float(line[len(adj_statistic_start):])\n                continue\n            if line.startswith(p_start):\n                if not unadjusted_finished:\n                    results['whiteness']['p-value'] = float(line[len(p_start):])\n                    unadjusted_finished = True\n                else:\n                    results['whiteness']['p-value adjusted'] = float(line[len(p_start):])\n                    break\n        whiteness_file.close()\n        if debug_mode:\n            print_debug_output(results, dt_string)\n        results_dict_per_det_terms[dt_s] = results\n    return results_dict_per_det_terms",
            "def load_results_jmulti(dataset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n\\n    Parameters\\n    ----------\\n    dataset : module\\n        A data module in the statsmodels/datasets directory that defines a\\n        __str__() method returning the dataset\\'s name.\\n    dt_s_list : list\\n        A list of strings where each string represents a combination of\\n        deterministic terms.\\n\\n    Returns\\n    -------\\n    result : dict\\n        A dict (keys: tuples of deterministic terms and seasonal terms)\\n        of dicts (keys: strings \"est\" (for estimators),\\n                              \"se\" (for standard errors),\\n                              \"t\" (for t-values),\\n                              \"p\" (for p-values))\\n        of dicts (keys: strings \"alpha\", \"beta\", \"Gamma\" and other results)\\n    '\n    source = 'jmulti'\n    results_dict_per_det_terms = dict.fromkeys(dataset.dt_s_list)\n    for dt_s in dataset.dt_s_list:\n        dt_string = dt_s_tup_to_string(dt_s)\n        params_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '.txt'\n        params_file = os.path.join(here, params_file)\n        section_header = ['Lagged endogenous term', 'Deterministic term', 'Loading coefficients', 'Estimated cointegration relation', 'Legend', 'Lagged endogenous term', 'Deterministic term']\n        sections = ['Gamma', 'C', 'alpha', 'beta', 'Legend', 'VAR A', 'VAR deterministic']\n        if 'co' not in dt_string and 'lo' not in dt_string and ('s' not in dt_string):\n            del section_header[1]\n            del sections[1]\n            if 'ci' not in dt_string and 'li' not in dt_string:\n                del section_header[-1]\n                del sections[-1]\n        results = dict()\n        results['est'] = dict.fromkeys(sections)\n        results['se'] = dict.fromkeys(sections)\n        results['t'] = dict.fromkeys(sections)\n        results['p'] = dict.fromkeys(sections)\n        section = -1\n        result = []\n        result_se = []\n        result_t = []\n        result_p = []\n        rows = 0\n        started_reading_section = False\n        start_end_mark = '-----'\n        params_file = open(params_file, encoding='latin_1')\n        for line in params_file:\n            if section == -1 and section_header[section + 1] not in line:\n                continue\n            if section < len(section_header) - 1 and section_header[section + 1] in line:\n                section += 1\n                continue\n            if not started_reading_section:\n                if line.startswith(start_end_mark):\n                    started_reading_section = True\n                continue\n            if started_reading_section:\n                if line.startswith(start_end_mark):\n                    if result == []:\n                        started_reading_section = False\n                        continue\n                    results['est'][sections[section]] = np.column_stack(result)\n                    result = []\n                    results['se'][sections[section]] = np.column_stack(result_se)\n                    result_se = []\n                    results['t'][sections[section]] = np.column_stack(result_t)\n                    result_t = []\n                    results['p'][sections[section]] = np.column_stack(result_p)\n                    result_p = []\n                    started_reading_section = False\n                    continue\n                str_number = '-?\\\\d+\\\\.\\\\d{3}'\n                regex_est = re.compile(str_number + '[^\\\\)\\\\]\\\\}]')\n                est_col = re.findall(regex_est, line)\n                regex_se = re.compile('\\\\(' + str_number + '\\\\)')\n                se_col = re.findall(regex_se, line)\n                regex_t_value = re.compile('\\\\[' + str_number + '\\\\]')\n                t_col = re.findall(regex_t_value, line)\n                regex_p_value = re.compile('\\\\{' + str_number + '\\\\}')\n                p_col = re.findall(regex_p_value, line)\n                if result == [] and est_col != []:\n                    rows = len(est_col)\n                if est_col != []:\n                    est_col = [float(el) for el in est_col]\n                    result.append(est_col)\n                elif se_col != []:\n                    for i in range(rows):\n                        se_col[i] = se_col[i].replace('(', '').replace(')', '')\n                    se_col = [float(el) for el in se_col]\n                    result_se.append(se_col)\n                elif t_col != []:\n                    for i in range(rows):\n                        t_col[i] = t_col[i].replace('[', '').replace(']', '')\n                    t_col = [float(el) for el in t_col]\n                    result_t.append(t_col)\n                elif p_col != []:\n                    for i in range(rows):\n                        p_col[i] = p_col[i].replace('{', '').replace('}', '')\n                    p_col = [float(el) for el in p_col]\n                    result_p.append(p_col)\n        params_file.close()\n        del results['est']['Legend']\n        del results['se']['Legend']\n        del results['t']['Legend']\n        del results['p']['Legend']\n        results['est']['beta'] = results['est']['beta'].T\n        results['se']['beta'] = results['se']['beta'].T\n        results['t']['beta'] = results['t']['beta'].T\n        results['p']['beta'] = results['p']['beta'].T\n        alpha = results['est']['alpha']\n        beta = results['est']['beta']\n        alpha_rows = alpha.shape[0]\n        if beta.shape[0] > alpha_rows:\n            (results['est']['beta'], results['est']['det_coint']) = np.vsplit(results['est']['beta'], [alpha_rows])\n            (results['se']['beta'], results['se']['det_coint']) = np.vsplit(results['se']['beta'], [alpha_rows])\n            (results['t']['beta'], results['t']['det_coint']) = np.vsplit(results['t']['beta'], [alpha_rows])\n            (results['p']['beta'], results['p']['det_coint']) = np.vsplit(results['p']['beta'], [alpha_rows])\n        sigmau_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_Sigmau' + '.txt'\n        sigmau_file = os.path.join(here, sigmau_file)\n        rows_to_parse = 0\n        regex_est = re.compile('\\\\s+\\\\S+e\\\\S+')\n        sigmau_section_reached = False\n        sigmau_file = open(sigmau_file, encoding='latin_1')\n        for line in sigmau_file:\n            if line.startswith('Log Likelihood:'):\n                line = line.split('Log Likelihood:')[1]\n                results['log_like'] = float(re.findall(regex_est, line)[0])\n            if not sigmau_section_reached and 'Covariance:' not in line:\n                continue\n            if 'Covariance:' in line:\n                sigmau_section_reached = True\n                row = re.findall(regex_est, line)\n                rows_to_parse = len(row)\n                sigma_u = np.empty((rows_to_parse, rows_to_parse))\n            row = re.findall(regex_est, line)\n            rows_to_parse -= 1\n            sigma_u[rows_to_parse] = row\n            if rows_to_parse == 0:\n                break\n        sigmau_file.close()\n        results['est']['Sigma_u'] = sigma_u[::-1]\n        fc_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_fc5' + '.txt'\n        fc_file = os.path.join(here, fc_file)\n        (fc, lower, upper, plu_min) = ([], [], [], [])\n        fc_file = open(fc_file, encoding='latin_1')\n        for line in fc_file:\n            str_number = '(\\\\s+-?\\\\d+\\\\.\\\\d{4}\\\\s*?)'\n            regex_number = re.compile(str_number)\n            numbers = re.findall(regex_number, line)\n            if numbers == []:\n                continue\n            fc.append(float(numbers[0]))\n            lower.append(float(numbers[1]))\n            upper.append(float(numbers[2]))\n            plu_min.append(float(numbers[3]))\n        fc_file.close()\n        variables = alpha.shape[0]\n        fc = np.hstack(np.vsplit(np.array(fc)[:, None], variables))\n        lower = np.hstack(np.vsplit(np.array(lower)[:, None], variables))\n        upper = np.hstack(np.vsplit(np.array(upper)[:, None], variables))\n        results['fc'] = dict.fromkeys(['fc', 'lower', 'upper'])\n        results['fc']['fc'] = fc\n        results['fc']['lower'] = lower\n        results['fc']['upper'] = upper\n        results['granger_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['granger_caus']['p'] = dict()\n        results['granger_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        for causing in var_combs:\n            caused = tuple((el for el in vn if el not in causing))\n            granger_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_granger_causality_' + stringify_var_names(causing) + '_' + stringify_var_names(caused) + '.txt'\n            granger_file = os.path.join(here, granger_file)\n            granger_file = open(granger_file, encoding='latin_1')\n            granger_results = []\n            for line in granger_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                granger_results.append(number)\n            granger_file.close()\n            results['granger_caus']['test_stat'][causing, caused] = granger_results[0]\n            results['granger_caus']['p'][causing, caused] = granger_results[1]\n        results['inst_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['inst_caus']['p'] = dict()\n        results['inst_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        for causing in var_combs:\n            caused = tuple((el for el in vn if el not in causing))\n            inst_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_inst_causality_' + stringify_var_names(causing) + '_' + stringify_var_names(caused) + '.txt'\n            inst_file = os.path.join(here, inst_file)\n            inst_file = open(inst_file, encoding='latin_1')\n            inst_results = []\n            for line in inst_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                inst_results.append(number)\n            inst_file.close()\n            results['inst_caus']['test_stat'][causing, caused] = inst_results[2]\n            results['inst_caus']['p'][causing, caused] = inst_results[3]\n        ir_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_ir' + '.txt'\n        ir_file = os.path.join(here, ir_file)\n        ir_file = open(ir_file, encoding='latin_1')\n        causing = None\n        caused = None\n        data = None\n        regex_vars = re.compile('\\\\w+')\n        regex_vals = re.compile('-?\\\\d+\\\\.\\\\d{4}')\n        line_start_causing = 'time'\n        data_line_indicator = 'point estimate'\n        data_rows_read = 0\n        for line in ir_file:\n            if causing is None and (not line.startswith(line_start_causing)):\n                continue\n            if line.startswith(line_start_causing):\n                line = line[4:]\n                causing = re.findall(regex_vars, line)\n                data = np.empty((21, len(causing)))\n                continue\n            if caused is None:\n                caused = re.findall(regex_vars, line)\n                continue\n            if data_line_indicator not in line:\n                continue\n            start = line.find(data_line_indicator) + len(data_line_indicator)\n            line = line[start:]\n            data[data_rows_read] = re.findall(regex_vals, line)\n            data_rows_read += 1\n        ir_file.close()\n        results['ir'] = data\n        lagorder_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_lagorder' + '.txt'\n        lagorder_file = os.path.join(here, lagorder_file)\n        lagorder_file = open(lagorder_file, encoding='latin_1')\n        results['lagorder'] = dict()\n        aic_start = 'Akaike Info Criterion:'\n        fpe_start = 'Final Prediction Error:'\n        hqic_start = 'Hannan-Quinn Criterion:'\n        bic_start = 'Schwarz Criterion:'\n        for line in lagorder_file:\n            if line.startswith(aic_start):\n                results['lagorder']['aic'] = int(line[len(aic_start):])\n            elif line.startswith(fpe_start):\n                results['lagorder']['fpe'] = int(line[len(fpe_start):])\n            elif line.startswith(hqic_start):\n                results['lagorder']['hqic'] = int(line[len(hqic_start):])\n            elif line.startswith(bic_start):\n                results['lagorder']['bic'] = int(line[len(bic_start):])\n        lagorder_file.close()\n        test_norm_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        test_norm_file = os.path.join(here, test_norm_file)\n        test_norm_file = open(test_norm_file, encoding='latin_1')\n        results['test_norm'] = dict()\n        reading_values = False\n        line_start_statistic = 'joint test statistic:'\n        line_start_pvalue = ' p-value:'\n        for line in test_norm_file:\n            if not reading_values:\n                if 'Introduction to Multiple Time Series Analysis' in line:\n                    reading_values = True\n                continue\n            if 'joint_pvalue' in results['test_norm'].keys():\n                break\n            if line.startswith(line_start_statistic):\n                line_end = line[len(line_start_statistic):]\n                results['test_norm']['joint_test_statistic'] = float(line_end)\n            if line.startswith(line_start_pvalue):\n                line_end = line[len(line_start_pvalue):]\n                results['test_norm']['joint_pvalue'] = float(line_end)\n        test_norm_file.close()\n        whiteness_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        whiteness_file = os.path.join(here, whiteness_file)\n        whiteness_file = open(whiteness_file, encoding='latin_1')\n        results['whiteness'] = dict()\n        section_start_marker = 'PORTMANTEAU TEST'\n        order_start = 'tested order:'\n        statistic_start = 'test statistic:'\n        p_start = ' p-value:'\n        adj_statistic_start = 'adjusted test statistic:'\n        unadjusted_finished = False\n        in_section = False\n        for line in whiteness_file:\n            if not in_section and section_start_marker not in line:\n                continue\n            if not in_section and section_start_marker in line:\n                in_section = True\n                continue\n            if line.startswith(order_start):\n                results['whiteness']['tested order'] = int(line[len(order_start):])\n                continue\n            if line.startswith(statistic_start):\n                results['whiteness']['test statistic'] = float(line[len(statistic_start):])\n                continue\n            if line.startswith(adj_statistic_start):\n                results['whiteness']['test statistic adj.'] = float(line[len(adj_statistic_start):])\n                continue\n            if line.startswith(p_start):\n                if not unadjusted_finished:\n                    results['whiteness']['p-value'] = float(line[len(p_start):])\n                    unadjusted_finished = True\n                else:\n                    results['whiteness']['p-value adjusted'] = float(line[len(p_start):])\n                    break\n        whiteness_file.close()\n        if debug_mode:\n            print_debug_output(results, dt_string)\n        results_dict_per_det_terms[dt_s] = results\n    return results_dict_per_det_terms",
            "def load_results_jmulti(dataset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n\\n    Parameters\\n    ----------\\n    dataset : module\\n        A data module in the statsmodels/datasets directory that defines a\\n        __str__() method returning the dataset\\'s name.\\n    dt_s_list : list\\n        A list of strings where each string represents a combination of\\n        deterministic terms.\\n\\n    Returns\\n    -------\\n    result : dict\\n        A dict (keys: tuples of deterministic terms and seasonal terms)\\n        of dicts (keys: strings \"est\" (for estimators),\\n                              \"se\" (for standard errors),\\n                              \"t\" (for t-values),\\n                              \"p\" (for p-values))\\n        of dicts (keys: strings \"alpha\", \"beta\", \"Gamma\" and other results)\\n    '\n    source = 'jmulti'\n    results_dict_per_det_terms = dict.fromkeys(dataset.dt_s_list)\n    for dt_s in dataset.dt_s_list:\n        dt_string = dt_s_tup_to_string(dt_s)\n        params_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '.txt'\n        params_file = os.path.join(here, params_file)\n        section_header = ['Lagged endogenous term', 'Deterministic term', 'Loading coefficients', 'Estimated cointegration relation', 'Legend', 'Lagged endogenous term', 'Deterministic term']\n        sections = ['Gamma', 'C', 'alpha', 'beta', 'Legend', 'VAR A', 'VAR deterministic']\n        if 'co' not in dt_string and 'lo' not in dt_string and ('s' not in dt_string):\n            del section_header[1]\n            del sections[1]\n            if 'ci' not in dt_string and 'li' not in dt_string:\n                del section_header[-1]\n                del sections[-1]\n        results = dict()\n        results['est'] = dict.fromkeys(sections)\n        results['se'] = dict.fromkeys(sections)\n        results['t'] = dict.fromkeys(sections)\n        results['p'] = dict.fromkeys(sections)\n        section = -1\n        result = []\n        result_se = []\n        result_t = []\n        result_p = []\n        rows = 0\n        started_reading_section = False\n        start_end_mark = '-----'\n        params_file = open(params_file, encoding='latin_1')\n        for line in params_file:\n            if section == -1 and section_header[section + 1] not in line:\n                continue\n            if section < len(section_header) - 1 and section_header[section + 1] in line:\n                section += 1\n                continue\n            if not started_reading_section:\n                if line.startswith(start_end_mark):\n                    started_reading_section = True\n                continue\n            if started_reading_section:\n                if line.startswith(start_end_mark):\n                    if result == []:\n                        started_reading_section = False\n                        continue\n                    results['est'][sections[section]] = np.column_stack(result)\n                    result = []\n                    results['se'][sections[section]] = np.column_stack(result_se)\n                    result_se = []\n                    results['t'][sections[section]] = np.column_stack(result_t)\n                    result_t = []\n                    results['p'][sections[section]] = np.column_stack(result_p)\n                    result_p = []\n                    started_reading_section = False\n                    continue\n                str_number = '-?\\\\d+\\\\.\\\\d{3}'\n                regex_est = re.compile(str_number + '[^\\\\)\\\\]\\\\}]')\n                est_col = re.findall(regex_est, line)\n                regex_se = re.compile('\\\\(' + str_number + '\\\\)')\n                se_col = re.findall(regex_se, line)\n                regex_t_value = re.compile('\\\\[' + str_number + '\\\\]')\n                t_col = re.findall(regex_t_value, line)\n                regex_p_value = re.compile('\\\\{' + str_number + '\\\\}')\n                p_col = re.findall(regex_p_value, line)\n                if result == [] and est_col != []:\n                    rows = len(est_col)\n                if est_col != []:\n                    est_col = [float(el) for el in est_col]\n                    result.append(est_col)\n                elif se_col != []:\n                    for i in range(rows):\n                        se_col[i] = se_col[i].replace('(', '').replace(')', '')\n                    se_col = [float(el) for el in se_col]\n                    result_se.append(se_col)\n                elif t_col != []:\n                    for i in range(rows):\n                        t_col[i] = t_col[i].replace('[', '').replace(']', '')\n                    t_col = [float(el) for el in t_col]\n                    result_t.append(t_col)\n                elif p_col != []:\n                    for i in range(rows):\n                        p_col[i] = p_col[i].replace('{', '').replace('}', '')\n                    p_col = [float(el) for el in p_col]\n                    result_p.append(p_col)\n        params_file.close()\n        del results['est']['Legend']\n        del results['se']['Legend']\n        del results['t']['Legend']\n        del results['p']['Legend']\n        results['est']['beta'] = results['est']['beta'].T\n        results['se']['beta'] = results['se']['beta'].T\n        results['t']['beta'] = results['t']['beta'].T\n        results['p']['beta'] = results['p']['beta'].T\n        alpha = results['est']['alpha']\n        beta = results['est']['beta']\n        alpha_rows = alpha.shape[0]\n        if beta.shape[0] > alpha_rows:\n            (results['est']['beta'], results['est']['det_coint']) = np.vsplit(results['est']['beta'], [alpha_rows])\n            (results['se']['beta'], results['se']['det_coint']) = np.vsplit(results['se']['beta'], [alpha_rows])\n            (results['t']['beta'], results['t']['det_coint']) = np.vsplit(results['t']['beta'], [alpha_rows])\n            (results['p']['beta'], results['p']['det_coint']) = np.vsplit(results['p']['beta'], [alpha_rows])\n        sigmau_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_Sigmau' + '.txt'\n        sigmau_file = os.path.join(here, sigmau_file)\n        rows_to_parse = 0\n        regex_est = re.compile('\\\\s+\\\\S+e\\\\S+')\n        sigmau_section_reached = False\n        sigmau_file = open(sigmau_file, encoding='latin_1')\n        for line in sigmau_file:\n            if line.startswith('Log Likelihood:'):\n                line = line.split('Log Likelihood:')[1]\n                results['log_like'] = float(re.findall(regex_est, line)[0])\n            if not sigmau_section_reached and 'Covariance:' not in line:\n                continue\n            if 'Covariance:' in line:\n                sigmau_section_reached = True\n                row = re.findall(regex_est, line)\n                rows_to_parse = len(row)\n                sigma_u = np.empty((rows_to_parse, rows_to_parse))\n            row = re.findall(regex_est, line)\n            rows_to_parse -= 1\n            sigma_u[rows_to_parse] = row\n            if rows_to_parse == 0:\n                break\n        sigmau_file.close()\n        results['est']['Sigma_u'] = sigma_u[::-1]\n        fc_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_fc5' + '.txt'\n        fc_file = os.path.join(here, fc_file)\n        (fc, lower, upper, plu_min) = ([], [], [], [])\n        fc_file = open(fc_file, encoding='latin_1')\n        for line in fc_file:\n            str_number = '(\\\\s+-?\\\\d+\\\\.\\\\d{4}\\\\s*?)'\n            regex_number = re.compile(str_number)\n            numbers = re.findall(regex_number, line)\n            if numbers == []:\n                continue\n            fc.append(float(numbers[0]))\n            lower.append(float(numbers[1]))\n            upper.append(float(numbers[2]))\n            plu_min.append(float(numbers[3]))\n        fc_file.close()\n        variables = alpha.shape[0]\n        fc = np.hstack(np.vsplit(np.array(fc)[:, None], variables))\n        lower = np.hstack(np.vsplit(np.array(lower)[:, None], variables))\n        upper = np.hstack(np.vsplit(np.array(upper)[:, None], variables))\n        results['fc'] = dict.fromkeys(['fc', 'lower', 'upper'])\n        results['fc']['fc'] = fc\n        results['fc']['lower'] = lower\n        results['fc']['upper'] = upper\n        results['granger_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['granger_caus']['p'] = dict()\n        results['granger_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        for causing in var_combs:\n            caused = tuple((el for el in vn if el not in causing))\n            granger_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_granger_causality_' + stringify_var_names(causing) + '_' + stringify_var_names(caused) + '.txt'\n            granger_file = os.path.join(here, granger_file)\n            granger_file = open(granger_file, encoding='latin_1')\n            granger_results = []\n            for line in granger_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                granger_results.append(number)\n            granger_file.close()\n            results['granger_caus']['test_stat'][causing, caused] = granger_results[0]\n            results['granger_caus']['p'][causing, caused] = granger_results[1]\n        results['inst_caus'] = dict.fromkeys(['p', 'test_stat'])\n        results['inst_caus']['p'] = dict()\n        results['inst_caus']['test_stat'] = dict()\n        vn = dataset.variable_names\n        var_combs = sublists(vn, 1, len(vn) - 1)\n        for causing in var_combs:\n            caused = tuple((el for el in vn if el not in causing))\n            inst_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_inst_causality_' + stringify_var_names(causing) + '_' + stringify_var_names(caused) + '.txt'\n            inst_file = os.path.join(here, inst_file)\n            inst_file = open(inst_file, encoding='latin_1')\n            inst_results = []\n            for line in inst_file:\n                str_number = '\\\\d+\\\\.\\\\d{4}'\n                regex_number = re.compile(str_number)\n                number = re.search(regex_number, line)\n                if number is None:\n                    continue\n                number = float(number.group(0))\n                inst_results.append(number)\n            inst_file.close()\n            results['inst_caus']['test_stat'][causing, caused] = inst_results[2]\n            results['inst_caus']['p'][causing, caused] = inst_results[3]\n        ir_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_ir' + '.txt'\n        ir_file = os.path.join(here, ir_file)\n        ir_file = open(ir_file, encoding='latin_1')\n        causing = None\n        caused = None\n        data = None\n        regex_vars = re.compile('\\\\w+')\n        regex_vals = re.compile('-?\\\\d+\\\\.\\\\d{4}')\n        line_start_causing = 'time'\n        data_line_indicator = 'point estimate'\n        data_rows_read = 0\n        for line in ir_file:\n            if causing is None and (not line.startswith(line_start_causing)):\n                continue\n            if line.startswith(line_start_causing):\n                line = line[4:]\n                causing = re.findall(regex_vars, line)\n                data = np.empty((21, len(causing)))\n                continue\n            if caused is None:\n                caused = re.findall(regex_vars, line)\n                continue\n            if data_line_indicator not in line:\n                continue\n            start = line.find(data_line_indicator) + len(data_line_indicator)\n            line = line[start:]\n            data[data_rows_read] = re.findall(regex_vals, line)\n            data_rows_read += 1\n        ir_file.close()\n        results['ir'] = data\n        lagorder_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_lagorder' + '.txt'\n        lagorder_file = os.path.join(here, lagorder_file)\n        lagorder_file = open(lagorder_file, encoding='latin_1')\n        results['lagorder'] = dict()\n        aic_start = 'Akaike Info Criterion:'\n        fpe_start = 'Final Prediction Error:'\n        hqic_start = 'Hannan-Quinn Criterion:'\n        bic_start = 'Schwarz Criterion:'\n        for line in lagorder_file:\n            if line.startswith(aic_start):\n                results['lagorder']['aic'] = int(line[len(aic_start):])\n            elif line.startswith(fpe_start):\n                results['lagorder']['fpe'] = int(line[len(fpe_start):])\n            elif line.startswith(hqic_start):\n                results['lagorder']['hqic'] = int(line[len(hqic_start):])\n            elif line.startswith(bic_start):\n                results['lagorder']['bic'] = int(line[len(bic_start):])\n        lagorder_file.close()\n        test_norm_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        test_norm_file = os.path.join(here, test_norm_file)\n        test_norm_file = open(test_norm_file, encoding='latin_1')\n        results['test_norm'] = dict()\n        reading_values = False\n        line_start_statistic = 'joint test statistic:'\n        line_start_pvalue = ' p-value:'\n        for line in test_norm_file:\n            if not reading_values:\n                if 'Introduction to Multiple Time Series Analysis' in line:\n                    reading_values = True\n                continue\n            if 'joint_pvalue' in results['test_norm'].keys():\n                break\n            if line.startswith(line_start_statistic):\n                line_end = line[len(line_start_statistic):]\n                results['test_norm']['joint_test_statistic'] = float(line_end)\n            if line.startswith(line_start_pvalue):\n                line_end = line[len(line_start_pvalue):]\n                results['test_norm']['joint_pvalue'] = float(line_end)\n        test_norm_file.close()\n        whiteness_file = 'vecm_' + dataset.__str__() + '_' + source + '_' + dt_string + '_diag' + '.txt'\n        whiteness_file = os.path.join(here, whiteness_file)\n        whiteness_file = open(whiteness_file, encoding='latin_1')\n        results['whiteness'] = dict()\n        section_start_marker = 'PORTMANTEAU TEST'\n        order_start = 'tested order:'\n        statistic_start = 'test statistic:'\n        p_start = ' p-value:'\n        adj_statistic_start = 'adjusted test statistic:'\n        unadjusted_finished = False\n        in_section = False\n        for line in whiteness_file:\n            if not in_section and section_start_marker not in line:\n                continue\n            if not in_section and section_start_marker in line:\n                in_section = True\n                continue\n            if line.startswith(order_start):\n                results['whiteness']['tested order'] = int(line[len(order_start):])\n                continue\n            if line.startswith(statistic_start):\n                results['whiteness']['test statistic'] = float(line[len(statistic_start):])\n                continue\n            if line.startswith(adj_statistic_start):\n                results['whiteness']['test statistic adj.'] = float(line[len(adj_statistic_start):])\n                continue\n            if line.startswith(p_start):\n                if not unadjusted_finished:\n                    results['whiteness']['p-value'] = float(line[len(p_start):])\n                    unadjusted_finished = True\n                else:\n                    results['whiteness']['p-value adjusted'] = float(line[len(p_start):])\n                    break\n        whiteness_file.close()\n        if debug_mode:\n            print_debug_output(results, dt_string)\n        results_dict_per_det_terms[dt_s] = results\n    return results_dict_per_det_terms"
        ]
    }
]