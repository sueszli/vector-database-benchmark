[
    {
        "func_name": "get_owlv2_config",
        "original": "def get_owlv2_config(model_name):\n    if 'large' in model_name:\n        image_size = 1008\n        patch_size = 14\n        vision_hidden_size = 1024\n        vision_intermediate_size = 4096\n        vision_num_hidden_layers = 24\n        vision_num_attention_heads = 16\n        projection_dim = 768\n        text_hidden_size = 768\n        text_intermediate_size = 3072\n        text_num_attention_heads = 12\n        text_num_hidden_layers = 12\n    else:\n        image_size = 960\n        patch_size = 16\n        vision_hidden_size = 768\n        vision_intermediate_size = 3072\n        vision_num_hidden_layers = 12\n        vision_num_attention_heads = 12\n        projection_dim = 512\n        text_hidden_size = 512\n        text_intermediate_size = 2048\n        text_num_attention_heads = 8\n        text_num_hidden_layers = 12\n    vision_config = Owlv2VisionConfig(patch_size=patch_size, image_size=image_size, hidden_size=vision_hidden_size, num_hidden_layers=vision_num_hidden_layers, intermediate_size=vision_intermediate_size, num_attention_heads=vision_num_attention_heads)\n    text_config = Owlv2TextConfig(hidden_size=text_hidden_size, intermediate_size=text_intermediate_size, num_attention_heads=text_num_attention_heads, num_hidden_layers=text_num_hidden_layers)\n    config = Owlv2Config(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), projection_dim=projection_dim)\n    return config",
        "mutated": [
            "def get_owlv2_config(model_name):\n    if False:\n        i = 10\n    if 'large' in model_name:\n        image_size = 1008\n        patch_size = 14\n        vision_hidden_size = 1024\n        vision_intermediate_size = 4096\n        vision_num_hidden_layers = 24\n        vision_num_attention_heads = 16\n        projection_dim = 768\n        text_hidden_size = 768\n        text_intermediate_size = 3072\n        text_num_attention_heads = 12\n        text_num_hidden_layers = 12\n    else:\n        image_size = 960\n        patch_size = 16\n        vision_hidden_size = 768\n        vision_intermediate_size = 3072\n        vision_num_hidden_layers = 12\n        vision_num_attention_heads = 12\n        projection_dim = 512\n        text_hidden_size = 512\n        text_intermediate_size = 2048\n        text_num_attention_heads = 8\n        text_num_hidden_layers = 12\n    vision_config = Owlv2VisionConfig(patch_size=patch_size, image_size=image_size, hidden_size=vision_hidden_size, num_hidden_layers=vision_num_hidden_layers, intermediate_size=vision_intermediate_size, num_attention_heads=vision_num_attention_heads)\n    text_config = Owlv2TextConfig(hidden_size=text_hidden_size, intermediate_size=text_intermediate_size, num_attention_heads=text_num_attention_heads, num_hidden_layers=text_num_hidden_layers)\n    config = Owlv2Config(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), projection_dim=projection_dim)\n    return config",
            "def get_owlv2_config(model_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if 'large' in model_name:\n        image_size = 1008\n        patch_size = 14\n        vision_hidden_size = 1024\n        vision_intermediate_size = 4096\n        vision_num_hidden_layers = 24\n        vision_num_attention_heads = 16\n        projection_dim = 768\n        text_hidden_size = 768\n        text_intermediate_size = 3072\n        text_num_attention_heads = 12\n        text_num_hidden_layers = 12\n    else:\n        image_size = 960\n        patch_size = 16\n        vision_hidden_size = 768\n        vision_intermediate_size = 3072\n        vision_num_hidden_layers = 12\n        vision_num_attention_heads = 12\n        projection_dim = 512\n        text_hidden_size = 512\n        text_intermediate_size = 2048\n        text_num_attention_heads = 8\n        text_num_hidden_layers = 12\n    vision_config = Owlv2VisionConfig(patch_size=patch_size, image_size=image_size, hidden_size=vision_hidden_size, num_hidden_layers=vision_num_hidden_layers, intermediate_size=vision_intermediate_size, num_attention_heads=vision_num_attention_heads)\n    text_config = Owlv2TextConfig(hidden_size=text_hidden_size, intermediate_size=text_intermediate_size, num_attention_heads=text_num_attention_heads, num_hidden_layers=text_num_hidden_layers)\n    config = Owlv2Config(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), projection_dim=projection_dim)\n    return config",
            "def get_owlv2_config(model_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if 'large' in model_name:\n        image_size = 1008\n        patch_size = 14\n        vision_hidden_size = 1024\n        vision_intermediate_size = 4096\n        vision_num_hidden_layers = 24\n        vision_num_attention_heads = 16\n        projection_dim = 768\n        text_hidden_size = 768\n        text_intermediate_size = 3072\n        text_num_attention_heads = 12\n        text_num_hidden_layers = 12\n    else:\n        image_size = 960\n        patch_size = 16\n        vision_hidden_size = 768\n        vision_intermediate_size = 3072\n        vision_num_hidden_layers = 12\n        vision_num_attention_heads = 12\n        projection_dim = 512\n        text_hidden_size = 512\n        text_intermediate_size = 2048\n        text_num_attention_heads = 8\n        text_num_hidden_layers = 12\n    vision_config = Owlv2VisionConfig(patch_size=patch_size, image_size=image_size, hidden_size=vision_hidden_size, num_hidden_layers=vision_num_hidden_layers, intermediate_size=vision_intermediate_size, num_attention_heads=vision_num_attention_heads)\n    text_config = Owlv2TextConfig(hidden_size=text_hidden_size, intermediate_size=text_intermediate_size, num_attention_heads=text_num_attention_heads, num_hidden_layers=text_num_hidden_layers)\n    config = Owlv2Config(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), projection_dim=projection_dim)\n    return config",
            "def get_owlv2_config(model_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if 'large' in model_name:\n        image_size = 1008\n        patch_size = 14\n        vision_hidden_size = 1024\n        vision_intermediate_size = 4096\n        vision_num_hidden_layers = 24\n        vision_num_attention_heads = 16\n        projection_dim = 768\n        text_hidden_size = 768\n        text_intermediate_size = 3072\n        text_num_attention_heads = 12\n        text_num_hidden_layers = 12\n    else:\n        image_size = 960\n        patch_size = 16\n        vision_hidden_size = 768\n        vision_intermediate_size = 3072\n        vision_num_hidden_layers = 12\n        vision_num_attention_heads = 12\n        projection_dim = 512\n        text_hidden_size = 512\n        text_intermediate_size = 2048\n        text_num_attention_heads = 8\n        text_num_hidden_layers = 12\n    vision_config = Owlv2VisionConfig(patch_size=patch_size, image_size=image_size, hidden_size=vision_hidden_size, num_hidden_layers=vision_num_hidden_layers, intermediate_size=vision_intermediate_size, num_attention_heads=vision_num_attention_heads)\n    text_config = Owlv2TextConfig(hidden_size=text_hidden_size, intermediate_size=text_intermediate_size, num_attention_heads=text_num_attention_heads, num_hidden_layers=text_num_hidden_layers)\n    config = Owlv2Config(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), projection_dim=projection_dim)\n    return config",
            "def get_owlv2_config(model_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if 'large' in model_name:\n        image_size = 1008\n        patch_size = 14\n        vision_hidden_size = 1024\n        vision_intermediate_size = 4096\n        vision_num_hidden_layers = 24\n        vision_num_attention_heads = 16\n        projection_dim = 768\n        text_hidden_size = 768\n        text_intermediate_size = 3072\n        text_num_attention_heads = 12\n        text_num_hidden_layers = 12\n    else:\n        image_size = 960\n        patch_size = 16\n        vision_hidden_size = 768\n        vision_intermediate_size = 3072\n        vision_num_hidden_layers = 12\n        vision_num_attention_heads = 12\n        projection_dim = 512\n        text_hidden_size = 512\n        text_intermediate_size = 2048\n        text_num_attention_heads = 8\n        text_num_hidden_layers = 12\n    vision_config = Owlv2VisionConfig(patch_size=patch_size, image_size=image_size, hidden_size=vision_hidden_size, num_hidden_layers=vision_num_hidden_layers, intermediate_size=vision_intermediate_size, num_attention_heads=vision_num_attention_heads)\n    text_config = Owlv2TextConfig(hidden_size=text_hidden_size, intermediate_size=text_intermediate_size, num_attention_heads=text_num_attention_heads, num_hidden_layers=text_num_hidden_layers)\n    config = Owlv2Config(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), projection_dim=projection_dim)\n    return config"
        ]
    },
    {
        "func_name": "flatten_nested_dict",
        "original": "def flatten_nested_dict(params, parent_key='', sep='/'):\n    items = []\n    for (k, v) in params.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten_nested_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)",
        "mutated": [
            "def flatten_nested_dict(params, parent_key='', sep='/'):\n    if False:\n        i = 10\n    items = []\n    for (k, v) in params.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten_nested_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)",
            "def flatten_nested_dict(params, parent_key='', sep='/'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    items = []\n    for (k, v) in params.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten_nested_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)",
            "def flatten_nested_dict(params, parent_key='', sep='/'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    items = []\n    for (k, v) in params.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten_nested_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)",
            "def flatten_nested_dict(params, parent_key='', sep='/'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    items = []\n    for (k, v) in params.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten_nested_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)",
            "def flatten_nested_dict(params, parent_key='', sep='/'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    items = []\n    for (k, v) in params.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten_nested_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)"
        ]
    },
    {
        "func_name": "create_rename_keys",
        "original": "def create_rename_keys(config, model_name):\n    rename_keys = []\n    rename_keys.append(('backbone/clip/visual/class_embedding', 'owlv2.vision_model.embeddings.class_embedding'))\n    rename_keys.append(('backbone/clip/visual/conv1/kernel', 'owlv2.vision_model.embeddings.patch_embedding.weight'))\n    rename_keys.append(('backbone/clip/visual/positional_embedding', 'owlv2.vision_model.embeddings.position_embedding.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_pre/scale', 'owlv2.vision_model.pre_layernorm.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_pre/bias', 'owlv2.vision_model.pre_layernorm.bias'))\n    for i in range(config.vision_config.num_hidden_layers):\n        if 'v2' in model_name:\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_0/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_0/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.bias'))\n        else:\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_2/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_2/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_fc/kernel', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc1.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_fc/bias', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc1.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_proj/kernel', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc2.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_proj/bias', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc2.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/query/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.q_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/query/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.q_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/key/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.k_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/key/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.k_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/value/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.v_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/value/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.v_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/out/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.out_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/out/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.out_proj.bias'))\n    rename_keys.append(('backbone/clip/visual/ln_post/scale', 'owlv2.vision_model.post_layernorm.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_post/bias', 'owlv2.vision_model.post_layernorm.bias'))\n    rename_keys.append(('backbone/clip/text/token_embedding/embedding', 'owlv2.text_model.embeddings.token_embedding.weight'))\n    rename_keys.append(('backbone/clip/text/positional_embedding', 'owlv2.text_model.embeddings.position_embedding.weight'))\n    for i in range(config.text_config.num_hidden_layers):\n        if 'v2' in model_name:\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_0/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_0/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.bias'))\n        else:\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_2/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_2/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_fc/kernel', f'owlv2.text_model.encoder.layers.{i}.mlp.fc1.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_fc/bias', f'owlv2.text_model.encoder.layers.{i}.mlp.fc1.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_proj/kernel', f'owlv2.text_model.encoder.layers.{i}.mlp.fc2.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_proj/bias', f'owlv2.text_model.encoder.layers.{i}.mlp.fc2.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/query/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.q_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/query/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.q_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/key/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.k_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/key/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.k_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/value/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.v_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/value/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.v_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/out/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.out_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/out/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.out_proj.bias'))\n    rename_keys.append(('backbone/clip/text/ln_final/scale', 'owlv2.text_model.final_layer_norm.weight'))\n    rename_keys.append(('backbone/clip/text/ln_final/bias', 'owlv2.text_model.final_layer_norm.bias'))\n    rename_keys.append(('backbone/clip/logit_scale', 'owlv2.logit_scale'))\n    rename_keys.append(('backbone/clip/text/text_projection/kernel', 'owlv2.text_projection.weight'))\n    rename_keys.append(('backbone/merged_class_token/scale', 'layer_norm.weight'))\n    rename_keys.append(('backbone/merged_class_token/bias', 'layer_norm.bias'))\n    rename_keys.append(('class_head/Dense_0/kernel', 'class_head.dense0.weight'))\n    rename_keys.append(('class_head/Dense_0/bias', 'class_head.dense0.bias'))\n    rename_keys.append(('class_head/logit_shift/kernel', 'class_head.logit_shift.weight'))\n    rename_keys.append(('class_head/logit_scale/kernel', 'class_head.logit_scale.weight'))\n    rename_keys.append(('class_head/logit_scale/bias', 'class_head.logit_scale.bias'))\n    rename_keys.append(('class_head/logit_shift/bias', 'class_head.logit_shift.bias'))\n    rename_keys.append(('obj_box_head/Dense_0/kernel', 'box_head.dense0.weight'))\n    rename_keys.append(('obj_box_head/Dense_0/bias', 'box_head.dense0.bias'))\n    rename_keys.append(('obj_box_head/Dense_1/kernel', 'box_head.dense1.weight'))\n    rename_keys.append(('obj_box_head/Dense_1/bias', 'box_head.dense1.bias'))\n    rename_keys.append(('obj_box_head/Dense_2/kernel', 'box_head.dense2.weight'))\n    rename_keys.append(('obj_box_head/Dense_2/bias', 'box_head.dense2.bias'))\n    if 'v2' in model_name:\n        rename_keys.append(('objectness_head/Dense_0/kernel', 'objectness_head.dense0.weight'))\n        rename_keys.append(('objectness_head/Dense_0/bias', 'objectness_head.dense0.bias'))\n        rename_keys.append(('objectness_head/Dense_1/kernel', 'objectness_head.dense1.weight'))\n        rename_keys.append(('objectness_head/Dense_1/bias', 'objectness_head.dense1.bias'))\n        rename_keys.append(('objectness_head/Dense_2/kernel', 'objectness_head.dense2.weight'))\n        rename_keys.append(('objectness_head/Dense_2/bias', 'objectness_head.dense2.bias'))\n    return rename_keys",
        "mutated": [
            "def create_rename_keys(config, model_name):\n    if False:\n        i = 10\n    rename_keys = []\n    rename_keys.append(('backbone/clip/visual/class_embedding', 'owlv2.vision_model.embeddings.class_embedding'))\n    rename_keys.append(('backbone/clip/visual/conv1/kernel', 'owlv2.vision_model.embeddings.patch_embedding.weight'))\n    rename_keys.append(('backbone/clip/visual/positional_embedding', 'owlv2.vision_model.embeddings.position_embedding.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_pre/scale', 'owlv2.vision_model.pre_layernorm.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_pre/bias', 'owlv2.vision_model.pre_layernorm.bias'))\n    for i in range(config.vision_config.num_hidden_layers):\n        if 'v2' in model_name:\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_0/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_0/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.bias'))\n        else:\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_2/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_2/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_fc/kernel', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc1.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_fc/bias', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc1.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_proj/kernel', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc2.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_proj/bias', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc2.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/query/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.q_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/query/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.q_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/key/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.k_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/key/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.k_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/value/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.v_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/value/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.v_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/out/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.out_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/out/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.out_proj.bias'))\n    rename_keys.append(('backbone/clip/visual/ln_post/scale', 'owlv2.vision_model.post_layernorm.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_post/bias', 'owlv2.vision_model.post_layernorm.bias'))\n    rename_keys.append(('backbone/clip/text/token_embedding/embedding', 'owlv2.text_model.embeddings.token_embedding.weight'))\n    rename_keys.append(('backbone/clip/text/positional_embedding', 'owlv2.text_model.embeddings.position_embedding.weight'))\n    for i in range(config.text_config.num_hidden_layers):\n        if 'v2' in model_name:\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_0/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_0/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.bias'))\n        else:\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_2/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_2/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_fc/kernel', f'owlv2.text_model.encoder.layers.{i}.mlp.fc1.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_fc/bias', f'owlv2.text_model.encoder.layers.{i}.mlp.fc1.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_proj/kernel', f'owlv2.text_model.encoder.layers.{i}.mlp.fc2.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_proj/bias', f'owlv2.text_model.encoder.layers.{i}.mlp.fc2.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/query/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.q_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/query/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.q_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/key/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.k_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/key/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.k_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/value/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.v_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/value/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.v_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/out/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.out_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/out/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.out_proj.bias'))\n    rename_keys.append(('backbone/clip/text/ln_final/scale', 'owlv2.text_model.final_layer_norm.weight'))\n    rename_keys.append(('backbone/clip/text/ln_final/bias', 'owlv2.text_model.final_layer_norm.bias'))\n    rename_keys.append(('backbone/clip/logit_scale', 'owlv2.logit_scale'))\n    rename_keys.append(('backbone/clip/text/text_projection/kernel', 'owlv2.text_projection.weight'))\n    rename_keys.append(('backbone/merged_class_token/scale', 'layer_norm.weight'))\n    rename_keys.append(('backbone/merged_class_token/bias', 'layer_norm.bias'))\n    rename_keys.append(('class_head/Dense_0/kernel', 'class_head.dense0.weight'))\n    rename_keys.append(('class_head/Dense_0/bias', 'class_head.dense0.bias'))\n    rename_keys.append(('class_head/logit_shift/kernel', 'class_head.logit_shift.weight'))\n    rename_keys.append(('class_head/logit_scale/kernel', 'class_head.logit_scale.weight'))\n    rename_keys.append(('class_head/logit_scale/bias', 'class_head.logit_scale.bias'))\n    rename_keys.append(('class_head/logit_shift/bias', 'class_head.logit_shift.bias'))\n    rename_keys.append(('obj_box_head/Dense_0/kernel', 'box_head.dense0.weight'))\n    rename_keys.append(('obj_box_head/Dense_0/bias', 'box_head.dense0.bias'))\n    rename_keys.append(('obj_box_head/Dense_1/kernel', 'box_head.dense1.weight'))\n    rename_keys.append(('obj_box_head/Dense_1/bias', 'box_head.dense1.bias'))\n    rename_keys.append(('obj_box_head/Dense_2/kernel', 'box_head.dense2.weight'))\n    rename_keys.append(('obj_box_head/Dense_2/bias', 'box_head.dense2.bias'))\n    if 'v2' in model_name:\n        rename_keys.append(('objectness_head/Dense_0/kernel', 'objectness_head.dense0.weight'))\n        rename_keys.append(('objectness_head/Dense_0/bias', 'objectness_head.dense0.bias'))\n        rename_keys.append(('objectness_head/Dense_1/kernel', 'objectness_head.dense1.weight'))\n        rename_keys.append(('objectness_head/Dense_1/bias', 'objectness_head.dense1.bias'))\n        rename_keys.append(('objectness_head/Dense_2/kernel', 'objectness_head.dense2.weight'))\n        rename_keys.append(('objectness_head/Dense_2/bias', 'objectness_head.dense2.bias'))\n    return rename_keys",
            "def create_rename_keys(config, model_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    rename_keys = []\n    rename_keys.append(('backbone/clip/visual/class_embedding', 'owlv2.vision_model.embeddings.class_embedding'))\n    rename_keys.append(('backbone/clip/visual/conv1/kernel', 'owlv2.vision_model.embeddings.patch_embedding.weight'))\n    rename_keys.append(('backbone/clip/visual/positional_embedding', 'owlv2.vision_model.embeddings.position_embedding.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_pre/scale', 'owlv2.vision_model.pre_layernorm.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_pre/bias', 'owlv2.vision_model.pre_layernorm.bias'))\n    for i in range(config.vision_config.num_hidden_layers):\n        if 'v2' in model_name:\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_0/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_0/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.bias'))\n        else:\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_2/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_2/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_fc/kernel', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc1.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_fc/bias', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc1.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_proj/kernel', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc2.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_proj/bias', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc2.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/query/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.q_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/query/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.q_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/key/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.k_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/key/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.k_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/value/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.v_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/value/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.v_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/out/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.out_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/out/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.out_proj.bias'))\n    rename_keys.append(('backbone/clip/visual/ln_post/scale', 'owlv2.vision_model.post_layernorm.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_post/bias', 'owlv2.vision_model.post_layernorm.bias'))\n    rename_keys.append(('backbone/clip/text/token_embedding/embedding', 'owlv2.text_model.embeddings.token_embedding.weight'))\n    rename_keys.append(('backbone/clip/text/positional_embedding', 'owlv2.text_model.embeddings.position_embedding.weight'))\n    for i in range(config.text_config.num_hidden_layers):\n        if 'v2' in model_name:\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_0/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_0/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.bias'))\n        else:\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_2/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_2/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_fc/kernel', f'owlv2.text_model.encoder.layers.{i}.mlp.fc1.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_fc/bias', f'owlv2.text_model.encoder.layers.{i}.mlp.fc1.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_proj/kernel', f'owlv2.text_model.encoder.layers.{i}.mlp.fc2.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_proj/bias', f'owlv2.text_model.encoder.layers.{i}.mlp.fc2.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/query/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.q_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/query/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.q_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/key/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.k_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/key/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.k_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/value/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.v_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/value/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.v_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/out/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.out_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/out/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.out_proj.bias'))\n    rename_keys.append(('backbone/clip/text/ln_final/scale', 'owlv2.text_model.final_layer_norm.weight'))\n    rename_keys.append(('backbone/clip/text/ln_final/bias', 'owlv2.text_model.final_layer_norm.bias'))\n    rename_keys.append(('backbone/clip/logit_scale', 'owlv2.logit_scale'))\n    rename_keys.append(('backbone/clip/text/text_projection/kernel', 'owlv2.text_projection.weight'))\n    rename_keys.append(('backbone/merged_class_token/scale', 'layer_norm.weight'))\n    rename_keys.append(('backbone/merged_class_token/bias', 'layer_norm.bias'))\n    rename_keys.append(('class_head/Dense_0/kernel', 'class_head.dense0.weight'))\n    rename_keys.append(('class_head/Dense_0/bias', 'class_head.dense0.bias'))\n    rename_keys.append(('class_head/logit_shift/kernel', 'class_head.logit_shift.weight'))\n    rename_keys.append(('class_head/logit_scale/kernel', 'class_head.logit_scale.weight'))\n    rename_keys.append(('class_head/logit_scale/bias', 'class_head.logit_scale.bias'))\n    rename_keys.append(('class_head/logit_shift/bias', 'class_head.logit_shift.bias'))\n    rename_keys.append(('obj_box_head/Dense_0/kernel', 'box_head.dense0.weight'))\n    rename_keys.append(('obj_box_head/Dense_0/bias', 'box_head.dense0.bias'))\n    rename_keys.append(('obj_box_head/Dense_1/kernel', 'box_head.dense1.weight'))\n    rename_keys.append(('obj_box_head/Dense_1/bias', 'box_head.dense1.bias'))\n    rename_keys.append(('obj_box_head/Dense_2/kernel', 'box_head.dense2.weight'))\n    rename_keys.append(('obj_box_head/Dense_2/bias', 'box_head.dense2.bias'))\n    if 'v2' in model_name:\n        rename_keys.append(('objectness_head/Dense_0/kernel', 'objectness_head.dense0.weight'))\n        rename_keys.append(('objectness_head/Dense_0/bias', 'objectness_head.dense0.bias'))\n        rename_keys.append(('objectness_head/Dense_1/kernel', 'objectness_head.dense1.weight'))\n        rename_keys.append(('objectness_head/Dense_1/bias', 'objectness_head.dense1.bias'))\n        rename_keys.append(('objectness_head/Dense_2/kernel', 'objectness_head.dense2.weight'))\n        rename_keys.append(('objectness_head/Dense_2/bias', 'objectness_head.dense2.bias'))\n    return rename_keys",
            "def create_rename_keys(config, model_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    rename_keys = []\n    rename_keys.append(('backbone/clip/visual/class_embedding', 'owlv2.vision_model.embeddings.class_embedding'))\n    rename_keys.append(('backbone/clip/visual/conv1/kernel', 'owlv2.vision_model.embeddings.patch_embedding.weight'))\n    rename_keys.append(('backbone/clip/visual/positional_embedding', 'owlv2.vision_model.embeddings.position_embedding.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_pre/scale', 'owlv2.vision_model.pre_layernorm.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_pre/bias', 'owlv2.vision_model.pre_layernorm.bias'))\n    for i in range(config.vision_config.num_hidden_layers):\n        if 'v2' in model_name:\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_0/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_0/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.bias'))\n        else:\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_2/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_2/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_fc/kernel', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc1.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_fc/bias', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc1.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_proj/kernel', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc2.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_proj/bias', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc2.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/query/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.q_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/query/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.q_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/key/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.k_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/key/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.k_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/value/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.v_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/value/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.v_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/out/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.out_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/out/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.out_proj.bias'))\n    rename_keys.append(('backbone/clip/visual/ln_post/scale', 'owlv2.vision_model.post_layernorm.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_post/bias', 'owlv2.vision_model.post_layernorm.bias'))\n    rename_keys.append(('backbone/clip/text/token_embedding/embedding', 'owlv2.text_model.embeddings.token_embedding.weight'))\n    rename_keys.append(('backbone/clip/text/positional_embedding', 'owlv2.text_model.embeddings.position_embedding.weight'))\n    for i in range(config.text_config.num_hidden_layers):\n        if 'v2' in model_name:\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_0/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_0/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.bias'))\n        else:\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_2/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_2/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_fc/kernel', f'owlv2.text_model.encoder.layers.{i}.mlp.fc1.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_fc/bias', f'owlv2.text_model.encoder.layers.{i}.mlp.fc1.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_proj/kernel', f'owlv2.text_model.encoder.layers.{i}.mlp.fc2.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_proj/bias', f'owlv2.text_model.encoder.layers.{i}.mlp.fc2.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/query/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.q_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/query/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.q_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/key/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.k_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/key/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.k_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/value/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.v_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/value/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.v_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/out/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.out_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/out/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.out_proj.bias'))\n    rename_keys.append(('backbone/clip/text/ln_final/scale', 'owlv2.text_model.final_layer_norm.weight'))\n    rename_keys.append(('backbone/clip/text/ln_final/bias', 'owlv2.text_model.final_layer_norm.bias'))\n    rename_keys.append(('backbone/clip/logit_scale', 'owlv2.logit_scale'))\n    rename_keys.append(('backbone/clip/text/text_projection/kernel', 'owlv2.text_projection.weight'))\n    rename_keys.append(('backbone/merged_class_token/scale', 'layer_norm.weight'))\n    rename_keys.append(('backbone/merged_class_token/bias', 'layer_norm.bias'))\n    rename_keys.append(('class_head/Dense_0/kernel', 'class_head.dense0.weight'))\n    rename_keys.append(('class_head/Dense_0/bias', 'class_head.dense0.bias'))\n    rename_keys.append(('class_head/logit_shift/kernel', 'class_head.logit_shift.weight'))\n    rename_keys.append(('class_head/logit_scale/kernel', 'class_head.logit_scale.weight'))\n    rename_keys.append(('class_head/logit_scale/bias', 'class_head.logit_scale.bias'))\n    rename_keys.append(('class_head/logit_shift/bias', 'class_head.logit_shift.bias'))\n    rename_keys.append(('obj_box_head/Dense_0/kernel', 'box_head.dense0.weight'))\n    rename_keys.append(('obj_box_head/Dense_0/bias', 'box_head.dense0.bias'))\n    rename_keys.append(('obj_box_head/Dense_1/kernel', 'box_head.dense1.weight'))\n    rename_keys.append(('obj_box_head/Dense_1/bias', 'box_head.dense1.bias'))\n    rename_keys.append(('obj_box_head/Dense_2/kernel', 'box_head.dense2.weight'))\n    rename_keys.append(('obj_box_head/Dense_2/bias', 'box_head.dense2.bias'))\n    if 'v2' in model_name:\n        rename_keys.append(('objectness_head/Dense_0/kernel', 'objectness_head.dense0.weight'))\n        rename_keys.append(('objectness_head/Dense_0/bias', 'objectness_head.dense0.bias'))\n        rename_keys.append(('objectness_head/Dense_1/kernel', 'objectness_head.dense1.weight'))\n        rename_keys.append(('objectness_head/Dense_1/bias', 'objectness_head.dense1.bias'))\n        rename_keys.append(('objectness_head/Dense_2/kernel', 'objectness_head.dense2.weight'))\n        rename_keys.append(('objectness_head/Dense_2/bias', 'objectness_head.dense2.bias'))\n    return rename_keys",
            "def create_rename_keys(config, model_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    rename_keys = []\n    rename_keys.append(('backbone/clip/visual/class_embedding', 'owlv2.vision_model.embeddings.class_embedding'))\n    rename_keys.append(('backbone/clip/visual/conv1/kernel', 'owlv2.vision_model.embeddings.patch_embedding.weight'))\n    rename_keys.append(('backbone/clip/visual/positional_embedding', 'owlv2.vision_model.embeddings.position_embedding.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_pre/scale', 'owlv2.vision_model.pre_layernorm.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_pre/bias', 'owlv2.vision_model.pre_layernorm.bias'))\n    for i in range(config.vision_config.num_hidden_layers):\n        if 'v2' in model_name:\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_0/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_0/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.bias'))\n        else:\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_2/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_2/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_fc/kernel', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc1.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_fc/bias', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc1.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_proj/kernel', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc2.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_proj/bias', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc2.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/query/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.q_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/query/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.q_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/key/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.k_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/key/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.k_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/value/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.v_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/value/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.v_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/out/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.out_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/out/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.out_proj.bias'))\n    rename_keys.append(('backbone/clip/visual/ln_post/scale', 'owlv2.vision_model.post_layernorm.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_post/bias', 'owlv2.vision_model.post_layernorm.bias'))\n    rename_keys.append(('backbone/clip/text/token_embedding/embedding', 'owlv2.text_model.embeddings.token_embedding.weight'))\n    rename_keys.append(('backbone/clip/text/positional_embedding', 'owlv2.text_model.embeddings.position_embedding.weight'))\n    for i in range(config.text_config.num_hidden_layers):\n        if 'v2' in model_name:\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_0/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_0/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.bias'))\n        else:\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_2/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_2/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_fc/kernel', f'owlv2.text_model.encoder.layers.{i}.mlp.fc1.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_fc/bias', f'owlv2.text_model.encoder.layers.{i}.mlp.fc1.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_proj/kernel', f'owlv2.text_model.encoder.layers.{i}.mlp.fc2.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_proj/bias', f'owlv2.text_model.encoder.layers.{i}.mlp.fc2.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/query/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.q_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/query/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.q_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/key/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.k_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/key/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.k_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/value/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.v_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/value/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.v_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/out/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.out_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/out/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.out_proj.bias'))\n    rename_keys.append(('backbone/clip/text/ln_final/scale', 'owlv2.text_model.final_layer_norm.weight'))\n    rename_keys.append(('backbone/clip/text/ln_final/bias', 'owlv2.text_model.final_layer_norm.bias'))\n    rename_keys.append(('backbone/clip/logit_scale', 'owlv2.logit_scale'))\n    rename_keys.append(('backbone/clip/text/text_projection/kernel', 'owlv2.text_projection.weight'))\n    rename_keys.append(('backbone/merged_class_token/scale', 'layer_norm.weight'))\n    rename_keys.append(('backbone/merged_class_token/bias', 'layer_norm.bias'))\n    rename_keys.append(('class_head/Dense_0/kernel', 'class_head.dense0.weight'))\n    rename_keys.append(('class_head/Dense_0/bias', 'class_head.dense0.bias'))\n    rename_keys.append(('class_head/logit_shift/kernel', 'class_head.logit_shift.weight'))\n    rename_keys.append(('class_head/logit_scale/kernel', 'class_head.logit_scale.weight'))\n    rename_keys.append(('class_head/logit_scale/bias', 'class_head.logit_scale.bias'))\n    rename_keys.append(('class_head/logit_shift/bias', 'class_head.logit_shift.bias'))\n    rename_keys.append(('obj_box_head/Dense_0/kernel', 'box_head.dense0.weight'))\n    rename_keys.append(('obj_box_head/Dense_0/bias', 'box_head.dense0.bias'))\n    rename_keys.append(('obj_box_head/Dense_1/kernel', 'box_head.dense1.weight'))\n    rename_keys.append(('obj_box_head/Dense_1/bias', 'box_head.dense1.bias'))\n    rename_keys.append(('obj_box_head/Dense_2/kernel', 'box_head.dense2.weight'))\n    rename_keys.append(('obj_box_head/Dense_2/bias', 'box_head.dense2.bias'))\n    if 'v2' in model_name:\n        rename_keys.append(('objectness_head/Dense_0/kernel', 'objectness_head.dense0.weight'))\n        rename_keys.append(('objectness_head/Dense_0/bias', 'objectness_head.dense0.bias'))\n        rename_keys.append(('objectness_head/Dense_1/kernel', 'objectness_head.dense1.weight'))\n        rename_keys.append(('objectness_head/Dense_1/bias', 'objectness_head.dense1.bias'))\n        rename_keys.append(('objectness_head/Dense_2/kernel', 'objectness_head.dense2.weight'))\n        rename_keys.append(('objectness_head/Dense_2/bias', 'objectness_head.dense2.bias'))\n    return rename_keys",
            "def create_rename_keys(config, model_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    rename_keys = []\n    rename_keys.append(('backbone/clip/visual/class_embedding', 'owlv2.vision_model.embeddings.class_embedding'))\n    rename_keys.append(('backbone/clip/visual/conv1/kernel', 'owlv2.vision_model.embeddings.patch_embedding.weight'))\n    rename_keys.append(('backbone/clip/visual/positional_embedding', 'owlv2.vision_model.embeddings.position_embedding.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_pre/scale', 'owlv2.vision_model.pre_layernorm.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_pre/bias', 'owlv2.vision_model.pre_layernorm.bias'))\n    for i in range(config.vision_config.num_hidden_layers):\n        if 'v2' in model_name:\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_0/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_0/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.bias'))\n        else:\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_1/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_2/scale', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/ln_2/bias', f'owlv2.vision_model.encoder.layers.{i}.layer_norm2.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_fc/kernel', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc1.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_fc/bias', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc1.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_proj/kernel', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc2.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/mlp/c_proj/bias', f'owlv2.vision_model.encoder.layers.{i}.mlp.fc2.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/query/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.q_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/query/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.q_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/key/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.k_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/key/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.k_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/value/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.v_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/value/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.v_proj.bias'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/out/kernel', f'owlv2.vision_model.encoder.layers.{i}.self_attn.out_proj.weight'))\n        rename_keys.append((f'backbone/clip/visual/transformer/resblocks.{i}/attn/out/bias', f'owlv2.vision_model.encoder.layers.{i}.self_attn.out_proj.bias'))\n    rename_keys.append(('backbone/clip/visual/ln_post/scale', 'owlv2.vision_model.post_layernorm.weight'))\n    rename_keys.append(('backbone/clip/visual/ln_post/bias', 'owlv2.vision_model.post_layernorm.bias'))\n    rename_keys.append(('backbone/clip/text/token_embedding/embedding', 'owlv2.text_model.embeddings.token_embedding.weight'))\n    rename_keys.append(('backbone/clip/text/positional_embedding', 'owlv2.text_model.embeddings.position_embedding.weight'))\n    for i in range(config.text_config.num_hidden_layers):\n        if 'v2' in model_name:\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_0/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_0/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.bias'))\n        else:\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_1/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm1.bias'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_2/scale', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.weight'))\n            rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/ln_2/bias', f'owlv2.text_model.encoder.layers.{i}.layer_norm2.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_fc/kernel', f'owlv2.text_model.encoder.layers.{i}.mlp.fc1.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_fc/bias', f'owlv2.text_model.encoder.layers.{i}.mlp.fc1.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_proj/kernel', f'owlv2.text_model.encoder.layers.{i}.mlp.fc2.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/mlp/c_proj/bias', f'owlv2.text_model.encoder.layers.{i}.mlp.fc2.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/query/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.q_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/query/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.q_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/key/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.k_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/key/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.k_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/value/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.v_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/value/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.v_proj.bias'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/out/kernel', f'owlv2.text_model.encoder.layers.{i}.self_attn.out_proj.weight'))\n        rename_keys.append((f'backbone/clip/text/transformer/resblocks.{i}/attn/out/bias', f'owlv2.text_model.encoder.layers.{i}.self_attn.out_proj.bias'))\n    rename_keys.append(('backbone/clip/text/ln_final/scale', 'owlv2.text_model.final_layer_norm.weight'))\n    rename_keys.append(('backbone/clip/text/ln_final/bias', 'owlv2.text_model.final_layer_norm.bias'))\n    rename_keys.append(('backbone/clip/logit_scale', 'owlv2.logit_scale'))\n    rename_keys.append(('backbone/clip/text/text_projection/kernel', 'owlv2.text_projection.weight'))\n    rename_keys.append(('backbone/merged_class_token/scale', 'layer_norm.weight'))\n    rename_keys.append(('backbone/merged_class_token/bias', 'layer_norm.bias'))\n    rename_keys.append(('class_head/Dense_0/kernel', 'class_head.dense0.weight'))\n    rename_keys.append(('class_head/Dense_0/bias', 'class_head.dense0.bias'))\n    rename_keys.append(('class_head/logit_shift/kernel', 'class_head.logit_shift.weight'))\n    rename_keys.append(('class_head/logit_scale/kernel', 'class_head.logit_scale.weight'))\n    rename_keys.append(('class_head/logit_scale/bias', 'class_head.logit_scale.bias'))\n    rename_keys.append(('class_head/logit_shift/bias', 'class_head.logit_shift.bias'))\n    rename_keys.append(('obj_box_head/Dense_0/kernel', 'box_head.dense0.weight'))\n    rename_keys.append(('obj_box_head/Dense_0/bias', 'box_head.dense0.bias'))\n    rename_keys.append(('obj_box_head/Dense_1/kernel', 'box_head.dense1.weight'))\n    rename_keys.append(('obj_box_head/Dense_1/bias', 'box_head.dense1.bias'))\n    rename_keys.append(('obj_box_head/Dense_2/kernel', 'box_head.dense2.weight'))\n    rename_keys.append(('obj_box_head/Dense_2/bias', 'box_head.dense2.bias'))\n    if 'v2' in model_name:\n        rename_keys.append(('objectness_head/Dense_0/kernel', 'objectness_head.dense0.weight'))\n        rename_keys.append(('objectness_head/Dense_0/bias', 'objectness_head.dense0.bias'))\n        rename_keys.append(('objectness_head/Dense_1/kernel', 'objectness_head.dense1.weight'))\n        rename_keys.append(('objectness_head/Dense_1/bias', 'objectness_head.dense1.bias'))\n        rename_keys.append(('objectness_head/Dense_2/kernel', 'objectness_head.dense2.weight'))\n        rename_keys.append(('objectness_head/Dense_2/bias', 'objectness_head.dense2.bias'))\n    return rename_keys"
        ]
    },
    {
        "func_name": "rename_and_reshape_key",
        "original": "def rename_and_reshape_key(dct, old, new, config):\n    val = dct.pop(old)\n    if ('out_proj' in new or 'v_proj' in new or 'k_proj' in new or ('q_proj' in new)) and 'vision' in new:\n        val = val.reshape(-1, config.vision_config.hidden_size)\n    if ('out_proj' in new or 'v_proj' in new or 'k_proj' in new or ('q_proj' in new)) and 'text' in new:\n        val = val.reshape(-1, config.text_config.hidden_size)\n    if 'patch_embedding' in new:\n        print('Reshaping patch embedding... for', new)\n        val = val.transpose(3, 2, 0, 1)\n    elif new.endswith('weight') and 'position_embedding' not in new and ('token_embedding' not in new):\n        val = val.T\n    if new.endswith('bias'):\n        val = val.reshape(-1)\n    dct[new] = torch.from_numpy(np.array(val))",
        "mutated": [
            "def rename_and_reshape_key(dct, old, new, config):\n    if False:\n        i = 10\n    val = dct.pop(old)\n    if ('out_proj' in new or 'v_proj' in new or 'k_proj' in new or ('q_proj' in new)) and 'vision' in new:\n        val = val.reshape(-1, config.vision_config.hidden_size)\n    if ('out_proj' in new or 'v_proj' in new or 'k_proj' in new or ('q_proj' in new)) and 'text' in new:\n        val = val.reshape(-1, config.text_config.hidden_size)\n    if 'patch_embedding' in new:\n        print('Reshaping patch embedding... for', new)\n        val = val.transpose(3, 2, 0, 1)\n    elif new.endswith('weight') and 'position_embedding' not in new and ('token_embedding' not in new):\n        val = val.T\n    if new.endswith('bias'):\n        val = val.reshape(-1)\n    dct[new] = torch.from_numpy(np.array(val))",
            "def rename_and_reshape_key(dct, old, new, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    val = dct.pop(old)\n    if ('out_proj' in new or 'v_proj' in new or 'k_proj' in new or ('q_proj' in new)) and 'vision' in new:\n        val = val.reshape(-1, config.vision_config.hidden_size)\n    if ('out_proj' in new or 'v_proj' in new or 'k_proj' in new or ('q_proj' in new)) and 'text' in new:\n        val = val.reshape(-1, config.text_config.hidden_size)\n    if 'patch_embedding' in new:\n        print('Reshaping patch embedding... for', new)\n        val = val.transpose(3, 2, 0, 1)\n    elif new.endswith('weight') and 'position_embedding' not in new and ('token_embedding' not in new):\n        val = val.T\n    if new.endswith('bias'):\n        val = val.reshape(-1)\n    dct[new] = torch.from_numpy(np.array(val))",
            "def rename_and_reshape_key(dct, old, new, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    val = dct.pop(old)\n    if ('out_proj' in new or 'v_proj' in new or 'k_proj' in new or ('q_proj' in new)) and 'vision' in new:\n        val = val.reshape(-1, config.vision_config.hidden_size)\n    if ('out_proj' in new or 'v_proj' in new or 'k_proj' in new or ('q_proj' in new)) and 'text' in new:\n        val = val.reshape(-1, config.text_config.hidden_size)\n    if 'patch_embedding' in new:\n        print('Reshaping patch embedding... for', new)\n        val = val.transpose(3, 2, 0, 1)\n    elif new.endswith('weight') and 'position_embedding' not in new and ('token_embedding' not in new):\n        val = val.T\n    if new.endswith('bias'):\n        val = val.reshape(-1)\n    dct[new] = torch.from_numpy(np.array(val))",
            "def rename_and_reshape_key(dct, old, new, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    val = dct.pop(old)\n    if ('out_proj' in new or 'v_proj' in new or 'k_proj' in new or ('q_proj' in new)) and 'vision' in new:\n        val = val.reshape(-1, config.vision_config.hidden_size)\n    if ('out_proj' in new or 'v_proj' in new or 'k_proj' in new or ('q_proj' in new)) and 'text' in new:\n        val = val.reshape(-1, config.text_config.hidden_size)\n    if 'patch_embedding' in new:\n        print('Reshaping patch embedding... for', new)\n        val = val.transpose(3, 2, 0, 1)\n    elif new.endswith('weight') and 'position_embedding' not in new and ('token_embedding' not in new):\n        val = val.T\n    if new.endswith('bias'):\n        val = val.reshape(-1)\n    dct[new] = torch.from_numpy(np.array(val))",
            "def rename_and_reshape_key(dct, old, new, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    val = dct.pop(old)\n    if ('out_proj' in new or 'v_proj' in new or 'k_proj' in new or ('q_proj' in new)) and 'vision' in new:\n        val = val.reshape(-1, config.vision_config.hidden_size)\n    if ('out_proj' in new or 'v_proj' in new or 'k_proj' in new or ('q_proj' in new)) and 'text' in new:\n        val = val.reshape(-1, config.text_config.hidden_size)\n    if 'patch_embedding' in new:\n        print('Reshaping patch embedding... for', new)\n        val = val.transpose(3, 2, 0, 1)\n    elif new.endswith('weight') and 'position_embedding' not in new and ('token_embedding' not in new):\n        val = val.T\n    if new.endswith('bias'):\n        val = val.reshape(-1)\n    dct[new] = torch.from_numpy(np.array(val))"
        ]
    },
    {
        "func_name": "convert_owlv2_checkpoint",
        "original": "@torch.no_grad()\ndef convert_owlv2_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path, push_to_hub, verify_logits):\n    \"\"\"\n    Copy/paste/tweak model's weights to our OWL-ViT structure.\n    \"\"\"\n    config = get_owlv2_config(model_name)\n    variables = checkpoints.restore_checkpoint(checkpoint_path, target=None)\n    variables = variables['params'] if 'v2' in model_name else variables['optimizer']['target']\n    flax_params = jax.tree_util.tree_map(lambda x: x.astype(jnp.float32) if x.dtype == jnp.bfloat16 else x, variables)\n    state_dict = flatten_nested_dict(flax_params)\n    rename_keys = create_rename_keys(config, model_name)\n    for (src, dest) in rename_keys:\n        rename_and_reshape_key(state_dict, src, dest, config)\n    model = Owlv2ForObjectDetection(config)\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    assert missing_keys == ['owlv2.visual_projection.weight']\n    assert unexpected_keys == []\n    model.eval()\n    size = {'height': config.vision_config.image_size, 'width': config.vision_config.image_size}\n    image_processor = Owlv2ImageProcessor(size=size)\n    tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32', pad_token='!', model_max_length=16)\n    processor = Owlv2Processor(image_processor=image_processor, tokenizer=tokenizer)\n    filepath = hf_hub_download(repo_id='nielsr/test-image', filename='owlvit_pixel_values_960.pt', repo_type='dataset')\n    original_pixel_values = torch.load(filepath).permute(0, 3, 1, 2)\n    filepath = hf_hub_download(repo_id='nielsr/test-image', filename='owlv2_input_ids.pt', repo_type='dataset')\n    original_input_ids = torch.load(filepath).squeeze()\n    filepath = hf_hub_download(repo_id='adirik/OWL-ViT', repo_type='space', filename='assets/astronaut.png')\n    image = Image.open(filepath)\n    texts = [['face', 'rocket', 'nasa badge', 'star-spangled banner']]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    if 'large' not in model_name:\n        assert torch.allclose(inputs.pixel_values, original_pixel_values.float(), atol=1e-06)\n    assert torch.allclose(inputs.input_ids[:4, :], original_input_ids[:4, :], atol=1e-06)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        pred_boxes = outputs.pred_boxes\n        objectness_logits = outputs.objectness_logits\n    if verify_logits:\n        if model_name == 'owlv2-base-patch16':\n            expected_logits = torch.tensor([[-10.0043, -9.0226, -8.0433], [-12.4569, -14.038, -12.6153], [-21.0731, -22.2705, -21.885]])\n            expected_boxes = torch.tensor([[0.0136, 0.0223, 0.0269], [0.0406, 0.0327, 0.0797], [0.0638, 0.1539, 0.1255]])\n            expected_objectness_logits = torch.tensor([[-5.6589, -7.7702, -16.3965]])\n        elif model_name == 'owlv2-base-patch16-finetuned':\n            expected_logits = torch.tensor([[-9.2391, -9.2313, -8.0295], [-14.5498, -16.845, -14.7166], [-15.1278, -17.306, -15.7169]])\n            expected_boxes = torch.tensor([[0.0103, 0.0094, 0.0207], [0.0483, 0.0729, 0.1013], [0.0629, 0.1396, 0.1313]])\n            expected_objectness_logits = torch.tensor([[-6.5234, -13.3788, -14.6627]])\n        elif model_name == 'owlv2-base-patch16-ensemble':\n            expected_logits = torch.tensor([[-8.6353, -9.5409, -6.6154], [-7.9442, -9.6151, -6.7117], [-12.4593, -15.3332, -12.1048]])\n            expected_boxes = torch.tensor([[0.0126, 0.009, 0.0238], [0.0387, 0.0227, 0.0754], [0.0582, 0.1058, 0.1139]])\n            expected_objectness_logits = torch.tensor([[-6.0628, -5.9507, -10.4486]])\n        elif model_name == 'owlv2-large-patch14':\n            expected_logits = torch.tensor([[-12.6662, -11.8384, -12.188], [-16.0599, -16.5835, -16.9364], [-21.4957, -26.7038, -25.1313]])\n            expected_boxes = torch.tensor([[0.0136, 0.0161, 0.0256], [0.0126, 0.0135, 0.0202], [0.0498, 0.0948, 0.0915]])\n            expected_objectness_logits = torch.tensor([[-6.7196, -9.459, -13.9472]])\n        elif model_name == 'owlv2-large-patch14-finetuned':\n            expected_logits = torch.tensor([[-9.5413, -9.713, -7.9762], [-9.5731, -9.7277, -8.2252], [-15.4434, -19.3084, -16.549]])\n            expected_boxes = torch.tensor([[0.0089, 0.008, 0.0175], [0.0112, 0.0098, 0.0179], [0.0375, 0.0821, 0.0528]])\n            expected_objectness_logits = torch.tensor([[-6.2655, -6.5845, -11.3105]])\n        elif model_name == 'owlv2-large-patch14-ensemble':\n            expected_logits = torch.tensor([[-12.2037, -12.207, -11.5371], [-13.4875, -13.8235, -13.1586], [-18.2007, -22.9834, -20.6816]])\n            expected_boxes = torch.tensor([[0.0126, 0.0127, 0.0222], [0.0107, 0.0113, 0.0164], [0.0482, 0.1162, 0.0885]])\n            expected_objectness_logits = torch.tensor([[-7.7572, -8.3637, -13.0334]])\n        print('Objectness logits:', objectness_logits[:3, :3])\n        print('Logits:', logits[0, :3, :3])\n        print('Pred boxes:', pred_boxes[0, :3, :3])\n        assert torch.allclose(logits[0, :3, :3], expected_logits, atol=0.001)\n        assert torch.allclose(pred_boxes[0, :3, :3], expected_boxes, atol=0.001)\n        assert torch.allclose(objectness_logits[:3, :3], expected_objectness_logits, atol=0.001)\n        print('Looks ok!')\n    else:\n        print('Model converted without verifying logits')\n    if pytorch_dump_folder_path is not None:\n        print('Saving model and processor locally...')\n        if not os.path.isdir(pytorch_dump_folder_path):\n            os.mkdir(pytorch_dump_folder_path)\n        model.save_pretrained(pytorch_dump_folder_path)\n        processor.save_pretrained(pytorch_dump_folder_path)\n    if push_to_hub:\n        print(f'Pushing {model_name} to the hub...')\n        model.push_to_hub(f'google/{model_name}')\n        processor.push_to_hub(f'google/{model_name}')",
        "mutated": [
            "@torch.no_grad()\ndef convert_owlv2_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path, push_to_hub, verify_logits):\n    if False:\n        i = 10\n    \"\\n    Copy/paste/tweak model's weights to our OWL-ViT structure.\\n    \"\n    config = get_owlv2_config(model_name)\n    variables = checkpoints.restore_checkpoint(checkpoint_path, target=None)\n    variables = variables['params'] if 'v2' in model_name else variables['optimizer']['target']\n    flax_params = jax.tree_util.tree_map(lambda x: x.astype(jnp.float32) if x.dtype == jnp.bfloat16 else x, variables)\n    state_dict = flatten_nested_dict(flax_params)\n    rename_keys = create_rename_keys(config, model_name)\n    for (src, dest) in rename_keys:\n        rename_and_reshape_key(state_dict, src, dest, config)\n    model = Owlv2ForObjectDetection(config)\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    assert missing_keys == ['owlv2.visual_projection.weight']\n    assert unexpected_keys == []\n    model.eval()\n    size = {'height': config.vision_config.image_size, 'width': config.vision_config.image_size}\n    image_processor = Owlv2ImageProcessor(size=size)\n    tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32', pad_token='!', model_max_length=16)\n    processor = Owlv2Processor(image_processor=image_processor, tokenizer=tokenizer)\n    filepath = hf_hub_download(repo_id='nielsr/test-image', filename='owlvit_pixel_values_960.pt', repo_type='dataset')\n    original_pixel_values = torch.load(filepath).permute(0, 3, 1, 2)\n    filepath = hf_hub_download(repo_id='nielsr/test-image', filename='owlv2_input_ids.pt', repo_type='dataset')\n    original_input_ids = torch.load(filepath).squeeze()\n    filepath = hf_hub_download(repo_id='adirik/OWL-ViT', repo_type='space', filename='assets/astronaut.png')\n    image = Image.open(filepath)\n    texts = [['face', 'rocket', 'nasa badge', 'star-spangled banner']]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    if 'large' not in model_name:\n        assert torch.allclose(inputs.pixel_values, original_pixel_values.float(), atol=1e-06)\n    assert torch.allclose(inputs.input_ids[:4, :], original_input_ids[:4, :], atol=1e-06)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        pred_boxes = outputs.pred_boxes\n        objectness_logits = outputs.objectness_logits\n    if verify_logits:\n        if model_name == 'owlv2-base-patch16':\n            expected_logits = torch.tensor([[-10.0043, -9.0226, -8.0433], [-12.4569, -14.038, -12.6153], [-21.0731, -22.2705, -21.885]])\n            expected_boxes = torch.tensor([[0.0136, 0.0223, 0.0269], [0.0406, 0.0327, 0.0797], [0.0638, 0.1539, 0.1255]])\n            expected_objectness_logits = torch.tensor([[-5.6589, -7.7702, -16.3965]])\n        elif model_name == 'owlv2-base-patch16-finetuned':\n            expected_logits = torch.tensor([[-9.2391, -9.2313, -8.0295], [-14.5498, -16.845, -14.7166], [-15.1278, -17.306, -15.7169]])\n            expected_boxes = torch.tensor([[0.0103, 0.0094, 0.0207], [0.0483, 0.0729, 0.1013], [0.0629, 0.1396, 0.1313]])\n            expected_objectness_logits = torch.tensor([[-6.5234, -13.3788, -14.6627]])\n        elif model_name == 'owlv2-base-patch16-ensemble':\n            expected_logits = torch.tensor([[-8.6353, -9.5409, -6.6154], [-7.9442, -9.6151, -6.7117], [-12.4593, -15.3332, -12.1048]])\n            expected_boxes = torch.tensor([[0.0126, 0.009, 0.0238], [0.0387, 0.0227, 0.0754], [0.0582, 0.1058, 0.1139]])\n            expected_objectness_logits = torch.tensor([[-6.0628, -5.9507, -10.4486]])\n        elif model_name == 'owlv2-large-patch14':\n            expected_logits = torch.tensor([[-12.6662, -11.8384, -12.188], [-16.0599, -16.5835, -16.9364], [-21.4957, -26.7038, -25.1313]])\n            expected_boxes = torch.tensor([[0.0136, 0.0161, 0.0256], [0.0126, 0.0135, 0.0202], [0.0498, 0.0948, 0.0915]])\n            expected_objectness_logits = torch.tensor([[-6.7196, -9.459, -13.9472]])\n        elif model_name == 'owlv2-large-patch14-finetuned':\n            expected_logits = torch.tensor([[-9.5413, -9.713, -7.9762], [-9.5731, -9.7277, -8.2252], [-15.4434, -19.3084, -16.549]])\n            expected_boxes = torch.tensor([[0.0089, 0.008, 0.0175], [0.0112, 0.0098, 0.0179], [0.0375, 0.0821, 0.0528]])\n            expected_objectness_logits = torch.tensor([[-6.2655, -6.5845, -11.3105]])\n        elif model_name == 'owlv2-large-patch14-ensemble':\n            expected_logits = torch.tensor([[-12.2037, -12.207, -11.5371], [-13.4875, -13.8235, -13.1586], [-18.2007, -22.9834, -20.6816]])\n            expected_boxes = torch.tensor([[0.0126, 0.0127, 0.0222], [0.0107, 0.0113, 0.0164], [0.0482, 0.1162, 0.0885]])\n            expected_objectness_logits = torch.tensor([[-7.7572, -8.3637, -13.0334]])\n        print('Objectness logits:', objectness_logits[:3, :3])\n        print('Logits:', logits[0, :3, :3])\n        print('Pred boxes:', pred_boxes[0, :3, :3])\n        assert torch.allclose(logits[0, :3, :3], expected_logits, atol=0.001)\n        assert torch.allclose(pred_boxes[0, :3, :3], expected_boxes, atol=0.001)\n        assert torch.allclose(objectness_logits[:3, :3], expected_objectness_logits, atol=0.001)\n        print('Looks ok!')\n    else:\n        print('Model converted without verifying logits')\n    if pytorch_dump_folder_path is not None:\n        print('Saving model and processor locally...')\n        if not os.path.isdir(pytorch_dump_folder_path):\n            os.mkdir(pytorch_dump_folder_path)\n        model.save_pretrained(pytorch_dump_folder_path)\n        processor.save_pretrained(pytorch_dump_folder_path)\n    if push_to_hub:\n        print(f'Pushing {model_name} to the hub...')\n        model.push_to_hub(f'google/{model_name}')\n        processor.push_to_hub(f'google/{model_name}')",
            "@torch.no_grad()\ndef convert_owlv2_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path, push_to_hub, verify_logits):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Copy/paste/tweak model's weights to our OWL-ViT structure.\\n    \"\n    config = get_owlv2_config(model_name)\n    variables = checkpoints.restore_checkpoint(checkpoint_path, target=None)\n    variables = variables['params'] if 'v2' in model_name else variables['optimizer']['target']\n    flax_params = jax.tree_util.tree_map(lambda x: x.astype(jnp.float32) if x.dtype == jnp.bfloat16 else x, variables)\n    state_dict = flatten_nested_dict(flax_params)\n    rename_keys = create_rename_keys(config, model_name)\n    for (src, dest) in rename_keys:\n        rename_and_reshape_key(state_dict, src, dest, config)\n    model = Owlv2ForObjectDetection(config)\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    assert missing_keys == ['owlv2.visual_projection.weight']\n    assert unexpected_keys == []\n    model.eval()\n    size = {'height': config.vision_config.image_size, 'width': config.vision_config.image_size}\n    image_processor = Owlv2ImageProcessor(size=size)\n    tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32', pad_token='!', model_max_length=16)\n    processor = Owlv2Processor(image_processor=image_processor, tokenizer=tokenizer)\n    filepath = hf_hub_download(repo_id='nielsr/test-image', filename='owlvit_pixel_values_960.pt', repo_type='dataset')\n    original_pixel_values = torch.load(filepath).permute(0, 3, 1, 2)\n    filepath = hf_hub_download(repo_id='nielsr/test-image', filename='owlv2_input_ids.pt', repo_type='dataset')\n    original_input_ids = torch.load(filepath).squeeze()\n    filepath = hf_hub_download(repo_id='adirik/OWL-ViT', repo_type='space', filename='assets/astronaut.png')\n    image = Image.open(filepath)\n    texts = [['face', 'rocket', 'nasa badge', 'star-spangled banner']]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    if 'large' not in model_name:\n        assert torch.allclose(inputs.pixel_values, original_pixel_values.float(), atol=1e-06)\n    assert torch.allclose(inputs.input_ids[:4, :], original_input_ids[:4, :], atol=1e-06)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        pred_boxes = outputs.pred_boxes\n        objectness_logits = outputs.objectness_logits\n    if verify_logits:\n        if model_name == 'owlv2-base-patch16':\n            expected_logits = torch.tensor([[-10.0043, -9.0226, -8.0433], [-12.4569, -14.038, -12.6153], [-21.0731, -22.2705, -21.885]])\n            expected_boxes = torch.tensor([[0.0136, 0.0223, 0.0269], [0.0406, 0.0327, 0.0797], [0.0638, 0.1539, 0.1255]])\n            expected_objectness_logits = torch.tensor([[-5.6589, -7.7702, -16.3965]])\n        elif model_name == 'owlv2-base-patch16-finetuned':\n            expected_logits = torch.tensor([[-9.2391, -9.2313, -8.0295], [-14.5498, -16.845, -14.7166], [-15.1278, -17.306, -15.7169]])\n            expected_boxes = torch.tensor([[0.0103, 0.0094, 0.0207], [0.0483, 0.0729, 0.1013], [0.0629, 0.1396, 0.1313]])\n            expected_objectness_logits = torch.tensor([[-6.5234, -13.3788, -14.6627]])\n        elif model_name == 'owlv2-base-patch16-ensemble':\n            expected_logits = torch.tensor([[-8.6353, -9.5409, -6.6154], [-7.9442, -9.6151, -6.7117], [-12.4593, -15.3332, -12.1048]])\n            expected_boxes = torch.tensor([[0.0126, 0.009, 0.0238], [0.0387, 0.0227, 0.0754], [0.0582, 0.1058, 0.1139]])\n            expected_objectness_logits = torch.tensor([[-6.0628, -5.9507, -10.4486]])\n        elif model_name == 'owlv2-large-patch14':\n            expected_logits = torch.tensor([[-12.6662, -11.8384, -12.188], [-16.0599, -16.5835, -16.9364], [-21.4957, -26.7038, -25.1313]])\n            expected_boxes = torch.tensor([[0.0136, 0.0161, 0.0256], [0.0126, 0.0135, 0.0202], [0.0498, 0.0948, 0.0915]])\n            expected_objectness_logits = torch.tensor([[-6.7196, -9.459, -13.9472]])\n        elif model_name == 'owlv2-large-patch14-finetuned':\n            expected_logits = torch.tensor([[-9.5413, -9.713, -7.9762], [-9.5731, -9.7277, -8.2252], [-15.4434, -19.3084, -16.549]])\n            expected_boxes = torch.tensor([[0.0089, 0.008, 0.0175], [0.0112, 0.0098, 0.0179], [0.0375, 0.0821, 0.0528]])\n            expected_objectness_logits = torch.tensor([[-6.2655, -6.5845, -11.3105]])\n        elif model_name == 'owlv2-large-patch14-ensemble':\n            expected_logits = torch.tensor([[-12.2037, -12.207, -11.5371], [-13.4875, -13.8235, -13.1586], [-18.2007, -22.9834, -20.6816]])\n            expected_boxes = torch.tensor([[0.0126, 0.0127, 0.0222], [0.0107, 0.0113, 0.0164], [0.0482, 0.1162, 0.0885]])\n            expected_objectness_logits = torch.tensor([[-7.7572, -8.3637, -13.0334]])\n        print('Objectness logits:', objectness_logits[:3, :3])\n        print('Logits:', logits[0, :3, :3])\n        print('Pred boxes:', pred_boxes[0, :3, :3])\n        assert torch.allclose(logits[0, :3, :3], expected_logits, atol=0.001)\n        assert torch.allclose(pred_boxes[0, :3, :3], expected_boxes, atol=0.001)\n        assert torch.allclose(objectness_logits[:3, :3], expected_objectness_logits, atol=0.001)\n        print('Looks ok!')\n    else:\n        print('Model converted without verifying logits')\n    if pytorch_dump_folder_path is not None:\n        print('Saving model and processor locally...')\n        if not os.path.isdir(pytorch_dump_folder_path):\n            os.mkdir(pytorch_dump_folder_path)\n        model.save_pretrained(pytorch_dump_folder_path)\n        processor.save_pretrained(pytorch_dump_folder_path)\n    if push_to_hub:\n        print(f'Pushing {model_name} to the hub...')\n        model.push_to_hub(f'google/{model_name}')\n        processor.push_to_hub(f'google/{model_name}')",
            "@torch.no_grad()\ndef convert_owlv2_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path, push_to_hub, verify_logits):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Copy/paste/tweak model's weights to our OWL-ViT structure.\\n    \"\n    config = get_owlv2_config(model_name)\n    variables = checkpoints.restore_checkpoint(checkpoint_path, target=None)\n    variables = variables['params'] if 'v2' in model_name else variables['optimizer']['target']\n    flax_params = jax.tree_util.tree_map(lambda x: x.astype(jnp.float32) if x.dtype == jnp.bfloat16 else x, variables)\n    state_dict = flatten_nested_dict(flax_params)\n    rename_keys = create_rename_keys(config, model_name)\n    for (src, dest) in rename_keys:\n        rename_and_reshape_key(state_dict, src, dest, config)\n    model = Owlv2ForObjectDetection(config)\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    assert missing_keys == ['owlv2.visual_projection.weight']\n    assert unexpected_keys == []\n    model.eval()\n    size = {'height': config.vision_config.image_size, 'width': config.vision_config.image_size}\n    image_processor = Owlv2ImageProcessor(size=size)\n    tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32', pad_token='!', model_max_length=16)\n    processor = Owlv2Processor(image_processor=image_processor, tokenizer=tokenizer)\n    filepath = hf_hub_download(repo_id='nielsr/test-image', filename='owlvit_pixel_values_960.pt', repo_type='dataset')\n    original_pixel_values = torch.load(filepath).permute(0, 3, 1, 2)\n    filepath = hf_hub_download(repo_id='nielsr/test-image', filename='owlv2_input_ids.pt', repo_type='dataset')\n    original_input_ids = torch.load(filepath).squeeze()\n    filepath = hf_hub_download(repo_id='adirik/OWL-ViT', repo_type='space', filename='assets/astronaut.png')\n    image = Image.open(filepath)\n    texts = [['face', 'rocket', 'nasa badge', 'star-spangled banner']]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    if 'large' not in model_name:\n        assert torch.allclose(inputs.pixel_values, original_pixel_values.float(), atol=1e-06)\n    assert torch.allclose(inputs.input_ids[:4, :], original_input_ids[:4, :], atol=1e-06)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        pred_boxes = outputs.pred_boxes\n        objectness_logits = outputs.objectness_logits\n    if verify_logits:\n        if model_name == 'owlv2-base-patch16':\n            expected_logits = torch.tensor([[-10.0043, -9.0226, -8.0433], [-12.4569, -14.038, -12.6153], [-21.0731, -22.2705, -21.885]])\n            expected_boxes = torch.tensor([[0.0136, 0.0223, 0.0269], [0.0406, 0.0327, 0.0797], [0.0638, 0.1539, 0.1255]])\n            expected_objectness_logits = torch.tensor([[-5.6589, -7.7702, -16.3965]])\n        elif model_name == 'owlv2-base-patch16-finetuned':\n            expected_logits = torch.tensor([[-9.2391, -9.2313, -8.0295], [-14.5498, -16.845, -14.7166], [-15.1278, -17.306, -15.7169]])\n            expected_boxes = torch.tensor([[0.0103, 0.0094, 0.0207], [0.0483, 0.0729, 0.1013], [0.0629, 0.1396, 0.1313]])\n            expected_objectness_logits = torch.tensor([[-6.5234, -13.3788, -14.6627]])\n        elif model_name == 'owlv2-base-patch16-ensemble':\n            expected_logits = torch.tensor([[-8.6353, -9.5409, -6.6154], [-7.9442, -9.6151, -6.7117], [-12.4593, -15.3332, -12.1048]])\n            expected_boxes = torch.tensor([[0.0126, 0.009, 0.0238], [0.0387, 0.0227, 0.0754], [0.0582, 0.1058, 0.1139]])\n            expected_objectness_logits = torch.tensor([[-6.0628, -5.9507, -10.4486]])\n        elif model_name == 'owlv2-large-patch14':\n            expected_logits = torch.tensor([[-12.6662, -11.8384, -12.188], [-16.0599, -16.5835, -16.9364], [-21.4957, -26.7038, -25.1313]])\n            expected_boxes = torch.tensor([[0.0136, 0.0161, 0.0256], [0.0126, 0.0135, 0.0202], [0.0498, 0.0948, 0.0915]])\n            expected_objectness_logits = torch.tensor([[-6.7196, -9.459, -13.9472]])\n        elif model_name == 'owlv2-large-patch14-finetuned':\n            expected_logits = torch.tensor([[-9.5413, -9.713, -7.9762], [-9.5731, -9.7277, -8.2252], [-15.4434, -19.3084, -16.549]])\n            expected_boxes = torch.tensor([[0.0089, 0.008, 0.0175], [0.0112, 0.0098, 0.0179], [0.0375, 0.0821, 0.0528]])\n            expected_objectness_logits = torch.tensor([[-6.2655, -6.5845, -11.3105]])\n        elif model_name == 'owlv2-large-patch14-ensemble':\n            expected_logits = torch.tensor([[-12.2037, -12.207, -11.5371], [-13.4875, -13.8235, -13.1586], [-18.2007, -22.9834, -20.6816]])\n            expected_boxes = torch.tensor([[0.0126, 0.0127, 0.0222], [0.0107, 0.0113, 0.0164], [0.0482, 0.1162, 0.0885]])\n            expected_objectness_logits = torch.tensor([[-7.7572, -8.3637, -13.0334]])\n        print('Objectness logits:', objectness_logits[:3, :3])\n        print('Logits:', logits[0, :3, :3])\n        print('Pred boxes:', pred_boxes[0, :3, :3])\n        assert torch.allclose(logits[0, :3, :3], expected_logits, atol=0.001)\n        assert torch.allclose(pred_boxes[0, :3, :3], expected_boxes, atol=0.001)\n        assert torch.allclose(objectness_logits[:3, :3], expected_objectness_logits, atol=0.001)\n        print('Looks ok!')\n    else:\n        print('Model converted without verifying logits')\n    if pytorch_dump_folder_path is not None:\n        print('Saving model and processor locally...')\n        if not os.path.isdir(pytorch_dump_folder_path):\n            os.mkdir(pytorch_dump_folder_path)\n        model.save_pretrained(pytorch_dump_folder_path)\n        processor.save_pretrained(pytorch_dump_folder_path)\n    if push_to_hub:\n        print(f'Pushing {model_name} to the hub...')\n        model.push_to_hub(f'google/{model_name}')\n        processor.push_to_hub(f'google/{model_name}')",
            "@torch.no_grad()\ndef convert_owlv2_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path, push_to_hub, verify_logits):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Copy/paste/tweak model's weights to our OWL-ViT structure.\\n    \"\n    config = get_owlv2_config(model_name)\n    variables = checkpoints.restore_checkpoint(checkpoint_path, target=None)\n    variables = variables['params'] if 'v2' in model_name else variables['optimizer']['target']\n    flax_params = jax.tree_util.tree_map(lambda x: x.astype(jnp.float32) if x.dtype == jnp.bfloat16 else x, variables)\n    state_dict = flatten_nested_dict(flax_params)\n    rename_keys = create_rename_keys(config, model_name)\n    for (src, dest) in rename_keys:\n        rename_and_reshape_key(state_dict, src, dest, config)\n    model = Owlv2ForObjectDetection(config)\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    assert missing_keys == ['owlv2.visual_projection.weight']\n    assert unexpected_keys == []\n    model.eval()\n    size = {'height': config.vision_config.image_size, 'width': config.vision_config.image_size}\n    image_processor = Owlv2ImageProcessor(size=size)\n    tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32', pad_token='!', model_max_length=16)\n    processor = Owlv2Processor(image_processor=image_processor, tokenizer=tokenizer)\n    filepath = hf_hub_download(repo_id='nielsr/test-image', filename='owlvit_pixel_values_960.pt', repo_type='dataset')\n    original_pixel_values = torch.load(filepath).permute(0, 3, 1, 2)\n    filepath = hf_hub_download(repo_id='nielsr/test-image', filename='owlv2_input_ids.pt', repo_type='dataset')\n    original_input_ids = torch.load(filepath).squeeze()\n    filepath = hf_hub_download(repo_id='adirik/OWL-ViT', repo_type='space', filename='assets/astronaut.png')\n    image = Image.open(filepath)\n    texts = [['face', 'rocket', 'nasa badge', 'star-spangled banner']]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    if 'large' not in model_name:\n        assert torch.allclose(inputs.pixel_values, original_pixel_values.float(), atol=1e-06)\n    assert torch.allclose(inputs.input_ids[:4, :], original_input_ids[:4, :], atol=1e-06)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        pred_boxes = outputs.pred_boxes\n        objectness_logits = outputs.objectness_logits\n    if verify_logits:\n        if model_name == 'owlv2-base-patch16':\n            expected_logits = torch.tensor([[-10.0043, -9.0226, -8.0433], [-12.4569, -14.038, -12.6153], [-21.0731, -22.2705, -21.885]])\n            expected_boxes = torch.tensor([[0.0136, 0.0223, 0.0269], [0.0406, 0.0327, 0.0797], [0.0638, 0.1539, 0.1255]])\n            expected_objectness_logits = torch.tensor([[-5.6589, -7.7702, -16.3965]])\n        elif model_name == 'owlv2-base-patch16-finetuned':\n            expected_logits = torch.tensor([[-9.2391, -9.2313, -8.0295], [-14.5498, -16.845, -14.7166], [-15.1278, -17.306, -15.7169]])\n            expected_boxes = torch.tensor([[0.0103, 0.0094, 0.0207], [0.0483, 0.0729, 0.1013], [0.0629, 0.1396, 0.1313]])\n            expected_objectness_logits = torch.tensor([[-6.5234, -13.3788, -14.6627]])\n        elif model_name == 'owlv2-base-patch16-ensemble':\n            expected_logits = torch.tensor([[-8.6353, -9.5409, -6.6154], [-7.9442, -9.6151, -6.7117], [-12.4593, -15.3332, -12.1048]])\n            expected_boxes = torch.tensor([[0.0126, 0.009, 0.0238], [0.0387, 0.0227, 0.0754], [0.0582, 0.1058, 0.1139]])\n            expected_objectness_logits = torch.tensor([[-6.0628, -5.9507, -10.4486]])\n        elif model_name == 'owlv2-large-patch14':\n            expected_logits = torch.tensor([[-12.6662, -11.8384, -12.188], [-16.0599, -16.5835, -16.9364], [-21.4957, -26.7038, -25.1313]])\n            expected_boxes = torch.tensor([[0.0136, 0.0161, 0.0256], [0.0126, 0.0135, 0.0202], [0.0498, 0.0948, 0.0915]])\n            expected_objectness_logits = torch.tensor([[-6.7196, -9.459, -13.9472]])\n        elif model_name == 'owlv2-large-patch14-finetuned':\n            expected_logits = torch.tensor([[-9.5413, -9.713, -7.9762], [-9.5731, -9.7277, -8.2252], [-15.4434, -19.3084, -16.549]])\n            expected_boxes = torch.tensor([[0.0089, 0.008, 0.0175], [0.0112, 0.0098, 0.0179], [0.0375, 0.0821, 0.0528]])\n            expected_objectness_logits = torch.tensor([[-6.2655, -6.5845, -11.3105]])\n        elif model_name == 'owlv2-large-patch14-ensemble':\n            expected_logits = torch.tensor([[-12.2037, -12.207, -11.5371], [-13.4875, -13.8235, -13.1586], [-18.2007, -22.9834, -20.6816]])\n            expected_boxes = torch.tensor([[0.0126, 0.0127, 0.0222], [0.0107, 0.0113, 0.0164], [0.0482, 0.1162, 0.0885]])\n            expected_objectness_logits = torch.tensor([[-7.7572, -8.3637, -13.0334]])\n        print('Objectness logits:', objectness_logits[:3, :3])\n        print('Logits:', logits[0, :3, :3])\n        print('Pred boxes:', pred_boxes[0, :3, :3])\n        assert torch.allclose(logits[0, :3, :3], expected_logits, atol=0.001)\n        assert torch.allclose(pred_boxes[0, :3, :3], expected_boxes, atol=0.001)\n        assert torch.allclose(objectness_logits[:3, :3], expected_objectness_logits, atol=0.001)\n        print('Looks ok!')\n    else:\n        print('Model converted without verifying logits')\n    if pytorch_dump_folder_path is not None:\n        print('Saving model and processor locally...')\n        if not os.path.isdir(pytorch_dump_folder_path):\n            os.mkdir(pytorch_dump_folder_path)\n        model.save_pretrained(pytorch_dump_folder_path)\n        processor.save_pretrained(pytorch_dump_folder_path)\n    if push_to_hub:\n        print(f'Pushing {model_name} to the hub...')\n        model.push_to_hub(f'google/{model_name}')\n        processor.push_to_hub(f'google/{model_name}')",
            "@torch.no_grad()\ndef convert_owlv2_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path, push_to_hub, verify_logits):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Copy/paste/tweak model's weights to our OWL-ViT structure.\\n    \"\n    config = get_owlv2_config(model_name)\n    variables = checkpoints.restore_checkpoint(checkpoint_path, target=None)\n    variables = variables['params'] if 'v2' in model_name else variables['optimizer']['target']\n    flax_params = jax.tree_util.tree_map(lambda x: x.astype(jnp.float32) if x.dtype == jnp.bfloat16 else x, variables)\n    state_dict = flatten_nested_dict(flax_params)\n    rename_keys = create_rename_keys(config, model_name)\n    for (src, dest) in rename_keys:\n        rename_and_reshape_key(state_dict, src, dest, config)\n    model = Owlv2ForObjectDetection(config)\n    (missing_keys, unexpected_keys) = model.load_state_dict(state_dict, strict=False)\n    assert missing_keys == ['owlv2.visual_projection.weight']\n    assert unexpected_keys == []\n    model.eval()\n    size = {'height': config.vision_config.image_size, 'width': config.vision_config.image_size}\n    image_processor = Owlv2ImageProcessor(size=size)\n    tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32', pad_token='!', model_max_length=16)\n    processor = Owlv2Processor(image_processor=image_processor, tokenizer=tokenizer)\n    filepath = hf_hub_download(repo_id='nielsr/test-image', filename='owlvit_pixel_values_960.pt', repo_type='dataset')\n    original_pixel_values = torch.load(filepath).permute(0, 3, 1, 2)\n    filepath = hf_hub_download(repo_id='nielsr/test-image', filename='owlv2_input_ids.pt', repo_type='dataset')\n    original_input_ids = torch.load(filepath).squeeze()\n    filepath = hf_hub_download(repo_id='adirik/OWL-ViT', repo_type='space', filename='assets/astronaut.png')\n    image = Image.open(filepath)\n    texts = [['face', 'rocket', 'nasa badge', 'star-spangled banner']]\n    inputs = processor(text=texts, images=image, return_tensors='pt')\n    if 'large' not in model_name:\n        assert torch.allclose(inputs.pixel_values, original_pixel_values.float(), atol=1e-06)\n    assert torch.allclose(inputs.input_ids[:4, :], original_input_ids[:4, :], atol=1e-06)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        pred_boxes = outputs.pred_boxes\n        objectness_logits = outputs.objectness_logits\n    if verify_logits:\n        if model_name == 'owlv2-base-patch16':\n            expected_logits = torch.tensor([[-10.0043, -9.0226, -8.0433], [-12.4569, -14.038, -12.6153], [-21.0731, -22.2705, -21.885]])\n            expected_boxes = torch.tensor([[0.0136, 0.0223, 0.0269], [0.0406, 0.0327, 0.0797], [0.0638, 0.1539, 0.1255]])\n            expected_objectness_logits = torch.tensor([[-5.6589, -7.7702, -16.3965]])\n        elif model_name == 'owlv2-base-patch16-finetuned':\n            expected_logits = torch.tensor([[-9.2391, -9.2313, -8.0295], [-14.5498, -16.845, -14.7166], [-15.1278, -17.306, -15.7169]])\n            expected_boxes = torch.tensor([[0.0103, 0.0094, 0.0207], [0.0483, 0.0729, 0.1013], [0.0629, 0.1396, 0.1313]])\n            expected_objectness_logits = torch.tensor([[-6.5234, -13.3788, -14.6627]])\n        elif model_name == 'owlv2-base-patch16-ensemble':\n            expected_logits = torch.tensor([[-8.6353, -9.5409, -6.6154], [-7.9442, -9.6151, -6.7117], [-12.4593, -15.3332, -12.1048]])\n            expected_boxes = torch.tensor([[0.0126, 0.009, 0.0238], [0.0387, 0.0227, 0.0754], [0.0582, 0.1058, 0.1139]])\n            expected_objectness_logits = torch.tensor([[-6.0628, -5.9507, -10.4486]])\n        elif model_name == 'owlv2-large-patch14':\n            expected_logits = torch.tensor([[-12.6662, -11.8384, -12.188], [-16.0599, -16.5835, -16.9364], [-21.4957, -26.7038, -25.1313]])\n            expected_boxes = torch.tensor([[0.0136, 0.0161, 0.0256], [0.0126, 0.0135, 0.0202], [0.0498, 0.0948, 0.0915]])\n            expected_objectness_logits = torch.tensor([[-6.7196, -9.459, -13.9472]])\n        elif model_name == 'owlv2-large-patch14-finetuned':\n            expected_logits = torch.tensor([[-9.5413, -9.713, -7.9762], [-9.5731, -9.7277, -8.2252], [-15.4434, -19.3084, -16.549]])\n            expected_boxes = torch.tensor([[0.0089, 0.008, 0.0175], [0.0112, 0.0098, 0.0179], [0.0375, 0.0821, 0.0528]])\n            expected_objectness_logits = torch.tensor([[-6.2655, -6.5845, -11.3105]])\n        elif model_name == 'owlv2-large-patch14-ensemble':\n            expected_logits = torch.tensor([[-12.2037, -12.207, -11.5371], [-13.4875, -13.8235, -13.1586], [-18.2007, -22.9834, -20.6816]])\n            expected_boxes = torch.tensor([[0.0126, 0.0127, 0.0222], [0.0107, 0.0113, 0.0164], [0.0482, 0.1162, 0.0885]])\n            expected_objectness_logits = torch.tensor([[-7.7572, -8.3637, -13.0334]])\n        print('Objectness logits:', objectness_logits[:3, :3])\n        print('Logits:', logits[0, :3, :3])\n        print('Pred boxes:', pred_boxes[0, :3, :3])\n        assert torch.allclose(logits[0, :3, :3], expected_logits, atol=0.001)\n        assert torch.allclose(pred_boxes[0, :3, :3], expected_boxes, atol=0.001)\n        assert torch.allclose(objectness_logits[:3, :3], expected_objectness_logits, atol=0.001)\n        print('Looks ok!')\n    else:\n        print('Model converted without verifying logits')\n    if pytorch_dump_folder_path is not None:\n        print('Saving model and processor locally...')\n        if not os.path.isdir(pytorch_dump_folder_path):\n            os.mkdir(pytorch_dump_folder_path)\n        model.save_pretrained(pytorch_dump_folder_path)\n        processor.save_pretrained(pytorch_dump_folder_path)\n    if push_to_hub:\n        print(f'Pushing {model_name} to the hub...')\n        model.push_to_hub(f'google/{model_name}')\n        processor.push_to_hub(f'google/{model_name}')"
        ]
    }
]