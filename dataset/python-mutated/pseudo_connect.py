import chainer
from chainer import backend
import chainer.utils

class PseudoConnect(chainer.FunctionNode):
    """Connect a variable to a delegating variable."""

    def forward(self, inputs):
        if False:
            return 10
        self.retain_inputs((0,))
        actual_variables = inputs[1:]
        return actual_variables

    def backward(self, target_input_indexes, grad_outputs):
        if False:
            for i in range(10):
                print('nop')
        (delegate_variable,) = self.get_retained_inputs()
        xp = backend.get_array_module(delegate_variable)
        grad_delegate_variable = xp.zeros_like(delegate_variable.array)
        return (chainer.Variable(grad_delegate_variable),) + grad_outputs

def pseudo_connect(delegate_variable, *actual_variables):
    if False:
        print('Hello World!')
    'Connect independent connected graph component.\n\n    This function is implemented to return received arguments directly,\n    except the first ``delegate_variable``.\n    In backward computation, it returns received gradients directly,\n    adding a zero grad corresponding to ``delegate_variable``.\n    The detail of ``delegate_variable`` is described in the following notes.\n\n    .. note::\n        In model-parallel framework, models on each process might have many\n        non-connected components. Here we call a given graph non-connected\n        when multiple inter-process communications are needed for its\n        computation. For example, consider the following example::\n\n            class ConnectedGraph(chainermn.MultiNodeChainList):\n\n                def __init__(self, comm):\n                    super(ConnectedGraph, self).__init__(comm)\n                    self.add_link(ConnectedGraphSub(), rank_in=3, rank_out=1)\n\n        This model receives inputs from rank=3 process and sends its outputs\n        to rank=1 process. The entire graph can be seen as one connected\n        component ``ConnectedGraphSub``. Please refer the documentation of\n        ``MultiNodeChainList`` for detail.\n\n        On the other hand, see the next example::\n\n            class NonConnectedGraph(chainermn.MultiNodeChainList):\n\n                def __init__(self, comm):\n                    super(NonConnectedGraph, self).__init__(comm)\n                    self.add_link(NonConnectedGraphSubA(), rank_in=3, rank_out=1)\n                    self.add_link(NonConnectedGraphSubB(), rank_in=1, rank_out=2)\n\n        This model consists of two components: at first,\n        ``NonConnectedGraphSubA`` receives inputs from rank=3 process and\n        sends its outputs to rank=1 process, and then\n        ``NonConnectedGraphSubB`` receives inputs from rank=1 process and\n        sends its outputs to rank=2 process. Here multiple inter-process\n        communications are invoked between ``NonConnectedGraphSubA`` and\n        ``NonConnectedGraphSubB``, so it is regarded as non-connected.\n\n        Such kind of non-connected models can be problematic in backward\n        computation. Chainer traces back the computational graph from the\n        output variable, however naive implementation of\n        ``chainermn.functions.recv`` does not take any inputs rather receives\n        inputs by ``MPI_Recv``, where backward path vanishes.\n\n        To prevent this, dummy variables what we call ``delegate_variable``\n        are used. In principle, ``chainermn.functions.send`` does not return\n        any outputs because it sends data to the other process by ``MPI_Send``.\n        However, ``chainermn.functions.send`` returns a dummy / empty variable\n        in our implementation, which is called ``delegate_variable``. This\n        variable does not hold any data, just used for retaining backward\n        computation path. We can guarantee the backward computation just by\n        putting ``delegate_variable`` to the next ``chainermn.functions.recv``\n        (``chainermn.functions.recv`` has an optional argument to receive\n        ``delegate_variable``).\n\n    .. note::\n        In some cases the intermediate graph component returns model outputs.\n        See the next example::\n\n            class NonConnectedGraph2(chainermn.MultiNodeChainList):\n\n                def __init__(self, comm):\n                    super(NonConnectedGraph2, self).__init__(comm)\n                    self.add_link(NonConnectedGraphSubA(), rank_in=1, rank_out=None)\n                    self.add_link(NonConnectedGraphSubB(), rank_in=None, rank_out=1)\n\n        This model first receives inputs from rank=1 process and make model\n        outputs (specified by ``rank_out=None``) in ``NonConnectedGraphSubA``.\n        Then using model inputs (specified by ``rank_in=None``),\n        ``NonConnectedGraphSubB`` sends its outputs to rank=1 process. Since\n        ``MultiNodeChainList.__call__`` returns outputs of the last component\n        (in this case, outputs of ``NonConnectedGraphSubB``), naive\n        implementation cannot output the returned value of\n        ``NonConnectedGraphSubA`` as the model outputs. In this case,\n        ``pseudo_connect`` should be used.\n\n        ``pseudo_connect`` takes two arguments. The first one\n        ``delegate_variable`` is what we explained in above note. In this\n        case, returned value of ``NonConnectedGraphSubB`` corresponds to\n        ``delegate_variable``. The second one ``actual_variables`` is\n        "what we want ``delegate_variable`` to imitate". In\n        ``NonConnectedGraph2``, we obtain returned value of\n        ``NonConnectedGraphSubB`` as the model outputs, but what we actually\n        want is returned value of ``NonConnectedGraphSubA``. At the same time\n        we want to trace back this resulted variable in backward computation.\n        Using ``pseudo_connect``, we can make a variable whose data is the\n        same as the returned value of ``NonConnectedGraphSubA``, and which\n        traces back ``NonConnectedGraphSubB`` first.\n\n        ``pseudo_connect`` should also be used in some pathological cases,\n        for example, where multiple ``chainermn.functions.send`` occurs\n        sequentially.\n\n    Args:\n        delegate_variable (chainer.Variable):\n            Pointer to the previous non-connected graph component.\n        actual_variables (tuple of chainer.Variable):\n            Actual values which ``delegate_variable`` imitate.\n\n    Returns:\n        tuple of chainer.Variable:\n            A variable with the given values combined with delegating variable.\n    '
    chainer.utils.experimental('chainermn.functions.pseudo_connect')
    if delegate_variable is None:
        xp = backend.get_array_module(*actual_variables)
        delegate_variable = xp.empty((0,), xp.float32)
    return PseudoConnect().apply((delegate_variable,) + actual_variables)