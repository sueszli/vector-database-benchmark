[
    {
        "func_name": "__init__",
        "original": "def __init__(self, input_feature_num, output_target_num, past_seq_len, future_seq_len, optimizer, loss, metric, metric_mode=None, hidden_units=None, levels=None, num_channels=None, kernel_size=7, lr=0.001, dropout=0.2, backend='torch', logs_dir='/tmp/auto_tcn', cpus_per_trial=1, name='auto_tcn', remote_dir=None):\n    \"\"\"\n        Create an AutoTCN.\n\n        :param input_feature_num: Int. The number of features in the input\n        :param output_target_num: Int. The number of targets in the output\n        :param past_seq_len: Int. The number of historical steps used for forecasting.\n        :param future_seq_len: Int. The number of future steps to forecast.\n        :param optimizer: String or pyTorch optimizer creator function or\n               tf.keras optimizer instance.\n        :param loss: String or pytorch/tf.keras loss instance or pytorch loss creator function.\n        :param metric: String or customized evaluation metric function.\n            If string, metric is the evaluation metric name to optimize, e.g. \"mse\".\n            If callable function, it signature should be func(y_true, y_pred), where y_true and\n            y_pred are numpy ndarray. The function should return a float value as evaluation result.\n        :param metric_mode: One of [\"min\", \"max\"]. \"max\" means greater metric value is better.\n            You have to specify metric_mode if you use a customized metric function.\n            You don't have to specify metric_mode if you use the built-in metric in\n            bigdl.orca.automl.metrics.Evaluator.\n        :param hidden_units: Int or hp sampling function from an integer space. The number of hidden\n               units or filters for each convolutional layer. It is similar to `units` for LSTM.\n               It defaults to 30. We will omit the hidden_units value if num_channels is specified.\n               For hp sampling, see bigdl.orca.automl.hp for more details.\n               e.g. hp.grid_search([32, 64]).\n        :param levels: Int or hp sampling function from an integer space. The number of levels of\n               TemporalBlocks to use. It defaults to 8. We will omit the levels value if\n               num_channels is specified.\n        :param num_channels: List of integers. A list of hidden_units for each level. You could\n               specify num_channels if you want different hidden_units for different levels.\n               By default, num_channels equals to\n               [hidden_units] * (levels - 1) + [output_target_num].\n        :param kernel_size: Int or hp sampling function from an integer space.\n               The size of the kernel to use in each convolutional layer.\n        :param lr: float or hp sampling function from a float space. Learning rate.\n               e.g. hp.choice([0.001, 0.003, 0.01])\n        :param dropout: float or hp sampling function from a float space. Learning rate. Dropout\n               rate. e.g. hp.uniform(0.1, 0.3)\n        :param backend: The backend of the TCN model. support \"keras\" and \"torch\".\n        :param logs_dir: Local directory to save logs and results. It defaults to \"/tmp/auto_tcn\"\n        :param cpus_per_trial: Int. Number of cpus for each trial. It defaults to 1.\n        :param name: name of the AutoTCN. It defaults to \"auto_tcn\"\n        :param remote_dir: String. Remote directory to sync training results and checkpoints. It\n            defaults to None and doesn't take effects while running in local. While running in\n            cluster, it defaults to \"hdfs:///tmp/{name}\".\n        \"\"\"\n    self.search_space = dict(input_feature_num=input_feature_num, output_feature_num=output_target_num, past_seq_len=past_seq_len, future_seq_len=future_seq_len, nhid=hidden_units, levels=levels, num_channels=num_channels, kernel_size=kernel_size, lr=lr, dropout=dropout)\n    self.metric = metric\n    self.metric_mode = metric_mode\n    self.backend = backend\n    self.optimizer = optimizer\n    self.loss = loss\n    self._auto_est_config = dict(logs_dir=logs_dir, resources_per_trial={'cpu': cpus_per_trial}, remote_dir=remote_dir, name=name)\n    if self.backend.startswith('torch'):\n        from bigdl.chronos.model.tcn import model_creator\n    elif self.backend.startswith('keras'):\n        from bigdl.chronos.model.tf2.TCN_keras import model_creator\n    else:\n        from bigdl.nano.utils.common import invalidInputError\n        invalidInputError(False, f'We only support keras and torch as backend, but got {self.backend}')\n    self._model_creator = model_creator\n    super().__init__()",
        "mutated": [
            "def __init__(self, input_feature_num, output_target_num, past_seq_len, future_seq_len, optimizer, loss, metric, metric_mode=None, hidden_units=None, levels=None, num_channels=None, kernel_size=7, lr=0.001, dropout=0.2, backend='torch', logs_dir='/tmp/auto_tcn', cpus_per_trial=1, name='auto_tcn', remote_dir=None):\n    if False:\n        i = 10\n    '\\n        Create an AutoTCN.\\n\\n        :param input_feature_num: Int. The number of features in the input\\n        :param output_target_num: Int. The number of targets in the output\\n        :param past_seq_len: Int. The number of historical steps used for forecasting.\\n        :param future_seq_len: Int. The number of future steps to forecast.\\n        :param optimizer: String or pyTorch optimizer creator function or\\n               tf.keras optimizer instance.\\n        :param loss: String or pytorch/tf.keras loss instance or pytorch loss creator function.\\n        :param metric: String or customized evaluation metric function.\\n            If string, metric is the evaluation metric name to optimize, e.g. \"mse\".\\n            If callable function, it signature should be func(y_true, y_pred), where y_true and\\n            y_pred are numpy ndarray. The function should return a float value as evaluation result.\\n        :param metric_mode: One of [\"min\", \"max\"]. \"max\" means greater metric value is better.\\n            You have to specify metric_mode if you use a customized metric function.\\n            You don\\'t have to specify metric_mode if you use the built-in metric in\\n            bigdl.orca.automl.metrics.Evaluator.\\n        :param hidden_units: Int or hp sampling function from an integer space. The number of hidden\\n               units or filters for each convolutional layer. It is similar to `units` for LSTM.\\n               It defaults to 30. We will omit the hidden_units value if num_channels is specified.\\n               For hp sampling, see bigdl.orca.automl.hp for more details.\\n               e.g. hp.grid_search([32, 64]).\\n        :param levels: Int or hp sampling function from an integer space. The number of levels of\\n               TemporalBlocks to use. It defaults to 8. We will omit the levels value if\\n               num_channels is specified.\\n        :param num_channels: List of integers. A list of hidden_units for each level. You could\\n               specify num_channels if you want different hidden_units for different levels.\\n               By default, num_channels equals to\\n               [hidden_units] * (levels - 1) + [output_target_num].\\n        :param kernel_size: Int or hp sampling function from an integer space.\\n               The size of the kernel to use in each convolutional layer.\\n        :param lr: float or hp sampling function from a float space. Learning rate.\\n               e.g. hp.choice([0.001, 0.003, 0.01])\\n        :param dropout: float or hp sampling function from a float space. Learning rate. Dropout\\n               rate. e.g. hp.uniform(0.1, 0.3)\\n        :param backend: The backend of the TCN model. support \"keras\" and \"torch\".\\n        :param logs_dir: Local directory to save logs and results. It defaults to \"/tmp/auto_tcn\"\\n        :param cpus_per_trial: Int. Number of cpus for each trial. It defaults to 1.\\n        :param name: name of the AutoTCN. It defaults to \"auto_tcn\"\\n        :param remote_dir: String. Remote directory to sync training results and checkpoints. It\\n            defaults to None and doesn\\'t take effects while running in local. While running in\\n            cluster, it defaults to \"hdfs:///tmp/{name}\".\\n        '\n    self.search_space = dict(input_feature_num=input_feature_num, output_feature_num=output_target_num, past_seq_len=past_seq_len, future_seq_len=future_seq_len, nhid=hidden_units, levels=levels, num_channels=num_channels, kernel_size=kernel_size, lr=lr, dropout=dropout)\n    self.metric = metric\n    self.metric_mode = metric_mode\n    self.backend = backend\n    self.optimizer = optimizer\n    self.loss = loss\n    self._auto_est_config = dict(logs_dir=logs_dir, resources_per_trial={'cpu': cpus_per_trial}, remote_dir=remote_dir, name=name)\n    if self.backend.startswith('torch'):\n        from bigdl.chronos.model.tcn import model_creator\n    elif self.backend.startswith('keras'):\n        from bigdl.chronos.model.tf2.TCN_keras import model_creator\n    else:\n        from bigdl.nano.utils.common import invalidInputError\n        invalidInputError(False, f'We only support keras and torch as backend, but got {self.backend}')\n    self._model_creator = model_creator\n    super().__init__()",
            "def __init__(self, input_feature_num, output_target_num, past_seq_len, future_seq_len, optimizer, loss, metric, metric_mode=None, hidden_units=None, levels=None, num_channels=None, kernel_size=7, lr=0.001, dropout=0.2, backend='torch', logs_dir='/tmp/auto_tcn', cpus_per_trial=1, name='auto_tcn', remote_dir=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create an AutoTCN.\\n\\n        :param input_feature_num: Int. The number of features in the input\\n        :param output_target_num: Int. The number of targets in the output\\n        :param past_seq_len: Int. The number of historical steps used for forecasting.\\n        :param future_seq_len: Int. The number of future steps to forecast.\\n        :param optimizer: String or pyTorch optimizer creator function or\\n               tf.keras optimizer instance.\\n        :param loss: String or pytorch/tf.keras loss instance or pytorch loss creator function.\\n        :param metric: String or customized evaluation metric function.\\n            If string, metric is the evaluation metric name to optimize, e.g. \"mse\".\\n            If callable function, it signature should be func(y_true, y_pred), where y_true and\\n            y_pred are numpy ndarray. The function should return a float value as evaluation result.\\n        :param metric_mode: One of [\"min\", \"max\"]. \"max\" means greater metric value is better.\\n            You have to specify metric_mode if you use a customized metric function.\\n            You don\\'t have to specify metric_mode if you use the built-in metric in\\n            bigdl.orca.automl.metrics.Evaluator.\\n        :param hidden_units: Int or hp sampling function from an integer space. The number of hidden\\n               units or filters for each convolutional layer. It is similar to `units` for LSTM.\\n               It defaults to 30. We will omit the hidden_units value if num_channels is specified.\\n               For hp sampling, see bigdl.orca.automl.hp for more details.\\n               e.g. hp.grid_search([32, 64]).\\n        :param levels: Int or hp sampling function from an integer space. The number of levels of\\n               TemporalBlocks to use. It defaults to 8. We will omit the levels value if\\n               num_channels is specified.\\n        :param num_channels: List of integers. A list of hidden_units for each level. You could\\n               specify num_channels if you want different hidden_units for different levels.\\n               By default, num_channels equals to\\n               [hidden_units] * (levels - 1) + [output_target_num].\\n        :param kernel_size: Int or hp sampling function from an integer space.\\n               The size of the kernel to use in each convolutional layer.\\n        :param lr: float or hp sampling function from a float space. Learning rate.\\n               e.g. hp.choice([0.001, 0.003, 0.01])\\n        :param dropout: float or hp sampling function from a float space. Learning rate. Dropout\\n               rate. e.g. hp.uniform(0.1, 0.3)\\n        :param backend: The backend of the TCN model. support \"keras\" and \"torch\".\\n        :param logs_dir: Local directory to save logs and results. It defaults to \"/tmp/auto_tcn\"\\n        :param cpus_per_trial: Int. Number of cpus for each trial. It defaults to 1.\\n        :param name: name of the AutoTCN. It defaults to \"auto_tcn\"\\n        :param remote_dir: String. Remote directory to sync training results and checkpoints. It\\n            defaults to None and doesn\\'t take effects while running in local. While running in\\n            cluster, it defaults to \"hdfs:///tmp/{name}\".\\n        '\n    self.search_space = dict(input_feature_num=input_feature_num, output_feature_num=output_target_num, past_seq_len=past_seq_len, future_seq_len=future_seq_len, nhid=hidden_units, levels=levels, num_channels=num_channels, kernel_size=kernel_size, lr=lr, dropout=dropout)\n    self.metric = metric\n    self.metric_mode = metric_mode\n    self.backend = backend\n    self.optimizer = optimizer\n    self.loss = loss\n    self._auto_est_config = dict(logs_dir=logs_dir, resources_per_trial={'cpu': cpus_per_trial}, remote_dir=remote_dir, name=name)\n    if self.backend.startswith('torch'):\n        from bigdl.chronos.model.tcn import model_creator\n    elif self.backend.startswith('keras'):\n        from bigdl.chronos.model.tf2.TCN_keras import model_creator\n    else:\n        from bigdl.nano.utils.common import invalidInputError\n        invalidInputError(False, f'We only support keras and torch as backend, but got {self.backend}')\n    self._model_creator = model_creator\n    super().__init__()",
            "def __init__(self, input_feature_num, output_target_num, past_seq_len, future_seq_len, optimizer, loss, metric, metric_mode=None, hidden_units=None, levels=None, num_channels=None, kernel_size=7, lr=0.001, dropout=0.2, backend='torch', logs_dir='/tmp/auto_tcn', cpus_per_trial=1, name='auto_tcn', remote_dir=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create an AutoTCN.\\n\\n        :param input_feature_num: Int. The number of features in the input\\n        :param output_target_num: Int. The number of targets in the output\\n        :param past_seq_len: Int. The number of historical steps used for forecasting.\\n        :param future_seq_len: Int. The number of future steps to forecast.\\n        :param optimizer: String or pyTorch optimizer creator function or\\n               tf.keras optimizer instance.\\n        :param loss: String or pytorch/tf.keras loss instance or pytorch loss creator function.\\n        :param metric: String or customized evaluation metric function.\\n            If string, metric is the evaluation metric name to optimize, e.g. \"mse\".\\n            If callable function, it signature should be func(y_true, y_pred), where y_true and\\n            y_pred are numpy ndarray. The function should return a float value as evaluation result.\\n        :param metric_mode: One of [\"min\", \"max\"]. \"max\" means greater metric value is better.\\n            You have to specify metric_mode if you use a customized metric function.\\n            You don\\'t have to specify metric_mode if you use the built-in metric in\\n            bigdl.orca.automl.metrics.Evaluator.\\n        :param hidden_units: Int or hp sampling function from an integer space. The number of hidden\\n               units or filters for each convolutional layer. It is similar to `units` for LSTM.\\n               It defaults to 30. We will omit the hidden_units value if num_channels is specified.\\n               For hp sampling, see bigdl.orca.automl.hp for more details.\\n               e.g. hp.grid_search([32, 64]).\\n        :param levels: Int or hp sampling function from an integer space. The number of levels of\\n               TemporalBlocks to use. It defaults to 8. We will omit the levels value if\\n               num_channels is specified.\\n        :param num_channels: List of integers. A list of hidden_units for each level. You could\\n               specify num_channels if you want different hidden_units for different levels.\\n               By default, num_channels equals to\\n               [hidden_units] * (levels - 1) + [output_target_num].\\n        :param kernel_size: Int or hp sampling function from an integer space.\\n               The size of the kernel to use in each convolutional layer.\\n        :param lr: float or hp sampling function from a float space. Learning rate.\\n               e.g. hp.choice([0.001, 0.003, 0.01])\\n        :param dropout: float or hp sampling function from a float space. Learning rate. Dropout\\n               rate. e.g. hp.uniform(0.1, 0.3)\\n        :param backend: The backend of the TCN model. support \"keras\" and \"torch\".\\n        :param logs_dir: Local directory to save logs and results. It defaults to \"/tmp/auto_tcn\"\\n        :param cpus_per_trial: Int. Number of cpus for each trial. It defaults to 1.\\n        :param name: name of the AutoTCN. It defaults to \"auto_tcn\"\\n        :param remote_dir: String. Remote directory to sync training results and checkpoints. It\\n            defaults to None and doesn\\'t take effects while running in local. While running in\\n            cluster, it defaults to \"hdfs:///tmp/{name}\".\\n        '\n    self.search_space = dict(input_feature_num=input_feature_num, output_feature_num=output_target_num, past_seq_len=past_seq_len, future_seq_len=future_seq_len, nhid=hidden_units, levels=levels, num_channels=num_channels, kernel_size=kernel_size, lr=lr, dropout=dropout)\n    self.metric = metric\n    self.metric_mode = metric_mode\n    self.backend = backend\n    self.optimizer = optimizer\n    self.loss = loss\n    self._auto_est_config = dict(logs_dir=logs_dir, resources_per_trial={'cpu': cpus_per_trial}, remote_dir=remote_dir, name=name)\n    if self.backend.startswith('torch'):\n        from bigdl.chronos.model.tcn import model_creator\n    elif self.backend.startswith('keras'):\n        from bigdl.chronos.model.tf2.TCN_keras import model_creator\n    else:\n        from bigdl.nano.utils.common import invalidInputError\n        invalidInputError(False, f'We only support keras and torch as backend, but got {self.backend}')\n    self._model_creator = model_creator\n    super().__init__()",
            "def __init__(self, input_feature_num, output_target_num, past_seq_len, future_seq_len, optimizer, loss, metric, metric_mode=None, hidden_units=None, levels=None, num_channels=None, kernel_size=7, lr=0.001, dropout=0.2, backend='torch', logs_dir='/tmp/auto_tcn', cpus_per_trial=1, name='auto_tcn', remote_dir=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create an AutoTCN.\\n\\n        :param input_feature_num: Int. The number of features in the input\\n        :param output_target_num: Int. The number of targets in the output\\n        :param past_seq_len: Int. The number of historical steps used for forecasting.\\n        :param future_seq_len: Int. The number of future steps to forecast.\\n        :param optimizer: String or pyTorch optimizer creator function or\\n               tf.keras optimizer instance.\\n        :param loss: String or pytorch/tf.keras loss instance or pytorch loss creator function.\\n        :param metric: String or customized evaluation metric function.\\n            If string, metric is the evaluation metric name to optimize, e.g. \"mse\".\\n            If callable function, it signature should be func(y_true, y_pred), where y_true and\\n            y_pred are numpy ndarray. The function should return a float value as evaluation result.\\n        :param metric_mode: One of [\"min\", \"max\"]. \"max\" means greater metric value is better.\\n            You have to specify metric_mode if you use a customized metric function.\\n            You don\\'t have to specify metric_mode if you use the built-in metric in\\n            bigdl.orca.automl.metrics.Evaluator.\\n        :param hidden_units: Int or hp sampling function from an integer space. The number of hidden\\n               units or filters for each convolutional layer. It is similar to `units` for LSTM.\\n               It defaults to 30. We will omit the hidden_units value if num_channels is specified.\\n               For hp sampling, see bigdl.orca.automl.hp for more details.\\n               e.g. hp.grid_search([32, 64]).\\n        :param levels: Int or hp sampling function from an integer space. The number of levels of\\n               TemporalBlocks to use. It defaults to 8. We will omit the levels value if\\n               num_channels is specified.\\n        :param num_channels: List of integers. A list of hidden_units for each level. You could\\n               specify num_channels if you want different hidden_units for different levels.\\n               By default, num_channels equals to\\n               [hidden_units] * (levels - 1) + [output_target_num].\\n        :param kernel_size: Int or hp sampling function from an integer space.\\n               The size of the kernel to use in each convolutional layer.\\n        :param lr: float or hp sampling function from a float space. Learning rate.\\n               e.g. hp.choice([0.001, 0.003, 0.01])\\n        :param dropout: float or hp sampling function from a float space. Learning rate. Dropout\\n               rate. e.g. hp.uniform(0.1, 0.3)\\n        :param backend: The backend of the TCN model. support \"keras\" and \"torch\".\\n        :param logs_dir: Local directory to save logs and results. It defaults to \"/tmp/auto_tcn\"\\n        :param cpus_per_trial: Int. Number of cpus for each trial. It defaults to 1.\\n        :param name: name of the AutoTCN. It defaults to \"auto_tcn\"\\n        :param remote_dir: String. Remote directory to sync training results and checkpoints. It\\n            defaults to None and doesn\\'t take effects while running in local. While running in\\n            cluster, it defaults to \"hdfs:///tmp/{name}\".\\n        '\n    self.search_space = dict(input_feature_num=input_feature_num, output_feature_num=output_target_num, past_seq_len=past_seq_len, future_seq_len=future_seq_len, nhid=hidden_units, levels=levels, num_channels=num_channels, kernel_size=kernel_size, lr=lr, dropout=dropout)\n    self.metric = metric\n    self.metric_mode = metric_mode\n    self.backend = backend\n    self.optimizer = optimizer\n    self.loss = loss\n    self._auto_est_config = dict(logs_dir=logs_dir, resources_per_trial={'cpu': cpus_per_trial}, remote_dir=remote_dir, name=name)\n    if self.backend.startswith('torch'):\n        from bigdl.chronos.model.tcn import model_creator\n    elif self.backend.startswith('keras'):\n        from bigdl.chronos.model.tf2.TCN_keras import model_creator\n    else:\n        from bigdl.nano.utils.common import invalidInputError\n        invalidInputError(False, f'We only support keras and torch as backend, but got {self.backend}')\n    self._model_creator = model_creator\n    super().__init__()",
            "def __init__(self, input_feature_num, output_target_num, past_seq_len, future_seq_len, optimizer, loss, metric, metric_mode=None, hidden_units=None, levels=None, num_channels=None, kernel_size=7, lr=0.001, dropout=0.2, backend='torch', logs_dir='/tmp/auto_tcn', cpus_per_trial=1, name='auto_tcn', remote_dir=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create an AutoTCN.\\n\\n        :param input_feature_num: Int. The number of features in the input\\n        :param output_target_num: Int. The number of targets in the output\\n        :param past_seq_len: Int. The number of historical steps used for forecasting.\\n        :param future_seq_len: Int. The number of future steps to forecast.\\n        :param optimizer: String or pyTorch optimizer creator function or\\n               tf.keras optimizer instance.\\n        :param loss: String or pytorch/tf.keras loss instance or pytorch loss creator function.\\n        :param metric: String or customized evaluation metric function.\\n            If string, metric is the evaluation metric name to optimize, e.g. \"mse\".\\n            If callable function, it signature should be func(y_true, y_pred), where y_true and\\n            y_pred are numpy ndarray. The function should return a float value as evaluation result.\\n        :param metric_mode: One of [\"min\", \"max\"]. \"max\" means greater metric value is better.\\n            You have to specify metric_mode if you use a customized metric function.\\n            You don\\'t have to specify metric_mode if you use the built-in metric in\\n            bigdl.orca.automl.metrics.Evaluator.\\n        :param hidden_units: Int or hp sampling function from an integer space. The number of hidden\\n               units or filters for each convolutional layer. It is similar to `units` for LSTM.\\n               It defaults to 30. We will omit the hidden_units value if num_channels is specified.\\n               For hp sampling, see bigdl.orca.automl.hp for more details.\\n               e.g. hp.grid_search([32, 64]).\\n        :param levels: Int or hp sampling function from an integer space. The number of levels of\\n               TemporalBlocks to use. It defaults to 8. We will omit the levels value if\\n               num_channels is specified.\\n        :param num_channels: List of integers. A list of hidden_units for each level. You could\\n               specify num_channels if you want different hidden_units for different levels.\\n               By default, num_channels equals to\\n               [hidden_units] * (levels - 1) + [output_target_num].\\n        :param kernel_size: Int or hp sampling function from an integer space.\\n               The size of the kernel to use in each convolutional layer.\\n        :param lr: float or hp sampling function from a float space. Learning rate.\\n               e.g. hp.choice([0.001, 0.003, 0.01])\\n        :param dropout: float or hp sampling function from a float space. Learning rate. Dropout\\n               rate. e.g. hp.uniform(0.1, 0.3)\\n        :param backend: The backend of the TCN model. support \"keras\" and \"torch\".\\n        :param logs_dir: Local directory to save logs and results. It defaults to \"/tmp/auto_tcn\"\\n        :param cpus_per_trial: Int. Number of cpus for each trial. It defaults to 1.\\n        :param name: name of the AutoTCN. It defaults to \"auto_tcn\"\\n        :param remote_dir: String. Remote directory to sync training results and checkpoints. It\\n            defaults to None and doesn\\'t take effects while running in local. While running in\\n            cluster, it defaults to \"hdfs:///tmp/{name}\".\\n        '\n    self.search_space = dict(input_feature_num=input_feature_num, output_feature_num=output_target_num, past_seq_len=past_seq_len, future_seq_len=future_seq_len, nhid=hidden_units, levels=levels, num_channels=num_channels, kernel_size=kernel_size, lr=lr, dropout=dropout)\n    self.metric = metric\n    self.metric_mode = metric_mode\n    self.backend = backend\n    self.optimizer = optimizer\n    self.loss = loss\n    self._auto_est_config = dict(logs_dir=logs_dir, resources_per_trial={'cpu': cpus_per_trial}, remote_dir=remote_dir, name=name)\n    if self.backend.startswith('torch'):\n        from bigdl.chronos.model.tcn import model_creator\n    elif self.backend.startswith('keras'):\n        from bigdl.chronos.model.tf2.TCN_keras import model_creator\n    else:\n        from bigdl.nano.utils.common import invalidInputError\n        invalidInputError(False, f'We only support keras and torch as backend, but got {self.backend}')\n    self._model_creator = model_creator\n    super().__init__()"
        ]
    }
]