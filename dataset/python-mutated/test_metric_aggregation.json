[
    {
        "func_name": "prepare_experiment",
        "original": "def prepare_experiment():\n    utils.set_global_seed(42)\n    (num_samples, num_features, num_classes) = (int(10000.0), int(10.0), 4)\n    X = torch.rand(num_samples, num_features)\n    y = (torch.rand(num_samples) * num_classes).to(torch.int64)\n    y = torch.nn.functional.one_hot(y, num_classes).double()\n    dataset = TensorDataset(X, y)\n    loader = DataLoader(dataset, batch_size=32, num_workers=1)\n    loaders = {'train': loader, 'valid': loader}\n    model = torch.nn.Linear(num_features, num_classes)\n    criterion = {'bce': torch.nn.BCEWithLogitsLoss(), 'focal': FocalLossBinary()}\n    optimizer = torch.optim.Adam(model.parameters())\n    return (loaders, model, criterion, optimizer)",
        "mutated": [
            "def prepare_experiment():\n    if False:\n        i = 10\n    utils.set_global_seed(42)\n    (num_samples, num_features, num_classes) = (int(10000.0), int(10.0), 4)\n    X = torch.rand(num_samples, num_features)\n    y = (torch.rand(num_samples) * num_classes).to(torch.int64)\n    y = torch.nn.functional.one_hot(y, num_classes).double()\n    dataset = TensorDataset(X, y)\n    loader = DataLoader(dataset, batch_size=32, num_workers=1)\n    loaders = {'train': loader, 'valid': loader}\n    model = torch.nn.Linear(num_features, num_classes)\n    criterion = {'bce': torch.nn.BCEWithLogitsLoss(), 'focal': FocalLossBinary()}\n    optimizer = torch.optim.Adam(model.parameters())\n    return (loaders, model, criterion, optimizer)",
            "def prepare_experiment():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    utils.set_global_seed(42)\n    (num_samples, num_features, num_classes) = (int(10000.0), int(10.0), 4)\n    X = torch.rand(num_samples, num_features)\n    y = (torch.rand(num_samples) * num_classes).to(torch.int64)\n    y = torch.nn.functional.one_hot(y, num_classes).double()\n    dataset = TensorDataset(X, y)\n    loader = DataLoader(dataset, batch_size=32, num_workers=1)\n    loaders = {'train': loader, 'valid': loader}\n    model = torch.nn.Linear(num_features, num_classes)\n    criterion = {'bce': torch.nn.BCEWithLogitsLoss(), 'focal': FocalLossBinary()}\n    optimizer = torch.optim.Adam(model.parameters())\n    return (loaders, model, criterion, optimizer)",
            "def prepare_experiment():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    utils.set_global_seed(42)\n    (num_samples, num_features, num_classes) = (int(10000.0), int(10.0), 4)\n    X = torch.rand(num_samples, num_features)\n    y = (torch.rand(num_samples) * num_classes).to(torch.int64)\n    y = torch.nn.functional.one_hot(y, num_classes).double()\n    dataset = TensorDataset(X, y)\n    loader = DataLoader(dataset, batch_size=32, num_workers=1)\n    loaders = {'train': loader, 'valid': loader}\n    model = torch.nn.Linear(num_features, num_classes)\n    criterion = {'bce': torch.nn.BCEWithLogitsLoss(), 'focal': FocalLossBinary()}\n    optimizer = torch.optim.Adam(model.parameters())\n    return (loaders, model, criterion, optimizer)",
            "def prepare_experiment():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    utils.set_global_seed(42)\n    (num_samples, num_features, num_classes) = (int(10000.0), int(10.0), 4)\n    X = torch.rand(num_samples, num_features)\n    y = (torch.rand(num_samples) * num_classes).to(torch.int64)\n    y = torch.nn.functional.one_hot(y, num_classes).double()\n    dataset = TensorDataset(X, y)\n    loader = DataLoader(dataset, batch_size=32, num_workers=1)\n    loaders = {'train': loader, 'valid': loader}\n    model = torch.nn.Linear(num_features, num_classes)\n    criterion = {'bce': torch.nn.BCEWithLogitsLoss(), 'focal': FocalLossBinary()}\n    optimizer = torch.optim.Adam(model.parameters())\n    return (loaders, model, criterion, optimizer)",
            "def prepare_experiment():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    utils.set_global_seed(42)\n    (num_samples, num_features, num_classes) = (int(10000.0), int(10.0), 4)\n    X = torch.rand(num_samples, num_features)\n    y = (torch.rand(num_samples) * num_classes).to(torch.int64)\n    y = torch.nn.functional.one_hot(y, num_classes).double()\n    dataset = TensorDataset(X, y)\n    loader = DataLoader(dataset, batch_size=32, num_workers=1)\n    loaders = {'train': loader, 'valid': loader}\n    model = torch.nn.Linear(num_features, num_classes)\n    criterion = {'bce': torch.nn.BCEWithLogitsLoss(), 'focal': FocalLossBinary()}\n    optimizer = torch.optim.Adam(model.parameters())\n    return (loaders, model, criterion, optimizer)"
        ]
    },
    {
        "func_name": "test_aggregation_1",
        "original": "def test_aggregation_1():\n    \"\"\"\n    Aggregation as weighted_sum\n    \"\"\"\n    (loaders, model, criterion, optimizer) = prepare_experiment()\n    runner = dl.SupervisedRunner()\n    runner.train(model=model, criterion=criterion, optimizer=optimizer, loaders=loaders, logdir='./logs/aggregation_1/', num_epochs=3, callbacks=[dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_bce', criterion_key='bce'), dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_focal', criterion_key='focal'), dl.MetricAggregationCallback(metric_key='loss', metrics={'loss_focal': 0.6, 'loss_bce': 0.4}, mode='weighted_sum')])\n    for loader in ['train', 'valid']:\n        metrics = runner.epoch_metrics[loader]\n        loss_1 = metrics['loss_bce'] * 0.4 + metrics['loss_focal'] * 0.6\n        loss_2 = metrics['loss']\n        assert np.abs(loss_1 - loss_2) < 1e-05",
        "mutated": [
            "def test_aggregation_1():\n    if False:\n        i = 10\n    '\\n    Aggregation as weighted_sum\\n    '\n    (loaders, model, criterion, optimizer) = prepare_experiment()\n    runner = dl.SupervisedRunner()\n    runner.train(model=model, criterion=criterion, optimizer=optimizer, loaders=loaders, logdir='./logs/aggregation_1/', num_epochs=3, callbacks=[dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_bce', criterion_key='bce'), dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_focal', criterion_key='focal'), dl.MetricAggregationCallback(metric_key='loss', metrics={'loss_focal': 0.6, 'loss_bce': 0.4}, mode='weighted_sum')])\n    for loader in ['train', 'valid']:\n        metrics = runner.epoch_metrics[loader]\n        loss_1 = metrics['loss_bce'] * 0.4 + metrics['loss_focal'] * 0.6\n        loss_2 = metrics['loss']\n        assert np.abs(loss_1 - loss_2) < 1e-05",
            "def test_aggregation_1():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Aggregation as weighted_sum\\n    '\n    (loaders, model, criterion, optimizer) = prepare_experiment()\n    runner = dl.SupervisedRunner()\n    runner.train(model=model, criterion=criterion, optimizer=optimizer, loaders=loaders, logdir='./logs/aggregation_1/', num_epochs=3, callbacks=[dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_bce', criterion_key='bce'), dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_focal', criterion_key='focal'), dl.MetricAggregationCallback(metric_key='loss', metrics={'loss_focal': 0.6, 'loss_bce': 0.4}, mode='weighted_sum')])\n    for loader in ['train', 'valid']:\n        metrics = runner.epoch_metrics[loader]\n        loss_1 = metrics['loss_bce'] * 0.4 + metrics['loss_focal'] * 0.6\n        loss_2 = metrics['loss']\n        assert np.abs(loss_1 - loss_2) < 1e-05",
            "def test_aggregation_1():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Aggregation as weighted_sum\\n    '\n    (loaders, model, criterion, optimizer) = prepare_experiment()\n    runner = dl.SupervisedRunner()\n    runner.train(model=model, criterion=criterion, optimizer=optimizer, loaders=loaders, logdir='./logs/aggregation_1/', num_epochs=3, callbacks=[dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_bce', criterion_key='bce'), dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_focal', criterion_key='focal'), dl.MetricAggregationCallback(metric_key='loss', metrics={'loss_focal': 0.6, 'loss_bce': 0.4}, mode='weighted_sum')])\n    for loader in ['train', 'valid']:\n        metrics = runner.epoch_metrics[loader]\n        loss_1 = metrics['loss_bce'] * 0.4 + metrics['loss_focal'] * 0.6\n        loss_2 = metrics['loss']\n        assert np.abs(loss_1 - loss_2) < 1e-05",
            "def test_aggregation_1():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Aggregation as weighted_sum\\n    '\n    (loaders, model, criterion, optimizer) = prepare_experiment()\n    runner = dl.SupervisedRunner()\n    runner.train(model=model, criterion=criterion, optimizer=optimizer, loaders=loaders, logdir='./logs/aggregation_1/', num_epochs=3, callbacks=[dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_bce', criterion_key='bce'), dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_focal', criterion_key='focal'), dl.MetricAggregationCallback(metric_key='loss', metrics={'loss_focal': 0.6, 'loss_bce': 0.4}, mode='weighted_sum')])\n    for loader in ['train', 'valid']:\n        metrics = runner.epoch_metrics[loader]\n        loss_1 = metrics['loss_bce'] * 0.4 + metrics['loss_focal'] * 0.6\n        loss_2 = metrics['loss']\n        assert np.abs(loss_1 - loss_2) < 1e-05",
            "def test_aggregation_1():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Aggregation as weighted_sum\\n    '\n    (loaders, model, criterion, optimizer) = prepare_experiment()\n    runner = dl.SupervisedRunner()\n    runner.train(model=model, criterion=criterion, optimizer=optimizer, loaders=loaders, logdir='./logs/aggregation_1/', num_epochs=3, callbacks=[dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_bce', criterion_key='bce'), dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_focal', criterion_key='focal'), dl.MetricAggregationCallback(metric_key='loss', metrics={'loss_focal': 0.6, 'loss_bce': 0.4}, mode='weighted_sum')])\n    for loader in ['train', 'valid']:\n        metrics = runner.epoch_metrics[loader]\n        loss_1 = metrics['loss_bce'] * 0.4 + metrics['loss_focal'] * 0.6\n        loss_2 = metrics['loss']\n        assert np.abs(loss_1 - loss_2) < 1e-05"
        ]
    },
    {
        "func_name": "aggregation_function",
        "original": "def aggregation_function(metrics, runner):\n    epoch = runner.epoch_step\n    loss = (3 / 2 - epoch / 2) * metrics['loss_focal'] + (1 / 2 * epoch - 1 / 2) * metrics['loss_bce']\n    return loss",
        "mutated": [
            "def aggregation_function(metrics, runner):\n    if False:\n        i = 10\n    epoch = runner.epoch_step\n    loss = (3 / 2 - epoch / 2) * metrics['loss_focal'] + (1 / 2 * epoch - 1 / 2) * metrics['loss_bce']\n    return loss",
            "def aggregation_function(metrics, runner):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    epoch = runner.epoch_step\n    loss = (3 / 2 - epoch / 2) * metrics['loss_focal'] + (1 / 2 * epoch - 1 / 2) * metrics['loss_bce']\n    return loss",
            "def aggregation_function(metrics, runner):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    epoch = runner.epoch_step\n    loss = (3 / 2 - epoch / 2) * metrics['loss_focal'] + (1 / 2 * epoch - 1 / 2) * metrics['loss_bce']\n    return loss",
            "def aggregation_function(metrics, runner):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    epoch = runner.epoch_step\n    loss = (3 / 2 - epoch / 2) * metrics['loss_focal'] + (1 / 2 * epoch - 1 / 2) * metrics['loss_bce']\n    return loss",
            "def aggregation_function(metrics, runner):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    epoch = runner.epoch_step\n    loss = (3 / 2 - epoch / 2) * metrics['loss_focal'] + (1 / 2 * epoch - 1 / 2) * metrics['loss_bce']\n    return loss"
        ]
    },
    {
        "func_name": "test_aggregation_2",
        "original": "def test_aggregation_2():\n    \"\"\"\n    Aggregation with custom function\n    \"\"\"\n    (loaders, model, criterion, optimizer) = prepare_experiment()\n    runner = dl.SupervisedRunner()\n\n    def aggregation_function(metrics, runner):\n        epoch = runner.epoch_step\n        loss = (3 / 2 - epoch / 2) * metrics['loss_focal'] + (1 / 2 * epoch - 1 / 2) * metrics['loss_bce']\n        return loss\n    runner.train(model=model, criterion=criterion, optimizer=optimizer, loaders=loaders, logdir='./logs/aggregation_2/', num_epochs=3, callbacks=[dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_bce', criterion_key='bce'), dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_focal', criterion_key='focal'), dl.MetricAggregationCallback(metric_key='loss', mode=aggregation_function)])\n    for loader in ['train', 'valid']:\n        metrics = runner.epoch_metrics[loader]\n        loss_1 = metrics['loss_bce']\n        loss_2 = metrics['loss']\n        assert np.abs(loss_1 - loss_2) < 1e-05",
        "mutated": [
            "def test_aggregation_2():\n    if False:\n        i = 10\n    '\\n    Aggregation with custom function\\n    '\n    (loaders, model, criterion, optimizer) = prepare_experiment()\n    runner = dl.SupervisedRunner()\n\n    def aggregation_function(metrics, runner):\n        epoch = runner.epoch_step\n        loss = (3 / 2 - epoch / 2) * metrics['loss_focal'] + (1 / 2 * epoch - 1 / 2) * metrics['loss_bce']\n        return loss\n    runner.train(model=model, criterion=criterion, optimizer=optimizer, loaders=loaders, logdir='./logs/aggregation_2/', num_epochs=3, callbacks=[dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_bce', criterion_key='bce'), dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_focal', criterion_key='focal'), dl.MetricAggregationCallback(metric_key='loss', mode=aggregation_function)])\n    for loader in ['train', 'valid']:\n        metrics = runner.epoch_metrics[loader]\n        loss_1 = metrics['loss_bce']\n        loss_2 = metrics['loss']\n        assert np.abs(loss_1 - loss_2) < 1e-05",
            "def test_aggregation_2():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Aggregation with custom function\\n    '\n    (loaders, model, criterion, optimizer) = prepare_experiment()\n    runner = dl.SupervisedRunner()\n\n    def aggregation_function(metrics, runner):\n        epoch = runner.epoch_step\n        loss = (3 / 2 - epoch / 2) * metrics['loss_focal'] + (1 / 2 * epoch - 1 / 2) * metrics['loss_bce']\n        return loss\n    runner.train(model=model, criterion=criterion, optimizer=optimizer, loaders=loaders, logdir='./logs/aggregation_2/', num_epochs=3, callbacks=[dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_bce', criterion_key='bce'), dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_focal', criterion_key='focal'), dl.MetricAggregationCallback(metric_key='loss', mode=aggregation_function)])\n    for loader in ['train', 'valid']:\n        metrics = runner.epoch_metrics[loader]\n        loss_1 = metrics['loss_bce']\n        loss_2 = metrics['loss']\n        assert np.abs(loss_1 - loss_2) < 1e-05",
            "def test_aggregation_2():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Aggregation with custom function\\n    '\n    (loaders, model, criterion, optimizer) = prepare_experiment()\n    runner = dl.SupervisedRunner()\n\n    def aggregation_function(metrics, runner):\n        epoch = runner.epoch_step\n        loss = (3 / 2 - epoch / 2) * metrics['loss_focal'] + (1 / 2 * epoch - 1 / 2) * metrics['loss_bce']\n        return loss\n    runner.train(model=model, criterion=criterion, optimizer=optimizer, loaders=loaders, logdir='./logs/aggregation_2/', num_epochs=3, callbacks=[dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_bce', criterion_key='bce'), dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_focal', criterion_key='focal'), dl.MetricAggregationCallback(metric_key='loss', mode=aggregation_function)])\n    for loader in ['train', 'valid']:\n        metrics = runner.epoch_metrics[loader]\n        loss_1 = metrics['loss_bce']\n        loss_2 = metrics['loss']\n        assert np.abs(loss_1 - loss_2) < 1e-05",
            "def test_aggregation_2():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Aggregation with custom function\\n    '\n    (loaders, model, criterion, optimizer) = prepare_experiment()\n    runner = dl.SupervisedRunner()\n\n    def aggregation_function(metrics, runner):\n        epoch = runner.epoch_step\n        loss = (3 / 2 - epoch / 2) * metrics['loss_focal'] + (1 / 2 * epoch - 1 / 2) * metrics['loss_bce']\n        return loss\n    runner.train(model=model, criterion=criterion, optimizer=optimizer, loaders=loaders, logdir='./logs/aggregation_2/', num_epochs=3, callbacks=[dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_bce', criterion_key='bce'), dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_focal', criterion_key='focal'), dl.MetricAggregationCallback(metric_key='loss', mode=aggregation_function)])\n    for loader in ['train', 'valid']:\n        metrics = runner.epoch_metrics[loader]\n        loss_1 = metrics['loss_bce']\n        loss_2 = metrics['loss']\n        assert np.abs(loss_1 - loss_2) < 1e-05",
            "def test_aggregation_2():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Aggregation with custom function\\n    '\n    (loaders, model, criterion, optimizer) = prepare_experiment()\n    runner = dl.SupervisedRunner()\n\n    def aggregation_function(metrics, runner):\n        epoch = runner.epoch_step\n        loss = (3 / 2 - epoch / 2) * metrics['loss_focal'] + (1 / 2 * epoch - 1 / 2) * metrics['loss_bce']\n        return loss\n    runner.train(model=model, criterion=criterion, optimizer=optimizer, loaders=loaders, logdir='./logs/aggregation_2/', num_epochs=3, callbacks=[dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_bce', criterion_key='bce'), dl.CriterionCallback(input_key='logits', target_key='targets', metric_key='loss_focal', criterion_key='focal'), dl.MetricAggregationCallback(metric_key='loss', mode=aggregation_function)])\n    for loader in ['train', 'valid']:\n        metrics = runner.epoch_metrics[loader]\n        loss_1 = metrics['loss_bce']\n        loss_2 = metrics['loss']\n        assert np.abs(loss_1 - loss_2) < 1e-05"
        ]
    }
]