[
    {
        "func_name": "ref",
        "original": "def ref(lr, grad, effgrad):\n    flattened_grad = grad.flatten()\n    flattened_effgrad = effgrad.flatten()\n    x = np.dot(flattened_grad, flattened_effgrad)\n    kEps = 1e-12\n    y = np.linalg.norm(flattened_grad, ord=2)\n    y = np.maximum(y, kEps)\n    z = np.linalg.norm(flattened_effgrad, ord=2)\n    z = np.maximum(z, kEps)\n    output_lr = lr\n    output_lr[0] -= lr[0] * lr_alpha * float(x / (y * z))\n    return (output_lr,)",
        "mutated": [
            "def ref(lr, grad, effgrad):\n    if False:\n        i = 10\n    flattened_grad = grad.flatten()\n    flattened_effgrad = effgrad.flatten()\n    x = np.dot(flattened_grad, flattened_effgrad)\n    kEps = 1e-12\n    y = np.linalg.norm(flattened_grad, ord=2)\n    y = np.maximum(y, kEps)\n    z = np.linalg.norm(flattened_effgrad, ord=2)\n    z = np.maximum(z, kEps)\n    output_lr = lr\n    output_lr[0] -= lr[0] * lr_alpha * float(x / (y * z))\n    return (output_lr,)",
            "def ref(lr, grad, effgrad):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    flattened_grad = grad.flatten()\n    flattened_effgrad = effgrad.flatten()\n    x = np.dot(flattened_grad, flattened_effgrad)\n    kEps = 1e-12\n    y = np.linalg.norm(flattened_grad, ord=2)\n    y = np.maximum(y, kEps)\n    z = np.linalg.norm(flattened_effgrad, ord=2)\n    z = np.maximum(z, kEps)\n    output_lr = lr\n    output_lr[0] -= lr[0] * lr_alpha * float(x / (y * z))\n    return (output_lr,)",
            "def ref(lr, grad, effgrad):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    flattened_grad = grad.flatten()\n    flattened_effgrad = effgrad.flatten()\n    x = np.dot(flattened_grad, flattened_effgrad)\n    kEps = 1e-12\n    y = np.linalg.norm(flattened_grad, ord=2)\n    y = np.maximum(y, kEps)\n    z = np.linalg.norm(flattened_effgrad, ord=2)\n    z = np.maximum(z, kEps)\n    output_lr = lr\n    output_lr[0] -= lr[0] * lr_alpha * float(x / (y * z))\n    return (output_lr,)",
            "def ref(lr, grad, effgrad):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    flattened_grad = grad.flatten()\n    flattened_effgrad = effgrad.flatten()\n    x = np.dot(flattened_grad, flattened_effgrad)\n    kEps = 1e-12\n    y = np.linalg.norm(flattened_grad, ord=2)\n    y = np.maximum(y, kEps)\n    z = np.linalg.norm(flattened_effgrad, ord=2)\n    z = np.maximum(z, kEps)\n    output_lr = lr\n    output_lr[0] -= lr[0] * lr_alpha * float(x / (y * z))\n    return (output_lr,)",
            "def ref(lr, grad, effgrad):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    flattened_grad = grad.flatten()\n    flattened_effgrad = effgrad.flatten()\n    x = np.dot(flattened_grad, flattened_effgrad)\n    kEps = 1e-12\n    y = np.linalg.norm(flattened_grad, ord=2)\n    y = np.maximum(y, kEps)\n    z = np.linalg.norm(flattened_effgrad, ord=2)\n    z = np.maximum(z, kEps)\n    output_lr = lr\n    output_lr[0] -= lr[0] * lr_alpha * float(x / (y * z))\n    return (output_lr,)"
        ]
    },
    {
        "func_name": "test_learning_rate_adaption_op_normalization",
        "original": "@given(inputs=hu.tensors(n=2), lr=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), lr_alpha=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), **hu.gcs_cpu_only)\n@settings(deadline=None, max_examples=50)\ndef test_learning_rate_adaption_op_normalization(self, inputs, lr, lr_alpha, gc, dc):\n    (grad, effgrad) = inputs\n    lr = np.array([lr], dtype=np.float32)\n    op = core.CreateOperator('LearningRateAdaption', ['lr', 'grad', 'effgrad'], ['output_lr'], lr_alpha=lr_alpha)\n\n    def ref(lr, grad, effgrad):\n        flattened_grad = grad.flatten()\n        flattened_effgrad = effgrad.flatten()\n        x = np.dot(flattened_grad, flattened_effgrad)\n        kEps = 1e-12\n        y = np.linalg.norm(flattened_grad, ord=2)\n        y = np.maximum(y, kEps)\n        z = np.linalg.norm(flattened_effgrad, ord=2)\n        z = np.maximum(z, kEps)\n        output_lr = lr\n        output_lr[0] -= lr[0] * lr_alpha * float(x / (y * z))\n        return (output_lr,)\n    self.assertReferenceChecks(gc, op, [lr, grad, effgrad], ref)",
        "mutated": [
            "@given(inputs=hu.tensors(n=2), lr=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), lr_alpha=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), **hu.gcs_cpu_only)\n@settings(deadline=None, max_examples=50)\ndef test_learning_rate_adaption_op_normalization(self, inputs, lr, lr_alpha, gc, dc):\n    if False:\n        i = 10\n    (grad, effgrad) = inputs\n    lr = np.array([lr], dtype=np.float32)\n    op = core.CreateOperator('LearningRateAdaption', ['lr', 'grad', 'effgrad'], ['output_lr'], lr_alpha=lr_alpha)\n\n    def ref(lr, grad, effgrad):\n        flattened_grad = grad.flatten()\n        flattened_effgrad = effgrad.flatten()\n        x = np.dot(flattened_grad, flattened_effgrad)\n        kEps = 1e-12\n        y = np.linalg.norm(flattened_grad, ord=2)\n        y = np.maximum(y, kEps)\n        z = np.linalg.norm(flattened_effgrad, ord=2)\n        z = np.maximum(z, kEps)\n        output_lr = lr\n        output_lr[0] -= lr[0] * lr_alpha * float(x / (y * z))\n        return (output_lr,)\n    self.assertReferenceChecks(gc, op, [lr, grad, effgrad], ref)",
            "@given(inputs=hu.tensors(n=2), lr=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), lr_alpha=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), **hu.gcs_cpu_only)\n@settings(deadline=None, max_examples=50)\ndef test_learning_rate_adaption_op_normalization(self, inputs, lr, lr_alpha, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (grad, effgrad) = inputs\n    lr = np.array([lr], dtype=np.float32)\n    op = core.CreateOperator('LearningRateAdaption', ['lr', 'grad', 'effgrad'], ['output_lr'], lr_alpha=lr_alpha)\n\n    def ref(lr, grad, effgrad):\n        flattened_grad = grad.flatten()\n        flattened_effgrad = effgrad.flatten()\n        x = np.dot(flattened_grad, flattened_effgrad)\n        kEps = 1e-12\n        y = np.linalg.norm(flattened_grad, ord=2)\n        y = np.maximum(y, kEps)\n        z = np.linalg.norm(flattened_effgrad, ord=2)\n        z = np.maximum(z, kEps)\n        output_lr = lr\n        output_lr[0] -= lr[0] * lr_alpha * float(x / (y * z))\n        return (output_lr,)\n    self.assertReferenceChecks(gc, op, [lr, grad, effgrad], ref)",
            "@given(inputs=hu.tensors(n=2), lr=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), lr_alpha=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), **hu.gcs_cpu_only)\n@settings(deadline=None, max_examples=50)\ndef test_learning_rate_adaption_op_normalization(self, inputs, lr, lr_alpha, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (grad, effgrad) = inputs\n    lr = np.array([lr], dtype=np.float32)\n    op = core.CreateOperator('LearningRateAdaption', ['lr', 'grad', 'effgrad'], ['output_lr'], lr_alpha=lr_alpha)\n\n    def ref(lr, grad, effgrad):\n        flattened_grad = grad.flatten()\n        flattened_effgrad = effgrad.flatten()\n        x = np.dot(flattened_grad, flattened_effgrad)\n        kEps = 1e-12\n        y = np.linalg.norm(flattened_grad, ord=2)\n        y = np.maximum(y, kEps)\n        z = np.linalg.norm(flattened_effgrad, ord=2)\n        z = np.maximum(z, kEps)\n        output_lr = lr\n        output_lr[0] -= lr[0] * lr_alpha * float(x / (y * z))\n        return (output_lr,)\n    self.assertReferenceChecks(gc, op, [lr, grad, effgrad], ref)",
            "@given(inputs=hu.tensors(n=2), lr=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), lr_alpha=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), **hu.gcs_cpu_only)\n@settings(deadline=None, max_examples=50)\ndef test_learning_rate_adaption_op_normalization(self, inputs, lr, lr_alpha, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (grad, effgrad) = inputs\n    lr = np.array([lr], dtype=np.float32)\n    op = core.CreateOperator('LearningRateAdaption', ['lr', 'grad', 'effgrad'], ['output_lr'], lr_alpha=lr_alpha)\n\n    def ref(lr, grad, effgrad):\n        flattened_grad = grad.flatten()\n        flattened_effgrad = effgrad.flatten()\n        x = np.dot(flattened_grad, flattened_effgrad)\n        kEps = 1e-12\n        y = np.linalg.norm(flattened_grad, ord=2)\n        y = np.maximum(y, kEps)\n        z = np.linalg.norm(flattened_effgrad, ord=2)\n        z = np.maximum(z, kEps)\n        output_lr = lr\n        output_lr[0] -= lr[0] * lr_alpha * float(x / (y * z))\n        return (output_lr,)\n    self.assertReferenceChecks(gc, op, [lr, grad, effgrad], ref)",
            "@given(inputs=hu.tensors(n=2), lr=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), lr_alpha=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), **hu.gcs_cpu_only)\n@settings(deadline=None, max_examples=50)\ndef test_learning_rate_adaption_op_normalization(self, inputs, lr, lr_alpha, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (grad, effgrad) = inputs\n    lr = np.array([lr], dtype=np.float32)\n    op = core.CreateOperator('LearningRateAdaption', ['lr', 'grad', 'effgrad'], ['output_lr'], lr_alpha=lr_alpha)\n\n    def ref(lr, grad, effgrad):\n        flattened_grad = grad.flatten()\n        flattened_effgrad = effgrad.flatten()\n        x = np.dot(flattened_grad, flattened_effgrad)\n        kEps = 1e-12\n        y = np.linalg.norm(flattened_grad, ord=2)\n        y = np.maximum(y, kEps)\n        z = np.linalg.norm(flattened_effgrad, ord=2)\n        z = np.maximum(z, kEps)\n        output_lr = lr\n        output_lr[0] -= lr[0] * lr_alpha * float(x / (y * z))\n        return (output_lr,)\n    self.assertReferenceChecks(gc, op, [lr, grad, effgrad], ref)"
        ]
    },
    {
        "func_name": "ref",
        "original": "def ref(lr, grad, effgrad):\n    flattened_grad = grad.flatten()\n    flattened_effgrad = effgrad.flatten()\n    x = np.dot(flattened_grad, flattened_effgrad)\n    output_lr = lr\n    output_lr[0] -= lr_alpha * x\n    return (output_lr,)",
        "mutated": [
            "def ref(lr, grad, effgrad):\n    if False:\n        i = 10\n    flattened_grad = grad.flatten()\n    flattened_effgrad = effgrad.flatten()\n    x = np.dot(flattened_grad, flattened_effgrad)\n    output_lr = lr\n    output_lr[0] -= lr_alpha * x\n    return (output_lr,)",
            "def ref(lr, grad, effgrad):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    flattened_grad = grad.flatten()\n    flattened_effgrad = effgrad.flatten()\n    x = np.dot(flattened_grad, flattened_effgrad)\n    output_lr = lr\n    output_lr[0] -= lr_alpha * x\n    return (output_lr,)",
            "def ref(lr, grad, effgrad):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    flattened_grad = grad.flatten()\n    flattened_effgrad = effgrad.flatten()\n    x = np.dot(flattened_grad, flattened_effgrad)\n    output_lr = lr\n    output_lr[0] -= lr_alpha * x\n    return (output_lr,)",
            "def ref(lr, grad, effgrad):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    flattened_grad = grad.flatten()\n    flattened_effgrad = effgrad.flatten()\n    x = np.dot(flattened_grad, flattened_effgrad)\n    output_lr = lr\n    output_lr[0] -= lr_alpha * x\n    return (output_lr,)",
            "def ref(lr, grad, effgrad):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    flattened_grad = grad.flatten()\n    flattened_effgrad = effgrad.flatten()\n    x = np.dot(flattened_grad, flattened_effgrad)\n    output_lr = lr\n    output_lr[0] -= lr_alpha * x\n    return (output_lr,)"
        ]
    },
    {
        "func_name": "test_learning_rate_adaption_op_without_normalization",
        "original": "@given(inputs=hu.tensors(n=2), lr=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), lr_alpha=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), **hu.gcs_cpu_only)\ndef test_learning_rate_adaption_op_without_normalization(self, inputs, lr, lr_alpha, gc, dc):\n    (grad, effgrad) = inputs\n    lr = np.array([lr], dtype=np.float32)\n    op = core.CreateOperator('LearningRateAdaption', ['lr', 'grad', 'effgrad'], ['output_lr'], lr_alpha=lr_alpha, normalized_lr_adaption=False)\n\n    def ref(lr, grad, effgrad):\n        flattened_grad = grad.flatten()\n        flattened_effgrad = effgrad.flatten()\n        x = np.dot(flattened_grad, flattened_effgrad)\n        output_lr = lr\n        output_lr[0] -= lr_alpha * x\n        return (output_lr,)\n    self.assertReferenceChecks(gc, op, [lr, grad, effgrad], ref)",
        "mutated": [
            "@given(inputs=hu.tensors(n=2), lr=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), lr_alpha=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), **hu.gcs_cpu_only)\ndef test_learning_rate_adaption_op_without_normalization(self, inputs, lr, lr_alpha, gc, dc):\n    if False:\n        i = 10\n    (grad, effgrad) = inputs\n    lr = np.array([lr], dtype=np.float32)\n    op = core.CreateOperator('LearningRateAdaption', ['lr', 'grad', 'effgrad'], ['output_lr'], lr_alpha=lr_alpha, normalized_lr_adaption=False)\n\n    def ref(lr, grad, effgrad):\n        flattened_grad = grad.flatten()\n        flattened_effgrad = effgrad.flatten()\n        x = np.dot(flattened_grad, flattened_effgrad)\n        output_lr = lr\n        output_lr[0] -= lr_alpha * x\n        return (output_lr,)\n    self.assertReferenceChecks(gc, op, [lr, grad, effgrad], ref)",
            "@given(inputs=hu.tensors(n=2), lr=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), lr_alpha=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), **hu.gcs_cpu_only)\ndef test_learning_rate_adaption_op_without_normalization(self, inputs, lr, lr_alpha, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (grad, effgrad) = inputs\n    lr = np.array([lr], dtype=np.float32)\n    op = core.CreateOperator('LearningRateAdaption', ['lr', 'grad', 'effgrad'], ['output_lr'], lr_alpha=lr_alpha, normalized_lr_adaption=False)\n\n    def ref(lr, grad, effgrad):\n        flattened_grad = grad.flatten()\n        flattened_effgrad = effgrad.flatten()\n        x = np.dot(flattened_grad, flattened_effgrad)\n        output_lr = lr\n        output_lr[0] -= lr_alpha * x\n        return (output_lr,)\n    self.assertReferenceChecks(gc, op, [lr, grad, effgrad], ref)",
            "@given(inputs=hu.tensors(n=2), lr=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), lr_alpha=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), **hu.gcs_cpu_only)\ndef test_learning_rate_adaption_op_without_normalization(self, inputs, lr, lr_alpha, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (grad, effgrad) = inputs\n    lr = np.array([lr], dtype=np.float32)\n    op = core.CreateOperator('LearningRateAdaption', ['lr', 'grad', 'effgrad'], ['output_lr'], lr_alpha=lr_alpha, normalized_lr_adaption=False)\n\n    def ref(lr, grad, effgrad):\n        flattened_grad = grad.flatten()\n        flattened_effgrad = effgrad.flatten()\n        x = np.dot(flattened_grad, flattened_effgrad)\n        output_lr = lr\n        output_lr[0] -= lr_alpha * x\n        return (output_lr,)\n    self.assertReferenceChecks(gc, op, [lr, grad, effgrad], ref)",
            "@given(inputs=hu.tensors(n=2), lr=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), lr_alpha=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), **hu.gcs_cpu_only)\ndef test_learning_rate_adaption_op_without_normalization(self, inputs, lr, lr_alpha, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (grad, effgrad) = inputs\n    lr = np.array([lr], dtype=np.float32)\n    op = core.CreateOperator('LearningRateAdaption', ['lr', 'grad', 'effgrad'], ['output_lr'], lr_alpha=lr_alpha, normalized_lr_adaption=False)\n\n    def ref(lr, grad, effgrad):\n        flattened_grad = grad.flatten()\n        flattened_effgrad = effgrad.flatten()\n        x = np.dot(flattened_grad, flattened_effgrad)\n        output_lr = lr\n        output_lr[0] -= lr_alpha * x\n        return (output_lr,)\n    self.assertReferenceChecks(gc, op, [lr, grad, effgrad], ref)",
            "@given(inputs=hu.tensors(n=2), lr=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), lr_alpha=st.floats(min_value=0.01, max_value=0.99, allow_nan=False, allow_infinity=False), **hu.gcs_cpu_only)\ndef test_learning_rate_adaption_op_without_normalization(self, inputs, lr, lr_alpha, gc, dc):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (grad, effgrad) = inputs\n    lr = np.array([lr], dtype=np.float32)\n    op = core.CreateOperator('LearningRateAdaption', ['lr', 'grad', 'effgrad'], ['output_lr'], lr_alpha=lr_alpha, normalized_lr_adaption=False)\n\n    def ref(lr, grad, effgrad):\n        flattened_grad = grad.flatten()\n        flattened_effgrad = effgrad.flatten()\n        x = np.dot(flattened_grad, flattened_effgrad)\n        output_lr = lr\n        output_lr[0] -= lr_alpha * x\n        return (output_lr,)\n    self.assertReferenceChecks(gc, op, [lr, grad, effgrad], ref)"
        ]
    }
]