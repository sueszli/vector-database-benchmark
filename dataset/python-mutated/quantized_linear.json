[
    {
        "func_name": "weight_quantize",
        "original": "def weight_quantize(x, algo='weight_only_int8'):\n    \"\"\"\n    Quantization function for weight_only and llm.int8's weight.\n\n    Args:\n        x (Tensor): The input Tensor to be quantized, the data type is float16 or bfloat16.\n        algo (str): The algo that is x will be apply, must be one of 'weight_only_int8',\n            'weight_only_int4' and 'llm.int8', default: 'weight_only_int8'.\n\n    Returns:\n        out (Tensor): The Tensor which is the quantitative results, the data type is int8, the shape is transposition of x.\n        scale (Tensor): The scale Tensor which is the scale of pre-channel, the data type is float32.\n    Examples:\n        .. code-block:: python\n\n            >>> # doctest: +SKIP('No testing required')\n            >>> import paddle\n            >>> from paddle.nn.quant import weight_quantize\n\n            >>> paddle.seed(2023)\n            >>> x = paddle.rand(shape=[64, 32], dtype=paddle.float16)\n            >>> out, scale = weight_quantize(x, algo='weight_only_int8')\n            >>> print(out.shape)\n            [32, 64]\n            >>> print(scale.shape)\n            [32]\n    \"\"\"\n    if in_dynamic_mode():\n        return _C_ops.weight_quantize(x, algo)\n    else:\n        type = 'weight_quantize'\n        helper = LayerHelper(type, **locals())\n        out = helper.create_variable_for_type_inference('int8')\n        scale = helper.create_variable_for_type_inference('float')\n        helper.append_op(type=type, inputs={'x': x}, outputs={'out': out, 'scale': scale}, attrs={'algo': algo})\n        return (out, scale)",
        "mutated": [
            "def weight_quantize(x, algo='weight_only_int8'):\n    if False:\n        i = 10\n    \"\\n    Quantization function for weight_only and llm.int8's weight.\\n\\n    Args:\\n        x (Tensor): The input Tensor to be quantized, the data type is float16 or bfloat16.\\n        algo (str): The algo that is x will be apply, must be one of 'weight_only_int8',\\n            'weight_only_int4' and 'llm.int8', default: 'weight_only_int8'.\\n\\n    Returns:\\n        out (Tensor): The Tensor which is the quantitative results, the data type is int8, the shape is transposition of x.\\n        scale (Tensor): The scale Tensor which is the scale of pre-channel, the data type is float32.\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_quantize\\n\\n            >>> paddle.seed(2023)\\n            >>> x = paddle.rand(shape=[64, 32], dtype=paddle.float16)\\n            >>> out, scale = weight_quantize(x, algo='weight_only_int8')\\n            >>> print(out.shape)\\n            [32, 64]\\n            >>> print(scale.shape)\\n            [32]\\n    \"\n    if in_dynamic_mode():\n        return _C_ops.weight_quantize(x, algo)\n    else:\n        type = 'weight_quantize'\n        helper = LayerHelper(type, **locals())\n        out = helper.create_variable_for_type_inference('int8')\n        scale = helper.create_variable_for_type_inference('float')\n        helper.append_op(type=type, inputs={'x': x}, outputs={'out': out, 'scale': scale}, attrs={'algo': algo})\n        return (out, scale)",
            "def weight_quantize(x, algo='weight_only_int8'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Quantization function for weight_only and llm.int8's weight.\\n\\n    Args:\\n        x (Tensor): The input Tensor to be quantized, the data type is float16 or bfloat16.\\n        algo (str): The algo that is x will be apply, must be one of 'weight_only_int8',\\n            'weight_only_int4' and 'llm.int8', default: 'weight_only_int8'.\\n\\n    Returns:\\n        out (Tensor): The Tensor which is the quantitative results, the data type is int8, the shape is transposition of x.\\n        scale (Tensor): The scale Tensor which is the scale of pre-channel, the data type is float32.\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_quantize\\n\\n            >>> paddle.seed(2023)\\n            >>> x = paddle.rand(shape=[64, 32], dtype=paddle.float16)\\n            >>> out, scale = weight_quantize(x, algo='weight_only_int8')\\n            >>> print(out.shape)\\n            [32, 64]\\n            >>> print(scale.shape)\\n            [32]\\n    \"\n    if in_dynamic_mode():\n        return _C_ops.weight_quantize(x, algo)\n    else:\n        type = 'weight_quantize'\n        helper = LayerHelper(type, **locals())\n        out = helper.create_variable_for_type_inference('int8')\n        scale = helper.create_variable_for_type_inference('float')\n        helper.append_op(type=type, inputs={'x': x}, outputs={'out': out, 'scale': scale}, attrs={'algo': algo})\n        return (out, scale)",
            "def weight_quantize(x, algo='weight_only_int8'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Quantization function for weight_only and llm.int8's weight.\\n\\n    Args:\\n        x (Tensor): The input Tensor to be quantized, the data type is float16 or bfloat16.\\n        algo (str): The algo that is x will be apply, must be one of 'weight_only_int8',\\n            'weight_only_int4' and 'llm.int8', default: 'weight_only_int8'.\\n\\n    Returns:\\n        out (Tensor): The Tensor which is the quantitative results, the data type is int8, the shape is transposition of x.\\n        scale (Tensor): The scale Tensor which is the scale of pre-channel, the data type is float32.\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_quantize\\n\\n            >>> paddle.seed(2023)\\n            >>> x = paddle.rand(shape=[64, 32], dtype=paddle.float16)\\n            >>> out, scale = weight_quantize(x, algo='weight_only_int8')\\n            >>> print(out.shape)\\n            [32, 64]\\n            >>> print(scale.shape)\\n            [32]\\n    \"\n    if in_dynamic_mode():\n        return _C_ops.weight_quantize(x, algo)\n    else:\n        type = 'weight_quantize'\n        helper = LayerHelper(type, **locals())\n        out = helper.create_variable_for_type_inference('int8')\n        scale = helper.create_variable_for_type_inference('float')\n        helper.append_op(type=type, inputs={'x': x}, outputs={'out': out, 'scale': scale}, attrs={'algo': algo})\n        return (out, scale)",
            "def weight_quantize(x, algo='weight_only_int8'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Quantization function for weight_only and llm.int8's weight.\\n\\n    Args:\\n        x (Tensor): The input Tensor to be quantized, the data type is float16 or bfloat16.\\n        algo (str): The algo that is x will be apply, must be one of 'weight_only_int8',\\n            'weight_only_int4' and 'llm.int8', default: 'weight_only_int8'.\\n\\n    Returns:\\n        out (Tensor): The Tensor which is the quantitative results, the data type is int8, the shape is transposition of x.\\n        scale (Tensor): The scale Tensor which is the scale of pre-channel, the data type is float32.\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_quantize\\n\\n            >>> paddle.seed(2023)\\n            >>> x = paddle.rand(shape=[64, 32], dtype=paddle.float16)\\n            >>> out, scale = weight_quantize(x, algo='weight_only_int8')\\n            >>> print(out.shape)\\n            [32, 64]\\n            >>> print(scale.shape)\\n            [32]\\n    \"\n    if in_dynamic_mode():\n        return _C_ops.weight_quantize(x, algo)\n    else:\n        type = 'weight_quantize'\n        helper = LayerHelper(type, **locals())\n        out = helper.create_variable_for_type_inference('int8')\n        scale = helper.create_variable_for_type_inference('float')\n        helper.append_op(type=type, inputs={'x': x}, outputs={'out': out, 'scale': scale}, attrs={'algo': algo})\n        return (out, scale)",
            "def weight_quantize(x, algo='weight_only_int8'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Quantization function for weight_only and llm.int8's weight.\\n\\n    Args:\\n        x (Tensor): The input Tensor to be quantized, the data type is float16 or bfloat16.\\n        algo (str): The algo that is x will be apply, must be one of 'weight_only_int8',\\n            'weight_only_int4' and 'llm.int8', default: 'weight_only_int8'.\\n\\n    Returns:\\n        out (Tensor): The Tensor which is the quantitative results, the data type is int8, the shape is transposition of x.\\n        scale (Tensor): The scale Tensor which is the scale of pre-channel, the data type is float32.\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_quantize\\n\\n            >>> paddle.seed(2023)\\n            >>> x = paddle.rand(shape=[64, 32], dtype=paddle.float16)\\n            >>> out, scale = weight_quantize(x, algo='weight_only_int8')\\n            >>> print(out.shape)\\n            [32, 64]\\n            >>> print(scale.shape)\\n            [32]\\n    \"\n    if in_dynamic_mode():\n        return _C_ops.weight_quantize(x, algo)\n    else:\n        type = 'weight_quantize'\n        helper = LayerHelper(type, **locals())\n        out = helper.create_variable_for_type_inference('int8')\n        scale = helper.create_variable_for_type_inference('float')\n        helper.append_op(type=type, inputs={'x': x}, outputs={'out': out, 'scale': scale}, attrs={'algo': algo})\n        return (out, scale)"
        ]
    },
    {
        "func_name": "weight_dequantize",
        "original": "def weight_dequantize(x, scale, algo='weight_only_int8', out_dtype='float16'):\n    \"\"\"\n    Dequantization function for weight_only and llm.int8's weight.\n\n    Args:\n        x (Tensor): The input Tensor to be dequantized, the data type is int8.\n        scale (Tensor): The scale Tensor which is the output of weight_quantize, the data type is float32.\n        algo (str): The algo that is x will be apply, must be one of 'weight_only_int8',\n            'weight_only_int4' and 'llm.int8', default: 'weight_only_int8'.\n        out_dtype (str|np.dtype): The output Tensor's data type, must be one of 'float16' and 'bfloat16', default: 'float16'.\n\n    Returns:\n        out (Tensor): The Tensor which is the dequantitative results, the data type is float16 or bfloat16, the shape is transposition of x.\n\n    Examples:\n        .. code-block:: python\n\n            >>> # doctest: +SKIP('No testing required')\n            >>> import paddle\n            >>> from paddle.nn.quant import weight_quantize, weight_dequantize\n\n            >>> paddle.seed(2023)\n            >>> x = paddle.rand(shape=[64, 32], dtype=paddle.float16)\n            >>> out, scale = weight_quantize(x, algo='weight_only_int8')\n            >>> x_dequant = weight_dequantize(out, scale)\n    \"\"\"\n    check_dtype(out_dtype, 'out_dtype', ['float16', 'bfloat16'], 'weight_dequantize')\n    out_dtype = convert_np_dtype_to_dtype_(out_dtype)\n    if in_dynamic_mode():\n        return _C_ops.weight_dequantize(x, scale, algo, out_dtype)\n    else:\n        type = 'weight_dequantize'\n        helper = LayerHelper(type, **locals())\n        out = helper.create_variable_for_type_inference(out_dtype)\n        helper.append_op(type=type, inputs={'x': x, 'scale': scale}, outputs={'out': out}, attrs={'algo': algo, 'out_dtype': out_dtype})\n        return out",
        "mutated": [
            "def weight_dequantize(x, scale, algo='weight_only_int8', out_dtype='float16'):\n    if False:\n        i = 10\n    \"\\n    Dequantization function for weight_only and llm.int8's weight.\\n\\n    Args:\\n        x (Tensor): The input Tensor to be dequantized, the data type is int8.\\n        scale (Tensor): The scale Tensor which is the output of weight_quantize, the data type is float32.\\n        algo (str): The algo that is x will be apply, must be one of 'weight_only_int8',\\n            'weight_only_int4' and 'llm.int8', default: 'weight_only_int8'.\\n        out_dtype (str|np.dtype): The output Tensor's data type, must be one of 'float16' and 'bfloat16', default: 'float16'.\\n\\n    Returns:\\n        out (Tensor): The Tensor which is the dequantitative results, the data type is float16 or bfloat16, the shape is transposition of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_quantize, weight_dequantize\\n\\n            >>> paddle.seed(2023)\\n            >>> x = paddle.rand(shape=[64, 32], dtype=paddle.float16)\\n            >>> out, scale = weight_quantize(x, algo='weight_only_int8')\\n            >>> x_dequant = weight_dequantize(out, scale)\\n    \"\n    check_dtype(out_dtype, 'out_dtype', ['float16', 'bfloat16'], 'weight_dequantize')\n    out_dtype = convert_np_dtype_to_dtype_(out_dtype)\n    if in_dynamic_mode():\n        return _C_ops.weight_dequantize(x, scale, algo, out_dtype)\n    else:\n        type = 'weight_dequantize'\n        helper = LayerHelper(type, **locals())\n        out = helper.create_variable_for_type_inference(out_dtype)\n        helper.append_op(type=type, inputs={'x': x, 'scale': scale}, outputs={'out': out}, attrs={'algo': algo, 'out_dtype': out_dtype})\n        return out",
            "def weight_dequantize(x, scale, algo='weight_only_int8', out_dtype='float16'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Dequantization function for weight_only and llm.int8's weight.\\n\\n    Args:\\n        x (Tensor): The input Tensor to be dequantized, the data type is int8.\\n        scale (Tensor): The scale Tensor which is the output of weight_quantize, the data type is float32.\\n        algo (str): The algo that is x will be apply, must be one of 'weight_only_int8',\\n            'weight_only_int4' and 'llm.int8', default: 'weight_only_int8'.\\n        out_dtype (str|np.dtype): The output Tensor's data type, must be one of 'float16' and 'bfloat16', default: 'float16'.\\n\\n    Returns:\\n        out (Tensor): The Tensor which is the dequantitative results, the data type is float16 or bfloat16, the shape is transposition of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_quantize, weight_dequantize\\n\\n            >>> paddle.seed(2023)\\n            >>> x = paddle.rand(shape=[64, 32], dtype=paddle.float16)\\n            >>> out, scale = weight_quantize(x, algo='weight_only_int8')\\n            >>> x_dequant = weight_dequantize(out, scale)\\n    \"\n    check_dtype(out_dtype, 'out_dtype', ['float16', 'bfloat16'], 'weight_dequantize')\n    out_dtype = convert_np_dtype_to_dtype_(out_dtype)\n    if in_dynamic_mode():\n        return _C_ops.weight_dequantize(x, scale, algo, out_dtype)\n    else:\n        type = 'weight_dequantize'\n        helper = LayerHelper(type, **locals())\n        out = helper.create_variable_for_type_inference(out_dtype)\n        helper.append_op(type=type, inputs={'x': x, 'scale': scale}, outputs={'out': out}, attrs={'algo': algo, 'out_dtype': out_dtype})\n        return out",
            "def weight_dequantize(x, scale, algo='weight_only_int8', out_dtype='float16'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Dequantization function for weight_only and llm.int8's weight.\\n\\n    Args:\\n        x (Tensor): The input Tensor to be dequantized, the data type is int8.\\n        scale (Tensor): The scale Tensor which is the output of weight_quantize, the data type is float32.\\n        algo (str): The algo that is x will be apply, must be one of 'weight_only_int8',\\n            'weight_only_int4' and 'llm.int8', default: 'weight_only_int8'.\\n        out_dtype (str|np.dtype): The output Tensor's data type, must be one of 'float16' and 'bfloat16', default: 'float16'.\\n\\n    Returns:\\n        out (Tensor): The Tensor which is the dequantitative results, the data type is float16 or bfloat16, the shape is transposition of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_quantize, weight_dequantize\\n\\n            >>> paddle.seed(2023)\\n            >>> x = paddle.rand(shape=[64, 32], dtype=paddle.float16)\\n            >>> out, scale = weight_quantize(x, algo='weight_only_int8')\\n            >>> x_dequant = weight_dequantize(out, scale)\\n    \"\n    check_dtype(out_dtype, 'out_dtype', ['float16', 'bfloat16'], 'weight_dequantize')\n    out_dtype = convert_np_dtype_to_dtype_(out_dtype)\n    if in_dynamic_mode():\n        return _C_ops.weight_dequantize(x, scale, algo, out_dtype)\n    else:\n        type = 'weight_dequantize'\n        helper = LayerHelper(type, **locals())\n        out = helper.create_variable_for_type_inference(out_dtype)\n        helper.append_op(type=type, inputs={'x': x, 'scale': scale}, outputs={'out': out}, attrs={'algo': algo, 'out_dtype': out_dtype})\n        return out",
            "def weight_dequantize(x, scale, algo='weight_only_int8', out_dtype='float16'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Dequantization function for weight_only and llm.int8's weight.\\n\\n    Args:\\n        x (Tensor): The input Tensor to be dequantized, the data type is int8.\\n        scale (Tensor): The scale Tensor which is the output of weight_quantize, the data type is float32.\\n        algo (str): The algo that is x will be apply, must be one of 'weight_only_int8',\\n            'weight_only_int4' and 'llm.int8', default: 'weight_only_int8'.\\n        out_dtype (str|np.dtype): The output Tensor's data type, must be one of 'float16' and 'bfloat16', default: 'float16'.\\n\\n    Returns:\\n        out (Tensor): The Tensor which is the dequantitative results, the data type is float16 or bfloat16, the shape is transposition of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_quantize, weight_dequantize\\n\\n            >>> paddle.seed(2023)\\n            >>> x = paddle.rand(shape=[64, 32], dtype=paddle.float16)\\n            >>> out, scale = weight_quantize(x, algo='weight_only_int8')\\n            >>> x_dequant = weight_dequantize(out, scale)\\n    \"\n    check_dtype(out_dtype, 'out_dtype', ['float16', 'bfloat16'], 'weight_dequantize')\n    out_dtype = convert_np_dtype_to_dtype_(out_dtype)\n    if in_dynamic_mode():\n        return _C_ops.weight_dequantize(x, scale, algo, out_dtype)\n    else:\n        type = 'weight_dequantize'\n        helper = LayerHelper(type, **locals())\n        out = helper.create_variable_for_type_inference(out_dtype)\n        helper.append_op(type=type, inputs={'x': x, 'scale': scale}, outputs={'out': out}, attrs={'algo': algo, 'out_dtype': out_dtype})\n        return out",
            "def weight_dequantize(x, scale, algo='weight_only_int8', out_dtype='float16'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Dequantization function for weight_only and llm.int8's weight.\\n\\n    Args:\\n        x (Tensor): The input Tensor to be dequantized, the data type is int8.\\n        scale (Tensor): The scale Tensor which is the output of weight_quantize, the data type is float32.\\n        algo (str): The algo that is x will be apply, must be one of 'weight_only_int8',\\n            'weight_only_int4' and 'llm.int8', default: 'weight_only_int8'.\\n        out_dtype (str|np.dtype): The output Tensor's data type, must be one of 'float16' and 'bfloat16', default: 'float16'.\\n\\n    Returns:\\n        out (Tensor): The Tensor which is the dequantitative results, the data type is float16 or bfloat16, the shape is transposition of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_quantize, weight_dequantize\\n\\n            >>> paddle.seed(2023)\\n            >>> x = paddle.rand(shape=[64, 32], dtype=paddle.float16)\\n            >>> out, scale = weight_quantize(x, algo='weight_only_int8')\\n            >>> x_dequant = weight_dequantize(out, scale)\\n    \"\n    check_dtype(out_dtype, 'out_dtype', ['float16', 'bfloat16'], 'weight_dequantize')\n    out_dtype = convert_np_dtype_to_dtype_(out_dtype)\n    if in_dynamic_mode():\n        return _C_ops.weight_dequantize(x, scale, algo, out_dtype)\n    else:\n        type = 'weight_dequantize'\n        helper = LayerHelper(type, **locals())\n        out = helper.create_variable_for_type_inference(out_dtype)\n        helper.append_op(type=type, inputs={'x': x, 'scale': scale}, outputs={'out': out}, attrs={'algo': algo, 'out_dtype': out_dtype})\n        return out"
        ]
    },
    {
        "func_name": "weight_only_linear",
        "original": "def weight_only_linear(x, weight, bias=None, weight_scale=None, weight_dtype='int8'):\n    \"\"\"\n    Applies matrix multiplication of two tensors and then bias addition if provided.\n    This method requires CUDA version >= 11.2.\n\n    Args:\n        x (Tensor): The first input Tensor to be multiplied, the data type is float16 or bfloat16.\n        weight (Tensor): The second input Tensor to be multiplied. Its rank must be 2.\n        bias (Tensor|None): The input bias Tensor. If it is None, no bias addition would\n            be performed. Otherwise, The bias is added to the matrix multiplication result.\n        weight_scale (Tensor|None): The input scale Tensor Provided to weight for dequantization. Its rank must be 1.\n        weight_dtype(str): The dtype of  weight Tensor, must be one of 'int8', 'int4', Defaulted to 'int8'.\n    Returns:\n        Tensor: the output Tensor, the data type is the same as that of x.\n\n    Examples:\n        .. code-block:: python\n\n            >>> # doctest: +SKIP('No testing required')\n            >>> import paddle\n            >>> from paddle.nn.quant import weight_only_linear\n\n            >>> x = paddle.cast(paddle.randn([1, 2, 64]), dtype='float16')\n            >>> weight = paddle.cast(paddle.randint(0, 127, [32, 64]), dtype='int8')\n            >>> scale = paddle.randn([32], dtype='float32')\n            >>> bias = paddle.cast(paddle.randn([32]), dtype='float16')\n            >>> if paddle.device.cuda.get_device_capability()[0] >= 8:\n            ...    out = weight_only_linear(x, weight, bias=bias, weight_scale=scale, weight_dtype='int8')\n            ...    print(out.shape)\n            [1, 2, 32]\n    \"\"\"\n    if in_dynamic_mode():\n        out = _C_ops.weight_only_linear(x, weight, bias, weight_scale, weight_dtype)\n        return out\n    else:\n        check_dtype(weight_dtype, 'weight_dtype', ['int8', 'int4'], 'weight_only_linear')\n        type = 'weight_only_linear'\n        helper = LayerHelper(type, **locals())\n        dtype = x.dtype\n        inputs = {'x': [x], 'weight': [weight], 'weight_scale': [weight_scale]}\n        if bias is not None:\n            inputs['bias'] = [bias]\n        attrs = {'weight_dtype': weight_dtype}\n        out = helper.create_variable_for_type_inference(dtype)\n        helper.append_op(type=type, inputs=inputs, outputs={'out': out}, attrs=attrs)\n        return out",
        "mutated": [
            "def weight_only_linear(x, weight, bias=None, weight_scale=None, weight_dtype='int8'):\n    if False:\n        i = 10\n    \"\\n    Applies matrix multiplication of two tensors and then bias addition if provided.\\n    This method requires CUDA version >= 11.2.\\n\\n    Args:\\n        x (Tensor): The first input Tensor to be multiplied, the data type is float16 or bfloat16.\\n        weight (Tensor): The second input Tensor to be multiplied. Its rank must be 2.\\n        bias (Tensor|None): The input bias Tensor. If it is None, no bias addition would\\n            be performed. Otherwise, The bias is added to the matrix multiplication result.\\n        weight_scale (Tensor|None): The input scale Tensor Provided to weight for dequantization. Its rank must be 1.\\n        weight_dtype(str): The dtype of  weight Tensor, must be one of 'int8', 'int4', Defaulted to 'int8'.\\n    Returns:\\n        Tensor: the output Tensor, the data type is the same as that of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_only_linear\\n\\n            >>> x = paddle.cast(paddle.randn([1, 2, 64]), dtype='float16')\\n            >>> weight = paddle.cast(paddle.randint(0, 127, [32, 64]), dtype='int8')\\n            >>> scale = paddle.randn([32], dtype='float32')\\n            >>> bias = paddle.cast(paddle.randn([32]), dtype='float16')\\n            >>> if paddle.device.cuda.get_device_capability()[0] >= 8:\\n            ...    out = weight_only_linear(x, weight, bias=bias, weight_scale=scale, weight_dtype='int8')\\n            ...    print(out.shape)\\n            [1, 2, 32]\\n    \"\n    if in_dynamic_mode():\n        out = _C_ops.weight_only_linear(x, weight, bias, weight_scale, weight_dtype)\n        return out\n    else:\n        check_dtype(weight_dtype, 'weight_dtype', ['int8', 'int4'], 'weight_only_linear')\n        type = 'weight_only_linear'\n        helper = LayerHelper(type, **locals())\n        dtype = x.dtype\n        inputs = {'x': [x], 'weight': [weight], 'weight_scale': [weight_scale]}\n        if bias is not None:\n            inputs['bias'] = [bias]\n        attrs = {'weight_dtype': weight_dtype}\n        out = helper.create_variable_for_type_inference(dtype)\n        helper.append_op(type=type, inputs=inputs, outputs={'out': out}, attrs=attrs)\n        return out",
            "def weight_only_linear(x, weight, bias=None, weight_scale=None, weight_dtype='int8'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Applies matrix multiplication of two tensors and then bias addition if provided.\\n    This method requires CUDA version >= 11.2.\\n\\n    Args:\\n        x (Tensor): The first input Tensor to be multiplied, the data type is float16 or bfloat16.\\n        weight (Tensor): The second input Tensor to be multiplied. Its rank must be 2.\\n        bias (Tensor|None): The input bias Tensor. If it is None, no bias addition would\\n            be performed. Otherwise, The bias is added to the matrix multiplication result.\\n        weight_scale (Tensor|None): The input scale Tensor Provided to weight for dequantization. Its rank must be 1.\\n        weight_dtype(str): The dtype of  weight Tensor, must be one of 'int8', 'int4', Defaulted to 'int8'.\\n    Returns:\\n        Tensor: the output Tensor, the data type is the same as that of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_only_linear\\n\\n            >>> x = paddle.cast(paddle.randn([1, 2, 64]), dtype='float16')\\n            >>> weight = paddle.cast(paddle.randint(0, 127, [32, 64]), dtype='int8')\\n            >>> scale = paddle.randn([32], dtype='float32')\\n            >>> bias = paddle.cast(paddle.randn([32]), dtype='float16')\\n            >>> if paddle.device.cuda.get_device_capability()[0] >= 8:\\n            ...    out = weight_only_linear(x, weight, bias=bias, weight_scale=scale, weight_dtype='int8')\\n            ...    print(out.shape)\\n            [1, 2, 32]\\n    \"\n    if in_dynamic_mode():\n        out = _C_ops.weight_only_linear(x, weight, bias, weight_scale, weight_dtype)\n        return out\n    else:\n        check_dtype(weight_dtype, 'weight_dtype', ['int8', 'int4'], 'weight_only_linear')\n        type = 'weight_only_linear'\n        helper = LayerHelper(type, **locals())\n        dtype = x.dtype\n        inputs = {'x': [x], 'weight': [weight], 'weight_scale': [weight_scale]}\n        if bias is not None:\n            inputs['bias'] = [bias]\n        attrs = {'weight_dtype': weight_dtype}\n        out = helper.create_variable_for_type_inference(dtype)\n        helper.append_op(type=type, inputs=inputs, outputs={'out': out}, attrs=attrs)\n        return out",
            "def weight_only_linear(x, weight, bias=None, weight_scale=None, weight_dtype='int8'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Applies matrix multiplication of two tensors and then bias addition if provided.\\n    This method requires CUDA version >= 11.2.\\n\\n    Args:\\n        x (Tensor): The first input Tensor to be multiplied, the data type is float16 or bfloat16.\\n        weight (Tensor): The second input Tensor to be multiplied. Its rank must be 2.\\n        bias (Tensor|None): The input bias Tensor. If it is None, no bias addition would\\n            be performed. Otherwise, The bias is added to the matrix multiplication result.\\n        weight_scale (Tensor|None): The input scale Tensor Provided to weight for dequantization. Its rank must be 1.\\n        weight_dtype(str): The dtype of  weight Tensor, must be one of 'int8', 'int4', Defaulted to 'int8'.\\n    Returns:\\n        Tensor: the output Tensor, the data type is the same as that of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_only_linear\\n\\n            >>> x = paddle.cast(paddle.randn([1, 2, 64]), dtype='float16')\\n            >>> weight = paddle.cast(paddle.randint(0, 127, [32, 64]), dtype='int8')\\n            >>> scale = paddle.randn([32], dtype='float32')\\n            >>> bias = paddle.cast(paddle.randn([32]), dtype='float16')\\n            >>> if paddle.device.cuda.get_device_capability()[0] >= 8:\\n            ...    out = weight_only_linear(x, weight, bias=bias, weight_scale=scale, weight_dtype='int8')\\n            ...    print(out.shape)\\n            [1, 2, 32]\\n    \"\n    if in_dynamic_mode():\n        out = _C_ops.weight_only_linear(x, weight, bias, weight_scale, weight_dtype)\n        return out\n    else:\n        check_dtype(weight_dtype, 'weight_dtype', ['int8', 'int4'], 'weight_only_linear')\n        type = 'weight_only_linear'\n        helper = LayerHelper(type, **locals())\n        dtype = x.dtype\n        inputs = {'x': [x], 'weight': [weight], 'weight_scale': [weight_scale]}\n        if bias is not None:\n            inputs['bias'] = [bias]\n        attrs = {'weight_dtype': weight_dtype}\n        out = helper.create_variable_for_type_inference(dtype)\n        helper.append_op(type=type, inputs=inputs, outputs={'out': out}, attrs=attrs)\n        return out",
            "def weight_only_linear(x, weight, bias=None, weight_scale=None, weight_dtype='int8'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Applies matrix multiplication of two tensors and then bias addition if provided.\\n    This method requires CUDA version >= 11.2.\\n\\n    Args:\\n        x (Tensor): The first input Tensor to be multiplied, the data type is float16 or bfloat16.\\n        weight (Tensor): The second input Tensor to be multiplied. Its rank must be 2.\\n        bias (Tensor|None): The input bias Tensor. If it is None, no bias addition would\\n            be performed. Otherwise, The bias is added to the matrix multiplication result.\\n        weight_scale (Tensor|None): The input scale Tensor Provided to weight for dequantization. Its rank must be 1.\\n        weight_dtype(str): The dtype of  weight Tensor, must be one of 'int8', 'int4', Defaulted to 'int8'.\\n    Returns:\\n        Tensor: the output Tensor, the data type is the same as that of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_only_linear\\n\\n            >>> x = paddle.cast(paddle.randn([1, 2, 64]), dtype='float16')\\n            >>> weight = paddle.cast(paddle.randint(0, 127, [32, 64]), dtype='int8')\\n            >>> scale = paddle.randn([32], dtype='float32')\\n            >>> bias = paddle.cast(paddle.randn([32]), dtype='float16')\\n            >>> if paddle.device.cuda.get_device_capability()[0] >= 8:\\n            ...    out = weight_only_linear(x, weight, bias=bias, weight_scale=scale, weight_dtype='int8')\\n            ...    print(out.shape)\\n            [1, 2, 32]\\n    \"\n    if in_dynamic_mode():\n        out = _C_ops.weight_only_linear(x, weight, bias, weight_scale, weight_dtype)\n        return out\n    else:\n        check_dtype(weight_dtype, 'weight_dtype', ['int8', 'int4'], 'weight_only_linear')\n        type = 'weight_only_linear'\n        helper = LayerHelper(type, **locals())\n        dtype = x.dtype\n        inputs = {'x': [x], 'weight': [weight], 'weight_scale': [weight_scale]}\n        if bias is not None:\n            inputs['bias'] = [bias]\n        attrs = {'weight_dtype': weight_dtype}\n        out = helper.create_variable_for_type_inference(dtype)\n        helper.append_op(type=type, inputs=inputs, outputs={'out': out}, attrs=attrs)\n        return out",
            "def weight_only_linear(x, weight, bias=None, weight_scale=None, weight_dtype='int8'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Applies matrix multiplication of two tensors and then bias addition if provided.\\n    This method requires CUDA version >= 11.2.\\n\\n    Args:\\n        x (Tensor): The first input Tensor to be multiplied, the data type is float16 or bfloat16.\\n        weight (Tensor): The second input Tensor to be multiplied. Its rank must be 2.\\n        bias (Tensor|None): The input bias Tensor. If it is None, no bias addition would\\n            be performed. Otherwise, The bias is added to the matrix multiplication result.\\n        weight_scale (Tensor|None): The input scale Tensor Provided to weight for dequantization. Its rank must be 1.\\n        weight_dtype(str): The dtype of  weight Tensor, must be one of 'int8', 'int4', Defaulted to 'int8'.\\n    Returns:\\n        Tensor: the output Tensor, the data type is the same as that of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import weight_only_linear\\n\\n            >>> x = paddle.cast(paddle.randn([1, 2, 64]), dtype='float16')\\n            >>> weight = paddle.cast(paddle.randint(0, 127, [32, 64]), dtype='int8')\\n            >>> scale = paddle.randn([32], dtype='float32')\\n            >>> bias = paddle.cast(paddle.randn([32]), dtype='float16')\\n            >>> if paddle.device.cuda.get_device_capability()[0] >= 8:\\n            ...    out = weight_only_linear(x, weight, bias=bias, weight_scale=scale, weight_dtype='int8')\\n            ...    print(out.shape)\\n            [1, 2, 32]\\n    \"\n    if in_dynamic_mode():\n        out = _C_ops.weight_only_linear(x, weight, bias, weight_scale, weight_dtype)\n        return out\n    else:\n        check_dtype(weight_dtype, 'weight_dtype', ['int8', 'int4'], 'weight_only_linear')\n        type = 'weight_only_linear'\n        helper = LayerHelper(type, **locals())\n        dtype = x.dtype\n        inputs = {'x': [x], 'weight': [weight], 'weight_scale': [weight_scale]}\n        if bias is not None:\n            inputs['bias'] = [bias]\n        attrs = {'weight_dtype': weight_dtype}\n        out = helper.create_variable_for_type_inference(dtype)\n        helper.append_op(type=type, inputs=inputs, outputs={'out': out}, attrs=attrs)\n        return out"
        ]
    },
    {
        "func_name": "llm_int8_linear",
        "original": "def llm_int8_linear(x, weight, bias=None, weight_scale=None, threshold=6.0):\n    \"\"\"\n    Applies matrix multiplication of two tensors and then bias addition if provided.\n    This method requires CUDA version >= 11.2.\n\n    Args:\n        x (Tensor): the first input Tensor to be multiplied, the data type is float16 or bfloat16.\n        weight (Tensor): the second input Tensor to be multiplied. Its rank must be 2.\n        bias (Tensor|None): the input bias Tensor. If it is None, no bias addition would\n            be performed. Otherwise, the bias is added to the matrix multiplication result.\n        weight_scale (Tensor|None): the input scale Tensor Provided to weight for dequantization. Its rank must be 1.\n        threshold(float): The min value of outlier in activation, outlier's channel will be apply multiply with x.dtype.\n\n    Returns:\n        Tensor: the output Tensor, the data type is the same as that of x.\n\n    Examples:\n        .. code-block:: python\n\n            >>> # doctest: +SKIP('No testing required')\n            >>> import paddle\n            >>> from paddle.nn.quant import llm_int8_linear\n\n            >>> x = paddle.cast(paddle.randn([1, 2, 64]), dtype='float16')\n            >>> weight = paddle.cast(paddle.randint(0, 127, [32, 64]), dtype='int8')\n            >>> scale = paddle.randn([32], dtype='float32')\n            >>> bias = paddle.cast(paddle.randn([32]), dtype='float16')\n            >>> if paddle.device.cuda.get_device_capability()[0] >= 8:\n            ...    out = llm_int8_linear(x, weight, bias=bias, weight_scale=scale, threshold=6.0)\n            ...    print(out.shape)\n            [1, 2, 32]\n    \"\"\"\n    if in_dynamic_mode():\n        out = _C_ops.llm_int8_linear(x, weight, bias, weight_scale, threshold)\n        return out\n    else:\n        type = 'llm_int8_linear'\n        helper = LayerHelper(type, **locals())\n        dtype = x.dtype\n        inputs = {'x': [x], 'weight': [weight], 'weight_scale': [weight_scale]}\n        if bias:\n            inputs['bias'] = [bias]\n        attrs = {'threshold': threshold}\n        out = helper.create_variable_for_type_inference(dtype)\n        helper.append_op(type=type, inputs=inputs, outputs={'out': out}, attrs=attrs)\n        return out",
        "mutated": [
            "def llm_int8_linear(x, weight, bias=None, weight_scale=None, threshold=6.0):\n    if False:\n        i = 10\n    \"\\n    Applies matrix multiplication of two tensors and then bias addition if provided.\\n    This method requires CUDA version >= 11.2.\\n\\n    Args:\\n        x (Tensor): the first input Tensor to be multiplied, the data type is float16 or bfloat16.\\n        weight (Tensor): the second input Tensor to be multiplied. Its rank must be 2.\\n        bias (Tensor|None): the input bias Tensor. If it is None, no bias addition would\\n            be performed. Otherwise, the bias is added to the matrix multiplication result.\\n        weight_scale (Tensor|None): the input scale Tensor Provided to weight for dequantization. Its rank must be 1.\\n        threshold(float): The min value of outlier in activation, outlier's channel will be apply multiply with x.dtype.\\n\\n    Returns:\\n        Tensor: the output Tensor, the data type is the same as that of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import llm_int8_linear\\n\\n            >>> x = paddle.cast(paddle.randn([1, 2, 64]), dtype='float16')\\n            >>> weight = paddle.cast(paddle.randint(0, 127, [32, 64]), dtype='int8')\\n            >>> scale = paddle.randn([32], dtype='float32')\\n            >>> bias = paddle.cast(paddle.randn([32]), dtype='float16')\\n            >>> if paddle.device.cuda.get_device_capability()[0] >= 8:\\n            ...    out = llm_int8_linear(x, weight, bias=bias, weight_scale=scale, threshold=6.0)\\n            ...    print(out.shape)\\n            [1, 2, 32]\\n    \"\n    if in_dynamic_mode():\n        out = _C_ops.llm_int8_linear(x, weight, bias, weight_scale, threshold)\n        return out\n    else:\n        type = 'llm_int8_linear'\n        helper = LayerHelper(type, **locals())\n        dtype = x.dtype\n        inputs = {'x': [x], 'weight': [weight], 'weight_scale': [weight_scale]}\n        if bias:\n            inputs['bias'] = [bias]\n        attrs = {'threshold': threshold}\n        out = helper.create_variable_for_type_inference(dtype)\n        helper.append_op(type=type, inputs=inputs, outputs={'out': out}, attrs=attrs)\n        return out",
            "def llm_int8_linear(x, weight, bias=None, weight_scale=None, threshold=6.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Applies matrix multiplication of two tensors and then bias addition if provided.\\n    This method requires CUDA version >= 11.2.\\n\\n    Args:\\n        x (Tensor): the first input Tensor to be multiplied, the data type is float16 or bfloat16.\\n        weight (Tensor): the second input Tensor to be multiplied. Its rank must be 2.\\n        bias (Tensor|None): the input bias Tensor. If it is None, no bias addition would\\n            be performed. Otherwise, the bias is added to the matrix multiplication result.\\n        weight_scale (Tensor|None): the input scale Tensor Provided to weight for dequantization. Its rank must be 1.\\n        threshold(float): The min value of outlier in activation, outlier's channel will be apply multiply with x.dtype.\\n\\n    Returns:\\n        Tensor: the output Tensor, the data type is the same as that of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import llm_int8_linear\\n\\n            >>> x = paddle.cast(paddle.randn([1, 2, 64]), dtype='float16')\\n            >>> weight = paddle.cast(paddle.randint(0, 127, [32, 64]), dtype='int8')\\n            >>> scale = paddle.randn([32], dtype='float32')\\n            >>> bias = paddle.cast(paddle.randn([32]), dtype='float16')\\n            >>> if paddle.device.cuda.get_device_capability()[0] >= 8:\\n            ...    out = llm_int8_linear(x, weight, bias=bias, weight_scale=scale, threshold=6.0)\\n            ...    print(out.shape)\\n            [1, 2, 32]\\n    \"\n    if in_dynamic_mode():\n        out = _C_ops.llm_int8_linear(x, weight, bias, weight_scale, threshold)\n        return out\n    else:\n        type = 'llm_int8_linear'\n        helper = LayerHelper(type, **locals())\n        dtype = x.dtype\n        inputs = {'x': [x], 'weight': [weight], 'weight_scale': [weight_scale]}\n        if bias:\n            inputs['bias'] = [bias]\n        attrs = {'threshold': threshold}\n        out = helper.create_variable_for_type_inference(dtype)\n        helper.append_op(type=type, inputs=inputs, outputs={'out': out}, attrs=attrs)\n        return out",
            "def llm_int8_linear(x, weight, bias=None, weight_scale=None, threshold=6.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Applies matrix multiplication of two tensors and then bias addition if provided.\\n    This method requires CUDA version >= 11.2.\\n\\n    Args:\\n        x (Tensor): the first input Tensor to be multiplied, the data type is float16 or bfloat16.\\n        weight (Tensor): the second input Tensor to be multiplied. Its rank must be 2.\\n        bias (Tensor|None): the input bias Tensor. If it is None, no bias addition would\\n            be performed. Otherwise, the bias is added to the matrix multiplication result.\\n        weight_scale (Tensor|None): the input scale Tensor Provided to weight for dequantization. Its rank must be 1.\\n        threshold(float): The min value of outlier in activation, outlier's channel will be apply multiply with x.dtype.\\n\\n    Returns:\\n        Tensor: the output Tensor, the data type is the same as that of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import llm_int8_linear\\n\\n            >>> x = paddle.cast(paddle.randn([1, 2, 64]), dtype='float16')\\n            >>> weight = paddle.cast(paddle.randint(0, 127, [32, 64]), dtype='int8')\\n            >>> scale = paddle.randn([32], dtype='float32')\\n            >>> bias = paddle.cast(paddle.randn([32]), dtype='float16')\\n            >>> if paddle.device.cuda.get_device_capability()[0] >= 8:\\n            ...    out = llm_int8_linear(x, weight, bias=bias, weight_scale=scale, threshold=6.0)\\n            ...    print(out.shape)\\n            [1, 2, 32]\\n    \"\n    if in_dynamic_mode():\n        out = _C_ops.llm_int8_linear(x, weight, bias, weight_scale, threshold)\n        return out\n    else:\n        type = 'llm_int8_linear'\n        helper = LayerHelper(type, **locals())\n        dtype = x.dtype\n        inputs = {'x': [x], 'weight': [weight], 'weight_scale': [weight_scale]}\n        if bias:\n            inputs['bias'] = [bias]\n        attrs = {'threshold': threshold}\n        out = helper.create_variable_for_type_inference(dtype)\n        helper.append_op(type=type, inputs=inputs, outputs={'out': out}, attrs=attrs)\n        return out",
            "def llm_int8_linear(x, weight, bias=None, weight_scale=None, threshold=6.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Applies matrix multiplication of two tensors and then bias addition if provided.\\n    This method requires CUDA version >= 11.2.\\n\\n    Args:\\n        x (Tensor): the first input Tensor to be multiplied, the data type is float16 or bfloat16.\\n        weight (Tensor): the second input Tensor to be multiplied. Its rank must be 2.\\n        bias (Tensor|None): the input bias Tensor. If it is None, no bias addition would\\n            be performed. Otherwise, the bias is added to the matrix multiplication result.\\n        weight_scale (Tensor|None): the input scale Tensor Provided to weight for dequantization. Its rank must be 1.\\n        threshold(float): The min value of outlier in activation, outlier's channel will be apply multiply with x.dtype.\\n\\n    Returns:\\n        Tensor: the output Tensor, the data type is the same as that of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import llm_int8_linear\\n\\n            >>> x = paddle.cast(paddle.randn([1, 2, 64]), dtype='float16')\\n            >>> weight = paddle.cast(paddle.randint(0, 127, [32, 64]), dtype='int8')\\n            >>> scale = paddle.randn([32], dtype='float32')\\n            >>> bias = paddle.cast(paddle.randn([32]), dtype='float16')\\n            >>> if paddle.device.cuda.get_device_capability()[0] >= 8:\\n            ...    out = llm_int8_linear(x, weight, bias=bias, weight_scale=scale, threshold=6.0)\\n            ...    print(out.shape)\\n            [1, 2, 32]\\n    \"\n    if in_dynamic_mode():\n        out = _C_ops.llm_int8_linear(x, weight, bias, weight_scale, threshold)\n        return out\n    else:\n        type = 'llm_int8_linear'\n        helper = LayerHelper(type, **locals())\n        dtype = x.dtype\n        inputs = {'x': [x], 'weight': [weight], 'weight_scale': [weight_scale]}\n        if bias:\n            inputs['bias'] = [bias]\n        attrs = {'threshold': threshold}\n        out = helper.create_variable_for_type_inference(dtype)\n        helper.append_op(type=type, inputs=inputs, outputs={'out': out}, attrs=attrs)\n        return out",
            "def llm_int8_linear(x, weight, bias=None, weight_scale=None, threshold=6.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Applies matrix multiplication of two tensors and then bias addition if provided.\\n    This method requires CUDA version >= 11.2.\\n\\n    Args:\\n        x (Tensor): the first input Tensor to be multiplied, the data type is float16 or bfloat16.\\n        weight (Tensor): the second input Tensor to be multiplied. Its rank must be 2.\\n        bias (Tensor|None): the input bias Tensor. If it is None, no bias addition would\\n            be performed. Otherwise, the bias is added to the matrix multiplication result.\\n        weight_scale (Tensor|None): the input scale Tensor Provided to weight for dequantization. Its rank must be 1.\\n        threshold(float): The min value of outlier in activation, outlier's channel will be apply multiply with x.dtype.\\n\\n    Returns:\\n        Tensor: the output Tensor, the data type is the same as that of x.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +SKIP('No testing required')\\n            >>> import paddle\\n            >>> from paddle.nn.quant import llm_int8_linear\\n\\n            >>> x = paddle.cast(paddle.randn([1, 2, 64]), dtype='float16')\\n            >>> weight = paddle.cast(paddle.randint(0, 127, [32, 64]), dtype='int8')\\n            >>> scale = paddle.randn([32], dtype='float32')\\n            >>> bias = paddle.cast(paddle.randn([32]), dtype='float16')\\n            >>> if paddle.device.cuda.get_device_capability()[0] >= 8:\\n            ...    out = llm_int8_linear(x, weight, bias=bias, weight_scale=scale, threshold=6.0)\\n            ...    print(out.shape)\\n            [1, 2, 32]\\n    \"\n    if in_dynamic_mode():\n        out = _C_ops.llm_int8_linear(x, weight, bias, weight_scale, threshold)\n        return out\n    else:\n        type = 'llm_int8_linear'\n        helper = LayerHelper(type, **locals())\n        dtype = x.dtype\n        inputs = {'x': [x], 'weight': [weight], 'weight_scale': [weight_scale]}\n        if bias:\n            inputs['bias'] = [bias]\n        attrs = {'threshold': threshold}\n        out = helper.create_variable_for_type_inference(dtype)\n        helper.append_op(type=type, inputs=inputs, outputs={'out': out}, attrs=attrs)\n        return out"
        ]
    }
]