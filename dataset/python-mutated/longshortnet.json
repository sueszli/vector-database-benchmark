[
    {
        "func_name": "__init__",
        "original": "def __init__(self, model_dir: str, *args, **kwargs):\n    super().__init__(model_dir, *args, **kwargs)\n    self.depth = kwargs.get('depth', 0.33)\n    self.width = kwargs.get('width', 0.5)\n    self.num_classes = kwargs.get('num_classes', 8)\n    self.test_size = kwargs.get('test_size', (960, 600))\n    self.test_conf = kwargs.get('test_conf', 0.3)\n    self.nmsthre = kwargs.get('nmsthre', 0.55)\n    self.label_mapping = kwargs.get('labels', ['person', 'bicycle', 'car', 'motorcycle', 'bus', 'truck', 'traffic light', 'stop sign'])\n    self.model_name = kwargs.get('model_name', 'longshortnet_s.pt')\n    self.short_cfg = kwargs.get('short_cfg', dict(frame_num=1, delta=1, with_short_cut=False, out_channels=[((64, 128, 256), 1)]))\n    self.long_cfg = kwargs.get('long_cfg', dict(frame_num=3, delta=1, with_short_cut=False, include_current_frame=False, out_channels=[((21, 42, 85), 3)]))\n    self.merge_cfg = kwargs.get('merge_cfg', dict(merge_form='long_fusion', with_short_cut=True))\n    self.exp = LongShortNetExp()\n    self.exp.depth = self.depth\n    self.exp.width = self.width\n    self.exp.num_classes = self.num_classes\n    self.exp.test_size = self.test_size\n    self.exp.test_conf = self.test_conf\n    self.exp.nmsthre = self.nmsthre\n    self.exp.short_cfg = self.short_cfg\n    self.exp.long_cfg = self.long_cfg\n    self.exp.merge_cfg = self.merge_cfg\n    self.model = self.exp.get_model()\n    model_path = osp.join(model_dir, self.model_name)\n    ckpt = torch.load(model_path, map_location='cpu')\n    self.model.load_state_dict(ckpt['model'])\n    self.preproc = ValTransform(legacy=False)",
        "mutated": [
            "def __init__(self, model_dir: str, *args, **kwargs):\n    if False:\n        i = 10\n    super().__init__(model_dir, *args, **kwargs)\n    self.depth = kwargs.get('depth', 0.33)\n    self.width = kwargs.get('width', 0.5)\n    self.num_classes = kwargs.get('num_classes', 8)\n    self.test_size = kwargs.get('test_size', (960, 600))\n    self.test_conf = kwargs.get('test_conf', 0.3)\n    self.nmsthre = kwargs.get('nmsthre', 0.55)\n    self.label_mapping = kwargs.get('labels', ['person', 'bicycle', 'car', 'motorcycle', 'bus', 'truck', 'traffic light', 'stop sign'])\n    self.model_name = kwargs.get('model_name', 'longshortnet_s.pt')\n    self.short_cfg = kwargs.get('short_cfg', dict(frame_num=1, delta=1, with_short_cut=False, out_channels=[((64, 128, 256), 1)]))\n    self.long_cfg = kwargs.get('long_cfg', dict(frame_num=3, delta=1, with_short_cut=False, include_current_frame=False, out_channels=[((21, 42, 85), 3)]))\n    self.merge_cfg = kwargs.get('merge_cfg', dict(merge_form='long_fusion', with_short_cut=True))\n    self.exp = LongShortNetExp()\n    self.exp.depth = self.depth\n    self.exp.width = self.width\n    self.exp.num_classes = self.num_classes\n    self.exp.test_size = self.test_size\n    self.exp.test_conf = self.test_conf\n    self.exp.nmsthre = self.nmsthre\n    self.exp.short_cfg = self.short_cfg\n    self.exp.long_cfg = self.long_cfg\n    self.exp.merge_cfg = self.merge_cfg\n    self.model = self.exp.get_model()\n    model_path = osp.join(model_dir, self.model_name)\n    ckpt = torch.load(model_path, map_location='cpu')\n    self.model.load_state_dict(ckpt['model'])\n    self.preproc = ValTransform(legacy=False)",
            "def __init__(self, model_dir: str, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(model_dir, *args, **kwargs)\n    self.depth = kwargs.get('depth', 0.33)\n    self.width = kwargs.get('width', 0.5)\n    self.num_classes = kwargs.get('num_classes', 8)\n    self.test_size = kwargs.get('test_size', (960, 600))\n    self.test_conf = kwargs.get('test_conf', 0.3)\n    self.nmsthre = kwargs.get('nmsthre', 0.55)\n    self.label_mapping = kwargs.get('labels', ['person', 'bicycle', 'car', 'motorcycle', 'bus', 'truck', 'traffic light', 'stop sign'])\n    self.model_name = kwargs.get('model_name', 'longshortnet_s.pt')\n    self.short_cfg = kwargs.get('short_cfg', dict(frame_num=1, delta=1, with_short_cut=False, out_channels=[((64, 128, 256), 1)]))\n    self.long_cfg = kwargs.get('long_cfg', dict(frame_num=3, delta=1, with_short_cut=False, include_current_frame=False, out_channels=[((21, 42, 85), 3)]))\n    self.merge_cfg = kwargs.get('merge_cfg', dict(merge_form='long_fusion', with_short_cut=True))\n    self.exp = LongShortNetExp()\n    self.exp.depth = self.depth\n    self.exp.width = self.width\n    self.exp.num_classes = self.num_classes\n    self.exp.test_size = self.test_size\n    self.exp.test_conf = self.test_conf\n    self.exp.nmsthre = self.nmsthre\n    self.exp.short_cfg = self.short_cfg\n    self.exp.long_cfg = self.long_cfg\n    self.exp.merge_cfg = self.merge_cfg\n    self.model = self.exp.get_model()\n    model_path = osp.join(model_dir, self.model_name)\n    ckpt = torch.load(model_path, map_location='cpu')\n    self.model.load_state_dict(ckpt['model'])\n    self.preproc = ValTransform(legacy=False)",
            "def __init__(self, model_dir: str, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(model_dir, *args, **kwargs)\n    self.depth = kwargs.get('depth', 0.33)\n    self.width = kwargs.get('width', 0.5)\n    self.num_classes = kwargs.get('num_classes', 8)\n    self.test_size = kwargs.get('test_size', (960, 600))\n    self.test_conf = kwargs.get('test_conf', 0.3)\n    self.nmsthre = kwargs.get('nmsthre', 0.55)\n    self.label_mapping = kwargs.get('labels', ['person', 'bicycle', 'car', 'motorcycle', 'bus', 'truck', 'traffic light', 'stop sign'])\n    self.model_name = kwargs.get('model_name', 'longshortnet_s.pt')\n    self.short_cfg = kwargs.get('short_cfg', dict(frame_num=1, delta=1, with_short_cut=False, out_channels=[((64, 128, 256), 1)]))\n    self.long_cfg = kwargs.get('long_cfg', dict(frame_num=3, delta=1, with_short_cut=False, include_current_frame=False, out_channels=[((21, 42, 85), 3)]))\n    self.merge_cfg = kwargs.get('merge_cfg', dict(merge_form='long_fusion', with_short_cut=True))\n    self.exp = LongShortNetExp()\n    self.exp.depth = self.depth\n    self.exp.width = self.width\n    self.exp.num_classes = self.num_classes\n    self.exp.test_size = self.test_size\n    self.exp.test_conf = self.test_conf\n    self.exp.nmsthre = self.nmsthre\n    self.exp.short_cfg = self.short_cfg\n    self.exp.long_cfg = self.long_cfg\n    self.exp.merge_cfg = self.merge_cfg\n    self.model = self.exp.get_model()\n    model_path = osp.join(model_dir, self.model_name)\n    ckpt = torch.load(model_path, map_location='cpu')\n    self.model.load_state_dict(ckpt['model'])\n    self.preproc = ValTransform(legacy=False)",
            "def __init__(self, model_dir: str, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(model_dir, *args, **kwargs)\n    self.depth = kwargs.get('depth', 0.33)\n    self.width = kwargs.get('width', 0.5)\n    self.num_classes = kwargs.get('num_classes', 8)\n    self.test_size = kwargs.get('test_size', (960, 600))\n    self.test_conf = kwargs.get('test_conf', 0.3)\n    self.nmsthre = kwargs.get('nmsthre', 0.55)\n    self.label_mapping = kwargs.get('labels', ['person', 'bicycle', 'car', 'motorcycle', 'bus', 'truck', 'traffic light', 'stop sign'])\n    self.model_name = kwargs.get('model_name', 'longshortnet_s.pt')\n    self.short_cfg = kwargs.get('short_cfg', dict(frame_num=1, delta=1, with_short_cut=False, out_channels=[((64, 128, 256), 1)]))\n    self.long_cfg = kwargs.get('long_cfg', dict(frame_num=3, delta=1, with_short_cut=False, include_current_frame=False, out_channels=[((21, 42, 85), 3)]))\n    self.merge_cfg = kwargs.get('merge_cfg', dict(merge_form='long_fusion', with_short_cut=True))\n    self.exp = LongShortNetExp()\n    self.exp.depth = self.depth\n    self.exp.width = self.width\n    self.exp.num_classes = self.num_classes\n    self.exp.test_size = self.test_size\n    self.exp.test_conf = self.test_conf\n    self.exp.nmsthre = self.nmsthre\n    self.exp.short_cfg = self.short_cfg\n    self.exp.long_cfg = self.long_cfg\n    self.exp.merge_cfg = self.merge_cfg\n    self.model = self.exp.get_model()\n    model_path = osp.join(model_dir, self.model_name)\n    ckpt = torch.load(model_path, map_location='cpu')\n    self.model.load_state_dict(ckpt['model'])\n    self.preproc = ValTransform(legacy=False)",
            "def __init__(self, model_dir: str, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(model_dir, *args, **kwargs)\n    self.depth = kwargs.get('depth', 0.33)\n    self.width = kwargs.get('width', 0.5)\n    self.num_classes = kwargs.get('num_classes', 8)\n    self.test_size = kwargs.get('test_size', (960, 600))\n    self.test_conf = kwargs.get('test_conf', 0.3)\n    self.nmsthre = kwargs.get('nmsthre', 0.55)\n    self.label_mapping = kwargs.get('labels', ['person', 'bicycle', 'car', 'motorcycle', 'bus', 'truck', 'traffic light', 'stop sign'])\n    self.model_name = kwargs.get('model_name', 'longshortnet_s.pt')\n    self.short_cfg = kwargs.get('short_cfg', dict(frame_num=1, delta=1, with_short_cut=False, out_channels=[((64, 128, 256), 1)]))\n    self.long_cfg = kwargs.get('long_cfg', dict(frame_num=3, delta=1, with_short_cut=False, include_current_frame=False, out_channels=[((21, 42, 85), 3)]))\n    self.merge_cfg = kwargs.get('merge_cfg', dict(merge_form='long_fusion', with_short_cut=True))\n    self.exp = LongShortNetExp()\n    self.exp.depth = self.depth\n    self.exp.width = self.width\n    self.exp.num_classes = self.num_classes\n    self.exp.test_size = self.test_size\n    self.exp.test_conf = self.test_conf\n    self.exp.nmsthre = self.nmsthre\n    self.exp.short_cfg = self.short_cfg\n    self.exp.long_cfg = self.long_cfg\n    self.exp.merge_cfg = self.merge_cfg\n    self.model = self.exp.get_model()\n    model_path = osp.join(model_dir, self.model_name)\n    ckpt = torch.load(model_path, map_location='cpu')\n    self.model.load_state_dict(ckpt['model'])\n    self.preproc = ValTransform(legacy=False)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, inputs):\n    return self.inference_video(inputs)",
        "mutated": [
            "def forward(self, inputs):\n    if False:\n        i = 10\n    return self.inference_video(inputs)",
            "def forward(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self.inference_video(inputs)",
            "def forward(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self.inference_video(inputs)",
            "def forward(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self.inference_video(inputs)",
            "def forward(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self.inference_video(inputs)"
        ]
    },
    {
        "func_name": "postprocess",
        "original": "def postprocess(self, input):\n    outputs = postprocess(input, self.num_classes, self.test_conf, self.nmsthre, class_agnostic=True)\n    if len(outputs) == 1 and outputs[0] is not None:\n        bboxes = outputs[0][:, 0:4].cpu().numpy() / self.resize_ratio\n        scores = outputs[0][:, 5].cpu().numpy()\n        labels = outputs[0][:, 6].cpu().int().numpy()\n        pred_label_names = []\n        for lab in labels:\n            pred_label_names.append(self.label_mapping[lab])\n    else:\n        bboxes = np.asarray([])\n        scores = np.asarray([])\n        pred_label_names = np.asarray([])\n    return (bboxes, scores, pred_label_names)",
        "mutated": [
            "def postprocess(self, input):\n    if False:\n        i = 10\n    outputs = postprocess(input, self.num_classes, self.test_conf, self.nmsthre, class_agnostic=True)\n    if len(outputs) == 1 and outputs[0] is not None:\n        bboxes = outputs[0][:, 0:4].cpu().numpy() / self.resize_ratio\n        scores = outputs[0][:, 5].cpu().numpy()\n        labels = outputs[0][:, 6].cpu().int().numpy()\n        pred_label_names = []\n        for lab in labels:\n            pred_label_names.append(self.label_mapping[lab])\n    else:\n        bboxes = np.asarray([])\n        scores = np.asarray([])\n        pred_label_names = np.asarray([])\n    return (bboxes, scores, pred_label_names)",
            "def postprocess(self, input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    outputs = postprocess(input, self.num_classes, self.test_conf, self.nmsthre, class_agnostic=True)\n    if len(outputs) == 1 and outputs[0] is not None:\n        bboxes = outputs[0][:, 0:4].cpu().numpy() / self.resize_ratio\n        scores = outputs[0][:, 5].cpu().numpy()\n        labels = outputs[0][:, 6].cpu().int().numpy()\n        pred_label_names = []\n        for lab in labels:\n            pred_label_names.append(self.label_mapping[lab])\n    else:\n        bboxes = np.asarray([])\n        scores = np.asarray([])\n        pred_label_names = np.asarray([])\n    return (bboxes, scores, pred_label_names)",
            "def postprocess(self, input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    outputs = postprocess(input, self.num_classes, self.test_conf, self.nmsthre, class_agnostic=True)\n    if len(outputs) == 1 and outputs[0] is not None:\n        bboxes = outputs[0][:, 0:4].cpu().numpy() / self.resize_ratio\n        scores = outputs[0][:, 5].cpu().numpy()\n        labels = outputs[0][:, 6].cpu().int().numpy()\n        pred_label_names = []\n        for lab in labels:\n            pred_label_names.append(self.label_mapping[lab])\n    else:\n        bboxes = np.asarray([])\n        scores = np.asarray([])\n        pred_label_names = np.asarray([])\n    return (bboxes, scores, pred_label_names)",
            "def postprocess(self, input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    outputs = postprocess(input, self.num_classes, self.test_conf, self.nmsthre, class_agnostic=True)\n    if len(outputs) == 1 and outputs[0] is not None:\n        bboxes = outputs[0][:, 0:4].cpu().numpy() / self.resize_ratio\n        scores = outputs[0][:, 5].cpu().numpy()\n        labels = outputs[0][:, 6].cpu().int().numpy()\n        pred_label_names = []\n        for lab in labels:\n            pred_label_names.append(self.label_mapping[lab])\n    else:\n        bboxes = np.asarray([])\n        scores = np.asarray([])\n        pred_label_names = np.asarray([])\n    return (bboxes, scores, pred_label_names)",
            "def postprocess(self, input):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    outputs = postprocess(input, self.num_classes, self.test_conf, self.nmsthre, class_agnostic=True)\n    if len(outputs) == 1 and outputs[0] is not None:\n        bboxes = outputs[0][:, 0:4].cpu().numpy() / self.resize_ratio\n        scores = outputs[0][:, 5].cpu().numpy()\n        labels = outputs[0][:, 6].cpu().int().numpy()\n        pred_label_names = []\n        for lab in labels:\n            pred_label_names.append(self.label_mapping[lab])\n    else:\n        bboxes = np.asarray([])\n        scores = np.asarray([])\n        pred_label_names = np.asarray([])\n    return (bboxes, scores, pred_label_names)"
        ]
    },
    {
        "func_name": "inference_video",
        "original": "def inference_video(self, v_path):\n    outputs = []\n    capture = cv2.VideoCapture(v_path)\n    self.fps = capture.get(cv2.CAP_PROP_FPS)\n    self.ori_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    self.ori_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n    self.ori_size = (self.ori_width, self.ori_height)\n    self.resize_ratio = min(self.test_size[0] / self.ori_size[0], self.test_size[1] / self.ori_size[1])\n    self.device = next(self.model.parameters()).device\n    frame_idx = 0\n    while capture.isOpened():\n        (ret, frame) = capture.read()\n        if not ret:\n            break\n        if frame_idx == 0:\n            short_imgs_queue = [frame.copy() for _ in range(self.short_cfg['frame_num'])]\n            long_imgs_queue = [frame.copy() for _ in range(self.long_cfg['frame_num'])]\n            short_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in short_imgs_queue]\n            long_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in long_imgs_queue]\n            short_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in short_imgs_queue]\n            long_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in long_imgs_queue]\n        else:\n            long_imgs_queue = long_imgs_queue[1:] + short_imgs_queue[:]\n            short_imgs_queue = [frame.copy() for _ in range(self.short_cfg['frame_num'])]\n            short_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in short_imgs_queue]\n            short_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in short_imgs_queue]\n        short_img = np.concatenate(short_imgs_queue, axis=0)\n        long_img = np.concatenate(long_imgs_queue, axis=0)\n        short_img = torch.from_numpy(short_img).unsqueeze(0)\n        long_img = torch.from_numpy(long_img).unsqueeze(0)\n        short_img = short_img.to(self.device)\n        long_img = long_img.to(self.device)\n        output = self.model((short_img, long_img))\n        output = self.postprocess(output)\n        output += (timestamp_format(seconds=frame_idx / self.fps),)\n        outputs.append(output)\n        frame_idx += 1\n    return outputs",
        "mutated": [
            "def inference_video(self, v_path):\n    if False:\n        i = 10\n    outputs = []\n    capture = cv2.VideoCapture(v_path)\n    self.fps = capture.get(cv2.CAP_PROP_FPS)\n    self.ori_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    self.ori_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n    self.ori_size = (self.ori_width, self.ori_height)\n    self.resize_ratio = min(self.test_size[0] / self.ori_size[0], self.test_size[1] / self.ori_size[1])\n    self.device = next(self.model.parameters()).device\n    frame_idx = 0\n    while capture.isOpened():\n        (ret, frame) = capture.read()\n        if not ret:\n            break\n        if frame_idx == 0:\n            short_imgs_queue = [frame.copy() for _ in range(self.short_cfg['frame_num'])]\n            long_imgs_queue = [frame.copy() for _ in range(self.long_cfg['frame_num'])]\n            short_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in short_imgs_queue]\n            long_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in long_imgs_queue]\n            short_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in short_imgs_queue]\n            long_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in long_imgs_queue]\n        else:\n            long_imgs_queue = long_imgs_queue[1:] + short_imgs_queue[:]\n            short_imgs_queue = [frame.copy() for _ in range(self.short_cfg['frame_num'])]\n            short_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in short_imgs_queue]\n            short_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in short_imgs_queue]\n        short_img = np.concatenate(short_imgs_queue, axis=0)\n        long_img = np.concatenate(long_imgs_queue, axis=0)\n        short_img = torch.from_numpy(short_img).unsqueeze(0)\n        long_img = torch.from_numpy(long_img).unsqueeze(0)\n        short_img = short_img.to(self.device)\n        long_img = long_img.to(self.device)\n        output = self.model((short_img, long_img))\n        output = self.postprocess(output)\n        output += (timestamp_format(seconds=frame_idx / self.fps),)\n        outputs.append(output)\n        frame_idx += 1\n    return outputs",
            "def inference_video(self, v_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    outputs = []\n    capture = cv2.VideoCapture(v_path)\n    self.fps = capture.get(cv2.CAP_PROP_FPS)\n    self.ori_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    self.ori_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n    self.ori_size = (self.ori_width, self.ori_height)\n    self.resize_ratio = min(self.test_size[0] / self.ori_size[0], self.test_size[1] / self.ori_size[1])\n    self.device = next(self.model.parameters()).device\n    frame_idx = 0\n    while capture.isOpened():\n        (ret, frame) = capture.read()\n        if not ret:\n            break\n        if frame_idx == 0:\n            short_imgs_queue = [frame.copy() for _ in range(self.short_cfg['frame_num'])]\n            long_imgs_queue = [frame.copy() for _ in range(self.long_cfg['frame_num'])]\n            short_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in short_imgs_queue]\n            long_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in long_imgs_queue]\n            short_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in short_imgs_queue]\n            long_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in long_imgs_queue]\n        else:\n            long_imgs_queue = long_imgs_queue[1:] + short_imgs_queue[:]\n            short_imgs_queue = [frame.copy() for _ in range(self.short_cfg['frame_num'])]\n            short_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in short_imgs_queue]\n            short_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in short_imgs_queue]\n        short_img = np.concatenate(short_imgs_queue, axis=0)\n        long_img = np.concatenate(long_imgs_queue, axis=0)\n        short_img = torch.from_numpy(short_img).unsqueeze(0)\n        long_img = torch.from_numpy(long_img).unsqueeze(0)\n        short_img = short_img.to(self.device)\n        long_img = long_img.to(self.device)\n        output = self.model((short_img, long_img))\n        output = self.postprocess(output)\n        output += (timestamp_format(seconds=frame_idx / self.fps),)\n        outputs.append(output)\n        frame_idx += 1\n    return outputs",
            "def inference_video(self, v_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    outputs = []\n    capture = cv2.VideoCapture(v_path)\n    self.fps = capture.get(cv2.CAP_PROP_FPS)\n    self.ori_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    self.ori_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n    self.ori_size = (self.ori_width, self.ori_height)\n    self.resize_ratio = min(self.test_size[0] / self.ori_size[0], self.test_size[1] / self.ori_size[1])\n    self.device = next(self.model.parameters()).device\n    frame_idx = 0\n    while capture.isOpened():\n        (ret, frame) = capture.read()\n        if not ret:\n            break\n        if frame_idx == 0:\n            short_imgs_queue = [frame.copy() for _ in range(self.short_cfg['frame_num'])]\n            long_imgs_queue = [frame.copy() for _ in range(self.long_cfg['frame_num'])]\n            short_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in short_imgs_queue]\n            long_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in long_imgs_queue]\n            short_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in short_imgs_queue]\n            long_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in long_imgs_queue]\n        else:\n            long_imgs_queue = long_imgs_queue[1:] + short_imgs_queue[:]\n            short_imgs_queue = [frame.copy() for _ in range(self.short_cfg['frame_num'])]\n            short_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in short_imgs_queue]\n            short_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in short_imgs_queue]\n        short_img = np.concatenate(short_imgs_queue, axis=0)\n        long_img = np.concatenate(long_imgs_queue, axis=0)\n        short_img = torch.from_numpy(short_img).unsqueeze(0)\n        long_img = torch.from_numpy(long_img).unsqueeze(0)\n        short_img = short_img.to(self.device)\n        long_img = long_img.to(self.device)\n        output = self.model((short_img, long_img))\n        output = self.postprocess(output)\n        output += (timestamp_format(seconds=frame_idx / self.fps),)\n        outputs.append(output)\n        frame_idx += 1\n    return outputs",
            "def inference_video(self, v_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    outputs = []\n    capture = cv2.VideoCapture(v_path)\n    self.fps = capture.get(cv2.CAP_PROP_FPS)\n    self.ori_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    self.ori_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n    self.ori_size = (self.ori_width, self.ori_height)\n    self.resize_ratio = min(self.test_size[0] / self.ori_size[0], self.test_size[1] / self.ori_size[1])\n    self.device = next(self.model.parameters()).device\n    frame_idx = 0\n    while capture.isOpened():\n        (ret, frame) = capture.read()\n        if not ret:\n            break\n        if frame_idx == 0:\n            short_imgs_queue = [frame.copy() for _ in range(self.short_cfg['frame_num'])]\n            long_imgs_queue = [frame.copy() for _ in range(self.long_cfg['frame_num'])]\n            short_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in short_imgs_queue]\n            long_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in long_imgs_queue]\n            short_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in short_imgs_queue]\n            long_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in long_imgs_queue]\n        else:\n            long_imgs_queue = long_imgs_queue[1:] + short_imgs_queue[:]\n            short_imgs_queue = [frame.copy() for _ in range(self.short_cfg['frame_num'])]\n            short_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in short_imgs_queue]\n            short_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in short_imgs_queue]\n        short_img = np.concatenate(short_imgs_queue, axis=0)\n        long_img = np.concatenate(long_imgs_queue, axis=0)\n        short_img = torch.from_numpy(short_img).unsqueeze(0)\n        long_img = torch.from_numpy(long_img).unsqueeze(0)\n        short_img = short_img.to(self.device)\n        long_img = long_img.to(self.device)\n        output = self.model((short_img, long_img))\n        output = self.postprocess(output)\n        output += (timestamp_format(seconds=frame_idx / self.fps),)\n        outputs.append(output)\n        frame_idx += 1\n    return outputs",
            "def inference_video(self, v_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    outputs = []\n    capture = cv2.VideoCapture(v_path)\n    self.fps = capture.get(cv2.CAP_PROP_FPS)\n    self.ori_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    self.ori_width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n    self.ori_size = (self.ori_width, self.ori_height)\n    self.resize_ratio = min(self.test_size[0] / self.ori_size[0], self.test_size[1] / self.ori_size[1])\n    self.device = next(self.model.parameters()).device\n    frame_idx = 0\n    while capture.isOpened():\n        (ret, frame) = capture.read()\n        if not ret:\n            break\n        if frame_idx == 0:\n            short_imgs_queue = [frame.copy() for _ in range(self.short_cfg['frame_num'])]\n            long_imgs_queue = [frame.copy() for _ in range(self.long_cfg['frame_num'])]\n            short_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in short_imgs_queue]\n            long_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in long_imgs_queue]\n            short_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in short_imgs_queue]\n            long_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in long_imgs_queue]\n        else:\n            long_imgs_queue = long_imgs_queue[1:] + short_imgs_queue[:]\n            short_imgs_queue = [frame.copy() for _ in range(self.short_cfg['frame_num'])]\n            short_imgs_queue = [cv2.resize(x, self.test_size, interpolation=cv2.INTER_LINEAR).astype(np.uint8) for x in short_imgs_queue]\n            short_imgs_queue = [self.preproc(x, None, (self.test_size[1], self.test_size[0]))[0] for x in short_imgs_queue]\n        short_img = np.concatenate(short_imgs_queue, axis=0)\n        long_img = np.concatenate(long_imgs_queue, axis=0)\n        short_img = torch.from_numpy(short_img).unsqueeze(0)\n        long_img = torch.from_numpy(long_img).unsqueeze(0)\n        short_img = short_img.to(self.device)\n        long_img = long_img.to(self.device)\n        output = self.model((short_img, long_img))\n        output = self.postprocess(output)\n        output += (timestamp_format(seconds=frame_idx / self.fps),)\n        outputs.append(output)\n        frame_idx += 1\n    return outputs"
        ]
    }
]