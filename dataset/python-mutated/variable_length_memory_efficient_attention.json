[
    {
        "func_name": "variable_length_memory_efficient_attention",
        "original": "def variable_length_memory_efficient_attention(query, key, value, seq_lens, kv_seq_lens, mask=None, scale=None, causal=False):\n    \"\"\"\n    Cutlass Memory Efficient Variable Attention.\n    This method requires SM_ARCH in sm70, sm75, sm80.\n\n    Args:\n        query (Tensor): The Query Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\n        key (Tensor): The Key Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\n        value (Tensor): The Value Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\n        seq_lens (Tensor): The sequence lengths of the sequences in the batch, used to index query. Its shape is [batchsize, 1].\n        kv_seq_lens (Tensor): The sequence lengths of the sequences in the batch, used to index key and value. Its shape is [batchsize, 1].\n        mask (Tensor): The Mask Tensor. Its shape is [batchsize, 1, query_seq_len, key_seq_len].\n        scale (Float): The attention matrix's scale. Default is sqrt(1.0 / head_size).\n        causal (Bool): Whether causal masking is used or not. Default is False.\n    Returns:\n        Tensor: the output Tensor.\n\n    Examples:\n        .. code-block:: python\n\n            >>> # doctest: +REQUIRES(env:GPU)\n            >>> import math\n            >>> import paddle\n            >>> from paddle.incubate.nn.functional import variable_length_memory_efficient_attention\n            >>> paddle.device.set_device('gpu')\n\n            >>> batch = 1\n            >>> num_head = 8\n            >>> seq_len = 256\n            >>> head_size = 32\n\n            >>> dtype = paddle.float16\n\n            >>> query = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\n            >>> key = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\n            >>> value = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\n            >>> seq_lens = paddle.to_tensor([seq_len, ] * batch, dtype='int32')\n            >>> mask = paddle.randn([batch, 1, seq_len, seq_len], dtype=dtype)\n\n            >>> scale = float(1.0 / math.sqrt(head_size))\n\n            >>> def naive_attention_impl(query, key, value, mask, scale):\n            ...     qk_res = paddle.matmul(query, key, transpose_y=True)\n            ...     attention = qk_res * scale\n            ...     attention = attention + mask\n            ...     softmax_result = paddle.nn.functional.softmax(attention, -1)\n            ...     result = paddle.matmul(softmax_result, value)\n            ...     return result\n\n            >>> out = naive_attention_impl(query, key, value, mask, scale)\n            >>> # equals to: out = variable_length_memory_efficient_attention(query, key, value, seq_lens, seq_lens, mask, scale)\n\n            >>> print(out.shape) # [batch, seq_len, num_head, head_size]\n            [1, 8, 256, 32]\n    \"\"\"\n    if scale is None:\n        head_size = query.shape[3]\n        scale = float(1.0 / math.sqrt(head_size))\n    if in_dynamic_or_pir_mode():\n        return _C_ops.variable_length_memory_efficient_attention(query, key, value, seq_lens, kv_seq_lens, mask, scale, causal)\n    helper = LayerHelper('variable_length_memory_efficient_attention', **locals())\n    out = helper.create_variable_for_type_inference(dtype=query.dtype)\n    helper.append_op(type='variable_length_memory_efficient_attention', inputs={'query': query, 'key': key, 'value': value, 'seq_lens': seq_lens, 'kv_seq_lens': kv_seq_lens, 'mask': mask}, attrs={'scale': scale, 'causal': causal}, outputs={'out': out})\n    return out",
        "mutated": [
            "def variable_length_memory_efficient_attention(query, key, value, seq_lens, kv_seq_lens, mask=None, scale=None, causal=False):\n    if False:\n        i = 10\n    \"\\n    Cutlass Memory Efficient Variable Attention.\\n    This method requires SM_ARCH in sm70, sm75, sm80.\\n\\n    Args:\\n        query (Tensor): The Query Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        key (Tensor): The Key Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        value (Tensor): The Value Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        seq_lens (Tensor): The sequence lengths of the sequences in the batch, used to index query. Its shape is [batchsize, 1].\\n        kv_seq_lens (Tensor): The sequence lengths of the sequences in the batch, used to index key and value. Its shape is [batchsize, 1].\\n        mask (Tensor): The Mask Tensor. Its shape is [batchsize, 1, query_seq_len, key_seq_len].\\n        scale (Float): The attention matrix's scale. Default is sqrt(1.0 / head_size).\\n        causal (Bool): Whether causal masking is used or not. Default is False.\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import math\\n            >>> import paddle\\n            >>> from paddle.incubate.nn.functional import variable_length_memory_efficient_attention\\n            >>> paddle.device.set_device('gpu')\\n\\n            >>> batch = 1\\n            >>> num_head = 8\\n            >>> seq_len = 256\\n            >>> head_size = 32\\n\\n            >>> dtype = paddle.float16\\n\\n            >>> query = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> key = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> value = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> seq_lens = paddle.to_tensor([seq_len, ] * batch, dtype='int32')\\n            >>> mask = paddle.randn([batch, 1, seq_len, seq_len], dtype=dtype)\\n\\n            >>> scale = float(1.0 / math.sqrt(head_size))\\n\\n            >>> def naive_attention_impl(query, key, value, mask, scale):\\n            ...     qk_res = paddle.matmul(query, key, transpose_y=True)\\n            ...     attention = qk_res * scale\\n            ...     attention = attention + mask\\n            ...     softmax_result = paddle.nn.functional.softmax(attention, -1)\\n            ...     result = paddle.matmul(softmax_result, value)\\n            ...     return result\\n\\n            >>> out = naive_attention_impl(query, key, value, mask, scale)\\n            >>> # equals to: out = variable_length_memory_efficient_attention(query, key, value, seq_lens, seq_lens, mask, scale)\\n\\n            >>> print(out.shape) # [batch, seq_len, num_head, head_size]\\n            [1, 8, 256, 32]\\n    \"\n    if scale is None:\n        head_size = query.shape[3]\n        scale = float(1.0 / math.sqrt(head_size))\n    if in_dynamic_or_pir_mode():\n        return _C_ops.variable_length_memory_efficient_attention(query, key, value, seq_lens, kv_seq_lens, mask, scale, causal)\n    helper = LayerHelper('variable_length_memory_efficient_attention', **locals())\n    out = helper.create_variable_for_type_inference(dtype=query.dtype)\n    helper.append_op(type='variable_length_memory_efficient_attention', inputs={'query': query, 'key': key, 'value': value, 'seq_lens': seq_lens, 'kv_seq_lens': kv_seq_lens, 'mask': mask}, attrs={'scale': scale, 'causal': causal}, outputs={'out': out})\n    return out",
            "def variable_length_memory_efficient_attention(query, key, value, seq_lens, kv_seq_lens, mask=None, scale=None, causal=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Cutlass Memory Efficient Variable Attention.\\n    This method requires SM_ARCH in sm70, sm75, sm80.\\n\\n    Args:\\n        query (Tensor): The Query Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        key (Tensor): The Key Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        value (Tensor): The Value Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        seq_lens (Tensor): The sequence lengths of the sequences in the batch, used to index query. Its shape is [batchsize, 1].\\n        kv_seq_lens (Tensor): The sequence lengths of the sequences in the batch, used to index key and value. Its shape is [batchsize, 1].\\n        mask (Tensor): The Mask Tensor. Its shape is [batchsize, 1, query_seq_len, key_seq_len].\\n        scale (Float): The attention matrix's scale. Default is sqrt(1.0 / head_size).\\n        causal (Bool): Whether causal masking is used or not. Default is False.\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import math\\n            >>> import paddle\\n            >>> from paddle.incubate.nn.functional import variable_length_memory_efficient_attention\\n            >>> paddle.device.set_device('gpu')\\n\\n            >>> batch = 1\\n            >>> num_head = 8\\n            >>> seq_len = 256\\n            >>> head_size = 32\\n\\n            >>> dtype = paddle.float16\\n\\n            >>> query = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> key = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> value = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> seq_lens = paddle.to_tensor([seq_len, ] * batch, dtype='int32')\\n            >>> mask = paddle.randn([batch, 1, seq_len, seq_len], dtype=dtype)\\n\\n            >>> scale = float(1.0 / math.sqrt(head_size))\\n\\n            >>> def naive_attention_impl(query, key, value, mask, scale):\\n            ...     qk_res = paddle.matmul(query, key, transpose_y=True)\\n            ...     attention = qk_res * scale\\n            ...     attention = attention + mask\\n            ...     softmax_result = paddle.nn.functional.softmax(attention, -1)\\n            ...     result = paddle.matmul(softmax_result, value)\\n            ...     return result\\n\\n            >>> out = naive_attention_impl(query, key, value, mask, scale)\\n            >>> # equals to: out = variable_length_memory_efficient_attention(query, key, value, seq_lens, seq_lens, mask, scale)\\n\\n            >>> print(out.shape) # [batch, seq_len, num_head, head_size]\\n            [1, 8, 256, 32]\\n    \"\n    if scale is None:\n        head_size = query.shape[3]\n        scale = float(1.0 / math.sqrt(head_size))\n    if in_dynamic_or_pir_mode():\n        return _C_ops.variable_length_memory_efficient_attention(query, key, value, seq_lens, kv_seq_lens, mask, scale, causal)\n    helper = LayerHelper('variable_length_memory_efficient_attention', **locals())\n    out = helper.create_variable_for_type_inference(dtype=query.dtype)\n    helper.append_op(type='variable_length_memory_efficient_attention', inputs={'query': query, 'key': key, 'value': value, 'seq_lens': seq_lens, 'kv_seq_lens': kv_seq_lens, 'mask': mask}, attrs={'scale': scale, 'causal': causal}, outputs={'out': out})\n    return out",
            "def variable_length_memory_efficient_attention(query, key, value, seq_lens, kv_seq_lens, mask=None, scale=None, causal=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Cutlass Memory Efficient Variable Attention.\\n    This method requires SM_ARCH in sm70, sm75, sm80.\\n\\n    Args:\\n        query (Tensor): The Query Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        key (Tensor): The Key Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        value (Tensor): The Value Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        seq_lens (Tensor): The sequence lengths of the sequences in the batch, used to index query. Its shape is [batchsize, 1].\\n        kv_seq_lens (Tensor): The sequence lengths of the sequences in the batch, used to index key and value. Its shape is [batchsize, 1].\\n        mask (Tensor): The Mask Tensor. Its shape is [batchsize, 1, query_seq_len, key_seq_len].\\n        scale (Float): The attention matrix's scale. Default is sqrt(1.0 / head_size).\\n        causal (Bool): Whether causal masking is used or not. Default is False.\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import math\\n            >>> import paddle\\n            >>> from paddle.incubate.nn.functional import variable_length_memory_efficient_attention\\n            >>> paddle.device.set_device('gpu')\\n\\n            >>> batch = 1\\n            >>> num_head = 8\\n            >>> seq_len = 256\\n            >>> head_size = 32\\n\\n            >>> dtype = paddle.float16\\n\\n            >>> query = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> key = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> value = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> seq_lens = paddle.to_tensor([seq_len, ] * batch, dtype='int32')\\n            >>> mask = paddle.randn([batch, 1, seq_len, seq_len], dtype=dtype)\\n\\n            >>> scale = float(1.0 / math.sqrt(head_size))\\n\\n            >>> def naive_attention_impl(query, key, value, mask, scale):\\n            ...     qk_res = paddle.matmul(query, key, transpose_y=True)\\n            ...     attention = qk_res * scale\\n            ...     attention = attention + mask\\n            ...     softmax_result = paddle.nn.functional.softmax(attention, -1)\\n            ...     result = paddle.matmul(softmax_result, value)\\n            ...     return result\\n\\n            >>> out = naive_attention_impl(query, key, value, mask, scale)\\n            >>> # equals to: out = variable_length_memory_efficient_attention(query, key, value, seq_lens, seq_lens, mask, scale)\\n\\n            >>> print(out.shape) # [batch, seq_len, num_head, head_size]\\n            [1, 8, 256, 32]\\n    \"\n    if scale is None:\n        head_size = query.shape[3]\n        scale = float(1.0 / math.sqrt(head_size))\n    if in_dynamic_or_pir_mode():\n        return _C_ops.variable_length_memory_efficient_attention(query, key, value, seq_lens, kv_seq_lens, mask, scale, causal)\n    helper = LayerHelper('variable_length_memory_efficient_attention', **locals())\n    out = helper.create_variable_for_type_inference(dtype=query.dtype)\n    helper.append_op(type='variable_length_memory_efficient_attention', inputs={'query': query, 'key': key, 'value': value, 'seq_lens': seq_lens, 'kv_seq_lens': kv_seq_lens, 'mask': mask}, attrs={'scale': scale, 'causal': causal}, outputs={'out': out})\n    return out",
            "def variable_length_memory_efficient_attention(query, key, value, seq_lens, kv_seq_lens, mask=None, scale=None, causal=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Cutlass Memory Efficient Variable Attention.\\n    This method requires SM_ARCH in sm70, sm75, sm80.\\n\\n    Args:\\n        query (Tensor): The Query Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        key (Tensor): The Key Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        value (Tensor): The Value Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        seq_lens (Tensor): The sequence lengths of the sequences in the batch, used to index query. Its shape is [batchsize, 1].\\n        kv_seq_lens (Tensor): The sequence lengths of the sequences in the batch, used to index key and value. Its shape is [batchsize, 1].\\n        mask (Tensor): The Mask Tensor. Its shape is [batchsize, 1, query_seq_len, key_seq_len].\\n        scale (Float): The attention matrix's scale. Default is sqrt(1.0 / head_size).\\n        causal (Bool): Whether causal masking is used or not. Default is False.\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import math\\n            >>> import paddle\\n            >>> from paddle.incubate.nn.functional import variable_length_memory_efficient_attention\\n            >>> paddle.device.set_device('gpu')\\n\\n            >>> batch = 1\\n            >>> num_head = 8\\n            >>> seq_len = 256\\n            >>> head_size = 32\\n\\n            >>> dtype = paddle.float16\\n\\n            >>> query = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> key = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> value = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> seq_lens = paddle.to_tensor([seq_len, ] * batch, dtype='int32')\\n            >>> mask = paddle.randn([batch, 1, seq_len, seq_len], dtype=dtype)\\n\\n            >>> scale = float(1.0 / math.sqrt(head_size))\\n\\n            >>> def naive_attention_impl(query, key, value, mask, scale):\\n            ...     qk_res = paddle.matmul(query, key, transpose_y=True)\\n            ...     attention = qk_res * scale\\n            ...     attention = attention + mask\\n            ...     softmax_result = paddle.nn.functional.softmax(attention, -1)\\n            ...     result = paddle.matmul(softmax_result, value)\\n            ...     return result\\n\\n            >>> out = naive_attention_impl(query, key, value, mask, scale)\\n            >>> # equals to: out = variable_length_memory_efficient_attention(query, key, value, seq_lens, seq_lens, mask, scale)\\n\\n            >>> print(out.shape) # [batch, seq_len, num_head, head_size]\\n            [1, 8, 256, 32]\\n    \"\n    if scale is None:\n        head_size = query.shape[3]\n        scale = float(1.0 / math.sqrt(head_size))\n    if in_dynamic_or_pir_mode():\n        return _C_ops.variable_length_memory_efficient_attention(query, key, value, seq_lens, kv_seq_lens, mask, scale, causal)\n    helper = LayerHelper('variable_length_memory_efficient_attention', **locals())\n    out = helper.create_variable_for_type_inference(dtype=query.dtype)\n    helper.append_op(type='variable_length_memory_efficient_attention', inputs={'query': query, 'key': key, 'value': value, 'seq_lens': seq_lens, 'kv_seq_lens': kv_seq_lens, 'mask': mask}, attrs={'scale': scale, 'causal': causal}, outputs={'out': out})\n    return out",
            "def variable_length_memory_efficient_attention(query, key, value, seq_lens, kv_seq_lens, mask=None, scale=None, causal=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Cutlass Memory Efficient Variable Attention.\\n    This method requires SM_ARCH in sm70, sm75, sm80.\\n\\n    Args:\\n        query (Tensor): The Query Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        key (Tensor): The Key Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        value (Tensor): The Value Tensor. Its shape is [batchsize, seq_len, num_head, head_size].\\n        seq_lens (Tensor): The sequence lengths of the sequences in the batch, used to index query. Its shape is [batchsize, 1].\\n        kv_seq_lens (Tensor): The sequence lengths of the sequences in the batch, used to index key and value. Its shape is [batchsize, 1].\\n        mask (Tensor): The Mask Tensor. Its shape is [batchsize, 1, query_seq_len, key_seq_len].\\n        scale (Float): The attention matrix's scale. Default is sqrt(1.0 / head_size).\\n        causal (Bool): Whether causal masking is used or not. Default is False.\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import math\\n            >>> import paddle\\n            >>> from paddle.incubate.nn.functional import variable_length_memory_efficient_attention\\n            >>> paddle.device.set_device('gpu')\\n\\n            >>> batch = 1\\n            >>> num_head = 8\\n            >>> seq_len = 256\\n            >>> head_size = 32\\n\\n            >>> dtype = paddle.float16\\n\\n            >>> query = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> key = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> value = paddle.randn([batch, num_head, seq_len, head_size], dtype=dtype)\\n            >>> seq_lens = paddle.to_tensor([seq_len, ] * batch, dtype='int32')\\n            >>> mask = paddle.randn([batch, 1, seq_len, seq_len], dtype=dtype)\\n\\n            >>> scale = float(1.0 / math.sqrt(head_size))\\n\\n            >>> def naive_attention_impl(query, key, value, mask, scale):\\n            ...     qk_res = paddle.matmul(query, key, transpose_y=True)\\n            ...     attention = qk_res * scale\\n            ...     attention = attention + mask\\n            ...     softmax_result = paddle.nn.functional.softmax(attention, -1)\\n            ...     result = paddle.matmul(softmax_result, value)\\n            ...     return result\\n\\n            >>> out = naive_attention_impl(query, key, value, mask, scale)\\n            >>> # equals to: out = variable_length_memory_efficient_attention(query, key, value, seq_lens, seq_lens, mask, scale)\\n\\n            >>> print(out.shape) # [batch, seq_len, num_head, head_size]\\n            [1, 8, 256, 32]\\n    \"\n    if scale is None:\n        head_size = query.shape[3]\n        scale = float(1.0 / math.sqrt(head_size))\n    if in_dynamic_or_pir_mode():\n        return _C_ops.variable_length_memory_efficient_attention(query, key, value, seq_lens, kv_seq_lens, mask, scale, causal)\n    helper = LayerHelper('variable_length_memory_efficient_attention', **locals())\n    out = helper.create_variable_for_type_inference(dtype=query.dtype)\n    helper.append_op(type='variable_length_memory_efficient_attention', inputs={'query': query, 'key': key, 'value': value, 'seq_lens': seq_lens, 'kv_seq_lens': kv_seq_lens, 'mask': mask}, attrs={'scale': scale, 'causal': causal}, outputs={'out': out})\n    return out"
        ]
    }
]