[
    {
        "func_name": "save_prefixed_metrics",
        "original": "def save_prefixed_metrics(results, output_dir, file_name: str='all_results.json', metric_key_prefix: str='eval'):\n    \"\"\"\n    Save results while prefixing metric names.\n\n    Args:\n        results: (:obj:`dict`):\n            A dictionary of results.\n        output_dir: (:obj:`str`):\n            An output directory.\n        file_name: (:obj:`str`, `optional`, defaults to :obj:`all_results.json`):\n            An output file name.\n        metric_key_prefix: (:obj:`str`, `optional`, defaults to :obj:`eval`):\n            A metric name prefix.\n    \"\"\"\n    for key in list(results.keys()):\n        if not key.startswith(f'{metric_key_prefix}_'):\n            results[f'{metric_key_prefix}_{key}'] = results.pop(key)\n    with open(os.path.join(output_dir, file_name), 'w') as f:\n        json.dump(results, f, indent=4)",
        "mutated": [
            "def save_prefixed_metrics(results, output_dir, file_name: str='all_results.json', metric_key_prefix: str='eval'):\n    if False:\n        i = 10\n    '\\n    Save results while prefixing metric names.\\n\\n    Args:\\n        results: (:obj:`dict`):\\n            A dictionary of results.\\n        output_dir: (:obj:`str`):\\n            An output directory.\\n        file_name: (:obj:`str`, `optional`, defaults to :obj:`all_results.json`):\\n            An output file name.\\n        metric_key_prefix: (:obj:`str`, `optional`, defaults to :obj:`eval`):\\n            A metric name prefix.\\n    '\n    for key in list(results.keys()):\n        if not key.startswith(f'{metric_key_prefix}_'):\n            results[f'{metric_key_prefix}_{key}'] = results.pop(key)\n    with open(os.path.join(output_dir, file_name), 'w') as f:\n        json.dump(results, f, indent=4)",
            "def save_prefixed_metrics(results, output_dir, file_name: str='all_results.json', metric_key_prefix: str='eval'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Save results while prefixing metric names.\\n\\n    Args:\\n        results: (:obj:`dict`):\\n            A dictionary of results.\\n        output_dir: (:obj:`str`):\\n            An output directory.\\n        file_name: (:obj:`str`, `optional`, defaults to :obj:`all_results.json`):\\n            An output file name.\\n        metric_key_prefix: (:obj:`str`, `optional`, defaults to :obj:`eval`):\\n            A metric name prefix.\\n    '\n    for key in list(results.keys()):\n        if not key.startswith(f'{metric_key_prefix}_'):\n            results[f'{metric_key_prefix}_{key}'] = results.pop(key)\n    with open(os.path.join(output_dir, file_name), 'w') as f:\n        json.dump(results, f, indent=4)",
            "def save_prefixed_metrics(results, output_dir, file_name: str='all_results.json', metric_key_prefix: str='eval'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Save results while prefixing metric names.\\n\\n    Args:\\n        results: (:obj:`dict`):\\n            A dictionary of results.\\n        output_dir: (:obj:`str`):\\n            An output directory.\\n        file_name: (:obj:`str`, `optional`, defaults to :obj:`all_results.json`):\\n            An output file name.\\n        metric_key_prefix: (:obj:`str`, `optional`, defaults to :obj:`eval`):\\n            A metric name prefix.\\n    '\n    for key in list(results.keys()):\n        if not key.startswith(f'{metric_key_prefix}_'):\n            results[f'{metric_key_prefix}_{key}'] = results.pop(key)\n    with open(os.path.join(output_dir, file_name), 'w') as f:\n        json.dump(results, f, indent=4)",
            "def save_prefixed_metrics(results, output_dir, file_name: str='all_results.json', metric_key_prefix: str='eval'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Save results while prefixing metric names.\\n\\n    Args:\\n        results: (:obj:`dict`):\\n            A dictionary of results.\\n        output_dir: (:obj:`str`):\\n            An output directory.\\n        file_name: (:obj:`str`, `optional`, defaults to :obj:`all_results.json`):\\n            An output file name.\\n        metric_key_prefix: (:obj:`str`, `optional`, defaults to :obj:`eval`):\\n            A metric name prefix.\\n    '\n    for key in list(results.keys()):\n        if not key.startswith(f'{metric_key_prefix}_'):\n            results[f'{metric_key_prefix}_{key}'] = results.pop(key)\n    with open(os.path.join(output_dir, file_name), 'w') as f:\n        json.dump(results, f, indent=4)",
            "def save_prefixed_metrics(results, output_dir, file_name: str='all_results.json', metric_key_prefix: str='eval'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Save results while prefixing metric names.\\n\\n    Args:\\n        results: (:obj:`dict`):\\n            A dictionary of results.\\n        output_dir: (:obj:`str`):\\n            An output directory.\\n        file_name: (:obj:`str`, `optional`, defaults to :obj:`all_results.json`):\\n            An output file name.\\n        metric_key_prefix: (:obj:`str`, `optional`, defaults to :obj:`eval`):\\n            A metric name prefix.\\n    '\n    for key in list(results.keys()):\n        if not key.startswith(f'{metric_key_prefix}_'):\n            results[f'{metric_key_prefix}_{key}'] = results.pop(key)\n    with open(os.path.join(output_dir, file_name), 'w') as f:\n        json.dump(results, f, indent=4)"
        ]
    },
    {
        "func_name": "parse_args",
        "original": "def parse_args():\n    parser = argparse.ArgumentParser(description='Finetune a transformers model on a Question Answering task')\n    parser.add_argument('--dataset_name', type=str, default=None, help='The name of the dataset to use (via the datasets library).')\n    parser.add_argument('--dataset_config_name', type=str, default=None, help='The configuration name of the dataset to use (via the datasets library).')\n    parser.add_argument('--train_file', type=str, default=None, help='A csv or a json file containing the training data.')\n    parser.add_argument('--preprocessing_num_workers', type=int, default=1, help='A csv or a json file containing the training data.')\n    parser.add_argument('--do_predict', action='store_true', help='To do prediction on the question answering model')\n    parser.add_argument('--validation_file', type=str, default=None, help='A csv or a json file containing the validation data.')\n    parser.add_argument('--test_file', type=str, default=None, help='A csv or a json file containing the Prediction data.')\n    parser.add_argument('--max_seq_length', type=int, default=384, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_lengh` is passed.')\n    parser.add_argument('--pad_to_max_length', action='store_true', help='If passed, pad all samples to `max_seq_length`. Otherwise, dynamic padding is used.')\n    parser.add_argument('--model_name_or_path', type=str, help='Path to pretrained model or model identifier from huggingface.co/models.', required=False)\n    parser.add_argument('--config_name', type=str, default=None, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--use_slow_tokenizer', action='store_true', help='If passed, will use a slow tokenizer (not backed by the \ud83e\udd17 Tokenizers library).')\n    parser.add_argument('--per_device_train_batch_size', type=int, default=8, help='Batch size (per device) for the training dataloader.')\n    parser.add_argument('--per_device_eval_batch_size', type=int, default=8, help='Batch size (per device) for the evaluation dataloader.')\n    parser.add_argument('--learning_rate', type=float, default=5e-05, help='Initial learning rate (after the potential warmup period) to use.')\n    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay to use.')\n    parser.add_argument('--num_train_epochs', type=int, default=3, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_train_steps', type=int, default=None, help='Total number of training steps to perform. If provided, overrides num_train_epochs.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--lr_scheduler_type', type=SchedulerType, default='linear', help='The scheduler type to use.', choices=['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'])\n    parser.add_argument('--num_warmup_steps', type=int, default=0, help='Number of steps for the warmup in the lr scheduler.')\n    parser.add_argument('--output_dir', type=str, default=None, help='Where to store the final model.')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--doc_stride', type=int, default=128, help='When splitting up a long document into chunks how much stride to take between chunks.')\n    parser.add_argument('--n_best_size', type=int, default=20, help='The total number of n-best predictions to generate when looking for an answer.')\n    parser.add_argument('--null_score_diff_threshold', type=float, default=0.0, help='The threshold used to select the null answer: if the best answer has a score that is less than the score of the null answer minus this threshold, the null answer is selected for this example. Only useful when `version_2_with_negative=True`.')\n    parser.add_argument('--version_2_with_negative', action='store_true', help='If true, some of the examples do not have an answer.')\n    parser.add_argument('--max_answer_length', type=int, default=30, help='The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.')\n    parser.add_argument('--max_train_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of training examples to this value if set.')\n    parser.add_argument('--max_eval_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of evaluation examples to this value if set.')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--max_predict_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of prediction examples to this')\n    parser.add_argument('--model_type', type=str, default=None, help='Model type to use if training from scratch.', choices=MODEL_TYPES)\n    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')\n    parser.add_argument('--hub_model_id', type=str, help='The name of the repository to keep in sync with the local `output_dir`.')\n    parser.add_argument('--hub_token', type=str, help='The token to use to push to the Model Hub.')\n    parser.add_argument('--trust_remote_code', type=bool, default=False, help='Whether or not to allow for custom models defined on the Hub in their own modeling files. This optionshould only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.')\n    parser.add_argument('--checkpointing_steps', type=str, default=None, help=\"Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.\")\n    parser.add_argument('--resume_from_checkpoint', type=str, default=None, help='If the training should continue from a checkpoint folder.')\n    parser.add_argument('--with_tracking', action='store_true', help='Whether to enable experiment trackers for logging.')\n    parser.add_argument('--report_to', type=str, default='all', help='The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`, `\"wandb\"`, `\"comet_ml\"` and `\"clearml\"`. Use `\"all\"` (default) to report to all integrations. Only applicable when `--with_tracking` is passed.')\n    args = parser.parse_args()\n    if args.dataset_name is None and args.train_file is None and (args.validation_file is None) and (args.test_file is None):\n        raise ValueError('Need either a dataset name or a training/validation/test file.')\n    else:\n        if args.train_file is not None:\n            extension = args.train_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`train_file` should be a csv or a json file.'\n        if args.validation_file is not None:\n            extension = args.validation_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`validation_file` should be a csv or a json file.'\n        if args.test_file is not None:\n            extension = args.test_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`test_file` should be a csv or a json file.'\n    if args.push_to_hub:\n        assert args.output_dir is not None, 'Need an `output_dir` to create a repo when `--push_to_hub` is passed.'\n    return args",
        "mutated": [
            "def parse_args():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser(description='Finetune a transformers model on a Question Answering task')\n    parser.add_argument('--dataset_name', type=str, default=None, help='The name of the dataset to use (via the datasets library).')\n    parser.add_argument('--dataset_config_name', type=str, default=None, help='The configuration name of the dataset to use (via the datasets library).')\n    parser.add_argument('--train_file', type=str, default=None, help='A csv or a json file containing the training data.')\n    parser.add_argument('--preprocessing_num_workers', type=int, default=1, help='A csv or a json file containing the training data.')\n    parser.add_argument('--do_predict', action='store_true', help='To do prediction on the question answering model')\n    parser.add_argument('--validation_file', type=str, default=None, help='A csv or a json file containing the validation data.')\n    parser.add_argument('--test_file', type=str, default=None, help='A csv or a json file containing the Prediction data.')\n    parser.add_argument('--max_seq_length', type=int, default=384, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_lengh` is passed.')\n    parser.add_argument('--pad_to_max_length', action='store_true', help='If passed, pad all samples to `max_seq_length`. Otherwise, dynamic padding is used.')\n    parser.add_argument('--model_name_or_path', type=str, help='Path to pretrained model or model identifier from huggingface.co/models.', required=False)\n    parser.add_argument('--config_name', type=str, default=None, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--use_slow_tokenizer', action='store_true', help='If passed, will use a slow tokenizer (not backed by the \ud83e\udd17 Tokenizers library).')\n    parser.add_argument('--per_device_train_batch_size', type=int, default=8, help='Batch size (per device) for the training dataloader.')\n    parser.add_argument('--per_device_eval_batch_size', type=int, default=8, help='Batch size (per device) for the evaluation dataloader.')\n    parser.add_argument('--learning_rate', type=float, default=5e-05, help='Initial learning rate (after the potential warmup period) to use.')\n    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay to use.')\n    parser.add_argument('--num_train_epochs', type=int, default=3, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_train_steps', type=int, default=None, help='Total number of training steps to perform. If provided, overrides num_train_epochs.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--lr_scheduler_type', type=SchedulerType, default='linear', help='The scheduler type to use.', choices=['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'])\n    parser.add_argument('--num_warmup_steps', type=int, default=0, help='Number of steps for the warmup in the lr scheduler.')\n    parser.add_argument('--output_dir', type=str, default=None, help='Where to store the final model.')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--doc_stride', type=int, default=128, help='When splitting up a long document into chunks how much stride to take between chunks.')\n    parser.add_argument('--n_best_size', type=int, default=20, help='The total number of n-best predictions to generate when looking for an answer.')\n    parser.add_argument('--null_score_diff_threshold', type=float, default=0.0, help='The threshold used to select the null answer: if the best answer has a score that is less than the score of the null answer minus this threshold, the null answer is selected for this example. Only useful when `version_2_with_negative=True`.')\n    parser.add_argument('--version_2_with_negative', action='store_true', help='If true, some of the examples do not have an answer.')\n    parser.add_argument('--max_answer_length', type=int, default=30, help='The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.')\n    parser.add_argument('--max_train_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of training examples to this value if set.')\n    parser.add_argument('--max_eval_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of evaluation examples to this value if set.')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--max_predict_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of prediction examples to this')\n    parser.add_argument('--model_type', type=str, default=None, help='Model type to use if training from scratch.', choices=MODEL_TYPES)\n    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')\n    parser.add_argument('--hub_model_id', type=str, help='The name of the repository to keep in sync with the local `output_dir`.')\n    parser.add_argument('--hub_token', type=str, help='The token to use to push to the Model Hub.')\n    parser.add_argument('--trust_remote_code', type=bool, default=False, help='Whether or not to allow for custom models defined on the Hub in their own modeling files. This optionshould only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.')\n    parser.add_argument('--checkpointing_steps', type=str, default=None, help=\"Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.\")\n    parser.add_argument('--resume_from_checkpoint', type=str, default=None, help='If the training should continue from a checkpoint folder.')\n    parser.add_argument('--with_tracking', action='store_true', help='Whether to enable experiment trackers for logging.')\n    parser.add_argument('--report_to', type=str, default='all', help='The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`, `\"wandb\"`, `\"comet_ml\"` and `\"clearml\"`. Use `\"all\"` (default) to report to all integrations. Only applicable when `--with_tracking` is passed.')\n    args = parser.parse_args()\n    if args.dataset_name is None and args.train_file is None and (args.validation_file is None) and (args.test_file is None):\n        raise ValueError('Need either a dataset name or a training/validation/test file.')\n    else:\n        if args.train_file is not None:\n            extension = args.train_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`train_file` should be a csv or a json file.'\n        if args.validation_file is not None:\n            extension = args.validation_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`validation_file` should be a csv or a json file.'\n        if args.test_file is not None:\n            extension = args.test_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`test_file` should be a csv or a json file.'\n    if args.push_to_hub:\n        assert args.output_dir is not None, 'Need an `output_dir` to create a repo when `--push_to_hub` is passed.'\n    return args",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser(description='Finetune a transformers model on a Question Answering task')\n    parser.add_argument('--dataset_name', type=str, default=None, help='The name of the dataset to use (via the datasets library).')\n    parser.add_argument('--dataset_config_name', type=str, default=None, help='The configuration name of the dataset to use (via the datasets library).')\n    parser.add_argument('--train_file', type=str, default=None, help='A csv or a json file containing the training data.')\n    parser.add_argument('--preprocessing_num_workers', type=int, default=1, help='A csv or a json file containing the training data.')\n    parser.add_argument('--do_predict', action='store_true', help='To do prediction on the question answering model')\n    parser.add_argument('--validation_file', type=str, default=None, help='A csv or a json file containing the validation data.')\n    parser.add_argument('--test_file', type=str, default=None, help='A csv or a json file containing the Prediction data.')\n    parser.add_argument('--max_seq_length', type=int, default=384, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_lengh` is passed.')\n    parser.add_argument('--pad_to_max_length', action='store_true', help='If passed, pad all samples to `max_seq_length`. Otherwise, dynamic padding is used.')\n    parser.add_argument('--model_name_or_path', type=str, help='Path to pretrained model or model identifier from huggingface.co/models.', required=False)\n    parser.add_argument('--config_name', type=str, default=None, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--use_slow_tokenizer', action='store_true', help='If passed, will use a slow tokenizer (not backed by the \ud83e\udd17 Tokenizers library).')\n    parser.add_argument('--per_device_train_batch_size', type=int, default=8, help='Batch size (per device) for the training dataloader.')\n    parser.add_argument('--per_device_eval_batch_size', type=int, default=8, help='Batch size (per device) for the evaluation dataloader.')\n    parser.add_argument('--learning_rate', type=float, default=5e-05, help='Initial learning rate (after the potential warmup period) to use.')\n    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay to use.')\n    parser.add_argument('--num_train_epochs', type=int, default=3, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_train_steps', type=int, default=None, help='Total number of training steps to perform. If provided, overrides num_train_epochs.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--lr_scheduler_type', type=SchedulerType, default='linear', help='The scheduler type to use.', choices=['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'])\n    parser.add_argument('--num_warmup_steps', type=int, default=0, help='Number of steps for the warmup in the lr scheduler.')\n    parser.add_argument('--output_dir', type=str, default=None, help='Where to store the final model.')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--doc_stride', type=int, default=128, help='When splitting up a long document into chunks how much stride to take between chunks.')\n    parser.add_argument('--n_best_size', type=int, default=20, help='The total number of n-best predictions to generate when looking for an answer.')\n    parser.add_argument('--null_score_diff_threshold', type=float, default=0.0, help='The threshold used to select the null answer: if the best answer has a score that is less than the score of the null answer minus this threshold, the null answer is selected for this example. Only useful when `version_2_with_negative=True`.')\n    parser.add_argument('--version_2_with_negative', action='store_true', help='If true, some of the examples do not have an answer.')\n    parser.add_argument('--max_answer_length', type=int, default=30, help='The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.')\n    parser.add_argument('--max_train_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of training examples to this value if set.')\n    parser.add_argument('--max_eval_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of evaluation examples to this value if set.')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--max_predict_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of prediction examples to this')\n    parser.add_argument('--model_type', type=str, default=None, help='Model type to use if training from scratch.', choices=MODEL_TYPES)\n    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')\n    parser.add_argument('--hub_model_id', type=str, help='The name of the repository to keep in sync with the local `output_dir`.')\n    parser.add_argument('--hub_token', type=str, help='The token to use to push to the Model Hub.')\n    parser.add_argument('--trust_remote_code', type=bool, default=False, help='Whether or not to allow for custom models defined on the Hub in their own modeling files. This optionshould only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.')\n    parser.add_argument('--checkpointing_steps', type=str, default=None, help=\"Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.\")\n    parser.add_argument('--resume_from_checkpoint', type=str, default=None, help='If the training should continue from a checkpoint folder.')\n    parser.add_argument('--with_tracking', action='store_true', help='Whether to enable experiment trackers for logging.')\n    parser.add_argument('--report_to', type=str, default='all', help='The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`, `\"wandb\"`, `\"comet_ml\"` and `\"clearml\"`. Use `\"all\"` (default) to report to all integrations. Only applicable when `--with_tracking` is passed.')\n    args = parser.parse_args()\n    if args.dataset_name is None and args.train_file is None and (args.validation_file is None) and (args.test_file is None):\n        raise ValueError('Need either a dataset name or a training/validation/test file.')\n    else:\n        if args.train_file is not None:\n            extension = args.train_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`train_file` should be a csv or a json file.'\n        if args.validation_file is not None:\n            extension = args.validation_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`validation_file` should be a csv or a json file.'\n        if args.test_file is not None:\n            extension = args.test_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`test_file` should be a csv or a json file.'\n    if args.push_to_hub:\n        assert args.output_dir is not None, 'Need an `output_dir` to create a repo when `--push_to_hub` is passed.'\n    return args",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser(description='Finetune a transformers model on a Question Answering task')\n    parser.add_argument('--dataset_name', type=str, default=None, help='The name of the dataset to use (via the datasets library).')\n    parser.add_argument('--dataset_config_name', type=str, default=None, help='The configuration name of the dataset to use (via the datasets library).')\n    parser.add_argument('--train_file', type=str, default=None, help='A csv or a json file containing the training data.')\n    parser.add_argument('--preprocessing_num_workers', type=int, default=1, help='A csv or a json file containing the training data.')\n    parser.add_argument('--do_predict', action='store_true', help='To do prediction on the question answering model')\n    parser.add_argument('--validation_file', type=str, default=None, help='A csv or a json file containing the validation data.')\n    parser.add_argument('--test_file', type=str, default=None, help='A csv or a json file containing the Prediction data.')\n    parser.add_argument('--max_seq_length', type=int, default=384, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_lengh` is passed.')\n    parser.add_argument('--pad_to_max_length', action='store_true', help='If passed, pad all samples to `max_seq_length`. Otherwise, dynamic padding is used.')\n    parser.add_argument('--model_name_or_path', type=str, help='Path to pretrained model or model identifier from huggingface.co/models.', required=False)\n    parser.add_argument('--config_name', type=str, default=None, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--use_slow_tokenizer', action='store_true', help='If passed, will use a slow tokenizer (not backed by the \ud83e\udd17 Tokenizers library).')\n    parser.add_argument('--per_device_train_batch_size', type=int, default=8, help='Batch size (per device) for the training dataloader.')\n    parser.add_argument('--per_device_eval_batch_size', type=int, default=8, help='Batch size (per device) for the evaluation dataloader.')\n    parser.add_argument('--learning_rate', type=float, default=5e-05, help='Initial learning rate (after the potential warmup period) to use.')\n    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay to use.')\n    parser.add_argument('--num_train_epochs', type=int, default=3, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_train_steps', type=int, default=None, help='Total number of training steps to perform. If provided, overrides num_train_epochs.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--lr_scheduler_type', type=SchedulerType, default='linear', help='The scheduler type to use.', choices=['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'])\n    parser.add_argument('--num_warmup_steps', type=int, default=0, help='Number of steps for the warmup in the lr scheduler.')\n    parser.add_argument('--output_dir', type=str, default=None, help='Where to store the final model.')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--doc_stride', type=int, default=128, help='When splitting up a long document into chunks how much stride to take between chunks.')\n    parser.add_argument('--n_best_size', type=int, default=20, help='The total number of n-best predictions to generate when looking for an answer.')\n    parser.add_argument('--null_score_diff_threshold', type=float, default=0.0, help='The threshold used to select the null answer: if the best answer has a score that is less than the score of the null answer minus this threshold, the null answer is selected for this example. Only useful when `version_2_with_negative=True`.')\n    parser.add_argument('--version_2_with_negative', action='store_true', help='If true, some of the examples do not have an answer.')\n    parser.add_argument('--max_answer_length', type=int, default=30, help='The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.')\n    parser.add_argument('--max_train_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of training examples to this value if set.')\n    parser.add_argument('--max_eval_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of evaluation examples to this value if set.')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--max_predict_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of prediction examples to this')\n    parser.add_argument('--model_type', type=str, default=None, help='Model type to use if training from scratch.', choices=MODEL_TYPES)\n    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')\n    parser.add_argument('--hub_model_id', type=str, help='The name of the repository to keep in sync with the local `output_dir`.')\n    parser.add_argument('--hub_token', type=str, help='The token to use to push to the Model Hub.')\n    parser.add_argument('--trust_remote_code', type=bool, default=False, help='Whether or not to allow for custom models defined on the Hub in their own modeling files. This optionshould only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.')\n    parser.add_argument('--checkpointing_steps', type=str, default=None, help=\"Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.\")\n    parser.add_argument('--resume_from_checkpoint', type=str, default=None, help='If the training should continue from a checkpoint folder.')\n    parser.add_argument('--with_tracking', action='store_true', help='Whether to enable experiment trackers for logging.')\n    parser.add_argument('--report_to', type=str, default='all', help='The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`, `\"wandb\"`, `\"comet_ml\"` and `\"clearml\"`. Use `\"all\"` (default) to report to all integrations. Only applicable when `--with_tracking` is passed.')\n    args = parser.parse_args()\n    if args.dataset_name is None and args.train_file is None and (args.validation_file is None) and (args.test_file is None):\n        raise ValueError('Need either a dataset name or a training/validation/test file.')\n    else:\n        if args.train_file is not None:\n            extension = args.train_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`train_file` should be a csv or a json file.'\n        if args.validation_file is not None:\n            extension = args.validation_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`validation_file` should be a csv or a json file.'\n        if args.test_file is not None:\n            extension = args.test_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`test_file` should be a csv or a json file.'\n    if args.push_to_hub:\n        assert args.output_dir is not None, 'Need an `output_dir` to create a repo when `--push_to_hub` is passed.'\n    return args",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser(description='Finetune a transformers model on a Question Answering task')\n    parser.add_argument('--dataset_name', type=str, default=None, help='The name of the dataset to use (via the datasets library).')\n    parser.add_argument('--dataset_config_name', type=str, default=None, help='The configuration name of the dataset to use (via the datasets library).')\n    parser.add_argument('--train_file', type=str, default=None, help='A csv or a json file containing the training data.')\n    parser.add_argument('--preprocessing_num_workers', type=int, default=1, help='A csv or a json file containing the training data.')\n    parser.add_argument('--do_predict', action='store_true', help='To do prediction on the question answering model')\n    parser.add_argument('--validation_file', type=str, default=None, help='A csv or a json file containing the validation data.')\n    parser.add_argument('--test_file', type=str, default=None, help='A csv or a json file containing the Prediction data.')\n    parser.add_argument('--max_seq_length', type=int, default=384, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_lengh` is passed.')\n    parser.add_argument('--pad_to_max_length', action='store_true', help='If passed, pad all samples to `max_seq_length`. Otherwise, dynamic padding is used.')\n    parser.add_argument('--model_name_or_path', type=str, help='Path to pretrained model or model identifier from huggingface.co/models.', required=False)\n    parser.add_argument('--config_name', type=str, default=None, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--use_slow_tokenizer', action='store_true', help='If passed, will use a slow tokenizer (not backed by the \ud83e\udd17 Tokenizers library).')\n    parser.add_argument('--per_device_train_batch_size', type=int, default=8, help='Batch size (per device) for the training dataloader.')\n    parser.add_argument('--per_device_eval_batch_size', type=int, default=8, help='Batch size (per device) for the evaluation dataloader.')\n    parser.add_argument('--learning_rate', type=float, default=5e-05, help='Initial learning rate (after the potential warmup period) to use.')\n    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay to use.')\n    parser.add_argument('--num_train_epochs', type=int, default=3, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_train_steps', type=int, default=None, help='Total number of training steps to perform. If provided, overrides num_train_epochs.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--lr_scheduler_type', type=SchedulerType, default='linear', help='The scheduler type to use.', choices=['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'])\n    parser.add_argument('--num_warmup_steps', type=int, default=0, help='Number of steps for the warmup in the lr scheduler.')\n    parser.add_argument('--output_dir', type=str, default=None, help='Where to store the final model.')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--doc_stride', type=int, default=128, help='When splitting up a long document into chunks how much stride to take between chunks.')\n    parser.add_argument('--n_best_size', type=int, default=20, help='The total number of n-best predictions to generate when looking for an answer.')\n    parser.add_argument('--null_score_diff_threshold', type=float, default=0.0, help='The threshold used to select the null answer: if the best answer has a score that is less than the score of the null answer minus this threshold, the null answer is selected for this example. Only useful when `version_2_with_negative=True`.')\n    parser.add_argument('--version_2_with_negative', action='store_true', help='If true, some of the examples do not have an answer.')\n    parser.add_argument('--max_answer_length', type=int, default=30, help='The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.')\n    parser.add_argument('--max_train_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of training examples to this value if set.')\n    parser.add_argument('--max_eval_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of evaluation examples to this value if set.')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--max_predict_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of prediction examples to this')\n    parser.add_argument('--model_type', type=str, default=None, help='Model type to use if training from scratch.', choices=MODEL_TYPES)\n    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')\n    parser.add_argument('--hub_model_id', type=str, help='The name of the repository to keep in sync with the local `output_dir`.')\n    parser.add_argument('--hub_token', type=str, help='The token to use to push to the Model Hub.')\n    parser.add_argument('--trust_remote_code', type=bool, default=False, help='Whether or not to allow for custom models defined on the Hub in their own modeling files. This optionshould only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.')\n    parser.add_argument('--checkpointing_steps', type=str, default=None, help=\"Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.\")\n    parser.add_argument('--resume_from_checkpoint', type=str, default=None, help='If the training should continue from a checkpoint folder.')\n    parser.add_argument('--with_tracking', action='store_true', help='Whether to enable experiment trackers for logging.')\n    parser.add_argument('--report_to', type=str, default='all', help='The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`, `\"wandb\"`, `\"comet_ml\"` and `\"clearml\"`. Use `\"all\"` (default) to report to all integrations. Only applicable when `--with_tracking` is passed.')\n    args = parser.parse_args()\n    if args.dataset_name is None and args.train_file is None and (args.validation_file is None) and (args.test_file is None):\n        raise ValueError('Need either a dataset name or a training/validation/test file.')\n    else:\n        if args.train_file is not None:\n            extension = args.train_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`train_file` should be a csv or a json file.'\n        if args.validation_file is not None:\n            extension = args.validation_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`validation_file` should be a csv or a json file.'\n        if args.test_file is not None:\n            extension = args.test_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`test_file` should be a csv or a json file.'\n    if args.push_to_hub:\n        assert args.output_dir is not None, 'Need an `output_dir` to create a repo when `--push_to_hub` is passed.'\n    return args",
            "def parse_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser(description='Finetune a transformers model on a Question Answering task')\n    parser.add_argument('--dataset_name', type=str, default=None, help='The name of the dataset to use (via the datasets library).')\n    parser.add_argument('--dataset_config_name', type=str, default=None, help='The configuration name of the dataset to use (via the datasets library).')\n    parser.add_argument('--train_file', type=str, default=None, help='A csv or a json file containing the training data.')\n    parser.add_argument('--preprocessing_num_workers', type=int, default=1, help='A csv or a json file containing the training data.')\n    parser.add_argument('--do_predict', action='store_true', help='To do prediction on the question answering model')\n    parser.add_argument('--validation_file', type=str, default=None, help='A csv or a json file containing the validation data.')\n    parser.add_argument('--test_file', type=str, default=None, help='A csv or a json file containing the Prediction data.')\n    parser.add_argument('--max_seq_length', type=int, default=384, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded if `--pad_to_max_lengh` is passed.')\n    parser.add_argument('--pad_to_max_length', action='store_true', help='If passed, pad all samples to `max_seq_length`. Otherwise, dynamic padding is used.')\n    parser.add_argument('--model_name_or_path', type=str, help='Path to pretrained model or model identifier from huggingface.co/models.', required=False)\n    parser.add_argument('--config_name', type=str, default=None, help='Pretrained config name or path if not the same as model_name')\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--use_slow_tokenizer', action='store_true', help='If passed, will use a slow tokenizer (not backed by the \ud83e\udd17 Tokenizers library).')\n    parser.add_argument('--per_device_train_batch_size', type=int, default=8, help='Batch size (per device) for the training dataloader.')\n    parser.add_argument('--per_device_eval_batch_size', type=int, default=8, help='Batch size (per device) for the evaluation dataloader.')\n    parser.add_argument('--learning_rate', type=float, default=5e-05, help='Initial learning rate (after the potential warmup period) to use.')\n    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay to use.')\n    parser.add_argument('--num_train_epochs', type=int, default=3, help='Total number of training epochs to perform.')\n    parser.add_argument('--max_train_steps', type=int, default=None, help='Total number of training steps to perform. If provided, overrides num_train_epochs.')\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='Number of updates steps to accumulate before performing a backward/update pass.')\n    parser.add_argument('--lr_scheduler_type', type=SchedulerType, default='linear', help='The scheduler type to use.', choices=['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup'])\n    parser.add_argument('--num_warmup_steps', type=int, default=0, help='Number of steps for the warmup in the lr scheduler.')\n    parser.add_argument('--output_dir', type=str, default=None, help='Where to store the final model.')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--doc_stride', type=int, default=128, help='When splitting up a long document into chunks how much stride to take between chunks.')\n    parser.add_argument('--n_best_size', type=int, default=20, help='The total number of n-best predictions to generate when looking for an answer.')\n    parser.add_argument('--null_score_diff_threshold', type=float, default=0.0, help='The threshold used to select the null answer: if the best answer has a score that is less than the score of the null answer minus this threshold, the null answer is selected for this example. Only useful when `version_2_with_negative=True`.')\n    parser.add_argument('--version_2_with_negative', action='store_true', help='If true, some of the examples do not have an answer.')\n    parser.add_argument('--max_answer_length', type=int, default=30, help='The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.')\n    parser.add_argument('--max_train_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of training examples to this value if set.')\n    parser.add_argument('--max_eval_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of evaluation examples to this value if set.')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--max_predict_samples', type=int, default=None, help='For debugging purposes or quicker training, truncate the number of prediction examples to this')\n    parser.add_argument('--model_type', type=str, default=None, help='Model type to use if training from scratch.', choices=MODEL_TYPES)\n    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')\n    parser.add_argument('--hub_model_id', type=str, help='The name of the repository to keep in sync with the local `output_dir`.')\n    parser.add_argument('--hub_token', type=str, help='The token to use to push to the Model Hub.')\n    parser.add_argument('--trust_remote_code', type=bool, default=False, help='Whether or not to allow for custom models defined on the Hub in their own modeling files. This optionshould only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.')\n    parser.add_argument('--checkpointing_steps', type=str, default=None, help=\"Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.\")\n    parser.add_argument('--resume_from_checkpoint', type=str, default=None, help='If the training should continue from a checkpoint folder.')\n    parser.add_argument('--with_tracking', action='store_true', help='Whether to enable experiment trackers for logging.')\n    parser.add_argument('--report_to', type=str, default='all', help='The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`, `\"wandb\"`, `\"comet_ml\"` and `\"clearml\"`. Use `\"all\"` (default) to report to all integrations. Only applicable when `--with_tracking` is passed.')\n    args = parser.parse_args()\n    if args.dataset_name is None and args.train_file is None and (args.validation_file is None) and (args.test_file is None):\n        raise ValueError('Need either a dataset name or a training/validation/test file.')\n    else:\n        if args.train_file is not None:\n            extension = args.train_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`train_file` should be a csv or a json file.'\n        if args.validation_file is not None:\n            extension = args.validation_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`validation_file` should be a csv or a json file.'\n        if args.test_file is not None:\n            extension = args.test_file.split('.')[-1]\n            assert extension in ['csv', 'json'], '`test_file` should be a csv or a json file.'\n    if args.push_to_hub:\n        assert args.output_dir is not None, 'Need an `output_dir` to create a repo when `--push_to_hub` is passed.'\n    return args"
        ]
    },
    {
        "func_name": "prepare_train_features",
        "original": "def prepare_train_features(examples):\n    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n    tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    offset_mapping = tokenized_examples.pop('offset_mapping')\n    tokenized_examples['start_positions'] = []\n    tokenized_examples['end_positions'] = []\n    for (i, offsets) in enumerate(offset_mapping):\n        input_ids = tokenized_examples['input_ids'][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        sample_index = sample_mapping[i]\n        answers = examples[answer_column_name][sample_index]\n        if len(answers['answer_start']) == 0:\n            tokenized_examples['start_positions'].append(cls_index)\n            tokenized_examples['end_positions'].append(cls_index)\n        else:\n            start_char = answers['answer_start'][0]\n            end_char = start_char + len(answers['text'][0])\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples['start_positions'].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples['end_positions'].append(token_end_index + 1)\n    return tokenized_examples",
        "mutated": [
            "def prepare_train_features(examples):\n    if False:\n        i = 10\n    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n    tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    offset_mapping = tokenized_examples.pop('offset_mapping')\n    tokenized_examples['start_positions'] = []\n    tokenized_examples['end_positions'] = []\n    for (i, offsets) in enumerate(offset_mapping):\n        input_ids = tokenized_examples['input_ids'][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        sample_index = sample_mapping[i]\n        answers = examples[answer_column_name][sample_index]\n        if len(answers['answer_start']) == 0:\n            tokenized_examples['start_positions'].append(cls_index)\n            tokenized_examples['end_positions'].append(cls_index)\n        else:\n            start_char = answers['answer_start'][0]\n            end_char = start_char + len(answers['text'][0])\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples['start_positions'].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples['end_positions'].append(token_end_index + 1)\n    return tokenized_examples",
            "def prepare_train_features(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n    tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    offset_mapping = tokenized_examples.pop('offset_mapping')\n    tokenized_examples['start_positions'] = []\n    tokenized_examples['end_positions'] = []\n    for (i, offsets) in enumerate(offset_mapping):\n        input_ids = tokenized_examples['input_ids'][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        sample_index = sample_mapping[i]\n        answers = examples[answer_column_name][sample_index]\n        if len(answers['answer_start']) == 0:\n            tokenized_examples['start_positions'].append(cls_index)\n            tokenized_examples['end_positions'].append(cls_index)\n        else:\n            start_char = answers['answer_start'][0]\n            end_char = start_char + len(answers['text'][0])\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples['start_positions'].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples['end_positions'].append(token_end_index + 1)\n    return tokenized_examples",
            "def prepare_train_features(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n    tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    offset_mapping = tokenized_examples.pop('offset_mapping')\n    tokenized_examples['start_positions'] = []\n    tokenized_examples['end_positions'] = []\n    for (i, offsets) in enumerate(offset_mapping):\n        input_ids = tokenized_examples['input_ids'][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        sample_index = sample_mapping[i]\n        answers = examples[answer_column_name][sample_index]\n        if len(answers['answer_start']) == 0:\n            tokenized_examples['start_positions'].append(cls_index)\n            tokenized_examples['end_positions'].append(cls_index)\n        else:\n            start_char = answers['answer_start'][0]\n            end_char = start_char + len(answers['text'][0])\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples['start_positions'].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples['end_positions'].append(token_end_index + 1)\n    return tokenized_examples",
            "def prepare_train_features(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n    tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    offset_mapping = tokenized_examples.pop('offset_mapping')\n    tokenized_examples['start_positions'] = []\n    tokenized_examples['end_positions'] = []\n    for (i, offsets) in enumerate(offset_mapping):\n        input_ids = tokenized_examples['input_ids'][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        sample_index = sample_mapping[i]\n        answers = examples[answer_column_name][sample_index]\n        if len(answers['answer_start']) == 0:\n            tokenized_examples['start_positions'].append(cls_index)\n            tokenized_examples['end_positions'].append(cls_index)\n        else:\n            start_char = answers['answer_start'][0]\n            end_char = start_char + len(answers['text'][0])\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples['start_positions'].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples['end_positions'].append(token_end_index + 1)\n    return tokenized_examples",
            "def prepare_train_features(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n    tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    offset_mapping = tokenized_examples.pop('offset_mapping')\n    tokenized_examples['start_positions'] = []\n    tokenized_examples['end_positions'] = []\n    for (i, offsets) in enumerate(offset_mapping):\n        input_ids = tokenized_examples['input_ids'][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        sample_index = sample_mapping[i]\n        answers = examples[answer_column_name][sample_index]\n        if len(answers['answer_start']) == 0:\n            tokenized_examples['start_positions'].append(cls_index)\n            tokenized_examples['end_positions'].append(cls_index)\n        else:\n            start_char = answers['answer_start'][0]\n            end_char = start_char + len(answers['text'][0])\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples['start_positions'].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples['end_positions'].append(token_end_index + 1)\n    return tokenized_examples"
        ]
    },
    {
        "func_name": "prepare_validation_features",
        "original": "def prepare_validation_features(examples):\n    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n    tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    tokenized_examples['example_id'] = []\n    for i in range(len(tokenized_examples['input_ids'])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples['example_id'].append(examples['id'][sample_index])\n        tokenized_examples['offset_mapping'][i] = [o if sequence_ids[k] == context_index else None for (k, o) in enumerate(tokenized_examples['offset_mapping'][i])]\n    return tokenized_examples",
        "mutated": [
            "def prepare_validation_features(examples):\n    if False:\n        i = 10\n    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n    tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    tokenized_examples['example_id'] = []\n    for i in range(len(tokenized_examples['input_ids'])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples['example_id'].append(examples['id'][sample_index])\n        tokenized_examples['offset_mapping'][i] = [o if sequence_ids[k] == context_index else None for (k, o) in enumerate(tokenized_examples['offset_mapping'][i])]\n    return tokenized_examples",
            "def prepare_validation_features(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n    tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    tokenized_examples['example_id'] = []\n    for i in range(len(tokenized_examples['input_ids'])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples['example_id'].append(examples['id'][sample_index])\n        tokenized_examples['offset_mapping'][i] = [o if sequence_ids[k] == context_index else None for (k, o) in enumerate(tokenized_examples['offset_mapping'][i])]\n    return tokenized_examples",
            "def prepare_validation_features(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n    tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    tokenized_examples['example_id'] = []\n    for i in range(len(tokenized_examples['input_ids'])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples['example_id'].append(examples['id'][sample_index])\n        tokenized_examples['offset_mapping'][i] = [o if sequence_ids[k] == context_index else None for (k, o) in enumerate(tokenized_examples['offset_mapping'][i])]\n    return tokenized_examples",
            "def prepare_validation_features(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n    tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    tokenized_examples['example_id'] = []\n    for i in range(len(tokenized_examples['input_ids'])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples['example_id'].append(examples['id'][sample_index])\n        tokenized_examples['offset_mapping'][i] = [o if sequence_ids[k] == context_index else None for (k, o) in enumerate(tokenized_examples['offset_mapping'][i])]\n    return tokenized_examples",
            "def prepare_validation_features(examples):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n    tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    tokenized_examples['example_id'] = []\n    for i in range(len(tokenized_examples['input_ids'])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples['example_id'].append(examples['id'][sample_index])\n        tokenized_examples['offset_mapping'][i] = [o if sequence_ids[k] == context_index else None for (k, o) in enumerate(tokenized_examples['offset_mapping'][i])]\n    return tokenized_examples"
        ]
    },
    {
        "func_name": "post_processing_function",
        "original": "def post_processing_function(examples, features, predictions, stage='eval'):\n    predictions = postprocess_qa_predictions(examples=examples, features=features, predictions=predictions, version_2_with_negative=args.version_2_with_negative, n_best_size=args.n_best_size, max_answer_length=args.max_answer_length, null_score_diff_threshold=args.null_score_diff_threshold, output_dir=args.output_dir, prefix=stage)\n    if args.version_2_with_negative:\n        formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.0} for (k, v) in predictions.items()]\n    else:\n        formatted_predictions = [{'id': k, 'prediction_text': v} for (k, v) in predictions.items()]\n    references = [{'id': ex['id'], 'answers': ex[answer_column_name]} for ex in examples]\n    return EvalPrediction(predictions=formatted_predictions, label_ids=references)",
        "mutated": [
            "def post_processing_function(examples, features, predictions, stage='eval'):\n    if False:\n        i = 10\n    predictions = postprocess_qa_predictions(examples=examples, features=features, predictions=predictions, version_2_with_negative=args.version_2_with_negative, n_best_size=args.n_best_size, max_answer_length=args.max_answer_length, null_score_diff_threshold=args.null_score_diff_threshold, output_dir=args.output_dir, prefix=stage)\n    if args.version_2_with_negative:\n        formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.0} for (k, v) in predictions.items()]\n    else:\n        formatted_predictions = [{'id': k, 'prediction_text': v} for (k, v) in predictions.items()]\n    references = [{'id': ex['id'], 'answers': ex[answer_column_name]} for ex in examples]\n    return EvalPrediction(predictions=formatted_predictions, label_ids=references)",
            "def post_processing_function(examples, features, predictions, stage='eval'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    predictions = postprocess_qa_predictions(examples=examples, features=features, predictions=predictions, version_2_with_negative=args.version_2_with_negative, n_best_size=args.n_best_size, max_answer_length=args.max_answer_length, null_score_diff_threshold=args.null_score_diff_threshold, output_dir=args.output_dir, prefix=stage)\n    if args.version_2_with_negative:\n        formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.0} for (k, v) in predictions.items()]\n    else:\n        formatted_predictions = [{'id': k, 'prediction_text': v} for (k, v) in predictions.items()]\n    references = [{'id': ex['id'], 'answers': ex[answer_column_name]} for ex in examples]\n    return EvalPrediction(predictions=formatted_predictions, label_ids=references)",
            "def post_processing_function(examples, features, predictions, stage='eval'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    predictions = postprocess_qa_predictions(examples=examples, features=features, predictions=predictions, version_2_with_negative=args.version_2_with_negative, n_best_size=args.n_best_size, max_answer_length=args.max_answer_length, null_score_diff_threshold=args.null_score_diff_threshold, output_dir=args.output_dir, prefix=stage)\n    if args.version_2_with_negative:\n        formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.0} for (k, v) in predictions.items()]\n    else:\n        formatted_predictions = [{'id': k, 'prediction_text': v} for (k, v) in predictions.items()]\n    references = [{'id': ex['id'], 'answers': ex[answer_column_name]} for ex in examples]\n    return EvalPrediction(predictions=formatted_predictions, label_ids=references)",
            "def post_processing_function(examples, features, predictions, stage='eval'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    predictions = postprocess_qa_predictions(examples=examples, features=features, predictions=predictions, version_2_with_negative=args.version_2_with_negative, n_best_size=args.n_best_size, max_answer_length=args.max_answer_length, null_score_diff_threshold=args.null_score_diff_threshold, output_dir=args.output_dir, prefix=stage)\n    if args.version_2_with_negative:\n        formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.0} for (k, v) in predictions.items()]\n    else:\n        formatted_predictions = [{'id': k, 'prediction_text': v} for (k, v) in predictions.items()]\n    references = [{'id': ex['id'], 'answers': ex[answer_column_name]} for ex in examples]\n    return EvalPrediction(predictions=formatted_predictions, label_ids=references)",
            "def post_processing_function(examples, features, predictions, stage='eval'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    predictions = postprocess_qa_predictions(examples=examples, features=features, predictions=predictions, version_2_with_negative=args.version_2_with_negative, n_best_size=args.n_best_size, max_answer_length=args.max_answer_length, null_score_diff_threshold=args.null_score_diff_threshold, output_dir=args.output_dir, prefix=stage)\n    if args.version_2_with_negative:\n        formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.0} for (k, v) in predictions.items()]\n    else:\n        formatted_predictions = [{'id': k, 'prediction_text': v} for (k, v) in predictions.items()]\n    references = [{'id': ex['id'], 'answers': ex[answer_column_name]} for ex in examples]\n    return EvalPrediction(predictions=formatted_predictions, label_ids=references)"
        ]
    },
    {
        "func_name": "create_and_fill_np_array",
        "original": "def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n    \"\"\"\n        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n\n        Args:\n            start_or_end_logits(:obj:`tensor`):\n                This is the output predictions of the model. We can only enter either start or end logits.\n            eval_dataset: Evaluation dataset\n            max_len(:obj:`int`):\n                The maximum length of the output tensor. ( See the model.eval() part for more details )\n        \"\"\"\n    step = 0\n    logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n    for (i, output_logit) in enumerate(start_or_end_logits):\n        batch_size = output_logit.shape[0]\n        cols = output_logit.shape[1]\n        if step + batch_size < len(dataset):\n            logits_concat[step:step + batch_size, :cols] = output_logit\n        else:\n            logits_concat[step:, :cols] = output_logit[:len(dataset) - step]\n        step += batch_size\n    return logits_concat",
        "mutated": [
            "def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n    if False:\n        i = 10\n    '\\n        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\\n\\n        Args:\\n            start_or_end_logits(:obj:`tensor`):\\n                This is the output predictions of the model. We can only enter either start or end logits.\\n            eval_dataset: Evaluation dataset\\n            max_len(:obj:`int`):\\n                The maximum length of the output tensor. ( See the model.eval() part for more details )\\n        '\n    step = 0\n    logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n    for (i, output_logit) in enumerate(start_or_end_logits):\n        batch_size = output_logit.shape[0]\n        cols = output_logit.shape[1]\n        if step + batch_size < len(dataset):\n            logits_concat[step:step + batch_size, :cols] = output_logit\n        else:\n            logits_concat[step:, :cols] = output_logit[:len(dataset) - step]\n        step += batch_size\n    return logits_concat",
            "def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\\n\\n        Args:\\n            start_or_end_logits(:obj:`tensor`):\\n                This is the output predictions of the model. We can only enter either start or end logits.\\n            eval_dataset: Evaluation dataset\\n            max_len(:obj:`int`):\\n                The maximum length of the output tensor. ( See the model.eval() part for more details )\\n        '\n    step = 0\n    logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n    for (i, output_logit) in enumerate(start_or_end_logits):\n        batch_size = output_logit.shape[0]\n        cols = output_logit.shape[1]\n        if step + batch_size < len(dataset):\n            logits_concat[step:step + batch_size, :cols] = output_logit\n        else:\n            logits_concat[step:, :cols] = output_logit[:len(dataset) - step]\n        step += batch_size\n    return logits_concat",
            "def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\\n\\n        Args:\\n            start_or_end_logits(:obj:`tensor`):\\n                This is the output predictions of the model. We can only enter either start or end logits.\\n            eval_dataset: Evaluation dataset\\n            max_len(:obj:`int`):\\n                The maximum length of the output tensor. ( See the model.eval() part for more details )\\n        '\n    step = 0\n    logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n    for (i, output_logit) in enumerate(start_or_end_logits):\n        batch_size = output_logit.shape[0]\n        cols = output_logit.shape[1]\n        if step + batch_size < len(dataset):\n            logits_concat[step:step + batch_size, :cols] = output_logit\n        else:\n            logits_concat[step:, :cols] = output_logit[:len(dataset) - step]\n        step += batch_size\n    return logits_concat",
            "def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\\n\\n        Args:\\n            start_or_end_logits(:obj:`tensor`):\\n                This is the output predictions of the model. We can only enter either start or end logits.\\n            eval_dataset: Evaluation dataset\\n            max_len(:obj:`int`):\\n                The maximum length of the output tensor. ( See the model.eval() part for more details )\\n        '\n    step = 0\n    logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n    for (i, output_logit) in enumerate(start_or_end_logits):\n        batch_size = output_logit.shape[0]\n        cols = output_logit.shape[1]\n        if step + batch_size < len(dataset):\n            logits_concat[step:step + batch_size, :cols] = output_logit\n        else:\n            logits_concat[step:, :cols] = output_logit[:len(dataset) - step]\n        step += batch_size\n    return logits_concat",
            "def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\\n\\n        Args:\\n            start_or_end_logits(:obj:`tensor`):\\n                This is the output predictions of the model. We can only enter either start or end logits.\\n            eval_dataset: Evaluation dataset\\n            max_len(:obj:`int`):\\n                The maximum length of the output tensor. ( See the model.eval() part for more details )\\n        '\n    step = 0\n    logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n    for (i, output_logit) in enumerate(start_or_end_logits):\n        batch_size = output_logit.shape[0]\n        cols = output_logit.shape[1]\n        if step + batch_size < len(dataset):\n            logits_concat[step:step + batch_size, :cols] = output_logit\n        else:\n            logits_concat[step:, :cols] = output_logit[:len(dataset) - step]\n        step += batch_size\n    return logits_concat"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    args = parse_args()\n    send_example_telemetry('run_qa_no_trainer', args)\n    accelerator_log_kwargs = {}\n    if args.with_tracking:\n        accelerator_log_kwargs['log_with'] = args.report_to\n        accelerator_log_kwargs['project_dir'] = args.output_dir\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n            with open(os.path.join(args.output_dir, '.gitignore'), 'w+') as gitignore:\n                if 'step_*' not in gitignore:\n                    gitignore.write('step_*\\n')\n                if 'epoch_*' not in gitignore:\n                    gitignore.write('epoch_*\\n')\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    if args.dataset_name is not None:\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files['train'] = args.train_file\n        if args.validation_file is not None:\n            data_files['validation'] = args.validation_file\n        if args.test_file is not None:\n            data_files['test'] = args.test_file\n        extension = args.train_file.split('.')[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files, field='data')\n    if args.config_name:\n        config = AutoConfig.from_pretrained(args.config_name, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning('You are instantiating a new config instance from scratch.')\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=True, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True, trust_remote_code=args.trust_remote_code)\n    else:\n        raise ValueError('You are instantiating a new tokenizer from scratch. This is not supported by this script. You can do it from another script, save it, and load it from here, using --tokenizer_name.')\n    if args.model_name_or_path:\n        model = AutoModelForQuestionAnswering.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, trust_remote_code=args.trust_remote_code)\n    else:\n        logger.info('Training new model from scratch')\n        model = AutoModelForQuestionAnswering.from_config(config, trust_remote_code=args.trust_remote_code)\n    column_names = raw_datasets['train'].column_names\n    question_column_name = 'question' if 'question' in column_names else column_names[0]\n    context_column_name = 'context' if 'context' in column_names else column_names[1]\n    answer_column_name = 'answers' if 'answers' in column_names else column_names[2]\n    pad_on_right = tokenizer.padding_side == 'right'\n    if args.max_seq_length > tokenizer.model_max_length:\n        logger.warning(f'The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.')\n    max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n\n    def prepare_train_features(examples):\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n        tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n        sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n        offset_mapping = tokenized_examples.pop('offset_mapping')\n        tokenized_examples['start_positions'] = []\n        tokenized_examples['end_positions'] = []\n        for (i, offsets) in enumerate(offset_mapping):\n            input_ids = tokenized_examples['input_ids'][i]\n            cls_index = input_ids.index(tokenizer.cls_token_id)\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            sample_index = sample_mapping[i]\n            answers = examples[answer_column_name][sample_index]\n            if len(answers['answer_start']) == 0:\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                start_char = answers['answer_start'][0]\n                end_char = start_char + len(answers['text'][0])\n                token_start_index = 0\n                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                    token_start_index += 1\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                    token_end_index -= 1\n                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                    tokenized_examples['start_positions'].append(cls_index)\n                    tokenized_examples['end_positions'].append(cls_index)\n                else:\n                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    tokenized_examples['start_positions'].append(token_start_index - 1)\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    tokenized_examples['end_positions'].append(token_end_index + 1)\n        return tokenized_examples\n    if 'train' not in raw_datasets:\n        raise ValueError('--do_train requires a train dataset')\n    train_dataset = raw_datasets['train']\n    if args.max_train_samples is not None:\n        train_dataset = train_dataset.select(range(args.max_train_samples))\n    with accelerator.main_process_first():\n        train_dataset = train_dataset.map(prepare_train_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on train dataset')\n        if args.max_train_samples is not None:\n            train_dataset = train_dataset.select(range(args.max_train_samples))\n\n    def prepare_validation_features(examples):\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n        tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n        sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n        tokenized_examples['example_id'] = []\n        for i in range(len(tokenized_examples['input_ids'])):\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            context_index = 1 if pad_on_right else 0\n            sample_index = sample_mapping[i]\n            tokenized_examples['example_id'].append(examples['id'][sample_index])\n            tokenized_examples['offset_mapping'][i] = [o if sequence_ids[k] == context_index else None for (k, o) in enumerate(tokenized_examples['offset_mapping'][i])]\n        return tokenized_examples\n    if 'validation' not in raw_datasets:\n        raise ValueError('--do_eval requires a validation dataset')\n    eval_examples = raw_datasets['validation']\n    if args.max_eval_samples is not None:\n        eval_examples = eval_examples.select(range(args.max_eval_samples))\n    with accelerator.main_process_first():\n        eval_dataset = eval_examples.map(prepare_validation_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on validation dataset')\n    if args.max_eval_samples is not None:\n        eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n    if args.do_predict:\n        if 'test' not in raw_datasets:\n            raise ValueError('--do_predict requires a test dataset')\n        predict_examples = raw_datasets['test']\n        if args.max_predict_samples is not None:\n            predict_examples = predict_examples.select(range(args.max_predict_samples))\n        with accelerator.main_process_first():\n            predict_dataset = predict_examples.map(prepare_validation_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on prediction dataset')\n            if args.max_predict_samples is not None:\n                predict_dataset = predict_dataset.select(range(args.max_predict_samples))\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f'Sample {index} of the training set: {train_dataset[index]}.')\n    if args.pad_to_max_length:\n        data_collator = default_data_collator\n    else:\n        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if accelerator.use_fp16 else None)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n    eval_dataset_for_model = eval_dataset.remove_columns(['example_id', 'offset_mapping'])\n    eval_dataloader = DataLoader(eval_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n    if args.do_predict:\n        predict_dataset_for_model = predict_dataset.remove_columns(['example_id', 'offset_mapping'])\n        predict_dataloader = DataLoader(predict_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    def post_processing_function(examples, features, predictions, stage='eval'):\n        predictions = postprocess_qa_predictions(examples=examples, features=features, predictions=predictions, version_2_with_negative=args.version_2_with_negative, n_best_size=args.n_best_size, max_answer_length=args.max_answer_length, null_score_diff_threshold=args.null_score_diff_threshold, output_dir=args.output_dir, prefix=stage)\n        if args.version_2_with_negative:\n            formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.0} for (k, v) in predictions.items()]\n        else:\n            formatted_predictions = [{'id': k, 'prediction_text': v} for (k, v) in predictions.items()]\n        references = [{'id': ex['id'], 'answers': ex[answer_column_name]} for ex in examples]\n        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n    metric = evaluate.load('squad_v2' if args.version_2_with_negative else 'squad')\n\n    def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n        \"\"\"\n        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n\n        Args:\n            start_or_end_logits(:obj:`tensor`):\n                This is the output predictions of the model. We can only enter either start or end logits.\n            eval_dataset: Evaluation dataset\n            max_len(:obj:`int`):\n                The maximum length of the output tensor. ( See the model.eval() part for more details )\n        \"\"\"\n        step = 0\n        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n        for (i, output_logit) in enumerate(start_or_end_logits):\n            batch_size = output_logit.shape[0]\n            cols = output_logit.shape[1]\n            if step + batch_size < len(dataset):\n                logits_concat[step:step + batch_size, :cols] = output_logit\n            else:\n                logits_concat[step:, :cols] = output_logit[:len(dataset) - step]\n            step += batch_size\n        return logits_concat\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if not any((nd in n for nd in no_decay))], 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if any((nd in n for nd in no_decay))], 'weight_decay': 0.0}]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n    lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer, num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps, num_training_steps=args.max_train_steps * args.gradient_accumulation_steps)\n    (model, optimizer, train_dataloader, eval_dataloader, lr_scheduler) = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader, lr_scheduler)\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n    if args.with_tracking:\n        experiment_config = vars(args)\n        experiment_config['lr_scheduler_type'] = experiment_config['lr_scheduler_type'].value\n        accelerator.init_trackers('qa_no_trainer', experiment_config)\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    logger.info('***** Running training *****')\n    logger.info(f'  Num examples = {len(train_dataset)}')\n    logger.info(f'  Num Epochs = {args.num_train_epochs}')\n    logger.info(f'  Instantaneous batch size per device = {args.per_device_train_batch_size}')\n    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')\n    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')\n    logger.info(f'  Total optimization steps = {args.max_train_steps}')\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != '':\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n        accelerator.print(f'Resumed from checkpoint: {checkpoint_path}')\n        accelerator.load_state(checkpoint_path)\n        training_difference = os.path.splitext(path)[0]\n        if 'epoch' in training_difference:\n            starting_epoch = int(training_difference.replace('epoch_', '')) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            resume_step = int(training_difference.replace('step_', '')) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n    progress_bar.update(completed_steps)\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and (resume_step is not None):\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for (step, batch) in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0:\n                    output_dir = f'step_{completed_steps}'\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n            if completed_steps >= args.max_train_steps:\n                break\n        if args.checkpointing_steps == 'epoch':\n            output_dir = f'epoch_{epoch}'\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save)\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(commit_message=f'Training in progress epoch {epoch}', blocking=False, auto_lfs_prune=True)\n    logger.info('***** Running Evaluation *****')\n    logger.info(f'  Num examples = {len(eval_dataset)}')\n    logger.info(f'  Batch size = {args.per_device_eval_batch_size}')\n    all_start_logits = []\n    all_end_logits = []\n    model.eval()\n    for (step, batch) in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(**batch)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n            if not args.pad_to_max_length:\n                start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n            all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n            all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n    max_len = max([x.shape[1] for x in all_start_logits])\n    start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n    end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n    del all_start_logits\n    del all_end_logits\n    outputs_numpy = (start_logits_concat, end_logits_concat)\n    prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n    eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n    logger.info(f'Evaluation metrics: {eval_metric}')\n    if args.do_predict:\n        logger.info('***** Running Prediction *****')\n        logger.info(f'  Num examples = {len(predict_dataset)}')\n        logger.info(f'  Batch size = {args.per_device_eval_batch_size}')\n        all_start_logits = []\n        all_end_logits = []\n        model.eval()\n        for (step, batch) in enumerate(predict_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n                start_logits = outputs.start_logits\n                end_logits = outputs.end_logits\n                if not args.pad_to_max_length:\n                    start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                    end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n                all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n                all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n        max_len = max([x.shape[1] for x in all_start_logits])\n        start_logits_concat = create_and_fill_np_array(all_start_logits, predict_dataset, max_len)\n        end_logits_concat = create_and_fill_np_array(all_end_logits, predict_dataset, max_len)\n        del all_start_logits\n        del all_end_logits\n        outputs_numpy = (start_logits_concat, end_logits_concat)\n        prediction = post_processing_function(predict_examples, predict_dataset, outputs_numpy)\n        predict_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n        logger.info(f'Predict metrics: {predict_metric}')\n    if args.with_tracking:\n        log = {'squad_v2' if args.version_2_with_negative else 'squad': eval_metric, 'train_loss': total_loss.item() / len(train_dataloader), 'epoch': epoch, 'step': completed_steps}\n    if args.do_predict:\n        log['squad_v2_predict' if args.version_2_with_negative else 'squad_predict'] = predict_metric\n        accelerator.log(log, step=completed_steps)\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save)\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message='End of training', auto_lfs_prune=True)\n            logger.info(json.dumps(eval_metric, indent=4))\n            save_prefixed_metrics(eval_metric, args.output_dir)",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    args = parse_args()\n    send_example_telemetry('run_qa_no_trainer', args)\n    accelerator_log_kwargs = {}\n    if args.with_tracking:\n        accelerator_log_kwargs['log_with'] = args.report_to\n        accelerator_log_kwargs['project_dir'] = args.output_dir\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n            with open(os.path.join(args.output_dir, '.gitignore'), 'w+') as gitignore:\n                if 'step_*' not in gitignore:\n                    gitignore.write('step_*\\n')\n                if 'epoch_*' not in gitignore:\n                    gitignore.write('epoch_*\\n')\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    if args.dataset_name is not None:\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files['train'] = args.train_file\n        if args.validation_file is not None:\n            data_files['validation'] = args.validation_file\n        if args.test_file is not None:\n            data_files['test'] = args.test_file\n        extension = args.train_file.split('.')[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files, field='data')\n    if args.config_name:\n        config = AutoConfig.from_pretrained(args.config_name, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning('You are instantiating a new config instance from scratch.')\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=True, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True, trust_remote_code=args.trust_remote_code)\n    else:\n        raise ValueError('You are instantiating a new tokenizer from scratch. This is not supported by this script. You can do it from another script, save it, and load it from here, using --tokenizer_name.')\n    if args.model_name_or_path:\n        model = AutoModelForQuestionAnswering.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, trust_remote_code=args.trust_remote_code)\n    else:\n        logger.info('Training new model from scratch')\n        model = AutoModelForQuestionAnswering.from_config(config, trust_remote_code=args.trust_remote_code)\n    column_names = raw_datasets['train'].column_names\n    question_column_name = 'question' if 'question' in column_names else column_names[0]\n    context_column_name = 'context' if 'context' in column_names else column_names[1]\n    answer_column_name = 'answers' if 'answers' in column_names else column_names[2]\n    pad_on_right = tokenizer.padding_side == 'right'\n    if args.max_seq_length > tokenizer.model_max_length:\n        logger.warning(f'The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.')\n    max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n\n    def prepare_train_features(examples):\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n        tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n        sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n        offset_mapping = tokenized_examples.pop('offset_mapping')\n        tokenized_examples['start_positions'] = []\n        tokenized_examples['end_positions'] = []\n        for (i, offsets) in enumerate(offset_mapping):\n            input_ids = tokenized_examples['input_ids'][i]\n            cls_index = input_ids.index(tokenizer.cls_token_id)\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            sample_index = sample_mapping[i]\n            answers = examples[answer_column_name][sample_index]\n            if len(answers['answer_start']) == 0:\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                start_char = answers['answer_start'][0]\n                end_char = start_char + len(answers['text'][0])\n                token_start_index = 0\n                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                    token_start_index += 1\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                    token_end_index -= 1\n                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                    tokenized_examples['start_positions'].append(cls_index)\n                    tokenized_examples['end_positions'].append(cls_index)\n                else:\n                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    tokenized_examples['start_positions'].append(token_start_index - 1)\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    tokenized_examples['end_positions'].append(token_end_index + 1)\n        return tokenized_examples\n    if 'train' not in raw_datasets:\n        raise ValueError('--do_train requires a train dataset')\n    train_dataset = raw_datasets['train']\n    if args.max_train_samples is not None:\n        train_dataset = train_dataset.select(range(args.max_train_samples))\n    with accelerator.main_process_first():\n        train_dataset = train_dataset.map(prepare_train_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on train dataset')\n        if args.max_train_samples is not None:\n            train_dataset = train_dataset.select(range(args.max_train_samples))\n\n    def prepare_validation_features(examples):\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n        tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n        sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n        tokenized_examples['example_id'] = []\n        for i in range(len(tokenized_examples['input_ids'])):\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            context_index = 1 if pad_on_right else 0\n            sample_index = sample_mapping[i]\n            tokenized_examples['example_id'].append(examples['id'][sample_index])\n            tokenized_examples['offset_mapping'][i] = [o if sequence_ids[k] == context_index else None for (k, o) in enumerate(tokenized_examples['offset_mapping'][i])]\n        return tokenized_examples\n    if 'validation' not in raw_datasets:\n        raise ValueError('--do_eval requires a validation dataset')\n    eval_examples = raw_datasets['validation']\n    if args.max_eval_samples is not None:\n        eval_examples = eval_examples.select(range(args.max_eval_samples))\n    with accelerator.main_process_first():\n        eval_dataset = eval_examples.map(prepare_validation_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on validation dataset')\n    if args.max_eval_samples is not None:\n        eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n    if args.do_predict:\n        if 'test' not in raw_datasets:\n            raise ValueError('--do_predict requires a test dataset')\n        predict_examples = raw_datasets['test']\n        if args.max_predict_samples is not None:\n            predict_examples = predict_examples.select(range(args.max_predict_samples))\n        with accelerator.main_process_first():\n            predict_dataset = predict_examples.map(prepare_validation_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on prediction dataset')\n            if args.max_predict_samples is not None:\n                predict_dataset = predict_dataset.select(range(args.max_predict_samples))\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f'Sample {index} of the training set: {train_dataset[index]}.')\n    if args.pad_to_max_length:\n        data_collator = default_data_collator\n    else:\n        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if accelerator.use_fp16 else None)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n    eval_dataset_for_model = eval_dataset.remove_columns(['example_id', 'offset_mapping'])\n    eval_dataloader = DataLoader(eval_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n    if args.do_predict:\n        predict_dataset_for_model = predict_dataset.remove_columns(['example_id', 'offset_mapping'])\n        predict_dataloader = DataLoader(predict_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    def post_processing_function(examples, features, predictions, stage='eval'):\n        predictions = postprocess_qa_predictions(examples=examples, features=features, predictions=predictions, version_2_with_negative=args.version_2_with_negative, n_best_size=args.n_best_size, max_answer_length=args.max_answer_length, null_score_diff_threshold=args.null_score_diff_threshold, output_dir=args.output_dir, prefix=stage)\n        if args.version_2_with_negative:\n            formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.0} for (k, v) in predictions.items()]\n        else:\n            formatted_predictions = [{'id': k, 'prediction_text': v} for (k, v) in predictions.items()]\n        references = [{'id': ex['id'], 'answers': ex[answer_column_name]} for ex in examples]\n        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n    metric = evaluate.load('squad_v2' if args.version_2_with_negative else 'squad')\n\n    def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n        \"\"\"\n        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n\n        Args:\n            start_or_end_logits(:obj:`tensor`):\n                This is the output predictions of the model. We can only enter either start or end logits.\n            eval_dataset: Evaluation dataset\n            max_len(:obj:`int`):\n                The maximum length of the output tensor. ( See the model.eval() part for more details )\n        \"\"\"\n        step = 0\n        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n        for (i, output_logit) in enumerate(start_or_end_logits):\n            batch_size = output_logit.shape[0]\n            cols = output_logit.shape[1]\n            if step + batch_size < len(dataset):\n                logits_concat[step:step + batch_size, :cols] = output_logit\n            else:\n                logits_concat[step:, :cols] = output_logit[:len(dataset) - step]\n            step += batch_size\n        return logits_concat\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if not any((nd in n for nd in no_decay))], 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if any((nd in n for nd in no_decay))], 'weight_decay': 0.0}]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n    lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer, num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps, num_training_steps=args.max_train_steps * args.gradient_accumulation_steps)\n    (model, optimizer, train_dataloader, eval_dataloader, lr_scheduler) = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader, lr_scheduler)\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n    if args.with_tracking:\n        experiment_config = vars(args)\n        experiment_config['lr_scheduler_type'] = experiment_config['lr_scheduler_type'].value\n        accelerator.init_trackers('qa_no_trainer', experiment_config)\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    logger.info('***** Running training *****')\n    logger.info(f'  Num examples = {len(train_dataset)}')\n    logger.info(f'  Num Epochs = {args.num_train_epochs}')\n    logger.info(f'  Instantaneous batch size per device = {args.per_device_train_batch_size}')\n    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')\n    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')\n    logger.info(f'  Total optimization steps = {args.max_train_steps}')\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != '':\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n        accelerator.print(f'Resumed from checkpoint: {checkpoint_path}')\n        accelerator.load_state(checkpoint_path)\n        training_difference = os.path.splitext(path)[0]\n        if 'epoch' in training_difference:\n            starting_epoch = int(training_difference.replace('epoch_', '')) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            resume_step = int(training_difference.replace('step_', '')) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n    progress_bar.update(completed_steps)\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and (resume_step is not None):\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for (step, batch) in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0:\n                    output_dir = f'step_{completed_steps}'\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n            if completed_steps >= args.max_train_steps:\n                break\n        if args.checkpointing_steps == 'epoch':\n            output_dir = f'epoch_{epoch}'\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save)\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(commit_message=f'Training in progress epoch {epoch}', blocking=False, auto_lfs_prune=True)\n    logger.info('***** Running Evaluation *****')\n    logger.info(f'  Num examples = {len(eval_dataset)}')\n    logger.info(f'  Batch size = {args.per_device_eval_batch_size}')\n    all_start_logits = []\n    all_end_logits = []\n    model.eval()\n    for (step, batch) in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(**batch)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n            if not args.pad_to_max_length:\n                start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n            all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n            all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n    max_len = max([x.shape[1] for x in all_start_logits])\n    start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n    end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n    del all_start_logits\n    del all_end_logits\n    outputs_numpy = (start_logits_concat, end_logits_concat)\n    prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n    eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n    logger.info(f'Evaluation metrics: {eval_metric}')\n    if args.do_predict:\n        logger.info('***** Running Prediction *****')\n        logger.info(f'  Num examples = {len(predict_dataset)}')\n        logger.info(f'  Batch size = {args.per_device_eval_batch_size}')\n        all_start_logits = []\n        all_end_logits = []\n        model.eval()\n        for (step, batch) in enumerate(predict_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n                start_logits = outputs.start_logits\n                end_logits = outputs.end_logits\n                if not args.pad_to_max_length:\n                    start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                    end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n                all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n                all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n        max_len = max([x.shape[1] for x in all_start_logits])\n        start_logits_concat = create_and_fill_np_array(all_start_logits, predict_dataset, max_len)\n        end_logits_concat = create_and_fill_np_array(all_end_logits, predict_dataset, max_len)\n        del all_start_logits\n        del all_end_logits\n        outputs_numpy = (start_logits_concat, end_logits_concat)\n        prediction = post_processing_function(predict_examples, predict_dataset, outputs_numpy)\n        predict_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n        logger.info(f'Predict metrics: {predict_metric}')\n    if args.with_tracking:\n        log = {'squad_v2' if args.version_2_with_negative else 'squad': eval_metric, 'train_loss': total_loss.item() / len(train_dataloader), 'epoch': epoch, 'step': completed_steps}\n    if args.do_predict:\n        log['squad_v2_predict' if args.version_2_with_negative else 'squad_predict'] = predict_metric\n        accelerator.log(log, step=completed_steps)\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save)\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message='End of training', auto_lfs_prune=True)\n            logger.info(json.dumps(eval_metric, indent=4))\n            save_prefixed_metrics(eval_metric, args.output_dir)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    args = parse_args()\n    send_example_telemetry('run_qa_no_trainer', args)\n    accelerator_log_kwargs = {}\n    if args.with_tracking:\n        accelerator_log_kwargs['log_with'] = args.report_to\n        accelerator_log_kwargs['project_dir'] = args.output_dir\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n            with open(os.path.join(args.output_dir, '.gitignore'), 'w+') as gitignore:\n                if 'step_*' not in gitignore:\n                    gitignore.write('step_*\\n')\n                if 'epoch_*' not in gitignore:\n                    gitignore.write('epoch_*\\n')\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    if args.dataset_name is not None:\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files['train'] = args.train_file\n        if args.validation_file is not None:\n            data_files['validation'] = args.validation_file\n        if args.test_file is not None:\n            data_files['test'] = args.test_file\n        extension = args.train_file.split('.')[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files, field='data')\n    if args.config_name:\n        config = AutoConfig.from_pretrained(args.config_name, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning('You are instantiating a new config instance from scratch.')\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=True, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True, trust_remote_code=args.trust_remote_code)\n    else:\n        raise ValueError('You are instantiating a new tokenizer from scratch. This is not supported by this script. You can do it from another script, save it, and load it from here, using --tokenizer_name.')\n    if args.model_name_or_path:\n        model = AutoModelForQuestionAnswering.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, trust_remote_code=args.trust_remote_code)\n    else:\n        logger.info('Training new model from scratch')\n        model = AutoModelForQuestionAnswering.from_config(config, trust_remote_code=args.trust_remote_code)\n    column_names = raw_datasets['train'].column_names\n    question_column_name = 'question' if 'question' in column_names else column_names[0]\n    context_column_name = 'context' if 'context' in column_names else column_names[1]\n    answer_column_name = 'answers' if 'answers' in column_names else column_names[2]\n    pad_on_right = tokenizer.padding_side == 'right'\n    if args.max_seq_length > tokenizer.model_max_length:\n        logger.warning(f'The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.')\n    max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n\n    def prepare_train_features(examples):\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n        tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n        sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n        offset_mapping = tokenized_examples.pop('offset_mapping')\n        tokenized_examples['start_positions'] = []\n        tokenized_examples['end_positions'] = []\n        for (i, offsets) in enumerate(offset_mapping):\n            input_ids = tokenized_examples['input_ids'][i]\n            cls_index = input_ids.index(tokenizer.cls_token_id)\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            sample_index = sample_mapping[i]\n            answers = examples[answer_column_name][sample_index]\n            if len(answers['answer_start']) == 0:\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                start_char = answers['answer_start'][0]\n                end_char = start_char + len(answers['text'][0])\n                token_start_index = 0\n                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                    token_start_index += 1\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                    token_end_index -= 1\n                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                    tokenized_examples['start_positions'].append(cls_index)\n                    tokenized_examples['end_positions'].append(cls_index)\n                else:\n                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    tokenized_examples['start_positions'].append(token_start_index - 1)\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    tokenized_examples['end_positions'].append(token_end_index + 1)\n        return tokenized_examples\n    if 'train' not in raw_datasets:\n        raise ValueError('--do_train requires a train dataset')\n    train_dataset = raw_datasets['train']\n    if args.max_train_samples is not None:\n        train_dataset = train_dataset.select(range(args.max_train_samples))\n    with accelerator.main_process_first():\n        train_dataset = train_dataset.map(prepare_train_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on train dataset')\n        if args.max_train_samples is not None:\n            train_dataset = train_dataset.select(range(args.max_train_samples))\n\n    def prepare_validation_features(examples):\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n        tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n        sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n        tokenized_examples['example_id'] = []\n        for i in range(len(tokenized_examples['input_ids'])):\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            context_index = 1 if pad_on_right else 0\n            sample_index = sample_mapping[i]\n            tokenized_examples['example_id'].append(examples['id'][sample_index])\n            tokenized_examples['offset_mapping'][i] = [o if sequence_ids[k] == context_index else None for (k, o) in enumerate(tokenized_examples['offset_mapping'][i])]\n        return tokenized_examples\n    if 'validation' not in raw_datasets:\n        raise ValueError('--do_eval requires a validation dataset')\n    eval_examples = raw_datasets['validation']\n    if args.max_eval_samples is not None:\n        eval_examples = eval_examples.select(range(args.max_eval_samples))\n    with accelerator.main_process_first():\n        eval_dataset = eval_examples.map(prepare_validation_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on validation dataset')\n    if args.max_eval_samples is not None:\n        eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n    if args.do_predict:\n        if 'test' not in raw_datasets:\n            raise ValueError('--do_predict requires a test dataset')\n        predict_examples = raw_datasets['test']\n        if args.max_predict_samples is not None:\n            predict_examples = predict_examples.select(range(args.max_predict_samples))\n        with accelerator.main_process_first():\n            predict_dataset = predict_examples.map(prepare_validation_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on prediction dataset')\n            if args.max_predict_samples is not None:\n                predict_dataset = predict_dataset.select(range(args.max_predict_samples))\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f'Sample {index} of the training set: {train_dataset[index]}.')\n    if args.pad_to_max_length:\n        data_collator = default_data_collator\n    else:\n        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if accelerator.use_fp16 else None)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n    eval_dataset_for_model = eval_dataset.remove_columns(['example_id', 'offset_mapping'])\n    eval_dataloader = DataLoader(eval_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n    if args.do_predict:\n        predict_dataset_for_model = predict_dataset.remove_columns(['example_id', 'offset_mapping'])\n        predict_dataloader = DataLoader(predict_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    def post_processing_function(examples, features, predictions, stage='eval'):\n        predictions = postprocess_qa_predictions(examples=examples, features=features, predictions=predictions, version_2_with_negative=args.version_2_with_negative, n_best_size=args.n_best_size, max_answer_length=args.max_answer_length, null_score_diff_threshold=args.null_score_diff_threshold, output_dir=args.output_dir, prefix=stage)\n        if args.version_2_with_negative:\n            formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.0} for (k, v) in predictions.items()]\n        else:\n            formatted_predictions = [{'id': k, 'prediction_text': v} for (k, v) in predictions.items()]\n        references = [{'id': ex['id'], 'answers': ex[answer_column_name]} for ex in examples]\n        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n    metric = evaluate.load('squad_v2' if args.version_2_with_negative else 'squad')\n\n    def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n        \"\"\"\n        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n\n        Args:\n            start_or_end_logits(:obj:`tensor`):\n                This is the output predictions of the model. We can only enter either start or end logits.\n            eval_dataset: Evaluation dataset\n            max_len(:obj:`int`):\n                The maximum length of the output tensor. ( See the model.eval() part for more details )\n        \"\"\"\n        step = 0\n        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n        for (i, output_logit) in enumerate(start_or_end_logits):\n            batch_size = output_logit.shape[0]\n            cols = output_logit.shape[1]\n            if step + batch_size < len(dataset):\n                logits_concat[step:step + batch_size, :cols] = output_logit\n            else:\n                logits_concat[step:, :cols] = output_logit[:len(dataset) - step]\n            step += batch_size\n        return logits_concat\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if not any((nd in n for nd in no_decay))], 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if any((nd in n for nd in no_decay))], 'weight_decay': 0.0}]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n    lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer, num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps, num_training_steps=args.max_train_steps * args.gradient_accumulation_steps)\n    (model, optimizer, train_dataloader, eval_dataloader, lr_scheduler) = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader, lr_scheduler)\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n    if args.with_tracking:\n        experiment_config = vars(args)\n        experiment_config['lr_scheduler_type'] = experiment_config['lr_scheduler_type'].value\n        accelerator.init_trackers('qa_no_trainer', experiment_config)\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    logger.info('***** Running training *****')\n    logger.info(f'  Num examples = {len(train_dataset)}')\n    logger.info(f'  Num Epochs = {args.num_train_epochs}')\n    logger.info(f'  Instantaneous batch size per device = {args.per_device_train_batch_size}')\n    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')\n    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')\n    logger.info(f'  Total optimization steps = {args.max_train_steps}')\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != '':\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n        accelerator.print(f'Resumed from checkpoint: {checkpoint_path}')\n        accelerator.load_state(checkpoint_path)\n        training_difference = os.path.splitext(path)[0]\n        if 'epoch' in training_difference:\n            starting_epoch = int(training_difference.replace('epoch_', '')) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            resume_step = int(training_difference.replace('step_', '')) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n    progress_bar.update(completed_steps)\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and (resume_step is not None):\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for (step, batch) in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0:\n                    output_dir = f'step_{completed_steps}'\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n            if completed_steps >= args.max_train_steps:\n                break\n        if args.checkpointing_steps == 'epoch':\n            output_dir = f'epoch_{epoch}'\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save)\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(commit_message=f'Training in progress epoch {epoch}', blocking=False, auto_lfs_prune=True)\n    logger.info('***** Running Evaluation *****')\n    logger.info(f'  Num examples = {len(eval_dataset)}')\n    logger.info(f'  Batch size = {args.per_device_eval_batch_size}')\n    all_start_logits = []\n    all_end_logits = []\n    model.eval()\n    for (step, batch) in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(**batch)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n            if not args.pad_to_max_length:\n                start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n            all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n            all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n    max_len = max([x.shape[1] for x in all_start_logits])\n    start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n    end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n    del all_start_logits\n    del all_end_logits\n    outputs_numpy = (start_logits_concat, end_logits_concat)\n    prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n    eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n    logger.info(f'Evaluation metrics: {eval_metric}')\n    if args.do_predict:\n        logger.info('***** Running Prediction *****')\n        logger.info(f'  Num examples = {len(predict_dataset)}')\n        logger.info(f'  Batch size = {args.per_device_eval_batch_size}')\n        all_start_logits = []\n        all_end_logits = []\n        model.eval()\n        for (step, batch) in enumerate(predict_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n                start_logits = outputs.start_logits\n                end_logits = outputs.end_logits\n                if not args.pad_to_max_length:\n                    start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                    end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n                all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n                all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n        max_len = max([x.shape[1] for x in all_start_logits])\n        start_logits_concat = create_and_fill_np_array(all_start_logits, predict_dataset, max_len)\n        end_logits_concat = create_and_fill_np_array(all_end_logits, predict_dataset, max_len)\n        del all_start_logits\n        del all_end_logits\n        outputs_numpy = (start_logits_concat, end_logits_concat)\n        prediction = post_processing_function(predict_examples, predict_dataset, outputs_numpy)\n        predict_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n        logger.info(f'Predict metrics: {predict_metric}')\n    if args.with_tracking:\n        log = {'squad_v2' if args.version_2_with_negative else 'squad': eval_metric, 'train_loss': total_loss.item() / len(train_dataloader), 'epoch': epoch, 'step': completed_steps}\n    if args.do_predict:\n        log['squad_v2_predict' if args.version_2_with_negative else 'squad_predict'] = predict_metric\n        accelerator.log(log, step=completed_steps)\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save)\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message='End of training', auto_lfs_prune=True)\n            logger.info(json.dumps(eval_metric, indent=4))\n            save_prefixed_metrics(eval_metric, args.output_dir)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    args = parse_args()\n    send_example_telemetry('run_qa_no_trainer', args)\n    accelerator_log_kwargs = {}\n    if args.with_tracking:\n        accelerator_log_kwargs['log_with'] = args.report_to\n        accelerator_log_kwargs['project_dir'] = args.output_dir\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n            with open(os.path.join(args.output_dir, '.gitignore'), 'w+') as gitignore:\n                if 'step_*' not in gitignore:\n                    gitignore.write('step_*\\n')\n                if 'epoch_*' not in gitignore:\n                    gitignore.write('epoch_*\\n')\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    if args.dataset_name is not None:\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files['train'] = args.train_file\n        if args.validation_file is not None:\n            data_files['validation'] = args.validation_file\n        if args.test_file is not None:\n            data_files['test'] = args.test_file\n        extension = args.train_file.split('.')[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files, field='data')\n    if args.config_name:\n        config = AutoConfig.from_pretrained(args.config_name, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning('You are instantiating a new config instance from scratch.')\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=True, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True, trust_remote_code=args.trust_remote_code)\n    else:\n        raise ValueError('You are instantiating a new tokenizer from scratch. This is not supported by this script. You can do it from another script, save it, and load it from here, using --tokenizer_name.')\n    if args.model_name_or_path:\n        model = AutoModelForQuestionAnswering.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, trust_remote_code=args.trust_remote_code)\n    else:\n        logger.info('Training new model from scratch')\n        model = AutoModelForQuestionAnswering.from_config(config, trust_remote_code=args.trust_remote_code)\n    column_names = raw_datasets['train'].column_names\n    question_column_name = 'question' if 'question' in column_names else column_names[0]\n    context_column_name = 'context' if 'context' in column_names else column_names[1]\n    answer_column_name = 'answers' if 'answers' in column_names else column_names[2]\n    pad_on_right = tokenizer.padding_side == 'right'\n    if args.max_seq_length > tokenizer.model_max_length:\n        logger.warning(f'The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.')\n    max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n\n    def prepare_train_features(examples):\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n        tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n        sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n        offset_mapping = tokenized_examples.pop('offset_mapping')\n        tokenized_examples['start_positions'] = []\n        tokenized_examples['end_positions'] = []\n        for (i, offsets) in enumerate(offset_mapping):\n            input_ids = tokenized_examples['input_ids'][i]\n            cls_index = input_ids.index(tokenizer.cls_token_id)\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            sample_index = sample_mapping[i]\n            answers = examples[answer_column_name][sample_index]\n            if len(answers['answer_start']) == 0:\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                start_char = answers['answer_start'][0]\n                end_char = start_char + len(answers['text'][0])\n                token_start_index = 0\n                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                    token_start_index += 1\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                    token_end_index -= 1\n                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                    tokenized_examples['start_positions'].append(cls_index)\n                    tokenized_examples['end_positions'].append(cls_index)\n                else:\n                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    tokenized_examples['start_positions'].append(token_start_index - 1)\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    tokenized_examples['end_positions'].append(token_end_index + 1)\n        return tokenized_examples\n    if 'train' not in raw_datasets:\n        raise ValueError('--do_train requires a train dataset')\n    train_dataset = raw_datasets['train']\n    if args.max_train_samples is not None:\n        train_dataset = train_dataset.select(range(args.max_train_samples))\n    with accelerator.main_process_first():\n        train_dataset = train_dataset.map(prepare_train_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on train dataset')\n        if args.max_train_samples is not None:\n            train_dataset = train_dataset.select(range(args.max_train_samples))\n\n    def prepare_validation_features(examples):\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n        tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n        sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n        tokenized_examples['example_id'] = []\n        for i in range(len(tokenized_examples['input_ids'])):\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            context_index = 1 if pad_on_right else 0\n            sample_index = sample_mapping[i]\n            tokenized_examples['example_id'].append(examples['id'][sample_index])\n            tokenized_examples['offset_mapping'][i] = [o if sequence_ids[k] == context_index else None for (k, o) in enumerate(tokenized_examples['offset_mapping'][i])]\n        return tokenized_examples\n    if 'validation' not in raw_datasets:\n        raise ValueError('--do_eval requires a validation dataset')\n    eval_examples = raw_datasets['validation']\n    if args.max_eval_samples is not None:\n        eval_examples = eval_examples.select(range(args.max_eval_samples))\n    with accelerator.main_process_first():\n        eval_dataset = eval_examples.map(prepare_validation_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on validation dataset')\n    if args.max_eval_samples is not None:\n        eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n    if args.do_predict:\n        if 'test' not in raw_datasets:\n            raise ValueError('--do_predict requires a test dataset')\n        predict_examples = raw_datasets['test']\n        if args.max_predict_samples is not None:\n            predict_examples = predict_examples.select(range(args.max_predict_samples))\n        with accelerator.main_process_first():\n            predict_dataset = predict_examples.map(prepare_validation_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on prediction dataset')\n            if args.max_predict_samples is not None:\n                predict_dataset = predict_dataset.select(range(args.max_predict_samples))\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f'Sample {index} of the training set: {train_dataset[index]}.')\n    if args.pad_to_max_length:\n        data_collator = default_data_collator\n    else:\n        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if accelerator.use_fp16 else None)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n    eval_dataset_for_model = eval_dataset.remove_columns(['example_id', 'offset_mapping'])\n    eval_dataloader = DataLoader(eval_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n    if args.do_predict:\n        predict_dataset_for_model = predict_dataset.remove_columns(['example_id', 'offset_mapping'])\n        predict_dataloader = DataLoader(predict_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    def post_processing_function(examples, features, predictions, stage='eval'):\n        predictions = postprocess_qa_predictions(examples=examples, features=features, predictions=predictions, version_2_with_negative=args.version_2_with_negative, n_best_size=args.n_best_size, max_answer_length=args.max_answer_length, null_score_diff_threshold=args.null_score_diff_threshold, output_dir=args.output_dir, prefix=stage)\n        if args.version_2_with_negative:\n            formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.0} for (k, v) in predictions.items()]\n        else:\n            formatted_predictions = [{'id': k, 'prediction_text': v} for (k, v) in predictions.items()]\n        references = [{'id': ex['id'], 'answers': ex[answer_column_name]} for ex in examples]\n        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n    metric = evaluate.load('squad_v2' if args.version_2_with_negative else 'squad')\n\n    def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n        \"\"\"\n        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n\n        Args:\n            start_or_end_logits(:obj:`tensor`):\n                This is the output predictions of the model. We can only enter either start or end logits.\n            eval_dataset: Evaluation dataset\n            max_len(:obj:`int`):\n                The maximum length of the output tensor. ( See the model.eval() part for more details )\n        \"\"\"\n        step = 0\n        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n        for (i, output_logit) in enumerate(start_or_end_logits):\n            batch_size = output_logit.shape[0]\n            cols = output_logit.shape[1]\n            if step + batch_size < len(dataset):\n                logits_concat[step:step + batch_size, :cols] = output_logit\n            else:\n                logits_concat[step:, :cols] = output_logit[:len(dataset) - step]\n            step += batch_size\n        return logits_concat\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if not any((nd in n for nd in no_decay))], 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if any((nd in n for nd in no_decay))], 'weight_decay': 0.0}]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n    lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer, num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps, num_training_steps=args.max_train_steps * args.gradient_accumulation_steps)\n    (model, optimizer, train_dataloader, eval_dataloader, lr_scheduler) = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader, lr_scheduler)\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n    if args.with_tracking:\n        experiment_config = vars(args)\n        experiment_config['lr_scheduler_type'] = experiment_config['lr_scheduler_type'].value\n        accelerator.init_trackers('qa_no_trainer', experiment_config)\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    logger.info('***** Running training *****')\n    logger.info(f'  Num examples = {len(train_dataset)}')\n    logger.info(f'  Num Epochs = {args.num_train_epochs}')\n    logger.info(f'  Instantaneous batch size per device = {args.per_device_train_batch_size}')\n    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')\n    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')\n    logger.info(f'  Total optimization steps = {args.max_train_steps}')\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != '':\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n        accelerator.print(f'Resumed from checkpoint: {checkpoint_path}')\n        accelerator.load_state(checkpoint_path)\n        training_difference = os.path.splitext(path)[0]\n        if 'epoch' in training_difference:\n            starting_epoch = int(training_difference.replace('epoch_', '')) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            resume_step = int(training_difference.replace('step_', '')) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n    progress_bar.update(completed_steps)\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and (resume_step is not None):\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for (step, batch) in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0:\n                    output_dir = f'step_{completed_steps}'\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n            if completed_steps >= args.max_train_steps:\n                break\n        if args.checkpointing_steps == 'epoch':\n            output_dir = f'epoch_{epoch}'\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save)\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(commit_message=f'Training in progress epoch {epoch}', blocking=False, auto_lfs_prune=True)\n    logger.info('***** Running Evaluation *****')\n    logger.info(f'  Num examples = {len(eval_dataset)}')\n    logger.info(f'  Batch size = {args.per_device_eval_batch_size}')\n    all_start_logits = []\n    all_end_logits = []\n    model.eval()\n    for (step, batch) in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(**batch)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n            if not args.pad_to_max_length:\n                start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n            all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n            all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n    max_len = max([x.shape[1] for x in all_start_logits])\n    start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n    end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n    del all_start_logits\n    del all_end_logits\n    outputs_numpy = (start_logits_concat, end_logits_concat)\n    prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n    eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n    logger.info(f'Evaluation metrics: {eval_metric}')\n    if args.do_predict:\n        logger.info('***** Running Prediction *****')\n        logger.info(f'  Num examples = {len(predict_dataset)}')\n        logger.info(f'  Batch size = {args.per_device_eval_batch_size}')\n        all_start_logits = []\n        all_end_logits = []\n        model.eval()\n        for (step, batch) in enumerate(predict_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n                start_logits = outputs.start_logits\n                end_logits = outputs.end_logits\n                if not args.pad_to_max_length:\n                    start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                    end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n                all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n                all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n        max_len = max([x.shape[1] for x in all_start_logits])\n        start_logits_concat = create_and_fill_np_array(all_start_logits, predict_dataset, max_len)\n        end_logits_concat = create_and_fill_np_array(all_end_logits, predict_dataset, max_len)\n        del all_start_logits\n        del all_end_logits\n        outputs_numpy = (start_logits_concat, end_logits_concat)\n        prediction = post_processing_function(predict_examples, predict_dataset, outputs_numpy)\n        predict_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n        logger.info(f'Predict metrics: {predict_metric}')\n    if args.with_tracking:\n        log = {'squad_v2' if args.version_2_with_negative else 'squad': eval_metric, 'train_loss': total_loss.item() / len(train_dataloader), 'epoch': epoch, 'step': completed_steps}\n    if args.do_predict:\n        log['squad_v2_predict' if args.version_2_with_negative else 'squad_predict'] = predict_metric\n        accelerator.log(log, step=completed_steps)\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save)\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message='End of training', auto_lfs_prune=True)\n            logger.info(json.dumps(eval_metric, indent=4))\n            save_prefixed_metrics(eval_metric, args.output_dir)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    args = parse_args()\n    send_example_telemetry('run_qa_no_trainer', args)\n    accelerator_log_kwargs = {}\n    if args.with_tracking:\n        accelerator_log_kwargs['log_with'] = args.report_to\n        accelerator_log_kwargs['project_dir'] = args.output_dir\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n            with open(os.path.join(args.output_dir, '.gitignore'), 'w+') as gitignore:\n                if 'step_*' not in gitignore:\n                    gitignore.write('step_*\\n')\n                if 'epoch_*' not in gitignore:\n                    gitignore.write('epoch_*\\n')\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    if args.dataset_name is not None:\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files['train'] = args.train_file\n        if args.validation_file is not None:\n            data_files['validation'] = args.validation_file\n        if args.test_file is not None:\n            data_files['test'] = args.test_file\n        extension = args.train_file.split('.')[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files, field='data')\n    if args.config_name:\n        config = AutoConfig.from_pretrained(args.config_name, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning('You are instantiating a new config instance from scratch.')\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=True, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True, trust_remote_code=args.trust_remote_code)\n    else:\n        raise ValueError('You are instantiating a new tokenizer from scratch. This is not supported by this script. You can do it from another script, save it, and load it from here, using --tokenizer_name.')\n    if args.model_name_or_path:\n        model = AutoModelForQuestionAnswering.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, trust_remote_code=args.trust_remote_code)\n    else:\n        logger.info('Training new model from scratch')\n        model = AutoModelForQuestionAnswering.from_config(config, trust_remote_code=args.trust_remote_code)\n    column_names = raw_datasets['train'].column_names\n    question_column_name = 'question' if 'question' in column_names else column_names[0]\n    context_column_name = 'context' if 'context' in column_names else column_names[1]\n    answer_column_name = 'answers' if 'answers' in column_names else column_names[2]\n    pad_on_right = tokenizer.padding_side == 'right'\n    if args.max_seq_length > tokenizer.model_max_length:\n        logger.warning(f'The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.')\n    max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n\n    def prepare_train_features(examples):\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n        tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n        sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n        offset_mapping = tokenized_examples.pop('offset_mapping')\n        tokenized_examples['start_positions'] = []\n        tokenized_examples['end_positions'] = []\n        for (i, offsets) in enumerate(offset_mapping):\n            input_ids = tokenized_examples['input_ids'][i]\n            cls_index = input_ids.index(tokenizer.cls_token_id)\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            sample_index = sample_mapping[i]\n            answers = examples[answer_column_name][sample_index]\n            if len(answers['answer_start']) == 0:\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                start_char = answers['answer_start'][0]\n                end_char = start_char + len(answers['text'][0])\n                token_start_index = 0\n                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                    token_start_index += 1\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                    token_end_index -= 1\n                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                    tokenized_examples['start_positions'].append(cls_index)\n                    tokenized_examples['end_positions'].append(cls_index)\n                else:\n                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    tokenized_examples['start_positions'].append(token_start_index - 1)\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    tokenized_examples['end_positions'].append(token_end_index + 1)\n        return tokenized_examples\n    if 'train' not in raw_datasets:\n        raise ValueError('--do_train requires a train dataset')\n    train_dataset = raw_datasets['train']\n    if args.max_train_samples is not None:\n        train_dataset = train_dataset.select(range(args.max_train_samples))\n    with accelerator.main_process_first():\n        train_dataset = train_dataset.map(prepare_train_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on train dataset')\n        if args.max_train_samples is not None:\n            train_dataset = train_dataset.select(range(args.max_train_samples))\n\n    def prepare_validation_features(examples):\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n        tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n        sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n        tokenized_examples['example_id'] = []\n        for i in range(len(tokenized_examples['input_ids'])):\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            context_index = 1 if pad_on_right else 0\n            sample_index = sample_mapping[i]\n            tokenized_examples['example_id'].append(examples['id'][sample_index])\n            tokenized_examples['offset_mapping'][i] = [o if sequence_ids[k] == context_index else None for (k, o) in enumerate(tokenized_examples['offset_mapping'][i])]\n        return tokenized_examples\n    if 'validation' not in raw_datasets:\n        raise ValueError('--do_eval requires a validation dataset')\n    eval_examples = raw_datasets['validation']\n    if args.max_eval_samples is not None:\n        eval_examples = eval_examples.select(range(args.max_eval_samples))\n    with accelerator.main_process_first():\n        eval_dataset = eval_examples.map(prepare_validation_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on validation dataset')\n    if args.max_eval_samples is not None:\n        eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n    if args.do_predict:\n        if 'test' not in raw_datasets:\n            raise ValueError('--do_predict requires a test dataset')\n        predict_examples = raw_datasets['test']\n        if args.max_predict_samples is not None:\n            predict_examples = predict_examples.select(range(args.max_predict_samples))\n        with accelerator.main_process_first():\n            predict_dataset = predict_examples.map(prepare_validation_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on prediction dataset')\n            if args.max_predict_samples is not None:\n                predict_dataset = predict_dataset.select(range(args.max_predict_samples))\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f'Sample {index} of the training set: {train_dataset[index]}.')\n    if args.pad_to_max_length:\n        data_collator = default_data_collator\n    else:\n        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if accelerator.use_fp16 else None)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n    eval_dataset_for_model = eval_dataset.remove_columns(['example_id', 'offset_mapping'])\n    eval_dataloader = DataLoader(eval_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n    if args.do_predict:\n        predict_dataset_for_model = predict_dataset.remove_columns(['example_id', 'offset_mapping'])\n        predict_dataloader = DataLoader(predict_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    def post_processing_function(examples, features, predictions, stage='eval'):\n        predictions = postprocess_qa_predictions(examples=examples, features=features, predictions=predictions, version_2_with_negative=args.version_2_with_negative, n_best_size=args.n_best_size, max_answer_length=args.max_answer_length, null_score_diff_threshold=args.null_score_diff_threshold, output_dir=args.output_dir, prefix=stage)\n        if args.version_2_with_negative:\n            formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.0} for (k, v) in predictions.items()]\n        else:\n            formatted_predictions = [{'id': k, 'prediction_text': v} for (k, v) in predictions.items()]\n        references = [{'id': ex['id'], 'answers': ex[answer_column_name]} for ex in examples]\n        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n    metric = evaluate.load('squad_v2' if args.version_2_with_negative else 'squad')\n\n    def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n        \"\"\"\n        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n\n        Args:\n            start_or_end_logits(:obj:`tensor`):\n                This is the output predictions of the model. We can only enter either start or end logits.\n            eval_dataset: Evaluation dataset\n            max_len(:obj:`int`):\n                The maximum length of the output tensor. ( See the model.eval() part for more details )\n        \"\"\"\n        step = 0\n        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n        for (i, output_logit) in enumerate(start_or_end_logits):\n            batch_size = output_logit.shape[0]\n            cols = output_logit.shape[1]\n            if step + batch_size < len(dataset):\n                logits_concat[step:step + batch_size, :cols] = output_logit\n            else:\n                logits_concat[step:, :cols] = output_logit[:len(dataset) - step]\n            step += batch_size\n        return logits_concat\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if not any((nd in n for nd in no_decay))], 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if any((nd in n for nd in no_decay))], 'weight_decay': 0.0}]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n    lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer, num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps, num_training_steps=args.max_train_steps * args.gradient_accumulation_steps)\n    (model, optimizer, train_dataloader, eval_dataloader, lr_scheduler) = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader, lr_scheduler)\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n    if args.with_tracking:\n        experiment_config = vars(args)\n        experiment_config['lr_scheduler_type'] = experiment_config['lr_scheduler_type'].value\n        accelerator.init_trackers('qa_no_trainer', experiment_config)\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    logger.info('***** Running training *****')\n    logger.info(f'  Num examples = {len(train_dataset)}')\n    logger.info(f'  Num Epochs = {args.num_train_epochs}')\n    logger.info(f'  Instantaneous batch size per device = {args.per_device_train_batch_size}')\n    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')\n    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')\n    logger.info(f'  Total optimization steps = {args.max_train_steps}')\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != '':\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n        accelerator.print(f'Resumed from checkpoint: {checkpoint_path}')\n        accelerator.load_state(checkpoint_path)\n        training_difference = os.path.splitext(path)[0]\n        if 'epoch' in training_difference:\n            starting_epoch = int(training_difference.replace('epoch_', '')) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            resume_step = int(training_difference.replace('step_', '')) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n    progress_bar.update(completed_steps)\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and (resume_step is not None):\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for (step, batch) in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0:\n                    output_dir = f'step_{completed_steps}'\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n            if completed_steps >= args.max_train_steps:\n                break\n        if args.checkpointing_steps == 'epoch':\n            output_dir = f'epoch_{epoch}'\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save)\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(commit_message=f'Training in progress epoch {epoch}', blocking=False, auto_lfs_prune=True)\n    logger.info('***** Running Evaluation *****')\n    logger.info(f'  Num examples = {len(eval_dataset)}')\n    logger.info(f'  Batch size = {args.per_device_eval_batch_size}')\n    all_start_logits = []\n    all_end_logits = []\n    model.eval()\n    for (step, batch) in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(**batch)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n            if not args.pad_to_max_length:\n                start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n            all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n            all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n    max_len = max([x.shape[1] for x in all_start_logits])\n    start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n    end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n    del all_start_logits\n    del all_end_logits\n    outputs_numpy = (start_logits_concat, end_logits_concat)\n    prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n    eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n    logger.info(f'Evaluation metrics: {eval_metric}')\n    if args.do_predict:\n        logger.info('***** Running Prediction *****')\n        logger.info(f'  Num examples = {len(predict_dataset)}')\n        logger.info(f'  Batch size = {args.per_device_eval_batch_size}')\n        all_start_logits = []\n        all_end_logits = []\n        model.eval()\n        for (step, batch) in enumerate(predict_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n                start_logits = outputs.start_logits\n                end_logits = outputs.end_logits\n                if not args.pad_to_max_length:\n                    start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                    end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n                all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n                all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n        max_len = max([x.shape[1] for x in all_start_logits])\n        start_logits_concat = create_and_fill_np_array(all_start_logits, predict_dataset, max_len)\n        end_logits_concat = create_and_fill_np_array(all_end_logits, predict_dataset, max_len)\n        del all_start_logits\n        del all_end_logits\n        outputs_numpy = (start_logits_concat, end_logits_concat)\n        prediction = post_processing_function(predict_examples, predict_dataset, outputs_numpy)\n        predict_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n        logger.info(f'Predict metrics: {predict_metric}')\n    if args.with_tracking:\n        log = {'squad_v2' if args.version_2_with_negative else 'squad': eval_metric, 'train_loss': total_loss.item() / len(train_dataloader), 'epoch': epoch, 'step': completed_steps}\n    if args.do_predict:\n        log['squad_v2_predict' if args.version_2_with_negative else 'squad_predict'] = predict_metric\n        accelerator.log(log, step=completed_steps)\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save)\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message='End of training', auto_lfs_prune=True)\n            logger.info(json.dumps(eval_metric, indent=4))\n            save_prefixed_metrics(eval_metric, args.output_dir)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    args = parse_args()\n    send_example_telemetry('run_qa_no_trainer', args)\n    accelerator_log_kwargs = {}\n    if args.with_tracking:\n        accelerator_log_kwargs['log_with'] = args.report_to\n        accelerator_log_kwargs['project_dir'] = args.output_dir\n    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    if args.seed is not None:\n        set_seed(args.seed)\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            repo_id = create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n            with open(os.path.join(args.output_dir, '.gitignore'), 'w+') as gitignore:\n                if 'step_*' not in gitignore:\n                    gitignore.write('step_*\\n')\n                if 'epoch_*' not in gitignore:\n                    gitignore.write('epoch_*\\n')\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n    if args.dataset_name is not None:\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n    else:\n        data_files = {}\n        if args.train_file is not None:\n            data_files['train'] = args.train_file\n        if args.validation_file is not None:\n            data_files['validation'] = args.validation_file\n        if args.test_file is not None:\n            data_files['test'] = args.test_file\n        extension = args.train_file.split('.')[-1]\n        raw_datasets = load_dataset(extension, data_files=data_files, field='data')\n    if args.config_name:\n        config = AutoConfig.from_pretrained(args.config_name, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=args.trust_remote_code)\n    else:\n        config = CONFIG_MAPPING[args.model_type]()\n        logger.warning('You are instantiating a new config instance from scratch.')\n    if args.tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=True, trust_remote_code=args.trust_remote_code)\n    elif args.model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True, trust_remote_code=args.trust_remote_code)\n    else:\n        raise ValueError('You are instantiating a new tokenizer from scratch. This is not supported by this script. You can do it from another script, save it, and load it from here, using --tokenizer_name.')\n    if args.model_name_or_path:\n        model = AutoModelForQuestionAnswering.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config, trust_remote_code=args.trust_remote_code)\n    else:\n        logger.info('Training new model from scratch')\n        model = AutoModelForQuestionAnswering.from_config(config, trust_remote_code=args.trust_remote_code)\n    column_names = raw_datasets['train'].column_names\n    question_column_name = 'question' if 'question' in column_names else column_names[0]\n    context_column_name = 'context' if 'context' in column_names else column_names[1]\n    answer_column_name = 'answers' if 'answers' in column_names else column_names[2]\n    pad_on_right = tokenizer.padding_side == 'right'\n    if args.max_seq_length > tokenizer.model_max_length:\n        logger.warning(f'The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.')\n    max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n\n    def prepare_train_features(examples):\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n        tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n        sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n        offset_mapping = tokenized_examples.pop('offset_mapping')\n        tokenized_examples['start_positions'] = []\n        tokenized_examples['end_positions'] = []\n        for (i, offsets) in enumerate(offset_mapping):\n            input_ids = tokenized_examples['input_ids'][i]\n            cls_index = input_ids.index(tokenizer.cls_token_id)\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            sample_index = sample_mapping[i]\n            answers = examples[answer_column_name][sample_index]\n            if len(answers['answer_start']) == 0:\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                start_char = answers['answer_start'][0]\n                end_char = start_char + len(answers['text'][0])\n                token_start_index = 0\n                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                    token_start_index += 1\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                    token_end_index -= 1\n                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                    tokenized_examples['start_positions'].append(cls_index)\n                    tokenized_examples['end_positions'].append(cls_index)\n                else:\n                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    tokenized_examples['start_positions'].append(token_start_index - 1)\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    tokenized_examples['end_positions'].append(token_end_index + 1)\n        return tokenized_examples\n    if 'train' not in raw_datasets:\n        raise ValueError('--do_train requires a train dataset')\n    train_dataset = raw_datasets['train']\n    if args.max_train_samples is not None:\n        train_dataset = train_dataset.select(range(args.max_train_samples))\n    with accelerator.main_process_first():\n        train_dataset = train_dataset.map(prepare_train_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on train dataset')\n        if args.max_train_samples is not None:\n            train_dataset = train_dataset.select(range(args.max_train_samples))\n\n    def prepare_validation_features(examples):\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n        tokenized_examples = tokenizer(examples[question_column_name if pad_on_right else context_column_name], examples[context_column_name if pad_on_right else question_column_name], truncation='only_second' if pad_on_right else 'only_first', max_length=max_seq_length, stride=args.doc_stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding='max_length' if args.pad_to_max_length else False)\n        sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n        tokenized_examples['example_id'] = []\n        for i in range(len(tokenized_examples['input_ids'])):\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            context_index = 1 if pad_on_right else 0\n            sample_index = sample_mapping[i]\n            tokenized_examples['example_id'].append(examples['id'][sample_index])\n            tokenized_examples['offset_mapping'][i] = [o if sequence_ids[k] == context_index else None for (k, o) in enumerate(tokenized_examples['offset_mapping'][i])]\n        return tokenized_examples\n    if 'validation' not in raw_datasets:\n        raise ValueError('--do_eval requires a validation dataset')\n    eval_examples = raw_datasets['validation']\n    if args.max_eval_samples is not None:\n        eval_examples = eval_examples.select(range(args.max_eval_samples))\n    with accelerator.main_process_first():\n        eval_dataset = eval_examples.map(prepare_validation_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on validation dataset')\n    if args.max_eval_samples is not None:\n        eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n    if args.do_predict:\n        if 'test' not in raw_datasets:\n            raise ValueError('--do_predict requires a test dataset')\n        predict_examples = raw_datasets['test']\n        if args.max_predict_samples is not None:\n            predict_examples = predict_examples.select(range(args.max_predict_samples))\n        with accelerator.main_process_first():\n            predict_dataset = predict_examples.map(prepare_validation_features, batched=True, num_proc=args.preprocessing_num_workers, remove_columns=column_names, load_from_cache_file=not args.overwrite_cache, desc='Running tokenizer on prediction dataset')\n            if args.max_predict_samples is not None:\n                predict_dataset = predict_dataset.select(range(args.max_predict_samples))\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f'Sample {index} of the training set: {train_dataset[index]}.')\n    if args.pad_to_max_length:\n        data_collator = default_data_collator\n    else:\n        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if accelerator.use_fp16 else None)\n    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n    eval_dataset_for_model = eval_dataset.remove_columns(['example_id', 'offset_mapping'])\n    eval_dataloader = DataLoader(eval_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n    if args.do_predict:\n        predict_dataset_for_model = predict_dataset.remove_columns(['example_id', 'offset_mapping'])\n        predict_dataloader = DataLoader(predict_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n    def post_processing_function(examples, features, predictions, stage='eval'):\n        predictions = postprocess_qa_predictions(examples=examples, features=features, predictions=predictions, version_2_with_negative=args.version_2_with_negative, n_best_size=args.n_best_size, max_answer_length=args.max_answer_length, null_score_diff_threshold=args.null_score_diff_threshold, output_dir=args.output_dir, prefix=stage)\n        if args.version_2_with_negative:\n            formatted_predictions = [{'id': k, 'prediction_text': v, 'no_answer_probability': 0.0} for (k, v) in predictions.items()]\n        else:\n            formatted_predictions = [{'id': k, 'prediction_text': v} for (k, v) in predictions.items()]\n        references = [{'id': ex['id'], 'answers': ex[answer_column_name]} for ex in examples]\n        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n    metric = evaluate.load('squad_v2' if args.version_2_with_negative else 'squad')\n\n    def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n        \"\"\"\n        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n\n        Args:\n            start_or_end_logits(:obj:`tensor`):\n                This is the output predictions of the model. We can only enter either start or end logits.\n            eval_dataset: Evaluation dataset\n            max_len(:obj:`int`):\n                The maximum length of the output tensor. ( See the model.eval() part for more details )\n        \"\"\"\n        step = 0\n        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n        for (i, output_logit) in enumerate(start_or_end_logits):\n            batch_size = output_logit.shape[0]\n            cols = output_logit.shape[1]\n            if step + batch_size < len(dataset):\n                logits_concat[step:step + batch_size, :cols] = output_logit\n            else:\n                logits_concat[step:, :cols] = output_logit[:len(dataset) - step]\n            step += batch_size\n        return logits_concat\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [{'params': [p for (n, p) in model.named_parameters() if not any((nd in n for nd in no_decay))], 'weight_decay': args.weight_decay}, {'params': [p for (n, p) in model.named_parameters() if any((nd in n for nd in no_decay))], 'weight_decay': 0.0}]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n    lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer, num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps, num_training_steps=args.max_train_steps * args.gradient_accumulation_steps)\n    (model, optimizer, train_dataloader, eval_dataloader, lr_scheduler) = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader, lr_scheduler)\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n    if args.with_tracking:\n        experiment_config = vars(args)\n        experiment_config['lr_scheduler_type'] = experiment_config['lr_scheduler_type'].value\n        accelerator.init_trackers('qa_no_trainer', experiment_config)\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n    logger.info('***** Running training *****')\n    logger.info(f'  Num examples = {len(train_dataset)}')\n    logger.info(f'  Num Epochs = {args.num_train_epochs}')\n    logger.info(f'  Instantaneous batch size per device = {args.per_device_train_batch_size}')\n    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')\n    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')\n    logger.info(f'  Total optimization steps = {args.max_train_steps}')\n    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n    completed_steps = 0\n    starting_epoch = 0\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != '':\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[-1]\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n        accelerator.print(f'Resumed from checkpoint: {checkpoint_path}')\n        accelerator.load_state(checkpoint_path)\n        training_difference = os.path.splitext(path)[0]\n        if 'epoch' in training_difference:\n            starting_epoch = int(training_difference.replace('epoch_', '')) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            resume_step = int(training_difference.replace('step_', '')) * args.gradient_accumulation_steps\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n    progress_bar.update(completed_steps)\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        model.train()\n        if args.with_tracking:\n            total_loss = 0\n        if args.resume_from_checkpoint and epoch == starting_epoch and (resume_step is not None):\n            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n        else:\n            active_dataloader = train_dataloader\n        for (step, batch) in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs = model(**batch)\n                loss = outputs.loss\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0:\n                    output_dir = f'step_{completed_steps}'\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n            if completed_steps >= args.max_train_steps:\n                break\n        if args.checkpointing_steps == 'epoch':\n            output_dir = f'epoch_{epoch}'\n            if args.output_dir is not None:\n                output_dir = os.path.join(args.output_dir, output_dir)\n            accelerator.save_state(output_dir)\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save)\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(commit_message=f'Training in progress epoch {epoch}', blocking=False, auto_lfs_prune=True)\n    logger.info('***** Running Evaluation *****')\n    logger.info(f'  Num examples = {len(eval_dataset)}')\n    logger.info(f'  Batch size = {args.per_device_eval_batch_size}')\n    all_start_logits = []\n    all_end_logits = []\n    model.eval()\n    for (step, batch) in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(**batch)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n            if not args.pad_to_max_length:\n                start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n            all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n            all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n    max_len = max([x.shape[1] for x in all_start_logits])\n    start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n    end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n    del all_start_logits\n    del all_end_logits\n    outputs_numpy = (start_logits_concat, end_logits_concat)\n    prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n    eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n    logger.info(f'Evaluation metrics: {eval_metric}')\n    if args.do_predict:\n        logger.info('***** Running Prediction *****')\n        logger.info(f'  Num examples = {len(predict_dataset)}')\n        logger.info(f'  Batch size = {args.per_device_eval_batch_size}')\n        all_start_logits = []\n        all_end_logits = []\n        model.eval()\n        for (step, batch) in enumerate(predict_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n                start_logits = outputs.start_logits\n                end_logits = outputs.end_logits\n                if not args.pad_to_max_length:\n                    start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                    end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n                all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n                all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n        max_len = max([x.shape[1] for x in all_start_logits])\n        start_logits_concat = create_and_fill_np_array(all_start_logits, predict_dataset, max_len)\n        end_logits_concat = create_and_fill_np_array(all_end_logits, predict_dataset, max_len)\n        del all_start_logits\n        del all_end_logits\n        outputs_numpy = (start_logits_concat, end_logits_concat)\n        prediction = post_processing_function(predict_examples, predict_dataset, outputs_numpy)\n        predict_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n        logger.info(f'Predict metrics: {predict_metric}')\n    if args.with_tracking:\n        log = {'squad_v2' if args.version_2_with_negative else 'squad': eval_metric, 'train_loss': total_loss.item() / len(train_dataloader), 'epoch': epoch, 'step': completed_steps}\n    if args.do_predict:\n        log['squad_v2_predict' if args.version_2_with_negative else 'squad_predict'] = predict_metric\n        accelerator.log(log, step=completed_steps)\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save)\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message='End of training', auto_lfs_prune=True)\n            logger.info(json.dumps(eval_metric, indent=4))\n            save_prefixed_metrics(eval_metric, args.output_dir)"
        ]
    }
]