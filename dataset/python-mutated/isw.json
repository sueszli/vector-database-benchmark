[
    {
        "func_name": "compute_importance_weights",
        "original": "def compute_importance_weights(target_output: Union[torch.Tensor, dict], behaviour_output: Union[torch.Tensor, dict], action: torch.Tensor, action_space_type: str='discrete', requires_grad: bool=False):\n    \"\"\"\n    Overview:\n        Computing importance sampling weight with given output and action\n    Arguments:\n        - target_output (:obj:`Union[torch.Tensor,dict]`): the output taking the action             by the current policy network,             usually this output is network output logit if action space is discrete,             or is a dict containing parameters of action distribution if action space is continuous.\n        - behaviour_output (:obj:`Union[torch.Tensor,dict]`): the output taking the action             by the behaviour policy network,            usually this output is network output logit,  if action space is discrete,             or is a dict containing parameters of action distribution if action space is continuous.\n        - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,            i.e.: behaviour_action\n        - action_space_type (:obj:`str`): action space types in ['discrete', 'continuous']\n        - requires_grad (:obj:`bool`): whether requires grad computation\n    Returns:\n        - rhos (:obj:`torch.Tensor`): Importance sampling weight\n    Shapes:\n        - target_output (:obj:`Union[torch.FloatTensor,dict]`): :math:`(T, B, N)`,             where T is timestep, B is batch size and N is action dim\n        - behaviour_output (:obj:`Union[torch.FloatTensor,dict]`): :math:`(T, B, N)`\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\n        - rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`\n    Examples:\n        >>> target_output = torch.randn(2, 3, 4)\n        >>> behaviour_output = torch.randn(2, 3, 4)\n        >>> action = torch.randint(0, 4, (2, 3))\n        >>> rhos = compute_importance_weights(target_output, behaviour_output, action)\n    \"\"\"\n    grad_context = torch.enable_grad() if requires_grad else torch.no_grad()\n    assert isinstance(action, torch.Tensor)\n    assert action_space_type in ['discrete', 'continuous']\n    with grad_context:\n        if action_space_type == 'continuous':\n            dist_target = Independent(Normal(loc=target_output['mu'], scale=target_output['sigma']), 1)\n            dist_behaviour = Independent(Normal(loc=behaviour_output['mu'], scale=behaviour_output['sigma']), 1)\n            rhos = dist_target.log_prob(action) - dist_behaviour.log_prob(action)\n            rhos = torch.exp(rhos)\n            return rhos\n        elif action_space_type == 'discrete':\n            dist_target = Categorical(logits=target_output)\n            dist_behaviour = Categorical(logits=behaviour_output)\n            rhos = dist_target.log_prob(action) - dist_behaviour.log_prob(action)\n            rhos = torch.exp(rhos)\n            return rhos",
        "mutated": [
            "def compute_importance_weights(target_output: Union[torch.Tensor, dict], behaviour_output: Union[torch.Tensor, dict], action: torch.Tensor, action_space_type: str='discrete', requires_grad: bool=False):\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Computing importance sampling weight with given output and action\\n    Arguments:\\n        - target_output (:obj:`Union[torch.Tensor,dict]`): the output taking the action             by the current policy network,             usually this output is network output logit if action space is discrete,             or is a dict containing parameters of action distribution if action space is continuous.\\n        - behaviour_output (:obj:`Union[torch.Tensor,dict]`): the output taking the action             by the behaviour policy network,            usually this output is network output logit,  if action space is discrete,             or is a dict containing parameters of action distribution if action space is continuous.\\n        - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,            i.e.: behaviour_action\\n        - action_space_type (:obj:`str`): action space types in ['discrete', 'continuous']\\n        - requires_grad (:obj:`bool`): whether requires grad computation\\n    Returns:\\n        - rhos (:obj:`torch.Tensor`): Importance sampling weight\\n    Shapes:\\n        - target_output (:obj:`Union[torch.FloatTensor,dict]`): :math:`(T, B, N)`,             where T is timestep, B is batch size and N is action dim\\n        - behaviour_output (:obj:`Union[torch.FloatTensor,dict]`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> target_output = torch.randn(2, 3, 4)\\n        >>> behaviour_output = torch.randn(2, 3, 4)\\n        >>> action = torch.randint(0, 4, (2, 3))\\n        >>> rhos = compute_importance_weights(target_output, behaviour_output, action)\\n    \"\n    grad_context = torch.enable_grad() if requires_grad else torch.no_grad()\n    assert isinstance(action, torch.Tensor)\n    assert action_space_type in ['discrete', 'continuous']\n    with grad_context:\n        if action_space_type == 'continuous':\n            dist_target = Independent(Normal(loc=target_output['mu'], scale=target_output['sigma']), 1)\n            dist_behaviour = Independent(Normal(loc=behaviour_output['mu'], scale=behaviour_output['sigma']), 1)\n            rhos = dist_target.log_prob(action) - dist_behaviour.log_prob(action)\n            rhos = torch.exp(rhos)\n            return rhos\n        elif action_space_type == 'discrete':\n            dist_target = Categorical(logits=target_output)\n            dist_behaviour = Categorical(logits=behaviour_output)\n            rhos = dist_target.log_prob(action) - dist_behaviour.log_prob(action)\n            rhos = torch.exp(rhos)\n            return rhos",
            "def compute_importance_weights(target_output: Union[torch.Tensor, dict], behaviour_output: Union[torch.Tensor, dict], action: torch.Tensor, action_space_type: str='discrete', requires_grad: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Computing importance sampling weight with given output and action\\n    Arguments:\\n        - target_output (:obj:`Union[torch.Tensor,dict]`): the output taking the action             by the current policy network,             usually this output is network output logit if action space is discrete,             or is a dict containing parameters of action distribution if action space is continuous.\\n        - behaviour_output (:obj:`Union[torch.Tensor,dict]`): the output taking the action             by the behaviour policy network,            usually this output is network output logit,  if action space is discrete,             or is a dict containing parameters of action distribution if action space is continuous.\\n        - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,            i.e.: behaviour_action\\n        - action_space_type (:obj:`str`): action space types in ['discrete', 'continuous']\\n        - requires_grad (:obj:`bool`): whether requires grad computation\\n    Returns:\\n        - rhos (:obj:`torch.Tensor`): Importance sampling weight\\n    Shapes:\\n        - target_output (:obj:`Union[torch.FloatTensor,dict]`): :math:`(T, B, N)`,             where T is timestep, B is batch size and N is action dim\\n        - behaviour_output (:obj:`Union[torch.FloatTensor,dict]`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> target_output = torch.randn(2, 3, 4)\\n        >>> behaviour_output = torch.randn(2, 3, 4)\\n        >>> action = torch.randint(0, 4, (2, 3))\\n        >>> rhos = compute_importance_weights(target_output, behaviour_output, action)\\n    \"\n    grad_context = torch.enable_grad() if requires_grad else torch.no_grad()\n    assert isinstance(action, torch.Tensor)\n    assert action_space_type in ['discrete', 'continuous']\n    with grad_context:\n        if action_space_type == 'continuous':\n            dist_target = Independent(Normal(loc=target_output['mu'], scale=target_output['sigma']), 1)\n            dist_behaviour = Independent(Normal(loc=behaviour_output['mu'], scale=behaviour_output['sigma']), 1)\n            rhos = dist_target.log_prob(action) - dist_behaviour.log_prob(action)\n            rhos = torch.exp(rhos)\n            return rhos\n        elif action_space_type == 'discrete':\n            dist_target = Categorical(logits=target_output)\n            dist_behaviour = Categorical(logits=behaviour_output)\n            rhos = dist_target.log_prob(action) - dist_behaviour.log_prob(action)\n            rhos = torch.exp(rhos)\n            return rhos",
            "def compute_importance_weights(target_output: Union[torch.Tensor, dict], behaviour_output: Union[torch.Tensor, dict], action: torch.Tensor, action_space_type: str='discrete', requires_grad: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Computing importance sampling weight with given output and action\\n    Arguments:\\n        - target_output (:obj:`Union[torch.Tensor,dict]`): the output taking the action             by the current policy network,             usually this output is network output logit if action space is discrete,             or is a dict containing parameters of action distribution if action space is continuous.\\n        - behaviour_output (:obj:`Union[torch.Tensor,dict]`): the output taking the action             by the behaviour policy network,            usually this output is network output logit,  if action space is discrete,             or is a dict containing parameters of action distribution if action space is continuous.\\n        - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,            i.e.: behaviour_action\\n        - action_space_type (:obj:`str`): action space types in ['discrete', 'continuous']\\n        - requires_grad (:obj:`bool`): whether requires grad computation\\n    Returns:\\n        - rhos (:obj:`torch.Tensor`): Importance sampling weight\\n    Shapes:\\n        - target_output (:obj:`Union[torch.FloatTensor,dict]`): :math:`(T, B, N)`,             where T is timestep, B is batch size and N is action dim\\n        - behaviour_output (:obj:`Union[torch.FloatTensor,dict]`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> target_output = torch.randn(2, 3, 4)\\n        >>> behaviour_output = torch.randn(2, 3, 4)\\n        >>> action = torch.randint(0, 4, (2, 3))\\n        >>> rhos = compute_importance_weights(target_output, behaviour_output, action)\\n    \"\n    grad_context = torch.enable_grad() if requires_grad else torch.no_grad()\n    assert isinstance(action, torch.Tensor)\n    assert action_space_type in ['discrete', 'continuous']\n    with grad_context:\n        if action_space_type == 'continuous':\n            dist_target = Independent(Normal(loc=target_output['mu'], scale=target_output['sigma']), 1)\n            dist_behaviour = Independent(Normal(loc=behaviour_output['mu'], scale=behaviour_output['sigma']), 1)\n            rhos = dist_target.log_prob(action) - dist_behaviour.log_prob(action)\n            rhos = torch.exp(rhos)\n            return rhos\n        elif action_space_type == 'discrete':\n            dist_target = Categorical(logits=target_output)\n            dist_behaviour = Categorical(logits=behaviour_output)\n            rhos = dist_target.log_prob(action) - dist_behaviour.log_prob(action)\n            rhos = torch.exp(rhos)\n            return rhos",
            "def compute_importance_weights(target_output: Union[torch.Tensor, dict], behaviour_output: Union[torch.Tensor, dict], action: torch.Tensor, action_space_type: str='discrete', requires_grad: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Computing importance sampling weight with given output and action\\n    Arguments:\\n        - target_output (:obj:`Union[torch.Tensor,dict]`): the output taking the action             by the current policy network,             usually this output is network output logit if action space is discrete,             or is a dict containing parameters of action distribution if action space is continuous.\\n        - behaviour_output (:obj:`Union[torch.Tensor,dict]`): the output taking the action             by the behaviour policy network,            usually this output is network output logit,  if action space is discrete,             or is a dict containing parameters of action distribution if action space is continuous.\\n        - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,            i.e.: behaviour_action\\n        - action_space_type (:obj:`str`): action space types in ['discrete', 'continuous']\\n        - requires_grad (:obj:`bool`): whether requires grad computation\\n    Returns:\\n        - rhos (:obj:`torch.Tensor`): Importance sampling weight\\n    Shapes:\\n        - target_output (:obj:`Union[torch.FloatTensor,dict]`): :math:`(T, B, N)`,             where T is timestep, B is batch size and N is action dim\\n        - behaviour_output (:obj:`Union[torch.FloatTensor,dict]`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> target_output = torch.randn(2, 3, 4)\\n        >>> behaviour_output = torch.randn(2, 3, 4)\\n        >>> action = torch.randint(0, 4, (2, 3))\\n        >>> rhos = compute_importance_weights(target_output, behaviour_output, action)\\n    \"\n    grad_context = torch.enable_grad() if requires_grad else torch.no_grad()\n    assert isinstance(action, torch.Tensor)\n    assert action_space_type in ['discrete', 'continuous']\n    with grad_context:\n        if action_space_type == 'continuous':\n            dist_target = Independent(Normal(loc=target_output['mu'], scale=target_output['sigma']), 1)\n            dist_behaviour = Independent(Normal(loc=behaviour_output['mu'], scale=behaviour_output['sigma']), 1)\n            rhos = dist_target.log_prob(action) - dist_behaviour.log_prob(action)\n            rhos = torch.exp(rhos)\n            return rhos\n        elif action_space_type == 'discrete':\n            dist_target = Categorical(logits=target_output)\n            dist_behaviour = Categorical(logits=behaviour_output)\n            rhos = dist_target.log_prob(action) - dist_behaviour.log_prob(action)\n            rhos = torch.exp(rhos)\n            return rhos",
            "def compute_importance_weights(target_output: Union[torch.Tensor, dict], behaviour_output: Union[torch.Tensor, dict], action: torch.Tensor, action_space_type: str='discrete', requires_grad: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Computing importance sampling weight with given output and action\\n    Arguments:\\n        - target_output (:obj:`Union[torch.Tensor,dict]`): the output taking the action             by the current policy network,             usually this output is network output logit if action space is discrete,             or is a dict containing parameters of action distribution if action space is continuous.\\n        - behaviour_output (:obj:`Union[torch.Tensor,dict]`): the output taking the action             by the behaviour policy network,            usually this output is network output logit,  if action space is discrete,             or is a dict containing parameters of action distribution if action space is continuous.\\n        - action (:obj:`torch.Tensor`): the chosen action(index for the discrete action space) in trajectory,            i.e.: behaviour_action\\n        - action_space_type (:obj:`str`): action space types in ['discrete', 'continuous']\\n        - requires_grad (:obj:`bool`): whether requires grad computation\\n    Returns:\\n        - rhos (:obj:`torch.Tensor`): Importance sampling weight\\n    Shapes:\\n        - target_output (:obj:`Union[torch.FloatTensor,dict]`): :math:`(T, B, N)`,             where T is timestep, B is batch size and N is action dim\\n        - behaviour_output (:obj:`Union[torch.FloatTensor,dict]`): :math:`(T, B, N)`\\n        - action (:obj:`torch.LongTensor`): :math:`(T, B)`\\n        - rhos (:obj:`torch.FloatTensor`): :math:`(T, B)`\\n    Examples:\\n        >>> target_output = torch.randn(2, 3, 4)\\n        >>> behaviour_output = torch.randn(2, 3, 4)\\n        >>> action = torch.randint(0, 4, (2, 3))\\n        >>> rhos = compute_importance_weights(target_output, behaviour_output, action)\\n    \"\n    grad_context = torch.enable_grad() if requires_grad else torch.no_grad()\n    assert isinstance(action, torch.Tensor)\n    assert action_space_type in ['discrete', 'continuous']\n    with grad_context:\n        if action_space_type == 'continuous':\n            dist_target = Independent(Normal(loc=target_output['mu'], scale=target_output['sigma']), 1)\n            dist_behaviour = Independent(Normal(loc=behaviour_output['mu'], scale=behaviour_output['sigma']), 1)\n            rhos = dist_target.log_prob(action) - dist_behaviour.log_prob(action)\n            rhos = torch.exp(rhos)\n            return rhos\n        elif action_space_type == 'discrete':\n            dist_target = Categorical(logits=target_output)\n            dist_behaviour = Categorical(logits=behaviour_output)\n            rhos = dist_target.log_prob(action) - dist_behaviour.log_prob(action)\n            rhos = torch.exp(rhos)\n            return rhos"
        ]
    }
]