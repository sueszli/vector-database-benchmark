[
    {
        "func_name": "_fetch_file",
        "original": "def _fetch_file(session: requests.Session, package_name: str, url: str, path: str) -> tuple[str, bool]:\n    \"\"\"\n    Download a file, validate Sphinx Inventory headers and returns status information as a tuple with package\n    name and success status(bool value).\n    \"\"\"\n    try:\n        response = session.get(url, allow_redirects=True, stream=True)\n    except (requests.RequestException, urllib3.exceptions.HTTPError):\n        print(f'{package_name}: Failed to fetch inventory: {url}')\n        traceback.print_exc(file=sys.stderr)\n        return (package_name, False)\n    if not response.ok:\n        print(f'{package_name}: Failed to fetch inventory: {url}')\n        print(f'{package_name}: Failed with status: {response.status_code}', file=sys.stderr)\n        return (package_name, False)\n    if response.url != url:\n        print(f'{package_name}: {url} redirected to {response.url}')\n    with NamedTemporaryFile(suffix=package_name, mode='wb+') as tf:\n        for chunk in response.iter_content(chunk_size=4096):\n            tf.write(chunk)\n        tf.flush()\n        tf.seek(0, 0)\n        line = InventoryFileReader(tf).readline()\n        if not line.startswith('# Sphinx inventory version'):\n            print(f'{package_name}: Response contain unexpected Sphinx Inventory header: {line!r}.')\n            return (package_name, False)\n        tf.seek(0, 0)\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        with open(path, 'wb') as f:\n            shutil.copyfileobj(tf, f)\n    print(f'{package_name}: Fetched inventory: {response.url}')\n    return (package_name, True)",
        "mutated": [
            "def _fetch_file(session: requests.Session, package_name: str, url: str, path: str) -> tuple[str, bool]:\n    if False:\n        i = 10\n    '\\n    Download a file, validate Sphinx Inventory headers and returns status information as a tuple with package\\n    name and success status(bool value).\\n    '\n    try:\n        response = session.get(url, allow_redirects=True, stream=True)\n    except (requests.RequestException, urllib3.exceptions.HTTPError):\n        print(f'{package_name}: Failed to fetch inventory: {url}')\n        traceback.print_exc(file=sys.stderr)\n        return (package_name, False)\n    if not response.ok:\n        print(f'{package_name}: Failed to fetch inventory: {url}')\n        print(f'{package_name}: Failed with status: {response.status_code}', file=sys.stderr)\n        return (package_name, False)\n    if response.url != url:\n        print(f'{package_name}: {url} redirected to {response.url}')\n    with NamedTemporaryFile(suffix=package_name, mode='wb+') as tf:\n        for chunk in response.iter_content(chunk_size=4096):\n            tf.write(chunk)\n        tf.flush()\n        tf.seek(0, 0)\n        line = InventoryFileReader(tf).readline()\n        if not line.startswith('# Sphinx inventory version'):\n            print(f'{package_name}: Response contain unexpected Sphinx Inventory header: {line!r}.')\n            return (package_name, False)\n        tf.seek(0, 0)\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        with open(path, 'wb') as f:\n            shutil.copyfileobj(tf, f)\n    print(f'{package_name}: Fetched inventory: {response.url}')\n    return (package_name, True)",
            "def _fetch_file(session: requests.Session, package_name: str, url: str, path: str) -> tuple[str, bool]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Download a file, validate Sphinx Inventory headers and returns status information as a tuple with package\\n    name and success status(bool value).\\n    '\n    try:\n        response = session.get(url, allow_redirects=True, stream=True)\n    except (requests.RequestException, urllib3.exceptions.HTTPError):\n        print(f'{package_name}: Failed to fetch inventory: {url}')\n        traceback.print_exc(file=sys.stderr)\n        return (package_name, False)\n    if not response.ok:\n        print(f'{package_name}: Failed to fetch inventory: {url}')\n        print(f'{package_name}: Failed with status: {response.status_code}', file=sys.stderr)\n        return (package_name, False)\n    if response.url != url:\n        print(f'{package_name}: {url} redirected to {response.url}')\n    with NamedTemporaryFile(suffix=package_name, mode='wb+') as tf:\n        for chunk in response.iter_content(chunk_size=4096):\n            tf.write(chunk)\n        tf.flush()\n        tf.seek(0, 0)\n        line = InventoryFileReader(tf).readline()\n        if not line.startswith('# Sphinx inventory version'):\n            print(f'{package_name}: Response contain unexpected Sphinx Inventory header: {line!r}.')\n            return (package_name, False)\n        tf.seek(0, 0)\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        with open(path, 'wb') as f:\n            shutil.copyfileobj(tf, f)\n    print(f'{package_name}: Fetched inventory: {response.url}')\n    return (package_name, True)",
            "def _fetch_file(session: requests.Session, package_name: str, url: str, path: str) -> tuple[str, bool]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Download a file, validate Sphinx Inventory headers and returns status information as a tuple with package\\n    name and success status(bool value).\\n    '\n    try:\n        response = session.get(url, allow_redirects=True, stream=True)\n    except (requests.RequestException, urllib3.exceptions.HTTPError):\n        print(f'{package_name}: Failed to fetch inventory: {url}')\n        traceback.print_exc(file=sys.stderr)\n        return (package_name, False)\n    if not response.ok:\n        print(f'{package_name}: Failed to fetch inventory: {url}')\n        print(f'{package_name}: Failed with status: {response.status_code}', file=sys.stderr)\n        return (package_name, False)\n    if response.url != url:\n        print(f'{package_name}: {url} redirected to {response.url}')\n    with NamedTemporaryFile(suffix=package_name, mode='wb+') as tf:\n        for chunk in response.iter_content(chunk_size=4096):\n            tf.write(chunk)\n        tf.flush()\n        tf.seek(0, 0)\n        line = InventoryFileReader(tf).readline()\n        if not line.startswith('# Sphinx inventory version'):\n            print(f'{package_name}: Response contain unexpected Sphinx Inventory header: {line!r}.')\n            return (package_name, False)\n        tf.seek(0, 0)\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        with open(path, 'wb') as f:\n            shutil.copyfileobj(tf, f)\n    print(f'{package_name}: Fetched inventory: {response.url}')\n    return (package_name, True)",
            "def _fetch_file(session: requests.Session, package_name: str, url: str, path: str) -> tuple[str, bool]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Download a file, validate Sphinx Inventory headers and returns status information as a tuple with package\\n    name and success status(bool value).\\n    '\n    try:\n        response = session.get(url, allow_redirects=True, stream=True)\n    except (requests.RequestException, urllib3.exceptions.HTTPError):\n        print(f'{package_name}: Failed to fetch inventory: {url}')\n        traceback.print_exc(file=sys.stderr)\n        return (package_name, False)\n    if not response.ok:\n        print(f'{package_name}: Failed to fetch inventory: {url}')\n        print(f'{package_name}: Failed with status: {response.status_code}', file=sys.stderr)\n        return (package_name, False)\n    if response.url != url:\n        print(f'{package_name}: {url} redirected to {response.url}')\n    with NamedTemporaryFile(suffix=package_name, mode='wb+') as tf:\n        for chunk in response.iter_content(chunk_size=4096):\n            tf.write(chunk)\n        tf.flush()\n        tf.seek(0, 0)\n        line = InventoryFileReader(tf).readline()\n        if not line.startswith('# Sphinx inventory version'):\n            print(f'{package_name}: Response contain unexpected Sphinx Inventory header: {line!r}.')\n            return (package_name, False)\n        tf.seek(0, 0)\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        with open(path, 'wb') as f:\n            shutil.copyfileobj(tf, f)\n    print(f'{package_name}: Fetched inventory: {response.url}')\n    return (package_name, True)",
            "def _fetch_file(session: requests.Session, package_name: str, url: str, path: str) -> tuple[str, bool]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Download a file, validate Sphinx Inventory headers and returns status information as a tuple with package\\n    name and success status(bool value).\\n    '\n    try:\n        response = session.get(url, allow_redirects=True, stream=True)\n    except (requests.RequestException, urllib3.exceptions.HTTPError):\n        print(f'{package_name}: Failed to fetch inventory: {url}')\n        traceback.print_exc(file=sys.stderr)\n        return (package_name, False)\n    if not response.ok:\n        print(f'{package_name}: Failed to fetch inventory: {url}')\n        print(f'{package_name}: Failed with status: {response.status_code}', file=sys.stderr)\n        return (package_name, False)\n    if response.url != url:\n        print(f'{package_name}: {url} redirected to {response.url}')\n    with NamedTemporaryFile(suffix=package_name, mode='wb+') as tf:\n        for chunk in response.iter_content(chunk_size=4096):\n            tf.write(chunk)\n        tf.flush()\n        tf.seek(0, 0)\n        line = InventoryFileReader(tf).readline()\n        if not line.startswith('# Sphinx inventory version'):\n            print(f'{package_name}: Response contain unexpected Sphinx Inventory header: {line!r}.')\n            return (package_name, False)\n        tf.seek(0, 0)\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        with open(path, 'wb') as f:\n            shutil.copyfileobj(tf, f)\n    print(f'{package_name}: Fetched inventory: {response.url}')\n    return (package_name, True)"
        ]
    },
    {
        "func_name": "_is_outdated",
        "original": "def _is_outdated(path: str):\n    if not os.path.exists(path):\n        return True\n    delta = datetime.datetime.now() - datetime.datetime.fromtimestamp(os.path.getmtime(path))\n    return delta > datetime.timedelta(hours=12)",
        "mutated": [
            "def _is_outdated(path: str):\n    if False:\n        i = 10\n    if not os.path.exists(path):\n        return True\n    delta = datetime.datetime.now() - datetime.datetime.fromtimestamp(os.path.getmtime(path))\n    return delta > datetime.timedelta(hours=12)",
            "def _is_outdated(path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not os.path.exists(path):\n        return True\n    delta = datetime.datetime.now() - datetime.datetime.fromtimestamp(os.path.getmtime(path))\n    return delta > datetime.timedelta(hours=12)",
            "def _is_outdated(path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not os.path.exists(path):\n        return True\n    delta = datetime.datetime.now() - datetime.datetime.fromtimestamp(os.path.getmtime(path))\n    return delta > datetime.timedelta(hours=12)",
            "def _is_outdated(path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not os.path.exists(path):\n        return True\n    delta = datetime.datetime.now() - datetime.datetime.fromtimestamp(os.path.getmtime(path))\n    return delta > datetime.timedelta(hours=12)",
            "def _is_outdated(path: str):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not os.path.exists(path):\n        return True\n    delta = datetime.datetime.now() - datetime.datetime.fromtimestamp(os.path.getmtime(path))\n    return delta > datetime.timedelta(hours=12)"
        ]
    },
    {
        "func_name": "fetch_inventories",
        "original": "def fetch_inventories():\n    \"\"\"Fetch all inventories for Airflow documentation packages and store in cache.\"\"\"\n    os.makedirs(os.path.dirname(CACHE_DIR), exist_ok=True)\n    to_download: list[tuple[str, str, str]] = []\n    for pkg_name in get_available_providers_packages():\n        to_download.append((pkg_name, S3_DOC_URL_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    for pkg_name in ['apache-airflow', 'helm-chart']:\n        to_download.append((pkg_name, S3_DOC_URL_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    for pkg_name in ['apache-airflow-providers', 'docker-stack']:\n        to_download.append((pkg_name, S3_DOC_URL_NON_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    to_download.extend(((pkg_name, f'{doc_url}/objects.inv', f'{CACHE_DIR}/{pkg_name}/objects.inv') for (pkg_name, doc_url) in THIRD_PARTY_INDEXES.items()))\n    to_download = [(pkg_name, url, path) for (pkg_name, url, path) in to_download if _is_outdated(path)]\n    if not to_download:\n        print('Nothing to do')\n        return []\n    print(f'To download {len(to_download)} inventorie(s)')\n    with requests.Session() as session, concurrent.futures.ThreadPoolExecutor(DEFAULT_POOLSIZE) as pool:\n        download_results: Iterator[tuple[str, bool]] = pool.map(_fetch_file, itertools.repeat(session, len(to_download)), (pkg_name for (pkg_name, _, _) in to_download), (url for (_, url, _) in to_download), (path for (_, _, path) in to_download))\n    (failed, success) = partition(lambda d: d[1], download_results)\n    (failed, success) = (list(failed), list(success))\n    print(f'Result: {len(success)} success, {len(failed)} failed')\n    if failed:\n        terminate = False\n        print('Failed packages:')\n        for (pkg_no, (pkg_name, _)) in enumerate(failed, start=1):\n            print(f'{pkg_no}. {pkg_name}')\n            if not terminate and (not pkg_name.startswith('apache-airflow')):\n                terminate = True\n        if terminate:\n            print('Terminate execution.')\n            raise SystemExit(1)\n    return [pkg_name for (pkg_name, status) in failed]",
        "mutated": [
            "def fetch_inventories():\n    if False:\n        i = 10\n    'Fetch all inventories for Airflow documentation packages and store in cache.'\n    os.makedirs(os.path.dirname(CACHE_DIR), exist_ok=True)\n    to_download: list[tuple[str, str, str]] = []\n    for pkg_name in get_available_providers_packages():\n        to_download.append((pkg_name, S3_DOC_URL_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    for pkg_name in ['apache-airflow', 'helm-chart']:\n        to_download.append((pkg_name, S3_DOC_URL_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    for pkg_name in ['apache-airflow-providers', 'docker-stack']:\n        to_download.append((pkg_name, S3_DOC_URL_NON_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    to_download.extend(((pkg_name, f'{doc_url}/objects.inv', f'{CACHE_DIR}/{pkg_name}/objects.inv') for (pkg_name, doc_url) in THIRD_PARTY_INDEXES.items()))\n    to_download = [(pkg_name, url, path) for (pkg_name, url, path) in to_download if _is_outdated(path)]\n    if not to_download:\n        print('Nothing to do')\n        return []\n    print(f'To download {len(to_download)} inventorie(s)')\n    with requests.Session() as session, concurrent.futures.ThreadPoolExecutor(DEFAULT_POOLSIZE) as pool:\n        download_results: Iterator[tuple[str, bool]] = pool.map(_fetch_file, itertools.repeat(session, len(to_download)), (pkg_name for (pkg_name, _, _) in to_download), (url for (_, url, _) in to_download), (path for (_, _, path) in to_download))\n    (failed, success) = partition(lambda d: d[1], download_results)\n    (failed, success) = (list(failed), list(success))\n    print(f'Result: {len(success)} success, {len(failed)} failed')\n    if failed:\n        terminate = False\n        print('Failed packages:')\n        for (pkg_no, (pkg_name, _)) in enumerate(failed, start=1):\n            print(f'{pkg_no}. {pkg_name}')\n            if not terminate and (not pkg_name.startswith('apache-airflow')):\n                terminate = True\n        if terminate:\n            print('Terminate execution.')\n            raise SystemExit(1)\n    return [pkg_name for (pkg_name, status) in failed]",
            "def fetch_inventories():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Fetch all inventories for Airflow documentation packages and store in cache.'\n    os.makedirs(os.path.dirname(CACHE_DIR), exist_ok=True)\n    to_download: list[tuple[str, str, str]] = []\n    for pkg_name in get_available_providers_packages():\n        to_download.append((pkg_name, S3_DOC_URL_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    for pkg_name in ['apache-airflow', 'helm-chart']:\n        to_download.append((pkg_name, S3_DOC_URL_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    for pkg_name in ['apache-airflow-providers', 'docker-stack']:\n        to_download.append((pkg_name, S3_DOC_URL_NON_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    to_download.extend(((pkg_name, f'{doc_url}/objects.inv', f'{CACHE_DIR}/{pkg_name}/objects.inv') for (pkg_name, doc_url) in THIRD_PARTY_INDEXES.items()))\n    to_download = [(pkg_name, url, path) for (pkg_name, url, path) in to_download if _is_outdated(path)]\n    if not to_download:\n        print('Nothing to do')\n        return []\n    print(f'To download {len(to_download)} inventorie(s)')\n    with requests.Session() as session, concurrent.futures.ThreadPoolExecutor(DEFAULT_POOLSIZE) as pool:\n        download_results: Iterator[tuple[str, bool]] = pool.map(_fetch_file, itertools.repeat(session, len(to_download)), (pkg_name for (pkg_name, _, _) in to_download), (url for (_, url, _) in to_download), (path for (_, _, path) in to_download))\n    (failed, success) = partition(lambda d: d[1], download_results)\n    (failed, success) = (list(failed), list(success))\n    print(f'Result: {len(success)} success, {len(failed)} failed')\n    if failed:\n        terminate = False\n        print('Failed packages:')\n        for (pkg_no, (pkg_name, _)) in enumerate(failed, start=1):\n            print(f'{pkg_no}. {pkg_name}')\n            if not terminate and (not pkg_name.startswith('apache-airflow')):\n                terminate = True\n        if terminate:\n            print('Terminate execution.')\n            raise SystemExit(1)\n    return [pkg_name for (pkg_name, status) in failed]",
            "def fetch_inventories():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Fetch all inventories for Airflow documentation packages and store in cache.'\n    os.makedirs(os.path.dirname(CACHE_DIR), exist_ok=True)\n    to_download: list[tuple[str, str, str]] = []\n    for pkg_name in get_available_providers_packages():\n        to_download.append((pkg_name, S3_DOC_URL_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    for pkg_name in ['apache-airflow', 'helm-chart']:\n        to_download.append((pkg_name, S3_DOC_URL_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    for pkg_name in ['apache-airflow-providers', 'docker-stack']:\n        to_download.append((pkg_name, S3_DOC_URL_NON_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    to_download.extend(((pkg_name, f'{doc_url}/objects.inv', f'{CACHE_DIR}/{pkg_name}/objects.inv') for (pkg_name, doc_url) in THIRD_PARTY_INDEXES.items()))\n    to_download = [(pkg_name, url, path) for (pkg_name, url, path) in to_download if _is_outdated(path)]\n    if not to_download:\n        print('Nothing to do')\n        return []\n    print(f'To download {len(to_download)} inventorie(s)')\n    with requests.Session() as session, concurrent.futures.ThreadPoolExecutor(DEFAULT_POOLSIZE) as pool:\n        download_results: Iterator[tuple[str, bool]] = pool.map(_fetch_file, itertools.repeat(session, len(to_download)), (pkg_name for (pkg_name, _, _) in to_download), (url for (_, url, _) in to_download), (path for (_, _, path) in to_download))\n    (failed, success) = partition(lambda d: d[1], download_results)\n    (failed, success) = (list(failed), list(success))\n    print(f'Result: {len(success)} success, {len(failed)} failed')\n    if failed:\n        terminate = False\n        print('Failed packages:')\n        for (pkg_no, (pkg_name, _)) in enumerate(failed, start=1):\n            print(f'{pkg_no}. {pkg_name}')\n            if not terminate and (not pkg_name.startswith('apache-airflow')):\n                terminate = True\n        if terminate:\n            print('Terminate execution.')\n            raise SystemExit(1)\n    return [pkg_name for (pkg_name, status) in failed]",
            "def fetch_inventories():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Fetch all inventories for Airflow documentation packages and store in cache.'\n    os.makedirs(os.path.dirname(CACHE_DIR), exist_ok=True)\n    to_download: list[tuple[str, str, str]] = []\n    for pkg_name in get_available_providers_packages():\n        to_download.append((pkg_name, S3_DOC_URL_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    for pkg_name in ['apache-airflow', 'helm-chart']:\n        to_download.append((pkg_name, S3_DOC_URL_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    for pkg_name in ['apache-airflow-providers', 'docker-stack']:\n        to_download.append((pkg_name, S3_DOC_URL_NON_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    to_download.extend(((pkg_name, f'{doc_url}/objects.inv', f'{CACHE_DIR}/{pkg_name}/objects.inv') for (pkg_name, doc_url) in THIRD_PARTY_INDEXES.items()))\n    to_download = [(pkg_name, url, path) for (pkg_name, url, path) in to_download if _is_outdated(path)]\n    if not to_download:\n        print('Nothing to do')\n        return []\n    print(f'To download {len(to_download)} inventorie(s)')\n    with requests.Session() as session, concurrent.futures.ThreadPoolExecutor(DEFAULT_POOLSIZE) as pool:\n        download_results: Iterator[tuple[str, bool]] = pool.map(_fetch_file, itertools.repeat(session, len(to_download)), (pkg_name for (pkg_name, _, _) in to_download), (url for (_, url, _) in to_download), (path for (_, _, path) in to_download))\n    (failed, success) = partition(lambda d: d[1], download_results)\n    (failed, success) = (list(failed), list(success))\n    print(f'Result: {len(success)} success, {len(failed)} failed')\n    if failed:\n        terminate = False\n        print('Failed packages:')\n        for (pkg_no, (pkg_name, _)) in enumerate(failed, start=1):\n            print(f'{pkg_no}. {pkg_name}')\n            if not terminate and (not pkg_name.startswith('apache-airflow')):\n                terminate = True\n        if terminate:\n            print('Terminate execution.')\n            raise SystemExit(1)\n    return [pkg_name for (pkg_name, status) in failed]",
            "def fetch_inventories():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Fetch all inventories for Airflow documentation packages and store in cache.'\n    os.makedirs(os.path.dirname(CACHE_DIR), exist_ok=True)\n    to_download: list[tuple[str, str, str]] = []\n    for pkg_name in get_available_providers_packages():\n        to_download.append((pkg_name, S3_DOC_URL_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    for pkg_name in ['apache-airflow', 'helm-chart']:\n        to_download.append((pkg_name, S3_DOC_URL_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    for pkg_name in ['apache-airflow-providers', 'docker-stack']:\n        to_download.append((pkg_name, S3_DOC_URL_NON_VERSIONED.format(package_name=pkg_name), f'{CACHE_DIR}/{pkg_name}/objects.inv'))\n    to_download.extend(((pkg_name, f'{doc_url}/objects.inv', f'{CACHE_DIR}/{pkg_name}/objects.inv') for (pkg_name, doc_url) in THIRD_PARTY_INDEXES.items()))\n    to_download = [(pkg_name, url, path) for (pkg_name, url, path) in to_download if _is_outdated(path)]\n    if not to_download:\n        print('Nothing to do')\n        return []\n    print(f'To download {len(to_download)} inventorie(s)')\n    with requests.Session() as session, concurrent.futures.ThreadPoolExecutor(DEFAULT_POOLSIZE) as pool:\n        download_results: Iterator[tuple[str, bool]] = pool.map(_fetch_file, itertools.repeat(session, len(to_download)), (pkg_name for (pkg_name, _, _) in to_download), (url for (_, url, _) in to_download), (path for (_, _, path) in to_download))\n    (failed, success) = partition(lambda d: d[1], download_results)\n    (failed, success) = (list(failed), list(success))\n    print(f'Result: {len(success)} success, {len(failed)} failed')\n    if failed:\n        terminate = False\n        print('Failed packages:')\n        for (pkg_no, (pkg_name, _)) in enumerate(failed, start=1):\n            print(f'{pkg_no}. {pkg_name}')\n            if not terminate and (not pkg_name.startswith('apache-airflow')):\n                terminate = True\n        if terminate:\n            print('Terminate execution.')\n            raise SystemExit(1)\n    return [pkg_name for (pkg_name, status) in failed]"
        ]
    }
]