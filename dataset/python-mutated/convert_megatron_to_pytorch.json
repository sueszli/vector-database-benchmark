[
    {
        "func_name": "recursive_print",
        "original": "def recursive_print(name, val, spaces=0):\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
        "mutated": [
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)"
        ]
    },
    {
        "func_name": "fix_query_key_value_ordering",
        "original": "def fix_query_key_value_ordering(param, num_splits, num_heads, hidden_size):\n    input_shape = param.size()\n    saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n    param = param.view(*saved_shape)\n    param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
        "mutated": [
            "def fix_query_key_value_ordering(param, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n    input_shape = param.size()\n    saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n    param = param.view(*saved_shape)\n    param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def fix_query_key_value_ordering(param, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    input_shape = param.size()\n    saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n    param = param.view(*saved_shape)\n    param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def fix_query_key_value_ordering(param, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    input_shape = param.size()\n    saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n    param = param.view(*saved_shape)\n    param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def fix_query_key_value_ordering(param, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    input_shape = param.size()\n    saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n    param = param.view(*saved_shape)\n    param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def fix_query_key_value_ordering(param, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    input_shape = param.size()\n    saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n    param = param.view(*saved_shape)\n    param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param"
        ]
    },
    {
        "func_name": "convert_megatron_checkpoint",
        "original": "def convert_megatron_checkpoint(sd_megatron, config):\n    \"\"\"\n    Converts a Megatron checkpoint to a HuggingFace GPT-SW3 checkpoint.\n    \"\"\"\n    n_positions = config.n_positions\n    layers = config.n_layer\n    vocab_size = config.vocab_size\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    word_embeddings = sd_megatron['model.language_model.embedding.word_embeddings.weight'][:vocab_size, :]\n    sd_hf = {'transformer.wte.weight': word_embeddings, 'transformer.wpe.weight': sd_megatron['model.language_model.embedding.position_embeddings.weight'], 'transformer.ln_f.weight': sd_megatron['model.language_model.encoder.final_layernorm.weight'], 'transformer.ln_f.bias': sd_megatron['model.language_model.encoder.final_layernorm.bias']}\n    pf = 'model.language_model.encoder.layers.'\n    for i in range(layers):\n        causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.bool))\n        causal_mask = causal_mask.view(1, 1, n_positions, n_positions)\n        sd_hf[f'transformer.h.{i}.attn.bias'] = causal_mask\n        sd_hf[f'transformer.h.{i}.attn.masked_bias'] = torch.tensor(-10000.0, dtype=torch.bfloat16)\n        sd_hf[f'transformer.h.{i}.ln_1.weight'] = sd_megatron[f'{pf}{i}.input_layernorm.weight']\n        sd_hf[f'transformer.h.{i}.ln_1.bias'] = sd_megatron[f'{pf}{i}.input_layernorm.bias']\n        val1 = sd_megatron[f'{pf}{i}.self_attention.query_key_value.weight']\n        val1 = fix_query_key_value_ordering(val1, 3, heads, hidden_size_per_head)\n        sd_hf[f'transformer.h.{i}.attn.c_attn.weight'] = val1.transpose(0, 1).contiguous()\n        val2 = sd_megatron[f'{pf}{i}.self_attention.query_key_value.bias']\n        val2 = fix_query_key_value_ordering(val2, 3, heads, hidden_size_per_head)\n        sd_hf[f'transformer.h.{i}.attn.c_attn.bias'] = val2\n        sd_hf[f'transformer.h.{i}.attn.c_proj.weight'] = sd_megatron[f'{pf}{i}.self_attention.dense.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.attn.c_proj.bias'] = sd_megatron[f'{pf}{i}.self_attention.dense.bias']\n        sd_hf[f'transformer.h.{i}.ln_2.weight'] = sd_megatron[f'{pf}{i}.post_attention_layernorm.weight']\n        sd_hf[f'transformer.h.{i}.ln_2.bias'] = sd_megatron[f'{pf}{i}.post_attention_layernorm.bias']\n        sd_hf[f'transformer.h.{i}.mlp.c_fc.weight'] = sd_megatron[f'{pf}{i}.mlp.dense_h_to_4h.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.mlp.c_fc.bias'] = sd_megatron[f'{pf}{i}.mlp.dense_h_to_4h.bias']\n        sd_hf[f'transformer.h.{i}.mlp.c_proj.weight'] = sd_megatron[f'{pf}{i}.mlp.dense_4h_to_h.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.mlp.c_proj.bias'] = sd_megatron[f'{pf}{i}.mlp.dense_4h_to_h.bias']\n    sd_hf['lm_head.weight'] = word_embeddings\n    return sd_hf",
        "mutated": [
            "def convert_megatron_checkpoint(sd_megatron, config):\n    if False:\n        i = 10\n    '\\n    Converts a Megatron checkpoint to a HuggingFace GPT-SW3 checkpoint.\\n    '\n    n_positions = config.n_positions\n    layers = config.n_layer\n    vocab_size = config.vocab_size\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    word_embeddings = sd_megatron['model.language_model.embedding.word_embeddings.weight'][:vocab_size, :]\n    sd_hf = {'transformer.wte.weight': word_embeddings, 'transformer.wpe.weight': sd_megatron['model.language_model.embedding.position_embeddings.weight'], 'transformer.ln_f.weight': sd_megatron['model.language_model.encoder.final_layernorm.weight'], 'transformer.ln_f.bias': sd_megatron['model.language_model.encoder.final_layernorm.bias']}\n    pf = 'model.language_model.encoder.layers.'\n    for i in range(layers):\n        causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.bool))\n        causal_mask = causal_mask.view(1, 1, n_positions, n_positions)\n        sd_hf[f'transformer.h.{i}.attn.bias'] = causal_mask\n        sd_hf[f'transformer.h.{i}.attn.masked_bias'] = torch.tensor(-10000.0, dtype=torch.bfloat16)\n        sd_hf[f'transformer.h.{i}.ln_1.weight'] = sd_megatron[f'{pf}{i}.input_layernorm.weight']\n        sd_hf[f'transformer.h.{i}.ln_1.bias'] = sd_megatron[f'{pf}{i}.input_layernorm.bias']\n        val1 = sd_megatron[f'{pf}{i}.self_attention.query_key_value.weight']\n        val1 = fix_query_key_value_ordering(val1, 3, heads, hidden_size_per_head)\n        sd_hf[f'transformer.h.{i}.attn.c_attn.weight'] = val1.transpose(0, 1).contiguous()\n        val2 = sd_megatron[f'{pf}{i}.self_attention.query_key_value.bias']\n        val2 = fix_query_key_value_ordering(val2, 3, heads, hidden_size_per_head)\n        sd_hf[f'transformer.h.{i}.attn.c_attn.bias'] = val2\n        sd_hf[f'transformer.h.{i}.attn.c_proj.weight'] = sd_megatron[f'{pf}{i}.self_attention.dense.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.attn.c_proj.bias'] = sd_megatron[f'{pf}{i}.self_attention.dense.bias']\n        sd_hf[f'transformer.h.{i}.ln_2.weight'] = sd_megatron[f'{pf}{i}.post_attention_layernorm.weight']\n        sd_hf[f'transformer.h.{i}.ln_2.bias'] = sd_megatron[f'{pf}{i}.post_attention_layernorm.bias']\n        sd_hf[f'transformer.h.{i}.mlp.c_fc.weight'] = sd_megatron[f'{pf}{i}.mlp.dense_h_to_4h.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.mlp.c_fc.bias'] = sd_megatron[f'{pf}{i}.mlp.dense_h_to_4h.bias']\n        sd_hf[f'transformer.h.{i}.mlp.c_proj.weight'] = sd_megatron[f'{pf}{i}.mlp.dense_4h_to_h.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.mlp.c_proj.bias'] = sd_megatron[f'{pf}{i}.mlp.dense_4h_to_h.bias']\n    sd_hf['lm_head.weight'] = word_embeddings\n    return sd_hf",
            "def convert_megatron_checkpoint(sd_megatron, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Converts a Megatron checkpoint to a HuggingFace GPT-SW3 checkpoint.\\n    '\n    n_positions = config.n_positions\n    layers = config.n_layer\n    vocab_size = config.vocab_size\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    word_embeddings = sd_megatron['model.language_model.embedding.word_embeddings.weight'][:vocab_size, :]\n    sd_hf = {'transformer.wte.weight': word_embeddings, 'transformer.wpe.weight': sd_megatron['model.language_model.embedding.position_embeddings.weight'], 'transformer.ln_f.weight': sd_megatron['model.language_model.encoder.final_layernorm.weight'], 'transformer.ln_f.bias': sd_megatron['model.language_model.encoder.final_layernorm.bias']}\n    pf = 'model.language_model.encoder.layers.'\n    for i in range(layers):\n        causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.bool))\n        causal_mask = causal_mask.view(1, 1, n_positions, n_positions)\n        sd_hf[f'transformer.h.{i}.attn.bias'] = causal_mask\n        sd_hf[f'transformer.h.{i}.attn.masked_bias'] = torch.tensor(-10000.0, dtype=torch.bfloat16)\n        sd_hf[f'transformer.h.{i}.ln_1.weight'] = sd_megatron[f'{pf}{i}.input_layernorm.weight']\n        sd_hf[f'transformer.h.{i}.ln_1.bias'] = sd_megatron[f'{pf}{i}.input_layernorm.bias']\n        val1 = sd_megatron[f'{pf}{i}.self_attention.query_key_value.weight']\n        val1 = fix_query_key_value_ordering(val1, 3, heads, hidden_size_per_head)\n        sd_hf[f'transformer.h.{i}.attn.c_attn.weight'] = val1.transpose(0, 1).contiguous()\n        val2 = sd_megatron[f'{pf}{i}.self_attention.query_key_value.bias']\n        val2 = fix_query_key_value_ordering(val2, 3, heads, hidden_size_per_head)\n        sd_hf[f'transformer.h.{i}.attn.c_attn.bias'] = val2\n        sd_hf[f'transformer.h.{i}.attn.c_proj.weight'] = sd_megatron[f'{pf}{i}.self_attention.dense.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.attn.c_proj.bias'] = sd_megatron[f'{pf}{i}.self_attention.dense.bias']\n        sd_hf[f'transformer.h.{i}.ln_2.weight'] = sd_megatron[f'{pf}{i}.post_attention_layernorm.weight']\n        sd_hf[f'transformer.h.{i}.ln_2.bias'] = sd_megatron[f'{pf}{i}.post_attention_layernorm.bias']\n        sd_hf[f'transformer.h.{i}.mlp.c_fc.weight'] = sd_megatron[f'{pf}{i}.mlp.dense_h_to_4h.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.mlp.c_fc.bias'] = sd_megatron[f'{pf}{i}.mlp.dense_h_to_4h.bias']\n        sd_hf[f'transformer.h.{i}.mlp.c_proj.weight'] = sd_megatron[f'{pf}{i}.mlp.dense_4h_to_h.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.mlp.c_proj.bias'] = sd_megatron[f'{pf}{i}.mlp.dense_4h_to_h.bias']\n    sd_hf['lm_head.weight'] = word_embeddings\n    return sd_hf",
            "def convert_megatron_checkpoint(sd_megatron, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Converts a Megatron checkpoint to a HuggingFace GPT-SW3 checkpoint.\\n    '\n    n_positions = config.n_positions\n    layers = config.n_layer\n    vocab_size = config.vocab_size\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    word_embeddings = sd_megatron['model.language_model.embedding.word_embeddings.weight'][:vocab_size, :]\n    sd_hf = {'transformer.wte.weight': word_embeddings, 'transformer.wpe.weight': sd_megatron['model.language_model.embedding.position_embeddings.weight'], 'transformer.ln_f.weight': sd_megatron['model.language_model.encoder.final_layernorm.weight'], 'transformer.ln_f.bias': sd_megatron['model.language_model.encoder.final_layernorm.bias']}\n    pf = 'model.language_model.encoder.layers.'\n    for i in range(layers):\n        causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.bool))\n        causal_mask = causal_mask.view(1, 1, n_positions, n_positions)\n        sd_hf[f'transformer.h.{i}.attn.bias'] = causal_mask\n        sd_hf[f'transformer.h.{i}.attn.masked_bias'] = torch.tensor(-10000.0, dtype=torch.bfloat16)\n        sd_hf[f'transformer.h.{i}.ln_1.weight'] = sd_megatron[f'{pf}{i}.input_layernorm.weight']\n        sd_hf[f'transformer.h.{i}.ln_1.bias'] = sd_megatron[f'{pf}{i}.input_layernorm.bias']\n        val1 = sd_megatron[f'{pf}{i}.self_attention.query_key_value.weight']\n        val1 = fix_query_key_value_ordering(val1, 3, heads, hidden_size_per_head)\n        sd_hf[f'transformer.h.{i}.attn.c_attn.weight'] = val1.transpose(0, 1).contiguous()\n        val2 = sd_megatron[f'{pf}{i}.self_attention.query_key_value.bias']\n        val2 = fix_query_key_value_ordering(val2, 3, heads, hidden_size_per_head)\n        sd_hf[f'transformer.h.{i}.attn.c_attn.bias'] = val2\n        sd_hf[f'transformer.h.{i}.attn.c_proj.weight'] = sd_megatron[f'{pf}{i}.self_attention.dense.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.attn.c_proj.bias'] = sd_megatron[f'{pf}{i}.self_attention.dense.bias']\n        sd_hf[f'transformer.h.{i}.ln_2.weight'] = sd_megatron[f'{pf}{i}.post_attention_layernorm.weight']\n        sd_hf[f'transformer.h.{i}.ln_2.bias'] = sd_megatron[f'{pf}{i}.post_attention_layernorm.bias']\n        sd_hf[f'transformer.h.{i}.mlp.c_fc.weight'] = sd_megatron[f'{pf}{i}.mlp.dense_h_to_4h.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.mlp.c_fc.bias'] = sd_megatron[f'{pf}{i}.mlp.dense_h_to_4h.bias']\n        sd_hf[f'transformer.h.{i}.mlp.c_proj.weight'] = sd_megatron[f'{pf}{i}.mlp.dense_4h_to_h.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.mlp.c_proj.bias'] = sd_megatron[f'{pf}{i}.mlp.dense_4h_to_h.bias']\n    sd_hf['lm_head.weight'] = word_embeddings\n    return sd_hf",
            "def convert_megatron_checkpoint(sd_megatron, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Converts a Megatron checkpoint to a HuggingFace GPT-SW3 checkpoint.\\n    '\n    n_positions = config.n_positions\n    layers = config.n_layer\n    vocab_size = config.vocab_size\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    word_embeddings = sd_megatron['model.language_model.embedding.word_embeddings.weight'][:vocab_size, :]\n    sd_hf = {'transformer.wte.weight': word_embeddings, 'transformer.wpe.weight': sd_megatron['model.language_model.embedding.position_embeddings.weight'], 'transformer.ln_f.weight': sd_megatron['model.language_model.encoder.final_layernorm.weight'], 'transformer.ln_f.bias': sd_megatron['model.language_model.encoder.final_layernorm.bias']}\n    pf = 'model.language_model.encoder.layers.'\n    for i in range(layers):\n        causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.bool))\n        causal_mask = causal_mask.view(1, 1, n_positions, n_positions)\n        sd_hf[f'transformer.h.{i}.attn.bias'] = causal_mask\n        sd_hf[f'transformer.h.{i}.attn.masked_bias'] = torch.tensor(-10000.0, dtype=torch.bfloat16)\n        sd_hf[f'transformer.h.{i}.ln_1.weight'] = sd_megatron[f'{pf}{i}.input_layernorm.weight']\n        sd_hf[f'transformer.h.{i}.ln_1.bias'] = sd_megatron[f'{pf}{i}.input_layernorm.bias']\n        val1 = sd_megatron[f'{pf}{i}.self_attention.query_key_value.weight']\n        val1 = fix_query_key_value_ordering(val1, 3, heads, hidden_size_per_head)\n        sd_hf[f'transformer.h.{i}.attn.c_attn.weight'] = val1.transpose(0, 1).contiguous()\n        val2 = sd_megatron[f'{pf}{i}.self_attention.query_key_value.bias']\n        val2 = fix_query_key_value_ordering(val2, 3, heads, hidden_size_per_head)\n        sd_hf[f'transformer.h.{i}.attn.c_attn.bias'] = val2\n        sd_hf[f'transformer.h.{i}.attn.c_proj.weight'] = sd_megatron[f'{pf}{i}.self_attention.dense.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.attn.c_proj.bias'] = sd_megatron[f'{pf}{i}.self_attention.dense.bias']\n        sd_hf[f'transformer.h.{i}.ln_2.weight'] = sd_megatron[f'{pf}{i}.post_attention_layernorm.weight']\n        sd_hf[f'transformer.h.{i}.ln_2.bias'] = sd_megatron[f'{pf}{i}.post_attention_layernorm.bias']\n        sd_hf[f'transformer.h.{i}.mlp.c_fc.weight'] = sd_megatron[f'{pf}{i}.mlp.dense_h_to_4h.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.mlp.c_fc.bias'] = sd_megatron[f'{pf}{i}.mlp.dense_h_to_4h.bias']\n        sd_hf[f'transformer.h.{i}.mlp.c_proj.weight'] = sd_megatron[f'{pf}{i}.mlp.dense_4h_to_h.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.mlp.c_proj.bias'] = sd_megatron[f'{pf}{i}.mlp.dense_4h_to_h.bias']\n    sd_hf['lm_head.weight'] = word_embeddings\n    return sd_hf",
            "def convert_megatron_checkpoint(sd_megatron, config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Converts a Megatron checkpoint to a HuggingFace GPT-SW3 checkpoint.\\n    '\n    n_positions = config.n_positions\n    layers = config.n_layer\n    vocab_size = config.vocab_size\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    word_embeddings = sd_megatron['model.language_model.embedding.word_embeddings.weight'][:vocab_size, :]\n    sd_hf = {'transformer.wte.weight': word_embeddings, 'transformer.wpe.weight': sd_megatron['model.language_model.embedding.position_embeddings.weight'], 'transformer.ln_f.weight': sd_megatron['model.language_model.encoder.final_layernorm.weight'], 'transformer.ln_f.bias': sd_megatron['model.language_model.encoder.final_layernorm.bias']}\n    pf = 'model.language_model.encoder.layers.'\n    for i in range(layers):\n        causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=torch.bool))\n        causal_mask = causal_mask.view(1, 1, n_positions, n_positions)\n        sd_hf[f'transformer.h.{i}.attn.bias'] = causal_mask\n        sd_hf[f'transformer.h.{i}.attn.masked_bias'] = torch.tensor(-10000.0, dtype=torch.bfloat16)\n        sd_hf[f'transformer.h.{i}.ln_1.weight'] = sd_megatron[f'{pf}{i}.input_layernorm.weight']\n        sd_hf[f'transformer.h.{i}.ln_1.bias'] = sd_megatron[f'{pf}{i}.input_layernorm.bias']\n        val1 = sd_megatron[f'{pf}{i}.self_attention.query_key_value.weight']\n        val1 = fix_query_key_value_ordering(val1, 3, heads, hidden_size_per_head)\n        sd_hf[f'transformer.h.{i}.attn.c_attn.weight'] = val1.transpose(0, 1).contiguous()\n        val2 = sd_megatron[f'{pf}{i}.self_attention.query_key_value.bias']\n        val2 = fix_query_key_value_ordering(val2, 3, heads, hidden_size_per_head)\n        sd_hf[f'transformer.h.{i}.attn.c_attn.bias'] = val2\n        sd_hf[f'transformer.h.{i}.attn.c_proj.weight'] = sd_megatron[f'{pf}{i}.self_attention.dense.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.attn.c_proj.bias'] = sd_megatron[f'{pf}{i}.self_attention.dense.bias']\n        sd_hf[f'transformer.h.{i}.ln_2.weight'] = sd_megatron[f'{pf}{i}.post_attention_layernorm.weight']\n        sd_hf[f'transformer.h.{i}.ln_2.bias'] = sd_megatron[f'{pf}{i}.post_attention_layernorm.bias']\n        sd_hf[f'transformer.h.{i}.mlp.c_fc.weight'] = sd_megatron[f'{pf}{i}.mlp.dense_h_to_4h.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.mlp.c_fc.bias'] = sd_megatron[f'{pf}{i}.mlp.dense_h_to_4h.bias']\n        sd_hf[f'transformer.h.{i}.mlp.c_proj.weight'] = sd_megatron[f'{pf}{i}.mlp.dense_4h_to_h.weight'].transpose(0, 1)\n        sd_hf[f'transformer.h.{i}.mlp.c_proj.bias'] = sd_megatron[f'{pf}{i}.mlp.dense_4h_to_h.bias']\n    sd_hf['lm_head.weight'] = word_embeddings\n    return sd_hf"
        ]
    },
    {
        "func_name": "copy_config",
        "original": "def copy_config(config_hf, config_megatron):\n    \"\"\"Copy the config from Megatron to hf.\"\"\"\n    config_hf.vocab_size = 64000\n    config_hf.n_positions = config_megatron['encoder_seq_length']\n    config_hf.n_embd = config_megatron['hidden_size']\n    config_hf.n_layer = config_megatron['num_layers']\n    config_hf.n_head = config_megatron['num_attention_heads']\n    config_hf.n_inner = config_megatron['ffn_hidden_size']\n    config_hf.activation_function = 'gelu'\n    config_hf.resid_pdrop = 0.1\n    config_hf.embd_pdrop = 0.1\n    config_hf.attn_pdrop = 0.1\n    config_hf.layer_norm_epsilon = config_megatron['layernorm_epsilon']\n    config_hf.initializer_range = config_megatron['init_method_std']\n    config_hf.apply_query_key_layer_scaling = config_megatron['apply_query_key_layer_scaling']\n    config_hf.normalize_attention_scores = True\n    config_hf.use_cache = True\n    if config_megatron['hidden_size'] == 4096:\n        config_hf.bos_token_id = 1\n        config_hf.eos_token_id = 1\n        config_hf.pad_token_id = 0\n    else:\n        config_hf.bos_token_id = 2\n        config_hf.eos_token_id = 3\n        config_hf.pad_token_id = 0\n    return config_hf",
        "mutated": [
            "def copy_config(config_hf, config_megatron):\n    if False:\n        i = 10\n    'Copy the config from Megatron to hf.'\n    config_hf.vocab_size = 64000\n    config_hf.n_positions = config_megatron['encoder_seq_length']\n    config_hf.n_embd = config_megatron['hidden_size']\n    config_hf.n_layer = config_megatron['num_layers']\n    config_hf.n_head = config_megatron['num_attention_heads']\n    config_hf.n_inner = config_megatron['ffn_hidden_size']\n    config_hf.activation_function = 'gelu'\n    config_hf.resid_pdrop = 0.1\n    config_hf.embd_pdrop = 0.1\n    config_hf.attn_pdrop = 0.1\n    config_hf.layer_norm_epsilon = config_megatron['layernorm_epsilon']\n    config_hf.initializer_range = config_megatron['init_method_std']\n    config_hf.apply_query_key_layer_scaling = config_megatron['apply_query_key_layer_scaling']\n    config_hf.normalize_attention_scores = True\n    config_hf.use_cache = True\n    if config_megatron['hidden_size'] == 4096:\n        config_hf.bos_token_id = 1\n        config_hf.eos_token_id = 1\n        config_hf.pad_token_id = 0\n    else:\n        config_hf.bos_token_id = 2\n        config_hf.eos_token_id = 3\n        config_hf.pad_token_id = 0\n    return config_hf",
            "def copy_config(config_hf, config_megatron):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Copy the config from Megatron to hf.'\n    config_hf.vocab_size = 64000\n    config_hf.n_positions = config_megatron['encoder_seq_length']\n    config_hf.n_embd = config_megatron['hidden_size']\n    config_hf.n_layer = config_megatron['num_layers']\n    config_hf.n_head = config_megatron['num_attention_heads']\n    config_hf.n_inner = config_megatron['ffn_hidden_size']\n    config_hf.activation_function = 'gelu'\n    config_hf.resid_pdrop = 0.1\n    config_hf.embd_pdrop = 0.1\n    config_hf.attn_pdrop = 0.1\n    config_hf.layer_norm_epsilon = config_megatron['layernorm_epsilon']\n    config_hf.initializer_range = config_megatron['init_method_std']\n    config_hf.apply_query_key_layer_scaling = config_megatron['apply_query_key_layer_scaling']\n    config_hf.normalize_attention_scores = True\n    config_hf.use_cache = True\n    if config_megatron['hidden_size'] == 4096:\n        config_hf.bos_token_id = 1\n        config_hf.eos_token_id = 1\n        config_hf.pad_token_id = 0\n    else:\n        config_hf.bos_token_id = 2\n        config_hf.eos_token_id = 3\n        config_hf.pad_token_id = 0\n    return config_hf",
            "def copy_config(config_hf, config_megatron):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Copy the config from Megatron to hf.'\n    config_hf.vocab_size = 64000\n    config_hf.n_positions = config_megatron['encoder_seq_length']\n    config_hf.n_embd = config_megatron['hidden_size']\n    config_hf.n_layer = config_megatron['num_layers']\n    config_hf.n_head = config_megatron['num_attention_heads']\n    config_hf.n_inner = config_megatron['ffn_hidden_size']\n    config_hf.activation_function = 'gelu'\n    config_hf.resid_pdrop = 0.1\n    config_hf.embd_pdrop = 0.1\n    config_hf.attn_pdrop = 0.1\n    config_hf.layer_norm_epsilon = config_megatron['layernorm_epsilon']\n    config_hf.initializer_range = config_megatron['init_method_std']\n    config_hf.apply_query_key_layer_scaling = config_megatron['apply_query_key_layer_scaling']\n    config_hf.normalize_attention_scores = True\n    config_hf.use_cache = True\n    if config_megatron['hidden_size'] == 4096:\n        config_hf.bos_token_id = 1\n        config_hf.eos_token_id = 1\n        config_hf.pad_token_id = 0\n    else:\n        config_hf.bos_token_id = 2\n        config_hf.eos_token_id = 3\n        config_hf.pad_token_id = 0\n    return config_hf",
            "def copy_config(config_hf, config_megatron):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Copy the config from Megatron to hf.'\n    config_hf.vocab_size = 64000\n    config_hf.n_positions = config_megatron['encoder_seq_length']\n    config_hf.n_embd = config_megatron['hidden_size']\n    config_hf.n_layer = config_megatron['num_layers']\n    config_hf.n_head = config_megatron['num_attention_heads']\n    config_hf.n_inner = config_megatron['ffn_hidden_size']\n    config_hf.activation_function = 'gelu'\n    config_hf.resid_pdrop = 0.1\n    config_hf.embd_pdrop = 0.1\n    config_hf.attn_pdrop = 0.1\n    config_hf.layer_norm_epsilon = config_megatron['layernorm_epsilon']\n    config_hf.initializer_range = config_megatron['init_method_std']\n    config_hf.apply_query_key_layer_scaling = config_megatron['apply_query_key_layer_scaling']\n    config_hf.normalize_attention_scores = True\n    config_hf.use_cache = True\n    if config_megatron['hidden_size'] == 4096:\n        config_hf.bos_token_id = 1\n        config_hf.eos_token_id = 1\n        config_hf.pad_token_id = 0\n    else:\n        config_hf.bos_token_id = 2\n        config_hf.eos_token_id = 3\n        config_hf.pad_token_id = 0\n    return config_hf",
            "def copy_config(config_hf, config_megatron):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Copy the config from Megatron to hf.'\n    config_hf.vocab_size = 64000\n    config_hf.n_positions = config_megatron['encoder_seq_length']\n    config_hf.n_embd = config_megatron['hidden_size']\n    config_hf.n_layer = config_megatron['num_layers']\n    config_hf.n_head = config_megatron['num_attention_heads']\n    config_hf.n_inner = config_megatron['ffn_hidden_size']\n    config_hf.activation_function = 'gelu'\n    config_hf.resid_pdrop = 0.1\n    config_hf.embd_pdrop = 0.1\n    config_hf.attn_pdrop = 0.1\n    config_hf.layer_norm_epsilon = config_megatron['layernorm_epsilon']\n    config_hf.initializer_range = config_megatron['init_method_std']\n    config_hf.apply_query_key_layer_scaling = config_megatron['apply_query_key_layer_scaling']\n    config_hf.normalize_attention_scores = True\n    config_hf.use_cache = True\n    if config_megatron['hidden_size'] == 4096:\n        config_hf.bos_token_id = 1\n        config_hf.eos_token_id = 1\n        config_hf.pad_token_id = 0\n    else:\n        config_hf.bos_token_id = 2\n        config_hf.eos_token_id = 3\n        config_hf.pad_token_id = 0\n    return config_hf"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(args):\n    print(args)\n    checkpoint_path = args.checkpoint_path\n    save_path = args.save_path\n    if isfile(checkpoint_path):\n        raise FileNotFoundError(f'ERROR! could not find file {checkpoint_path}')\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    config_megatron = checkpoint['hyper_parameters']['cfg']\n    config_hf = GPT2Config()\n    config_hf = copy_config(config_hf=config_hf, config_megatron=config_megatron)\n    config_hf.architectures = ['GPT2LMHeadModel']\n    sd_megatron = checkpoint['state_dict']\n    print('Converting')\n    sd_hf = convert_megatron_checkpoint(sd_megatron, config_hf)\n    if args.print_checkpoint_structure:\n        recursive_print(None, sd_hf)\n    config_hf.tokenizer_class = 'GPTSw3Tokenizer'\n    print('Saving config')\n    config_hf.save_pretrained(save_path)\n    output_checkpoint_file = os.path.join(save_path, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(sd_hf, output_checkpoint_file)",
        "mutated": [
            "def main(args):\n    if False:\n        i = 10\n    print(args)\n    checkpoint_path = args.checkpoint_path\n    save_path = args.save_path\n    if isfile(checkpoint_path):\n        raise FileNotFoundError(f'ERROR! could not find file {checkpoint_path}')\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    config_megatron = checkpoint['hyper_parameters']['cfg']\n    config_hf = GPT2Config()\n    config_hf = copy_config(config_hf=config_hf, config_megatron=config_megatron)\n    config_hf.architectures = ['GPT2LMHeadModel']\n    sd_megatron = checkpoint['state_dict']\n    print('Converting')\n    sd_hf = convert_megatron_checkpoint(sd_megatron, config_hf)\n    if args.print_checkpoint_structure:\n        recursive_print(None, sd_hf)\n    config_hf.tokenizer_class = 'GPTSw3Tokenizer'\n    print('Saving config')\n    config_hf.save_pretrained(save_path)\n    output_checkpoint_file = os.path.join(save_path, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(sd_hf, output_checkpoint_file)",
            "def main(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    print(args)\n    checkpoint_path = args.checkpoint_path\n    save_path = args.save_path\n    if isfile(checkpoint_path):\n        raise FileNotFoundError(f'ERROR! could not find file {checkpoint_path}')\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    config_megatron = checkpoint['hyper_parameters']['cfg']\n    config_hf = GPT2Config()\n    config_hf = copy_config(config_hf=config_hf, config_megatron=config_megatron)\n    config_hf.architectures = ['GPT2LMHeadModel']\n    sd_megatron = checkpoint['state_dict']\n    print('Converting')\n    sd_hf = convert_megatron_checkpoint(sd_megatron, config_hf)\n    if args.print_checkpoint_structure:\n        recursive_print(None, sd_hf)\n    config_hf.tokenizer_class = 'GPTSw3Tokenizer'\n    print('Saving config')\n    config_hf.save_pretrained(save_path)\n    output_checkpoint_file = os.path.join(save_path, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(sd_hf, output_checkpoint_file)",
            "def main(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    print(args)\n    checkpoint_path = args.checkpoint_path\n    save_path = args.save_path\n    if isfile(checkpoint_path):\n        raise FileNotFoundError(f'ERROR! could not find file {checkpoint_path}')\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    config_megatron = checkpoint['hyper_parameters']['cfg']\n    config_hf = GPT2Config()\n    config_hf = copy_config(config_hf=config_hf, config_megatron=config_megatron)\n    config_hf.architectures = ['GPT2LMHeadModel']\n    sd_megatron = checkpoint['state_dict']\n    print('Converting')\n    sd_hf = convert_megatron_checkpoint(sd_megatron, config_hf)\n    if args.print_checkpoint_structure:\n        recursive_print(None, sd_hf)\n    config_hf.tokenizer_class = 'GPTSw3Tokenizer'\n    print('Saving config')\n    config_hf.save_pretrained(save_path)\n    output_checkpoint_file = os.path.join(save_path, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(sd_hf, output_checkpoint_file)",
            "def main(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    print(args)\n    checkpoint_path = args.checkpoint_path\n    save_path = args.save_path\n    if isfile(checkpoint_path):\n        raise FileNotFoundError(f'ERROR! could not find file {checkpoint_path}')\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    config_megatron = checkpoint['hyper_parameters']['cfg']\n    config_hf = GPT2Config()\n    config_hf = copy_config(config_hf=config_hf, config_megatron=config_megatron)\n    config_hf.architectures = ['GPT2LMHeadModel']\n    sd_megatron = checkpoint['state_dict']\n    print('Converting')\n    sd_hf = convert_megatron_checkpoint(sd_megatron, config_hf)\n    if args.print_checkpoint_structure:\n        recursive_print(None, sd_hf)\n    config_hf.tokenizer_class = 'GPTSw3Tokenizer'\n    print('Saving config')\n    config_hf.save_pretrained(save_path)\n    output_checkpoint_file = os.path.join(save_path, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(sd_hf, output_checkpoint_file)",
            "def main(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    print(args)\n    checkpoint_path = args.checkpoint_path\n    save_path = args.save_path\n    if isfile(checkpoint_path):\n        raise FileNotFoundError(f'ERROR! could not find file {checkpoint_path}')\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    config_megatron = checkpoint['hyper_parameters']['cfg']\n    config_hf = GPT2Config()\n    config_hf = copy_config(config_hf=config_hf, config_megatron=config_megatron)\n    config_hf.architectures = ['GPT2LMHeadModel']\n    sd_megatron = checkpoint['state_dict']\n    print('Converting')\n    sd_hf = convert_megatron_checkpoint(sd_megatron, config_hf)\n    if args.print_checkpoint_structure:\n        recursive_print(None, sd_hf)\n    config_hf.tokenizer_class = 'GPTSw3Tokenizer'\n    print('Saving config')\n    config_hf.save_pretrained(save_path)\n    output_checkpoint_file = os.path.join(save_path, 'pytorch_model.bin')\n    print(f'Saving checkpoint to \"{output_checkpoint_file}\"')\n    torch.save(sd_hf, output_checkpoint_file)"
        ]
    }
]