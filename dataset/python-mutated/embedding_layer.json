[
    {
        "func_name": "__init__",
        "original": "def __init__(self, vocab_size, hidden_size):\n    \"\"\"Specify characteristic parameters of embedding layer.\n\n    Args:\n      vocab_size: Number of tokens in the embedding. (Typically ~32,000)\n      hidden_size: Dimensionality of the embedding. (Typically 512 or 1024)\n    \"\"\"\n    super(EmbeddingSharedWeights, self).__init__()\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size",
        "mutated": [
            "def __init__(self, vocab_size, hidden_size):\n    if False:\n        i = 10\n    'Specify characteristic parameters of embedding layer.\\n\\n    Args:\\n      vocab_size: Number of tokens in the embedding. (Typically ~32,000)\\n      hidden_size: Dimensionality of the embedding. (Typically 512 or 1024)\\n    '\n    super(EmbeddingSharedWeights, self).__init__()\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size",
            "def __init__(self, vocab_size, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Specify characteristic parameters of embedding layer.\\n\\n    Args:\\n      vocab_size: Number of tokens in the embedding. (Typically ~32,000)\\n      hidden_size: Dimensionality of the embedding. (Typically 512 or 1024)\\n    '\n    super(EmbeddingSharedWeights, self).__init__()\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size",
            "def __init__(self, vocab_size, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Specify characteristic parameters of embedding layer.\\n\\n    Args:\\n      vocab_size: Number of tokens in the embedding. (Typically ~32,000)\\n      hidden_size: Dimensionality of the embedding. (Typically 512 or 1024)\\n    '\n    super(EmbeddingSharedWeights, self).__init__()\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size",
            "def __init__(self, vocab_size, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Specify characteristic parameters of embedding layer.\\n\\n    Args:\\n      vocab_size: Number of tokens in the embedding. (Typically ~32,000)\\n      hidden_size: Dimensionality of the embedding. (Typically 512 or 1024)\\n    '\n    super(EmbeddingSharedWeights, self).__init__()\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size",
            "def __init__(self, vocab_size, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Specify characteristic parameters of embedding layer.\\n\\n    Args:\\n      vocab_size: Number of tokens in the embedding. (Typically ~32,000)\\n      hidden_size: Dimensionality of the embedding. (Typically 512 or 1024)\\n    '\n    super(EmbeddingSharedWeights, self).__init__()\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size"
        ]
    },
    {
        "func_name": "build",
        "original": "def build(self, input_shape):\n    \"\"\"Build embedding layer.\"\"\"\n    with tf.name_scope('embedding_and_softmax'):\n        self.shared_weights = self.add_weight('weights', shape=[self.vocab_size, self.hidden_size], initializer=tf.random_normal_initializer(mean=0.0, stddev=self.hidden_size ** (-0.5)))\n    super(EmbeddingSharedWeights, self).build(input_shape)",
        "mutated": [
            "def build(self, input_shape):\n    if False:\n        i = 10\n    'Build embedding layer.'\n    with tf.name_scope('embedding_and_softmax'):\n        self.shared_weights = self.add_weight('weights', shape=[self.vocab_size, self.hidden_size], initializer=tf.random_normal_initializer(mean=0.0, stddev=self.hidden_size ** (-0.5)))\n    super(EmbeddingSharedWeights, self).build(input_shape)",
            "def build(self, input_shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Build embedding layer.'\n    with tf.name_scope('embedding_and_softmax'):\n        self.shared_weights = self.add_weight('weights', shape=[self.vocab_size, self.hidden_size], initializer=tf.random_normal_initializer(mean=0.0, stddev=self.hidden_size ** (-0.5)))\n    super(EmbeddingSharedWeights, self).build(input_shape)",
            "def build(self, input_shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Build embedding layer.'\n    with tf.name_scope('embedding_and_softmax'):\n        self.shared_weights = self.add_weight('weights', shape=[self.vocab_size, self.hidden_size], initializer=tf.random_normal_initializer(mean=0.0, stddev=self.hidden_size ** (-0.5)))\n    super(EmbeddingSharedWeights, self).build(input_shape)",
            "def build(self, input_shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Build embedding layer.'\n    with tf.name_scope('embedding_and_softmax'):\n        self.shared_weights = self.add_weight('weights', shape=[self.vocab_size, self.hidden_size], initializer=tf.random_normal_initializer(mean=0.0, stddev=self.hidden_size ** (-0.5)))\n    super(EmbeddingSharedWeights, self).build(input_shape)",
            "def build(self, input_shape):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Build embedding layer.'\n    with tf.name_scope('embedding_and_softmax'):\n        self.shared_weights = self.add_weight('weights', shape=[self.vocab_size, self.hidden_size], initializer=tf.random_normal_initializer(mean=0.0, stddev=self.hidden_size ** (-0.5)))\n    super(EmbeddingSharedWeights, self).build(input_shape)"
        ]
    },
    {
        "func_name": "get_config",
        "original": "def get_config(self):\n    return {'vocab_size': self.vocab_size, 'hidden_size': self.hidden_size}",
        "mutated": [
            "def get_config(self):\n    if False:\n        i = 10\n    return {'vocab_size': self.vocab_size, 'hidden_size': self.hidden_size}",
            "def get_config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return {'vocab_size': self.vocab_size, 'hidden_size': self.hidden_size}",
            "def get_config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return {'vocab_size': self.vocab_size, 'hidden_size': self.hidden_size}",
            "def get_config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return {'vocab_size': self.vocab_size, 'hidden_size': self.hidden_size}",
            "def get_config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return {'vocab_size': self.vocab_size, 'hidden_size': self.hidden_size}"
        ]
    },
    {
        "func_name": "call",
        "original": "def call(self, inputs, mode='embedding'):\n    \"\"\"Get token embeddings of inputs.\n\n    Args:\n      inputs: An int64 tensor with shape [batch_size, length]\n      mode: string, a valid value is one of \"embedding\" and \"linear\".\n    Returns:\n      outputs: (1) If mode == \"embedding\", output embedding tensor, float32 with\n        shape [batch_size, length, embedding_size]; (2) mode == \"linear\", output\n        linear tensor, float32 with shape [batch_size, length, vocab_size].\n    Raises:\n      ValueError: if mode is not valid.\n    \"\"\"\n    if mode == 'embedding':\n        return self._embedding(inputs)\n    elif mode == 'linear':\n        return self._linear(inputs)\n    else:\n        raise ValueError('mode {} is not valid.'.format(mode))",
        "mutated": [
            "def call(self, inputs, mode='embedding'):\n    if False:\n        i = 10\n    'Get token embeddings of inputs.\\n\\n    Args:\\n      inputs: An int64 tensor with shape [batch_size, length]\\n      mode: string, a valid value is one of \"embedding\" and \"linear\".\\n    Returns:\\n      outputs: (1) If mode == \"embedding\", output embedding tensor, float32 with\\n        shape [batch_size, length, embedding_size]; (2) mode == \"linear\", output\\n        linear tensor, float32 with shape [batch_size, length, vocab_size].\\n    Raises:\\n      ValueError: if mode is not valid.\\n    '\n    if mode == 'embedding':\n        return self._embedding(inputs)\n    elif mode == 'linear':\n        return self._linear(inputs)\n    else:\n        raise ValueError('mode {} is not valid.'.format(mode))",
            "def call(self, inputs, mode='embedding'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get token embeddings of inputs.\\n\\n    Args:\\n      inputs: An int64 tensor with shape [batch_size, length]\\n      mode: string, a valid value is one of \"embedding\" and \"linear\".\\n    Returns:\\n      outputs: (1) If mode == \"embedding\", output embedding tensor, float32 with\\n        shape [batch_size, length, embedding_size]; (2) mode == \"linear\", output\\n        linear tensor, float32 with shape [batch_size, length, vocab_size].\\n    Raises:\\n      ValueError: if mode is not valid.\\n    '\n    if mode == 'embedding':\n        return self._embedding(inputs)\n    elif mode == 'linear':\n        return self._linear(inputs)\n    else:\n        raise ValueError('mode {} is not valid.'.format(mode))",
            "def call(self, inputs, mode='embedding'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get token embeddings of inputs.\\n\\n    Args:\\n      inputs: An int64 tensor with shape [batch_size, length]\\n      mode: string, a valid value is one of \"embedding\" and \"linear\".\\n    Returns:\\n      outputs: (1) If mode == \"embedding\", output embedding tensor, float32 with\\n        shape [batch_size, length, embedding_size]; (2) mode == \"linear\", output\\n        linear tensor, float32 with shape [batch_size, length, vocab_size].\\n    Raises:\\n      ValueError: if mode is not valid.\\n    '\n    if mode == 'embedding':\n        return self._embedding(inputs)\n    elif mode == 'linear':\n        return self._linear(inputs)\n    else:\n        raise ValueError('mode {} is not valid.'.format(mode))",
            "def call(self, inputs, mode='embedding'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get token embeddings of inputs.\\n\\n    Args:\\n      inputs: An int64 tensor with shape [batch_size, length]\\n      mode: string, a valid value is one of \"embedding\" and \"linear\".\\n    Returns:\\n      outputs: (1) If mode == \"embedding\", output embedding tensor, float32 with\\n        shape [batch_size, length, embedding_size]; (2) mode == \"linear\", output\\n        linear tensor, float32 with shape [batch_size, length, vocab_size].\\n    Raises:\\n      ValueError: if mode is not valid.\\n    '\n    if mode == 'embedding':\n        return self._embedding(inputs)\n    elif mode == 'linear':\n        return self._linear(inputs)\n    else:\n        raise ValueError('mode {} is not valid.'.format(mode))",
            "def call(self, inputs, mode='embedding'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get token embeddings of inputs.\\n\\n    Args:\\n      inputs: An int64 tensor with shape [batch_size, length]\\n      mode: string, a valid value is one of \"embedding\" and \"linear\".\\n    Returns:\\n      outputs: (1) If mode == \"embedding\", output embedding tensor, float32 with\\n        shape [batch_size, length, embedding_size]; (2) mode == \"linear\", output\\n        linear tensor, float32 with shape [batch_size, length, vocab_size].\\n    Raises:\\n      ValueError: if mode is not valid.\\n    '\n    if mode == 'embedding':\n        return self._embedding(inputs)\n    elif mode == 'linear':\n        return self._linear(inputs)\n    else:\n        raise ValueError('mode {} is not valid.'.format(mode))"
        ]
    },
    {
        "func_name": "_embedding",
        "original": "def _embedding(self, inputs):\n    \"\"\"Applies embedding based on inputs tensor.\"\"\"\n    with tf.name_scope('embedding'):\n        embeddings = tf.gather(self.shared_weights, inputs)\n        mask = tf.cast(tf.not_equal(inputs, 0), embeddings.dtype)\n        embeddings *= tf.expand_dims(mask, -1)\n        embeddings *= self.hidden_size ** 0.5\n        return embeddings",
        "mutated": [
            "def _embedding(self, inputs):\n    if False:\n        i = 10\n    'Applies embedding based on inputs tensor.'\n    with tf.name_scope('embedding'):\n        embeddings = tf.gather(self.shared_weights, inputs)\n        mask = tf.cast(tf.not_equal(inputs, 0), embeddings.dtype)\n        embeddings *= tf.expand_dims(mask, -1)\n        embeddings *= self.hidden_size ** 0.5\n        return embeddings",
            "def _embedding(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Applies embedding based on inputs tensor.'\n    with tf.name_scope('embedding'):\n        embeddings = tf.gather(self.shared_weights, inputs)\n        mask = tf.cast(tf.not_equal(inputs, 0), embeddings.dtype)\n        embeddings *= tf.expand_dims(mask, -1)\n        embeddings *= self.hidden_size ** 0.5\n        return embeddings",
            "def _embedding(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Applies embedding based on inputs tensor.'\n    with tf.name_scope('embedding'):\n        embeddings = tf.gather(self.shared_weights, inputs)\n        mask = tf.cast(tf.not_equal(inputs, 0), embeddings.dtype)\n        embeddings *= tf.expand_dims(mask, -1)\n        embeddings *= self.hidden_size ** 0.5\n        return embeddings",
            "def _embedding(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Applies embedding based on inputs tensor.'\n    with tf.name_scope('embedding'):\n        embeddings = tf.gather(self.shared_weights, inputs)\n        mask = tf.cast(tf.not_equal(inputs, 0), embeddings.dtype)\n        embeddings *= tf.expand_dims(mask, -1)\n        embeddings *= self.hidden_size ** 0.5\n        return embeddings",
            "def _embedding(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Applies embedding based on inputs tensor.'\n    with tf.name_scope('embedding'):\n        embeddings = tf.gather(self.shared_weights, inputs)\n        mask = tf.cast(tf.not_equal(inputs, 0), embeddings.dtype)\n        embeddings *= tf.expand_dims(mask, -1)\n        embeddings *= self.hidden_size ** 0.5\n        return embeddings"
        ]
    },
    {
        "func_name": "_linear",
        "original": "def _linear(self, inputs):\n    \"\"\"Computes logits by running inputs through a linear layer.\n\n    Args:\n      inputs: A float32 tensor with shape [batch_size, length, hidden_size]\n    Returns:\n      float32 tensor with shape [batch_size, length, vocab_size].\n    \"\"\"\n    with tf.name_scope('presoftmax_linear'):\n        batch_size = tf.shape(inputs)[0]\n        length = tf.shape(inputs)[1]\n        x = tf.reshape(inputs, [-1, self.hidden_size])\n        logits = tf.matmul(x, self.shared_weights, transpose_b=True)\n        return tf.reshape(logits, [batch_size, length, self.vocab_size])",
        "mutated": [
            "def _linear(self, inputs):\n    if False:\n        i = 10\n    'Computes logits by running inputs through a linear layer.\\n\\n    Args:\\n      inputs: A float32 tensor with shape [batch_size, length, hidden_size]\\n    Returns:\\n      float32 tensor with shape [batch_size, length, vocab_size].\\n    '\n    with tf.name_scope('presoftmax_linear'):\n        batch_size = tf.shape(inputs)[0]\n        length = tf.shape(inputs)[1]\n        x = tf.reshape(inputs, [-1, self.hidden_size])\n        logits = tf.matmul(x, self.shared_weights, transpose_b=True)\n        return tf.reshape(logits, [batch_size, length, self.vocab_size])",
            "def _linear(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes logits by running inputs through a linear layer.\\n\\n    Args:\\n      inputs: A float32 tensor with shape [batch_size, length, hidden_size]\\n    Returns:\\n      float32 tensor with shape [batch_size, length, vocab_size].\\n    '\n    with tf.name_scope('presoftmax_linear'):\n        batch_size = tf.shape(inputs)[0]\n        length = tf.shape(inputs)[1]\n        x = tf.reshape(inputs, [-1, self.hidden_size])\n        logits = tf.matmul(x, self.shared_weights, transpose_b=True)\n        return tf.reshape(logits, [batch_size, length, self.vocab_size])",
            "def _linear(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes logits by running inputs through a linear layer.\\n\\n    Args:\\n      inputs: A float32 tensor with shape [batch_size, length, hidden_size]\\n    Returns:\\n      float32 tensor with shape [batch_size, length, vocab_size].\\n    '\n    with tf.name_scope('presoftmax_linear'):\n        batch_size = tf.shape(inputs)[0]\n        length = tf.shape(inputs)[1]\n        x = tf.reshape(inputs, [-1, self.hidden_size])\n        logits = tf.matmul(x, self.shared_weights, transpose_b=True)\n        return tf.reshape(logits, [batch_size, length, self.vocab_size])",
            "def _linear(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes logits by running inputs through a linear layer.\\n\\n    Args:\\n      inputs: A float32 tensor with shape [batch_size, length, hidden_size]\\n    Returns:\\n      float32 tensor with shape [batch_size, length, vocab_size].\\n    '\n    with tf.name_scope('presoftmax_linear'):\n        batch_size = tf.shape(inputs)[0]\n        length = tf.shape(inputs)[1]\n        x = tf.reshape(inputs, [-1, self.hidden_size])\n        logits = tf.matmul(x, self.shared_weights, transpose_b=True)\n        return tf.reshape(logits, [batch_size, length, self.vocab_size])",
            "def _linear(self, inputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes logits by running inputs through a linear layer.\\n\\n    Args:\\n      inputs: A float32 tensor with shape [batch_size, length, hidden_size]\\n    Returns:\\n      float32 tensor with shape [batch_size, length, vocab_size].\\n    '\n    with tf.name_scope('presoftmax_linear'):\n        batch_size = tf.shape(inputs)[0]\n        length = tf.shape(inputs)[1]\n        x = tf.reshape(inputs, [-1, self.hidden_size])\n        logits = tf.matmul(x, self.shared_weights, transpose_b=True)\n        return tf.reshape(logits, [batch_size, length, self.vocab_size])"
        ]
    }
]