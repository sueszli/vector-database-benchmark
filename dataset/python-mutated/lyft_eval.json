[
    {
        "func_name": "load_lyft_gts",
        "original": "def load_lyft_gts(lyft, data_root, eval_split, logger=None):\n    \"\"\"Loads ground truth boxes from database.\n\n    Args:\n        lyft (:obj:`LyftDataset`): Lyft class in the sdk.\n        data_root (str): Root of data for reading splits.\n        eval_split (str): Name of the split for evaluation.\n        logger (logging.Logger | str, optional): Logger used for printing\n        related information during evaluation. Default: None.\n\n    Returns:\n        list[dict]: List of annotation dictionaries.\n    \"\"\"\n    split_scenes = mmcv.list_from_file(osp.join(data_root, f'{eval_split}.txt'))\n    sample_tokens_all = [s['token'] for s in lyft.sample]\n    assert len(sample_tokens_all) > 0, 'Error: Database has no samples!'\n    if eval_split == 'test':\n        assert len(lyft.sample_annotation) > 0, 'Error: You are trying to evaluate on the test set              but you do not have the annotations!'\n    sample_tokens = []\n    for sample_token in sample_tokens_all:\n        scene_token = lyft.get('sample', sample_token)['scene_token']\n        scene_record = lyft.get('scene', scene_token)\n        if scene_record['name'] in split_scenes:\n            sample_tokens.append(sample_token)\n    all_annotations = []\n    print_log('Loading ground truth annotations...', logger=logger)\n    for sample_token in mmcv.track_iter_progress(sample_tokens):\n        sample = lyft.get('sample', sample_token)\n        sample_annotation_tokens = sample['anns']\n        for sample_annotation_token in sample_annotation_tokens:\n            sample_annotation = lyft.get('sample_annotation', sample_annotation_token)\n            detection_name = sample_annotation['category_name']\n            if detection_name is None:\n                continue\n            annotation = {'sample_token': sample_token, 'translation': sample_annotation['translation'], 'size': sample_annotation['size'], 'rotation': sample_annotation['rotation'], 'name': detection_name}\n            all_annotations.append(annotation)\n    return all_annotations",
        "mutated": [
            "def load_lyft_gts(lyft, data_root, eval_split, logger=None):\n    if False:\n        i = 10\n    'Loads ground truth boxes from database.\\n\\n    Args:\\n        lyft (:obj:`LyftDataset`): Lyft class in the sdk.\\n        data_root (str): Root of data for reading splits.\\n        eval_split (str): Name of the split for evaluation.\\n        logger (logging.Logger | str, optional): Logger used for printing\\n        related information during evaluation. Default: None.\\n\\n    Returns:\\n        list[dict]: List of annotation dictionaries.\\n    '\n    split_scenes = mmcv.list_from_file(osp.join(data_root, f'{eval_split}.txt'))\n    sample_tokens_all = [s['token'] for s in lyft.sample]\n    assert len(sample_tokens_all) > 0, 'Error: Database has no samples!'\n    if eval_split == 'test':\n        assert len(lyft.sample_annotation) > 0, 'Error: You are trying to evaluate on the test set              but you do not have the annotations!'\n    sample_tokens = []\n    for sample_token in sample_tokens_all:\n        scene_token = lyft.get('sample', sample_token)['scene_token']\n        scene_record = lyft.get('scene', scene_token)\n        if scene_record['name'] in split_scenes:\n            sample_tokens.append(sample_token)\n    all_annotations = []\n    print_log('Loading ground truth annotations...', logger=logger)\n    for sample_token in mmcv.track_iter_progress(sample_tokens):\n        sample = lyft.get('sample', sample_token)\n        sample_annotation_tokens = sample['anns']\n        for sample_annotation_token in sample_annotation_tokens:\n            sample_annotation = lyft.get('sample_annotation', sample_annotation_token)\n            detection_name = sample_annotation['category_name']\n            if detection_name is None:\n                continue\n            annotation = {'sample_token': sample_token, 'translation': sample_annotation['translation'], 'size': sample_annotation['size'], 'rotation': sample_annotation['rotation'], 'name': detection_name}\n            all_annotations.append(annotation)\n    return all_annotations",
            "def load_lyft_gts(lyft, data_root, eval_split, logger=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Loads ground truth boxes from database.\\n\\n    Args:\\n        lyft (:obj:`LyftDataset`): Lyft class in the sdk.\\n        data_root (str): Root of data for reading splits.\\n        eval_split (str): Name of the split for evaluation.\\n        logger (logging.Logger | str, optional): Logger used for printing\\n        related information during evaluation. Default: None.\\n\\n    Returns:\\n        list[dict]: List of annotation dictionaries.\\n    '\n    split_scenes = mmcv.list_from_file(osp.join(data_root, f'{eval_split}.txt'))\n    sample_tokens_all = [s['token'] for s in lyft.sample]\n    assert len(sample_tokens_all) > 0, 'Error: Database has no samples!'\n    if eval_split == 'test':\n        assert len(lyft.sample_annotation) > 0, 'Error: You are trying to evaluate on the test set              but you do not have the annotations!'\n    sample_tokens = []\n    for sample_token in sample_tokens_all:\n        scene_token = lyft.get('sample', sample_token)['scene_token']\n        scene_record = lyft.get('scene', scene_token)\n        if scene_record['name'] in split_scenes:\n            sample_tokens.append(sample_token)\n    all_annotations = []\n    print_log('Loading ground truth annotations...', logger=logger)\n    for sample_token in mmcv.track_iter_progress(sample_tokens):\n        sample = lyft.get('sample', sample_token)\n        sample_annotation_tokens = sample['anns']\n        for sample_annotation_token in sample_annotation_tokens:\n            sample_annotation = lyft.get('sample_annotation', sample_annotation_token)\n            detection_name = sample_annotation['category_name']\n            if detection_name is None:\n                continue\n            annotation = {'sample_token': sample_token, 'translation': sample_annotation['translation'], 'size': sample_annotation['size'], 'rotation': sample_annotation['rotation'], 'name': detection_name}\n            all_annotations.append(annotation)\n    return all_annotations",
            "def load_lyft_gts(lyft, data_root, eval_split, logger=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Loads ground truth boxes from database.\\n\\n    Args:\\n        lyft (:obj:`LyftDataset`): Lyft class in the sdk.\\n        data_root (str): Root of data for reading splits.\\n        eval_split (str): Name of the split for evaluation.\\n        logger (logging.Logger | str, optional): Logger used for printing\\n        related information during evaluation. Default: None.\\n\\n    Returns:\\n        list[dict]: List of annotation dictionaries.\\n    '\n    split_scenes = mmcv.list_from_file(osp.join(data_root, f'{eval_split}.txt'))\n    sample_tokens_all = [s['token'] for s in lyft.sample]\n    assert len(sample_tokens_all) > 0, 'Error: Database has no samples!'\n    if eval_split == 'test':\n        assert len(lyft.sample_annotation) > 0, 'Error: You are trying to evaluate on the test set              but you do not have the annotations!'\n    sample_tokens = []\n    for sample_token in sample_tokens_all:\n        scene_token = lyft.get('sample', sample_token)['scene_token']\n        scene_record = lyft.get('scene', scene_token)\n        if scene_record['name'] in split_scenes:\n            sample_tokens.append(sample_token)\n    all_annotations = []\n    print_log('Loading ground truth annotations...', logger=logger)\n    for sample_token in mmcv.track_iter_progress(sample_tokens):\n        sample = lyft.get('sample', sample_token)\n        sample_annotation_tokens = sample['anns']\n        for sample_annotation_token in sample_annotation_tokens:\n            sample_annotation = lyft.get('sample_annotation', sample_annotation_token)\n            detection_name = sample_annotation['category_name']\n            if detection_name is None:\n                continue\n            annotation = {'sample_token': sample_token, 'translation': sample_annotation['translation'], 'size': sample_annotation['size'], 'rotation': sample_annotation['rotation'], 'name': detection_name}\n            all_annotations.append(annotation)\n    return all_annotations",
            "def load_lyft_gts(lyft, data_root, eval_split, logger=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Loads ground truth boxes from database.\\n\\n    Args:\\n        lyft (:obj:`LyftDataset`): Lyft class in the sdk.\\n        data_root (str): Root of data for reading splits.\\n        eval_split (str): Name of the split for evaluation.\\n        logger (logging.Logger | str, optional): Logger used for printing\\n        related information during evaluation. Default: None.\\n\\n    Returns:\\n        list[dict]: List of annotation dictionaries.\\n    '\n    split_scenes = mmcv.list_from_file(osp.join(data_root, f'{eval_split}.txt'))\n    sample_tokens_all = [s['token'] for s in lyft.sample]\n    assert len(sample_tokens_all) > 0, 'Error: Database has no samples!'\n    if eval_split == 'test':\n        assert len(lyft.sample_annotation) > 0, 'Error: You are trying to evaluate on the test set              but you do not have the annotations!'\n    sample_tokens = []\n    for sample_token in sample_tokens_all:\n        scene_token = lyft.get('sample', sample_token)['scene_token']\n        scene_record = lyft.get('scene', scene_token)\n        if scene_record['name'] in split_scenes:\n            sample_tokens.append(sample_token)\n    all_annotations = []\n    print_log('Loading ground truth annotations...', logger=logger)\n    for sample_token in mmcv.track_iter_progress(sample_tokens):\n        sample = lyft.get('sample', sample_token)\n        sample_annotation_tokens = sample['anns']\n        for sample_annotation_token in sample_annotation_tokens:\n            sample_annotation = lyft.get('sample_annotation', sample_annotation_token)\n            detection_name = sample_annotation['category_name']\n            if detection_name is None:\n                continue\n            annotation = {'sample_token': sample_token, 'translation': sample_annotation['translation'], 'size': sample_annotation['size'], 'rotation': sample_annotation['rotation'], 'name': detection_name}\n            all_annotations.append(annotation)\n    return all_annotations",
            "def load_lyft_gts(lyft, data_root, eval_split, logger=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Loads ground truth boxes from database.\\n\\n    Args:\\n        lyft (:obj:`LyftDataset`): Lyft class in the sdk.\\n        data_root (str): Root of data for reading splits.\\n        eval_split (str): Name of the split for evaluation.\\n        logger (logging.Logger | str, optional): Logger used for printing\\n        related information during evaluation. Default: None.\\n\\n    Returns:\\n        list[dict]: List of annotation dictionaries.\\n    '\n    split_scenes = mmcv.list_from_file(osp.join(data_root, f'{eval_split}.txt'))\n    sample_tokens_all = [s['token'] for s in lyft.sample]\n    assert len(sample_tokens_all) > 0, 'Error: Database has no samples!'\n    if eval_split == 'test':\n        assert len(lyft.sample_annotation) > 0, 'Error: You are trying to evaluate on the test set              but you do not have the annotations!'\n    sample_tokens = []\n    for sample_token in sample_tokens_all:\n        scene_token = lyft.get('sample', sample_token)['scene_token']\n        scene_record = lyft.get('scene', scene_token)\n        if scene_record['name'] in split_scenes:\n            sample_tokens.append(sample_token)\n    all_annotations = []\n    print_log('Loading ground truth annotations...', logger=logger)\n    for sample_token in mmcv.track_iter_progress(sample_tokens):\n        sample = lyft.get('sample', sample_token)\n        sample_annotation_tokens = sample['anns']\n        for sample_annotation_token in sample_annotation_tokens:\n            sample_annotation = lyft.get('sample_annotation', sample_annotation_token)\n            detection_name = sample_annotation['category_name']\n            if detection_name is None:\n                continue\n            annotation = {'sample_token': sample_token, 'translation': sample_annotation['translation'], 'size': sample_annotation['size'], 'rotation': sample_annotation['rotation'], 'name': detection_name}\n            all_annotations.append(annotation)\n    return all_annotations"
        ]
    },
    {
        "func_name": "load_lyft_predictions",
        "original": "def load_lyft_predictions(res_path):\n    \"\"\"Load Lyft predictions from json file.\n\n    Args:\n        res_path (str): Path of result json file recording detections.\n\n    Returns:\n        list[dict]: List of prediction dictionaries.\n    \"\"\"\n    predictions = mmcv.load(res_path)\n    predictions = predictions['results']\n    all_preds = []\n    for sample_token in predictions.keys():\n        all_preds.extend(predictions[sample_token])\n    return all_preds",
        "mutated": [
            "def load_lyft_predictions(res_path):\n    if False:\n        i = 10\n    'Load Lyft predictions from json file.\\n\\n    Args:\\n        res_path (str): Path of result json file recording detections.\\n\\n    Returns:\\n        list[dict]: List of prediction dictionaries.\\n    '\n    predictions = mmcv.load(res_path)\n    predictions = predictions['results']\n    all_preds = []\n    for sample_token in predictions.keys():\n        all_preds.extend(predictions[sample_token])\n    return all_preds",
            "def load_lyft_predictions(res_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Load Lyft predictions from json file.\\n\\n    Args:\\n        res_path (str): Path of result json file recording detections.\\n\\n    Returns:\\n        list[dict]: List of prediction dictionaries.\\n    '\n    predictions = mmcv.load(res_path)\n    predictions = predictions['results']\n    all_preds = []\n    for sample_token in predictions.keys():\n        all_preds.extend(predictions[sample_token])\n    return all_preds",
            "def load_lyft_predictions(res_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Load Lyft predictions from json file.\\n\\n    Args:\\n        res_path (str): Path of result json file recording detections.\\n\\n    Returns:\\n        list[dict]: List of prediction dictionaries.\\n    '\n    predictions = mmcv.load(res_path)\n    predictions = predictions['results']\n    all_preds = []\n    for sample_token in predictions.keys():\n        all_preds.extend(predictions[sample_token])\n    return all_preds",
            "def load_lyft_predictions(res_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Load Lyft predictions from json file.\\n\\n    Args:\\n        res_path (str): Path of result json file recording detections.\\n\\n    Returns:\\n        list[dict]: List of prediction dictionaries.\\n    '\n    predictions = mmcv.load(res_path)\n    predictions = predictions['results']\n    all_preds = []\n    for sample_token in predictions.keys():\n        all_preds.extend(predictions[sample_token])\n    return all_preds",
            "def load_lyft_predictions(res_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Load Lyft predictions from json file.\\n\\n    Args:\\n        res_path (str): Path of result json file recording detections.\\n\\n    Returns:\\n        list[dict]: List of prediction dictionaries.\\n    '\n    predictions = mmcv.load(res_path)\n    predictions = predictions['results']\n    all_preds = []\n    for sample_token in predictions.keys():\n        all_preds.extend(predictions[sample_token])\n    return all_preds"
        ]
    },
    {
        "func_name": "lyft_eval",
        "original": "def lyft_eval(lyft, data_root, res_path, eval_set, output_dir, logger=None):\n    \"\"\"Evaluation API for Lyft dataset.\n\n    Args:\n        lyft (:obj:`LyftDataset`): Lyft class in the sdk.\n        data_root (str): Root of data for reading splits.\n        res_path (str): Path of result json file recording detections.\n        eval_set (str): Name of the split for evaluation.\n        output_dir (str): Output directory for output json files.\n        logger (logging.Logger | str, optional): Logger used for printing\n                related information during evaluation. Default: None.\n\n    Returns:\n        dict[str, float]: The evaluation results.\n    \"\"\"\n    gts = load_lyft_gts(lyft, data_root, eval_set, logger)\n    predictions = load_lyft_predictions(res_path)\n    class_names = get_class_names(gts)\n    print('Calculating mAP@0.5:0.95...')\n    iou_thresholds = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n    metrics = {}\n    average_precisions = get_classwise_aps(gts, predictions, class_names, iou_thresholds)\n    APs_data = [['IOU', 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]]\n    mAPs = np.mean(average_precisions, axis=0)\n    mAPs_cate = np.mean(average_precisions, axis=1)\n    final_mAP = np.mean(mAPs)\n    metrics['average_precisions'] = average_precisions.tolist()\n    metrics['mAPs'] = mAPs.tolist()\n    metrics['Final mAP'] = float(final_mAP)\n    metrics['class_names'] = class_names\n    metrics['mAPs_cate'] = mAPs_cate.tolist()\n    APs_data = [['class', 'mAP@0.5:0.95']]\n    for i in range(len(class_names)):\n        row = [class_names[i], round(mAPs_cate[i], 3)]\n        APs_data.append(row)\n    APs_data.append(['Overall', round(final_mAP, 3)])\n    APs_table = AsciiTable(APs_data, title='mAPs@0.5:0.95')\n    APs_table.inner_footing_row_border = True\n    print_log(APs_table.table, logger=logger)\n    res_path = osp.join(output_dir, 'lyft_metrics.json')\n    mmcv.dump(metrics, res_path)\n    return metrics",
        "mutated": [
            "def lyft_eval(lyft, data_root, res_path, eval_set, output_dir, logger=None):\n    if False:\n        i = 10\n    'Evaluation API for Lyft dataset.\\n\\n    Args:\\n        lyft (:obj:`LyftDataset`): Lyft class in the sdk.\\n        data_root (str): Root of data for reading splits.\\n        res_path (str): Path of result json file recording detections.\\n        eval_set (str): Name of the split for evaluation.\\n        output_dir (str): Output directory for output json files.\\n        logger (logging.Logger | str, optional): Logger used for printing\\n                related information during evaluation. Default: None.\\n\\n    Returns:\\n        dict[str, float]: The evaluation results.\\n    '\n    gts = load_lyft_gts(lyft, data_root, eval_set, logger)\n    predictions = load_lyft_predictions(res_path)\n    class_names = get_class_names(gts)\n    print('Calculating mAP@0.5:0.95...')\n    iou_thresholds = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n    metrics = {}\n    average_precisions = get_classwise_aps(gts, predictions, class_names, iou_thresholds)\n    APs_data = [['IOU', 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]]\n    mAPs = np.mean(average_precisions, axis=0)\n    mAPs_cate = np.mean(average_precisions, axis=1)\n    final_mAP = np.mean(mAPs)\n    metrics['average_precisions'] = average_precisions.tolist()\n    metrics['mAPs'] = mAPs.tolist()\n    metrics['Final mAP'] = float(final_mAP)\n    metrics['class_names'] = class_names\n    metrics['mAPs_cate'] = mAPs_cate.tolist()\n    APs_data = [['class', 'mAP@0.5:0.95']]\n    for i in range(len(class_names)):\n        row = [class_names[i], round(mAPs_cate[i], 3)]\n        APs_data.append(row)\n    APs_data.append(['Overall', round(final_mAP, 3)])\n    APs_table = AsciiTable(APs_data, title='mAPs@0.5:0.95')\n    APs_table.inner_footing_row_border = True\n    print_log(APs_table.table, logger=logger)\n    res_path = osp.join(output_dir, 'lyft_metrics.json')\n    mmcv.dump(metrics, res_path)\n    return metrics",
            "def lyft_eval(lyft, data_root, res_path, eval_set, output_dir, logger=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Evaluation API for Lyft dataset.\\n\\n    Args:\\n        lyft (:obj:`LyftDataset`): Lyft class in the sdk.\\n        data_root (str): Root of data for reading splits.\\n        res_path (str): Path of result json file recording detections.\\n        eval_set (str): Name of the split for evaluation.\\n        output_dir (str): Output directory for output json files.\\n        logger (logging.Logger | str, optional): Logger used for printing\\n                related information during evaluation. Default: None.\\n\\n    Returns:\\n        dict[str, float]: The evaluation results.\\n    '\n    gts = load_lyft_gts(lyft, data_root, eval_set, logger)\n    predictions = load_lyft_predictions(res_path)\n    class_names = get_class_names(gts)\n    print('Calculating mAP@0.5:0.95...')\n    iou_thresholds = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n    metrics = {}\n    average_precisions = get_classwise_aps(gts, predictions, class_names, iou_thresholds)\n    APs_data = [['IOU', 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]]\n    mAPs = np.mean(average_precisions, axis=0)\n    mAPs_cate = np.mean(average_precisions, axis=1)\n    final_mAP = np.mean(mAPs)\n    metrics['average_precisions'] = average_precisions.tolist()\n    metrics['mAPs'] = mAPs.tolist()\n    metrics['Final mAP'] = float(final_mAP)\n    metrics['class_names'] = class_names\n    metrics['mAPs_cate'] = mAPs_cate.tolist()\n    APs_data = [['class', 'mAP@0.5:0.95']]\n    for i in range(len(class_names)):\n        row = [class_names[i], round(mAPs_cate[i], 3)]\n        APs_data.append(row)\n    APs_data.append(['Overall', round(final_mAP, 3)])\n    APs_table = AsciiTable(APs_data, title='mAPs@0.5:0.95')\n    APs_table.inner_footing_row_border = True\n    print_log(APs_table.table, logger=logger)\n    res_path = osp.join(output_dir, 'lyft_metrics.json')\n    mmcv.dump(metrics, res_path)\n    return metrics",
            "def lyft_eval(lyft, data_root, res_path, eval_set, output_dir, logger=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Evaluation API for Lyft dataset.\\n\\n    Args:\\n        lyft (:obj:`LyftDataset`): Lyft class in the sdk.\\n        data_root (str): Root of data for reading splits.\\n        res_path (str): Path of result json file recording detections.\\n        eval_set (str): Name of the split for evaluation.\\n        output_dir (str): Output directory for output json files.\\n        logger (logging.Logger | str, optional): Logger used for printing\\n                related information during evaluation. Default: None.\\n\\n    Returns:\\n        dict[str, float]: The evaluation results.\\n    '\n    gts = load_lyft_gts(lyft, data_root, eval_set, logger)\n    predictions = load_lyft_predictions(res_path)\n    class_names = get_class_names(gts)\n    print('Calculating mAP@0.5:0.95...')\n    iou_thresholds = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n    metrics = {}\n    average_precisions = get_classwise_aps(gts, predictions, class_names, iou_thresholds)\n    APs_data = [['IOU', 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]]\n    mAPs = np.mean(average_precisions, axis=0)\n    mAPs_cate = np.mean(average_precisions, axis=1)\n    final_mAP = np.mean(mAPs)\n    metrics['average_precisions'] = average_precisions.tolist()\n    metrics['mAPs'] = mAPs.tolist()\n    metrics['Final mAP'] = float(final_mAP)\n    metrics['class_names'] = class_names\n    metrics['mAPs_cate'] = mAPs_cate.tolist()\n    APs_data = [['class', 'mAP@0.5:0.95']]\n    for i in range(len(class_names)):\n        row = [class_names[i], round(mAPs_cate[i], 3)]\n        APs_data.append(row)\n    APs_data.append(['Overall', round(final_mAP, 3)])\n    APs_table = AsciiTable(APs_data, title='mAPs@0.5:0.95')\n    APs_table.inner_footing_row_border = True\n    print_log(APs_table.table, logger=logger)\n    res_path = osp.join(output_dir, 'lyft_metrics.json')\n    mmcv.dump(metrics, res_path)\n    return metrics",
            "def lyft_eval(lyft, data_root, res_path, eval_set, output_dir, logger=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Evaluation API for Lyft dataset.\\n\\n    Args:\\n        lyft (:obj:`LyftDataset`): Lyft class in the sdk.\\n        data_root (str): Root of data for reading splits.\\n        res_path (str): Path of result json file recording detections.\\n        eval_set (str): Name of the split for evaluation.\\n        output_dir (str): Output directory for output json files.\\n        logger (logging.Logger | str, optional): Logger used for printing\\n                related information during evaluation. Default: None.\\n\\n    Returns:\\n        dict[str, float]: The evaluation results.\\n    '\n    gts = load_lyft_gts(lyft, data_root, eval_set, logger)\n    predictions = load_lyft_predictions(res_path)\n    class_names = get_class_names(gts)\n    print('Calculating mAP@0.5:0.95...')\n    iou_thresholds = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n    metrics = {}\n    average_precisions = get_classwise_aps(gts, predictions, class_names, iou_thresholds)\n    APs_data = [['IOU', 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]]\n    mAPs = np.mean(average_precisions, axis=0)\n    mAPs_cate = np.mean(average_precisions, axis=1)\n    final_mAP = np.mean(mAPs)\n    metrics['average_precisions'] = average_precisions.tolist()\n    metrics['mAPs'] = mAPs.tolist()\n    metrics['Final mAP'] = float(final_mAP)\n    metrics['class_names'] = class_names\n    metrics['mAPs_cate'] = mAPs_cate.tolist()\n    APs_data = [['class', 'mAP@0.5:0.95']]\n    for i in range(len(class_names)):\n        row = [class_names[i], round(mAPs_cate[i], 3)]\n        APs_data.append(row)\n    APs_data.append(['Overall', round(final_mAP, 3)])\n    APs_table = AsciiTable(APs_data, title='mAPs@0.5:0.95')\n    APs_table.inner_footing_row_border = True\n    print_log(APs_table.table, logger=logger)\n    res_path = osp.join(output_dir, 'lyft_metrics.json')\n    mmcv.dump(metrics, res_path)\n    return metrics",
            "def lyft_eval(lyft, data_root, res_path, eval_set, output_dir, logger=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Evaluation API for Lyft dataset.\\n\\n    Args:\\n        lyft (:obj:`LyftDataset`): Lyft class in the sdk.\\n        data_root (str): Root of data for reading splits.\\n        res_path (str): Path of result json file recording detections.\\n        eval_set (str): Name of the split for evaluation.\\n        output_dir (str): Output directory for output json files.\\n        logger (logging.Logger | str, optional): Logger used for printing\\n                related information during evaluation. Default: None.\\n\\n    Returns:\\n        dict[str, float]: The evaluation results.\\n    '\n    gts = load_lyft_gts(lyft, data_root, eval_set, logger)\n    predictions = load_lyft_predictions(res_path)\n    class_names = get_class_names(gts)\n    print('Calculating mAP@0.5:0.95...')\n    iou_thresholds = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n    metrics = {}\n    average_precisions = get_classwise_aps(gts, predictions, class_names, iou_thresholds)\n    APs_data = [['IOU', 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]]\n    mAPs = np.mean(average_precisions, axis=0)\n    mAPs_cate = np.mean(average_precisions, axis=1)\n    final_mAP = np.mean(mAPs)\n    metrics['average_precisions'] = average_precisions.tolist()\n    metrics['mAPs'] = mAPs.tolist()\n    metrics['Final mAP'] = float(final_mAP)\n    metrics['class_names'] = class_names\n    metrics['mAPs_cate'] = mAPs_cate.tolist()\n    APs_data = [['class', 'mAP@0.5:0.95']]\n    for i in range(len(class_names)):\n        row = [class_names[i], round(mAPs_cate[i], 3)]\n        APs_data.append(row)\n    APs_data.append(['Overall', round(final_mAP, 3)])\n    APs_table = AsciiTable(APs_data, title='mAPs@0.5:0.95')\n    APs_table.inner_footing_row_border = True\n    print_log(APs_table.table, logger=logger)\n    res_path = osp.join(output_dir, 'lyft_metrics.json')\n    mmcv.dump(metrics, res_path)\n    return metrics"
        ]
    },
    {
        "func_name": "get_classwise_aps",
        "original": "def get_classwise_aps(gt, predictions, class_names, iou_thresholds):\n    \"\"\"Returns an array with an average precision per class.\n\n    Note: Ground truth and predictions should have the following format.\n\n    .. code-block::\n\n    gt = [{\n        'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207\n                         fbb039a550991a5149214f98cec136ac',\n        'translation': [974.2811881299899, 1714.6815014457964,\n                        -23.689857123368846],\n        'size': [1.796, 4.488, 1.664],\n        'rotation': [0.14882026466054782, 0, 0, 0.9888642620837121],\n        'name': 'car'\n    }]\n\n    predictions = [{\n        'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207\n                         fbb039a550991a5149214f98cec136ac',\n        'translation': [971.8343488872263, 1713.6816097857359,\n                        -25.82534357061308],\n        'size': [2.519726579986132, 7.810161372666739, 3.483438286096803],\n        'rotation': [0.10913582721095375, 0.04099572636992043,\n                     0.01927712319721745, 1.029328402625659],\n        'name': 'car',\n        'score': 0.3077029437237213\n    }]\n\n    Args:\n        gt (list[dict]): list of dictionaries in the format described below.\n        predictions (list[dict]): list of dictionaries in the format\n            described below.\n        class_names (list[str]): list of the class names.\n        iou_thresholds (list[float]): IOU thresholds used to calculate\n            TP / FN\n\n    Returns:\n        np.ndarray: an array with an average precision per class.\n    \"\"\"\n    assert all([0 <= iou_th <= 1 for iou_th in iou_thresholds])\n    gt_by_class_name = group_by_key(gt, 'name')\n    pred_by_class_name = group_by_key(predictions, 'name')\n    average_precisions = np.zeros((len(class_names), len(iou_thresholds)))\n    for (class_id, class_name) in enumerate(class_names):\n        if class_name in pred_by_class_name:\n            (recalls, precisions, average_precision) = get_single_class_aps(gt_by_class_name[class_name], pred_by_class_name[class_name], iou_thresholds)\n            average_precisions[class_id, :] = average_precision\n    return average_precisions",
        "mutated": [
            "def get_classwise_aps(gt, predictions, class_names, iou_thresholds):\n    if False:\n        i = 10\n    \"Returns an array with an average precision per class.\\n\\n    Note: Ground truth and predictions should have the following format.\\n\\n    .. code-block::\\n\\n    gt = [{\\n        'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207\\n                         fbb039a550991a5149214f98cec136ac',\\n        'translation': [974.2811881299899, 1714.6815014457964,\\n                        -23.689857123368846],\\n        'size': [1.796, 4.488, 1.664],\\n        'rotation': [0.14882026466054782, 0, 0, 0.9888642620837121],\\n        'name': 'car'\\n    }]\\n\\n    predictions = [{\\n        'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207\\n                         fbb039a550991a5149214f98cec136ac',\\n        'translation': [971.8343488872263, 1713.6816097857359,\\n                        -25.82534357061308],\\n        'size': [2.519726579986132, 7.810161372666739, 3.483438286096803],\\n        'rotation': [0.10913582721095375, 0.04099572636992043,\\n                     0.01927712319721745, 1.029328402625659],\\n        'name': 'car',\\n        'score': 0.3077029437237213\\n    }]\\n\\n    Args:\\n        gt (list[dict]): list of dictionaries in the format described below.\\n        predictions (list[dict]): list of dictionaries in the format\\n            described below.\\n        class_names (list[str]): list of the class names.\\n        iou_thresholds (list[float]): IOU thresholds used to calculate\\n            TP / FN\\n\\n    Returns:\\n        np.ndarray: an array with an average precision per class.\\n    \"\n    assert all([0 <= iou_th <= 1 for iou_th in iou_thresholds])\n    gt_by_class_name = group_by_key(gt, 'name')\n    pred_by_class_name = group_by_key(predictions, 'name')\n    average_precisions = np.zeros((len(class_names), len(iou_thresholds)))\n    for (class_id, class_name) in enumerate(class_names):\n        if class_name in pred_by_class_name:\n            (recalls, precisions, average_precision) = get_single_class_aps(gt_by_class_name[class_name], pred_by_class_name[class_name], iou_thresholds)\n            average_precisions[class_id, :] = average_precision\n    return average_precisions",
            "def get_classwise_aps(gt, predictions, class_names, iou_thresholds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Returns an array with an average precision per class.\\n\\n    Note: Ground truth and predictions should have the following format.\\n\\n    .. code-block::\\n\\n    gt = [{\\n        'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207\\n                         fbb039a550991a5149214f98cec136ac',\\n        'translation': [974.2811881299899, 1714.6815014457964,\\n                        -23.689857123368846],\\n        'size': [1.796, 4.488, 1.664],\\n        'rotation': [0.14882026466054782, 0, 0, 0.9888642620837121],\\n        'name': 'car'\\n    }]\\n\\n    predictions = [{\\n        'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207\\n                         fbb039a550991a5149214f98cec136ac',\\n        'translation': [971.8343488872263, 1713.6816097857359,\\n                        -25.82534357061308],\\n        'size': [2.519726579986132, 7.810161372666739, 3.483438286096803],\\n        'rotation': [0.10913582721095375, 0.04099572636992043,\\n                     0.01927712319721745, 1.029328402625659],\\n        'name': 'car',\\n        'score': 0.3077029437237213\\n    }]\\n\\n    Args:\\n        gt (list[dict]): list of dictionaries in the format described below.\\n        predictions (list[dict]): list of dictionaries in the format\\n            described below.\\n        class_names (list[str]): list of the class names.\\n        iou_thresholds (list[float]): IOU thresholds used to calculate\\n            TP / FN\\n\\n    Returns:\\n        np.ndarray: an array with an average precision per class.\\n    \"\n    assert all([0 <= iou_th <= 1 for iou_th in iou_thresholds])\n    gt_by_class_name = group_by_key(gt, 'name')\n    pred_by_class_name = group_by_key(predictions, 'name')\n    average_precisions = np.zeros((len(class_names), len(iou_thresholds)))\n    for (class_id, class_name) in enumerate(class_names):\n        if class_name in pred_by_class_name:\n            (recalls, precisions, average_precision) = get_single_class_aps(gt_by_class_name[class_name], pred_by_class_name[class_name], iou_thresholds)\n            average_precisions[class_id, :] = average_precision\n    return average_precisions",
            "def get_classwise_aps(gt, predictions, class_names, iou_thresholds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Returns an array with an average precision per class.\\n\\n    Note: Ground truth and predictions should have the following format.\\n\\n    .. code-block::\\n\\n    gt = [{\\n        'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207\\n                         fbb039a550991a5149214f98cec136ac',\\n        'translation': [974.2811881299899, 1714.6815014457964,\\n                        -23.689857123368846],\\n        'size': [1.796, 4.488, 1.664],\\n        'rotation': [0.14882026466054782, 0, 0, 0.9888642620837121],\\n        'name': 'car'\\n    }]\\n\\n    predictions = [{\\n        'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207\\n                         fbb039a550991a5149214f98cec136ac',\\n        'translation': [971.8343488872263, 1713.6816097857359,\\n                        -25.82534357061308],\\n        'size': [2.519726579986132, 7.810161372666739, 3.483438286096803],\\n        'rotation': [0.10913582721095375, 0.04099572636992043,\\n                     0.01927712319721745, 1.029328402625659],\\n        'name': 'car',\\n        'score': 0.3077029437237213\\n    }]\\n\\n    Args:\\n        gt (list[dict]): list of dictionaries in the format described below.\\n        predictions (list[dict]): list of dictionaries in the format\\n            described below.\\n        class_names (list[str]): list of the class names.\\n        iou_thresholds (list[float]): IOU thresholds used to calculate\\n            TP / FN\\n\\n    Returns:\\n        np.ndarray: an array with an average precision per class.\\n    \"\n    assert all([0 <= iou_th <= 1 for iou_th in iou_thresholds])\n    gt_by_class_name = group_by_key(gt, 'name')\n    pred_by_class_name = group_by_key(predictions, 'name')\n    average_precisions = np.zeros((len(class_names), len(iou_thresholds)))\n    for (class_id, class_name) in enumerate(class_names):\n        if class_name in pred_by_class_name:\n            (recalls, precisions, average_precision) = get_single_class_aps(gt_by_class_name[class_name], pred_by_class_name[class_name], iou_thresholds)\n            average_precisions[class_id, :] = average_precision\n    return average_precisions",
            "def get_classwise_aps(gt, predictions, class_names, iou_thresholds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Returns an array with an average precision per class.\\n\\n    Note: Ground truth and predictions should have the following format.\\n\\n    .. code-block::\\n\\n    gt = [{\\n        'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207\\n                         fbb039a550991a5149214f98cec136ac',\\n        'translation': [974.2811881299899, 1714.6815014457964,\\n                        -23.689857123368846],\\n        'size': [1.796, 4.488, 1.664],\\n        'rotation': [0.14882026466054782, 0, 0, 0.9888642620837121],\\n        'name': 'car'\\n    }]\\n\\n    predictions = [{\\n        'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207\\n                         fbb039a550991a5149214f98cec136ac',\\n        'translation': [971.8343488872263, 1713.6816097857359,\\n                        -25.82534357061308],\\n        'size': [2.519726579986132, 7.810161372666739, 3.483438286096803],\\n        'rotation': [0.10913582721095375, 0.04099572636992043,\\n                     0.01927712319721745, 1.029328402625659],\\n        'name': 'car',\\n        'score': 0.3077029437237213\\n    }]\\n\\n    Args:\\n        gt (list[dict]): list of dictionaries in the format described below.\\n        predictions (list[dict]): list of dictionaries in the format\\n            described below.\\n        class_names (list[str]): list of the class names.\\n        iou_thresholds (list[float]): IOU thresholds used to calculate\\n            TP / FN\\n\\n    Returns:\\n        np.ndarray: an array with an average precision per class.\\n    \"\n    assert all([0 <= iou_th <= 1 for iou_th in iou_thresholds])\n    gt_by_class_name = group_by_key(gt, 'name')\n    pred_by_class_name = group_by_key(predictions, 'name')\n    average_precisions = np.zeros((len(class_names), len(iou_thresholds)))\n    for (class_id, class_name) in enumerate(class_names):\n        if class_name in pred_by_class_name:\n            (recalls, precisions, average_precision) = get_single_class_aps(gt_by_class_name[class_name], pred_by_class_name[class_name], iou_thresholds)\n            average_precisions[class_id, :] = average_precision\n    return average_precisions",
            "def get_classwise_aps(gt, predictions, class_names, iou_thresholds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Returns an array with an average precision per class.\\n\\n    Note: Ground truth and predictions should have the following format.\\n\\n    .. code-block::\\n\\n    gt = [{\\n        'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207\\n                         fbb039a550991a5149214f98cec136ac',\\n        'translation': [974.2811881299899, 1714.6815014457964,\\n                        -23.689857123368846],\\n        'size': [1.796, 4.488, 1.664],\\n        'rotation': [0.14882026466054782, 0, 0, 0.9888642620837121],\\n        'name': 'car'\\n    }]\\n\\n    predictions = [{\\n        'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207\\n                         fbb039a550991a5149214f98cec136ac',\\n        'translation': [971.8343488872263, 1713.6816097857359,\\n                        -25.82534357061308],\\n        'size': [2.519726579986132, 7.810161372666739, 3.483438286096803],\\n        'rotation': [0.10913582721095375, 0.04099572636992043,\\n                     0.01927712319721745, 1.029328402625659],\\n        'name': 'car',\\n        'score': 0.3077029437237213\\n    }]\\n\\n    Args:\\n        gt (list[dict]): list of dictionaries in the format described below.\\n        predictions (list[dict]): list of dictionaries in the format\\n            described below.\\n        class_names (list[str]): list of the class names.\\n        iou_thresholds (list[float]): IOU thresholds used to calculate\\n            TP / FN\\n\\n    Returns:\\n        np.ndarray: an array with an average precision per class.\\n    \"\n    assert all([0 <= iou_th <= 1 for iou_th in iou_thresholds])\n    gt_by_class_name = group_by_key(gt, 'name')\n    pred_by_class_name = group_by_key(predictions, 'name')\n    average_precisions = np.zeros((len(class_names), len(iou_thresholds)))\n    for (class_id, class_name) in enumerate(class_names):\n        if class_name in pred_by_class_name:\n            (recalls, precisions, average_precision) = get_single_class_aps(gt_by_class_name[class_name], pred_by_class_name[class_name], iou_thresholds)\n            average_precisions[class_id, :] = average_precision\n    return average_precisions"
        ]
    },
    {
        "func_name": "get_single_class_aps",
        "original": "def get_single_class_aps(gt, predictions, iou_thresholds):\n    \"\"\"Compute recall and precision for all iou thresholds. Adapted from\n    LyftDatasetDevkit.\n\n    Args:\n        gt (list[dict]): list of dictionaries in the format described above.\n        predictions (list[dict]): list of dictionaries in the format\n            described below.\n        iou_thresholds (list[float]): IOU thresholds used to calculate\n            TP / FN\n\n    Returns:\n        tuple[np.ndarray]: Returns (recalls, precisions, average precisions)\n            for each class.\n    \"\"\"\n    num_gts = len(gt)\n    image_gts = group_by_key(gt, 'sample_token')\n    image_gts = wrap_in_box(image_gts)\n    sample_gt_checked = {sample_token: np.zeros((len(boxes), len(iou_thresholds))) for (sample_token, boxes) in image_gts.items()}\n    predictions = sorted(predictions, key=lambda x: x['score'], reverse=True)\n    num_predictions = len(predictions)\n    tps = np.zeros((num_predictions, len(iou_thresholds)))\n    fps = np.zeros((num_predictions, len(iou_thresholds)))\n    for (prediction_index, prediction) in enumerate(predictions):\n        predicted_box = Box3D(**prediction)\n        sample_token = prediction['sample_token']\n        max_overlap = -np.inf\n        jmax = -1\n        if sample_token in image_gts:\n            gt_boxes = image_gts[sample_token]\n            gt_checked = sample_gt_checked[sample_token]\n        else:\n            gt_boxes = []\n            gt_checked = None\n        if len(gt_boxes) > 0:\n            overlaps = get_ious(gt_boxes, predicted_box)\n            max_overlap = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n        for (i, iou_threshold) in enumerate(iou_thresholds):\n            if max_overlap > iou_threshold:\n                if gt_checked[jmax, i] == 0:\n                    tps[prediction_index, i] = 1.0\n                    gt_checked[jmax, i] = 1\n                else:\n                    fps[prediction_index, i] = 1.0\n            else:\n                fps[prediction_index, i] = 1.0\n    fps = np.cumsum(fps, axis=0)\n    tps = np.cumsum(tps, axis=0)\n    recalls = tps / float(num_gts)\n    precisions = tps / np.maximum(tps + fps, np.finfo(np.float64).eps)\n    aps = []\n    for i in range(len(iou_thresholds)):\n        recall = recalls[:, i]\n        precision = precisions[:, i]\n        assert np.all(0 <= recall) & np.all(recall <= 1)\n        assert np.all(0 <= precision) & np.all(precision <= 1)\n        ap = get_ap(recall, precision)\n        aps.append(ap)\n    aps = np.array(aps)\n    return (recalls, precisions, aps)",
        "mutated": [
            "def get_single_class_aps(gt, predictions, iou_thresholds):\n    if False:\n        i = 10\n    'Compute recall and precision for all iou thresholds. Adapted from\\n    LyftDatasetDevkit.\\n\\n    Args:\\n        gt (list[dict]): list of dictionaries in the format described above.\\n        predictions (list[dict]): list of dictionaries in the format\\n            described below.\\n        iou_thresholds (list[float]): IOU thresholds used to calculate\\n            TP / FN\\n\\n    Returns:\\n        tuple[np.ndarray]: Returns (recalls, precisions, average precisions)\\n            for each class.\\n    '\n    num_gts = len(gt)\n    image_gts = group_by_key(gt, 'sample_token')\n    image_gts = wrap_in_box(image_gts)\n    sample_gt_checked = {sample_token: np.zeros((len(boxes), len(iou_thresholds))) for (sample_token, boxes) in image_gts.items()}\n    predictions = sorted(predictions, key=lambda x: x['score'], reverse=True)\n    num_predictions = len(predictions)\n    tps = np.zeros((num_predictions, len(iou_thresholds)))\n    fps = np.zeros((num_predictions, len(iou_thresholds)))\n    for (prediction_index, prediction) in enumerate(predictions):\n        predicted_box = Box3D(**prediction)\n        sample_token = prediction['sample_token']\n        max_overlap = -np.inf\n        jmax = -1\n        if sample_token in image_gts:\n            gt_boxes = image_gts[sample_token]\n            gt_checked = sample_gt_checked[sample_token]\n        else:\n            gt_boxes = []\n            gt_checked = None\n        if len(gt_boxes) > 0:\n            overlaps = get_ious(gt_boxes, predicted_box)\n            max_overlap = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n        for (i, iou_threshold) in enumerate(iou_thresholds):\n            if max_overlap > iou_threshold:\n                if gt_checked[jmax, i] == 0:\n                    tps[prediction_index, i] = 1.0\n                    gt_checked[jmax, i] = 1\n                else:\n                    fps[prediction_index, i] = 1.0\n            else:\n                fps[prediction_index, i] = 1.0\n    fps = np.cumsum(fps, axis=0)\n    tps = np.cumsum(tps, axis=0)\n    recalls = tps / float(num_gts)\n    precisions = tps / np.maximum(tps + fps, np.finfo(np.float64).eps)\n    aps = []\n    for i in range(len(iou_thresholds)):\n        recall = recalls[:, i]\n        precision = precisions[:, i]\n        assert np.all(0 <= recall) & np.all(recall <= 1)\n        assert np.all(0 <= precision) & np.all(precision <= 1)\n        ap = get_ap(recall, precision)\n        aps.append(ap)\n    aps = np.array(aps)\n    return (recalls, precisions, aps)",
            "def get_single_class_aps(gt, predictions, iou_thresholds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute recall and precision for all iou thresholds. Adapted from\\n    LyftDatasetDevkit.\\n\\n    Args:\\n        gt (list[dict]): list of dictionaries in the format described above.\\n        predictions (list[dict]): list of dictionaries in the format\\n            described below.\\n        iou_thresholds (list[float]): IOU thresholds used to calculate\\n            TP / FN\\n\\n    Returns:\\n        tuple[np.ndarray]: Returns (recalls, precisions, average precisions)\\n            for each class.\\n    '\n    num_gts = len(gt)\n    image_gts = group_by_key(gt, 'sample_token')\n    image_gts = wrap_in_box(image_gts)\n    sample_gt_checked = {sample_token: np.zeros((len(boxes), len(iou_thresholds))) for (sample_token, boxes) in image_gts.items()}\n    predictions = sorted(predictions, key=lambda x: x['score'], reverse=True)\n    num_predictions = len(predictions)\n    tps = np.zeros((num_predictions, len(iou_thresholds)))\n    fps = np.zeros((num_predictions, len(iou_thresholds)))\n    for (prediction_index, prediction) in enumerate(predictions):\n        predicted_box = Box3D(**prediction)\n        sample_token = prediction['sample_token']\n        max_overlap = -np.inf\n        jmax = -1\n        if sample_token in image_gts:\n            gt_boxes = image_gts[sample_token]\n            gt_checked = sample_gt_checked[sample_token]\n        else:\n            gt_boxes = []\n            gt_checked = None\n        if len(gt_boxes) > 0:\n            overlaps = get_ious(gt_boxes, predicted_box)\n            max_overlap = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n        for (i, iou_threshold) in enumerate(iou_thresholds):\n            if max_overlap > iou_threshold:\n                if gt_checked[jmax, i] == 0:\n                    tps[prediction_index, i] = 1.0\n                    gt_checked[jmax, i] = 1\n                else:\n                    fps[prediction_index, i] = 1.0\n            else:\n                fps[prediction_index, i] = 1.0\n    fps = np.cumsum(fps, axis=0)\n    tps = np.cumsum(tps, axis=0)\n    recalls = tps / float(num_gts)\n    precisions = tps / np.maximum(tps + fps, np.finfo(np.float64).eps)\n    aps = []\n    for i in range(len(iou_thresholds)):\n        recall = recalls[:, i]\n        precision = precisions[:, i]\n        assert np.all(0 <= recall) & np.all(recall <= 1)\n        assert np.all(0 <= precision) & np.all(precision <= 1)\n        ap = get_ap(recall, precision)\n        aps.append(ap)\n    aps = np.array(aps)\n    return (recalls, precisions, aps)",
            "def get_single_class_aps(gt, predictions, iou_thresholds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute recall and precision for all iou thresholds. Adapted from\\n    LyftDatasetDevkit.\\n\\n    Args:\\n        gt (list[dict]): list of dictionaries in the format described above.\\n        predictions (list[dict]): list of dictionaries in the format\\n            described below.\\n        iou_thresholds (list[float]): IOU thresholds used to calculate\\n            TP / FN\\n\\n    Returns:\\n        tuple[np.ndarray]: Returns (recalls, precisions, average precisions)\\n            for each class.\\n    '\n    num_gts = len(gt)\n    image_gts = group_by_key(gt, 'sample_token')\n    image_gts = wrap_in_box(image_gts)\n    sample_gt_checked = {sample_token: np.zeros((len(boxes), len(iou_thresholds))) for (sample_token, boxes) in image_gts.items()}\n    predictions = sorted(predictions, key=lambda x: x['score'], reverse=True)\n    num_predictions = len(predictions)\n    tps = np.zeros((num_predictions, len(iou_thresholds)))\n    fps = np.zeros((num_predictions, len(iou_thresholds)))\n    for (prediction_index, prediction) in enumerate(predictions):\n        predicted_box = Box3D(**prediction)\n        sample_token = prediction['sample_token']\n        max_overlap = -np.inf\n        jmax = -1\n        if sample_token in image_gts:\n            gt_boxes = image_gts[sample_token]\n            gt_checked = sample_gt_checked[sample_token]\n        else:\n            gt_boxes = []\n            gt_checked = None\n        if len(gt_boxes) > 0:\n            overlaps = get_ious(gt_boxes, predicted_box)\n            max_overlap = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n        for (i, iou_threshold) in enumerate(iou_thresholds):\n            if max_overlap > iou_threshold:\n                if gt_checked[jmax, i] == 0:\n                    tps[prediction_index, i] = 1.0\n                    gt_checked[jmax, i] = 1\n                else:\n                    fps[prediction_index, i] = 1.0\n            else:\n                fps[prediction_index, i] = 1.0\n    fps = np.cumsum(fps, axis=0)\n    tps = np.cumsum(tps, axis=0)\n    recalls = tps / float(num_gts)\n    precisions = tps / np.maximum(tps + fps, np.finfo(np.float64).eps)\n    aps = []\n    for i in range(len(iou_thresholds)):\n        recall = recalls[:, i]\n        precision = precisions[:, i]\n        assert np.all(0 <= recall) & np.all(recall <= 1)\n        assert np.all(0 <= precision) & np.all(precision <= 1)\n        ap = get_ap(recall, precision)\n        aps.append(ap)\n    aps = np.array(aps)\n    return (recalls, precisions, aps)",
            "def get_single_class_aps(gt, predictions, iou_thresholds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute recall and precision for all iou thresholds. Adapted from\\n    LyftDatasetDevkit.\\n\\n    Args:\\n        gt (list[dict]): list of dictionaries in the format described above.\\n        predictions (list[dict]): list of dictionaries in the format\\n            described below.\\n        iou_thresholds (list[float]): IOU thresholds used to calculate\\n            TP / FN\\n\\n    Returns:\\n        tuple[np.ndarray]: Returns (recalls, precisions, average precisions)\\n            for each class.\\n    '\n    num_gts = len(gt)\n    image_gts = group_by_key(gt, 'sample_token')\n    image_gts = wrap_in_box(image_gts)\n    sample_gt_checked = {sample_token: np.zeros((len(boxes), len(iou_thresholds))) for (sample_token, boxes) in image_gts.items()}\n    predictions = sorted(predictions, key=lambda x: x['score'], reverse=True)\n    num_predictions = len(predictions)\n    tps = np.zeros((num_predictions, len(iou_thresholds)))\n    fps = np.zeros((num_predictions, len(iou_thresholds)))\n    for (prediction_index, prediction) in enumerate(predictions):\n        predicted_box = Box3D(**prediction)\n        sample_token = prediction['sample_token']\n        max_overlap = -np.inf\n        jmax = -1\n        if sample_token in image_gts:\n            gt_boxes = image_gts[sample_token]\n            gt_checked = sample_gt_checked[sample_token]\n        else:\n            gt_boxes = []\n            gt_checked = None\n        if len(gt_boxes) > 0:\n            overlaps = get_ious(gt_boxes, predicted_box)\n            max_overlap = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n        for (i, iou_threshold) in enumerate(iou_thresholds):\n            if max_overlap > iou_threshold:\n                if gt_checked[jmax, i] == 0:\n                    tps[prediction_index, i] = 1.0\n                    gt_checked[jmax, i] = 1\n                else:\n                    fps[prediction_index, i] = 1.0\n            else:\n                fps[prediction_index, i] = 1.0\n    fps = np.cumsum(fps, axis=0)\n    tps = np.cumsum(tps, axis=0)\n    recalls = tps / float(num_gts)\n    precisions = tps / np.maximum(tps + fps, np.finfo(np.float64).eps)\n    aps = []\n    for i in range(len(iou_thresholds)):\n        recall = recalls[:, i]\n        precision = precisions[:, i]\n        assert np.all(0 <= recall) & np.all(recall <= 1)\n        assert np.all(0 <= precision) & np.all(precision <= 1)\n        ap = get_ap(recall, precision)\n        aps.append(ap)\n    aps = np.array(aps)\n    return (recalls, precisions, aps)",
            "def get_single_class_aps(gt, predictions, iou_thresholds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute recall and precision for all iou thresholds. Adapted from\\n    LyftDatasetDevkit.\\n\\n    Args:\\n        gt (list[dict]): list of dictionaries in the format described above.\\n        predictions (list[dict]): list of dictionaries in the format\\n            described below.\\n        iou_thresholds (list[float]): IOU thresholds used to calculate\\n            TP / FN\\n\\n    Returns:\\n        tuple[np.ndarray]: Returns (recalls, precisions, average precisions)\\n            for each class.\\n    '\n    num_gts = len(gt)\n    image_gts = group_by_key(gt, 'sample_token')\n    image_gts = wrap_in_box(image_gts)\n    sample_gt_checked = {sample_token: np.zeros((len(boxes), len(iou_thresholds))) for (sample_token, boxes) in image_gts.items()}\n    predictions = sorted(predictions, key=lambda x: x['score'], reverse=True)\n    num_predictions = len(predictions)\n    tps = np.zeros((num_predictions, len(iou_thresholds)))\n    fps = np.zeros((num_predictions, len(iou_thresholds)))\n    for (prediction_index, prediction) in enumerate(predictions):\n        predicted_box = Box3D(**prediction)\n        sample_token = prediction['sample_token']\n        max_overlap = -np.inf\n        jmax = -1\n        if sample_token in image_gts:\n            gt_boxes = image_gts[sample_token]\n            gt_checked = sample_gt_checked[sample_token]\n        else:\n            gt_boxes = []\n            gt_checked = None\n        if len(gt_boxes) > 0:\n            overlaps = get_ious(gt_boxes, predicted_box)\n            max_overlap = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n        for (i, iou_threshold) in enumerate(iou_thresholds):\n            if max_overlap > iou_threshold:\n                if gt_checked[jmax, i] == 0:\n                    tps[prediction_index, i] = 1.0\n                    gt_checked[jmax, i] = 1\n                else:\n                    fps[prediction_index, i] = 1.0\n            else:\n                fps[prediction_index, i] = 1.0\n    fps = np.cumsum(fps, axis=0)\n    tps = np.cumsum(tps, axis=0)\n    recalls = tps / float(num_gts)\n    precisions = tps / np.maximum(tps + fps, np.finfo(np.float64).eps)\n    aps = []\n    for i in range(len(iou_thresholds)):\n        recall = recalls[:, i]\n        precision = precisions[:, i]\n        assert np.all(0 <= recall) & np.all(recall <= 1)\n        assert np.all(0 <= precision) & np.all(precision <= 1)\n        ap = get_ap(recall, precision)\n        aps.append(ap)\n    aps = np.array(aps)\n    return (recalls, precisions, aps)"
        ]
    }
]