[
    {
        "func_name": "smape",
        "original": "def smape(a, b):\n    return 2.0 * abs(a - b) / float(a + b)",
        "mutated": [
            "def smape(a, b):\n    if False:\n        i = 10\n    return 2.0 * abs(a - b) / float(a + b)",
            "def smape(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return 2.0 * abs(a - b) / float(a + b)",
            "def smape(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return 2.0 * abs(a - b) / float(a + b)",
            "def smape(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return 2.0 * abs(a - b) / float(a + b)",
            "def smape(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return 2.0 * abs(a - b) / float(a + b)"
        ]
    },
    {
        "func_name": "onehot",
        "original": "def onehot(dim, num_dims):\n    value = np.zeros(num_dims, dtype=np.float32)\n    value[dim] = 1\n    return value",
        "mutated": [
            "def onehot(dim, num_dims):\n    if False:\n        i = 10\n    value = np.zeros(num_dims, dtype=np.float32)\n    value[dim] = 1\n    return value",
            "def onehot(dim, num_dims):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    value = np.zeros(num_dims, dtype=np.float32)\n    value[dim] = 1\n    return value",
            "def onehot(dim, num_dims):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    value = np.zeros(num_dims, dtype=np.float32)\n    value[dim] = 1\n    return value",
            "def onehot(dim, num_dims):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    value = np.zeros(num_dims, dtype=np.float32)\n    value[dim] = 1\n    return value",
            "def onehot(dim, num_dims):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    value = np.zeros(num_dims, dtype=np.float32)\n    value[dim] = 1\n    return value"
        ]
    },
    {
        "func_name": "random_sequence",
        "original": "def random_sequence(max_length, num_tokens, eos=0):\n    length = np.random.randint(1, max_length - 1)\n    return np.append(np.random.randint(1, num_tokens, length), eos)",
        "mutated": [
            "def random_sequence(max_length, num_tokens, eos=0):\n    if False:\n        i = 10\n    length = np.random.randint(1, max_length - 1)\n    return np.append(np.random.randint(1, num_tokens, length), eos)",
            "def random_sequence(max_length, num_tokens, eos=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    length = np.random.randint(1, max_length - 1)\n    return np.append(np.random.randint(1, num_tokens, length), eos)",
            "def random_sequence(max_length, num_tokens, eos=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    length = np.random.randint(1, max_length - 1)\n    return np.append(np.random.randint(1, num_tokens, length), eos)",
            "def random_sequence(max_length, num_tokens, eos=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    length = np.random.randint(1, max_length - 1)\n    return np.append(np.random.randint(1, num_tokens, length), eos)",
            "def random_sequence(max_length, num_tokens, eos=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    length = np.random.randint(1, max_length - 1)\n    return np.append(np.random.randint(1, num_tokens, length), eos)"
        ]
    },
    {
        "func_name": "repeat_and_pad",
        "original": "def repeat_and_pad(v, rep, total_len):\n    return [v] * rep + [0.0] * (total_len - rep)",
        "mutated": [
            "def repeat_and_pad(v, rep, total_len):\n    if False:\n        i = 10\n    return [v] * rep + [0.0] * (total_len - rep)",
            "def repeat_and_pad(v, rep, total_len):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return [v] * rep + [0.0] * (total_len - rep)",
            "def repeat_and_pad(v, rep, total_len):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return [v] * rep + [0.0] * (total_len - rep)",
            "def repeat_and_pad(v, rep, total_len):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return [v] * rep + [0.0] * (total_len - rep)",
            "def repeat_and_pad(v, rep, total_len):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return [v] * rep + [0.0] * (total_len - rep)"
        ]
    },
    {
        "func_name": "reward_fn",
        "original": "def reward_fn(code_string):\n    return misc.RewardInfo(episode_rewards=[float(ord(c)) for c in code_string], input_case=[], correct_output=[], code_output=[], input_type=misc.IOType.integer, output_type=misc.IOType.integer, reason='none')",
        "mutated": [
            "def reward_fn(code_string):\n    if False:\n        i = 10\n    return misc.RewardInfo(episode_rewards=[float(ord(c)) for c in code_string], input_case=[], correct_output=[], code_output=[], input_type=misc.IOType.integer, output_type=misc.IOType.integer, reason='none')",
            "def reward_fn(code_string):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return misc.RewardInfo(episode_rewards=[float(ord(c)) for c in code_string], input_case=[], correct_output=[], code_output=[], input_type=misc.IOType.integer, output_type=misc.IOType.integer, reason='none')",
            "def reward_fn(code_string):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return misc.RewardInfo(episode_rewards=[float(ord(c)) for c in code_string], input_case=[], correct_output=[], code_output=[], input_type=misc.IOType.integer, output_type=misc.IOType.integer, reason='none')",
            "def reward_fn(code_string):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return misc.RewardInfo(episode_rewards=[float(ord(c)) for c in code_string], input_case=[], correct_output=[], code_output=[], input_type=misc.IOType.integer, output_type=misc.IOType.integer, reason='none')",
            "def reward_fn(code_string):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return misc.RewardInfo(episode_rewards=[float(ord(c)) for c in code_string], input_case=[], correct_output=[], code_output=[], input_type=misc.IOType.integer, output_type=misc.IOType.integer, reason='none')"
        ]
    },
    {
        "func_name": "testProcessEpisodes",
        "original": "def testProcessEpisodes(self):\n    batch_size = 3\n\n    def reward_fn(code_string):\n        return misc.RewardInfo(episode_rewards=[float(ord(c)) for c in code_string], input_case=[], correct_output=[], code_output=[], input_type=misc.IOType.integer, output_type=misc.IOType.integer, reason='none')\n    rl_batch = data.RLBatch(reward_fns=[reward_fn for _ in range(batch_size)], batch_size=batch_size, good_reward=10.0)\n    batch_actions = np.asarray([[4, 5, 3, 6, 8, 1, 0, 0], [1, 2, 3, 4, 0, 0, 0, 0], [8, 7, 6, 5, 4, 3, 2, 1]], dtype=np.int32)\n    batch_values = np.asarray([[0, 1, 2, 1, 0, 1, 1, 0], [0, 2, 1, 2, 1, 0, 0, 0], [0, 1, 1, 0, 0, 0, 1, 1]], dtype=np.float32)\n    episode_lengths = np.asarray([7, 5, 8], dtype=np.int32)\n    scores = agent_lib.compute_rewards(rl_batch, batch_actions, episode_lengths)\n    (batch_targets, batch_returns) = agent_lib.process_episodes(scores.batch_rewards, episode_lengths, a2c=True, batch_values=batch_values)\n    self.assertEqual([[473.0, 428.0, 337.0, 294.0, 201.0, 157.0, 95.0, 0.0], [305.0, 243.0, 183.0, 140.0, 95.0, 0.0, 0.0, 0.0], [484.0, 440.0, 394.0, 301.0, 210.0, 165.0, 122.0, 62.0]], batch_returns.tolist())\n    self.assertEqual([[473.0, 427.0, 335.0, 293.0, 201.0, 156.0, 94.0, 0.0], [305.0, 241.0, 182.0, 138.0, 94.0, 0.0, 0.0, 0.0], [484.0, 439.0, 393.0, 301.0, 210.0, 165.0, 121.0, 61.0]], batch_targets.tolist())",
        "mutated": [
            "def testProcessEpisodes(self):\n    if False:\n        i = 10\n    batch_size = 3\n\n    def reward_fn(code_string):\n        return misc.RewardInfo(episode_rewards=[float(ord(c)) for c in code_string], input_case=[], correct_output=[], code_output=[], input_type=misc.IOType.integer, output_type=misc.IOType.integer, reason='none')\n    rl_batch = data.RLBatch(reward_fns=[reward_fn for _ in range(batch_size)], batch_size=batch_size, good_reward=10.0)\n    batch_actions = np.asarray([[4, 5, 3, 6, 8, 1, 0, 0], [1, 2, 3, 4, 0, 0, 0, 0], [8, 7, 6, 5, 4, 3, 2, 1]], dtype=np.int32)\n    batch_values = np.asarray([[0, 1, 2, 1, 0, 1, 1, 0], [0, 2, 1, 2, 1, 0, 0, 0], [0, 1, 1, 0, 0, 0, 1, 1]], dtype=np.float32)\n    episode_lengths = np.asarray([7, 5, 8], dtype=np.int32)\n    scores = agent_lib.compute_rewards(rl_batch, batch_actions, episode_lengths)\n    (batch_targets, batch_returns) = agent_lib.process_episodes(scores.batch_rewards, episode_lengths, a2c=True, batch_values=batch_values)\n    self.assertEqual([[473.0, 428.0, 337.0, 294.0, 201.0, 157.0, 95.0, 0.0], [305.0, 243.0, 183.0, 140.0, 95.0, 0.0, 0.0, 0.0], [484.0, 440.0, 394.0, 301.0, 210.0, 165.0, 122.0, 62.0]], batch_returns.tolist())\n    self.assertEqual([[473.0, 427.0, 335.0, 293.0, 201.0, 156.0, 94.0, 0.0], [305.0, 241.0, 182.0, 138.0, 94.0, 0.0, 0.0, 0.0], [484.0, 439.0, 393.0, 301.0, 210.0, 165.0, 121.0, 61.0]], batch_targets.tolist())",
            "def testProcessEpisodes(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    batch_size = 3\n\n    def reward_fn(code_string):\n        return misc.RewardInfo(episode_rewards=[float(ord(c)) for c in code_string], input_case=[], correct_output=[], code_output=[], input_type=misc.IOType.integer, output_type=misc.IOType.integer, reason='none')\n    rl_batch = data.RLBatch(reward_fns=[reward_fn for _ in range(batch_size)], batch_size=batch_size, good_reward=10.0)\n    batch_actions = np.asarray([[4, 5, 3, 6, 8, 1, 0, 0], [1, 2, 3, 4, 0, 0, 0, 0], [8, 7, 6, 5, 4, 3, 2, 1]], dtype=np.int32)\n    batch_values = np.asarray([[0, 1, 2, 1, 0, 1, 1, 0], [0, 2, 1, 2, 1, 0, 0, 0], [0, 1, 1, 0, 0, 0, 1, 1]], dtype=np.float32)\n    episode_lengths = np.asarray([7, 5, 8], dtype=np.int32)\n    scores = agent_lib.compute_rewards(rl_batch, batch_actions, episode_lengths)\n    (batch_targets, batch_returns) = agent_lib.process_episodes(scores.batch_rewards, episode_lengths, a2c=True, batch_values=batch_values)\n    self.assertEqual([[473.0, 428.0, 337.0, 294.0, 201.0, 157.0, 95.0, 0.0], [305.0, 243.0, 183.0, 140.0, 95.0, 0.0, 0.0, 0.0], [484.0, 440.0, 394.0, 301.0, 210.0, 165.0, 122.0, 62.0]], batch_returns.tolist())\n    self.assertEqual([[473.0, 427.0, 335.0, 293.0, 201.0, 156.0, 94.0, 0.0], [305.0, 241.0, 182.0, 138.0, 94.0, 0.0, 0.0, 0.0], [484.0, 439.0, 393.0, 301.0, 210.0, 165.0, 121.0, 61.0]], batch_targets.tolist())",
            "def testProcessEpisodes(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    batch_size = 3\n\n    def reward_fn(code_string):\n        return misc.RewardInfo(episode_rewards=[float(ord(c)) for c in code_string], input_case=[], correct_output=[], code_output=[], input_type=misc.IOType.integer, output_type=misc.IOType.integer, reason='none')\n    rl_batch = data.RLBatch(reward_fns=[reward_fn for _ in range(batch_size)], batch_size=batch_size, good_reward=10.0)\n    batch_actions = np.asarray([[4, 5, 3, 6, 8, 1, 0, 0], [1, 2, 3, 4, 0, 0, 0, 0], [8, 7, 6, 5, 4, 3, 2, 1]], dtype=np.int32)\n    batch_values = np.asarray([[0, 1, 2, 1, 0, 1, 1, 0], [0, 2, 1, 2, 1, 0, 0, 0], [0, 1, 1, 0, 0, 0, 1, 1]], dtype=np.float32)\n    episode_lengths = np.asarray([7, 5, 8], dtype=np.int32)\n    scores = agent_lib.compute_rewards(rl_batch, batch_actions, episode_lengths)\n    (batch_targets, batch_returns) = agent_lib.process_episodes(scores.batch_rewards, episode_lengths, a2c=True, batch_values=batch_values)\n    self.assertEqual([[473.0, 428.0, 337.0, 294.0, 201.0, 157.0, 95.0, 0.0], [305.0, 243.0, 183.0, 140.0, 95.0, 0.0, 0.0, 0.0], [484.0, 440.0, 394.0, 301.0, 210.0, 165.0, 122.0, 62.0]], batch_returns.tolist())\n    self.assertEqual([[473.0, 427.0, 335.0, 293.0, 201.0, 156.0, 94.0, 0.0], [305.0, 241.0, 182.0, 138.0, 94.0, 0.0, 0.0, 0.0], [484.0, 439.0, 393.0, 301.0, 210.0, 165.0, 121.0, 61.0]], batch_targets.tolist())",
            "def testProcessEpisodes(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    batch_size = 3\n\n    def reward_fn(code_string):\n        return misc.RewardInfo(episode_rewards=[float(ord(c)) for c in code_string], input_case=[], correct_output=[], code_output=[], input_type=misc.IOType.integer, output_type=misc.IOType.integer, reason='none')\n    rl_batch = data.RLBatch(reward_fns=[reward_fn for _ in range(batch_size)], batch_size=batch_size, good_reward=10.0)\n    batch_actions = np.asarray([[4, 5, 3, 6, 8, 1, 0, 0], [1, 2, 3, 4, 0, 0, 0, 0], [8, 7, 6, 5, 4, 3, 2, 1]], dtype=np.int32)\n    batch_values = np.asarray([[0, 1, 2, 1, 0, 1, 1, 0], [0, 2, 1, 2, 1, 0, 0, 0], [0, 1, 1, 0, 0, 0, 1, 1]], dtype=np.float32)\n    episode_lengths = np.asarray([7, 5, 8], dtype=np.int32)\n    scores = agent_lib.compute_rewards(rl_batch, batch_actions, episode_lengths)\n    (batch_targets, batch_returns) = agent_lib.process_episodes(scores.batch_rewards, episode_lengths, a2c=True, batch_values=batch_values)\n    self.assertEqual([[473.0, 428.0, 337.0, 294.0, 201.0, 157.0, 95.0, 0.0], [305.0, 243.0, 183.0, 140.0, 95.0, 0.0, 0.0, 0.0], [484.0, 440.0, 394.0, 301.0, 210.0, 165.0, 122.0, 62.0]], batch_returns.tolist())\n    self.assertEqual([[473.0, 427.0, 335.0, 293.0, 201.0, 156.0, 94.0, 0.0], [305.0, 241.0, 182.0, 138.0, 94.0, 0.0, 0.0, 0.0], [484.0, 439.0, 393.0, 301.0, 210.0, 165.0, 121.0, 61.0]], batch_targets.tolist())",
            "def testProcessEpisodes(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    batch_size = 3\n\n    def reward_fn(code_string):\n        return misc.RewardInfo(episode_rewards=[float(ord(c)) for c in code_string], input_case=[], correct_output=[], code_output=[], input_type=misc.IOType.integer, output_type=misc.IOType.integer, reason='none')\n    rl_batch = data.RLBatch(reward_fns=[reward_fn for _ in range(batch_size)], batch_size=batch_size, good_reward=10.0)\n    batch_actions = np.asarray([[4, 5, 3, 6, 8, 1, 0, 0], [1, 2, 3, 4, 0, 0, 0, 0], [8, 7, 6, 5, 4, 3, 2, 1]], dtype=np.int32)\n    batch_values = np.asarray([[0, 1, 2, 1, 0, 1, 1, 0], [0, 2, 1, 2, 1, 0, 0, 0], [0, 1, 1, 0, 0, 0, 1, 1]], dtype=np.float32)\n    episode_lengths = np.asarray([7, 5, 8], dtype=np.int32)\n    scores = agent_lib.compute_rewards(rl_batch, batch_actions, episode_lengths)\n    (batch_targets, batch_returns) = agent_lib.process_episodes(scores.batch_rewards, episode_lengths, a2c=True, batch_values=batch_values)\n    self.assertEqual([[473.0, 428.0, 337.0, 294.0, 201.0, 157.0, 95.0, 0.0], [305.0, 243.0, 183.0, 140.0, 95.0, 0.0, 0.0, 0.0], [484.0, 440.0, 394.0, 301.0, 210.0, 165.0, 122.0, 62.0]], batch_returns.tolist())\n    self.assertEqual([[473.0, 427.0, 335.0, 293.0, 201.0, 156.0, 94.0, 0.0], [305.0, 241.0, 182.0, 138.0, 94.0, 0.0, 0.0, 0.0], [484.0, 439.0, 393.0, 301.0, 210.0, 165.0, 121.0, 61.0]], batch_targets.tolist())"
        ]
    },
    {
        "func_name": "testVarUpdates",
        "original": "def testVarUpdates(self):\n    \"\"\"Tests that variables get updated as expected.\n\n    For the RL update, check that gradients are non-zero and that the global\n    model gets updated.\n    \"\"\"\n    config = defaults.default_config_with_updates('env=c(task=\"reverse\"),agent=c(algorithm=\"pg\",eos_token=True,optimizer=\"sgd\",lr=1.0)')\n    lr = config.agent.lr\n    tf.reset_default_graph()\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1)\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    with tf.Session() as sess:\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        model = trainer.model\n        global_vars = sess.run(trainer.global_model.trainable_variables)\n        local_vars = sess.run(model.trainable_variables)\n        g_prefix = 'global/'\n        l_prefix = 'local/'\n        for (g, l) in zip(trainer.global_model.trainable_variables, model.trainable_variables):\n            self.assertEqual(g.name[len(g_prefix):], l.name[len(l_prefix):])\n        for (g, l) in zip(global_vars, local_vars):\n            self.assertEqual(g.shape, l.shape)\n            self.assertTrue(np.array_equal(g, l))\n        for (param, grad) in model.gradients_dict.items():\n            if isinstance(grad, tf.IndexedSlices):\n                model.gradients_dict[param] = tf.multiply(grad, 1.0)\n        results = model.update_step(sess, trainer.data_manager.sample_rl_batch(), trainer.train_op, trainer.global_step, return_gradients=True)\n        grads_dict = results.gradients_dict\n        for grad in grads_dict.values():\n            self.assertIsNotNone(grad)\n            self.assertTrue(np.count_nonzero(grad) > 0)\n        global_update = sess.run(trainer.global_model.trainable_variables)\n        for (tf_var, var_before, var_after) in zip(model.trainable_variables, local_vars, global_update):\n            self.assertTrue(np.allclose(var_after, var_before - grads_dict[tf_var] * lr))\n        sess.run(trainer.sync_op)\n        global_vars = sess.run(trainer.global_model.trainable_variables)\n        local_vars = sess.run(model.trainable_variables)\n        for (l, g) in zip(local_vars, global_vars):\n            self.assertTrue(np.allclose(l, g))",
        "mutated": [
            "def testVarUpdates(self):\n    if False:\n        i = 10\n    'Tests that variables get updated as expected.\\n\\n    For the RL update, check that gradients are non-zero and that the global\\n    model gets updated.\\n    '\n    config = defaults.default_config_with_updates('env=c(task=\"reverse\"),agent=c(algorithm=\"pg\",eos_token=True,optimizer=\"sgd\",lr=1.0)')\n    lr = config.agent.lr\n    tf.reset_default_graph()\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1)\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    with tf.Session() as sess:\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        model = trainer.model\n        global_vars = sess.run(trainer.global_model.trainable_variables)\n        local_vars = sess.run(model.trainable_variables)\n        g_prefix = 'global/'\n        l_prefix = 'local/'\n        for (g, l) in zip(trainer.global_model.trainable_variables, model.trainable_variables):\n            self.assertEqual(g.name[len(g_prefix):], l.name[len(l_prefix):])\n        for (g, l) in zip(global_vars, local_vars):\n            self.assertEqual(g.shape, l.shape)\n            self.assertTrue(np.array_equal(g, l))\n        for (param, grad) in model.gradients_dict.items():\n            if isinstance(grad, tf.IndexedSlices):\n                model.gradients_dict[param] = tf.multiply(grad, 1.0)\n        results = model.update_step(sess, trainer.data_manager.sample_rl_batch(), trainer.train_op, trainer.global_step, return_gradients=True)\n        grads_dict = results.gradients_dict\n        for grad in grads_dict.values():\n            self.assertIsNotNone(grad)\n            self.assertTrue(np.count_nonzero(grad) > 0)\n        global_update = sess.run(trainer.global_model.trainable_variables)\n        for (tf_var, var_before, var_after) in zip(model.trainable_variables, local_vars, global_update):\n            self.assertTrue(np.allclose(var_after, var_before - grads_dict[tf_var] * lr))\n        sess.run(trainer.sync_op)\n        global_vars = sess.run(trainer.global_model.trainable_variables)\n        local_vars = sess.run(model.trainable_variables)\n        for (l, g) in zip(local_vars, global_vars):\n            self.assertTrue(np.allclose(l, g))",
            "def testVarUpdates(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Tests that variables get updated as expected.\\n\\n    For the RL update, check that gradients are non-zero and that the global\\n    model gets updated.\\n    '\n    config = defaults.default_config_with_updates('env=c(task=\"reverse\"),agent=c(algorithm=\"pg\",eos_token=True,optimizer=\"sgd\",lr=1.0)')\n    lr = config.agent.lr\n    tf.reset_default_graph()\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1)\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    with tf.Session() as sess:\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        model = trainer.model\n        global_vars = sess.run(trainer.global_model.trainable_variables)\n        local_vars = sess.run(model.trainable_variables)\n        g_prefix = 'global/'\n        l_prefix = 'local/'\n        for (g, l) in zip(trainer.global_model.trainable_variables, model.trainable_variables):\n            self.assertEqual(g.name[len(g_prefix):], l.name[len(l_prefix):])\n        for (g, l) in zip(global_vars, local_vars):\n            self.assertEqual(g.shape, l.shape)\n            self.assertTrue(np.array_equal(g, l))\n        for (param, grad) in model.gradients_dict.items():\n            if isinstance(grad, tf.IndexedSlices):\n                model.gradients_dict[param] = tf.multiply(grad, 1.0)\n        results = model.update_step(sess, trainer.data_manager.sample_rl_batch(), trainer.train_op, trainer.global_step, return_gradients=True)\n        grads_dict = results.gradients_dict\n        for grad in grads_dict.values():\n            self.assertIsNotNone(grad)\n            self.assertTrue(np.count_nonzero(grad) > 0)\n        global_update = sess.run(trainer.global_model.trainable_variables)\n        for (tf_var, var_before, var_after) in zip(model.trainable_variables, local_vars, global_update):\n            self.assertTrue(np.allclose(var_after, var_before - grads_dict[tf_var] * lr))\n        sess.run(trainer.sync_op)\n        global_vars = sess.run(trainer.global_model.trainable_variables)\n        local_vars = sess.run(model.trainable_variables)\n        for (l, g) in zip(local_vars, global_vars):\n            self.assertTrue(np.allclose(l, g))",
            "def testVarUpdates(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Tests that variables get updated as expected.\\n\\n    For the RL update, check that gradients are non-zero and that the global\\n    model gets updated.\\n    '\n    config = defaults.default_config_with_updates('env=c(task=\"reverse\"),agent=c(algorithm=\"pg\",eos_token=True,optimizer=\"sgd\",lr=1.0)')\n    lr = config.agent.lr\n    tf.reset_default_graph()\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1)\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    with tf.Session() as sess:\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        model = trainer.model\n        global_vars = sess.run(trainer.global_model.trainable_variables)\n        local_vars = sess.run(model.trainable_variables)\n        g_prefix = 'global/'\n        l_prefix = 'local/'\n        for (g, l) in zip(trainer.global_model.trainable_variables, model.trainable_variables):\n            self.assertEqual(g.name[len(g_prefix):], l.name[len(l_prefix):])\n        for (g, l) in zip(global_vars, local_vars):\n            self.assertEqual(g.shape, l.shape)\n            self.assertTrue(np.array_equal(g, l))\n        for (param, grad) in model.gradients_dict.items():\n            if isinstance(grad, tf.IndexedSlices):\n                model.gradients_dict[param] = tf.multiply(grad, 1.0)\n        results = model.update_step(sess, trainer.data_manager.sample_rl_batch(), trainer.train_op, trainer.global_step, return_gradients=True)\n        grads_dict = results.gradients_dict\n        for grad in grads_dict.values():\n            self.assertIsNotNone(grad)\n            self.assertTrue(np.count_nonzero(grad) > 0)\n        global_update = sess.run(trainer.global_model.trainable_variables)\n        for (tf_var, var_before, var_after) in zip(model.trainable_variables, local_vars, global_update):\n            self.assertTrue(np.allclose(var_after, var_before - grads_dict[tf_var] * lr))\n        sess.run(trainer.sync_op)\n        global_vars = sess.run(trainer.global_model.trainable_variables)\n        local_vars = sess.run(model.trainable_variables)\n        for (l, g) in zip(local_vars, global_vars):\n            self.assertTrue(np.allclose(l, g))",
            "def testVarUpdates(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Tests that variables get updated as expected.\\n\\n    For the RL update, check that gradients are non-zero and that the global\\n    model gets updated.\\n    '\n    config = defaults.default_config_with_updates('env=c(task=\"reverse\"),agent=c(algorithm=\"pg\",eos_token=True,optimizer=\"sgd\",lr=1.0)')\n    lr = config.agent.lr\n    tf.reset_default_graph()\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1)\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    with tf.Session() as sess:\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        model = trainer.model\n        global_vars = sess.run(trainer.global_model.trainable_variables)\n        local_vars = sess.run(model.trainable_variables)\n        g_prefix = 'global/'\n        l_prefix = 'local/'\n        for (g, l) in zip(trainer.global_model.trainable_variables, model.trainable_variables):\n            self.assertEqual(g.name[len(g_prefix):], l.name[len(l_prefix):])\n        for (g, l) in zip(global_vars, local_vars):\n            self.assertEqual(g.shape, l.shape)\n            self.assertTrue(np.array_equal(g, l))\n        for (param, grad) in model.gradients_dict.items():\n            if isinstance(grad, tf.IndexedSlices):\n                model.gradients_dict[param] = tf.multiply(grad, 1.0)\n        results = model.update_step(sess, trainer.data_manager.sample_rl_batch(), trainer.train_op, trainer.global_step, return_gradients=True)\n        grads_dict = results.gradients_dict\n        for grad in grads_dict.values():\n            self.assertIsNotNone(grad)\n            self.assertTrue(np.count_nonzero(grad) > 0)\n        global_update = sess.run(trainer.global_model.trainable_variables)\n        for (tf_var, var_before, var_after) in zip(model.trainable_variables, local_vars, global_update):\n            self.assertTrue(np.allclose(var_after, var_before - grads_dict[tf_var] * lr))\n        sess.run(trainer.sync_op)\n        global_vars = sess.run(trainer.global_model.trainable_variables)\n        local_vars = sess.run(model.trainable_variables)\n        for (l, g) in zip(local_vars, global_vars):\n            self.assertTrue(np.allclose(l, g))",
            "def testVarUpdates(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Tests that variables get updated as expected.\\n\\n    For the RL update, check that gradients are non-zero and that the global\\n    model gets updated.\\n    '\n    config = defaults.default_config_with_updates('env=c(task=\"reverse\"),agent=c(algorithm=\"pg\",eos_token=True,optimizer=\"sgd\",lr=1.0)')\n    lr = config.agent.lr\n    tf.reset_default_graph()\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1)\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    with tf.Session() as sess:\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        model = trainer.model\n        global_vars = sess.run(trainer.global_model.trainable_variables)\n        local_vars = sess.run(model.trainable_variables)\n        g_prefix = 'global/'\n        l_prefix = 'local/'\n        for (g, l) in zip(trainer.global_model.trainable_variables, model.trainable_variables):\n            self.assertEqual(g.name[len(g_prefix):], l.name[len(l_prefix):])\n        for (g, l) in zip(global_vars, local_vars):\n            self.assertEqual(g.shape, l.shape)\n            self.assertTrue(np.array_equal(g, l))\n        for (param, grad) in model.gradients_dict.items():\n            if isinstance(grad, tf.IndexedSlices):\n                model.gradients_dict[param] = tf.multiply(grad, 1.0)\n        results = model.update_step(sess, trainer.data_manager.sample_rl_batch(), trainer.train_op, trainer.global_step, return_gradients=True)\n        grads_dict = results.gradients_dict\n        for grad in grads_dict.values():\n            self.assertIsNotNone(grad)\n            self.assertTrue(np.count_nonzero(grad) > 0)\n        global_update = sess.run(trainer.global_model.trainable_variables)\n        for (tf_var, var_before, var_after) in zip(model.trainable_variables, local_vars, global_update):\n            self.assertTrue(np.allclose(var_after, var_before - grads_dict[tf_var] * lr))\n        sess.run(trainer.sync_op)\n        global_vars = sess.run(trainer.global_model.trainable_variables)\n        local_vars = sess.run(model.trainable_variables)\n        for (l, g) in zip(local_vars, global_vars):\n            self.assertTrue(np.allclose(l, g))"
        ]
    },
    {
        "func_name": "sequence_iterator",
        "original": "def sequence_iterator(max_length):\n    \"\"\"Iterates through all sequences up to the given length.\"\"\"\n    yield [eos]\n    for a in xrange(1, num_tokens):\n        if max_length > 1:\n            for sub_seq in sequence_iterator(max_length - 1):\n                yield ([a] + sub_seq)\n        else:\n            yield [a]",
        "mutated": [
            "def sequence_iterator(max_length):\n    if False:\n        i = 10\n    'Iterates through all sequences up to the given length.'\n    yield [eos]\n    for a in xrange(1, num_tokens):\n        if max_length > 1:\n            for sub_seq in sequence_iterator(max_length - 1):\n                yield ([a] + sub_seq)\n        else:\n            yield [a]",
            "def sequence_iterator(max_length):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Iterates through all sequences up to the given length.'\n    yield [eos]\n    for a in xrange(1, num_tokens):\n        if max_length > 1:\n            for sub_seq in sequence_iterator(max_length - 1):\n                yield ([a] + sub_seq)\n        else:\n            yield [a]",
            "def sequence_iterator(max_length):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Iterates through all sequences up to the given length.'\n    yield [eos]\n    for a in xrange(1, num_tokens):\n        if max_length > 1:\n            for sub_seq in sequence_iterator(max_length - 1):\n                yield ([a] + sub_seq)\n        else:\n            yield [a]",
            "def sequence_iterator(max_length):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Iterates through all sequences up to the given length.'\n    yield [eos]\n    for a in xrange(1, num_tokens):\n        if max_length > 1:\n            for sub_seq in sequence_iterator(max_length - 1):\n                yield ([a] + sub_seq)\n        else:\n            yield [a]",
            "def sequence_iterator(max_length):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Iterates through all sequences up to the given length.'\n    yield [eos]\n    for a in xrange(1, num_tokens):\n        if max_length > 1:\n            for sub_seq in sequence_iterator(max_length - 1):\n                yield ([a] + sub_seq)\n        else:\n            yield [a]"
        ]
    },
    {
        "func_name": "testMonteCarloGradients",
        "original": "def testMonteCarloGradients(self):\n    \"\"\"Test Monte Carlo estimate of REINFORCE gradient.\n\n    Test that the Monte Carlo estimate of the REINFORCE gradient is\n    approximately equal to the true gradient. We compute the true gradient for a\n    toy environment with a very small action space.\n\n    Similar to section 5 of https://arxiv.org/pdf/1505.00521.pdf.\n    \"\"\"\n    tf.reset_default_graph()\n    tf.set_random_seed(12345678987654321)\n    np.random.seed(1294024302)\n    max_length = 2\n    num_tokens = misc.bf_num_tokens()\n    eos = misc.BF_EOS_INT\n    assert eos == 0\n\n    def sequence_iterator(max_length):\n        \"\"\"Iterates through all sequences up to the given length.\"\"\"\n        yield [eos]\n        for a in xrange(1, num_tokens):\n            if max_length > 1:\n                for sub_seq in sequence_iterator(max_length - 1):\n                    yield ([a] + sub_seq)\n            else:\n                yield [a]\n    actions = list(sequence_iterator(max_length))\n    actions_batch = utils.stack_pad(actions, 0)\n    lengths_batch = [len(s) for s in actions]\n    reward_map = {tuple(a): np.random.randint(-1, 7) for a in actions_batch}\n    n = 100000\n    config = defaults.default_config_with_updates('env=c(task=\"print\"),agent=c(algorithm=\"pg\",optimizer=\"sgd\",lr=1.0,ema_baseline_decay=0.99,entropy_beta=0.0,topk_loss_hparam=0.0,regularizer=0.0,policy_lstm_sizes=[10],eos_token=True),batch_size=' + str(n) + ',timestep_limit=' + str(max_length))\n    dtype = tf.float64\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1, dtype=dtype)\n    model = trainer.model\n    actions_ph = model.actions\n    lengths_ph = model.adjusted_lengths\n    multipliers_ph = model.policy_multipliers\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    with tf.Session() as sess, sess.graph.as_default():\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        true_loss_unnormalized = 0.0\n        exact_grads = [np.zeros(v.shape) for v in model.trainable_variables]\n        episode_probs_map = {}\n        grads_map = {}\n        for a_idx in xrange(len(actions_batch)):\n            a = actions_batch[a_idx]\n            (grads_result, probs_result, loss) = sess.run([model.dense_unclipped_grads, model.chosen_probs, model.loss], {actions_ph: [a], lengths_ph: [lengths_batch[a_idx]], multipliers_ph: [repeat_and_pad(reward_map[tuple(a)], lengths_batch[a_idx], max_length)]})\n            episode_probs_result = np.prod(probs_result[0, :lengths_batch[a_idx]])\n            for i in range(0, len(exact_grads)):\n                exact_grads[i] += grads_result[i] * episode_probs_result\n            episode_probs_map[tuple(a)] = episode_probs_result\n            reward_map[tuple(a)] = reward_map[tuple(a)]\n            grads_map[tuple(a)] = grads_result\n            true_loss_unnormalized += loss\n        true_loss = true_loss_unnormalized / float(len(actions_batch))\n        (sampled_actions, sampled_lengths) = sess.run([model.sampled_tokens, model.episode_lengths])\n        pi_multipliers = [repeat_and_pad(reward_map[tuple(a)], l, max_length) for (a, l) in zip(sampled_actions, sampled_lengths)]\n        (mc_grads_unnormalized, sampled_probs, mc_loss_unnormalized) = sess.run([model.dense_unclipped_grads, model.chosen_probs, model.loss], {actions_ph: sampled_actions, multipliers_ph: pi_multipliers, lengths_ph: sampled_lengths})\n        mc_grads = mc_grads_unnormalized\n        mc_loss = mc_loss_unnormalized\n    loss_error = smape(true_loss, mc_loss)\n    self.assertTrue(loss_error < 0.15, msg='actual: %s' % loss_error)\n    for i in range(100):\n        acs = tuple(sampled_actions[i].tolist())\n        sampled_prob = np.prod(sampled_probs[i, :sampled_lengths[i]])\n        self.assertTrue(np.isclose(episode_probs_map[acs], sampled_prob))\n    counter = Counter((tuple(e) for e in sampled_actions))\n    for (acs, count) in counter.iteritems():\n        mc_prob = count / float(len(sampled_actions))\n        true_prob = episode_probs_map[acs]\n        error = smape(mc_prob, true_prob)\n        self.assertTrue(error < 0.15, msg='actual: %s; count: %s; mc_prob: %s; true_prob: %s' % (error, count, mc_prob, true_prob))\n    mc_grads_recompute = [np.zeros(v.shape) for v in model.trainable_variables]\n    for i in range(n):\n        acs = tuple(sampled_actions[i].tolist())\n        for i in range(0, len(mc_grads_recompute)):\n            mc_grads_recompute[i] += grads_map[acs][i]\n    for i in range(0, len(mc_grads_recompute)):\n        self.assertTrue(np.allclose(mc_grads[i], mc_grads_recompute[i] / n))\n    for index in range(len(mc_grads)):\n        v1 = mc_grads[index].reshape(-1)\n        v2 = exact_grads[index].reshape(-1)\n        angle_rad = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n        logging.info('angle / pi: %s', angle_rad / np.pi)\n        angle_frac = angle_rad / np.pi\n        self.assertTrue(angle_frac < 0.02, msg='actual: %s' % angle_frac)\n    for index in range(len(mc_grads)):\n        v1_norm = np.linalg.norm(mc_grads[index].reshape(-1))\n        v2_norm = np.linalg.norm(exact_grads[index].reshape(-1))\n        error = smape(v1_norm, v2_norm)\n        self.assertTrue(error < 0.02, msg='actual: %s' % error)\n    mc_expected_reward = np.mean([reward_map[tuple(a)] for a in sampled_actions])\n    exact_expected_reward = np.sum([episode_probs_map[k] * reward_map[k] for k in reward_map])\n    error = smape(mc_expected_reward, exact_expected_reward)\n    self.assertTrue(error < 0.005, msg='actual: %s' % angle_frac)",
        "mutated": [
            "def testMonteCarloGradients(self):\n    if False:\n        i = 10\n    'Test Monte Carlo estimate of REINFORCE gradient.\\n\\n    Test that the Monte Carlo estimate of the REINFORCE gradient is\\n    approximately equal to the true gradient. We compute the true gradient for a\\n    toy environment with a very small action space.\\n\\n    Similar to section 5 of https://arxiv.org/pdf/1505.00521.pdf.\\n    '\n    tf.reset_default_graph()\n    tf.set_random_seed(12345678987654321)\n    np.random.seed(1294024302)\n    max_length = 2\n    num_tokens = misc.bf_num_tokens()\n    eos = misc.BF_EOS_INT\n    assert eos == 0\n\n    def sequence_iterator(max_length):\n        \"\"\"Iterates through all sequences up to the given length.\"\"\"\n        yield [eos]\n        for a in xrange(1, num_tokens):\n            if max_length > 1:\n                for sub_seq in sequence_iterator(max_length - 1):\n                    yield ([a] + sub_seq)\n            else:\n                yield [a]\n    actions = list(sequence_iterator(max_length))\n    actions_batch = utils.stack_pad(actions, 0)\n    lengths_batch = [len(s) for s in actions]\n    reward_map = {tuple(a): np.random.randint(-1, 7) for a in actions_batch}\n    n = 100000\n    config = defaults.default_config_with_updates('env=c(task=\"print\"),agent=c(algorithm=\"pg\",optimizer=\"sgd\",lr=1.0,ema_baseline_decay=0.99,entropy_beta=0.0,topk_loss_hparam=0.0,regularizer=0.0,policy_lstm_sizes=[10],eos_token=True),batch_size=' + str(n) + ',timestep_limit=' + str(max_length))\n    dtype = tf.float64\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1, dtype=dtype)\n    model = trainer.model\n    actions_ph = model.actions\n    lengths_ph = model.adjusted_lengths\n    multipliers_ph = model.policy_multipliers\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    with tf.Session() as sess, sess.graph.as_default():\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        true_loss_unnormalized = 0.0\n        exact_grads = [np.zeros(v.shape) for v in model.trainable_variables]\n        episode_probs_map = {}\n        grads_map = {}\n        for a_idx in xrange(len(actions_batch)):\n            a = actions_batch[a_idx]\n            (grads_result, probs_result, loss) = sess.run([model.dense_unclipped_grads, model.chosen_probs, model.loss], {actions_ph: [a], lengths_ph: [lengths_batch[a_idx]], multipliers_ph: [repeat_and_pad(reward_map[tuple(a)], lengths_batch[a_idx], max_length)]})\n            episode_probs_result = np.prod(probs_result[0, :lengths_batch[a_idx]])\n            for i in range(0, len(exact_grads)):\n                exact_grads[i] += grads_result[i] * episode_probs_result\n            episode_probs_map[tuple(a)] = episode_probs_result\n            reward_map[tuple(a)] = reward_map[tuple(a)]\n            grads_map[tuple(a)] = grads_result\n            true_loss_unnormalized += loss\n        true_loss = true_loss_unnormalized / float(len(actions_batch))\n        (sampled_actions, sampled_lengths) = sess.run([model.sampled_tokens, model.episode_lengths])\n        pi_multipliers = [repeat_and_pad(reward_map[tuple(a)], l, max_length) for (a, l) in zip(sampled_actions, sampled_lengths)]\n        (mc_grads_unnormalized, sampled_probs, mc_loss_unnormalized) = sess.run([model.dense_unclipped_grads, model.chosen_probs, model.loss], {actions_ph: sampled_actions, multipliers_ph: pi_multipliers, lengths_ph: sampled_lengths})\n        mc_grads = mc_grads_unnormalized\n        mc_loss = mc_loss_unnormalized\n    loss_error = smape(true_loss, mc_loss)\n    self.assertTrue(loss_error < 0.15, msg='actual: %s' % loss_error)\n    for i in range(100):\n        acs = tuple(sampled_actions[i].tolist())\n        sampled_prob = np.prod(sampled_probs[i, :sampled_lengths[i]])\n        self.assertTrue(np.isclose(episode_probs_map[acs], sampled_prob))\n    counter = Counter((tuple(e) for e in sampled_actions))\n    for (acs, count) in counter.iteritems():\n        mc_prob = count / float(len(sampled_actions))\n        true_prob = episode_probs_map[acs]\n        error = smape(mc_prob, true_prob)\n        self.assertTrue(error < 0.15, msg='actual: %s; count: %s; mc_prob: %s; true_prob: %s' % (error, count, mc_prob, true_prob))\n    mc_grads_recompute = [np.zeros(v.shape) for v in model.trainable_variables]\n    for i in range(n):\n        acs = tuple(sampled_actions[i].tolist())\n        for i in range(0, len(mc_grads_recompute)):\n            mc_grads_recompute[i] += grads_map[acs][i]\n    for i in range(0, len(mc_grads_recompute)):\n        self.assertTrue(np.allclose(mc_grads[i], mc_grads_recompute[i] / n))\n    for index in range(len(mc_grads)):\n        v1 = mc_grads[index].reshape(-1)\n        v2 = exact_grads[index].reshape(-1)\n        angle_rad = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n        logging.info('angle / pi: %s', angle_rad / np.pi)\n        angle_frac = angle_rad / np.pi\n        self.assertTrue(angle_frac < 0.02, msg='actual: %s' % angle_frac)\n    for index in range(len(mc_grads)):\n        v1_norm = np.linalg.norm(mc_grads[index].reshape(-1))\n        v2_norm = np.linalg.norm(exact_grads[index].reshape(-1))\n        error = smape(v1_norm, v2_norm)\n        self.assertTrue(error < 0.02, msg='actual: %s' % error)\n    mc_expected_reward = np.mean([reward_map[tuple(a)] for a in sampled_actions])\n    exact_expected_reward = np.sum([episode_probs_map[k] * reward_map[k] for k in reward_map])\n    error = smape(mc_expected_reward, exact_expected_reward)\n    self.assertTrue(error < 0.005, msg='actual: %s' % angle_frac)",
            "def testMonteCarloGradients(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Test Monte Carlo estimate of REINFORCE gradient.\\n\\n    Test that the Monte Carlo estimate of the REINFORCE gradient is\\n    approximately equal to the true gradient. We compute the true gradient for a\\n    toy environment with a very small action space.\\n\\n    Similar to section 5 of https://arxiv.org/pdf/1505.00521.pdf.\\n    '\n    tf.reset_default_graph()\n    tf.set_random_seed(12345678987654321)\n    np.random.seed(1294024302)\n    max_length = 2\n    num_tokens = misc.bf_num_tokens()\n    eos = misc.BF_EOS_INT\n    assert eos == 0\n\n    def sequence_iterator(max_length):\n        \"\"\"Iterates through all sequences up to the given length.\"\"\"\n        yield [eos]\n        for a in xrange(1, num_tokens):\n            if max_length > 1:\n                for sub_seq in sequence_iterator(max_length - 1):\n                    yield ([a] + sub_seq)\n            else:\n                yield [a]\n    actions = list(sequence_iterator(max_length))\n    actions_batch = utils.stack_pad(actions, 0)\n    lengths_batch = [len(s) for s in actions]\n    reward_map = {tuple(a): np.random.randint(-1, 7) for a in actions_batch}\n    n = 100000\n    config = defaults.default_config_with_updates('env=c(task=\"print\"),agent=c(algorithm=\"pg\",optimizer=\"sgd\",lr=1.0,ema_baseline_decay=0.99,entropy_beta=0.0,topk_loss_hparam=0.0,regularizer=0.0,policy_lstm_sizes=[10],eos_token=True),batch_size=' + str(n) + ',timestep_limit=' + str(max_length))\n    dtype = tf.float64\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1, dtype=dtype)\n    model = trainer.model\n    actions_ph = model.actions\n    lengths_ph = model.adjusted_lengths\n    multipliers_ph = model.policy_multipliers\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    with tf.Session() as sess, sess.graph.as_default():\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        true_loss_unnormalized = 0.0\n        exact_grads = [np.zeros(v.shape) for v in model.trainable_variables]\n        episode_probs_map = {}\n        grads_map = {}\n        for a_idx in xrange(len(actions_batch)):\n            a = actions_batch[a_idx]\n            (grads_result, probs_result, loss) = sess.run([model.dense_unclipped_grads, model.chosen_probs, model.loss], {actions_ph: [a], lengths_ph: [lengths_batch[a_idx]], multipliers_ph: [repeat_and_pad(reward_map[tuple(a)], lengths_batch[a_idx], max_length)]})\n            episode_probs_result = np.prod(probs_result[0, :lengths_batch[a_idx]])\n            for i in range(0, len(exact_grads)):\n                exact_grads[i] += grads_result[i] * episode_probs_result\n            episode_probs_map[tuple(a)] = episode_probs_result\n            reward_map[tuple(a)] = reward_map[tuple(a)]\n            grads_map[tuple(a)] = grads_result\n            true_loss_unnormalized += loss\n        true_loss = true_loss_unnormalized / float(len(actions_batch))\n        (sampled_actions, sampled_lengths) = sess.run([model.sampled_tokens, model.episode_lengths])\n        pi_multipliers = [repeat_and_pad(reward_map[tuple(a)], l, max_length) for (a, l) in zip(sampled_actions, sampled_lengths)]\n        (mc_grads_unnormalized, sampled_probs, mc_loss_unnormalized) = sess.run([model.dense_unclipped_grads, model.chosen_probs, model.loss], {actions_ph: sampled_actions, multipliers_ph: pi_multipliers, lengths_ph: sampled_lengths})\n        mc_grads = mc_grads_unnormalized\n        mc_loss = mc_loss_unnormalized\n    loss_error = smape(true_loss, mc_loss)\n    self.assertTrue(loss_error < 0.15, msg='actual: %s' % loss_error)\n    for i in range(100):\n        acs = tuple(sampled_actions[i].tolist())\n        sampled_prob = np.prod(sampled_probs[i, :sampled_lengths[i]])\n        self.assertTrue(np.isclose(episode_probs_map[acs], sampled_prob))\n    counter = Counter((tuple(e) for e in sampled_actions))\n    for (acs, count) in counter.iteritems():\n        mc_prob = count / float(len(sampled_actions))\n        true_prob = episode_probs_map[acs]\n        error = smape(mc_prob, true_prob)\n        self.assertTrue(error < 0.15, msg='actual: %s; count: %s; mc_prob: %s; true_prob: %s' % (error, count, mc_prob, true_prob))\n    mc_grads_recompute = [np.zeros(v.shape) for v in model.trainable_variables]\n    for i in range(n):\n        acs = tuple(sampled_actions[i].tolist())\n        for i in range(0, len(mc_grads_recompute)):\n            mc_grads_recompute[i] += grads_map[acs][i]\n    for i in range(0, len(mc_grads_recompute)):\n        self.assertTrue(np.allclose(mc_grads[i], mc_grads_recompute[i] / n))\n    for index in range(len(mc_grads)):\n        v1 = mc_grads[index].reshape(-1)\n        v2 = exact_grads[index].reshape(-1)\n        angle_rad = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n        logging.info('angle / pi: %s', angle_rad / np.pi)\n        angle_frac = angle_rad / np.pi\n        self.assertTrue(angle_frac < 0.02, msg='actual: %s' % angle_frac)\n    for index in range(len(mc_grads)):\n        v1_norm = np.linalg.norm(mc_grads[index].reshape(-1))\n        v2_norm = np.linalg.norm(exact_grads[index].reshape(-1))\n        error = smape(v1_norm, v2_norm)\n        self.assertTrue(error < 0.02, msg='actual: %s' % error)\n    mc_expected_reward = np.mean([reward_map[tuple(a)] for a in sampled_actions])\n    exact_expected_reward = np.sum([episode_probs_map[k] * reward_map[k] for k in reward_map])\n    error = smape(mc_expected_reward, exact_expected_reward)\n    self.assertTrue(error < 0.005, msg='actual: %s' % angle_frac)",
            "def testMonteCarloGradients(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Test Monte Carlo estimate of REINFORCE gradient.\\n\\n    Test that the Monte Carlo estimate of the REINFORCE gradient is\\n    approximately equal to the true gradient. We compute the true gradient for a\\n    toy environment with a very small action space.\\n\\n    Similar to section 5 of https://arxiv.org/pdf/1505.00521.pdf.\\n    '\n    tf.reset_default_graph()\n    tf.set_random_seed(12345678987654321)\n    np.random.seed(1294024302)\n    max_length = 2\n    num_tokens = misc.bf_num_tokens()\n    eos = misc.BF_EOS_INT\n    assert eos == 0\n\n    def sequence_iterator(max_length):\n        \"\"\"Iterates through all sequences up to the given length.\"\"\"\n        yield [eos]\n        for a in xrange(1, num_tokens):\n            if max_length > 1:\n                for sub_seq in sequence_iterator(max_length - 1):\n                    yield ([a] + sub_seq)\n            else:\n                yield [a]\n    actions = list(sequence_iterator(max_length))\n    actions_batch = utils.stack_pad(actions, 0)\n    lengths_batch = [len(s) for s in actions]\n    reward_map = {tuple(a): np.random.randint(-1, 7) for a in actions_batch}\n    n = 100000\n    config = defaults.default_config_with_updates('env=c(task=\"print\"),agent=c(algorithm=\"pg\",optimizer=\"sgd\",lr=1.0,ema_baseline_decay=0.99,entropy_beta=0.0,topk_loss_hparam=0.0,regularizer=0.0,policy_lstm_sizes=[10],eos_token=True),batch_size=' + str(n) + ',timestep_limit=' + str(max_length))\n    dtype = tf.float64\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1, dtype=dtype)\n    model = trainer.model\n    actions_ph = model.actions\n    lengths_ph = model.adjusted_lengths\n    multipliers_ph = model.policy_multipliers\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    with tf.Session() as sess, sess.graph.as_default():\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        true_loss_unnormalized = 0.0\n        exact_grads = [np.zeros(v.shape) for v in model.trainable_variables]\n        episode_probs_map = {}\n        grads_map = {}\n        for a_idx in xrange(len(actions_batch)):\n            a = actions_batch[a_idx]\n            (grads_result, probs_result, loss) = sess.run([model.dense_unclipped_grads, model.chosen_probs, model.loss], {actions_ph: [a], lengths_ph: [lengths_batch[a_idx]], multipliers_ph: [repeat_and_pad(reward_map[tuple(a)], lengths_batch[a_idx], max_length)]})\n            episode_probs_result = np.prod(probs_result[0, :lengths_batch[a_idx]])\n            for i in range(0, len(exact_grads)):\n                exact_grads[i] += grads_result[i] * episode_probs_result\n            episode_probs_map[tuple(a)] = episode_probs_result\n            reward_map[tuple(a)] = reward_map[tuple(a)]\n            grads_map[tuple(a)] = grads_result\n            true_loss_unnormalized += loss\n        true_loss = true_loss_unnormalized / float(len(actions_batch))\n        (sampled_actions, sampled_lengths) = sess.run([model.sampled_tokens, model.episode_lengths])\n        pi_multipliers = [repeat_and_pad(reward_map[tuple(a)], l, max_length) for (a, l) in zip(sampled_actions, sampled_lengths)]\n        (mc_grads_unnormalized, sampled_probs, mc_loss_unnormalized) = sess.run([model.dense_unclipped_grads, model.chosen_probs, model.loss], {actions_ph: sampled_actions, multipliers_ph: pi_multipliers, lengths_ph: sampled_lengths})\n        mc_grads = mc_grads_unnormalized\n        mc_loss = mc_loss_unnormalized\n    loss_error = smape(true_loss, mc_loss)\n    self.assertTrue(loss_error < 0.15, msg='actual: %s' % loss_error)\n    for i in range(100):\n        acs = tuple(sampled_actions[i].tolist())\n        sampled_prob = np.prod(sampled_probs[i, :sampled_lengths[i]])\n        self.assertTrue(np.isclose(episode_probs_map[acs], sampled_prob))\n    counter = Counter((tuple(e) for e in sampled_actions))\n    for (acs, count) in counter.iteritems():\n        mc_prob = count / float(len(sampled_actions))\n        true_prob = episode_probs_map[acs]\n        error = smape(mc_prob, true_prob)\n        self.assertTrue(error < 0.15, msg='actual: %s; count: %s; mc_prob: %s; true_prob: %s' % (error, count, mc_prob, true_prob))\n    mc_grads_recompute = [np.zeros(v.shape) for v in model.trainable_variables]\n    for i in range(n):\n        acs = tuple(sampled_actions[i].tolist())\n        for i in range(0, len(mc_grads_recompute)):\n            mc_grads_recompute[i] += grads_map[acs][i]\n    for i in range(0, len(mc_grads_recompute)):\n        self.assertTrue(np.allclose(mc_grads[i], mc_grads_recompute[i] / n))\n    for index in range(len(mc_grads)):\n        v1 = mc_grads[index].reshape(-1)\n        v2 = exact_grads[index].reshape(-1)\n        angle_rad = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n        logging.info('angle / pi: %s', angle_rad / np.pi)\n        angle_frac = angle_rad / np.pi\n        self.assertTrue(angle_frac < 0.02, msg='actual: %s' % angle_frac)\n    for index in range(len(mc_grads)):\n        v1_norm = np.linalg.norm(mc_grads[index].reshape(-1))\n        v2_norm = np.linalg.norm(exact_grads[index].reshape(-1))\n        error = smape(v1_norm, v2_norm)\n        self.assertTrue(error < 0.02, msg='actual: %s' % error)\n    mc_expected_reward = np.mean([reward_map[tuple(a)] for a in sampled_actions])\n    exact_expected_reward = np.sum([episode_probs_map[k] * reward_map[k] for k in reward_map])\n    error = smape(mc_expected_reward, exact_expected_reward)\n    self.assertTrue(error < 0.005, msg='actual: %s' % angle_frac)",
            "def testMonteCarloGradients(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Test Monte Carlo estimate of REINFORCE gradient.\\n\\n    Test that the Monte Carlo estimate of the REINFORCE gradient is\\n    approximately equal to the true gradient. We compute the true gradient for a\\n    toy environment with a very small action space.\\n\\n    Similar to section 5 of https://arxiv.org/pdf/1505.00521.pdf.\\n    '\n    tf.reset_default_graph()\n    tf.set_random_seed(12345678987654321)\n    np.random.seed(1294024302)\n    max_length = 2\n    num_tokens = misc.bf_num_tokens()\n    eos = misc.BF_EOS_INT\n    assert eos == 0\n\n    def sequence_iterator(max_length):\n        \"\"\"Iterates through all sequences up to the given length.\"\"\"\n        yield [eos]\n        for a in xrange(1, num_tokens):\n            if max_length > 1:\n                for sub_seq in sequence_iterator(max_length - 1):\n                    yield ([a] + sub_seq)\n            else:\n                yield [a]\n    actions = list(sequence_iterator(max_length))\n    actions_batch = utils.stack_pad(actions, 0)\n    lengths_batch = [len(s) for s in actions]\n    reward_map = {tuple(a): np.random.randint(-1, 7) for a in actions_batch}\n    n = 100000\n    config = defaults.default_config_with_updates('env=c(task=\"print\"),agent=c(algorithm=\"pg\",optimizer=\"sgd\",lr=1.0,ema_baseline_decay=0.99,entropy_beta=0.0,topk_loss_hparam=0.0,regularizer=0.0,policy_lstm_sizes=[10],eos_token=True),batch_size=' + str(n) + ',timestep_limit=' + str(max_length))\n    dtype = tf.float64\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1, dtype=dtype)\n    model = trainer.model\n    actions_ph = model.actions\n    lengths_ph = model.adjusted_lengths\n    multipliers_ph = model.policy_multipliers\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    with tf.Session() as sess, sess.graph.as_default():\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        true_loss_unnormalized = 0.0\n        exact_grads = [np.zeros(v.shape) for v in model.trainable_variables]\n        episode_probs_map = {}\n        grads_map = {}\n        for a_idx in xrange(len(actions_batch)):\n            a = actions_batch[a_idx]\n            (grads_result, probs_result, loss) = sess.run([model.dense_unclipped_grads, model.chosen_probs, model.loss], {actions_ph: [a], lengths_ph: [lengths_batch[a_idx]], multipliers_ph: [repeat_and_pad(reward_map[tuple(a)], lengths_batch[a_idx], max_length)]})\n            episode_probs_result = np.prod(probs_result[0, :lengths_batch[a_idx]])\n            for i in range(0, len(exact_grads)):\n                exact_grads[i] += grads_result[i] * episode_probs_result\n            episode_probs_map[tuple(a)] = episode_probs_result\n            reward_map[tuple(a)] = reward_map[tuple(a)]\n            grads_map[tuple(a)] = grads_result\n            true_loss_unnormalized += loss\n        true_loss = true_loss_unnormalized / float(len(actions_batch))\n        (sampled_actions, sampled_lengths) = sess.run([model.sampled_tokens, model.episode_lengths])\n        pi_multipliers = [repeat_and_pad(reward_map[tuple(a)], l, max_length) for (a, l) in zip(sampled_actions, sampled_lengths)]\n        (mc_grads_unnormalized, sampled_probs, mc_loss_unnormalized) = sess.run([model.dense_unclipped_grads, model.chosen_probs, model.loss], {actions_ph: sampled_actions, multipliers_ph: pi_multipliers, lengths_ph: sampled_lengths})\n        mc_grads = mc_grads_unnormalized\n        mc_loss = mc_loss_unnormalized\n    loss_error = smape(true_loss, mc_loss)\n    self.assertTrue(loss_error < 0.15, msg='actual: %s' % loss_error)\n    for i in range(100):\n        acs = tuple(sampled_actions[i].tolist())\n        sampled_prob = np.prod(sampled_probs[i, :sampled_lengths[i]])\n        self.assertTrue(np.isclose(episode_probs_map[acs], sampled_prob))\n    counter = Counter((tuple(e) for e in sampled_actions))\n    for (acs, count) in counter.iteritems():\n        mc_prob = count / float(len(sampled_actions))\n        true_prob = episode_probs_map[acs]\n        error = smape(mc_prob, true_prob)\n        self.assertTrue(error < 0.15, msg='actual: %s; count: %s; mc_prob: %s; true_prob: %s' % (error, count, mc_prob, true_prob))\n    mc_grads_recompute = [np.zeros(v.shape) for v in model.trainable_variables]\n    for i in range(n):\n        acs = tuple(sampled_actions[i].tolist())\n        for i in range(0, len(mc_grads_recompute)):\n            mc_grads_recompute[i] += grads_map[acs][i]\n    for i in range(0, len(mc_grads_recompute)):\n        self.assertTrue(np.allclose(mc_grads[i], mc_grads_recompute[i] / n))\n    for index in range(len(mc_grads)):\n        v1 = mc_grads[index].reshape(-1)\n        v2 = exact_grads[index].reshape(-1)\n        angle_rad = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n        logging.info('angle / pi: %s', angle_rad / np.pi)\n        angle_frac = angle_rad / np.pi\n        self.assertTrue(angle_frac < 0.02, msg='actual: %s' % angle_frac)\n    for index in range(len(mc_grads)):\n        v1_norm = np.linalg.norm(mc_grads[index].reshape(-1))\n        v2_norm = np.linalg.norm(exact_grads[index].reshape(-1))\n        error = smape(v1_norm, v2_norm)\n        self.assertTrue(error < 0.02, msg='actual: %s' % error)\n    mc_expected_reward = np.mean([reward_map[tuple(a)] for a in sampled_actions])\n    exact_expected_reward = np.sum([episode_probs_map[k] * reward_map[k] for k in reward_map])\n    error = smape(mc_expected_reward, exact_expected_reward)\n    self.assertTrue(error < 0.005, msg='actual: %s' % angle_frac)",
            "def testMonteCarloGradients(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Test Monte Carlo estimate of REINFORCE gradient.\\n\\n    Test that the Monte Carlo estimate of the REINFORCE gradient is\\n    approximately equal to the true gradient. We compute the true gradient for a\\n    toy environment with a very small action space.\\n\\n    Similar to section 5 of https://arxiv.org/pdf/1505.00521.pdf.\\n    '\n    tf.reset_default_graph()\n    tf.set_random_seed(12345678987654321)\n    np.random.seed(1294024302)\n    max_length = 2\n    num_tokens = misc.bf_num_tokens()\n    eos = misc.BF_EOS_INT\n    assert eos == 0\n\n    def sequence_iterator(max_length):\n        \"\"\"Iterates through all sequences up to the given length.\"\"\"\n        yield [eos]\n        for a in xrange(1, num_tokens):\n            if max_length > 1:\n                for sub_seq in sequence_iterator(max_length - 1):\n                    yield ([a] + sub_seq)\n            else:\n                yield [a]\n    actions = list(sequence_iterator(max_length))\n    actions_batch = utils.stack_pad(actions, 0)\n    lengths_batch = [len(s) for s in actions]\n    reward_map = {tuple(a): np.random.randint(-1, 7) for a in actions_batch}\n    n = 100000\n    config = defaults.default_config_with_updates('env=c(task=\"print\"),agent=c(algorithm=\"pg\",optimizer=\"sgd\",lr=1.0,ema_baseline_decay=0.99,entropy_beta=0.0,topk_loss_hparam=0.0,regularizer=0.0,policy_lstm_sizes=[10],eos_token=True),batch_size=' + str(n) + ',timestep_limit=' + str(max_length))\n    dtype = tf.float64\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1, dtype=dtype)\n    model = trainer.model\n    actions_ph = model.actions\n    lengths_ph = model.adjusted_lengths\n    multipliers_ph = model.policy_multipliers\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    with tf.Session() as sess, sess.graph.as_default():\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        true_loss_unnormalized = 0.0\n        exact_grads = [np.zeros(v.shape) for v in model.trainable_variables]\n        episode_probs_map = {}\n        grads_map = {}\n        for a_idx in xrange(len(actions_batch)):\n            a = actions_batch[a_idx]\n            (grads_result, probs_result, loss) = sess.run([model.dense_unclipped_grads, model.chosen_probs, model.loss], {actions_ph: [a], lengths_ph: [lengths_batch[a_idx]], multipliers_ph: [repeat_and_pad(reward_map[tuple(a)], lengths_batch[a_idx], max_length)]})\n            episode_probs_result = np.prod(probs_result[0, :lengths_batch[a_idx]])\n            for i in range(0, len(exact_grads)):\n                exact_grads[i] += grads_result[i] * episode_probs_result\n            episode_probs_map[tuple(a)] = episode_probs_result\n            reward_map[tuple(a)] = reward_map[tuple(a)]\n            grads_map[tuple(a)] = grads_result\n            true_loss_unnormalized += loss\n        true_loss = true_loss_unnormalized / float(len(actions_batch))\n        (sampled_actions, sampled_lengths) = sess.run([model.sampled_tokens, model.episode_lengths])\n        pi_multipliers = [repeat_and_pad(reward_map[tuple(a)], l, max_length) for (a, l) in zip(sampled_actions, sampled_lengths)]\n        (mc_grads_unnormalized, sampled_probs, mc_loss_unnormalized) = sess.run([model.dense_unclipped_grads, model.chosen_probs, model.loss], {actions_ph: sampled_actions, multipliers_ph: pi_multipliers, lengths_ph: sampled_lengths})\n        mc_grads = mc_grads_unnormalized\n        mc_loss = mc_loss_unnormalized\n    loss_error = smape(true_loss, mc_loss)\n    self.assertTrue(loss_error < 0.15, msg='actual: %s' % loss_error)\n    for i in range(100):\n        acs = tuple(sampled_actions[i].tolist())\n        sampled_prob = np.prod(sampled_probs[i, :sampled_lengths[i]])\n        self.assertTrue(np.isclose(episode_probs_map[acs], sampled_prob))\n    counter = Counter((tuple(e) for e in sampled_actions))\n    for (acs, count) in counter.iteritems():\n        mc_prob = count / float(len(sampled_actions))\n        true_prob = episode_probs_map[acs]\n        error = smape(mc_prob, true_prob)\n        self.assertTrue(error < 0.15, msg='actual: %s; count: %s; mc_prob: %s; true_prob: %s' % (error, count, mc_prob, true_prob))\n    mc_grads_recompute = [np.zeros(v.shape) for v in model.trainable_variables]\n    for i in range(n):\n        acs = tuple(sampled_actions[i].tolist())\n        for i in range(0, len(mc_grads_recompute)):\n            mc_grads_recompute[i] += grads_map[acs][i]\n    for i in range(0, len(mc_grads_recompute)):\n        self.assertTrue(np.allclose(mc_grads[i], mc_grads_recompute[i] / n))\n    for index in range(len(mc_grads)):\n        v1 = mc_grads[index].reshape(-1)\n        v2 = exact_grads[index].reshape(-1)\n        angle_rad = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n        logging.info('angle / pi: %s', angle_rad / np.pi)\n        angle_frac = angle_rad / np.pi\n        self.assertTrue(angle_frac < 0.02, msg='actual: %s' % angle_frac)\n    for index in range(len(mc_grads)):\n        v1_norm = np.linalg.norm(mc_grads[index].reshape(-1))\n        v2_norm = np.linalg.norm(exact_grads[index].reshape(-1))\n        error = smape(v1_norm, v2_norm)\n        self.assertTrue(error < 0.02, msg='actual: %s' % error)\n    mc_expected_reward = np.mean([reward_map[tuple(a)] for a in sampled_actions])\n    exact_expected_reward = np.sum([episode_probs_map[k] * reward_map[k] for k in reward_map])\n    error = smape(mc_expected_reward, exact_expected_reward)\n    self.assertTrue(error < 0.005, msg='actual: %s' % angle_frac)"
        ]
    },
    {
        "func_name": "testNumericalGradChecking",
        "original": "def testNumericalGradChecking(self):\n    epsilon = 0.0001\n    eos = misc.BF_EOS_INT\n    self.assertEqual(0, eos)\n    config = defaults.default_config_with_updates('env=c(task=\"print\"),agent=c(algorithm=\"pg\",optimizer=\"sgd\",lr=1.0,ema_baseline_decay=0.99,entropy_beta=0.0,topk_loss_hparam=0.0,policy_lstm_sizes=[10],eos_token=True),batch_size=64')\n    dtype = tf.float64\n    tf.reset_default_graph()\n    tf.set_random_seed(12345678987654321)\n    np.random.seed(1294024302)\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1, dtype=dtype)\n    model = trainer.model\n    actions_ph = model.actions\n    lengths_ph = model.adjusted_lengths\n    multipliers_ph = model.policy_multipliers\n    loss = model.pi_loss\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    assign_add_placeholders = [None] * len(model.trainable_variables)\n    assign_add_ops = [None] * len(model.trainable_variables)\n    param_shapes = [None] * len(model.trainable_variables)\n    for (i, param) in enumerate(model.trainable_variables):\n        param_shapes[i] = param.get_shape().as_list()\n        assign_add_placeholders[i] = tf.placeholder(dtype, np.prod(param_shapes[i]))\n        assign_add_ops[i] = param.assign_add(tf.reshape(assign_add_placeholders[i], param_shapes[i]))\n    with tf.Session() as sess:\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        actions_raw = [random_sequence(10, 9) for _ in xrange(16)]\n        actions_batch = utils.stack_pad(actions_raw, 0)\n        lengths_batch = [len(l) for l in actions_raw]\n        feed = {actions_ph: actions_batch, multipliers_ph: np.ones_like(actions_batch), lengths_ph: lengths_batch}\n        estimated_grads = [None] * len(model.trainable_variables)\n        for (i, param) in enumerate(model.trainable_variables):\n            param_size = np.prod(param_shapes[i])\n            estimated_grads[i] = np.zeros(param_size, dtype=np.float64)\n            for index in xrange(param_size):\n                e = onehot(index, param_size) * epsilon\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: e})\n                j_plus = sess.run(loss, feed)\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: -2 * e})\n                j_minus = sess.run(loss, feed)\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: e})\n                estimated_grads[i][index] = (j_plus - j_minus) / (2 * epsilon)\n            estimated_grads[i] = estimated_grads[i].reshape(param_shapes[i])\n        analytic_grads = sess.run(model.dense_unclipped_grads, feed)\n        for (g1, g2) in zip(estimated_grads[1:], analytic_grads[1:]):\n            logging.info('norm (g1-g2): %s', np.abs(g1 - g2).mean())\n            self.assertTrue(np.allclose(g1, g2))",
        "mutated": [
            "def testNumericalGradChecking(self):\n    if False:\n        i = 10\n    epsilon = 0.0001\n    eos = misc.BF_EOS_INT\n    self.assertEqual(0, eos)\n    config = defaults.default_config_with_updates('env=c(task=\"print\"),agent=c(algorithm=\"pg\",optimizer=\"sgd\",lr=1.0,ema_baseline_decay=0.99,entropy_beta=0.0,topk_loss_hparam=0.0,policy_lstm_sizes=[10],eos_token=True),batch_size=64')\n    dtype = tf.float64\n    tf.reset_default_graph()\n    tf.set_random_seed(12345678987654321)\n    np.random.seed(1294024302)\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1, dtype=dtype)\n    model = trainer.model\n    actions_ph = model.actions\n    lengths_ph = model.adjusted_lengths\n    multipliers_ph = model.policy_multipliers\n    loss = model.pi_loss\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    assign_add_placeholders = [None] * len(model.trainable_variables)\n    assign_add_ops = [None] * len(model.trainable_variables)\n    param_shapes = [None] * len(model.trainable_variables)\n    for (i, param) in enumerate(model.trainable_variables):\n        param_shapes[i] = param.get_shape().as_list()\n        assign_add_placeholders[i] = tf.placeholder(dtype, np.prod(param_shapes[i]))\n        assign_add_ops[i] = param.assign_add(tf.reshape(assign_add_placeholders[i], param_shapes[i]))\n    with tf.Session() as sess:\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        actions_raw = [random_sequence(10, 9) for _ in xrange(16)]\n        actions_batch = utils.stack_pad(actions_raw, 0)\n        lengths_batch = [len(l) for l in actions_raw]\n        feed = {actions_ph: actions_batch, multipliers_ph: np.ones_like(actions_batch), lengths_ph: lengths_batch}\n        estimated_grads = [None] * len(model.trainable_variables)\n        for (i, param) in enumerate(model.trainable_variables):\n            param_size = np.prod(param_shapes[i])\n            estimated_grads[i] = np.zeros(param_size, dtype=np.float64)\n            for index in xrange(param_size):\n                e = onehot(index, param_size) * epsilon\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: e})\n                j_plus = sess.run(loss, feed)\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: -2 * e})\n                j_minus = sess.run(loss, feed)\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: e})\n                estimated_grads[i][index] = (j_plus - j_minus) / (2 * epsilon)\n            estimated_grads[i] = estimated_grads[i].reshape(param_shapes[i])\n        analytic_grads = sess.run(model.dense_unclipped_grads, feed)\n        for (g1, g2) in zip(estimated_grads[1:], analytic_grads[1:]):\n            logging.info('norm (g1-g2): %s', np.abs(g1 - g2).mean())\n            self.assertTrue(np.allclose(g1, g2))",
            "def testNumericalGradChecking(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    epsilon = 0.0001\n    eos = misc.BF_EOS_INT\n    self.assertEqual(0, eos)\n    config = defaults.default_config_with_updates('env=c(task=\"print\"),agent=c(algorithm=\"pg\",optimizer=\"sgd\",lr=1.0,ema_baseline_decay=0.99,entropy_beta=0.0,topk_loss_hparam=0.0,policy_lstm_sizes=[10],eos_token=True),batch_size=64')\n    dtype = tf.float64\n    tf.reset_default_graph()\n    tf.set_random_seed(12345678987654321)\n    np.random.seed(1294024302)\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1, dtype=dtype)\n    model = trainer.model\n    actions_ph = model.actions\n    lengths_ph = model.adjusted_lengths\n    multipliers_ph = model.policy_multipliers\n    loss = model.pi_loss\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    assign_add_placeholders = [None] * len(model.trainable_variables)\n    assign_add_ops = [None] * len(model.trainable_variables)\n    param_shapes = [None] * len(model.trainable_variables)\n    for (i, param) in enumerate(model.trainable_variables):\n        param_shapes[i] = param.get_shape().as_list()\n        assign_add_placeholders[i] = tf.placeholder(dtype, np.prod(param_shapes[i]))\n        assign_add_ops[i] = param.assign_add(tf.reshape(assign_add_placeholders[i], param_shapes[i]))\n    with tf.Session() as sess:\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        actions_raw = [random_sequence(10, 9) for _ in xrange(16)]\n        actions_batch = utils.stack_pad(actions_raw, 0)\n        lengths_batch = [len(l) for l in actions_raw]\n        feed = {actions_ph: actions_batch, multipliers_ph: np.ones_like(actions_batch), lengths_ph: lengths_batch}\n        estimated_grads = [None] * len(model.trainable_variables)\n        for (i, param) in enumerate(model.trainable_variables):\n            param_size = np.prod(param_shapes[i])\n            estimated_grads[i] = np.zeros(param_size, dtype=np.float64)\n            for index in xrange(param_size):\n                e = onehot(index, param_size) * epsilon\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: e})\n                j_plus = sess.run(loss, feed)\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: -2 * e})\n                j_minus = sess.run(loss, feed)\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: e})\n                estimated_grads[i][index] = (j_plus - j_minus) / (2 * epsilon)\n            estimated_grads[i] = estimated_grads[i].reshape(param_shapes[i])\n        analytic_grads = sess.run(model.dense_unclipped_grads, feed)\n        for (g1, g2) in zip(estimated_grads[1:], analytic_grads[1:]):\n            logging.info('norm (g1-g2): %s', np.abs(g1 - g2).mean())\n            self.assertTrue(np.allclose(g1, g2))",
            "def testNumericalGradChecking(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    epsilon = 0.0001\n    eos = misc.BF_EOS_INT\n    self.assertEqual(0, eos)\n    config = defaults.default_config_with_updates('env=c(task=\"print\"),agent=c(algorithm=\"pg\",optimizer=\"sgd\",lr=1.0,ema_baseline_decay=0.99,entropy_beta=0.0,topk_loss_hparam=0.0,policy_lstm_sizes=[10],eos_token=True),batch_size=64')\n    dtype = tf.float64\n    tf.reset_default_graph()\n    tf.set_random_seed(12345678987654321)\n    np.random.seed(1294024302)\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1, dtype=dtype)\n    model = trainer.model\n    actions_ph = model.actions\n    lengths_ph = model.adjusted_lengths\n    multipliers_ph = model.policy_multipliers\n    loss = model.pi_loss\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    assign_add_placeholders = [None] * len(model.trainable_variables)\n    assign_add_ops = [None] * len(model.trainable_variables)\n    param_shapes = [None] * len(model.trainable_variables)\n    for (i, param) in enumerate(model.trainable_variables):\n        param_shapes[i] = param.get_shape().as_list()\n        assign_add_placeholders[i] = tf.placeholder(dtype, np.prod(param_shapes[i]))\n        assign_add_ops[i] = param.assign_add(tf.reshape(assign_add_placeholders[i], param_shapes[i]))\n    with tf.Session() as sess:\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        actions_raw = [random_sequence(10, 9) for _ in xrange(16)]\n        actions_batch = utils.stack_pad(actions_raw, 0)\n        lengths_batch = [len(l) for l in actions_raw]\n        feed = {actions_ph: actions_batch, multipliers_ph: np.ones_like(actions_batch), lengths_ph: lengths_batch}\n        estimated_grads = [None] * len(model.trainable_variables)\n        for (i, param) in enumerate(model.trainable_variables):\n            param_size = np.prod(param_shapes[i])\n            estimated_grads[i] = np.zeros(param_size, dtype=np.float64)\n            for index in xrange(param_size):\n                e = onehot(index, param_size) * epsilon\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: e})\n                j_plus = sess.run(loss, feed)\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: -2 * e})\n                j_minus = sess.run(loss, feed)\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: e})\n                estimated_grads[i][index] = (j_plus - j_minus) / (2 * epsilon)\n            estimated_grads[i] = estimated_grads[i].reshape(param_shapes[i])\n        analytic_grads = sess.run(model.dense_unclipped_grads, feed)\n        for (g1, g2) in zip(estimated_grads[1:], analytic_grads[1:]):\n            logging.info('norm (g1-g2): %s', np.abs(g1 - g2).mean())\n            self.assertTrue(np.allclose(g1, g2))",
            "def testNumericalGradChecking(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    epsilon = 0.0001\n    eos = misc.BF_EOS_INT\n    self.assertEqual(0, eos)\n    config = defaults.default_config_with_updates('env=c(task=\"print\"),agent=c(algorithm=\"pg\",optimizer=\"sgd\",lr=1.0,ema_baseline_decay=0.99,entropy_beta=0.0,topk_loss_hparam=0.0,policy_lstm_sizes=[10],eos_token=True),batch_size=64')\n    dtype = tf.float64\n    tf.reset_default_graph()\n    tf.set_random_seed(12345678987654321)\n    np.random.seed(1294024302)\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1, dtype=dtype)\n    model = trainer.model\n    actions_ph = model.actions\n    lengths_ph = model.adjusted_lengths\n    multipliers_ph = model.policy_multipliers\n    loss = model.pi_loss\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    assign_add_placeholders = [None] * len(model.trainable_variables)\n    assign_add_ops = [None] * len(model.trainable_variables)\n    param_shapes = [None] * len(model.trainable_variables)\n    for (i, param) in enumerate(model.trainable_variables):\n        param_shapes[i] = param.get_shape().as_list()\n        assign_add_placeholders[i] = tf.placeholder(dtype, np.prod(param_shapes[i]))\n        assign_add_ops[i] = param.assign_add(tf.reshape(assign_add_placeholders[i], param_shapes[i]))\n    with tf.Session() as sess:\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        actions_raw = [random_sequence(10, 9) for _ in xrange(16)]\n        actions_batch = utils.stack_pad(actions_raw, 0)\n        lengths_batch = [len(l) for l in actions_raw]\n        feed = {actions_ph: actions_batch, multipliers_ph: np.ones_like(actions_batch), lengths_ph: lengths_batch}\n        estimated_grads = [None] * len(model.trainable_variables)\n        for (i, param) in enumerate(model.trainable_variables):\n            param_size = np.prod(param_shapes[i])\n            estimated_grads[i] = np.zeros(param_size, dtype=np.float64)\n            for index in xrange(param_size):\n                e = onehot(index, param_size) * epsilon\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: e})\n                j_plus = sess.run(loss, feed)\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: -2 * e})\n                j_minus = sess.run(loss, feed)\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: e})\n                estimated_grads[i][index] = (j_plus - j_minus) / (2 * epsilon)\n            estimated_grads[i] = estimated_grads[i].reshape(param_shapes[i])\n        analytic_grads = sess.run(model.dense_unclipped_grads, feed)\n        for (g1, g2) in zip(estimated_grads[1:], analytic_grads[1:]):\n            logging.info('norm (g1-g2): %s', np.abs(g1 - g2).mean())\n            self.assertTrue(np.allclose(g1, g2))",
            "def testNumericalGradChecking(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    epsilon = 0.0001\n    eos = misc.BF_EOS_INT\n    self.assertEqual(0, eos)\n    config = defaults.default_config_with_updates('env=c(task=\"print\"),agent=c(algorithm=\"pg\",optimizer=\"sgd\",lr=1.0,ema_baseline_decay=0.99,entropy_beta=0.0,topk_loss_hparam=0.0,policy_lstm_sizes=[10],eos_token=True),batch_size=64')\n    dtype = tf.float64\n    tf.reset_default_graph()\n    tf.set_random_seed(12345678987654321)\n    np.random.seed(1294024302)\n    trainer = pg_train.AsyncTrainer(config, task_id=0, ps_tasks=0, num_workers=1, dtype=dtype)\n    model = trainer.model\n    actions_ph = model.actions\n    lengths_ph = model.adjusted_lengths\n    multipliers_ph = model.policy_multipliers\n    loss = model.pi_loss\n    global_init_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'global'))\n    assign_add_placeholders = [None] * len(model.trainable_variables)\n    assign_add_ops = [None] * len(model.trainable_variables)\n    param_shapes = [None] * len(model.trainable_variables)\n    for (i, param) in enumerate(model.trainable_variables):\n        param_shapes[i] = param.get_shape().as_list()\n        assign_add_placeholders[i] = tf.placeholder(dtype, np.prod(param_shapes[i]))\n        assign_add_ops[i] = param.assign_add(tf.reshape(assign_add_placeholders[i], param_shapes[i]))\n    with tf.Session() as sess:\n        sess.run(global_init_op)\n        trainer.initialize(sess)\n        actions_raw = [random_sequence(10, 9) for _ in xrange(16)]\n        actions_batch = utils.stack_pad(actions_raw, 0)\n        lengths_batch = [len(l) for l in actions_raw]\n        feed = {actions_ph: actions_batch, multipliers_ph: np.ones_like(actions_batch), lengths_ph: lengths_batch}\n        estimated_grads = [None] * len(model.trainable_variables)\n        for (i, param) in enumerate(model.trainable_variables):\n            param_size = np.prod(param_shapes[i])\n            estimated_grads[i] = np.zeros(param_size, dtype=np.float64)\n            for index in xrange(param_size):\n                e = onehot(index, param_size) * epsilon\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: e})\n                j_plus = sess.run(loss, feed)\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: -2 * e})\n                j_minus = sess.run(loss, feed)\n                sess.run(assign_add_ops[i], {assign_add_placeholders[i]: e})\n                estimated_grads[i][index] = (j_plus - j_minus) / (2 * epsilon)\n            estimated_grads[i] = estimated_grads[i].reshape(param_shapes[i])\n        analytic_grads = sess.run(model.dense_unclipped_grads, feed)\n        for (g1, g2) in zip(estimated_grads[1:], analytic_grads[1:]):\n            logging.info('norm (g1-g2): %s', np.abs(g1 - g2).mean())\n            self.assertTrue(np.allclose(g1, g2))"
        ]
    }
]