[
    {
        "func_name": "compute",
        "original": "def compute(regid, InType, use_weights, isa, prefetch):\n    code = []\n    if InType == 'float':\n        code.append('        vop%d = _mm256_fmadd_ps(vwgt, _mm256_loadu_ps(ip + (%d)), vop%d);' % (regid, regid, regid))\n    elif InType == 'at::Half':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtph_ps(\\n                _mm_loadu_si128(reinterpret_cast<const __m128i*>(ip + (%d)))),\\n            vop%d);' % (regid, regid, regid))\n    elif InType == 'at::BFloat16':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_castsi256_ps(_mm256_slli_epi32(\\n                _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                    reinterpret_cast<const __m128i*>(ip + (%d)))),\\n                16)),\\n            vop%d);' % (regid, regid, regid))\n    elif InType == 'uint8_t':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(\\n                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (%d))))),\\n            _mm256_add_ps(vop%d, vbio));' % (regid, regid, regid))\n    else:\n        assert False\n    if prefetch:\n        code.append('        _mm_prefetch(\\n            reinterpret_cast<const char*>(&ip_next_T0[%d]), _MM_HINT_T0);' % regid)\n    else:\n        code.append('        // skip unnecessary prefetch of (&ip_next_T0[%d])' % regid)\n    return code",
        "mutated": [
            "def compute(regid, InType, use_weights, isa, prefetch):\n    if False:\n        i = 10\n    code = []\n    if InType == 'float':\n        code.append('        vop%d = _mm256_fmadd_ps(vwgt, _mm256_loadu_ps(ip + (%d)), vop%d);' % (regid, regid, regid))\n    elif InType == 'at::Half':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtph_ps(\\n                _mm_loadu_si128(reinterpret_cast<const __m128i*>(ip + (%d)))),\\n            vop%d);' % (regid, regid, regid))\n    elif InType == 'at::BFloat16':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_castsi256_ps(_mm256_slli_epi32(\\n                _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                    reinterpret_cast<const __m128i*>(ip + (%d)))),\\n                16)),\\n            vop%d);' % (regid, regid, regid))\n    elif InType == 'uint8_t':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(\\n                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (%d))))),\\n            _mm256_add_ps(vop%d, vbio));' % (regid, regid, regid))\n    else:\n        assert False\n    if prefetch:\n        code.append('        _mm_prefetch(\\n            reinterpret_cast<const char*>(&ip_next_T0[%d]), _MM_HINT_T0);' % regid)\n    else:\n        code.append('        // skip unnecessary prefetch of (&ip_next_T0[%d])' % regid)\n    return code",
            "def compute(regid, InType, use_weights, isa, prefetch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    code = []\n    if InType == 'float':\n        code.append('        vop%d = _mm256_fmadd_ps(vwgt, _mm256_loadu_ps(ip + (%d)), vop%d);' % (regid, regid, regid))\n    elif InType == 'at::Half':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtph_ps(\\n                _mm_loadu_si128(reinterpret_cast<const __m128i*>(ip + (%d)))),\\n            vop%d);' % (regid, regid, regid))\n    elif InType == 'at::BFloat16':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_castsi256_ps(_mm256_slli_epi32(\\n                _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                    reinterpret_cast<const __m128i*>(ip + (%d)))),\\n                16)),\\n            vop%d);' % (regid, regid, regid))\n    elif InType == 'uint8_t':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(\\n                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (%d))))),\\n            _mm256_add_ps(vop%d, vbio));' % (regid, regid, regid))\n    else:\n        assert False\n    if prefetch:\n        code.append('        _mm_prefetch(\\n            reinterpret_cast<const char*>(&ip_next_T0[%d]), _MM_HINT_T0);' % regid)\n    else:\n        code.append('        // skip unnecessary prefetch of (&ip_next_T0[%d])' % regid)\n    return code",
            "def compute(regid, InType, use_weights, isa, prefetch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    code = []\n    if InType == 'float':\n        code.append('        vop%d = _mm256_fmadd_ps(vwgt, _mm256_loadu_ps(ip + (%d)), vop%d);' % (regid, regid, regid))\n    elif InType == 'at::Half':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtph_ps(\\n                _mm_loadu_si128(reinterpret_cast<const __m128i*>(ip + (%d)))),\\n            vop%d);' % (regid, regid, regid))\n    elif InType == 'at::BFloat16':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_castsi256_ps(_mm256_slli_epi32(\\n                _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                    reinterpret_cast<const __m128i*>(ip + (%d)))),\\n                16)),\\n            vop%d);' % (regid, regid, regid))\n    elif InType == 'uint8_t':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(\\n                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (%d))))),\\n            _mm256_add_ps(vop%d, vbio));' % (regid, regid, regid))\n    else:\n        assert False\n    if prefetch:\n        code.append('        _mm_prefetch(\\n            reinterpret_cast<const char*>(&ip_next_T0[%d]), _MM_HINT_T0);' % regid)\n    else:\n        code.append('        // skip unnecessary prefetch of (&ip_next_T0[%d])' % regid)\n    return code",
            "def compute(regid, InType, use_weights, isa, prefetch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    code = []\n    if InType == 'float':\n        code.append('        vop%d = _mm256_fmadd_ps(vwgt, _mm256_loadu_ps(ip + (%d)), vop%d);' % (regid, regid, regid))\n    elif InType == 'at::Half':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtph_ps(\\n                _mm_loadu_si128(reinterpret_cast<const __m128i*>(ip + (%d)))),\\n            vop%d);' % (regid, regid, regid))\n    elif InType == 'at::BFloat16':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_castsi256_ps(_mm256_slli_epi32(\\n                _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                    reinterpret_cast<const __m128i*>(ip + (%d)))),\\n                16)),\\n            vop%d);' % (regid, regid, regid))\n    elif InType == 'uint8_t':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(\\n                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (%d))))),\\n            _mm256_add_ps(vop%d, vbio));' % (regid, regid, regid))\n    else:\n        assert False\n    if prefetch:\n        code.append('        _mm_prefetch(\\n            reinterpret_cast<const char*>(&ip_next_T0[%d]), _MM_HINT_T0);' % regid)\n    else:\n        code.append('        // skip unnecessary prefetch of (&ip_next_T0[%d])' % regid)\n    return code",
            "def compute(regid, InType, use_weights, isa, prefetch):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    code = []\n    if InType == 'float':\n        code.append('        vop%d = _mm256_fmadd_ps(vwgt, _mm256_loadu_ps(ip + (%d)), vop%d);' % (regid, regid, regid))\n    elif InType == 'at::Half':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtph_ps(\\n                _mm_loadu_si128(reinterpret_cast<const __m128i*>(ip + (%d)))),\\n            vop%d);' % (regid, regid, regid))\n    elif InType == 'at::BFloat16':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_castsi256_ps(_mm256_slli_epi32(\\n                _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                    reinterpret_cast<const __m128i*>(ip + (%d)))),\\n                16)),\\n            vop%d);' % (regid, regid, regid))\n    elif InType == 'uint8_t':\n        code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(\\n                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (%d))))),\\n            _mm256_add_ps(vop%d, vbio));' % (regid, regid, regid))\n    else:\n        assert False\n    if prefetch:\n        code.append('        _mm_prefetch(\\n            reinterpret_cast<const char*>(&ip_next_T0[%d]), _MM_HINT_T0);' % regid)\n    else:\n        code.append('        // skip unnecessary prefetch of (&ip_next_T0[%d])' % regid)\n    return code"
        ]
    },
    {
        "func_name": "unroll",
        "original": "def unroll(uf, IndexType, InType, OutType, use_weights, isa, fused, use_offsets):\n\n    def compute(regid, InType, use_weights, isa, prefetch):\n        code = []\n        if InType == 'float':\n            code.append('        vop%d = _mm256_fmadd_ps(vwgt, _mm256_loadu_ps(ip + (%d)), vop%d);' % (regid, regid, regid))\n        elif InType == 'at::Half':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtph_ps(\\n                _mm_loadu_si128(reinterpret_cast<const __m128i*>(ip + (%d)))),\\n            vop%d);' % (regid, regid, regid))\n        elif InType == 'at::BFloat16':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_castsi256_ps(_mm256_slli_epi32(\\n                _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                    reinterpret_cast<const __m128i*>(ip + (%d)))),\\n                16)),\\n            vop%d);' % (regid, regid, regid))\n        elif InType == 'uint8_t':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(\\n                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (%d))))),\\n            _mm256_add_ps(vop%d, vbio));' % (regid, regid, regid))\n        else:\n            assert False\n        if prefetch:\n            code.append('        _mm_prefetch(\\n            reinterpret_cast<const char*>(&ip_next_T0[%d]), _MM_HINT_T0);' % regid)\n        else:\n            code.append('        // skip unnecessary prefetch of (&ip_next_T0[%d])' % regid)\n        return code\n    code = []\n    code.append('    // unrolling ' + str(uf) + ' times')\n    if use_offsets:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    else:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    code.append('      ' + OutType + '* op = &out[rangeIndex * block_size];')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('      __m256 vop' + str(j) + ' = _mm256_setzero_ps();')\n    if use_offsets:\n        code.append('      if (dataInd != offsets[rangeIndex] - offsets[0]) {\\n' + '        return false;\\n' + '      }')\n        code.append('      int64_t end_offset = offsets[rangeIndex + 1];\\n      int64_t length = end_offset - offsets[rangeIndex];')\n        code.append('      for (' + 'int64_t' + ' start = dataInd; dataInd < end_offset - offsets[0];\\n           ++dataInd) {')\n    else:\n        code.append('      if (dataInd + lengths[rangeIndex] > index_size) {\\n' + '        return false;\\n' + '      }')\n        code.append('      for (' + IndexType + ' start = dataInd; dataInd < start + lengths[rangeIndex];\\n           ++dataInd) {')\n    code.append('        const ' + IndexType + ' idx = indices[dataInd];')\n    code.append('        if (idx < 0 || idx >= data_size) {\\n' + '          return false;\\n' + '        }')\n    if InType == 'uint8_t':\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        // NOLINTNEXTLINE(cppcoreguidelines-init-variables)')\n        code.append('        ' + OutType + ' bio;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n        if fused:\n            code.append('        const float* scale_bias = reinterpret_cast<const float*>(\\n            &input[idx * fused_block_size + block_size]);')\n            code.append('        bio = wgt * scale_bias[1];')\n            code.append('        wgt = wgt * scale_bias[0];')\n        else:\n            code.append('        bio = wgt * scale_bias[2 * idx + 1];')\n            code.append('        wgt = wgt * scale_bias[2 * idx];')\n        code.append('        __m256 vbio = _mm256_set1_ps(bio);')\n    else:\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n    code.append('        __m256 vwgt = _mm256_set1_ps(wgt);')\n    code.append('        const {}* ip = &input[idx * fused_block_size];'.format(InType))\n    code.append('        const {} next_T0 = (dataInd < index_size - prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            ? (dataInd + prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            : dataInd;'.format(IndexType))\n    code.append('        const ' + IndexType + ' idx_pref_T0 = indices[next_T0];')\n    code.append('        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {\\n' + '          return false;\\n' + '        }')\n    code.append('        const {}* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];'.format(InType))\n    for i in range(0, uf):\n        j = 8 * i\n        cachelinesize = 64\n        byteoffset = sizeof[InType] * j\n        prefetch = byteoffset % cachelinesize == 0\n        code.extend(compute(j, InType, use_weights, isa, prefetch))\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (!normalize_by_lengths || length == 0) {')\n    else:\n        code.append('      if (!normalize_by_lengths || lengths[rangeIndex] == 0) {')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('        _mm256_storeu_ps(&op[' + str(j) + '], vop' + str(j) + ');')\n    code.append('      } else {')\n    if use_offsets:\n        code.append('        __m256 vlen_inv = _mm256_set1_ps(1.0f / length);')\n    else:\n        code.append('        __m256 vlen_inv = _mm256_set1_ps(1.0f / lengths[rangeIndex]);')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('        _mm256_storeu_ps(&op[' + str(j) + '], _mm256_mul_ps(' + 'vop' + str(j) + ', vlen_inv));')\n    code.append('      }')\n    code.append('    }')\n    return code",
        "mutated": [
            "def unroll(uf, IndexType, InType, OutType, use_weights, isa, fused, use_offsets):\n    if False:\n        i = 10\n\n    def compute(regid, InType, use_weights, isa, prefetch):\n        code = []\n        if InType == 'float':\n            code.append('        vop%d = _mm256_fmadd_ps(vwgt, _mm256_loadu_ps(ip + (%d)), vop%d);' % (regid, regid, regid))\n        elif InType == 'at::Half':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtph_ps(\\n                _mm_loadu_si128(reinterpret_cast<const __m128i*>(ip + (%d)))),\\n            vop%d);' % (regid, regid, regid))\n        elif InType == 'at::BFloat16':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_castsi256_ps(_mm256_slli_epi32(\\n                _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                    reinterpret_cast<const __m128i*>(ip + (%d)))),\\n                16)),\\n            vop%d);' % (regid, regid, regid))\n        elif InType == 'uint8_t':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(\\n                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (%d))))),\\n            _mm256_add_ps(vop%d, vbio));' % (regid, regid, regid))\n        else:\n            assert False\n        if prefetch:\n            code.append('        _mm_prefetch(\\n            reinterpret_cast<const char*>(&ip_next_T0[%d]), _MM_HINT_T0);' % regid)\n        else:\n            code.append('        // skip unnecessary prefetch of (&ip_next_T0[%d])' % regid)\n        return code\n    code = []\n    code.append('    // unrolling ' + str(uf) + ' times')\n    if use_offsets:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    else:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    code.append('      ' + OutType + '* op = &out[rangeIndex * block_size];')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('      __m256 vop' + str(j) + ' = _mm256_setzero_ps();')\n    if use_offsets:\n        code.append('      if (dataInd != offsets[rangeIndex] - offsets[0]) {\\n' + '        return false;\\n' + '      }')\n        code.append('      int64_t end_offset = offsets[rangeIndex + 1];\\n      int64_t length = end_offset - offsets[rangeIndex];')\n        code.append('      for (' + 'int64_t' + ' start = dataInd; dataInd < end_offset - offsets[0];\\n           ++dataInd) {')\n    else:\n        code.append('      if (dataInd + lengths[rangeIndex] > index_size) {\\n' + '        return false;\\n' + '      }')\n        code.append('      for (' + IndexType + ' start = dataInd; dataInd < start + lengths[rangeIndex];\\n           ++dataInd) {')\n    code.append('        const ' + IndexType + ' idx = indices[dataInd];')\n    code.append('        if (idx < 0 || idx >= data_size) {\\n' + '          return false;\\n' + '        }')\n    if InType == 'uint8_t':\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        // NOLINTNEXTLINE(cppcoreguidelines-init-variables)')\n        code.append('        ' + OutType + ' bio;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n        if fused:\n            code.append('        const float* scale_bias = reinterpret_cast<const float*>(\\n            &input[idx * fused_block_size + block_size]);')\n            code.append('        bio = wgt * scale_bias[1];')\n            code.append('        wgt = wgt * scale_bias[0];')\n        else:\n            code.append('        bio = wgt * scale_bias[2 * idx + 1];')\n            code.append('        wgt = wgt * scale_bias[2 * idx];')\n        code.append('        __m256 vbio = _mm256_set1_ps(bio);')\n    else:\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n    code.append('        __m256 vwgt = _mm256_set1_ps(wgt);')\n    code.append('        const {}* ip = &input[idx * fused_block_size];'.format(InType))\n    code.append('        const {} next_T0 = (dataInd < index_size - prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            ? (dataInd + prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            : dataInd;'.format(IndexType))\n    code.append('        const ' + IndexType + ' idx_pref_T0 = indices[next_T0];')\n    code.append('        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {\\n' + '          return false;\\n' + '        }')\n    code.append('        const {}* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];'.format(InType))\n    for i in range(0, uf):\n        j = 8 * i\n        cachelinesize = 64\n        byteoffset = sizeof[InType] * j\n        prefetch = byteoffset % cachelinesize == 0\n        code.extend(compute(j, InType, use_weights, isa, prefetch))\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (!normalize_by_lengths || length == 0) {')\n    else:\n        code.append('      if (!normalize_by_lengths || lengths[rangeIndex] == 0) {')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('        _mm256_storeu_ps(&op[' + str(j) + '], vop' + str(j) + ');')\n    code.append('      } else {')\n    if use_offsets:\n        code.append('        __m256 vlen_inv = _mm256_set1_ps(1.0f / length);')\n    else:\n        code.append('        __m256 vlen_inv = _mm256_set1_ps(1.0f / lengths[rangeIndex]);')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('        _mm256_storeu_ps(&op[' + str(j) + '], _mm256_mul_ps(' + 'vop' + str(j) + ', vlen_inv));')\n    code.append('      }')\n    code.append('    }')\n    return code",
            "def unroll(uf, IndexType, InType, OutType, use_weights, isa, fused, use_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def compute(regid, InType, use_weights, isa, prefetch):\n        code = []\n        if InType == 'float':\n            code.append('        vop%d = _mm256_fmadd_ps(vwgt, _mm256_loadu_ps(ip + (%d)), vop%d);' % (regid, regid, regid))\n        elif InType == 'at::Half':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtph_ps(\\n                _mm_loadu_si128(reinterpret_cast<const __m128i*>(ip + (%d)))),\\n            vop%d);' % (regid, regid, regid))\n        elif InType == 'at::BFloat16':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_castsi256_ps(_mm256_slli_epi32(\\n                _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                    reinterpret_cast<const __m128i*>(ip + (%d)))),\\n                16)),\\n            vop%d);' % (regid, regid, regid))\n        elif InType == 'uint8_t':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(\\n                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (%d))))),\\n            _mm256_add_ps(vop%d, vbio));' % (regid, regid, regid))\n        else:\n            assert False\n        if prefetch:\n            code.append('        _mm_prefetch(\\n            reinterpret_cast<const char*>(&ip_next_T0[%d]), _MM_HINT_T0);' % regid)\n        else:\n            code.append('        // skip unnecessary prefetch of (&ip_next_T0[%d])' % regid)\n        return code\n    code = []\n    code.append('    // unrolling ' + str(uf) + ' times')\n    if use_offsets:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    else:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    code.append('      ' + OutType + '* op = &out[rangeIndex * block_size];')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('      __m256 vop' + str(j) + ' = _mm256_setzero_ps();')\n    if use_offsets:\n        code.append('      if (dataInd != offsets[rangeIndex] - offsets[0]) {\\n' + '        return false;\\n' + '      }')\n        code.append('      int64_t end_offset = offsets[rangeIndex + 1];\\n      int64_t length = end_offset - offsets[rangeIndex];')\n        code.append('      for (' + 'int64_t' + ' start = dataInd; dataInd < end_offset - offsets[0];\\n           ++dataInd) {')\n    else:\n        code.append('      if (dataInd + lengths[rangeIndex] > index_size) {\\n' + '        return false;\\n' + '      }')\n        code.append('      for (' + IndexType + ' start = dataInd; dataInd < start + lengths[rangeIndex];\\n           ++dataInd) {')\n    code.append('        const ' + IndexType + ' idx = indices[dataInd];')\n    code.append('        if (idx < 0 || idx >= data_size) {\\n' + '          return false;\\n' + '        }')\n    if InType == 'uint8_t':\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        // NOLINTNEXTLINE(cppcoreguidelines-init-variables)')\n        code.append('        ' + OutType + ' bio;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n        if fused:\n            code.append('        const float* scale_bias = reinterpret_cast<const float*>(\\n            &input[idx * fused_block_size + block_size]);')\n            code.append('        bio = wgt * scale_bias[1];')\n            code.append('        wgt = wgt * scale_bias[0];')\n        else:\n            code.append('        bio = wgt * scale_bias[2 * idx + 1];')\n            code.append('        wgt = wgt * scale_bias[2 * idx];')\n        code.append('        __m256 vbio = _mm256_set1_ps(bio);')\n    else:\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n    code.append('        __m256 vwgt = _mm256_set1_ps(wgt);')\n    code.append('        const {}* ip = &input[idx * fused_block_size];'.format(InType))\n    code.append('        const {} next_T0 = (dataInd < index_size - prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            ? (dataInd + prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            : dataInd;'.format(IndexType))\n    code.append('        const ' + IndexType + ' idx_pref_T0 = indices[next_T0];')\n    code.append('        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {\\n' + '          return false;\\n' + '        }')\n    code.append('        const {}* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];'.format(InType))\n    for i in range(0, uf):\n        j = 8 * i\n        cachelinesize = 64\n        byteoffset = sizeof[InType] * j\n        prefetch = byteoffset % cachelinesize == 0\n        code.extend(compute(j, InType, use_weights, isa, prefetch))\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (!normalize_by_lengths || length == 0) {')\n    else:\n        code.append('      if (!normalize_by_lengths || lengths[rangeIndex] == 0) {')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('        _mm256_storeu_ps(&op[' + str(j) + '], vop' + str(j) + ');')\n    code.append('      } else {')\n    if use_offsets:\n        code.append('        __m256 vlen_inv = _mm256_set1_ps(1.0f / length);')\n    else:\n        code.append('        __m256 vlen_inv = _mm256_set1_ps(1.0f / lengths[rangeIndex]);')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('        _mm256_storeu_ps(&op[' + str(j) + '], _mm256_mul_ps(' + 'vop' + str(j) + ', vlen_inv));')\n    code.append('      }')\n    code.append('    }')\n    return code",
            "def unroll(uf, IndexType, InType, OutType, use_weights, isa, fused, use_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def compute(regid, InType, use_weights, isa, prefetch):\n        code = []\n        if InType == 'float':\n            code.append('        vop%d = _mm256_fmadd_ps(vwgt, _mm256_loadu_ps(ip + (%d)), vop%d);' % (regid, regid, regid))\n        elif InType == 'at::Half':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtph_ps(\\n                _mm_loadu_si128(reinterpret_cast<const __m128i*>(ip + (%d)))),\\n            vop%d);' % (regid, regid, regid))\n        elif InType == 'at::BFloat16':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_castsi256_ps(_mm256_slli_epi32(\\n                _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                    reinterpret_cast<const __m128i*>(ip + (%d)))),\\n                16)),\\n            vop%d);' % (regid, regid, regid))\n        elif InType == 'uint8_t':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(\\n                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (%d))))),\\n            _mm256_add_ps(vop%d, vbio));' % (regid, regid, regid))\n        else:\n            assert False\n        if prefetch:\n            code.append('        _mm_prefetch(\\n            reinterpret_cast<const char*>(&ip_next_T0[%d]), _MM_HINT_T0);' % regid)\n        else:\n            code.append('        // skip unnecessary prefetch of (&ip_next_T0[%d])' % regid)\n        return code\n    code = []\n    code.append('    // unrolling ' + str(uf) + ' times')\n    if use_offsets:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    else:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    code.append('      ' + OutType + '* op = &out[rangeIndex * block_size];')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('      __m256 vop' + str(j) + ' = _mm256_setzero_ps();')\n    if use_offsets:\n        code.append('      if (dataInd != offsets[rangeIndex] - offsets[0]) {\\n' + '        return false;\\n' + '      }')\n        code.append('      int64_t end_offset = offsets[rangeIndex + 1];\\n      int64_t length = end_offset - offsets[rangeIndex];')\n        code.append('      for (' + 'int64_t' + ' start = dataInd; dataInd < end_offset - offsets[0];\\n           ++dataInd) {')\n    else:\n        code.append('      if (dataInd + lengths[rangeIndex] > index_size) {\\n' + '        return false;\\n' + '      }')\n        code.append('      for (' + IndexType + ' start = dataInd; dataInd < start + lengths[rangeIndex];\\n           ++dataInd) {')\n    code.append('        const ' + IndexType + ' idx = indices[dataInd];')\n    code.append('        if (idx < 0 || idx >= data_size) {\\n' + '          return false;\\n' + '        }')\n    if InType == 'uint8_t':\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        // NOLINTNEXTLINE(cppcoreguidelines-init-variables)')\n        code.append('        ' + OutType + ' bio;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n        if fused:\n            code.append('        const float* scale_bias = reinterpret_cast<const float*>(\\n            &input[idx * fused_block_size + block_size]);')\n            code.append('        bio = wgt * scale_bias[1];')\n            code.append('        wgt = wgt * scale_bias[0];')\n        else:\n            code.append('        bio = wgt * scale_bias[2 * idx + 1];')\n            code.append('        wgt = wgt * scale_bias[2 * idx];')\n        code.append('        __m256 vbio = _mm256_set1_ps(bio);')\n    else:\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n    code.append('        __m256 vwgt = _mm256_set1_ps(wgt);')\n    code.append('        const {}* ip = &input[idx * fused_block_size];'.format(InType))\n    code.append('        const {} next_T0 = (dataInd < index_size - prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            ? (dataInd + prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            : dataInd;'.format(IndexType))\n    code.append('        const ' + IndexType + ' idx_pref_T0 = indices[next_T0];')\n    code.append('        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {\\n' + '          return false;\\n' + '        }')\n    code.append('        const {}* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];'.format(InType))\n    for i in range(0, uf):\n        j = 8 * i\n        cachelinesize = 64\n        byteoffset = sizeof[InType] * j\n        prefetch = byteoffset % cachelinesize == 0\n        code.extend(compute(j, InType, use_weights, isa, prefetch))\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (!normalize_by_lengths || length == 0) {')\n    else:\n        code.append('      if (!normalize_by_lengths || lengths[rangeIndex] == 0) {')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('        _mm256_storeu_ps(&op[' + str(j) + '], vop' + str(j) + ');')\n    code.append('      } else {')\n    if use_offsets:\n        code.append('        __m256 vlen_inv = _mm256_set1_ps(1.0f / length);')\n    else:\n        code.append('        __m256 vlen_inv = _mm256_set1_ps(1.0f / lengths[rangeIndex]);')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('        _mm256_storeu_ps(&op[' + str(j) + '], _mm256_mul_ps(' + 'vop' + str(j) + ', vlen_inv));')\n    code.append('      }')\n    code.append('    }')\n    return code",
            "def unroll(uf, IndexType, InType, OutType, use_weights, isa, fused, use_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def compute(regid, InType, use_weights, isa, prefetch):\n        code = []\n        if InType == 'float':\n            code.append('        vop%d = _mm256_fmadd_ps(vwgt, _mm256_loadu_ps(ip + (%d)), vop%d);' % (regid, regid, regid))\n        elif InType == 'at::Half':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtph_ps(\\n                _mm_loadu_si128(reinterpret_cast<const __m128i*>(ip + (%d)))),\\n            vop%d);' % (regid, regid, regid))\n        elif InType == 'at::BFloat16':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_castsi256_ps(_mm256_slli_epi32(\\n                _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                    reinterpret_cast<const __m128i*>(ip + (%d)))),\\n                16)),\\n            vop%d);' % (regid, regid, regid))\n        elif InType == 'uint8_t':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(\\n                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (%d))))),\\n            _mm256_add_ps(vop%d, vbio));' % (regid, regid, regid))\n        else:\n            assert False\n        if prefetch:\n            code.append('        _mm_prefetch(\\n            reinterpret_cast<const char*>(&ip_next_T0[%d]), _MM_HINT_T0);' % regid)\n        else:\n            code.append('        // skip unnecessary prefetch of (&ip_next_T0[%d])' % regid)\n        return code\n    code = []\n    code.append('    // unrolling ' + str(uf) + ' times')\n    if use_offsets:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    else:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    code.append('      ' + OutType + '* op = &out[rangeIndex * block_size];')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('      __m256 vop' + str(j) + ' = _mm256_setzero_ps();')\n    if use_offsets:\n        code.append('      if (dataInd != offsets[rangeIndex] - offsets[0]) {\\n' + '        return false;\\n' + '      }')\n        code.append('      int64_t end_offset = offsets[rangeIndex + 1];\\n      int64_t length = end_offset - offsets[rangeIndex];')\n        code.append('      for (' + 'int64_t' + ' start = dataInd; dataInd < end_offset - offsets[0];\\n           ++dataInd) {')\n    else:\n        code.append('      if (dataInd + lengths[rangeIndex] > index_size) {\\n' + '        return false;\\n' + '      }')\n        code.append('      for (' + IndexType + ' start = dataInd; dataInd < start + lengths[rangeIndex];\\n           ++dataInd) {')\n    code.append('        const ' + IndexType + ' idx = indices[dataInd];')\n    code.append('        if (idx < 0 || idx >= data_size) {\\n' + '          return false;\\n' + '        }')\n    if InType == 'uint8_t':\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        // NOLINTNEXTLINE(cppcoreguidelines-init-variables)')\n        code.append('        ' + OutType + ' bio;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n        if fused:\n            code.append('        const float* scale_bias = reinterpret_cast<const float*>(\\n            &input[idx * fused_block_size + block_size]);')\n            code.append('        bio = wgt * scale_bias[1];')\n            code.append('        wgt = wgt * scale_bias[0];')\n        else:\n            code.append('        bio = wgt * scale_bias[2 * idx + 1];')\n            code.append('        wgt = wgt * scale_bias[2 * idx];')\n        code.append('        __m256 vbio = _mm256_set1_ps(bio);')\n    else:\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n    code.append('        __m256 vwgt = _mm256_set1_ps(wgt);')\n    code.append('        const {}* ip = &input[idx * fused_block_size];'.format(InType))\n    code.append('        const {} next_T0 = (dataInd < index_size - prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            ? (dataInd + prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            : dataInd;'.format(IndexType))\n    code.append('        const ' + IndexType + ' idx_pref_T0 = indices[next_T0];')\n    code.append('        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {\\n' + '          return false;\\n' + '        }')\n    code.append('        const {}* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];'.format(InType))\n    for i in range(0, uf):\n        j = 8 * i\n        cachelinesize = 64\n        byteoffset = sizeof[InType] * j\n        prefetch = byteoffset % cachelinesize == 0\n        code.extend(compute(j, InType, use_weights, isa, prefetch))\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (!normalize_by_lengths || length == 0) {')\n    else:\n        code.append('      if (!normalize_by_lengths || lengths[rangeIndex] == 0) {')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('        _mm256_storeu_ps(&op[' + str(j) + '], vop' + str(j) + ');')\n    code.append('      } else {')\n    if use_offsets:\n        code.append('        __m256 vlen_inv = _mm256_set1_ps(1.0f / length);')\n    else:\n        code.append('        __m256 vlen_inv = _mm256_set1_ps(1.0f / lengths[rangeIndex]);')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('        _mm256_storeu_ps(&op[' + str(j) + '], _mm256_mul_ps(' + 'vop' + str(j) + ', vlen_inv));')\n    code.append('      }')\n    code.append('    }')\n    return code",
            "def unroll(uf, IndexType, InType, OutType, use_weights, isa, fused, use_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def compute(regid, InType, use_weights, isa, prefetch):\n        code = []\n        if InType == 'float':\n            code.append('        vop%d = _mm256_fmadd_ps(vwgt, _mm256_loadu_ps(ip + (%d)), vop%d);' % (regid, regid, regid))\n        elif InType == 'at::Half':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtph_ps(\\n                _mm_loadu_si128(reinterpret_cast<const __m128i*>(ip + (%d)))),\\n            vop%d);' % (regid, regid, regid))\n        elif InType == 'at::BFloat16':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_castsi256_ps(_mm256_slli_epi32(\\n                _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                    reinterpret_cast<const __m128i*>(ip + (%d)))),\\n                16)),\\n            vop%d);' % (regid, regid, regid))\n        elif InType == 'uint8_t':\n            code.append('        vop%d = _mm256_fmadd_ps(\\n            vwgt,\\n            _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(\\n                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (%d))))),\\n            _mm256_add_ps(vop%d, vbio));' % (regid, regid, regid))\n        else:\n            assert False\n        if prefetch:\n            code.append('        _mm_prefetch(\\n            reinterpret_cast<const char*>(&ip_next_T0[%d]), _MM_HINT_T0);' % regid)\n        else:\n            code.append('        // skip unnecessary prefetch of (&ip_next_T0[%d])' % regid)\n        return code\n    code = []\n    code.append('    // unrolling ' + str(uf) + ' times')\n    if use_offsets:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    else:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    code.append('      ' + OutType + '* op = &out[rangeIndex * block_size];')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('      __m256 vop' + str(j) + ' = _mm256_setzero_ps();')\n    if use_offsets:\n        code.append('      if (dataInd != offsets[rangeIndex] - offsets[0]) {\\n' + '        return false;\\n' + '      }')\n        code.append('      int64_t end_offset = offsets[rangeIndex + 1];\\n      int64_t length = end_offset - offsets[rangeIndex];')\n        code.append('      for (' + 'int64_t' + ' start = dataInd; dataInd < end_offset - offsets[0];\\n           ++dataInd) {')\n    else:\n        code.append('      if (dataInd + lengths[rangeIndex] > index_size) {\\n' + '        return false;\\n' + '      }')\n        code.append('      for (' + IndexType + ' start = dataInd; dataInd < start + lengths[rangeIndex];\\n           ++dataInd) {')\n    code.append('        const ' + IndexType + ' idx = indices[dataInd];')\n    code.append('        if (idx < 0 || idx >= data_size) {\\n' + '          return false;\\n' + '        }')\n    if InType == 'uint8_t':\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        // NOLINTNEXTLINE(cppcoreguidelines-init-variables)')\n        code.append('        ' + OutType + ' bio;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n        if fused:\n            code.append('        const float* scale_bias = reinterpret_cast<const float*>(\\n            &input[idx * fused_block_size + block_size]);')\n            code.append('        bio = wgt * scale_bias[1];')\n            code.append('        wgt = wgt * scale_bias[0];')\n        else:\n            code.append('        bio = wgt * scale_bias[2 * idx + 1];')\n            code.append('        wgt = wgt * scale_bias[2 * idx];')\n        code.append('        __m256 vbio = _mm256_set1_ps(bio);')\n    else:\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n    code.append('        __m256 vwgt = _mm256_set1_ps(wgt);')\n    code.append('        const {}* ip = &input[idx * fused_block_size];'.format(InType))\n    code.append('        const {} next_T0 = (dataInd < index_size - prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            ? (dataInd + prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            : dataInd;'.format(IndexType))\n    code.append('        const ' + IndexType + ' idx_pref_T0 = indices[next_T0];')\n    code.append('        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {\\n' + '          return false;\\n' + '        }')\n    code.append('        const {}* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];'.format(InType))\n    for i in range(0, uf):\n        j = 8 * i\n        cachelinesize = 64\n        byteoffset = sizeof[InType] * j\n        prefetch = byteoffset % cachelinesize == 0\n        code.extend(compute(j, InType, use_weights, isa, prefetch))\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (!normalize_by_lengths || length == 0) {')\n    else:\n        code.append('      if (!normalize_by_lengths || lengths[rangeIndex] == 0) {')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('        _mm256_storeu_ps(&op[' + str(j) + '], vop' + str(j) + ');')\n    code.append('      } else {')\n    if use_offsets:\n        code.append('        __m256 vlen_inv = _mm256_set1_ps(1.0f / length);')\n    else:\n        code.append('        __m256 vlen_inv = _mm256_set1_ps(1.0f / lengths[rangeIndex]);')\n    for i in range(0, uf):\n        j = 8 * i\n        code.append('        _mm256_storeu_ps(&op[' + str(j) + '], _mm256_mul_ps(' + 'vop' + str(j) + ', vlen_inv));')\n    code.append('      }')\n    code.append('    }')\n    return code"
        ]
    },
    {
        "func_name": "compute",
        "original": "def compute(InType, use_weights, isa):\n    code = []\n    if InType == 'float':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt, _mm256_loadu_ps(&ip[j]), _mm256_loadu_ps(&op[j])));')\n    elif InType == 'at::Half':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtph_ps(_mm_loadu_si128(\\n                      reinterpret_cast<const __m128i*>(&ip[j]))),\\n                  _mm256_loadu_ps(&op[j])));')\n    elif InType == 'at::BFloat16':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_castsi256_ps(_mm256_slli_epi32(\\n                      _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                          reinterpret_cast<const __m128i*>(&ip[j]))),\\n                      16)),\\n                  _mm256_loadu_ps(&op[j])));')\n    elif InType == 'uint8_t':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(_mm_loadl_epi64(\\n                      reinterpret_cast<const __m128i*>(&ip[j])))),\\n                  _mm256_add_ps(_mm256_loadu_ps(&op[j]), vbio)));')\n    else:\n        assert False\n    code.append('          _mm_prefetch(\\n              reinterpret_cast<const char*>(&ip_next_T0[j]), _MM_HINT_T0);')\n    return code",
        "mutated": [
            "def compute(InType, use_weights, isa):\n    if False:\n        i = 10\n    code = []\n    if InType == 'float':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt, _mm256_loadu_ps(&ip[j]), _mm256_loadu_ps(&op[j])));')\n    elif InType == 'at::Half':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtph_ps(_mm_loadu_si128(\\n                      reinterpret_cast<const __m128i*>(&ip[j]))),\\n                  _mm256_loadu_ps(&op[j])));')\n    elif InType == 'at::BFloat16':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_castsi256_ps(_mm256_slli_epi32(\\n                      _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                          reinterpret_cast<const __m128i*>(&ip[j]))),\\n                      16)),\\n                  _mm256_loadu_ps(&op[j])));')\n    elif InType == 'uint8_t':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(_mm_loadl_epi64(\\n                      reinterpret_cast<const __m128i*>(&ip[j])))),\\n                  _mm256_add_ps(_mm256_loadu_ps(&op[j]), vbio)));')\n    else:\n        assert False\n    code.append('          _mm_prefetch(\\n              reinterpret_cast<const char*>(&ip_next_T0[j]), _MM_HINT_T0);')\n    return code",
            "def compute(InType, use_weights, isa):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    code = []\n    if InType == 'float':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt, _mm256_loadu_ps(&ip[j]), _mm256_loadu_ps(&op[j])));')\n    elif InType == 'at::Half':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtph_ps(_mm_loadu_si128(\\n                      reinterpret_cast<const __m128i*>(&ip[j]))),\\n                  _mm256_loadu_ps(&op[j])));')\n    elif InType == 'at::BFloat16':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_castsi256_ps(_mm256_slli_epi32(\\n                      _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                          reinterpret_cast<const __m128i*>(&ip[j]))),\\n                      16)),\\n                  _mm256_loadu_ps(&op[j])));')\n    elif InType == 'uint8_t':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(_mm_loadl_epi64(\\n                      reinterpret_cast<const __m128i*>(&ip[j])))),\\n                  _mm256_add_ps(_mm256_loadu_ps(&op[j]), vbio)));')\n    else:\n        assert False\n    code.append('          _mm_prefetch(\\n              reinterpret_cast<const char*>(&ip_next_T0[j]), _MM_HINT_T0);')\n    return code",
            "def compute(InType, use_weights, isa):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    code = []\n    if InType == 'float':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt, _mm256_loadu_ps(&ip[j]), _mm256_loadu_ps(&op[j])));')\n    elif InType == 'at::Half':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtph_ps(_mm_loadu_si128(\\n                      reinterpret_cast<const __m128i*>(&ip[j]))),\\n                  _mm256_loadu_ps(&op[j])));')\n    elif InType == 'at::BFloat16':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_castsi256_ps(_mm256_slli_epi32(\\n                      _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                          reinterpret_cast<const __m128i*>(&ip[j]))),\\n                      16)),\\n                  _mm256_loadu_ps(&op[j])));')\n    elif InType == 'uint8_t':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(_mm_loadl_epi64(\\n                      reinterpret_cast<const __m128i*>(&ip[j])))),\\n                  _mm256_add_ps(_mm256_loadu_ps(&op[j]), vbio)));')\n    else:\n        assert False\n    code.append('          _mm_prefetch(\\n              reinterpret_cast<const char*>(&ip_next_T0[j]), _MM_HINT_T0);')\n    return code",
            "def compute(InType, use_weights, isa):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    code = []\n    if InType == 'float':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt, _mm256_loadu_ps(&ip[j]), _mm256_loadu_ps(&op[j])));')\n    elif InType == 'at::Half':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtph_ps(_mm_loadu_si128(\\n                      reinterpret_cast<const __m128i*>(&ip[j]))),\\n                  _mm256_loadu_ps(&op[j])));')\n    elif InType == 'at::BFloat16':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_castsi256_ps(_mm256_slli_epi32(\\n                      _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                          reinterpret_cast<const __m128i*>(&ip[j]))),\\n                      16)),\\n                  _mm256_loadu_ps(&op[j])));')\n    elif InType == 'uint8_t':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(_mm_loadl_epi64(\\n                      reinterpret_cast<const __m128i*>(&ip[j])))),\\n                  _mm256_add_ps(_mm256_loadu_ps(&op[j]), vbio)));')\n    else:\n        assert False\n    code.append('          _mm_prefetch(\\n              reinterpret_cast<const char*>(&ip_next_T0[j]), _MM_HINT_T0);')\n    return code",
            "def compute(InType, use_weights, isa):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    code = []\n    if InType == 'float':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt, _mm256_loadu_ps(&ip[j]), _mm256_loadu_ps(&op[j])));')\n    elif InType == 'at::Half':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtph_ps(_mm_loadu_si128(\\n                      reinterpret_cast<const __m128i*>(&ip[j]))),\\n                  _mm256_loadu_ps(&op[j])));')\n    elif InType == 'at::BFloat16':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_castsi256_ps(_mm256_slli_epi32(\\n                      _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                          reinterpret_cast<const __m128i*>(&ip[j]))),\\n                      16)),\\n                  _mm256_loadu_ps(&op[j])));')\n    elif InType == 'uint8_t':\n        code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(_mm_loadl_epi64(\\n                      reinterpret_cast<const __m128i*>(&ip[j])))),\\n                  _mm256_add_ps(_mm256_loadu_ps(&op[j]), vbio)));')\n    else:\n        assert False\n    code.append('          _mm_prefetch(\\n              reinterpret_cast<const char*>(&ip_next_T0[j]), _MM_HINT_T0);')\n    return code"
        ]
    },
    {
        "func_name": "generic",
        "original": "def generic(IndexType, InType, OutType, use_weights, isa, fused, use_offsets):\n\n    def compute(InType, use_weights, isa):\n        code = []\n        if InType == 'float':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt, _mm256_loadu_ps(&ip[j]), _mm256_loadu_ps(&op[j])));')\n        elif InType == 'at::Half':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtph_ps(_mm_loadu_si128(\\n                      reinterpret_cast<const __m128i*>(&ip[j]))),\\n                  _mm256_loadu_ps(&op[j])));')\n        elif InType == 'at::BFloat16':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_castsi256_ps(_mm256_slli_epi32(\\n                      _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                          reinterpret_cast<const __m128i*>(&ip[j]))),\\n                      16)),\\n                  _mm256_loadu_ps(&op[j])));')\n        elif InType == 'uint8_t':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(_mm_loadl_epi64(\\n                      reinterpret_cast<const __m128i*>(&ip[j])))),\\n                  _mm256_add_ps(_mm256_loadu_ps(&op[j]), vbio)));')\n        else:\n            assert False\n        code.append('          _mm_prefetch(\\n              reinterpret_cast<const char*>(&ip_next_T0[j]), _MM_HINT_T0);')\n        return code\n    code = []\n    if InType == 'at::Half':\n        code.append('    alignas(64) at::Half vtmp1[8] = {0};')\n    if InType == 'at::BFloat16':\n        code.append('    alignas(64) at::BFloat16 vtmp1[8] = {0};')\n    if use_offsets:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    else:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    code.append('      ' + OutType + '* op = &out[rangeIndex * block_size];')\n    code.append('      int64_t j = 0;')\n    code.append('      for (; j + 8 <= block_size; j += 8) {')\n    code.append('        _mm256_storeu_ps(op + j, _mm256_setzero_ps());')\n    code.append('      }')\n    code.append('      for (; j < block_size; j++) {')\n    code.append('        op[j] = 0.0f;')\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (dataInd != offsets[rangeIndex] - offsets[0]) {\\n' + '        return false;\\n' + '      }')\n        code.append('      int64_t end_offset = offsets[rangeIndex + 1];\\n      int64_t length = end_offset - offsets[rangeIndex];')\n        code.append('      for (' + 'int64_t' + ' start = dataInd; dataInd < end_offset - offsets[0];\\n           ++dataInd) {')\n    else:\n        code.append('      if (dataInd + lengths[rangeIndex] > index_size) {\\n' + '        return false;\\n' + '      }')\n        code.append('      for (' + IndexType + ' start = dataInd; dataInd < start + lengths[rangeIndex];\\n           ++dataInd) {')\n    code.append('        const ' + IndexType + ' idx = indices[dataInd];')\n    code.append('        if (idx < 0 || idx >= data_size) {\\n' + '          return false;\\n' + '        }')\n    if InType == 'uint8_t':\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        // NOLINTNEXTLINE(cppcoreguidelines-init-variables)')\n        code.append('        ' + OutType + ' bio;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n        if fused:\n            code.append('        const float* scale_bias = reinterpret_cast<const float*>(\\n            &input[idx * fused_block_size + block_size]);')\n            code.append('        bio = wgt * scale_bias[1];')\n            code.append('        wgt = wgt * scale_bias[0];')\n        else:\n            code.append('        bio = wgt * scale_bias[2 * idx + 1];')\n            code.append('        wgt = wgt * scale_bias[2 * idx];')\n        code.append('        __m256 vbio = _mm256_set1_ps(bio);')\n    else:\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n    code.append('        __m256 vwgt = _mm256_set1_ps(wgt);')\n    code.append('        const {}* ip = &input[idx * fused_block_size];'.format(InType))\n    code.append('        const {} next_T0 = (dataInd < index_size - prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            ? (dataInd + prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            : dataInd;'.format(IndexType))\n    code.append('        const ' + IndexType + ' idx_pref_T0 = indices[next_T0];')\n    code.append('        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {\\n' + '          return false;\\n' + '        }')\n    code.append('        const {}* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];'.format(InType))\n    code.append('        j = 0;')\n    code.append('        for (; j + 8 <= block_size; j += 8) {')\n    code.extend(compute(InType, use_weights, isa))\n    code.append('        }')\n    code.append('        for (; j < block_size; j++) {')\n    if InType == 'float':\n        code.append('          op[j] = std::fma(wgt, ip[j], op[j]);')\n    elif InType == 'at::Half':\n        code.append('          vtmp1[0] = ip[j];')\n        code.append('          __m256 vtmp2 =\\n              _mm256_cvtph_ps(*(reinterpret_cast<const __m128i*>(vtmp1)));')\n        code.append('          op[j] = std::fma(wgt, ((float*)(&vtmp2))[0], op[j]);')\n    elif InType == 'at::BFloat16':\n        code.append('          vtmp1[0] = ip[j];')\n        code.append('          __m256 vtmp2 = _mm256_castsi256_ps(_mm256_slli_epi32(\\n              _mm256_cvtepu16_epi32(*(reinterpret_cast<const __m128i*>(vtmp1))),\\n              16));')\n        code.append('          op[j] = std::fma(wgt, ((float*)(&vtmp2))[0], op[j]);')\n    elif InType == 'uint8_t':\n        code.append('          op[j] = std::fma(wgt, (float)ip[j], bio + op[j]);')\n    else:\n        assert False\n    code.append('        }')\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (normalize_by_lengths && length) {')\n        code.append('        float len_inv = 1.0f / length;')\n    else:\n        code.append('      if (normalize_by_lengths && lengths[rangeIndex]) {')\n        code.append('        float len_inv = 1.0f / lengths[rangeIndex];')\n    code.append('        __m256 vlen_inv = _mm256_set1_ps(len_inv);')\n    code.append('        j = 0;')\n    code.append('        for (; j + 8 <= block_size; j += 8) {')\n    code.append('          _mm256_storeu_ps(\\n              &op[j], _mm256_mul_ps(_mm256_loadu_ps(&op[j]), vlen_inv));')\n    code.append('        }')\n    code.append('        for (; j < block_size; j++) {')\n    code.append('          op[j] = len_inv * op[j];')\n    code.append('        }')\n    code.append('      }')\n    code.append('    }')\n    return code",
        "mutated": [
            "def generic(IndexType, InType, OutType, use_weights, isa, fused, use_offsets):\n    if False:\n        i = 10\n\n    def compute(InType, use_weights, isa):\n        code = []\n        if InType == 'float':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt, _mm256_loadu_ps(&ip[j]), _mm256_loadu_ps(&op[j])));')\n        elif InType == 'at::Half':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtph_ps(_mm_loadu_si128(\\n                      reinterpret_cast<const __m128i*>(&ip[j]))),\\n                  _mm256_loadu_ps(&op[j])));')\n        elif InType == 'at::BFloat16':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_castsi256_ps(_mm256_slli_epi32(\\n                      _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                          reinterpret_cast<const __m128i*>(&ip[j]))),\\n                      16)),\\n                  _mm256_loadu_ps(&op[j])));')\n        elif InType == 'uint8_t':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(_mm_loadl_epi64(\\n                      reinterpret_cast<const __m128i*>(&ip[j])))),\\n                  _mm256_add_ps(_mm256_loadu_ps(&op[j]), vbio)));')\n        else:\n            assert False\n        code.append('          _mm_prefetch(\\n              reinterpret_cast<const char*>(&ip_next_T0[j]), _MM_HINT_T0);')\n        return code\n    code = []\n    if InType == 'at::Half':\n        code.append('    alignas(64) at::Half vtmp1[8] = {0};')\n    if InType == 'at::BFloat16':\n        code.append('    alignas(64) at::BFloat16 vtmp1[8] = {0};')\n    if use_offsets:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    else:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    code.append('      ' + OutType + '* op = &out[rangeIndex * block_size];')\n    code.append('      int64_t j = 0;')\n    code.append('      for (; j + 8 <= block_size; j += 8) {')\n    code.append('        _mm256_storeu_ps(op + j, _mm256_setzero_ps());')\n    code.append('      }')\n    code.append('      for (; j < block_size; j++) {')\n    code.append('        op[j] = 0.0f;')\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (dataInd != offsets[rangeIndex] - offsets[0]) {\\n' + '        return false;\\n' + '      }')\n        code.append('      int64_t end_offset = offsets[rangeIndex + 1];\\n      int64_t length = end_offset - offsets[rangeIndex];')\n        code.append('      for (' + 'int64_t' + ' start = dataInd; dataInd < end_offset - offsets[0];\\n           ++dataInd) {')\n    else:\n        code.append('      if (dataInd + lengths[rangeIndex] > index_size) {\\n' + '        return false;\\n' + '      }')\n        code.append('      for (' + IndexType + ' start = dataInd; dataInd < start + lengths[rangeIndex];\\n           ++dataInd) {')\n    code.append('        const ' + IndexType + ' idx = indices[dataInd];')\n    code.append('        if (idx < 0 || idx >= data_size) {\\n' + '          return false;\\n' + '        }')\n    if InType == 'uint8_t':\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        // NOLINTNEXTLINE(cppcoreguidelines-init-variables)')\n        code.append('        ' + OutType + ' bio;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n        if fused:\n            code.append('        const float* scale_bias = reinterpret_cast<const float*>(\\n            &input[idx * fused_block_size + block_size]);')\n            code.append('        bio = wgt * scale_bias[1];')\n            code.append('        wgt = wgt * scale_bias[0];')\n        else:\n            code.append('        bio = wgt * scale_bias[2 * idx + 1];')\n            code.append('        wgt = wgt * scale_bias[2 * idx];')\n        code.append('        __m256 vbio = _mm256_set1_ps(bio);')\n    else:\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n    code.append('        __m256 vwgt = _mm256_set1_ps(wgt);')\n    code.append('        const {}* ip = &input[idx * fused_block_size];'.format(InType))\n    code.append('        const {} next_T0 = (dataInd < index_size - prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            ? (dataInd + prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            : dataInd;'.format(IndexType))\n    code.append('        const ' + IndexType + ' idx_pref_T0 = indices[next_T0];')\n    code.append('        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {\\n' + '          return false;\\n' + '        }')\n    code.append('        const {}* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];'.format(InType))\n    code.append('        j = 0;')\n    code.append('        for (; j + 8 <= block_size; j += 8) {')\n    code.extend(compute(InType, use_weights, isa))\n    code.append('        }')\n    code.append('        for (; j < block_size; j++) {')\n    if InType == 'float':\n        code.append('          op[j] = std::fma(wgt, ip[j], op[j]);')\n    elif InType == 'at::Half':\n        code.append('          vtmp1[0] = ip[j];')\n        code.append('          __m256 vtmp2 =\\n              _mm256_cvtph_ps(*(reinterpret_cast<const __m128i*>(vtmp1)));')\n        code.append('          op[j] = std::fma(wgt, ((float*)(&vtmp2))[0], op[j]);')\n    elif InType == 'at::BFloat16':\n        code.append('          vtmp1[0] = ip[j];')\n        code.append('          __m256 vtmp2 = _mm256_castsi256_ps(_mm256_slli_epi32(\\n              _mm256_cvtepu16_epi32(*(reinterpret_cast<const __m128i*>(vtmp1))),\\n              16));')\n        code.append('          op[j] = std::fma(wgt, ((float*)(&vtmp2))[0], op[j]);')\n    elif InType == 'uint8_t':\n        code.append('          op[j] = std::fma(wgt, (float)ip[j], bio + op[j]);')\n    else:\n        assert False\n    code.append('        }')\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (normalize_by_lengths && length) {')\n        code.append('        float len_inv = 1.0f / length;')\n    else:\n        code.append('      if (normalize_by_lengths && lengths[rangeIndex]) {')\n        code.append('        float len_inv = 1.0f / lengths[rangeIndex];')\n    code.append('        __m256 vlen_inv = _mm256_set1_ps(len_inv);')\n    code.append('        j = 0;')\n    code.append('        for (; j + 8 <= block_size; j += 8) {')\n    code.append('          _mm256_storeu_ps(\\n              &op[j], _mm256_mul_ps(_mm256_loadu_ps(&op[j]), vlen_inv));')\n    code.append('        }')\n    code.append('        for (; j < block_size; j++) {')\n    code.append('          op[j] = len_inv * op[j];')\n    code.append('        }')\n    code.append('      }')\n    code.append('    }')\n    return code",
            "def generic(IndexType, InType, OutType, use_weights, isa, fused, use_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def compute(InType, use_weights, isa):\n        code = []\n        if InType == 'float':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt, _mm256_loadu_ps(&ip[j]), _mm256_loadu_ps(&op[j])));')\n        elif InType == 'at::Half':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtph_ps(_mm_loadu_si128(\\n                      reinterpret_cast<const __m128i*>(&ip[j]))),\\n                  _mm256_loadu_ps(&op[j])));')\n        elif InType == 'at::BFloat16':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_castsi256_ps(_mm256_slli_epi32(\\n                      _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                          reinterpret_cast<const __m128i*>(&ip[j]))),\\n                      16)),\\n                  _mm256_loadu_ps(&op[j])));')\n        elif InType == 'uint8_t':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(_mm_loadl_epi64(\\n                      reinterpret_cast<const __m128i*>(&ip[j])))),\\n                  _mm256_add_ps(_mm256_loadu_ps(&op[j]), vbio)));')\n        else:\n            assert False\n        code.append('          _mm_prefetch(\\n              reinterpret_cast<const char*>(&ip_next_T0[j]), _MM_HINT_T0);')\n        return code\n    code = []\n    if InType == 'at::Half':\n        code.append('    alignas(64) at::Half vtmp1[8] = {0};')\n    if InType == 'at::BFloat16':\n        code.append('    alignas(64) at::BFloat16 vtmp1[8] = {0};')\n    if use_offsets:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    else:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    code.append('      ' + OutType + '* op = &out[rangeIndex * block_size];')\n    code.append('      int64_t j = 0;')\n    code.append('      for (; j + 8 <= block_size; j += 8) {')\n    code.append('        _mm256_storeu_ps(op + j, _mm256_setzero_ps());')\n    code.append('      }')\n    code.append('      for (; j < block_size; j++) {')\n    code.append('        op[j] = 0.0f;')\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (dataInd != offsets[rangeIndex] - offsets[0]) {\\n' + '        return false;\\n' + '      }')\n        code.append('      int64_t end_offset = offsets[rangeIndex + 1];\\n      int64_t length = end_offset - offsets[rangeIndex];')\n        code.append('      for (' + 'int64_t' + ' start = dataInd; dataInd < end_offset - offsets[0];\\n           ++dataInd) {')\n    else:\n        code.append('      if (dataInd + lengths[rangeIndex] > index_size) {\\n' + '        return false;\\n' + '      }')\n        code.append('      for (' + IndexType + ' start = dataInd; dataInd < start + lengths[rangeIndex];\\n           ++dataInd) {')\n    code.append('        const ' + IndexType + ' idx = indices[dataInd];')\n    code.append('        if (idx < 0 || idx >= data_size) {\\n' + '          return false;\\n' + '        }')\n    if InType == 'uint8_t':\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        // NOLINTNEXTLINE(cppcoreguidelines-init-variables)')\n        code.append('        ' + OutType + ' bio;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n        if fused:\n            code.append('        const float* scale_bias = reinterpret_cast<const float*>(\\n            &input[idx * fused_block_size + block_size]);')\n            code.append('        bio = wgt * scale_bias[1];')\n            code.append('        wgt = wgt * scale_bias[0];')\n        else:\n            code.append('        bio = wgt * scale_bias[2 * idx + 1];')\n            code.append('        wgt = wgt * scale_bias[2 * idx];')\n        code.append('        __m256 vbio = _mm256_set1_ps(bio);')\n    else:\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n    code.append('        __m256 vwgt = _mm256_set1_ps(wgt);')\n    code.append('        const {}* ip = &input[idx * fused_block_size];'.format(InType))\n    code.append('        const {} next_T0 = (dataInd < index_size - prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            ? (dataInd + prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            : dataInd;'.format(IndexType))\n    code.append('        const ' + IndexType + ' idx_pref_T0 = indices[next_T0];')\n    code.append('        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {\\n' + '          return false;\\n' + '        }')\n    code.append('        const {}* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];'.format(InType))\n    code.append('        j = 0;')\n    code.append('        for (; j + 8 <= block_size; j += 8) {')\n    code.extend(compute(InType, use_weights, isa))\n    code.append('        }')\n    code.append('        for (; j < block_size; j++) {')\n    if InType == 'float':\n        code.append('          op[j] = std::fma(wgt, ip[j], op[j]);')\n    elif InType == 'at::Half':\n        code.append('          vtmp1[0] = ip[j];')\n        code.append('          __m256 vtmp2 =\\n              _mm256_cvtph_ps(*(reinterpret_cast<const __m128i*>(vtmp1)));')\n        code.append('          op[j] = std::fma(wgt, ((float*)(&vtmp2))[0], op[j]);')\n    elif InType == 'at::BFloat16':\n        code.append('          vtmp1[0] = ip[j];')\n        code.append('          __m256 vtmp2 = _mm256_castsi256_ps(_mm256_slli_epi32(\\n              _mm256_cvtepu16_epi32(*(reinterpret_cast<const __m128i*>(vtmp1))),\\n              16));')\n        code.append('          op[j] = std::fma(wgt, ((float*)(&vtmp2))[0], op[j]);')\n    elif InType == 'uint8_t':\n        code.append('          op[j] = std::fma(wgt, (float)ip[j], bio + op[j]);')\n    else:\n        assert False\n    code.append('        }')\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (normalize_by_lengths && length) {')\n        code.append('        float len_inv = 1.0f / length;')\n    else:\n        code.append('      if (normalize_by_lengths && lengths[rangeIndex]) {')\n        code.append('        float len_inv = 1.0f / lengths[rangeIndex];')\n    code.append('        __m256 vlen_inv = _mm256_set1_ps(len_inv);')\n    code.append('        j = 0;')\n    code.append('        for (; j + 8 <= block_size; j += 8) {')\n    code.append('          _mm256_storeu_ps(\\n              &op[j], _mm256_mul_ps(_mm256_loadu_ps(&op[j]), vlen_inv));')\n    code.append('        }')\n    code.append('        for (; j < block_size; j++) {')\n    code.append('          op[j] = len_inv * op[j];')\n    code.append('        }')\n    code.append('      }')\n    code.append('    }')\n    return code",
            "def generic(IndexType, InType, OutType, use_weights, isa, fused, use_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def compute(InType, use_weights, isa):\n        code = []\n        if InType == 'float':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt, _mm256_loadu_ps(&ip[j]), _mm256_loadu_ps(&op[j])));')\n        elif InType == 'at::Half':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtph_ps(_mm_loadu_si128(\\n                      reinterpret_cast<const __m128i*>(&ip[j]))),\\n                  _mm256_loadu_ps(&op[j])));')\n        elif InType == 'at::BFloat16':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_castsi256_ps(_mm256_slli_epi32(\\n                      _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                          reinterpret_cast<const __m128i*>(&ip[j]))),\\n                      16)),\\n                  _mm256_loadu_ps(&op[j])));')\n        elif InType == 'uint8_t':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(_mm_loadl_epi64(\\n                      reinterpret_cast<const __m128i*>(&ip[j])))),\\n                  _mm256_add_ps(_mm256_loadu_ps(&op[j]), vbio)));')\n        else:\n            assert False\n        code.append('          _mm_prefetch(\\n              reinterpret_cast<const char*>(&ip_next_T0[j]), _MM_HINT_T0);')\n        return code\n    code = []\n    if InType == 'at::Half':\n        code.append('    alignas(64) at::Half vtmp1[8] = {0};')\n    if InType == 'at::BFloat16':\n        code.append('    alignas(64) at::BFloat16 vtmp1[8] = {0};')\n    if use_offsets:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    else:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    code.append('      ' + OutType + '* op = &out[rangeIndex * block_size];')\n    code.append('      int64_t j = 0;')\n    code.append('      for (; j + 8 <= block_size; j += 8) {')\n    code.append('        _mm256_storeu_ps(op + j, _mm256_setzero_ps());')\n    code.append('      }')\n    code.append('      for (; j < block_size; j++) {')\n    code.append('        op[j] = 0.0f;')\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (dataInd != offsets[rangeIndex] - offsets[0]) {\\n' + '        return false;\\n' + '      }')\n        code.append('      int64_t end_offset = offsets[rangeIndex + 1];\\n      int64_t length = end_offset - offsets[rangeIndex];')\n        code.append('      for (' + 'int64_t' + ' start = dataInd; dataInd < end_offset - offsets[0];\\n           ++dataInd) {')\n    else:\n        code.append('      if (dataInd + lengths[rangeIndex] > index_size) {\\n' + '        return false;\\n' + '      }')\n        code.append('      for (' + IndexType + ' start = dataInd; dataInd < start + lengths[rangeIndex];\\n           ++dataInd) {')\n    code.append('        const ' + IndexType + ' idx = indices[dataInd];')\n    code.append('        if (idx < 0 || idx >= data_size) {\\n' + '          return false;\\n' + '        }')\n    if InType == 'uint8_t':\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        // NOLINTNEXTLINE(cppcoreguidelines-init-variables)')\n        code.append('        ' + OutType + ' bio;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n        if fused:\n            code.append('        const float* scale_bias = reinterpret_cast<const float*>(\\n            &input[idx * fused_block_size + block_size]);')\n            code.append('        bio = wgt * scale_bias[1];')\n            code.append('        wgt = wgt * scale_bias[0];')\n        else:\n            code.append('        bio = wgt * scale_bias[2 * idx + 1];')\n            code.append('        wgt = wgt * scale_bias[2 * idx];')\n        code.append('        __m256 vbio = _mm256_set1_ps(bio);')\n    else:\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n    code.append('        __m256 vwgt = _mm256_set1_ps(wgt);')\n    code.append('        const {}* ip = &input[idx * fused_block_size];'.format(InType))\n    code.append('        const {} next_T0 = (dataInd < index_size - prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            ? (dataInd + prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            : dataInd;'.format(IndexType))\n    code.append('        const ' + IndexType + ' idx_pref_T0 = indices[next_T0];')\n    code.append('        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {\\n' + '          return false;\\n' + '        }')\n    code.append('        const {}* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];'.format(InType))\n    code.append('        j = 0;')\n    code.append('        for (; j + 8 <= block_size; j += 8) {')\n    code.extend(compute(InType, use_weights, isa))\n    code.append('        }')\n    code.append('        for (; j < block_size; j++) {')\n    if InType == 'float':\n        code.append('          op[j] = std::fma(wgt, ip[j], op[j]);')\n    elif InType == 'at::Half':\n        code.append('          vtmp1[0] = ip[j];')\n        code.append('          __m256 vtmp2 =\\n              _mm256_cvtph_ps(*(reinterpret_cast<const __m128i*>(vtmp1)));')\n        code.append('          op[j] = std::fma(wgt, ((float*)(&vtmp2))[0], op[j]);')\n    elif InType == 'at::BFloat16':\n        code.append('          vtmp1[0] = ip[j];')\n        code.append('          __m256 vtmp2 = _mm256_castsi256_ps(_mm256_slli_epi32(\\n              _mm256_cvtepu16_epi32(*(reinterpret_cast<const __m128i*>(vtmp1))),\\n              16));')\n        code.append('          op[j] = std::fma(wgt, ((float*)(&vtmp2))[0], op[j]);')\n    elif InType == 'uint8_t':\n        code.append('          op[j] = std::fma(wgt, (float)ip[j], bio + op[j]);')\n    else:\n        assert False\n    code.append('        }')\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (normalize_by_lengths && length) {')\n        code.append('        float len_inv = 1.0f / length;')\n    else:\n        code.append('      if (normalize_by_lengths && lengths[rangeIndex]) {')\n        code.append('        float len_inv = 1.0f / lengths[rangeIndex];')\n    code.append('        __m256 vlen_inv = _mm256_set1_ps(len_inv);')\n    code.append('        j = 0;')\n    code.append('        for (; j + 8 <= block_size; j += 8) {')\n    code.append('          _mm256_storeu_ps(\\n              &op[j], _mm256_mul_ps(_mm256_loadu_ps(&op[j]), vlen_inv));')\n    code.append('        }')\n    code.append('        for (; j < block_size; j++) {')\n    code.append('          op[j] = len_inv * op[j];')\n    code.append('        }')\n    code.append('      }')\n    code.append('    }')\n    return code",
            "def generic(IndexType, InType, OutType, use_weights, isa, fused, use_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def compute(InType, use_weights, isa):\n        code = []\n        if InType == 'float':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt, _mm256_loadu_ps(&ip[j]), _mm256_loadu_ps(&op[j])));')\n        elif InType == 'at::Half':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtph_ps(_mm_loadu_si128(\\n                      reinterpret_cast<const __m128i*>(&ip[j]))),\\n                  _mm256_loadu_ps(&op[j])));')\n        elif InType == 'at::BFloat16':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_castsi256_ps(_mm256_slli_epi32(\\n                      _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                          reinterpret_cast<const __m128i*>(&ip[j]))),\\n                      16)),\\n                  _mm256_loadu_ps(&op[j])));')\n        elif InType == 'uint8_t':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(_mm_loadl_epi64(\\n                      reinterpret_cast<const __m128i*>(&ip[j])))),\\n                  _mm256_add_ps(_mm256_loadu_ps(&op[j]), vbio)));')\n        else:\n            assert False\n        code.append('          _mm_prefetch(\\n              reinterpret_cast<const char*>(&ip_next_T0[j]), _MM_HINT_T0);')\n        return code\n    code = []\n    if InType == 'at::Half':\n        code.append('    alignas(64) at::Half vtmp1[8] = {0};')\n    if InType == 'at::BFloat16':\n        code.append('    alignas(64) at::BFloat16 vtmp1[8] = {0};')\n    if use_offsets:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    else:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    code.append('      ' + OutType + '* op = &out[rangeIndex * block_size];')\n    code.append('      int64_t j = 0;')\n    code.append('      for (; j + 8 <= block_size; j += 8) {')\n    code.append('        _mm256_storeu_ps(op + j, _mm256_setzero_ps());')\n    code.append('      }')\n    code.append('      for (; j < block_size; j++) {')\n    code.append('        op[j] = 0.0f;')\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (dataInd != offsets[rangeIndex] - offsets[0]) {\\n' + '        return false;\\n' + '      }')\n        code.append('      int64_t end_offset = offsets[rangeIndex + 1];\\n      int64_t length = end_offset - offsets[rangeIndex];')\n        code.append('      for (' + 'int64_t' + ' start = dataInd; dataInd < end_offset - offsets[0];\\n           ++dataInd) {')\n    else:\n        code.append('      if (dataInd + lengths[rangeIndex] > index_size) {\\n' + '        return false;\\n' + '      }')\n        code.append('      for (' + IndexType + ' start = dataInd; dataInd < start + lengths[rangeIndex];\\n           ++dataInd) {')\n    code.append('        const ' + IndexType + ' idx = indices[dataInd];')\n    code.append('        if (idx < 0 || idx >= data_size) {\\n' + '          return false;\\n' + '        }')\n    if InType == 'uint8_t':\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        // NOLINTNEXTLINE(cppcoreguidelines-init-variables)')\n        code.append('        ' + OutType + ' bio;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n        if fused:\n            code.append('        const float* scale_bias = reinterpret_cast<const float*>(\\n            &input[idx * fused_block_size + block_size]);')\n            code.append('        bio = wgt * scale_bias[1];')\n            code.append('        wgt = wgt * scale_bias[0];')\n        else:\n            code.append('        bio = wgt * scale_bias[2 * idx + 1];')\n            code.append('        wgt = wgt * scale_bias[2 * idx];')\n        code.append('        __m256 vbio = _mm256_set1_ps(bio);')\n    else:\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n    code.append('        __m256 vwgt = _mm256_set1_ps(wgt);')\n    code.append('        const {}* ip = &input[idx * fused_block_size];'.format(InType))\n    code.append('        const {} next_T0 = (dataInd < index_size - prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            ? (dataInd + prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            : dataInd;'.format(IndexType))\n    code.append('        const ' + IndexType + ' idx_pref_T0 = indices[next_T0];')\n    code.append('        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {\\n' + '          return false;\\n' + '        }')\n    code.append('        const {}* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];'.format(InType))\n    code.append('        j = 0;')\n    code.append('        for (; j + 8 <= block_size; j += 8) {')\n    code.extend(compute(InType, use_weights, isa))\n    code.append('        }')\n    code.append('        for (; j < block_size; j++) {')\n    if InType == 'float':\n        code.append('          op[j] = std::fma(wgt, ip[j], op[j]);')\n    elif InType == 'at::Half':\n        code.append('          vtmp1[0] = ip[j];')\n        code.append('          __m256 vtmp2 =\\n              _mm256_cvtph_ps(*(reinterpret_cast<const __m128i*>(vtmp1)));')\n        code.append('          op[j] = std::fma(wgt, ((float*)(&vtmp2))[0], op[j]);')\n    elif InType == 'at::BFloat16':\n        code.append('          vtmp1[0] = ip[j];')\n        code.append('          __m256 vtmp2 = _mm256_castsi256_ps(_mm256_slli_epi32(\\n              _mm256_cvtepu16_epi32(*(reinterpret_cast<const __m128i*>(vtmp1))),\\n              16));')\n        code.append('          op[j] = std::fma(wgt, ((float*)(&vtmp2))[0], op[j]);')\n    elif InType == 'uint8_t':\n        code.append('          op[j] = std::fma(wgt, (float)ip[j], bio + op[j]);')\n    else:\n        assert False\n    code.append('        }')\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (normalize_by_lengths && length) {')\n        code.append('        float len_inv = 1.0f / length;')\n    else:\n        code.append('      if (normalize_by_lengths && lengths[rangeIndex]) {')\n        code.append('        float len_inv = 1.0f / lengths[rangeIndex];')\n    code.append('        __m256 vlen_inv = _mm256_set1_ps(len_inv);')\n    code.append('        j = 0;')\n    code.append('        for (; j + 8 <= block_size; j += 8) {')\n    code.append('          _mm256_storeu_ps(\\n              &op[j], _mm256_mul_ps(_mm256_loadu_ps(&op[j]), vlen_inv));')\n    code.append('        }')\n    code.append('        for (; j < block_size; j++) {')\n    code.append('          op[j] = len_inv * op[j];')\n    code.append('        }')\n    code.append('      }')\n    code.append('    }')\n    return code",
            "def generic(IndexType, InType, OutType, use_weights, isa, fused, use_offsets):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def compute(InType, use_weights, isa):\n        code = []\n        if InType == 'float':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt, _mm256_loadu_ps(&ip[j]), _mm256_loadu_ps(&op[j])));')\n        elif InType == 'at::Half':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtph_ps(_mm_loadu_si128(\\n                      reinterpret_cast<const __m128i*>(&ip[j]))),\\n                  _mm256_loadu_ps(&op[j])));')\n        elif InType == 'at::BFloat16':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_castsi256_ps(_mm256_slli_epi32(\\n                      _mm256_cvtepu16_epi32(_mm_loadu_si128(\\n                          reinterpret_cast<const __m128i*>(&ip[j]))),\\n                      16)),\\n                  _mm256_loadu_ps(&op[j])));')\n        elif InType == 'uint8_t':\n            code.append('          _mm256_storeu_ps(\\n              &op[j],\\n              _mm256_fmadd_ps(\\n                  vwgt,\\n                  _mm256_cvtepi32_ps(_mm256_cvtepu8_epi32(_mm_loadl_epi64(\\n                      reinterpret_cast<const __m128i*>(&ip[j])))),\\n                  _mm256_add_ps(_mm256_loadu_ps(&op[j]), vbio)));')\n        else:\n            assert False\n        code.append('          _mm_prefetch(\\n              reinterpret_cast<const char*>(&ip_next_T0[j]), _MM_HINT_T0);')\n        return code\n    code = []\n    if InType == 'at::Half':\n        code.append('    alignas(64) at::Half vtmp1[8] = {0};')\n    if InType == 'at::BFloat16':\n        code.append('    alignas(64) at::BFloat16 vtmp1[8] = {0};')\n    if use_offsets:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    else:\n        code.append('    for (' + IndexType + ' rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {')\n    code.append('      ' + OutType + '* op = &out[rangeIndex * block_size];')\n    code.append('      int64_t j = 0;')\n    code.append('      for (; j + 8 <= block_size; j += 8) {')\n    code.append('        _mm256_storeu_ps(op + j, _mm256_setzero_ps());')\n    code.append('      }')\n    code.append('      for (; j < block_size; j++) {')\n    code.append('        op[j] = 0.0f;')\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (dataInd != offsets[rangeIndex] - offsets[0]) {\\n' + '        return false;\\n' + '      }')\n        code.append('      int64_t end_offset = offsets[rangeIndex + 1];\\n      int64_t length = end_offset - offsets[rangeIndex];')\n        code.append('      for (' + 'int64_t' + ' start = dataInd; dataInd < end_offset - offsets[0];\\n           ++dataInd) {')\n    else:\n        code.append('      if (dataInd + lengths[rangeIndex] > index_size) {\\n' + '        return false;\\n' + '      }')\n        code.append('      for (' + IndexType + ' start = dataInd; dataInd < start + lengths[rangeIndex];\\n           ++dataInd) {')\n    code.append('        const ' + IndexType + ' idx = indices[dataInd];')\n    code.append('        if (idx < 0 || idx >= data_size) {\\n' + '          return false;\\n' + '        }')\n    if InType == 'uint8_t':\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        // NOLINTNEXTLINE(cppcoreguidelines-init-variables)')\n        code.append('        ' + OutType + ' bio;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n        if fused:\n            code.append('        const float* scale_bias = reinterpret_cast<const float*>(\\n            &input[idx * fused_block_size + block_size]);')\n            code.append('        bio = wgt * scale_bias[1];')\n            code.append('        wgt = wgt * scale_bias[0];')\n        else:\n            code.append('        bio = wgt * scale_bias[2 * idx + 1];')\n            code.append('        wgt = wgt * scale_bias[2 * idx];')\n        code.append('        __m256 vbio = _mm256_set1_ps(bio);')\n    else:\n        code.append('        ' + OutType + ' wgt = 1.f;')\n        code.append('        if (weights) {')\n        code.append('          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];')\n        code.append('        }')\n    code.append('        __m256 vwgt = _mm256_set1_ps(wgt);')\n    code.append('        const {}* ip = &input[idx * fused_block_size];'.format(InType))\n    code.append('        const {} next_T0 = (dataInd < index_size - prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            ? (dataInd + prefdist_T0)\\n            // NOLINTNEXTLINE(cppcoreguidelines-narrowing-conversions,bugprone-narrowing-conversions)\\n            : dataInd;'.format(IndexType))\n    code.append('        const ' + IndexType + ' idx_pref_T0 = indices[next_T0];')\n    code.append('        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {\\n' + '          return false;\\n' + '        }')\n    code.append('        const {}* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];'.format(InType))\n    code.append('        j = 0;')\n    code.append('        for (; j + 8 <= block_size; j += 8) {')\n    code.extend(compute(InType, use_weights, isa))\n    code.append('        }')\n    code.append('        for (; j < block_size; j++) {')\n    if InType == 'float':\n        code.append('          op[j] = std::fma(wgt, ip[j], op[j]);')\n    elif InType == 'at::Half':\n        code.append('          vtmp1[0] = ip[j];')\n        code.append('          __m256 vtmp2 =\\n              _mm256_cvtph_ps(*(reinterpret_cast<const __m128i*>(vtmp1)));')\n        code.append('          op[j] = std::fma(wgt, ((float*)(&vtmp2))[0], op[j]);')\n    elif InType == 'at::BFloat16':\n        code.append('          vtmp1[0] = ip[j];')\n        code.append('          __m256 vtmp2 = _mm256_castsi256_ps(_mm256_slli_epi32(\\n              _mm256_cvtepu16_epi32(*(reinterpret_cast<const __m128i*>(vtmp1))),\\n              16));')\n        code.append('          op[j] = std::fma(wgt, ((float*)(&vtmp2))[0], op[j]);')\n    elif InType == 'uint8_t':\n        code.append('          op[j] = std::fma(wgt, (float)ip[j], bio + op[j]);')\n    else:\n        assert False\n    code.append('        }')\n    code.append('      }')\n    if use_offsets:\n        code.append('      if (normalize_by_lengths && length) {')\n        code.append('        float len_inv = 1.0f / length;')\n    else:\n        code.append('      if (normalize_by_lengths && lengths[rangeIndex]) {')\n        code.append('        float len_inv = 1.0f / lengths[rangeIndex];')\n    code.append('        __m256 vlen_inv = _mm256_set1_ps(len_inv);')\n    code.append('        j = 0;')\n    code.append('        for (; j + 8 <= block_size; j += 8) {')\n    code.append('          _mm256_storeu_ps(\\n              &op[j], _mm256_mul_ps(_mm256_loadu_ps(&op[j]), vlen_inv));')\n    code.append('        }')\n    code.append('        for (; j < block_size; j++) {')\n    code.append('          op[j] = len_inv * op[j];')\n    code.append('        }')\n    code.append('      }')\n    code.append('    }')\n    return code"
        ]
    }
]