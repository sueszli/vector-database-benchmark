[
    {
        "func_name": "get_label_quality_multiannotator",
        "original": "def get_label_quality_multiannotator(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: np.ndarray, *, consensus_method: Union[str, List[str]]='best_quality', quality_method: str='crowdlab', calibrate_probs: bool=False, return_detailed_quality: bool=True, return_annotator_stats: bool=True, return_weights: bool=False, verbose: bool=True, label_quality_score_kwargs: dict={}) -> Dict[str, Any]:\n    \"\"\"Returns label quality scores for each example and for each annotator in a dataset labeled by multiple annotators.\n\n    This function is for multiclass classification datasets where examples have been labeled by\n    multiple annotators (not necessarily the same number of annotators per example).\n\n    It computes one consensus label for each example that best accounts for the labels chosen by each\n    annotator (and their quality), as well as a consensus quality score for how confident we are that this consensus label is actually correct.\n    It also computes similar quality scores for each annotator's individual labels, and the quality of each annotator.\n    Scores are between 0 and 1 (estimated via methods like CROWDLAB); lower scores indicate labels/annotators less likely to be correct.\n\n    To decide what data to collect additional labels for, try the :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>`\n    (ActiveLab) function, which is intended for active learning with multiple annotators.\n\n    Parameters\n    ----------\n    labels_multiannotator : pd.DataFrame or np.ndarray\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\n        where N is the number of examples and M is the number of annotators.\n        ``labels_multiannotator[n][m]`` = label for n-th example given by m-th annotator.\n\n        For a dataset with K classes, each given label must be an integer in 0, 1, ..., K-1 or ``NaN`` if this annotator did not label a particular example.\n        If you have string or other differently formatted labels, you can convert them to the proper format using :py:func:`format_multiannotator_labels <cleanlab.internal.multiannotator_utils.format_multiannotator_labels>`.\n        If pd.DataFrame, column names should correspond to each annotator's ID.\n    pred_probs : np.ndarray\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model.\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\n    consensus_method : str or List[str], default = \"majority_vote\"\n        Specifies the method used to aggregate labels from multiple annotators into a single consensus label.\n        Options include:\n\n        * ``majority_vote``: consensus obtained using a simple majority vote among annotators, with ties broken via ``pred_probs``.\n        * ``best_quality``: consensus obtained by selecting the label with highest label quality (quality determined by method specified in ``quality_method``).\n\n        A List may be passed if you want to consider multiple methods for producing consensus labels.\n        If a List is passed, then the 0th element of the list is the method used to produce columns `consensus_label`, `consensus_quality_score`, `annotator_agreement` in the returned DataFrame.\n        The remaning (1st, 2nd, 3rd, etc.) elements of this list are output as extra columns in the returned pandas DataFrame with names formatted as:\n        `consensus_label_SUFFIX`, `consensus_quality_score_SUFFIX` where `SUFFIX` = each element of this\n        list, which must correspond to a valid method for computing consensus labels.\n    quality_method : str, default = \"crowdlab\"\n        Specifies the method used to calculate the quality of the consensus label.\n        Options include:\n\n        * ``crowdlab``: an emsemble method that weighs both the annotators' labels as well as the model's prediction.\n        * ``agreement``: the fraction of annotators that agree with the consensus label.\n    calibrate_probs : bool, default = False\n        Boolean value that specifies whether the provided `pred_probs` should be re-calibrated to better match the annotators' empirical label distribution.\n        We recommend setting this to True in active learning applications, in order to prevent overconfident models from suggesting the wrong examples to collect labels for.\n    return_detailed_quality: bool, default = True\n        Boolean to specify if `detailed_label_quality` is returned.\n    return_annotator_stats : bool, default = True\n        Boolean to specify if `annotator_stats` is returned.\n    return_weights : bool, default = False\n        Boolean to specify if `model_weight` and `annotator_weight` is returned.\n        Model and annotator weights are applicable for ``quality_method == crowdlab``, will return ``None`` for any other quality methods.\n    verbose : bool, default = True\n        Important warnings and other printed statements may be suppressed if ``verbose`` is set to ``False``.\n    label_quality_score_kwargs : dict, optional\n        Keyword arguments to pass into :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\n\n    Returns\n    -------\n    labels_info : dict\n        Dictionary containing up to 5 pandas DataFrame with keys as below:\n\n        ``label_quality`` : pandas.DataFrame\n            pandas DataFrame in which each row corresponds to one example, with columns:\n\n            * ``num_annotations``: the number of annotators that have labeled each example.\n            * ``consensus_label``: the single label that is best for each example (you can control how it is derived from all annotators' labels via the argument: ``consensus_method``).\n            * ``annotator_agreement``: the fraction of annotators that agree with the consensus label (only consider the annotators that labeled that particular example).\n            * ``consensus_quality_score``: label quality score for consensus label, calculated by the method specified in ``quality_method``.\n\n        ``detailed_label_quality`` : pandas.DataFrame\n            Only returned if `return_detailed_quality=True`.\n            Returns a pandas DataFrame with columns `quality_annotator_1`, `quality_annotator_2`, ..., `quality_annotator_M` where each entry is\n            the label quality score for the labels provided by each annotator (is ``NaN`` for examples which this annotator did not label).\n\n        ``annotator_stats`` : pandas.DataFrame\n            Only returned if `return_annotator_stats=True`.\n            Returns overall statistics about each annotator, sorted by lowest annotator_quality first.\n            pandas DataFrame in which each row corresponds to one annotator (the row IDs correspond to annotator IDs), with columns:\n\n            * ``annotator_quality``: overall quality of a given annotator's labels, calculated by the method specified in ``quality_method``.\n            * ``num_examples_labeled``: number of examples annotated by a given annotator.\n            * ``agreement_with_consensus``: fraction of examples where a given annotator agrees with the consensus label.\n            * ``worst_class``: the class that is most frequently mislabeled by a given annotator.\n\n        ``model_weight`` : float\n            Only returned if `return_weights=True`. It is only applicable for ``quality_method == crowdlab``.\n            The model weight specifies the weight of classifier model in weighted averages used to estimate label quality\n            This number is an estimate of how trustworthy the model is relative the annotators.\n\n        ``annotator_weight`` : np.ndarray\n            Only returned if `return_weights=True`. It is only applicable for ``quality_method == crowdlab``.\n            An array of shape ``(M,)`` where M is the number of annotators, specifying the weight of each annotator in weighted averages used to estimate label quality.\n            These weights are estimates of how trustworthy each annotator is relative to the other annotators.\n\n    \"\"\"\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        index_col = labels_multiannotator.index\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n        index_col = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    if return_weights == True and quality_method != 'crowdlab':\n        raise ValueError(\"Model and annotator weights are only applicable to the crowdlab quality method. Either set return_weights=False or quality_method='crowdlab'.\")\n    assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, annotator_ids=annotator_ids)\n    num_annotations = np.sum(~np.isnan(labels_multiannotator), axis=1)\n    if calibrate_probs:\n        optimal_temp = find_best_temp_scaler(labels_multiannotator, pred_probs)\n        pred_probs = temp_scale_pred_probs(pred_probs, optimal_temp)\n    if not isinstance(consensus_method, list):\n        consensus_method = [consensus_method]\n    if 'best_quality' in consensus_method or 'majority_vote' in consensus_method:\n        majority_vote_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, verbose=False)\n        (MV_annotator_agreement, MV_consensus_quality_score, MV_post_pred_probs, MV_model_weight, MV_annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=majority_vote_label, quality_method=quality_method, verbose=verbose, label_quality_score_kwargs=label_quality_score_kwargs)\n    label_quality = pd.DataFrame({'num_annotations': num_annotations}, index=index_col)\n    valid_methods = ['majority_vote', 'best_quality']\n    main_method = True\n    for curr_method in consensus_method:\n        if curr_method == 'majority_vote':\n            consensus_label = majority_vote_label\n            annotator_agreement = MV_annotator_agreement\n            consensus_quality_score = MV_consensus_quality_score\n            post_pred_probs = MV_post_pred_probs\n            model_weight = MV_model_weight\n            annotator_weight = MV_annotator_weight\n        elif curr_method == 'best_quality':\n            consensus_label = np.full(len(majority_vote_label), np.nan)\n            for i in range(len(consensus_label)):\n                max_pred_probs_ind = np.where(MV_post_pred_probs[i] == np.max(MV_post_pred_probs[i]))[0]\n                if len(max_pred_probs_ind) == 1:\n                    consensus_label[i] = max_pred_probs_ind[0]\n                else:\n                    consensus_label[i] = majority_vote_label[i]\n            consensus_label = consensus_label.astype(int)\n            (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=consensus_label, quality_method=quality_method, verbose=verbose, label_quality_score_kwargs=label_quality_score_kwargs)\n        else:\n            raise ValueError(f'\\n                {curr_method} is not a valid consensus method!\\n                Please choose a valid consensus_method: {valid_methods}\\n                ')\n        if verbose:\n            check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_method=curr_method)\n        if main_method:\n            (label_quality['consensus_label'], label_quality['consensus_quality_score'], label_quality['annotator_agreement']) = (consensus_label, consensus_quality_score, annotator_agreement)\n            label_quality = label_quality.reindex(columns=['consensus_label', 'consensus_quality_score', 'annotator_agreement', 'num_annotations'])\n            detailed_label_quality = None\n            if return_detailed_quality:\n                detailed_label_quality = np.apply_along_axis(_get_annotator_label_quality_score, axis=0, arr=labels_multiannotator, pred_probs=post_pred_probs, label_quality_score_kwargs=label_quality_score_kwargs)\n                detailed_label_quality_df = pd.DataFrame(detailed_label_quality, index=index_col, columns=annotator_ids).add_prefix('quality_annotator_')\n            if return_annotator_stats:\n                annotator_stats = _get_annotator_stats(labels_multiannotator=labels_multiannotator, pred_probs=post_pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=model_weight, annotator_weight=annotator_weight, consensus_quality_score=consensus_quality_score, detailed_label_quality=detailed_label_quality, annotator_ids=annotator_ids, quality_method=quality_method)\n            main_method = False\n        else:\n            (label_quality[f'consensus_label_{curr_method}'], label_quality[f'consensus_quality_score_{curr_method}'], label_quality[f'annotator_agreement_{curr_method}']) = (consensus_label, consensus_quality_score, annotator_agreement)\n    labels_info = {'label_quality': label_quality}\n    if return_detailed_quality:\n        labels_info['detailed_label_quality'] = detailed_label_quality_df\n    if return_annotator_stats:\n        labels_info['annotator_stats'] = annotator_stats\n    if return_weights:\n        labels_info['model_weight'] = model_weight\n        labels_info['annotator_weight'] = annotator_weight\n    return labels_info",
        "mutated": [
            "def get_label_quality_multiannotator(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: np.ndarray, *, consensus_method: Union[str, List[str]]='best_quality', quality_method: str='crowdlab', calibrate_probs: bool=False, return_detailed_quality: bool=True, return_annotator_stats: bool=True, return_weights: bool=False, verbose: bool=True, label_quality_score_kwargs: dict={}) -> Dict[str, Any]:\n    if False:\n        i = 10\n    'Returns label quality scores for each example and for each annotator in a dataset labeled by multiple annotators.\\n\\n    This function is for multiclass classification datasets where examples have been labeled by\\n    multiple annotators (not necessarily the same number of annotators per example).\\n\\n    It computes one consensus label for each example that best accounts for the labels chosen by each\\n    annotator (and their quality), as well as a consensus quality score for how confident we are that this consensus label is actually correct.\\n    It also computes similar quality scores for each annotator\\'s individual labels, and the quality of each annotator.\\n    Scores are between 0 and 1 (estimated via methods like CROWDLAB); lower scores indicate labels/annotators less likely to be correct.\\n\\n    To decide what data to collect additional labels for, try the :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>`\\n    (ActiveLab) function, which is intended for active learning with multiple annotators.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        ``labels_multiannotator[n][m]`` = label for n-th example given by m-th annotator.\\n\\n        For a dataset with K classes, each given label must be an integer in 0, 1, ..., K-1 or ``NaN`` if this annotator did not label a particular example.\\n        If you have string or other differently formatted labels, you can convert them to the proper format using :py:func:`format_multiannotator_labels <cleanlab.internal.multiannotator_utils.format_multiannotator_labels>`.\\n        If pd.DataFrame, column names should correspond to each annotator\\'s ID.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    consensus_method : str or List[str], default = \"majority_vote\"\\n        Specifies the method used to aggregate labels from multiple annotators into a single consensus label.\\n        Options include:\\n\\n        * ``majority_vote``: consensus obtained using a simple majority vote among annotators, with ties broken via ``pred_probs``.\\n        * ``best_quality``: consensus obtained by selecting the label with highest label quality (quality determined by method specified in ``quality_method``).\\n\\n        A List may be passed if you want to consider multiple methods for producing consensus labels.\\n        If a List is passed, then the 0th element of the list is the method used to produce columns `consensus_label`, `consensus_quality_score`, `annotator_agreement` in the returned DataFrame.\\n        The remaning (1st, 2nd, 3rd, etc.) elements of this list are output as extra columns in the returned pandas DataFrame with names formatted as:\\n        `consensus_label_SUFFIX`, `consensus_quality_score_SUFFIX` where `SUFFIX` = each element of this\\n        list, which must correspond to a valid method for computing consensus labels.\\n    quality_method : str, default = \"crowdlab\"\\n        Specifies the method used to calculate the quality of the consensus label.\\n        Options include:\\n\\n        * ``crowdlab``: an emsemble method that weighs both the annotators\\' labels as well as the model\\'s prediction.\\n        * ``agreement``: the fraction of annotators that agree with the consensus label.\\n    calibrate_probs : bool, default = False\\n        Boolean value that specifies whether the provided `pred_probs` should be re-calibrated to better match the annotators\\' empirical label distribution.\\n        We recommend setting this to True in active learning applications, in order to prevent overconfident models from suggesting the wrong examples to collect labels for.\\n    return_detailed_quality: bool, default = True\\n        Boolean to specify if `detailed_label_quality` is returned.\\n    return_annotator_stats : bool, default = True\\n        Boolean to specify if `annotator_stats` is returned.\\n    return_weights : bool, default = False\\n        Boolean to specify if `model_weight` and `annotator_weight` is returned.\\n        Model and annotator weights are applicable for ``quality_method == crowdlab``, will return ``None`` for any other quality methods.\\n    verbose : bool, default = True\\n        Important warnings and other printed statements may be suppressed if ``verbose`` is set to ``False``.\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments to pass into :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n    Returns\\n    -------\\n    labels_info : dict\\n        Dictionary containing up to 5 pandas DataFrame with keys as below:\\n\\n        ``label_quality`` : pandas.DataFrame\\n            pandas DataFrame in which each row corresponds to one example, with columns:\\n\\n            * ``num_annotations``: the number of annotators that have labeled each example.\\n            * ``consensus_label``: the single label that is best for each example (you can control how it is derived from all annotators\\' labels via the argument: ``consensus_method``).\\n            * ``annotator_agreement``: the fraction of annotators that agree with the consensus label (only consider the annotators that labeled that particular example).\\n            * ``consensus_quality_score``: label quality score for consensus label, calculated by the method specified in ``quality_method``.\\n\\n        ``detailed_label_quality`` : pandas.DataFrame\\n            Only returned if `return_detailed_quality=True`.\\n            Returns a pandas DataFrame with columns `quality_annotator_1`, `quality_annotator_2`, ..., `quality_annotator_M` where each entry is\\n            the label quality score for the labels provided by each annotator (is ``NaN`` for examples which this annotator did not label).\\n\\n        ``annotator_stats`` : pandas.DataFrame\\n            Only returned if `return_annotator_stats=True`.\\n            Returns overall statistics about each annotator, sorted by lowest annotator_quality first.\\n            pandas DataFrame in which each row corresponds to one annotator (the row IDs correspond to annotator IDs), with columns:\\n\\n            * ``annotator_quality``: overall quality of a given annotator\\'s labels, calculated by the method specified in ``quality_method``.\\n            * ``num_examples_labeled``: number of examples annotated by a given annotator.\\n            * ``agreement_with_consensus``: fraction of examples where a given annotator agrees with the consensus label.\\n            * ``worst_class``: the class that is most frequently mislabeled by a given annotator.\\n\\n        ``model_weight`` : float\\n            Only returned if `return_weights=True`. It is only applicable for ``quality_method == crowdlab``.\\n            The model weight specifies the weight of classifier model in weighted averages used to estimate label quality\\n            This number is an estimate of how trustworthy the model is relative the annotators.\\n\\n        ``annotator_weight`` : np.ndarray\\n            Only returned if `return_weights=True`. It is only applicable for ``quality_method == crowdlab``.\\n            An array of shape ``(M,)`` where M is the number of annotators, specifying the weight of each annotator in weighted averages used to estimate label quality.\\n            These weights are estimates of how trustworthy each annotator is relative to the other annotators.\\n\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        index_col = labels_multiannotator.index\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n        index_col = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    if return_weights == True and quality_method != 'crowdlab':\n        raise ValueError(\"Model and annotator weights are only applicable to the crowdlab quality method. Either set return_weights=False or quality_method='crowdlab'.\")\n    assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, annotator_ids=annotator_ids)\n    num_annotations = np.sum(~np.isnan(labels_multiannotator), axis=1)\n    if calibrate_probs:\n        optimal_temp = find_best_temp_scaler(labels_multiannotator, pred_probs)\n        pred_probs = temp_scale_pred_probs(pred_probs, optimal_temp)\n    if not isinstance(consensus_method, list):\n        consensus_method = [consensus_method]\n    if 'best_quality' in consensus_method or 'majority_vote' in consensus_method:\n        majority_vote_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, verbose=False)\n        (MV_annotator_agreement, MV_consensus_quality_score, MV_post_pred_probs, MV_model_weight, MV_annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=majority_vote_label, quality_method=quality_method, verbose=verbose, label_quality_score_kwargs=label_quality_score_kwargs)\n    label_quality = pd.DataFrame({'num_annotations': num_annotations}, index=index_col)\n    valid_methods = ['majority_vote', 'best_quality']\n    main_method = True\n    for curr_method in consensus_method:\n        if curr_method == 'majority_vote':\n            consensus_label = majority_vote_label\n            annotator_agreement = MV_annotator_agreement\n            consensus_quality_score = MV_consensus_quality_score\n            post_pred_probs = MV_post_pred_probs\n            model_weight = MV_model_weight\n            annotator_weight = MV_annotator_weight\n        elif curr_method == 'best_quality':\n            consensus_label = np.full(len(majority_vote_label), np.nan)\n            for i in range(len(consensus_label)):\n                max_pred_probs_ind = np.where(MV_post_pred_probs[i] == np.max(MV_post_pred_probs[i]))[0]\n                if len(max_pred_probs_ind) == 1:\n                    consensus_label[i] = max_pred_probs_ind[0]\n                else:\n                    consensus_label[i] = majority_vote_label[i]\n            consensus_label = consensus_label.astype(int)\n            (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=consensus_label, quality_method=quality_method, verbose=verbose, label_quality_score_kwargs=label_quality_score_kwargs)\n        else:\n            raise ValueError(f'\\n                {curr_method} is not a valid consensus method!\\n                Please choose a valid consensus_method: {valid_methods}\\n                ')\n        if verbose:\n            check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_method=curr_method)\n        if main_method:\n            (label_quality['consensus_label'], label_quality['consensus_quality_score'], label_quality['annotator_agreement']) = (consensus_label, consensus_quality_score, annotator_agreement)\n            label_quality = label_quality.reindex(columns=['consensus_label', 'consensus_quality_score', 'annotator_agreement', 'num_annotations'])\n            detailed_label_quality = None\n            if return_detailed_quality:\n                detailed_label_quality = np.apply_along_axis(_get_annotator_label_quality_score, axis=0, arr=labels_multiannotator, pred_probs=post_pred_probs, label_quality_score_kwargs=label_quality_score_kwargs)\n                detailed_label_quality_df = pd.DataFrame(detailed_label_quality, index=index_col, columns=annotator_ids).add_prefix('quality_annotator_')\n            if return_annotator_stats:\n                annotator_stats = _get_annotator_stats(labels_multiannotator=labels_multiannotator, pred_probs=post_pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=model_weight, annotator_weight=annotator_weight, consensus_quality_score=consensus_quality_score, detailed_label_quality=detailed_label_quality, annotator_ids=annotator_ids, quality_method=quality_method)\n            main_method = False\n        else:\n            (label_quality[f'consensus_label_{curr_method}'], label_quality[f'consensus_quality_score_{curr_method}'], label_quality[f'annotator_agreement_{curr_method}']) = (consensus_label, consensus_quality_score, annotator_agreement)\n    labels_info = {'label_quality': label_quality}\n    if return_detailed_quality:\n        labels_info['detailed_label_quality'] = detailed_label_quality_df\n    if return_annotator_stats:\n        labels_info['annotator_stats'] = annotator_stats\n    if return_weights:\n        labels_info['model_weight'] = model_weight\n        labels_info['annotator_weight'] = annotator_weight\n    return labels_info",
            "def get_label_quality_multiannotator(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: np.ndarray, *, consensus_method: Union[str, List[str]]='best_quality', quality_method: str='crowdlab', calibrate_probs: bool=False, return_detailed_quality: bool=True, return_annotator_stats: bool=True, return_weights: bool=False, verbose: bool=True, label_quality_score_kwargs: dict={}) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns label quality scores for each example and for each annotator in a dataset labeled by multiple annotators.\\n\\n    This function is for multiclass classification datasets where examples have been labeled by\\n    multiple annotators (not necessarily the same number of annotators per example).\\n\\n    It computes one consensus label for each example that best accounts for the labels chosen by each\\n    annotator (and their quality), as well as a consensus quality score for how confident we are that this consensus label is actually correct.\\n    It also computes similar quality scores for each annotator\\'s individual labels, and the quality of each annotator.\\n    Scores are between 0 and 1 (estimated via methods like CROWDLAB); lower scores indicate labels/annotators less likely to be correct.\\n\\n    To decide what data to collect additional labels for, try the :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>`\\n    (ActiveLab) function, which is intended for active learning with multiple annotators.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        ``labels_multiannotator[n][m]`` = label for n-th example given by m-th annotator.\\n\\n        For a dataset with K classes, each given label must be an integer in 0, 1, ..., K-1 or ``NaN`` if this annotator did not label a particular example.\\n        If you have string or other differently formatted labels, you can convert them to the proper format using :py:func:`format_multiannotator_labels <cleanlab.internal.multiannotator_utils.format_multiannotator_labels>`.\\n        If pd.DataFrame, column names should correspond to each annotator\\'s ID.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    consensus_method : str or List[str], default = \"majority_vote\"\\n        Specifies the method used to aggregate labels from multiple annotators into a single consensus label.\\n        Options include:\\n\\n        * ``majority_vote``: consensus obtained using a simple majority vote among annotators, with ties broken via ``pred_probs``.\\n        * ``best_quality``: consensus obtained by selecting the label with highest label quality (quality determined by method specified in ``quality_method``).\\n\\n        A List may be passed if you want to consider multiple methods for producing consensus labels.\\n        If a List is passed, then the 0th element of the list is the method used to produce columns `consensus_label`, `consensus_quality_score`, `annotator_agreement` in the returned DataFrame.\\n        The remaning (1st, 2nd, 3rd, etc.) elements of this list are output as extra columns in the returned pandas DataFrame with names formatted as:\\n        `consensus_label_SUFFIX`, `consensus_quality_score_SUFFIX` where `SUFFIX` = each element of this\\n        list, which must correspond to a valid method for computing consensus labels.\\n    quality_method : str, default = \"crowdlab\"\\n        Specifies the method used to calculate the quality of the consensus label.\\n        Options include:\\n\\n        * ``crowdlab``: an emsemble method that weighs both the annotators\\' labels as well as the model\\'s prediction.\\n        * ``agreement``: the fraction of annotators that agree with the consensus label.\\n    calibrate_probs : bool, default = False\\n        Boolean value that specifies whether the provided `pred_probs` should be re-calibrated to better match the annotators\\' empirical label distribution.\\n        We recommend setting this to True in active learning applications, in order to prevent overconfident models from suggesting the wrong examples to collect labels for.\\n    return_detailed_quality: bool, default = True\\n        Boolean to specify if `detailed_label_quality` is returned.\\n    return_annotator_stats : bool, default = True\\n        Boolean to specify if `annotator_stats` is returned.\\n    return_weights : bool, default = False\\n        Boolean to specify if `model_weight` and `annotator_weight` is returned.\\n        Model and annotator weights are applicable for ``quality_method == crowdlab``, will return ``None`` for any other quality methods.\\n    verbose : bool, default = True\\n        Important warnings and other printed statements may be suppressed if ``verbose`` is set to ``False``.\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments to pass into :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n    Returns\\n    -------\\n    labels_info : dict\\n        Dictionary containing up to 5 pandas DataFrame with keys as below:\\n\\n        ``label_quality`` : pandas.DataFrame\\n            pandas DataFrame in which each row corresponds to one example, with columns:\\n\\n            * ``num_annotations``: the number of annotators that have labeled each example.\\n            * ``consensus_label``: the single label that is best for each example (you can control how it is derived from all annotators\\' labels via the argument: ``consensus_method``).\\n            * ``annotator_agreement``: the fraction of annotators that agree with the consensus label (only consider the annotators that labeled that particular example).\\n            * ``consensus_quality_score``: label quality score for consensus label, calculated by the method specified in ``quality_method``.\\n\\n        ``detailed_label_quality`` : pandas.DataFrame\\n            Only returned if `return_detailed_quality=True`.\\n            Returns a pandas DataFrame with columns `quality_annotator_1`, `quality_annotator_2`, ..., `quality_annotator_M` where each entry is\\n            the label quality score for the labels provided by each annotator (is ``NaN`` for examples which this annotator did not label).\\n\\n        ``annotator_stats`` : pandas.DataFrame\\n            Only returned if `return_annotator_stats=True`.\\n            Returns overall statistics about each annotator, sorted by lowest annotator_quality first.\\n            pandas DataFrame in which each row corresponds to one annotator (the row IDs correspond to annotator IDs), with columns:\\n\\n            * ``annotator_quality``: overall quality of a given annotator\\'s labels, calculated by the method specified in ``quality_method``.\\n            * ``num_examples_labeled``: number of examples annotated by a given annotator.\\n            * ``agreement_with_consensus``: fraction of examples where a given annotator agrees with the consensus label.\\n            * ``worst_class``: the class that is most frequently mislabeled by a given annotator.\\n\\n        ``model_weight`` : float\\n            Only returned if `return_weights=True`. It is only applicable for ``quality_method == crowdlab``.\\n            The model weight specifies the weight of classifier model in weighted averages used to estimate label quality\\n            This number is an estimate of how trustworthy the model is relative the annotators.\\n\\n        ``annotator_weight`` : np.ndarray\\n            Only returned if `return_weights=True`. It is only applicable for ``quality_method == crowdlab``.\\n            An array of shape ``(M,)`` where M is the number of annotators, specifying the weight of each annotator in weighted averages used to estimate label quality.\\n            These weights are estimates of how trustworthy each annotator is relative to the other annotators.\\n\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        index_col = labels_multiannotator.index\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n        index_col = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    if return_weights == True and quality_method != 'crowdlab':\n        raise ValueError(\"Model and annotator weights are only applicable to the crowdlab quality method. Either set return_weights=False or quality_method='crowdlab'.\")\n    assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, annotator_ids=annotator_ids)\n    num_annotations = np.sum(~np.isnan(labels_multiannotator), axis=1)\n    if calibrate_probs:\n        optimal_temp = find_best_temp_scaler(labels_multiannotator, pred_probs)\n        pred_probs = temp_scale_pred_probs(pred_probs, optimal_temp)\n    if not isinstance(consensus_method, list):\n        consensus_method = [consensus_method]\n    if 'best_quality' in consensus_method or 'majority_vote' in consensus_method:\n        majority_vote_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, verbose=False)\n        (MV_annotator_agreement, MV_consensus_quality_score, MV_post_pred_probs, MV_model_weight, MV_annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=majority_vote_label, quality_method=quality_method, verbose=verbose, label_quality_score_kwargs=label_quality_score_kwargs)\n    label_quality = pd.DataFrame({'num_annotations': num_annotations}, index=index_col)\n    valid_methods = ['majority_vote', 'best_quality']\n    main_method = True\n    for curr_method in consensus_method:\n        if curr_method == 'majority_vote':\n            consensus_label = majority_vote_label\n            annotator_agreement = MV_annotator_agreement\n            consensus_quality_score = MV_consensus_quality_score\n            post_pred_probs = MV_post_pred_probs\n            model_weight = MV_model_weight\n            annotator_weight = MV_annotator_weight\n        elif curr_method == 'best_quality':\n            consensus_label = np.full(len(majority_vote_label), np.nan)\n            for i in range(len(consensus_label)):\n                max_pred_probs_ind = np.where(MV_post_pred_probs[i] == np.max(MV_post_pred_probs[i]))[0]\n                if len(max_pred_probs_ind) == 1:\n                    consensus_label[i] = max_pred_probs_ind[0]\n                else:\n                    consensus_label[i] = majority_vote_label[i]\n            consensus_label = consensus_label.astype(int)\n            (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=consensus_label, quality_method=quality_method, verbose=verbose, label_quality_score_kwargs=label_quality_score_kwargs)\n        else:\n            raise ValueError(f'\\n                {curr_method} is not a valid consensus method!\\n                Please choose a valid consensus_method: {valid_methods}\\n                ')\n        if verbose:\n            check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_method=curr_method)\n        if main_method:\n            (label_quality['consensus_label'], label_quality['consensus_quality_score'], label_quality['annotator_agreement']) = (consensus_label, consensus_quality_score, annotator_agreement)\n            label_quality = label_quality.reindex(columns=['consensus_label', 'consensus_quality_score', 'annotator_agreement', 'num_annotations'])\n            detailed_label_quality = None\n            if return_detailed_quality:\n                detailed_label_quality = np.apply_along_axis(_get_annotator_label_quality_score, axis=0, arr=labels_multiannotator, pred_probs=post_pred_probs, label_quality_score_kwargs=label_quality_score_kwargs)\n                detailed_label_quality_df = pd.DataFrame(detailed_label_quality, index=index_col, columns=annotator_ids).add_prefix('quality_annotator_')\n            if return_annotator_stats:\n                annotator_stats = _get_annotator_stats(labels_multiannotator=labels_multiannotator, pred_probs=post_pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=model_weight, annotator_weight=annotator_weight, consensus_quality_score=consensus_quality_score, detailed_label_quality=detailed_label_quality, annotator_ids=annotator_ids, quality_method=quality_method)\n            main_method = False\n        else:\n            (label_quality[f'consensus_label_{curr_method}'], label_quality[f'consensus_quality_score_{curr_method}'], label_quality[f'annotator_agreement_{curr_method}']) = (consensus_label, consensus_quality_score, annotator_agreement)\n    labels_info = {'label_quality': label_quality}\n    if return_detailed_quality:\n        labels_info['detailed_label_quality'] = detailed_label_quality_df\n    if return_annotator_stats:\n        labels_info['annotator_stats'] = annotator_stats\n    if return_weights:\n        labels_info['model_weight'] = model_weight\n        labels_info['annotator_weight'] = annotator_weight\n    return labels_info",
            "def get_label_quality_multiannotator(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: np.ndarray, *, consensus_method: Union[str, List[str]]='best_quality', quality_method: str='crowdlab', calibrate_probs: bool=False, return_detailed_quality: bool=True, return_annotator_stats: bool=True, return_weights: bool=False, verbose: bool=True, label_quality_score_kwargs: dict={}) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns label quality scores for each example and for each annotator in a dataset labeled by multiple annotators.\\n\\n    This function is for multiclass classification datasets where examples have been labeled by\\n    multiple annotators (not necessarily the same number of annotators per example).\\n\\n    It computes one consensus label for each example that best accounts for the labels chosen by each\\n    annotator (and their quality), as well as a consensus quality score for how confident we are that this consensus label is actually correct.\\n    It also computes similar quality scores for each annotator\\'s individual labels, and the quality of each annotator.\\n    Scores are between 0 and 1 (estimated via methods like CROWDLAB); lower scores indicate labels/annotators less likely to be correct.\\n\\n    To decide what data to collect additional labels for, try the :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>`\\n    (ActiveLab) function, which is intended for active learning with multiple annotators.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        ``labels_multiannotator[n][m]`` = label for n-th example given by m-th annotator.\\n\\n        For a dataset with K classes, each given label must be an integer in 0, 1, ..., K-1 or ``NaN`` if this annotator did not label a particular example.\\n        If you have string or other differently formatted labels, you can convert them to the proper format using :py:func:`format_multiannotator_labels <cleanlab.internal.multiannotator_utils.format_multiannotator_labels>`.\\n        If pd.DataFrame, column names should correspond to each annotator\\'s ID.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    consensus_method : str or List[str], default = \"majority_vote\"\\n        Specifies the method used to aggregate labels from multiple annotators into a single consensus label.\\n        Options include:\\n\\n        * ``majority_vote``: consensus obtained using a simple majority vote among annotators, with ties broken via ``pred_probs``.\\n        * ``best_quality``: consensus obtained by selecting the label with highest label quality (quality determined by method specified in ``quality_method``).\\n\\n        A List may be passed if you want to consider multiple methods for producing consensus labels.\\n        If a List is passed, then the 0th element of the list is the method used to produce columns `consensus_label`, `consensus_quality_score`, `annotator_agreement` in the returned DataFrame.\\n        The remaning (1st, 2nd, 3rd, etc.) elements of this list are output as extra columns in the returned pandas DataFrame with names formatted as:\\n        `consensus_label_SUFFIX`, `consensus_quality_score_SUFFIX` where `SUFFIX` = each element of this\\n        list, which must correspond to a valid method for computing consensus labels.\\n    quality_method : str, default = \"crowdlab\"\\n        Specifies the method used to calculate the quality of the consensus label.\\n        Options include:\\n\\n        * ``crowdlab``: an emsemble method that weighs both the annotators\\' labels as well as the model\\'s prediction.\\n        * ``agreement``: the fraction of annotators that agree with the consensus label.\\n    calibrate_probs : bool, default = False\\n        Boolean value that specifies whether the provided `pred_probs` should be re-calibrated to better match the annotators\\' empirical label distribution.\\n        We recommend setting this to True in active learning applications, in order to prevent overconfident models from suggesting the wrong examples to collect labels for.\\n    return_detailed_quality: bool, default = True\\n        Boolean to specify if `detailed_label_quality` is returned.\\n    return_annotator_stats : bool, default = True\\n        Boolean to specify if `annotator_stats` is returned.\\n    return_weights : bool, default = False\\n        Boolean to specify if `model_weight` and `annotator_weight` is returned.\\n        Model and annotator weights are applicable for ``quality_method == crowdlab``, will return ``None`` for any other quality methods.\\n    verbose : bool, default = True\\n        Important warnings and other printed statements may be suppressed if ``verbose`` is set to ``False``.\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments to pass into :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n    Returns\\n    -------\\n    labels_info : dict\\n        Dictionary containing up to 5 pandas DataFrame with keys as below:\\n\\n        ``label_quality`` : pandas.DataFrame\\n            pandas DataFrame in which each row corresponds to one example, with columns:\\n\\n            * ``num_annotations``: the number of annotators that have labeled each example.\\n            * ``consensus_label``: the single label that is best for each example (you can control how it is derived from all annotators\\' labels via the argument: ``consensus_method``).\\n            * ``annotator_agreement``: the fraction of annotators that agree with the consensus label (only consider the annotators that labeled that particular example).\\n            * ``consensus_quality_score``: label quality score for consensus label, calculated by the method specified in ``quality_method``.\\n\\n        ``detailed_label_quality`` : pandas.DataFrame\\n            Only returned if `return_detailed_quality=True`.\\n            Returns a pandas DataFrame with columns `quality_annotator_1`, `quality_annotator_2`, ..., `quality_annotator_M` where each entry is\\n            the label quality score for the labels provided by each annotator (is ``NaN`` for examples which this annotator did not label).\\n\\n        ``annotator_stats`` : pandas.DataFrame\\n            Only returned if `return_annotator_stats=True`.\\n            Returns overall statistics about each annotator, sorted by lowest annotator_quality first.\\n            pandas DataFrame in which each row corresponds to one annotator (the row IDs correspond to annotator IDs), with columns:\\n\\n            * ``annotator_quality``: overall quality of a given annotator\\'s labels, calculated by the method specified in ``quality_method``.\\n            * ``num_examples_labeled``: number of examples annotated by a given annotator.\\n            * ``agreement_with_consensus``: fraction of examples where a given annotator agrees with the consensus label.\\n            * ``worst_class``: the class that is most frequently mislabeled by a given annotator.\\n\\n        ``model_weight`` : float\\n            Only returned if `return_weights=True`. It is only applicable for ``quality_method == crowdlab``.\\n            The model weight specifies the weight of classifier model in weighted averages used to estimate label quality\\n            This number is an estimate of how trustworthy the model is relative the annotators.\\n\\n        ``annotator_weight`` : np.ndarray\\n            Only returned if `return_weights=True`. It is only applicable for ``quality_method == crowdlab``.\\n            An array of shape ``(M,)`` where M is the number of annotators, specifying the weight of each annotator in weighted averages used to estimate label quality.\\n            These weights are estimates of how trustworthy each annotator is relative to the other annotators.\\n\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        index_col = labels_multiannotator.index\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n        index_col = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    if return_weights == True and quality_method != 'crowdlab':\n        raise ValueError(\"Model and annotator weights are only applicable to the crowdlab quality method. Either set return_weights=False or quality_method='crowdlab'.\")\n    assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, annotator_ids=annotator_ids)\n    num_annotations = np.sum(~np.isnan(labels_multiannotator), axis=1)\n    if calibrate_probs:\n        optimal_temp = find_best_temp_scaler(labels_multiannotator, pred_probs)\n        pred_probs = temp_scale_pred_probs(pred_probs, optimal_temp)\n    if not isinstance(consensus_method, list):\n        consensus_method = [consensus_method]\n    if 'best_quality' in consensus_method or 'majority_vote' in consensus_method:\n        majority_vote_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, verbose=False)\n        (MV_annotator_agreement, MV_consensus_quality_score, MV_post_pred_probs, MV_model_weight, MV_annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=majority_vote_label, quality_method=quality_method, verbose=verbose, label_quality_score_kwargs=label_quality_score_kwargs)\n    label_quality = pd.DataFrame({'num_annotations': num_annotations}, index=index_col)\n    valid_methods = ['majority_vote', 'best_quality']\n    main_method = True\n    for curr_method in consensus_method:\n        if curr_method == 'majority_vote':\n            consensus_label = majority_vote_label\n            annotator_agreement = MV_annotator_agreement\n            consensus_quality_score = MV_consensus_quality_score\n            post_pred_probs = MV_post_pred_probs\n            model_weight = MV_model_weight\n            annotator_weight = MV_annotator_weight\n        elif curr_method == 'best_quality':\n            consensus_label = np.full(len(majority_vote_label), np.nan)\n            for i in range(len(consensus_label)):\n                max_pred_probs_ind = np.where(MV_post_pred_probs[i] == np.max(MV_post_pred_probs[i]))[0]\n                if len(max_pred_probs_ind) == 1:\n                    consensus_label[i] = max_pred_probs_ind[0]\n                else:\n                    consensus_label[i] = majority_vote_label[i]\n            consensus_label = consensus_label.astype(int)\n            (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=consensus_label, quality_method=quality_method, verbose=verbose, label_quality_score_kwargs=label_quality_score_kwargs)\n        else:\n            raise ValueError(f'\\n                {curr_method} is not a valid consensus method!\\n                Please choose a valid consensus_method: {valid_methods}\\n                ')\n        if verbose:\n            check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_method=curr_method)\n        if main_method:\n            (label_quality['consensus_label'], label_quality['consensus_quality_score'], label_quality['annotator_agreement']) = (consensus_label, consensus_quality_score, annotator_agreement)\n            label_quality = label_quality.reindex(columns=['consensus_label', 'consensus_quality_score', 'annotator_agreement', 'num_annotations'])\n            detailed_label_quality = None\n            if return_detailed_quality:\n                detailed_label_quality = np.apply_along_axis(_get_annotator_label_quality_score, axis=0, arr=labels_multiannotator, pred_probs=post_pred_probs, label_quality_score_kwargs=label_quality_score_kwargs)\n                detailed_label_quality_df = pd.DataFrame(detailed_label_quality, index=index_col, columns=annotator_ids).add_prefix('quality_annotator_')\n            if return_annotator_stats:\n                annotator_stats = _get_annotator_stats(labels_multiannotator=labels_multiannotator, pred_probs=post_pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=model_weight, annotator_weight=annotator_weight, consensus_quality_score=consensus_quality_score, detailed_label_quality=detailed_label_quality, annotator_ids=annotator_ids, quality_method=quality_method)\n            main_method = False\n        else:\n            (label_quality[f'consensus_label_{curr_method}'], label_quality[f'consensus_quality_score_{curr_method}'], label_quality[f'annotator_agreement_{curr_method}']) = (consensus_label, consensus_quality_score, annotator_agreement)\n    labels_info = {'label_quality': label_quality}\n    if return_detailed_quality:\n        labels_info['detailed_label_quality'] = detailed_label_quality_df\n    if return_annotator_stats:\n        labels_info['annotator_stats'] = annotator_stats\n    if return_weights:\n        labels_info['model_weight'] = model_weight\n        labels_info['annotator_weight'] = annotator_weight\n    return labels_info",
            "def get_label_quality_multiannotator(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: np.ndarray, *, consensus_method: Union[str, List[str]]='best_quality', quality_method: str='crowdlab', calibrate_probs: bool=False, return_detailed_quality: bool=True, return_annotator_stats: bool=True, return_weights: bool=False, verbose: bool=True, label_quality_score_kwargs: dict={}) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns label quality scores for each example and for each annotator in a dataset labeled by multiple annotators.\\n\\n    This function is for multiclass classification datasets where examples have been labeled by\\n    multiple annotators (not necessarily the same number of annotators per example).\\n\\n    It computes one consensus label for each example that best accounts for the labels chosen by each\\n    annotator (and their quality), as well as a consensus quality score for how confident we are that this consensus label is actually correct.\\n    It also computes similar quality scores for each annotator\\'s individual labels, and the quality of each annotator.\\n    Scores are between 0 and 1 (estimated via methods like CROWDLAB); lower scores indicate labels/annotators less likely to be correct.\\n\\n    To decide what data to collect additional labels for, try the :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>`\\n    (ActiveLab) function, which is intended for active learning with multiple annotators.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        ``labels_multiannotator[n][m]`` = label for n-th example given by m-th annotator.\\n\\n        For a dataset with K classes, each given label must be an integer in 0, 1, ..., K-1 or ``NaN`` if this annotator did not label a particular example.\\n        If you have string or other differently formatted labels, you can convert them to the proper format using :py:func:`format_multiannotator_labels <cleanlab.internal.multiannotator_utils.format_multiannotator_labels>`.\\n        If pd.DataFrame, column names should correspond to each annotator\\'s ID.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    consensus_method : str or List[str], default = \"majority_vote\"\\n        Specifies the method used to aggregate labels from multiple annotators into a single consensus label.\\n        Options include:\\n\\n        * ``majority_vote``: consensus obtained using a simple majority vote among annotators, with ties broken via ``pred_probs``.\\n        * ``best_quality``: consensus obtained by selecting the label with highest label quality (quality determined by method specified in ``quality_method``).\\n\\n        A List may be passed if you want to consider multiple methods for producing consensus labels.\\n        If a List is passed, then the 0th element of the list is the method used to produce columns `consensus_label`, `consensus_quality_score`, `annotator_agreement` in the returned DataFrame.\\n        The remaning (1st, 2nd, 3rd, etc.) elements of this list are output as extra columns in the returned pandas DataFrame with names formatted as:\\n        `consensus_label_SUFFIX`, `consensus_quality_score_SUFFIX` where `SUFFIX` = each element of this\\n        list, which must correspond to a valid method for computing consensus labels.\\n    quality_method : str, default = \"crowdlab\"\\n        Specifies the method used to calculate the quality of the consensus label.\\n        Options include:\\n\\n        * ``crowdlab``: an emsemble method that weighs both the annotators\\' labels as well as the model\\'s prediction.\\n        * ``agreement``: the fraction of annotators that agree with the consensus label.\\n    calibrate_probs : bool, default = False\\n        Boolean value that specifies whether the provided `pred_probs` should be re-calibrated to better match the annotators\\' empirical label distribution.\\n        We recommend setting this to True in active learning applications, in order to prevent overconfident models from suggesting the wrong examples to collect labels for.\\n    return_detailed_quality: bool, default = True\\n        Boolean to specify if `detailed_label_quality` is returned.\\n    return_annotator_stats : bool, default = True\\n        Boolean to specify if `annotator_stats` is returned.\\n    return_weights : bool, default = False\\n        Boolean to specify if `model_weight` and `annotator_weight` is returned.\\n        Model and annotator weights are applicable for ``quality_method == crowdlab``, will return ``None`` for any other quality methods.\\n    verbose : bool, default = True\\n        Important warnings and other printed statements may be suppressed if ``verbose`` is set to ``False``.\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments to pass into :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n    Returns\\n    -------\\n    labels_info : dict\\n        Dictionary containing up to 5 pandas DataFrame with keys as below:\\n\\n        ``label_quality`` : pandas.DataFrame\\n            pandas DataFrame in which each row corresponds to one example, with columns:\\n\\n            * ``num_annotations``: the number of annotators that have labeled each example.\\n            * ``consensus_label``: the single label that is best for each example (you can control how it is derived from all annotators\\' labels via the argument: ``consensus_method``).\\n            * ``annotator_agreement``: the fraction of annotators that agree with the consensus label (only consider the annotators that labeled that particular example).\\n            * ``consensus_quality_score``: label quality score for consensus label, calculated by the method specified in ``quality_method``.\\n\\n        ``detailed_label_quality`` : pandas.DataFrame\\n            Only returned if `return_detailed_quality=True`.\\n            Returns a pandas DataFrame with columns `quality_annotator_1`, `quality_annotator_2`, ..., `quality_annotator_M` where each entry is\\n            the label quality score for the labels provided by each annotator (is ``NaN`` for examples which this annotator did not label).\\n\\n        ``annotator_stats`` : pandas.DataFrame\\n            Only returned if `return_annotator_stats=True`.\\n            Returns overall statistics about each annotator, sorted by lowest annotator_quality first.\\n            pandas DataFrame in which each row corresponds to one annotator (the row IDs correspond to annotator IDs), with columns:\\n\\n            * ``annotator_quality``: overall quality of a given annotator\\'s labels, calculated by the method specified in ``quality_method``.\\n            * ``num_examples_labeled``: number of examples annotated by a given annotator.\\n            * ``agreement_with_consensus``: fraction of examples where a given annotator agrees with the consensus label.\\n            * ``worst_class``: the class that is most frequently mislabeled by a given annotator.\\n\\n        ``model_weight`` : float\\n            Only returned if `return_weights=True`. It is only applicable for ``quality_method == crowdlab``.\\n            The model weight specifies the weight of classifier model in weighted averages used to estimate label quality\\n            This number is an estimate of how trustworthy the model is relative the annotators.\\n\\n        ``annotator_weight`` : np.ndarray\\n            Only returned if `return_weights=True`. It is only applicable for ``quality_method == crowdlab``.\\n            An array of shape ``(M,)`` where M is the number of annotators, specifying the weight of each annotator in weighted averages used to estimate label quality.\\n            These weights are estimates of how trustworthy each annotator is relative to the other annotators.\\n\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        index_col = labels_multiannotator.index\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n        index_col = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    if return_weights == True and quality_method != 'crowdlab':\n        raise ValueError(\"Model and annotator weights are only applicable to the crowdlab quality method. Either set return_weights=False or quality_method='crowdlab'.\")\n    assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, annotator_ids=annotator_ids)\n    num_annotations = np.sum(~np.isnan(labels_multiannotator), axis=1)\n    if calibrate_probs:\n        optimal_temp = find_best_temp_scaler(labels_multiannotator, pred_probs)\n        pred_probs = temp_scale_pred_probs(pred_probs, optimal_temp)\n    if not isinstance(consensus_method, list):\n        consensus_method = [consensus_method]\n    if 'best_quality' in consensus_method or 'majority_vote' in consensus_method:\n        majority_vote_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, verbose=False)\n        (MV_annotator_agreement, MV_consensus_quality_score, MV_post_pred_probs, MV_model_weight, MV_annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=majority_vote_label, quality_method=quality_method, verbose=verbose, label_quality_score_kwargs=label_quality_score_kwargs)\n    label_quality = pd.DataFrame({'num_annotations': num_annotations}, index=index_col)\n    valid_methods = ['majority_vote', 'best_quality']\n    main_method = True\n    for curr_method in consensus_method:\n        if curr_method == 'majority_vote':\n            consensus_label = majority_vote_label\n            annotator_agreement = MV_annotator_agreement\n            consensus_quality_score = MV_consensus_quality_score\n            post_pred_probs = MV_post_pred_probs\n            model_weight = MV_model_weight\n            annotator_weight = MV_annotator_weight\n        elif curr_method == 'best_quality':\n            consensus_label = np.full(len(majority_vote_label), np.nan)\n            for i in range(len(consensus_label)):\n                max_pred_probs_ind = np.where(MV_post_pred_probs[i] == np.max(MV_post_pred_probs[i]))[0]\n                if len(max_pred_probs_ind) == 1:\n                    consensus_label[i] = max_pred_probs_ind[0]\n                else:\n                    consensus_label[i] = majority_vote_label[i]\n            consensus_label = consensus_label.astype(int)\n            (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=consensus_label, quality_method=quality_method, verbose=verbose, label_quality_score_kwargs=label_quality_score_kwargs)\n        else:\n            raise ValueError(f'\\n                {curr_method} is not a valid consensus method!\\n                Please choose a valid consensus_method: {valid_methods}\\n                ')\n        if verbose:\n            check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_method=curr_method)\n        if main_method:\n            (label_quality['consensus_label'], label_quality['consensus_quality_score'], label_quality['annotator_agreement']) = (consensus_label, consensus_quality_score, annotator_agreement)\n            label_quality = label_quality.reindex(columns=['consensus_label', 'consensus_quality_score', 'annotator_agreement', 'num_annotations'])\n            detailed_label_quality = None\n            if return_detailed_quality:\n                detailed_label_quality = np.apply_along_axis(_get_annotator_label_quality_score, axis=0, arr=labels_multiannotator, pred_probs=post_pred_probs, label_quality_score_kwargs=label_quality_score_kwargs)\n                detailed_label_quality_df = pd.DataFrame(detailed_label_quality, index=index_col, columns=annotator_ids).add_prefix('quality_annotator_')\n            if return_annotator_stats:\n                annotator_stats = _get_annotator_stats(labels_multiannotator=labels_multiannotator, pred_probs=post_pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=model_weight, annotator_weight=annotator_weight, consensus_quality_score=consensus_quality_score, detailed_label_quality=detailed_label_quality, annotator_ids=annotator_ids, quality_method=quality_method)\n            main_method = False\n        else:\n            (label_quality[f'consensus_label_{curr_method}'], label_quality[f'consensus_quality_score_{curr_method}'], label_quality[f'annotator_agreement_{curr_method}']) = (consensus_label, consensus_quality_score, annotator_agreement)\n    labels_info = {'label_quality': label_quality}\n    if return_detailed_quality:\n        labels_info['detailed_label_quality'] = detailed_label_quality_df\n    if return_annotator_stats:\n        labels_info['annotator_stats'] = annotator_stats\n    if return_weights:\n        labels_info['model_weight'] = model_weight\n        labels_info['annotator_weight'] = annotator_weight\n    return labels_info",
            "def get_label_quality_multiannotator(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: np.ndarray, *, consensus_method: Union[str, List[str]]='best_quality', quality_method: str='crowdlab', calibrate_probs: bool=False, return_detailed_quality: bool=True, return_annotator_stats: bool=True, return_weights: bool=False, verbose: bool=True, label_quality_score_kwargs: dict={}) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns label quality scores for each example and for each annotator in a dataset labeled by multiple annotators.\\n\\n    This function is for multiclass classification datasets where examples have been labeled by\\n    multiple annotators (not necessarily the same number of annotators per example).\\n\\n    It computes one consensus label for each example that best accounts for the labels chosen by each\\n    annotator (and their quality), as well as a consensus quality score for how confident we are that this consensus label is actually correct.\\n    It also computes similar quality scores for each annotator\\'s individual labels, and the quality of each annotator.\\n    Scores are between 0 and 1 (estimated via methods like CROWDLAB); lower scores indicate labels/annotators less likely to be correct.\\n\\n    To decide what data to collect additional labels for, try the :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>`\\n    (ActiveLab) function, which is intended for active learning with multiple annotators.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        ``labels_multiannotator[n][m]`` = label for n-th example given by m-th annotator.\\n\\n        For a dataset with K classes, each given label must be an integer in 0, 1, ..., K-1 or ``NaN`` if this annotator did not label a particular example.\\n        If you have string or other differently formatted labels, you can convert them to the proper format using :py:func:`format_multiannotator_labels <cleanlab.internal.multiannotator_utils.format_multiannotator_labels>`.\\n        If pd.DataFrame, column names should correspond to each annotator\\'s ID.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    consensus_method : str or List[str], default = \"majority_vote\"\\n        Specifies the method used to aggregate labels from multiple annotators into a single consensus label.\\n        Options include:\\n\\n        * ``majority_vote``: consensus obtained using a simple majority vote among annotators, with ties broken via ``pred_probs``.\\n        * ``best_quality``: consensus obtained by selecting the label with highest label quality (quality determined by method specified in ``quality_method``).\\n\\n        A List may be passed if you want to consider multiple methods for producing consensus labels.\\n        If a List is passed, then the 0th element of the list is the method used to produce columns `consensus_label`, `consensus_quality_score`, `annotator_agreement` in the returned DataFrame.\\n        The remaning (1st, 2nd, 3rd, etc.) elements of this list are output as extra columns in the returned pandas DataFrame with names formatted as:\\n        `consensus_label_SUFFIX`, `consensus_quality_score_SUFFIX` where `SUFFIX` = each element of this\\n        list, which must correspond to a valid method for computing consensus labels.\\n    quality_method : str, default = \"crowdlab\"\\n        Specifies the method used to calculate the quality of the consensus label.\\n        Options include:\\n\\n        * ``crowdlab``: an emsemble method that weighs both the annotators\\' labels as well as the model\\'s prediction.\\n        * ``agreement``: the fraction of annotators that agree with the consensus label.\\n    calibrate_probs : bool, default = False\\n        Boolean value that specifies whether the provided `pred_probs` should be re-calibrated to better match the annotators\\' empirical label distribution.\\n        We recommend setting this to True in active learning applications, in order to prevent overconfident models from suggesting the wrong examples to collect labels for.\\n    return_detailed_quality: bool, default = True\\n        Boolean to specify if `detailed_label_quality` is returned.\\n    return_annotator_stats : bool, default = True\\n        Boolean to specify if `annotator_stats` is returned.\\n    return_weights : bool, default = False\\n        Boolean to specify if `model_weight` and `annotator_weight` is returned.\\n        Model and annotator weights are applicable for ``quality_method == crowdlab``, will return ``None`` for any other quality methods.\\n    verbose : bool, default = True\\n        Important warnings and other printed statements may be suppressed if ``verbose`` is set to ``False``.\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments to pass into :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n    Returns\\n    -------\\n    labels_info : dict\\n        Dictionary containing up to 5 pandas DataFrame with keys as below:\\n\\n        ``label_quality`` : pandas.DataFrame\\n            pandas DataFrame in which each row corresponds to one example, with columns:\\n\\n            * ``num_annotations``: the number of annotators that have labeled each example.\\n            * ``consensus_label``: the single label that is best for each example (you can control how it is derived from all annotators\\' labels via the argument: ``consensus_method``).\\n            * ``annotator_agreement``: the fraction of annotators that agree with the consensus label (only consider the annotators that labeled that particular example).\\n            * ``consensus_quality_score``: label quality score for consensus label, calculated by the method specified in ``quality_method``.\\n\\n        ``detailed_label_quality`` : pandas.DataFrame\\n            Only returned if `return_detailed_quality=True`.\\n            Returns a pandas DataFrame with columns `quality_annotator_1`, `quality_annotator_2`, ..., `quality_annotator_M` where each entry is\\n            the label quality score for the labels provided by each annotator (is ``NaN`` for examples which this annotator did not label).\\n\\n        ``annotator_stats`` : pandas.DataFrame\\n            Only returned if `return_annotator_stats=True`.\\n            Returns overall statistics about each annotator, sorted by lowest annotator_quality first.\\n            pandas DataFrame in which each row corresponds to one annotator (the row IDs correspond to annotator IDs), with columns:\\n\\n            * ``annotator_quality``: overall quality of a given annotator\\'s labels, calculated by the method specified in ``quality_method``.\\n            * ``num_examples_labeled``: number of examples annotated by a given annotator.\\n            * ``agreement_with_consensus``: fraction of examples where a given annotator agrees with the consensus label.\\n            * ``worst_class``: the class that is most frequently mislabeled by a given annotator.\\n\\n        ``model_weight`` : float\\n            Only returned if `return_weights=True`. It is only applicable for ``quality_method == crowdlab``.\\n            The model weight specifies the weight of classifier model in weighted averages used to estimate label quality\\n            This number is an estimate of how trustworthy the model is relative the annotators.\\n\\n        ``annotator_weight`` : np.ndarray\\n            Only returned if `return_weights=True`. It is only applicable for ``quality_method == crowdlab``.\\n            An array of shape ``(M,)`` where M is the number of annotators, specifying the weight of each annotator in weighted averages used to estimate label quality.\\n            These weights are estimates of how trustworthy each annotator is relative to the other annotators.\\n\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        index_col = labels_multiannotator.index\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n        index_col = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    if return_weights == True and quality_method != 'crowdlab':\n        raise ValueError(\"Model and annotator weights are only applicable to the crowdlab quality method. Either set return_weights=False or quality_method='crowdlab'.\")\n    assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, annotator_ids=annotator_ids)\n    num_annotations = np.sum(~np.isnan(labels_multiannotator), axis=1)\n    if calibrate_probs:\n        optimal_temp = find_best_temp_scaler(labels_multiannotator, pred_probs)\n        pred_probs = temp_scale_pred_probs(pred_probs, optimal_temp)\n    if not isinstance(consensus_method, list):\n        consensus_method = [consensus_method]\n    if 'best_quality' in consensus_method or 'majority_vote' in consensus_method:\n        majority_vote_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, verbose=False)\n        (MV_annotator_agreement, MV_consensus_quality_score, MV_post_pred_probs, MV_model_weight, MV_annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=majority_vote_label, quality_method=quality_method, verbose=verbose, label_quality_score_kwargs=label_quality_score_kwargs)\n    label_quality = pd.DataFrame({'num_annotations': num_annotations}, index=index_col)\n    valid_methods = ['majority_vote', 'best_quality']\n    main_method = True\n    for curr_method in consensus_method:\n        if curr_method == 'majority_vote':\n            consensus_label = majority_vote_label\n            annotator_agreement = MV_annotator_agreement\n            consensus_quality_score = MV_consensus_quality_score\n            post_pred_probs = MV_post_pred_probs\n            model_weight = MV_model_weight\n            annotator_weight = MV_annotator_weight\n        elif curr_method == 'best_quality':\n            consensus_label = np.full(len(majority_vote_label), np.nan)\n            for i in range(len(consensus_label)):\n                max_pred_probs_ind = np.where(MV_post_pred_probs[i] == np.max(MV_post_pred_probs[i]))[0]\n                if len(max_pred_probs_ind) == 1:\n                    consensus_label[i] = max_pred_probs_ind[0]\n                else:\n                    consensus_label[i] = majority_vote_label[i]\n            consensus_label = consensus_label.astype(int)\n            (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=consensus_label, quality_method=quality_method, verbose=verbose, label_quality_score_kwargs=label_quality_score_kwargs)\n        else:\n            raise ValueError(f'\\n                {curr_method} is not a valid consensus method!\\n                Please choose a valid consensus_method: {valid_methods}\\n                ')\n        if verbose:\n            check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_method=curr_method)\n        if main_method:\n            (label_quality['consensus_label'], label_quality['consensus_quality_score'], label_quality['annotator_agreement']) = (consensus_label, consensus_quality_score, annotator_agreement)\n            label_quality = label_quality.reindex(columns=['consensus_label', 'consensus_quality_score', 'annotator_agreement', 'num_annotations'])\n            detailed_label_quality = None\n            if return_detailed_quality:\n                detailed_label_quality = np.apply_along_axis(_get_annotator_label_quality_score, axis=0, arr=labels_multiannotator, pred_probs=post_pred_probs, label_quality_score_kwargs=label_quality_score_kwargs)\n                detailed_label_quality_df = pd.DataFrame(detailed_label_quality, index=index_col, columns=annotator_ids).add_prefix('quality_annotator_')\n            if return_annotator_stats:\n                annotator_stats = _get_annotator_stats(labels_multiannotator=labels_multiannotator, pred_probs=post_pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=model_weight, annotator_weight=annotator_weight, consensus_quality_score=consensus_quality_score, detailed_label_quality=detailed_label_quality, annotator_ids=annotator_ids, quality_method=quality_method)\n            main_method = False\n        else:\n            (label_quality[f'consensus_label_{curr_method}'], label_quality[f'consensus_quality_score_{curr_method}'], label_quality[f'annotator_agreement_{curr_method}']) = (consensus_label, consensus_quality_score, annotator_agreement)\n    labels_info = {'label_quality': label_quality}\n    if return_detailed_quality:\n        labels_info['detailed_label_quality'] = detailed_label_quality_df\n    if return_annotator_stats:\n        labels_info['annotator_stats'] = annotator_stats\n    if return_weights:\n        labels_info['model_weight'] = model_weight\n        labels_info['annotator_weight'] = annotator_weight\n    return labels_info"
        ]
    },
    {
        "func_name": "get_label_quality_multiannotator_ensemble",
        "original": "def get_label_quality_multiannotator_ensemble(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: np.ndarray, *, calibrate_probs: bool=False, return_detailed_quality: bool=True, return_annotator_stats: bool=True, return_weights: bool=False, verbose: bool=True, label_quality_score_kwargs: dict={}) -> Dict[str, Any]:\n    \"\"\"Returns label quality scores for each example and for each annotator, based on predictions from an ensemble of models.\n\n    This function is similar to :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>` but for settings where\n    you have trained an ensemble of multiple classifier models rather than a single model.\n\n    Parameters\n    ----------\n    labels_multiannotator : pd.DataFrame or np.ndarray\n        Multiannotator labels in the same format expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    pred_probs : np.ndarray\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\n    calibrate_probs : bool, default = False\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    return_detailed_quality: bool, default = True\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    return_annotator_stats : bool, default = True\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    return_weights : bool, default = False\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    verbose : bool, default = True\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    label_quality_score_kwargs : dict, optional\n        Keyword arguments in the same format expected by py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n\n    Returns\n    -------\n    labels_info : dict\n        Dictionary containing up to 5 pandas DataFrame with keys as below:\n\n        ``label_quality`` : pandas.DataFrame\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n\n        ``detailed_label_quality`` : pandas.DataFrame\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n\n        ``annotator_stats`` : pandas.DataFrame\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n\n        ``model_weight`` : np.ndarray\n            Only returned if `return_weights=True`.\n            An array of shape ``(P,)`` where is the number of models in the ensemble, specifying the weight of each classifier model in weighted averages used to estimate label quality.\n            These weigthts is an estimate of how trustworthy the model is relative the annotators.\n            An array of shape ``(P,)`` where is the number of models in the ensemble, specifying the model weight used in weighted averages.\n\n        ``annotator_weight`` : np.ndarray\n            Only returned if `return_weights=True`.\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n\n    See Also\n    --------\n    get_label_quality_multiannotator\n    \"\"\"\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        index_col = labels_multiannotator.index\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n        index_col = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, ensemble=True, annotator_ids=annotator_ids)\n    num_annotations = np.sum(~np.isnan(labels_multiannotator), axis=1)\n    if calibrate_probs:\n        for i in range(len(pred_probs)):\n            curr_pred_probs = pred_probs[i]\n            optimal_temp = find_best_temp_scaler(labels_multiannotator, curr_pred_probs)\n            pred_probs[i] = temp_scale_pred_probs(curr_pred_probs, optimal_temp)\n    label_quality = pd.DataFrame({'num_annotations': num_annotations}, index=index_col)\n    avg_pred_probs = np.mean(pred_probs, axis=0)\n    majority_vote_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=avg_pred_probs, verbose=False)\n    (MV_annotator_agreement, MV_consensus_quality_score, MV_post_pred_probs, MV_model_weight, MV_annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=majority_vote_label, verbose=verbose, ensemble=True, **label_quality_score_kwargs)\n    consensus_label = np.full(len(majority_vote_label), np.nan)\n    for i in range(len(consensus_label)):\n        max_pred_probs_ind = np.where(MV_post_pred_probs[i] == np.max(MV_post_pred_probs[i]))[0]\n        if len(max_pred_probs_ind) == 1:\n            consensus_label[i] = max_pred_probs_ind[0]\n        else:\n            consensus_label[i] = majority_vote_label[i]\n    consensus_label = consensus_label.astype(int)\n    (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=consensus_label, verbose=verbose, ensemble=True, **label_quality_score_kwargs)\n    if verbose:\n        check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_method='crowdlab')\n    (label_quality['consensus_label'], label_quality['consensus_quality_score'], label_quality['annotator_agreement']) = (consensus_label, consensus_quality_score, annotator_agreement)\n    label_quality = label_quality.reindex(columns=['consensus_label', 'consensus_quality_score', 'annotator_agreement', 'num_annotations'])\n    detailed_label_quality = None\n    if return_detailed_quality:\n        detailed_label_quality = np.apply_along_axis(_get_annotator_label_quality_score, axis=0, arr=labels_multiannotator, pred_probs=post_pred_probs, label_quality_score_kwargs=label_quality_score_kwargs)\n        detailed_label_quality_df = pd.DataFrame(detailed_label_quality, index=index_col, columns=annotator_ids).add_prefix('quality_annotator_')\n    if return_annotator_stats:\n        annotator_stats = _get_annotator_stats(labels_multiannotator=labels_multiannotator, pred_probs=post_pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=np.mean(model_weight), annotator_weight=annotator_weight, consensus_quality_score=consensus_quality_score, detailed_label_quality=detailed_label_quality, annotator_ids=annotator_ids)\n    labels_info = {'label_quality': label_quality}\n    if return_detailed_quality:\n        labels_info['detailed_label_quality'] = detailed_label_quality_df\n    if return_annotator_stats:\n        labels_info['annotator_stats'] = annotator_stats\n    if return_weights:\n        labels_info['model_weight'] = model_weight\n        labels_info['annotator_weight'] = annotator_weight\n    return labels_info",
        "mutated": [
            "def get_label_quality_multiannotator_ensemble(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: np.ndarray, *, calibrate_probs: bool=False, return_detailed_quality: bool=True, return_annotator_stats: bool=True, return_weights: bool=False, verbose: bool=True, label_quality_score_kwargs: dict={}) -> Dict[str, Any]:\n    if False:\n        i = 10\n    'Returns label quality scores for each example and for each annotator, based on predictions from an ensemble of models.\\n\\n    This function is similar to :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>` but for settings where\\n    you have trained an ensemble of multiple classifier models rather than a single model.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        Multiannotator labels in the same format expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    calibrate_probs : bool, default = False\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_detailed_quality: bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_annotator_stats : bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_weights : bool, default = False\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    verbose : bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments in the same format expected by py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    Returns\\n    -------\\n    labels_info : dict\\n        Dictionary containing up to 5 pandas DataFrame with keys as below:\\n\\n        ``label_quality`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``detailed_label_quality`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``annotator_stats`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``model_weight`` : np.ndarray\\n            Only returned if `return_weights=True`.\\n            An array of shape ``(P,)`` where is the number of models in the ensemble, specifying the weight of each classifier model in weighted averages used to estimate label quality.\\n            These weigthts is an estimate of how trustworthy the model is relative the annotators.\\n            An array of shape ``(P,)`` where is the number of models in the ensemble, specifying the model weight used in weighted averages.\\n\\n        ``annotator_weight`` : np.ndarray\\n            Only returned if `return_weights=True`.\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    See Also\\n    --------\\n    get_label_quality_multiannotator\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        index_col = labels_multiannotator.index\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n        index_col = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, ensemble=True, annotator_ids=annotator_ids)\n    num_annotations = np.sum(~np.isnan(labels_multiannotator), axis=1)\n    if calibrate_probs:\n        for i in range(len(pred_probs)):\n            curr_pred_probs = pred_probs[i]\n            optimal_temp = find_best_temp_scaler(labels_multiannotator, curr_pred_probs)\n            pred_probs[i] = temp_scale_pred_probs(curr_pred_probs, optimal_temp)\n    label_quality = pd.DataFrame({'num_annotations': num_annotations}, index=index_col)\n    avg_pred_probs = np.mean(pred_probs, axis=0)\n    majority_vote_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=avg_pred_probs, verbose=False)\n    (MV_annotator_agreement, MV_consensus_quality_score, MV_post_pred_probs, MV_model_weight, MV_annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=majority_vote_label, verbose=verbose, ensemble=True, **label_quality_score_kwargs)\n    consensus_label = np.full(len(majority_vote_label), np.nan)\n    for i in range(len(consensus_label)):\n        max_pred_probs_ind = np.where(MV_post_pred_probs[i] == np.max(MV_post_pred_probs[i]))[0]\n        if len(max_pred_probs_ind) == 1:\n            consensus_label[i] = max_pred_probs_ind[0]\n        else:\n            consensus_label[i] = majority_vote_label[i]\n    consensus_label = consensus_label.astype(int)\n    (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=consensus_label, verbose=verbose, ensemble=True, **label_quality_score_kwargs)\n    if verbose:\n        check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_method='crowdlab')\n    (label_quality['consensus_label'], label_quality['consensus_quality_score'], label_quality['annotator_agreement']) = (consensus_label, consensus_quality_score, annotator_agreement)\n    label_quality = label_quality.reindex(columns=['consensus_label', 'consensus_quality_score', 'annotator_agreement', 'num_annotations'])\n    detailed_label_quality = None\n    if return_detailed_quality:\n        detailed_label_quality = np.apply_along_axis(_get_annotator_label_quality_score, axis=0, arr=labels_multiannotator, pred_probs=post_pred_probs, label_quality_score_kwargs=label_quality_score_kwargs)\n        detailed_label_quality_df = pd.DataFrame(detailed_label_quality, index=index_col, columns=annotator_ids).add_prefix('quality_annotator_')\n    if return_annotator_stats:\n        annotator_stats = _get_annotator_stats(labels_multiannotator=labels_multiannotator, pred_probs=post_pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=np.mean(model_weight), annotator_weight=annotator_weight, consensus_quality_score=consensus_quality_score, detailed_label_quality=detailed_label_quality, annotator_ids=annotator_ids)\n    labels_info = {'label_quality': label_quality}\n    if return_detailed_quality:\n        labels_info['detailed_label_quality'] = detailed_label_quality_df\n    if return_annotator_stats:\n        labels_info['annotator_stats'] = annotator_stats\n    if return_weights:\n        labels_info['model_weight'] = model_weight\n        labels_info['annotator_weight'] = annotator_weight\n    return labels_info",
            "def get_label_quality_multiannotator_ensemble(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: np.ndarray, *, calibrate_probs: bool=False, return_detailed_quality: bool=True, return_annotator_stats: bool=True, return_weights: bool=False, verbose: bool=True, label_quality_score_kwargs: dict={}) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns label quality scores for each example and for each annotator, based on predictions from an ensemble of models.\\n\\n    This function is similar to :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>` but for settings where\\n    you have trained an ensemble of multiple classifier models rather than a single model.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        Multiannotator labels in the same format expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    calibrate_probs : bool, default = False\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_detailed_quality: bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_annotator_stats : bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_weights : bool, default = False\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    verbose : bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments in the same format expected by py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    Returns\\n    -------\\n    labels_info : dict\\n        Dictionary containing up to 5 pandas DataFrame with keys as below:\\n\\n        ``label_quality`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``detailed_label_quality`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``annotator_stats`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``model_weight`` : np.ndarray\\n            Only returned if `return_weights=True`.\\n            An array of shape ``(P,)`` where is the number of models in the ensemble, specifying the weight of each classifier model in weighted averages used to estimate label quality.\\n            These weigthts is an estimate of how trustworthy the model is relative the annotators.\\n            An array of shape ``(P,)`` where is the number of models in the ensemble, specifying the model weight used in weighted averages.\\n\\n        ``annotator_weight`` : np.ndarray\\n            Only returned if `return_weights=True`.\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    See Also\\n    --------\\n    get_label_quality_multiannotator\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        index_col = labels_multiannotator.index\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n        index_col = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, ensemble=True, annotator_ids=annotator_ids)\n    num_annotations = np.sum(~np.isnan(labels_multiannotator), axis=1)\n    if calibrate_probs:\n        for i in range(len(pred_probs)):\n            curr_pred_probs = pred_probs[i]\n            optimal_temp = find_best_temp_scaler(labels_multiannotator, curr_pred_probs)\n            pred_probs[i] = temp_scale_pred_probs(curr_pred_probs, optimal_temp)\n    label_quality = pd.DataFrame({'num_annotations': num_annotations}, index=index_col)\n    avg_pred_probs = np.mean(pred_probs, axis=0)\n    majority_vote_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=avg_pred_probs, verbose=False)\n    (MV_annotator_agreement, MV_consensus_quality_score, MV_post_pred_probs, MV_model_weight, MV_annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=majority_vote_label, verbose=verbose, ensemble=True, **label_quality_score_kwargs)\n    consensus_label = np.full(len(majority_vote_label), np.nan)\n    for i in range(len(consensus_label)):\n        max_pred_probs_ind = np.where(MV_post_pred_probs[i] == np.max(MV_post_pred_probs[i]))[0]\n        if len(max_pred_probs_ind) == 1:\n            consensus_label[i] = max_pred_probs_ind[0]\n        else:\n            consensus_label[i] = majority_vote_label[i]\n    consensus_label = consensus_label.astype(int)\n    (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=consensus_label, verbose=verbose, ensemble=True, **label_quality_score_kwargs)\n    if verbose:\n        check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_method='crowdlab')\n    (label_quality['consensus_label'], label_quality['consensus_quality_score'], label_quality['annotator_agreement']) = (consensus_label, consensus_quality_score, annotator_agreement)\n    label_quality = label_quality.reindex(columns=['consensus_label', 'consensus_quality_score', 'annotator_agreement', 'num_annotations'])\n    detailed_label_quality = None\n    if return_detailed_quality:\n        detailed_label_quality = np.apply_along_axis(_get_annotator_label_quality_score, axis=0, arr=labels_multiannotator, pred_probs=post_pred_probs, label_quality_score_kwargs=label_quality_score_kwargs)\n        detailed_label_quality_df = pd.DataFrame(detailed_label_quality, index=index_col, columns=annotator_ids).add_prefix('quality_annotator_')\n    if return_annotator_stats:\n        annotator_stats = _get_annotator_stats(labels_multiannotator=labels_multiannotator, pred_probs=post_pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=np.mean(model_weight), annotator_weight=annotator_weight, consensus_quality_score=consensus_quality_score, detailed_label_quality=detailed_label_quality, annotator_ids=annotator_ids)\n    labels_info = {'label_quality': label_quality}\n    if return_detailed_quality:\n        labels_info['detailed_label_quality'] = detailed_label_quality_df\n    if return_annotator_stats:\n        labels_info['annotator_stats'] = annotator_stats\n    if return_weights:\n        labels_info['model_weight'] = model_weight\n        labels_info['annotator_weight'] = annotator_weight\n    return labels_info",
            "def get_label_quality_multiannotator_ensemble(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: np.ndarray, *, calibrate_probs: bool=False, return_detailed_quality: bool=True, return_annotator_stats: bool=True, return_weights: bool=False, verbose: bool=True, label_quality_score_kwargs: dict={}) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns label quality scores for each example and for each annotator, based on predictions from an ensemble of models.\\n\\n    This function is similar to :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>` but for settings where\\n    you have trained an ensemble of multiple classifier models rather than a single model.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        Multiannotator labels in the same format expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    calibrate_probs : bool, default = False\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_detailed_quality: bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_annotator_stats : bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_weights : bool, default = False\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    verbose : bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments in the same format expected by py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    Returns\\n    -------\\n    labels_info : dict\\n        Dictionary containing up to 5 pandas DataFrame with keys as below:\\n\\n        ``label_quality`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``detailed_label_quality`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``annotator_stats`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``model_weight`` : np.ndarray\\n            Only returned if `return_weights=True`.\\n            An array of shape ``(P,)`` where is the number of models in the ensemble, specifying the weight of each classifier model in weighted averages used to estimate label quality.\\n            These weigthts is an estimate of how trustworthy the model is relative the annotators.\\n            An array of shape ``(P,)`` where is the number of models in the ensemble, specifying the model weight used in weighted averages.\\n\\n        ``annotator_weight`` : np.ndarray\\n            Only returned if `return_weights=True`.\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    See Also\\n    --------\\n    get_label_quality_multiannotator\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        index_col = labels_multiannotator.index\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n        index_col = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, ensemble=True, annotator_ids=annotator_ids)\n    num_annotations = np.sum(~np.isnan(labels_multiannotator), axis=1)\n    if calibrate_probs:\n        for i in range(len(pred_probs)):\n            curr_pred_probs = pred_probs[i]\n            optimal_temp = find_best_temp_scaler(labels_multiannotator, curr_pred_probs)\n            pred_probs[i] = temp_scale_pred_probs(curr_pred_probs, optimal_temp)\n    label_quality = pd.DataFrame({'num_annotations': num_annotations}, index=index_col)\n    avg_pred_probs = np.mean(pred_probs, axis=0)\n    majority_vote_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=avg_pred_probs, verbose=False)\n    (MV_annotator_agreement, MV_consensus_quality_score, MV_post_pred_probs, MV_model_weight, MV_annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=majority_vote_label, verbose=verbose, ensemble=True, **label_quality_score_kwargs)\n    consensus_label = np.full(len(majority_vote_label), np.nan)\n    for i in range(len(consensus_label)):\n        max_pred_probs_ind = np.where(MV_post_pred_probs[i] == np.max(MV_post_pred_probs[i]))[0]\n        if len(max_pred_probs_ind) == 1:\n            consensus_label[i] = max_pred_probs_ind[0]\n        else:\n            consensus_label[i] = majority_vote_label[i]\n    consensus_label = consensus_label.astype(int)\n    (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=consensus_label, verbose=verbose, ensemble=True, **label_quality_score_kwargs)\n    if verbose:\n        check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_method='crowdlab')\n    (label_quality['consensus_label'], label_quality['consensus_quality_score'], label_quality['annotator_agreement']) = (consensus_label, consensus_quality_score, annotator_agreement)\n    label_quality = label_quality.reindex(columns=['consensus_label', 'consensus_quality_score', 'annotator_agreement', 'num_annotations'])\n    detailed_label_quality = None\n    if return_detailed_quality:\n        detailed_label_quality = np.apply_along_axis(_get_annotator_label_quality_score, axis=0, arr=labels_multiannotator, pred_probs=post_pred_probs, label_quality_score_kwargs=label_quality_score_kwargs)\n        detailed_label_quality_df = pd.DataFrame(detailed_label_quality, index=index_col, columns=annotator_ids).add_prefix('quality_annotator_')\n    if return_annotator_stats:\n        annotator_stats = _get_annotator_stats(labels_multiannotator=labels_multiannotator, pred_probs=post_pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=np.mean(model_weight), annotator_weight=annotator_weight, consensus_quality_score=consensus_quality_score, detailed_label_quality=detailed_label_quality, annotator_ids=annotator_ids)\n    labels_info = {'label_quality': label_quality}\n    if return_detailed_quality:\n        labels_info['detailed_label_quality'] = detailed_label_quality_df\n    if return_annotator_stats:\n        labels_info['annotator_stats'] = annotator_stats\n    if return_weights:\n        labels_info['model_weight'] = model_weight\n        labels_info['annotator_weight'] = annotator_weight\n    return labels_info",
            "def get_label_quality_multiannotator_ensemble(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: np.ndarray, *, calibrate_probs: bool=False, return_detailed_quality: bool=True, return_annotator_stats: bool=True, return_weights: bool=False, verbose: bool=True, label_quality_score_kwargs: dict={}) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns label quality scores for each example and for each annotator, based on predictions from an ensemble of models.\\n\\n    This function is similar to :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>` but for settings where\\n    you have trained an ensemble of multiple classifier models rather than a single model.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        Multiannotator labels in the same format expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    calibrate_probs : bool, default = False\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_detailed_quality: bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_annotator_stats : bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_weights : bool, default = False\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    verbose : bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments in the same format expected by py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    Returns\\n    -------\\n    labels_info : dict\\n        Dictionary containing up to 5 pandas DataFrame with keys as below:\\n\\n        ``label_quality`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``detailed_label_quality`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``annotator_stats`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``model_weight`` : np.ndarray\\n            Only returned if `return_weights=True`.\\n            An array of shape ``(P,)`` where is the number of models in the ensemble, specifying the weight of each classifier model in weighted averages used to estimate label quality.\\n            These weigthts is an estimate of how trustworthy the model is relative the annotators.\\n            An array of shape ``(P,)`` where is the number of models in the ensemble, specifying the model weight used in weighted averages.\\n\\n        ``annotator_weight`` : np.ndarray\\n            Only returned if `return_weights=True`.\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    See Also\\n    --------\\n    get_label_quality_multiannotator\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        index_col = labels_multiannotator.index\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n        index_col = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, ensemble=True, annotator_ids=annotator_ids)\n    num_annotations = np.sum(~np.isnan(labels_multiannotator), axis=1)\n    if calibrate_probs:\n        for i in range(len(pred_probs)):\n            curr_pred_probs = pred_probs[i]\n            optimal_temp = find_best_temp_scaler(labels_multiannotator, curr_pred_probs)\n            pred_probs[i] = temp_scale_pred_probs(curr_pred_probs, optimal_temp)\n    label_quality = pd.DataFrame({'num_annotations': num_annotations}, index=index_col)\n    avg_pred_probs = np.mean(pred_probs, axis=0)\n    majority_vote_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=avg_pred_probs, verbose=False)\n    (MV_annotator_agreement, MV_consensus_quality_score, MV_post_pred_probs, MV_model_weight, MV_annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=majority_vote_label, verbose=verbose, ensemble=True, **label_quality_score_kwargs)\n    consensus_label = np.full(len(majority_vote_label), np.nan)\n    for i in range(len(consensus_label)):\n        max_pred_probs_ind = np.where(MV_post_pred_probs[i] == np.max(MV_post_pred_probs[i]))[0]\n        if len(max_pred_probs_ind) == 1:\n            consensus_label[i] = max_pred_probs_ind[0]\n        else:\n            consensus_label[i] = majority_vote_label[i]\n    consensus_label = consensus_label.astype(int)\n    (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=consensus_label, verbose=verbose, ensemble=True, **label_quality_score_kwargs)\n    if verbose:\n        check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_method='crowdlab')\n    (label_quality['consensus_label'], label_quality['consensus_quality_score'], label_quality['annotator_agreement']) = (consensus_label, consensus_quality_score, annotator_agreement)\n    label_quality = label_quality.reindex(columns=['consensus_label', 'consensus_quality_score', 'annotator_agreement', 'num_annotations'])\n    detailed_label_quality = None\n    if return_detailed_quality:\n        detailed_label_quality = np.apply_along_axis(_get_annotator_label_quality_score, axis=0, arr=labels_multiannotator, pred_probs=post_pred_probs, label_quality_score_kwargs=label_quality_score_kwargs)\n        detailed_label_quality_df = pd.DataFrame(detailed_label_quality, index=index_col, columns=annotator_ids).add_prefix('quality_annotator_')\n    if return_annotator_stats:\n        annotator_stats = _get_annotator_stats(labels_multiannotator=labels_multiannotator, pred_probs=post_pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=np.mean(model_weight), annotator_weight=annotator_weight, consensus_quality_score=consensus_quality_score, detailed_label_quality=detailed_label_quality, annotator_ids=annotator_ids)\n    labels_info = {'label_quality': label_quality}\n    if return_detailed_quality:\n        labels_info['detailed_label_quality'] = detailed_label_quality_df\n    if return_annotator_stats:\n        labels_info['annotator_stats'] = annotator_stats\n    if return_weights:\n        labels_info['model_weight'] = model_weight\n        labels_info['annotator_weight'] = annotator_weight\n    return labels_info",
            "def get_label_quality_multiannotator_ensemble(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: np.ndarray, *, calibrate_probs: bool=False, return_detailed_quality: bool=True, return_annotator_stats: bool=True, return_weights: bool=False, verbose: bool=True, label_quality_score_kwargs: dict={}) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns label quality scores for each example and for each annotator, based on predictions from an ensemble of models.\\n\\n    This function is similar to :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>` but for settings where\\n    you have trained an ensemble of multiple classifier models rather than a single model.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        Multiannotator labels in the same format expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    calibrate_probs : bool, default = False\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_detailed_quality: bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_annotator_stats : bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    return_weights : bool, default = False\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    verbose : bool, default = True\\n        Boolean value as expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments in the same format expected by py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    Returns\\n    -------\\n    labels_info : dict\\n        Dictionary containing up to 5 pandas DataFrame with keys as below:\\n\\n        ``label_quality`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``detailed_label_quality`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``annotator_stats`` : pandas.DataFrame\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n        ``model_weight`` : np.ndarray\\n            Only returned if `return_weights=True`.\\n            An array of shape ``(P,)`` where is the number of models in the ensemble, specifying the weight of each classifier model in weighted averages used to estimate label quality.\\n            These weigthts is an estimate of how trustworthy the model is relative the annotators.\\n            An array of shape ``(P,)`` where is the number of models in the ensemble, specifying the model weight used in weighted averages.\\n\\n        ``annotator_weight`` : np.ndarray\\n            Only returned if `return_weights=True`.\\n            Similar to output as :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    See Also\\n    --------\\n    get_label_quality_multiannotator\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        index_col = labels_multiannotator.index\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n        index_col = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, ensemble=True, annotator_ids=annotator_ids)\n    num_annotations = np.sum(~np.isnan(labels_multiannotator), axis=1)\n    if calibrate_probs:\n        for i in range(len(pred_probs)):\n            curr_pred_probs = pred_probs[i]\n            optimal_temp = find_best_temp_scaler(labels_multiannotator, curr_pred_probs)\n            pred_probs[i] = temp_scale_pred_probs(curr_pred_probs, optimal_temp)\n    label_quality = pd.DataFrame({'num_annotations': num_annotations}, index=index_col)\n    avg_pred_probs = np.mean(pred_probs, axis=0)\n    majority_vote_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=avg_pred_probs, verbose=False)\n    (MV_annotator_agreement, MV_consensus_quality_score, MV_post_pred_probs, MV_model_weight, MV_annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=majority_vote_label, verbose=verbose, ensemble=True, **label_quality_score_kwargs)\n    consensus_label = np.full(len(majority_vote_label), np.nan)\n    for i in range(len(consensus_label)):\n        max_pred_probs_ind = np.where(MV_post_pred_probs[i] == np.max(MV_post_pred_probs[i]))[0]\n        if len(max_pred_probs_ind) == 1:\n            consensus_label[i] = max_pred_probs_ind[0]\n        else:\n            consensus_label[i] = majority_vote_label[i]\n    consensus_label = consensus_label.astype(int)\n    (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight) = _get_consensus_stats(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, num_annotations=num_annotations, consensus_label=consensus_label, verbose=verbose, ensemble=True, **label_quality_score_kwargs)\n    if verbose:\n        check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_method='crowdlab')\n    (label_quality['consensus_label'], label_quality['consensus_quality_score'], label_quality['annotator_agreement']) = (consensus_label, consensus_quality_score, annotator_agreement)\n    label_quality = label_quality.reindex(columns=['consensus_label', 'consensus_quality_score', 'annotator_agreement', 'num_annotations'])\n    detailed_label_quality = None\n    if return_detailed_quality:\n        detailed_label_quality = np.apply_along_axis(_get_annotator_label_quality_score, axis=0, arr=labels_multiannotator, pred_probs=post_pred_probs, label_quality_score_kwargs=label_quality_score_kwargs)\n        detailed_label_quality_df = pd.DataFrame(detailed_label_quality, index=index_col, columns=annotator_ids).add_prefix('quality_annotator_')\n    if return_annotator_stats:\n        annotator_stats = _get_annotator_stats(labels_multiannotator=labels_multiannotator, pred_probs=post_pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=np.mean(model_weight), annotator_weight=annotator_weight, consensus_quality_score=consensus_quality_score, detailed_label_quality=detailed_label_quality, annotator_ids=annotator_ids)\n    labels_info = {'label_quality': label_quality}\n    if return_detailed_quality:\n        labels_info['detailed_label_quality'] = detailed_label_quality_df\n    if return_annotator_stats:\n        labels_info['annotator_stats'] = annotator_stats\n    if return_weights:\n        labels_info['model_weight'] = model_weight\n        labels_info['annotator_weight'] = annotator_weight\n    return labels_info"
        ]
    },
    {
        "func_name": "get_active_learning_scores",
        "original": "def get_active_learning_scores(labels_multiannotator: Optional[Union[pd.DataFrame, np.ndarray]]=None, pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns an ActiveLab quality score for each example in the dataset, to estimate which examples are most informative to (re)label next in active learning.\n\n    We consider settings where one example can be labeled by one or more annotators and some examples have no labels at all so far.\n\n    The score is in between 0 and 1, and can be used to prioritize what data to collect additional labels for.\n    Lower scores indicate examples whose true label we are least confident about based on the current data;\n    collecting additional labels for these low-scoring examples will be more informative than collecting labels for other examples.\n    To use an annotation budget most efficiently, select a batch of examples with the lowest scores and collect one additional label for each example,\n    and repeat this process after retraining your classifier.\n\n    You can use this function to get active learning scores for: examples that already have one or more labels (specify ``labels_multiannotator`` and ``pred_probs``\n    as arguments), or for unlabeled examples (specify ``pred_probs_unlabeled``), or for both types of examples (specify all of the above arguments).\n\n    To analyze a fixed dataset labeled by multiple annotators rather than collecting additional labels, try the\n    :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>` (CROWDLAB) function instead.\n\n    Parameters\n    ----------\n    labels_multiannotator : pd.DataFrame or np.ndarray, optional\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\n        where N is the number of examples and M is the number of annotators. Note that this function also works with\n        datasets where there is only one annotator (M=1).\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n        Note that examples that have no annotator labels should not be included in this DataFrame/array.\n        This argument is optional if ``pred_probs`` is not provided (you might only provide ``pred_probs_unlabeled`` to only get active learning scores for the unlabeled examples).\n    pred_probs : np.ndarray, optional\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model.\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\n        This argument is optional if you only want to get active learning scores for unlabeled examples (specify only ``pred_probs_unlabeled`` instead).\n    pred_probs_unlabeled : np.ndarray, optional\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model for examples that have no annotator labels.\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\n        This argument is optional if you only want to get active learning scores for already-labeled examples (specify only ``pred_probs`` instead).\n\n    Returns\n    -------\n    active_learning_scores : np.ndarray\n        Array of shape ``(N,)`` indicating the ActiveLab quality scores for each example.\n        This array is empty if no already-labeled data was provided via ``labels_multiannotator``.\n        Examples with the lowest scores are those we should label next in order to maximally improve our classifier model.\n\n    active_learning_scores_unlabeled : np.ndarray\n        Array of shape ``(N,)`` indicating the active learning quality scores for each unlabeled example.\n        Returns an empty array if no unlabeled data is provided.\n        Examples with the lowest scores are those we should label next in order to maximally improve our classifier model\n        (scores for unlabeled data are directly comparable with the `active_learning_scores` for labeled data).\n    \"\"\"\n    assert_valid_pred_probs(pred_probs=pred_probs, pred_probs_unlabeled=pred_probs_unlabeled)\n    if pred_probs is not None:\n        if labels_multiannotator is None:\n            raise ValueError('labels_multiannotator cannot be None when passing in pred_probs. ', 'Either provide labels_multiannotator to obtain active learning scores for the labeled examples, or just pass in pred_probs_unlabeled to get active learning scores for unlabeled examples.')\n        if isinstance(labels_multiannotator, pd.DataFrame):\n            labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n        elif not isinstance(labels_multiannotator, np.ndarray):\n            raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n        if labels_multiannotator.ndim != 2:\n            raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n        num_classes = get_num_classes(pred_probs=pred_probs)\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            optimal_temp = 1.0\n            assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, allow_single_label=True)\n            consensus_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, verbose=False)\n            quality_of_consensus_labeled = get_label_quality_scores(consensus_label, pred_probs)\n            model_weight = 1\n            annotator_weight = np.full(labels_multiannotator.shape[1], 1)\n            avg_annotator_weight = np.mean(annotator_weight)\n        else:\n            optimal_temp = find_best_temp_scaler(labels_multiannotator, pred_probs)\n            pred_probs = temp_scale_pred_probs(pred_probs, optimal_temp)\n            multiannotator_info = get_label_quality_multiannotator(labels_multiannotator, pred_probs, return_annotator_stats=False, return_detailed_quality=False, return_weights=True)\n            quality_of_consensus_labeled = multiannotator_info['label_quality']['consensus_quality_score']\n            model_weight = multiannotator_info['model_weight']\n            annotator_weight = multiannotator_info['annotator_weight']\n            avg_annotator_weight = np.mean(annotator_weight)\n        active_learning_scores = np.full(len(labels_multiannotator), np.nan)\n        for (i, annotator_labels) in enumerate(labels_multiannotator):\n            active_learning_scores[i] = np.average((quality_of_consensus_labeled[i], 1 / num_classes), weights=(np.sum(annotator_weight[~np.isnan(annotator_labels)]) + model_weight, avg_annotator_weight))\n    elif pred_probs_unlabeled is not None:\n        num_classes = get_num_classes(pred_probs=pred_probs_unlabeled)\n        optimal_temp = 1\n        model_weight = 1\n        avg_annotator_weight = 1\n        active_learning_scores = np.array([])\n    else:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if pred_probs_unlabeled is not None:\n        pred_probs_unlabeled = temp_scale_pred_probs(pred_probs_unlabeled, optimal_temp)\n        quality_of_consensus_unlabeled = np.max(pred_probs_unlabeled, axis=1)\n        active_learning_scores_unlabeled = np.average(np.stack([quality_of_consensus_unlabeled, np.full(len(quality_of_consensus_unlabeled), 1 / num_classes)]), weights=[model_weight, avg_annotator_weight], axis=0)\n    else:\n        active_learning_scores_unlabeled = np.array([])\n    return (active_learning_scores, active_learning_scores_unlabeled)",
        "mutated": [
            "def get_active_learning_scores(labels_multiannotator: Optional[Union[pd.DataFrame, np.ndarray]]=None, pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n    'Returns an ActiveLab quality score for each example in the dataset, to estimate which examples are most informative to (re)label next in active learning.\\n\\n    We consider settings where one example can be labeled by one or more annotators and some examples have no labels at all so far.\\n\\n    The score is in between 0 and 1, and can be used to prioritize what data to collect additional labels for.\\n    Lower scores indicate examples whose true label we are least confident about based on the current data;\\n    collecting additional labels for these low-scoring examples will be more informative than collecting labels for other examples.\\n    To use an annotation budget most efficiently, select a batch of examples with the lowest scores and collect one additional label for each example,\\n    and repeat this process after retraining your classifier.\\n\\n    You can use this function to get active learning scores for: examples that already have one or more labels (specify ``labels_multiannotator`` and ``pred_probs``\\n    as arguments), or for unlabeled examples (specify ``pred_probs_unlabeled``), or for both types of examples (specify all of the above arguments).\\n\\n    To analyze a fixed dataset labeled by multiple annotators rather than collecting additional labels, try the\\n    :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>` (CROWDLAB) function instead.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray, optional\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators. Note that this function also works with\\n        datasets where there is only one annotator (M=1).\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n        Note that examples that have no annotator labels should not be included in this DataFrame/array.\\n        This argument is optional if ``pred_probs`` is not provided (you might only provide ``pred_probs_unlabeled`` to only get active learning scores for the unlabeled examples).\\n    pred_probs : np.ndarray, optional\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for unlabeled examples (specify only ``pred_probs_unlabeled`` instead).\\n    pred_probs_unlabeled : np.ndarray, optional\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model for examples that have no annotator labels.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for already-labeled examples (specify only ``pred_probs`` instead).\\n\\n    Returns\\n    -------\\n    active_learning_scores : np.ndarray\\n        Array of shape ``(N,)`` indicating the ActiveLab quality scores for each example.\\n        This array is empty if no already-labeled data was provided via ``labels_multiannotator``.\\n        Examples with the lowest scores are those we should label next in order to maximally improve our classifier model.\\n\\n    active_learning_scores_unlabeled : np.ndarray\\n        Array of shape ``(N,)`` indicating the active learning quality scores for each unlabeled example.\\n        Returns an empty array if no unlabeled data is provided.\\n        Examples with the lowest scores are those we should label next in order to maximally improve our classifier model\\n        (scores for unlabeled data are directly comparable with the `active_learning_scores` for labeled data).\\n    '\n    assert_valid_pred_probs(pred_probs=pred_probs, pred_probs_unlabeled=pred_probs_unlabeled)\n    if pred_probs is not None:\n        if labels_multiannotator is None:\n            raise ValueError('labels_multiannotator cannot be None when passing in pred_probs. ', 'Either provide labels_multiannotator to obtain active learning scores for the labeled examples, or just pass in pred_probs_unlabeled to get active learning scores for unlabeled examples.')\n        if isinstance(labels_multiannotator, pd.DataFrame):\n            labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n        elif not isinstance(labels_multiannotator, np.ndarray):\n            raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n        if labels_multiannotator.ndim != 2:\n            raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n        num_classes = get_num_classes(pred_probs=pred_probs)\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            optimal_temp = 1.0\n            assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, allow_single_label=True)\n            consensus_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, verbose=False)\n            quality_of_consensus_labeled = get_label_quality_scores(consensus_label, pred_probs)\n            model_weight = 1\n            annotator_weight = np.full(labels_multiannotator.shape[1], 1)\n            avg_annotator_weight = np.mean(annotator_weight)\n        else:\n            optimal_temp = find_best_temp_scaler(labels_multiannotator, pred_probs)\n            pred_probs = temp_scale_pred_probs(pred_probs, optimal_temp)\n            multiannotator_info = get_label_quality_multiannotator(labels_multiannotator, pred_probs, return_annotator_stats=False, return_detailed_quality=False, return_weights=True)\n            quality_of_consensus_labeled = multiannotator_info['label_quality']['consensus_quality_score']\n            model_weight = multiannotator_info['model_weight']\n            annotator_weight = multiannotator_info['annotator_weight']\n            avg_annotator_weight = np.mean(annotator_weight)\n        active_learning_scores = np.full(len(labels_multiannotator), np.nan)\n        for (i, annotator_labels) in enumerate(labels_multiannotator):\n            active_learning_scores[i] = np.average((quality_of_consensus_labeled[i], 1 / num_classes), weights=(np.sum(annotator_weight[~np.isnan(annotator_labels)]) + model_weight, avg_annotator_weight))\n    elif pred_probs_unlabeled is not None:\n        num_classes = get_num_classes(pred_probs=pred_probs_unlabeled)\n        optimal_temp = 1\n        model_weight = 1\n        avg_annotator_weight = 1\n        active_learning_scores = np.array([])\n    else:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if pred_probs_unlabeled is not None:\n        pred_probs_unlabeled = temp_scale_pred_probs(pred_probs_unlabeled, optimal_temp)\n        quality_of_consensus_unlabeled = np.max(pred_probs_unlabeled, axis=1)\n        active_learning_scores_unlabeled = np.average(np.stack([quality_of_consensus_unlabeled, np.full(len(quality_of_consensus_unlabeled), 1 / num_classes)]), weights=[model_weight, avg_annotator_weight], axis=0)\n    else:\n        active_learning_scores_unlabeled = np.array([])\n    return (active_learning_scores, active_learning_scores_unlabeled)",
            "def get_active_learning_scores(labels_multiannotator: Optional[Union[pd.DataFrame, np.ndarray]]=None, pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns an ActiveLab quality score for each example in the dataset, to estimate which examples are most informative to (re)label next in active learning.\\n\\n    We consider settings where one example can be labeled by one or more annotators and some examples have no labels at all so far.\\n\\n    The score is in between 0 and 1, and can be used to prioritize what data to collect additional labels for.\\n    Lower scores indicate examples whose true label we are least confident about based on the current data;\\n    collecting additional labels for these low-scoring examples will be more informative than collecting labels for other examples.\\n    To use an annotation budget most efficiently, select a batch of examples with the lowest scores and collect one additional label for each example,\\n    and repeat this process after retraining your classifier.\\n\\n    You can use this function to get active learning scores for: examples that already have one or more labels (specify ``labels_multiannotator`` and ``pred_probs``\\n    as arguments), or for unlabeled examples (specify ``pred_probs_unlabeled``), or for both types of examples (specify all of the above arguments).\\n\\n    To analyze a fixed dataset labeled by multiple annotators rather than collecting additional labels, try the\\n    :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>` (CROWDLAB) function instead.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray, optional\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators. Note that this function also works with\\n        datasets where there is only one annotator (M=1).\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n        Note that examples that have no annotator labels should not be included in this DataFrame/array.\\n        This argument is optional if ``pred_probs`` is not provided (you might only provide ``pred_probs_unlabeled`` to only get active learning scores for the unlabeled examples).\\n    pred_probs : np.ndarray, optional\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for unlabeled examples (specify only ``pred_probs_unlabeled`` instead).\\n    pred_probs_unlabeled : np.ndarray, optional\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model for examples that have no annotator labels.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for already-labeled examples (specify only ``pred_probs`` instead).\\n\\n    Returns\\n    -------\\n    active_learning_scores : np.ndarray\\n        Array of shape ``(N,)`` indicating the ActiveLab quality scores for each example.\\n        This array is empty if no already-labeled data was provided via ``labels_multiannotator``.\\n        Examples with the lowest scores are those we should label next in order to maximally improve our classifier model.\\n\\n    active_learning_scores_unlabeled : np.ndarray\\n        Array of shape ``(N,)`` indicating the active learning quality scores for each unlabeled example.\\n        Returns an empty array if no unlabeled data is provided.\\n        Examples with the lowest scores are those we should label next in order to maximally improve our classifier model\\n        (scores for unlabeled data are directly comparable with the `active_learning_scores` for labeled data).\\n    '\n    assert_valid_pred_probs(pred_probs=pred_probs, pred_probs_unlabeled=pred_probs_unlabeled)\n    if pred_probs is not None:\n        if labels_multiannotator is None:\n            raise ValueError('labels_multiannotator cannot be None when passing in pred_probs. ', 'Either provide labels_multiannotator to obtain active learning scores for the labeled examples, or just pass in pred_probs_unlabeled to get active learning scores for unlabeled examples.')\n        if isinstance(labels_multiannotator, pd.DataFrame):\n            labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n        elif not isinstance(labels_multiannotator, np.ndarray):\n            raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n        if labels_multiannotator.ndim != 2:\n            raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n        num_classes = get_num_classes(pred_probs=pred_probs)\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            optimal_temp = 1.0\n            assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, allow_single_label=True)\n            consensus_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, verbose=False)\n            quality_of_consensus_labeled = get_label_quality_scores(consensus_label, pred_probs)\n            model_weight = 1\n            annotator_weight = np.full(labels_multiannotator.shape[1], 1)\n            avg_annotator_weight = np.mean(annotator_weight)\n        else:\n            optimal_temp = find_best_temp_scaler(labels_multiannotator, pred_probs)\n            pred_probs = temp_scale_pred_probs(pred_probs, optimal_temp)\n            multiannotator_info = get_label_quality_multiannotator(labels_multiannotator, pred_probs, return_annotator_stats=False, return_detailed_quality=False, return_weights=True)\n            quality_of_consensus_labeled = multiannotator_info['label_quality']['consensus_quality_score']\n            model_weight = multiannotator_info['model_weight']\n            annotator_weight = multiannotator_info['annotator_weight']\n            avg_annotator_weight = np.mean(annotator_weight)\n        active_learning_scores = np.full(len(labels_multiannotator), np.nan)\n        for (i, annotator_labels) in enumerate(labels_multiannotator):\n            active_learning_scores[i] = np.average((quality_of_consensus_labeled[i], 1 / num_classes), weights=(np.sum(annotator_weight[~np.isnan(annotator_labels)]) + model_weight, avg_annotator_weight))\n    elif pred_probs_unlabeled is not None:\n        num_classes = get_num_classes(pred_probs=pred_probs_unlabeled)\n        optimal_temp = 1\n        model_weight = 1\n        avg_annotator_weight = 1\n        active_learning_scores = np.array([])\n    else:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if pred_probs_unlabeled is not None:\n        pred_probs_unlabeled = temp_scale_pred_probs(pred_probs_unlabeled, optimal_temp)\n        quality_of_consensus_unlabeled = np.max(pred_probs_unlabeled, axis=1)\n        active_learning_scores_unlabeled = np.average(np.stack([quality_of_consensus_unlabeled, np.full(len(quality_of_consensus_unlabeled), 1 / num_classes)]), weights=[model_weight, avg_annotator_weight], axis=0)\n    else:\n        active_learning_scores_unlabeled = np.array([])\n    return (active_learning_scores, active_learning_scores_unlabeled)",
            "def get_active_learning_scores(labels_multiannotator: Optional[Union[pd.DataFrame, np.ndarray]]=None, pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns an ActiveLab quality score for each example in the dataset, to estimate which examples are most informative to (re)label next in active learning.\\n\\n    We consider settings where one example can be labeled by one or more annotators and some examples have no labels at all so far.\\n\\n    The score is in between 0 and 1, and can be used to prioritize what data to collect additional labels for.\\n    Lower scores indicate examples whose true label we are least confident about based on the current data;\\n    collecting additional labels for these low-scoring examples will be more informative than collecting labels for other examples.\\n    To use an annotation budget most efficiently, select a batch of examples with the lowest scores and collect one additional label for each example,\\n    and repeat this process after retraining your classifier.\\n\\n    You can use this function to get active learning scores for: examples that already have one or more labels (specify ``labels_multiannotator`` and ``pred_probs``\\n    as arguments), or for unlabeled examples (specify ``pred_probs_unlabeled``), or for both types of examples (specify all of the above arguments).\\n\\n    To analyze a fixed dataset labeled by multiple annotators rather than collecting additional labels, try the\\n    :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>` (CROWDLAB) function instead.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray, optional\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators. Note that this function also works with\\n        datasets where there is only one annotator (M=1).\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n        Note that examples that have no annotator labels should not be included in this DataFrame/array.\\n        This argument is optional if ``pred_probs`` is not provided (you might only provide ``pred_probs_unlabeled`` to only get active learning scores for the unlabeled examples).\\n    pred_probs : np.ndarray, optional\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for unlabeled examples (specify only ``pred_probs_unlabeled`` instead).\\n    pred_probs_unlabeled : np.ndarray, optional\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model for examples that have no annotator labels.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for already-labeled examples (specify only ``pred_probs`` instead).\\n\\n    Returns\\n    -------\\n    active_learning_scores : np.ndarray\\n        Array of shape ``(N,)`` indicating the ActiveLab quality scores for each example.\\n        This array is empty if no already-labeled data was provided via ``labels_multiannotator``.\\n        Examples with the lowest scores are those we should label next in order to maximally improve our classifier model.\\n\\n    active_learning_scores_unlabeled : np.ndarray\\n        Array of shape ``(N,)`` indicating the active learning quality scores for each unlabeled example.\\n        Returns an empty array if no unlabeled data is provided.\\n        Examples with the lowest scores are those we should label next in order to maximally improve our classifier model\\n        (scores for unlabeled data are directly comparable with the `active_learning_scores` for labeled data).\\n    '\n    assert_valid_pred_probs(pred_probs=pred_probs, pred_probs_unlabeled=pred_probs_unlabeled)\n    if pred_probs is not None:\n        if labels_multiannotator is None:\n            raise ValueError('labels_multiannotator cannot be None when passing in pred_probs. ', 'Either provide labels_multiannotator to obtain active learning scores for the labeled examples, or just pass in pred_probs_unlabeled to get active learning scores for unlabeled examples.')\n        if isinstance(labels_multiannotator, pd.DataFrame):\n            labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n        elif not isinstance(labels_multiannotator, np.ndarray):\n            raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n        if labels_multiannotator.ndim != 2:\n            raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n        num_classes = get_num_classes(pred_probs=pred_probs)\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            optimal_temp = 1.0\n            assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, allow_single_label=True)\n            consensus_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, verbose=False)\n            quality_of_consensus_labeled = get_label_quality_scores(consensus_label, pred_probs)\n            model_weight = 1\n            annotator_weight = np.full(labels_multiannotator.shape[1], 1)\n            avg_annotator_weight = np.mean(annotator_weight)\n        else:\n            optimal_temp = find_best_temp_scaler(labels_multiannotator, pred_probs)\n            pred_probs = temp_scale_pred_probs(pred_probs, optimal_temp)\n            multiannotator_info = get_label_quality_multiannotator(labels_multiannotator, pred_probs, return_annotator_stats=False, return_detailed_quality=False, return_weights=True)\n            quality_of_consensus_labeled = multiannotator_info['label_quality']['consensus_quality_score']\n            model_weight = multiannotator_info['model_weight']\n            annotator_weight = multiannotator_info['annotator_weight']\n            avg_annotator_weight = np.mean(annotator_weight)\n        active_learning_scores = np.full(len(labels_multiannotator), np.nan)\n        for (i, annotator_labels) in enumerate(labels_multiannotator):\n            active_learning_scores[i] = np.average((quality_of_consensus_labeled[i], 1 / num_classes), weights=(np.sum(annotator_weight[~np.isnan(annotator_labels)]) + model_weight, avg_annotator_weight))\n    elif pred_probs_unlabeled is not None:\n        num_classes = get_num_classes(pred_probs=pred_probs_unlabeled)\n        optimal_temp = 1\n        model_weight = 1\n        avg_annotator_weight = 1\n        active_learning_scores = np.array([])\n    else:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if pred_probs_unlabeled is not None:\n        pred_probs_unlabeled = temp_scale_pred_probs(pred_probs_unlabeled, optimal_temp)\n        quality_of_consensus_unlabeled = np.max(pred_probs_unlabeled, axis=1)\n        active_learning_scores_unlabeled = np.average(np.stack([quality_of_consensus_unlabeled, np.full(len(quality_of_consensus_unlabeled), 1 / num_classes)]), weights=[model_weight, avg_annotator_weight], axis=0)\n    else:\n        active_learning_scores_unlabeled = np.array([])\n    return (active_learning_scores, active_learning_scores_unlabeled)",
            "def get_active_learning_scores(labels_multiannotator: Optional[Union[pd.DataFrame, np.ndarray]]=None, pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns an ActiveLab quality score for each example in the dataset, to estimate which examples are most informative to (re)label next in active learning.\\n\\n    We consider settings where one example can be labeled by one or more annotators and some examples have no labels at all so far.\\n\\n    The score is in between 0 and 1, and can be used to prioritize what data to collect additional labels for.\\n    Lower scores indicate examples whose true label we are least confident about based on the current data;\\n    collecting additional labels for these low-scoring examples will be more informative than collecting labels for other examples.\\n    To use an annotation budget most efficiently, select a batch of examples with the lowest scores and collect one additional label for each example,\\n    and repeat this process after retraining your classifier.\\n\\n    You can use this function to get active learning scores for: examples that already have one or more labels (specify ``labels_multiannotator`` and ``pred_probs``\\n    as arguments), or for unlabeled examples (specify ``pred_probs_unlabeled``), or for both types of examples (specify all of the above arguments).\\n\\n    To analyze a fixed dataset labeled by multiple annotators rather than collecting additional labels, try the\\n    :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>` (CROWDLAB) function instead.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray, optional\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators. Note that this function also works with\\n        datasets where there is only one annotator (M=1).\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n        Note that examples that have no annotator labels should not be included in this DataFrame/array.\\n        This argument is optional if ``pred_probs`` is not provided (you might only provide ``pred_probs_unlabeled`` to only get active learning scores for the unlabeled examples).\\n    pred_probs : np.ndarray, optional\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for unlabeled examples (specify only ``pred_probs_unlabeled`` instead).\\n    pred_probs_unlabeled : np.ndarray, optional\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model for examples that have no annotator labels.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for already-labeled examples (specify only ``pred_probs`` instead).\\n\\n    Returns\\n    -------\\n    active_learning_scores : np.ndarray\\n        Array of shape ``(N,)`` indicating the ActiveLab quality scores for each example.\\n        This array is empty if no already-labeled data was provided via ``labels_multiannotator``.\\n        Examples with the lowest scores are those we should label next in order to maximally improve our classifier model.\\n\\n    active_learning_scores_unlabeled : np.ndarray\\n        Array of shape ``(N,)`` indicating the active learning quality scores for each unlabeled example.\\n        Returns an empty array if no unlabeled data is provided.\\n        Examples with the lowest scores are those we should label next in order to maximally improve our classifier model\\n        (scores for unlabeled data are directly comparable with the `active_learning_scores` for labeled data).\\n    '\n    assert_valid_pred_probs(pred_probs=pred_probs, pred_probs_unlabeled=pred_probs_unlabeled)\n    if pred_probs is not None:\n        if labels_multiannotator is None:\n            raise ValueError('labels_multiannotator cannot be None when passing in pred_probs. ', 'Either provide labels_multiannotator to obtain active learning scores for the labeled examples, or just pass in pred_probs_unlabeled to get active learning scores for unlabeled examples.')\n        if isinstance(labels_multiannotator, pd.DataFrame):\n            labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n        elif not isinstance(labels_multiannotator, np.ndarray):\n            raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n        if labels_multiannotator.ndim != 2:\n            raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n        num_classes = get_num_classes(pred_probs=pred_probs)\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            optimal_temp = 1.0\n            assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, allow_single_label=True)\n            consensus_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, verbose=False)\n            quality_of_consensus_labeled = get_label_quality_scores(consensus_label, pred_probs)\n            model_weight = 1\n            annotator_weight = np.full(labels_multiannotator.shape[1], 1)\n            avg_annotator_weight = np.mean(annotator_weight)\n        else:\n            optimal_temp = find_best_temp_scaler(labels_multiannotator, pred_probs)\n            pred_probs = temp_scale_pred_probs(pred_probs, optimal_temp)\n            multiannotator_info = get_label_quality_multiannotator(labels_multiannotator, pred_probs, return_annotator_stats=False, return_detailed_quality=False, return_weights=True)\n            quality_of_consensus_labeled = multiannotator_info['label_quality']['consensus_quality_score']\n            model_weight = multiannotator_info['model_weight']\n            annotator_weight = multiannotator_info['annotator_weight']\n            avg_annotator_weight = np.mean(annotator_weight)\n        active_learning_scores = np.full(len(labels_multiannotator), np.nan)\n        for (i, annotator_labels) in enumerate(labels_multiannotator):\n            active_learning_scores[i] = np.average((quality_of_consensus_labeled[i], 1 / num_classes), weights=(np.sum(annotator_weight[~np.isnan(annotator_labels)]) + model_weight, avg_annotator_weight))\n    elif pred_probs_unlabeled is not None:\n        num_classes = get_num_classes(pred_probs=pred_probs_unlabeled)\n        optimal_temp = 1\n        model_weight = 1\n        avg_annotator_weight = 1\n        active_learning_scores = np.array([])\n    else:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if pred_probs_unlabeled is not None:\n        pred_probs_unlabeled = temp_scale_pred_probs(pred_probs_unlabeled, optimal_temp)\n        quality_of_consensus_unlabeled = np.max(pred_probs_unlabeled, axis=1)\n        active_learning_scores_unlabeled = np.average(np.stack([quality_of_consensus_unlabeled, np.full(len(quality_of_consensus_unlabeled), 1 / num_classes)]), weights=[model_weight, avg_annotator_weight], axis=0)\n    else:\n        active_learning_scores_unlabeled = np.array([])\n    return (active_learning_scores, active_learning_scores_unlabeled)",
            "def get_active_learning_scores(labels_multiannotator: Optional[Union[pd.DataFrame, np.ndarray]]=None, pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns an ActiveLab quality score for each example in the dataset, to estimate which examples are most informative to (re)label next in active learning.\\n\\n    We consider settings where one example can be labeled by one or more annotators and some examples have no labels at all so far.\\n\\n    The score is in between 0 and 1, and can be used to prioritize what data to collect additional labels for.\\n    Lower scores indicate examples whose true label we are least confident about based on the current data;\\n    collecting additional labels for these low-scoring examples will be more informative than collecting labels for other examples.\\n    To use an annotation budget most efficiently, select a batch of examples with the lowest scores and collect one additional label for each example,\\n    and repeat this process after retraining your classifier.\\n\\n    You can use this function to get active learning scores for: examples that already have one or more labels (specify ``labels_multiannotator`` and ``pred_probs``\\n    as arguments), or for unlabeled examples (specify ``pred_probs_unlabeled``), or for both types of examples (specify all of the above arguments).\\n\\n    To analyze a fixed dataset labeled by multiple annotators rather than collecting additional labels, try the\\n    :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>` (CROWDLAB) function instead.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray, optional\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators. Note that this function also works with\\n        datasets where there is only one annotator (M=1).\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n        Note that examples that have no annotator labels should not be included in this DataFrame/array.\\n        This argument is optional if ``pred_probs`` is not provided (you might only provide ``pred_probs_unlabeled`` to only get active learning scores for the unlabeled examples).\\n    pred_probs : np.ndarray, optional\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for unlabeled examples (specify only ``pred_probs_unlabeled`` instead).\\n    pred_probs_unlabeled : np.ndarray, optional\\n        An array of shape ``(N, K)`` of predicted class probabilities from a trained classifier model for examples that have no annotator labels.\\n        Predicted probabilities in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for already-labeled examples (specify only ``pred_probs`` instead).\\n\\n    Returns\\n    -------\\n    active_learning_scores : np.ndarray\\n        Array of shape ``(N,)`` indicating the ActiveLab quality scores for each example.\\n        This array is empty if no already-labeled data was provided via ``labels_multiannotator``.\\n        Examples with the lowest scores are those we should label next in order to maximally improve our classifier model.\\n\\n    active_learning_scores_unlabeled : np.ndarray\\n        Array of shape ``(N,)`` indicating the active learning quality scores for each unlabeled example.\\n        Returns an empty array if no unlabeled data is provided.\\n        Examples with the lowest scores are those we should label next in order to maximally improve our classifier model\\n        (scores for unlabeled data are directly comparable with the `active_learning_scores` for labeled data).\\n    '\n    assert_valid_pred_probs(pred_probs=pred_probs, pred_probs_unlabeled=pred_probs_unlabeled)\n    if pred_probs is not None:\n        if labels_multiannotator is None:\n            raise ValueError('labels_multiannotator cannot be None when passing in pred_probs. ', 'Either provide labels_multiannotator to obtain active learning scores for the labeled examples, or just pass in pred_probs_unlabeled to get active learning scores for unlabeled examples.')\n        if isinstance(labels_multiannotator, pd.DataFrame):\n            labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n        elif not isinstance(labels_multiannotator, np.ndarray):\n            raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n        if labels_multiannotator.ndim != 2:\n            raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n        num_classes = get_num_classes(pred_probs=pred_probs)\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            optimal_temp = 1.0\n            assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, allow_single_label=True)\n            consensus_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, verbose=False)\n            quality_of_consensus_labeled = get_label_quality_scores(consensus_label, pred_probs)\n            model_weight = 1\n            annotator_weight = np.full(labels_multiannotator.shape[1], 1)\n            avg_annotator_weight = np.mean(annotator_weight)\n        else:\n            optimal_temp = find_best_temp_scaler(labels_multiannotator, pred_probs)\n            pred_probs = temp_scale_pred_probs(pred_probs, optimal_temp)\n            multiannotator_info = get_label_quality_multiannotator(labels_multiannotator, pred_probs, return_annotator_stats=False, return_detailed_quality=False, return_weights=True)\n            quality_of_consensus_labeled = multiannotator_info['label_quality']['consensus_quality_score']\n            model_weight = multiannotator_info['model_weight']\n            annotator_weight = multiannotator_info['annotator_weight']\n            avg_annotator_weight = np.mean(annotator_weight)\n        active_learning_scores = np.full(len(labels_multiannotator), np.nan)\n        for (i, annotator_labels) in enumerate(labels_multiannotator):\n            active_learning_scores[i] = np.average((quality_of_consensus_labeled[i], 1 / num_classes), weights=(np.sum(annotator_weight[~np.isnan(annotator_labels)]) + model_weight, avg_annotator_weight))\n    elif pred_probs_unlabeled is not None:\n        num_classes = get_num_classes(pred_probs=pred_probs_unlabeled)\n        optimal_temp = 1\n        model_weight = 1\n        avg_annotator_weight = 1\n        active_learning_scores = np.array([])\n    else:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if pred_probs_unlabeled is not None:\n        pred_probs_unlabeled = temp_scale_pred_probs(pred_probs_unlabeled, optimal_temp)\n        quality_of_consensus_unlabeled = np.max(pred_probs_unlabeled, axis=1)\n        active_learning_scores_unlabeled = np.average(np.stack([quality_of_consensus_unlabeled, np.full(len(quality_of_consensus_unlabeled), 1 / num_classes)]), weights=[model_weight, avg_annotator_weight], axis=0)\n    else:\n        active_learning_scores_unlabeled = np.array([])\n    return (active_learning_scores, active_learning_scores_unlabeled)"
        ]
    },
    {
        "func_name": "get_active_learning_scores_ensemble",
        "original": "def get_active_learning_scores_ensemble(labels_multiannotator: Optional[Union[pd.DataFrame, np.ndarray]]=None, pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns an ActiveLab quality score for each example in the dataset, based on predictions from an ensemble of models.\n\n    This function is similar to :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>` but allows for an\n    ensemble of multiple classifier models to be trained and will aggregate predictions from the models to compute the ActiveLab quality score.\n\n    Parameters\n    ----------\n    labels_multiannotator : pd.DataFrame or np.ndarray\n        Multiannotator labels in the same format expected by :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>`.\n        This argument is optional if ``pred_probs`` is not provided (in cases where you only provide ``pred_probs_unlabeled`` to get active learning scores for unlabeled examples).\n    pred_probs : np.ndarray\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\n        Note that this function also works with datasets where there is only one annotator (M=1).\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\n        This argument is optional if you only want to get active learning scores for unlabeled examples (pass in ``pred_probs_unlabeled`` instead).\n    pred_probs_unlabeled : np.ndarray, optional\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from a trained classifier model\n        for examples that have no annotated labels so far (but which we may want to label in the future, and hence compute active learning quality scores for).\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\n        This argument is optional if you only want to get active learning scores for labeled examples (pass in ``pred_probs`` instead).\n\n    Returns\n    -------\n    active_learning_scores : np.ndarray\n        Similar to output as :py:func:`get_label_quality_scores <cleanlab.multiannotator.get_label_quality_scores>`.\n    active_learning_scores_unlabeled : np.ndarray\n        Similar to output as :py:func:`get_label_quality_scores <cleanlab.multiannotator.get_label_quality_scores>`.\n\n    See Also\n    --------\n    get_active_learning_scores\n    \"\"\"\n    assert_valid_pred_probs(pred_probs=pred_probs, pred_probs_unlabeled=pred_probs_unlabeled, ensemble=True)\n    if pred_probs is not None:\n        if labels_multiannotator is None:\n            raise ValueError('labels_multiannotator cannot be None when passing in pred_probs. ', 'You can either provide labels_multiannotator to obtain active learning scores for the labeled examples, or just pass in pred_probs_unlabeled to get active learning scores for unlabeled examples.')\n        if isinstance(labels_multiannotator, pd.DataFrame):\n            labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n        elif not isinstance(labels_multiannotator, np.ndarray):\n            raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n        if labels_multiannotator.ndim != 2:\n            raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n        num_classes = get_num_classes(pred_probs=pred_probs[0])\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            optimal_temp = np.full(len(pred_probs), 1.0)\n            assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, ensemble=True, allow_single_label=True)\n            avg_pred_probs = np.mean(pred_probs, axis=0)\n            consensus_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=avg_pred_probs, verbose=False)\n            quality_of_consensus_labeled = get_label_quality_scores(consensus_label, avg_pred_probs)\n            model_weight = np.full(len(pred_probs), 1)\n            annotator_weight = np.full(labels_multiannotator.shape[1], 1)\n            avg_annotator_weight = np.mean(annotator_weight)\n        else:\n            optimal_temp = np.full(len(pred_probs), np.NaN)\n            for (i, curr_pred_probs) in enumerate(pred_probs):\n                curr_optimal_temp = find_best_temp_scaler(labels_multiannotator, curr_pred_probs)\n                pred_probs[i] = temp_scale_pred_probs(curr_pred_probs, curr_optimal_temp)\n                optimal_temp[i] = curr_optimal_temp\n            multiannotator_info = get_label_quality_multiannotator_ensemble(labels_multiannotator, pred_probs, return_annotator_stats=False, return_detailed_quality=False, return_weights=True)\n            quality_of_consensus_labeled = multiannotator_info['label_quality']['consensus_quality_score']\n            model_weight = multiannotator_info['model_weight']\n            annotator_weight = multiannotator_info['annotator_weight']\n            avg_annotator_weight = np.mean(annotator_weight)\n        active_learning_scores = np.full(len(labels_multiannotator), np.nan)\n        for (i, annotator_labels) in enumerate(labels_multiannotator):\n            active_learning_scores[i] = np.average((quality_of_consensus_labeled[i], 1 / num_classes), weights=(np.sum(annotator_weight[~np.isnan(annotator_labels)]) + np.sum(model_weight), avg_annotator_weight))\n    elif pred_probs_unlabeled is not None:\n        num_classes = get_num_classes(pred_probs=pred_probs_unlabeled[0])\n        optimal_temp = np.full(len(pred_probs_unlabeled), 1.0)\n        model_weight = np.full(len(pred_probs_unlabeled), 1)\n        avg_annotator_weight = 1\n        active_learning_scores = np.array([])\n    else:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if pred_probs_unlabeled is not None:\n        for i in range(len(pred_probs_unlabeled)):\n            pred_probs_unlabeled[i] = temp_scale_pred_probs(pred_probs_unlabeled[i], optimal_temp[i])\n        avg_pred_probs_unlabeled = np.mean(pred_probs_unlabeled, axis=0)\n        consensus_label_unlabeled = get_majority_vote_label(np.argmax(pred_probs_unlabeled, axis=2).T, avg_pred_probs_unlabeled)\n        modified_pred_probs_unlabeled = np.average(np.concatenate((pred_probs_unlabeled, np.full(pred_probs_unlabeled.shape[1:], 1 / num_classes)[np.newaxis, :, :])), weights=np.concatenate((model_weight, np.array([avg_annotator_weight]))), axis=0)\n        active_learning_scores_unlabeled = get_label_quality_scores(consensus_label_unlabeled, modified_pred_probs_unlabeled)\n    else:\n        active_learning_scores_unlabeled = np.array([])\n    return (active_learning_scores, active_learning_scores_unlabeled)",
        "mutated": [
            "def get_active_learning_scores_ensemble(labels_multiannotator: Optional[Union[pd.DataFrame, np.ndarray]]=None, pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n    'Returns an ActiveLab quality score for each example in the dataset, based on predictions from an ensemble of models.\\n\\n    This function is similar to :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>` but allows for an\\n    ensemble of multiple classifier models to be trained and will aggregate predictions from the models to compute the ActiveLab quality score.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        Multiannotator labels in the same format expected by :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>`.\\n        This argument is optional if ``pred_probs`` is not provided (in cases where you only provide ``pred_probs_unlabeled`` to get active learning scores for unlabeled examples).\\n    pred_probs : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Note that this function also works with datasets where there is only one annotator (M=1).\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for unlabeled examples (pass in ``pred_probs_unlabeled`` instead).\\n    pred_probs_unlabeled : np.ndarray, optional\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from a trained classifier model\\n        for examples that have no annotated labels so far (but which we may want to label in the future, and hence compute active learning quality scores for).\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for labeled examples (pass in ``pred_probs`` instead).\\n\\n    Returns\\n    -------\\n    active_learning_scores : np.ndarray\\n        Similar to output as :py:func:`get_label_quality_scores <cleanlab.multiannotator.get_label_quality_scores>`.\\n    active_learning_scores_unlabeled : np.ndarray\\n        Similar to output as :py:func:`get_label_quality_scores <cleanlab.multiannotator.get_label_quality_scores>`.\\n\\n    See Also\\n    --------\\n    get_active_learning_scores\\n    '\n    assert_valid_pred_probs(pred_probs=pred_probs, pred_probs_unlabeled=pred_probs_unlabeled, ensemble=True)\n    if pred_probs is not None:\n        if labels_multiannotator is None:\n            raise ValueError('labels_multiannotator cannot be None when passing in pred_probs. ', 'You can either provide labels_multiannotator to obtain active learning scores for the labeled examples, or just pass in pred_probs_unlabeled to get active learning scores for unlabeled examples.')\n        if isinstance(labels_multiannotator, pd.DataFrame):\n            labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n        elif not isinstance(labels_multiannotator, np.ndarray):\n            raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n        if labels_multiannotator.ndim != 2:\n            raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n        num_classes = get_num_classes(pred_probs=pred_probs[0])\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            optimal_temp = np.full(len(pred_probs), 1.0)\n            assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, ensemble=True, allow_single_label=True)\n            avg_pred_probs = np.mean(pred_probs, axis=0)\n            consensus_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=avg_pred_probs, verbose=False)\n            quality_of_consensus_labeled = get_label_quality_scores(consensus_label, avg_pred_probs)\n            model_weight = np.full(len(pred_probs), 1)\n            annotator_weight = np.full(labels_multiannotator.shape[1], 1)\n            avg_annotator_weight = np.mean(annotator_weight)\n        else:\n            optimal_temp = np.full(len(pred_probs), np.NaN)\n            for (i, curr_pred_probs) in enumerate(pred_probs):\n                curr_optimal_temp = find_best_temp_scaler(labels_multiannotator, curr_pred_probs)\n                pred_probs[i] = temp_scale_pred_probs(curr_pred_probs, curr_optimal_temp)\n                optimal_temp[i] = curr_optimal_temp\n            multiannotator_info = get_label_quality_multiannotator_ensemble(labels_multiannotator, pred_probs, return_annotator_stats=False, return_detailed_quality=False, return_weights=True)\n            quality_of_consensus_labeled = multiannotator_info['label_quality']['consensus_quality_score']\n            model_weight = multiannotator_info['model_weight']\n            annotator_weight = multiannotator_info['annotator_weight']\n            avg_annotator_weight = np.mean(annotator_weight)\n        active_learning_scores = np.full(len(labels_multiannotator), np.nan)\n        for (i, annotator_labels) in enumerate(labels_multiannotator):\n            active_learning_scores[i] = np.average((quality_of_consensus_labeled[i], 1 / num_classes), weights=(np.sum(annotator_weight[~np.isnan(annotator_labels)]) + np.sum(model_weight), avg_annotator_weight))\n    elif pred_probs_unlabeled is not None:\n        num_classes = get_num_classes(pred_probs=pred_probs_unlabeled[0])\n        optimal_temp = np.full(len(pred_probs_unlabeled), 1.0)\n        model_weight = np.full(len(pred_probs_unlabeled), 1)\n        avg_annotator_weight = 1\n        active_learning_scores = np.array([])\n    else:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if pred_probs_unlabeled is not None:\n        for i in range(len(pred_probs_unlabeled)):\n            pred_probs_unlabeled[i] = temp_scale_pred_probs(pred_probs_unlabeled[i], optimal_temp[i])\n        avg_pred_probs_unlabeled = np.mean(pred_probs_unlabeled, axis=0)\n        consensus_label_unlabeled = get_majority_vote_label(np.argmax(pred_probs_unlabeled, axis=2).T, avg_pred_probs_unlabeled)\n        modified_pred_probs_unlabeled = np.average(np.concatenate((pred_probs_unlabeled, np.full(pred_probs_unlabeled.shape[1:], 1 / num_classes)[np.newaxis, :, :])), weights=np.concatenate((model_weight, np.array([avg_annotator_weight]))), axis=0)\n        active_learning_scores_unlabeled = get_label_quality_scores(consensus_label_unlabeled, modified_pred_probs_unlabeled)\n    else:\n        active_learning_scores_unlabeled = np.array([])\n    return (active_learning_scores, active_learning_scores_unlabeled)",
            "def get_active_learning_scores_ensemble(labels_multiannotator: Optional[Union[pd.DataFrame, np.ndarray]]=None, pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns an ActiveLab quality score for each example in the dataset, based on predictions from an ensemble of models.\\n\\n    This function is similar to :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>` but allows for an\\n    ensemble of multiple classifier models to be trained and will aggregate predictions from the models to compute the ActiveLab quality score.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        Multiannotator labels in the same format expected by :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>`.\\n        This argument is optional if ``pred_probs`` is not provided (in cases where you only provide ``pred_probs_unlabeled`` to get active learning scores for unlabeled examples).\\n    pred_probs : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Note that this function also works with datasets where there is only one annotator (M=1).\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for unlabeled examples (pass in ``pred_probs_unlabeled`` instead).\\n    pred_probs_unlabeled : np.ndarray, optional\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from a trained classifier model\\n        for examples that have no annotated labels so far (but which we may want to label in the future, and hence compute active learning quality scores for).\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for labeled examples (pass in ``pred_probs`` instead).\\n\\n    Returns\\n    -------\\n    active_learning_scores : np.ndarray\\n        Similar to output as :py:func:`get_label_quality_scores <cleanlab.multiannotator.get_label_quality_scores>`.\\n    active_learning_scores_unlabeled : np.ndarray\\n        Similar to output as :py:func:`get_label_quality_scores <cleanlab.multiannotator.get_label_quality_scores>`.\\n\\n    See Also\\n    --------\\n    get_active_learning_scores\\n    '\n    assert_valid_pred_probs(pred_probs=pred_probs, pred_probs_unlabeled=pred_probs_unlabeled, ensemble=True)\n    if pred_probs is not None:\n        if labels_multiannotator is None:\n            raise ValueError('labels_multiannotator cannot be None when passing in pred_probs. ', 'You can either provide labels_multiannotator to obtain active learning scores for the labeled examples, or just pass in pred_probs_unlabeled to get active learning scores for unlabeled examples.')\n        if isinstance(labels_multiannotator, pd.DataFrame):\n            labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n        elif not isinstance(labels_multiannotator, np.ndarray):\n            raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n        if labels_multiannotator.ndim != 2:\n            raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n        num_classes = get_num_classes(pred_probs=pred_probs[0])\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            optimal_temp = np.full(len(pred_probs), 1.0)\n            assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, ensemble=True, allow_single_label=True)\n            avg_pred_probs = np.mean(pred_probs, axis=0)\n            consensus_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=avg_pred_probs, verbose=False)\n            quality_of_consensus_labeled = get_label_quality_scores(consensus_label, avg_pred_probs)\n            model_weight = np.full(len(pred_probs), 1)\n            annotator_weight = np.full(labels_multiannotator.shape[1], 1)\n            avg_annotator_weight = np.mean(annotator_weight)\n        else:\n            optimal_temp = np.full(len(pred_probs), np.NaN)\n            for (i, curr_pred_probs) in enumerate(pred_probs):\n                curr_optimal_temp = find_best_temp_scaler(labels_multiannotator, curr_pred_probs)\n                pred_probs[i] = temp_scale_pred_probs(curr_pred_probs, curr_optimal_temp)\n                optimal_temp[i] = curr_optimal_temp\n            multiannotator_info = get_label_quality_multiannotator_ensemble(labels_multiannotator, pred_probs, return_annotator_stats=False, return_detailed_quality=False, return_weights=True)\n            quality_of_consensus_labeled = multiannotator_info['label_quality']['consensus_quality_score']\n            model_weight = multiannotator_info['model_weight']\n            annotator_weight = multiannotator_info['annotator_weight']\n            avg_annotator_weight = np.mean(annotator_weight)\n        active_learning_scores = np.full(len(labels_multiannotator), np.nan)\n        for (i, annotator_labels) in enumerate(labels_multiannotator):\n            active_learning_scores[i] = np.average((quality_of_consensus_labeled[i], 1 / num_classes), weights=(np.sum(annotator_weight[~np.isnan(annotator_labels)]) + np.sum(model_weight), avg_annotator_weight))\n    elif pred_probs_unlabeled is not None:\n        num_classes = get_num_classes(pred_probs=pred_probs_unlabeled[0])\n        optimal_temp = np.full(len(pred_probs_unlabeled), 1.0)\n        model_weight = np.full(len(pred_probs_unlabeled), 1)\n        avg_annotator_weight = 1\n        active_learning_scores = np.array([])\n    else:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if pred_probs_unlabeled is not None:\n        for i in range(len(pred_probs_unlabeled)):\n            pred_probs_unlabeled[i] = temp_scale_pred_probs(pred_probs_unlabeled[i], optimal_temp[i])\n        avg_pred_probs_unlabeled = np.mean(pred_probs_unlabeled, axis=0)\n        consensus_label_unlabeled = get_majority_vote_label(np.argmax(pred_probs_unlabeled, axis=2).T, avg_pred_probs_unlabeled)\n        modified_pred_probs_unlabeled = np.average(np.concatenate((pred_probs_unlabeled, np.full(pred_probs_unlabeled.shape[1:], 1 / num_classes)[np.newaxis, :, :])), weights=np.concatenate((model_weight, np.array([avg_annotator_weight]))), axis=0)\n        active_learning_scores_unlabeled = get_label_quality_scores(consensus_label_unlabeled, modified_pred_probs_unlabeled)\n    else:\n        active_learning_scores_unlabeled = np.array([])\n    return (active_learning_scores, active_learning_scores_unlabeled)",
            "def get_active_learning_scores_ensemble(labels_multiannotator: Optional[Union[pd.DataFrame, np.ndarray]]=None, pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns an ActiveLab quality score for each example in the dataset, based on predictions from an ensemble of models.\\n\\n    This function is similar to :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>` but allows for an\\n    ensemble of multiple classifier models to be trained and will aggregate predictions from the models to compute the ActiveLab quality score.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        Multiannotator labels in the same format expected by :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>`.\\n        This argument is optional if ``pred_probs`` is not provided (in cases where you only provide ``pred_probs_unlabeled`` to get active learning scores for unlabeled examples).\\n    pred_probs : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Note that this function also works with datasets where there is only one annotator (M=1).\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for unlabeled examples (pass in ``pred_probs_unlabeled`` instead).\\n    pred_probs_unlabeled : np.ndarray, optional\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from a trained classifier model\\n        for examples that have no annotated labels so far (but which we may want to label in the future, and hence compute active learning quality scores for).\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for labeled examples (pass in ``pred_probs`` instead).\\n\\n    Returns\\n    -------\\n    active_learning_scores : np.ndarray\\n        Similar to output as :py:func:`get_label_quality_scores <cleanlab.multiannotator.get_label_quality_scores>`.\\n    active_learning_scores_unlabeled : np.ndarray\\n        Similar to output as :py:func:`get_label_quality_scores <cleanlab.multiannotator.get_label_quality_scores>`.\\n\\n    See Also\\n    --------\\n    get_active_learning_scores\\n    '\n    assert_valid_pred_probs(pred_probs=pred_probs, pred_probs_unlabeled=pred_probs_unlabeled, ensemble=True)\n    if pred_probs is not None:\n        if labels_multiannotator is None:\n            raise ValueError('labels_multiannotator cannot be None when passing in pred_probs. ', 'You can either provide labels_multiannotator to obtain active learning scores for the labeled examples, or just pass in pred_probs_unlabeled to get active learning scores for unlabeled examples.')\n        if isinstance(labels_multiannotator, pd.DataFrame):\n            labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n        elif not isinstance(labels_multiannotator, np.ndarray):\n            raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n        if labels_multiannotator.ndim != 2:\n            raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n        num_classes = get_num_classes(pred_probs=pred_probs[0])\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            optimal_temp = np.full(len(pred_probs), 1.0)\n            assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, ensemble=True, allow_single_label=True)\n            avg_pred_probs = np.mean(pred_probs, axis=0)\n            consensus_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=avg_pred_probs, verbose=False)\n            quality_of_consensus_labeled = get_label_quality_scores(consensus_label, avg_pred_probs)\n            model_weight = np.full(len(pred_probs), 1)\n            annotator_weight = np.full(labels_multiannotator.shape[1], 1)\n            avg_annotator_weight = np.mean(annotator_weight)\n        else:\n            optimal_temp = np.full(len(pred_probs), np.NaN)\n            for (i, curr_pred_probs) in enumerate(pred_probs):\n                curr_optimal_temp = find_best_temp_scaler(labels_multiannotator, curr_pred_probs)\n                pred_probs[i] = temp_scale_pred_probs(curr_pred_probs, curr_optimal_temp)\n                optimal_temp[i] = curr_optimal_temp\n            multiannotator_info = get_label_quality_multiannotator_ensemble(labels_multiannotator, pred_probs, return_annotator_stats=False, return_detailed_quality=False, return_weights=True)\n            quality_of_consensus_labeled = multiannotator_info['label_quality']['consensus_quality_score']\n            model_weight = multiannotator_info['model_weight']\n            annotator_weight = multiannotator_info['annotator_weight']\n            avg_annotator_weight = np.mean(annotator_weight)\n        active_learning_scores = np.full(len(labels_multiannotator), np.nan)\n        for (i, annotator_labels) in enumerate(labels_multiannotator):\n            active_learning_scores[i] = np.average((quality_of_consensus_labeled[i], 1 / num_classes), weights=(np.sum(annotator_weight[~np.isnan(annotator_labels)]) + np.sum(model_weight), avg_annotator_weight))\n    elif pred_probs_unlabeled is not None:\n        num_classes = get_num_classes(pred_probs=pred_probs_unlabeled[0])\n        optimal_temp = np.full(len(pred_probs_unlabeled), 1.0)\n        model_weight = np.full(len(pred_probs_unlabeled), 1)\n        avg_annotator_weight = 1\n        active_learning_scores = np.array([])\n    else:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if pred_probs_unlabeled is not None:\n        for i in range(len(pred_probs_unlabeled)):\n            pred_probs_unlabeled[i] = temp_scale_pred_probs(pred_probs_unlabeled[i], optimal_temp[i])\n        avg_pred_probs_unlabeled = np.mean(pred_probs_unlabeled, axis=0)\n        consensus_label_unlabeled = get_majority_vote_label(np.argmax(pred_probs_unlabeled, axis=2).T, avg_pred_probs_unlabeled)\n        modified_pred_probs_unlabeled = np.average(np.concatenate((pred_probs_unlabeled, np.full(pred_probs_unlabeled.shape[1:], 1 / num_classes)[np.newaxis, :, :])), weights=np.concatenate((model_weight, np.array([avg_annotator_weight]))), axis=0)\n        active_learning_scores_unlabeled = get_label_quality_scores(consensus_label_unlabeled, modified_pred_probs_unlabeled)\n    else:\n        active_learning_scores_unlabeled = np.array([])\n    return (active_learning_scores, active_learning_scores_unlabeled)",
            "def get_active_learning_scores_ensemble(labels_multiannotator: Optional[Union[pd.DataFrame, np.ndarray]]=None, pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns an ActiveLab quality score for each example in the dataset, based on predictions from an ensemble of models.\\n\\n    This function is similar to :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>` but allows for an\\n    ensemble of multiple classifier models to be trained and will aggregate predictions from the models to compute the ActiveLab quality score.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        Multiannotator labels in the same format expected by :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>`.\\n        This argument is optional if ``pred_probs`` is not provided (in cases where you only provide ``pred_probs_unlabeled`` to get active learning scores for unlabeled examples).\\n    pred_probs : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Note that this function also works with datasets where there is only one annotator (M=1).\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for unlabeled examples (pass in ``pred_probs_unlabeled`` instead).\\n    pred_probs_unlabeled : np.ndarray, optional\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from a trained classifier model\\n        for examples that have no annotated labels so far (but which we may want to label in the future, and hence compute active learning quality scores for).\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for labeled examples (pass in ``pred_probs`` instead).\\n\\n    Returns\\n    -------\\n    active_learning_scores : np.ndarray\\n        Similar to output as :py:func:`get_label_quality_scores <cleanlab.multiannotator.get_label_quality_scores>`.\\n    active_learning_scores_unlabeled : np.ndarray\\n        Similar to output as :py:func:`get_label_quality_scores <cleanlab.multiannotator.get_label_quality_scores>`.\\n\\n    See Also\\n    --------\\n    get_active_learning_scores\\n    '\n    assert_valid_pred_probs(pred_probs=pred_probs, pred_probs_unlabeled=pred_probs_unlabeled, ensemble=True)\n    if pred_probs is not None:\n        if labels_multiannotator is None:\n            raise ValueError('labels_multiannotator cannot be None when passing in pred_probs. ', 'You can either provide labels_multiannotator to obtain active learning scores for the labeled examples, or just pass in pred_probs_unlabeled to get active learning scores for unlabeled examples.')\n        if isinstance(labels_multiannotator, pd.DataFrame):\n            labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n        elif not isinstance(labels_multiannotator, np.ndarray):\n            raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n        if labels_multiannotator.ndim != 2:\n            raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n        num_classes = get_num_classes(pred_probs=pred_probs[0])\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            optimal_temp = np.full(len(pred_probs), 1.0)\n            assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, ensemble=True, allow_single_label=True)\n            avg_pred_probs = np.mean(pred_probs, axis=0)\n            consensus_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=avg_pred_probs, verbose=False)\n            quality_of_consensus_labeled = get_label_quality_scores(consensus_label, avg_pred_probs)\n            model_weight = np.full(len(pred_probs), 1)\n            annotator_weight = np.full(labels_multiannotator.shape[1], 1)\n            avg_annotator_weight = np.mean(annotator_weight)\n        else:\n            optimal_temp = np.full(len(pred_probs), np.NaN)\n            for (i, curr_pred_probs) in enumerate(pred_probs):\n                curr_optimal_temp = find_best_temp_scaler(labels_multiannotator, curr_pred_probs)\n                pred_probs[i] = temp_scale_pred_probs(curr_pred_probs, curr_optimal_temp)\n                optimal_temp[i] = curr_optimal_temp\n            multiannotator_info = get_label_quality_multiannotator_ensemble(labels_multiannotator, pred_probs, return_annotator_stats=False, return_detailed_quality=False, return_weights=True)\n            quality_of_consensus_labeled = multiannotator_info['label_quality']['consensus_quality_score']\n            model_weight = multiannotator_info['model_weight']\n            annotator_weight = multiannotator_info['annotator_weight']\n            avg_annotator_weight = np.mean(annotator_weight)\n        active_learning_scores = np.full(len(labels_multiannotator), np.nan)\n        for (i, annotator_labels) in enumerate(labels_multiannotator):\n            active_learning_scores[i] = np.average((quality_of_consensus_labeled[i], 1 / num_classes), weights=(np.sum(annotator_weight[~np.isnan(annotator_labels)]) + np.sum(model_weight), avg_annotator_weight))\n    elif pred_probs_unlabeled is not None:\n        num_classes = get_num_classes(pred_probs=pred_probs_unlabeled[0])\n        optimal_temp = np.full(len(pred_probs_unlabeled), 1.0)\n        model_weight = np.full(len(pred_probs_unlabeled), 1)\n        avg_annotator_weight = 1\n        active_learning_scores = np.array([])\n    else:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if pred_probs_unlabeled is not None:\n        for i in range(len(pred_probs_unlabeled)):\n            pred_probs_unlabeled[i] = temp_scale_pred_probs(pred_probs_unlabeled[i], optimal_temp[i])\n        avg_pred_probs_unlabeled = np.mean(pred_probs_unlabeled, axis=0)\n        consensus_label_unlabeled = get_majority_vote_label(np.argmax(pred_probs_unlabeled, axis=2).T, avg_pred_probs_unlabeled)\n        modified_pred_probs_unlabeled = np.average(np.concatenate((pred_probs_unlabeled, np.full(pred_probs_unlabeled.shape[1:], 1 / num_classes)[np.newaxis, :, :])), weights=np.concatenate((model_weight, np.array([avg_annotator_weight]))), axis=0)\n        active_learning_scores_unlabeled = get_label_quality_scores(consensus_label_unlabeled, modified_pred_probs_unlabeled)\n    else:\n        active_learning_scores_unlabeled = np.array([])\n    return (active_learning_scores, active_learning_scores_unlabeled)",
            "def get_active_learning_scores_ensemble(labels_multiannotator: Optional[Union[pd.DataFrame, np.ndarray]]=None, pred_probs: Optional[np.ndarray]=None, pred_probs_unlabeled: Optional[np.ndarray]=None) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns an ActiveLab quality score for each example in the dataset, based on predictions from an ensemble of models.\\n\\n    This function is similar to :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>` but allows for an\\n    ensemble of multiple classifier models to be trained and will aggregate predictions from the models to compute the ActiveLab quality score.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        Multiannotator labels in the same format expected by :py:func:`get_active_learning_scores <cleanlab.multiannotator.get_active_learning_scores>`.\\n        This argument is optional if ``pred_probs`` is not provided (in cases where you only provide ``pred_probs_unlabeled`` to get active learning scores for unlabeled examples).\\n    pred_probs : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Note that this function also works with datasets where there is only one annotator (M=1).\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for unlabeled examples (pass in ``pred_probs_unlabeled`` instead).\\n    pred_probs_unlabeled : np.ndarray, optional\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from a trained classifier model\\n        for examples that have no annotated labels so far (but which we may want to label in the future, and hence compute active learning quality scores for).\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n        This argument is optional if you only want to get active learning scores for labeled examples (pass in ``pred_probs`` instead).\\n\\n    Returns\\n    -------\\n    active_learning_scores : np.ndarray\\n        Similar to output as :py:func:`get_label_quality_scores <cleanlab.multiannotator.get_label_quality_scores>`.\\n    active_learning_scores_unlabeled : np.ndarray\\n        Similar to output as :py:func:`get_label_quality_scores <cleanlab.multiannotator.get_label_quality_scores>`.\\n\\n    See Also\\n    --------\\n    get_active_learning_scores\\n    '\n    assert_valid_pred_probs(pred_probs=pred_probs, pred_probs_unlabeled=pred_probs_unlabeled, ensemble=True)\n    if pred_probs is not None:\n        if labels_multiannotator is None:\n            raise ValueError('labels_multiannotator cannot be None when passing in pred_probs. ', 'You can either provide labels_multiannotator to obtain active learning scores for the labeled examples, or just pass in pred_probs_unlabeled to get active learning scores for unlabeled examples.')\n        if isinstance(labels_multiannotator, pd.DataFrame):\n            labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n        elif not isinstance(labels_multiannotator, np.ndarray):\n            raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n        if labels_multiannotator.ndim != 2:\n            raise ValueError('labels_multiannotator must be a 2D array or dataframe, each row represents an example and each column represents an annotator.')\n        num_classes = get_num_classes(pred_probs=pred_probs[0])\n        if (np.sum(~np.isnan(labels_multiannotator), axis=1) == 1).all():\n            optimal_temp = np.full(len(pred_probs), 1.0)\n            assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, ensemble=True, allow_single_label=True)\n            avg_pred_probs = np.mean(pred_probs, axis=0)\n            consensus_label = get_majority_vote_label(labels_multiannotator=labels_multiannotator, pred_probs=avg_pred_probs, verbose=False)\n            quality_of_consensus_labeled = get_label_quality_scores(consensus_label, avg_pred_probs)\n            model_weight = np.full(len(pred_probs), 1)\n            annotator_weight = np.full(labels_multiannotator.shape[1], 1)\n            avg_annotator_weight = np.mean(annotator_weight)\n        else:\n            optimal_temp = np.full(len(pred_probs), np.NaN)\n            for (i, curr_pred_probs) in enumerate(pred_probs):\n                curr_optimal_temp = find_best_temp_scaler(labels_multiannotator, curr_pred_probs)\n                pred_probs[i] = temp_scale_pred_probs(curr_pred_probs, curr_optimal_temp)\n                optimal_temp[i] = curr_optimal_temp\n            multiannotator_info = get_label_quality_multiannotator_ensemble(labels_multiannotator, pred_probs, return_annotator_stats=False, return_detailed_quality=False, return_weights=True)\n            quality_of_consensus_labeled = multiannotator_info['label_quality']['consensus_quality_score']\n            model_weight = multiannotator_info['model_weight']\n            annotator_weight = multiannotator_info['annotator_weight']\n            avg_annotator_weight = np.mean(annotator_weight)\n        active_learning_scores = np.full(len(labels_multiannotator), np.nan)\n        for (i, annotator_labels) in enumerate(labels_multiannotator):\n            active_learning_scores[i] = np.average((quality_of_consensus_labeled[i], 1 / num_classes), weights=(np.sum(annotator_weight[~np.isnan(annotator_labels)]) + np.sum(model_weight), avg_annotator_weight))\n    elif pred_probs_unlabeled is not None:\n        num_classes = get_num_classes(pred_probs=pred_probs_unlabeled[0])\n        optimal_temp = np.full(len(pred_probs_unlabeled), 1.0)\n        model_weight = np.full(len(pred_probs_unlabeled), 1)\n        avg_annotator_weight = 1\n        active_learning_scores = np.array([])\n    else:\n        raise ValueError('pred_probs and pred_probs_unlabeled cannot both be None, specify at least one of the two.')\n    if pred_probs_unlabeled is not None:\n        for i in range(len(pred_probs_unlabeled)):\n            pred_probs_unlabeled[i] = temp_scale_pred_probs(pred_probs_unlabeled[i], optimal_temp[i])\n        avg_pred_probs_unlabeled = np.mean(pred_probs_unlabeled, axis=0)\n        consensus_label_unlabeled = get_majority_vote_label(np.argmax(pred_probs_unlabeled, axis=2).T, avg_pred_probs_unlabeled)\n        modified_pred_probs_unlabeled = np.average(np.concatenate((pred_probs_unlabeled, np.full(pred_probs_unlabeled.shape[1:], 1 / num_classes)[np.newaxis, :, :])), weights=np.concatenate((model_weight, np.array([avg_annotator_weight]))), axis=0)\n        active_learning_scores_unlabeled = get_label_quality_scores(consensus_label_unlabeled, modified_pred_probs_unlabeled)\n    else:\n        active_learning_scores_unlabeled = np.array([])\n    return (active_learning_scores, active_learning_scores_unlabeled)"
        ]
    },
    {
        "func_name": "get_labels_mode",
        "original": "def get_labels_mode(label_count, num_classes):\n    max_count_idx = np.where(label_count == np.nanmax(label_count))[0].astype(float)\n    return np.pad(max_count_idx, (0, num_classes - len(max_count_idx)), 'constant', constant_values=np.NaN)",
        "mutated": [
            "def get_labels_mode(label_count, num_classes):\n    if False:\n        i = 10\n    max_count_idx = np.where(label_count == np.nanmax(label_count))[0].astype(float)\n    return np.pad(max_count_idx, (0, num_classes - len(max_count_idx)), 'constant', constant_values=np.NaN)",
            "def get_labels_mode(label_count, num_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    max_count_idx = np.where(label_count == np.nanmax(label_count))[0].astype(float)\n    return np.pad(max_count_idx, (0, num_classes - len(max_count_idx)), 'constant', constant_values=np.NaN)",
            "def get_labels_mode(label_count, num_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    max_count_idx = np.where(label_count == np.nanmax(label_count))[0].astype(float)\n    return np.pad(max_count_idx, (0, num_classes - len(max_count_idx)), 'constant', constant_values=np.NaN)",
            "def get_labels_mode(label_count, num_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    max_count_idx = np.where(label_count == np.nanmax(label_count))[0].astype(float)\n    return np.pad(max_count_idx, (0, num_classes - len(max_count_idx)), 'constant', constant_values=np.NaN)",
            "def get_labels_mode(label_count, num_classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    max_count_idx = np.where(label_count == np.nanmax(label_count))[0].astype(float)\n    return np.pad(max_count_idx, (0, num_classes - len(max_count_idx)), 'constant', constant_values=np.NaN)"
        ]
    },
    {
        "func_name": "get_majority_vote_label",
        "original": "def get_majority_vote_label(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: Optional[np.ndarray]=None, verbose: bool=True) -> np.ndarray:\n    \"\"\"Returns the majority vote label for each example, aggregated from the labels given by multiple annotators.\n\n    Parameters\n    ----------\n    labels_multiannotator : pd.DataFrame or np.ndarray\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\n        where N is the number of examples and M is the number of annotators.\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    pred_probs : np.ndarray, optional\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\n        For details, predicted probabilities in the same format expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    verbose : bool, optional\n        Important warnings and other printed statements may be suppressed if ``verbose`` is set to ``False``.\n    Returns\n    -------\n    consensus_label: np.ndarray\n        An array of shape ``(N,)`` with the majority vote label aggregated from all annotators.\n\n        In the event of majority vote ties, ties are broken in the following order:\n        using the model ``pred_probs`` (if provided) and selecting the class with highest predicted probability,\n        using the empirical class frequencies and selecting the class with highest frequency,\n        using an initial annotator quality score and selecting the class that has been labeled by annotators with higher quality,\n        and lastly by random selection.\n    \"\"\"\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    if verbose:\n        assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, annotator_ids=annotator_ids)\n    if pred_probs is not None:\n        num_classes = pred_probs.shape[1]\n    else:\n        num_classes = int(np.nanmax(labels_multiannotator) + 1)\n\n    def get_labels_mode(label_count, num_classes):\n        max_count_idx = np.where(label_count == np.nanmax(label_count))[0].astype(float)\n        return np.pad(max_count_idx, (0, num_classes - len(max_count_idx)), 'constant', constant_values=np.NaN)\n    majority_vote_label = np.full(len(labels_multiannotator), np.nan)\n    label_count = np.apply_along_axis(lambda s: np.bincount(s[~np.isnan(s)].astype(int), minlength=num_classes), axis=1, arr=labels_multiannotator)\n    mode_labels_multiannotator = np.apply_along_axis(get_labels_mode, axis=1, arr=label_count, num_classes=num_classes)\n    nontied_idx = []\n    tied_idx = dict()\n    for (idx, label_mode) in enumerate(mode_labels_multiannotator):\n        label_mode = label_mode[~np.isnan(label_mode)].astype(int)\n        if len(label_mode) == 1:\n            majority_vote_label[idx] = label_mode[0]\n            nontied_idx.append(idx)\n        else:\n            tied_idx[idx] = label_mode\n    if pred_probs is not None and len(tied_idx) > 0:\n        for (idx, label_mode) in tied_idx.copy().items():\n            max_pred_probs = np.where(pred_probs[idx, label_mode] == np.max(pred_probs[idx, label_mode]))[0]\n            if len(max_pred_probs) == 1:\n                majority_vote_label[idx] = label_mode[max_pred_probs[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[max_pred_probs]\n    if len(tied_idx) > 0:\n        class_frequencies = label_count.sum(axis=0)\n        for (idx, label_mode) in tied_idx.copy().items():\n            min_frequency = np.where(class_frequencies[label_mode] == np.min(class_frequencies[label_mode]))[0]\n            if len(min_frequency) == 1:\n                majority_vote_label[idx] = label_mode[min_frequency[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[min_frequency]\n    if len(tied_idx) > 0:\n        nontied_majority_vote_label = majority_vote_label[nontied_idx]\n        nontied_labels_multiannotator = labels_multiannotator[nontied_idx]\n        annotator_agreement_with_consensus = np.zeros(nontied_labels_multiannotator.shape[1])\n        for i in range(len(annotator_agreement_with_consensus)):\n            labels = nontied_labels_multiannotator[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_agreement_with_consensus[i] = np.NaN\n            else:\n                annotator_agreement_with_consensus[i] = np.mean(labels[labels_mask] == nontied_majority_vote_label[labels_mask])\n        nan_mask = np.isnan(annotator_agreement_with_consensus)\n        avg_annotator_agreement = np.mean(annotator_agreement_with_consensus[~nan_mask])\n        annotator_agreement_with_consensus[nan_mask] = avg_annotator_agreement\n        for (idx, label_mode) in tied_idx.copy().items():\n            label_quality_score = np.array([np.mean(annotator_agreement_with_consensus[np.where(labels_multiannotator[idx] == label)[0]]) for label in label_mode])\n            max_score = np.where(label_quality_score == label_quality_score.max())[0]\n            if len(max_score) == 1:\n                majority_vote_label[idx] = label_mode[max_score[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[max_score]\n    if len(tied_idx) > 0:\n        warnings.warn(f'breaking ties of examples {list(tied_idx.keys())} by random selection, you may want to set seed for reproducability')\n        for (idx, label_mode) in tied_idx.items():\n            majority_vote_label[idx] = np.random.choice(label_mode)\n    if verbose:\n        check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=majority_vote_label, consensus_method='majority_vote')\n    return majority_vote_label.astype(int)",
        "mutated": [
            "def get_majority_vote_label(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: Optional[np.ndarray]=None, verbose: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n    'Returns the majority vote label for each example, aggregated from the labels given by multiple annotators.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray, optional\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    verbose : bool, optional\\n        Important warnings and other printed statements may be suppressed if ``verbose`` is set to ``False``.\\n    Returns\\n    -------\\n    consensus_label: np.ndarray\\n        An array of shape ``(N,)`` with the majority vote label aggregated from all annotators.\\n\\n        In the event of majority vote ties, ties are broken in the following order:\\n        using the model ``pred_probs`` (if provided) and selecting the class with highest predicted probability,\\n        using the empirical class frequencies and selecting the class with highest frequency,\\n        using an initial annotator quality score and selecting the class that has been labeled by annotators with higher quality,\\n        and lastly by random selection.\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    if verbose:\n        assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, annotator_ids=annotator_ids)\n    if pred_probs is not None:\n        num_classes = pred_probs.shape[1]\n    else:\n        num_classes = int(np.nanmax(labels_multiannotator) + 1)\n\n    def get_labels_mode(label_count, num_classes):\n        max_count_idx = np.where(label_count == np.nanmax(label_count))[0].astype(float)\n        return np.pad(max_count_idx, (0, num_classes - len(max_count_idx)), 'constant', constant_values=np.NaN)\n    majority_vote_label = np.full(len(labels_multiannotator), np.nan)\n    label_count = np.apply_along_axis(lambda s: np.bincount(s[~np.isnan(s)].astype(int), minlength=num_classes), axis=1, arr=labels_multiannotator)\n    mode_labels_multiannotator = np.apply_along_axis(get_labels_mode, axis=1, arr=label_count, num_classes=num_classes)\n    nontied_idx = []\n    tied_idx = dict()\n    for (idx, label_mode) in enumerate(mode_labels_multiannotator):\n        label_mode = label_mode[~np.isnan(label_mode)].astype(int)\n        if len(label_mode) == 1:\n            majority_vote_label[idx] = label_mode[0]\n            nontied_idx.append(idx)\n        else:\n            tied_idx[idx] = label_mode\n    if pred_probs is not None and len(tied_idx) > 0:\n        for (idx, label_mode) in tied_idx.copy().items():\n            max_pred_probs = np.where(pred_probs[idx, label_mode] == np.max(pred_probs[idx, label_mode]))[0]\n            if len(max_pred_probs) == 1:\n                majority_vote_label[idx] = label_mode[max_pred_probs[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[max_pred_probs]\n    if len(tied_idx) > 0:\n        class_frequencies = label_count.sum(axis=0)\n        for (idx, label_mode) in tied_idx.copy().items():\n            min_frequency = np.where(class_frequencies[label_mode] == np.min(class_frequencies[label_mode]))[0]\n            if len(min_frequency) == 1:\n                majority_vote_label[idx] = label_mode[min_frequency[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[min_frequency]\n    if len(tied_idx) > 0:\n        nontied_majority_vote_label = majority_vote_label[nontied_idx]\n        nontied_labels_multiannotator = labels_multiannotator[nontied_idx]\n        annotator_agreement_with_consensus = np.zeros(nontied_labels_multiannotator.shape[1])\n        for i in range(len(annotator_agreement_with_consensus)):\n            labels = nontied_labels_multiannotator[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_agreement_with_consensus[i] = np.NaN\n            else:\n                annotator_agreement_with_consensus[i] = np.mean(labels[labels_mask] == nontied_majority_vote_label[labels_mask])\n        nan_mask = np.isnan(annotator_agreement_with_consensus)\n        avg_annotator_agreement = np.mean(annotator_agreement_with_consensus[~nan_mask])\n        annotator_agreement_with_consensus[nan_mask] = avg_annotator_agreement\n        for (idx, label_mode) in tied_idx.copy().items():\n            label_quality_score = np.array([np.mean(annotator_agreement_with_consensus[np.where(labels_multiannotator[idx] == label)[0]]) for label in label_mode])\n            max_score = np.where(label_quality_score == label_quality_score.max())[0]\n            if len(max_score) == 1:\n                majority_vote_label[idx] = label_mode[max_score[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[max_score]\n    if len(tied_idx) > 0:\n        warnings.warn(f'breaking ties of examples {list(tied_idx.keys())} by random selection, you may want to set seed for reproducability')\n        for (idx, label_mode) in tied_idx.items():\n            majority_vote_label[idx] = np.random.choice(label_mode)\n    if verbose:\n        check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=majority_vote_label, consensus_method='majority_vote')\n    return majority_vote_label.astype(int)",
            "def get_majority_vote_label(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: Optional[np.ndarray]=None, verbose: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the majority vote label for each example, aggregated from the labels given by multiple annotators.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray, optional\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    verbose : bool, optional\\n        Important warnings and other printed statements may be suppressed if ``verbose`` is set to ``False``.\\n    Returns\\n    -------\\n    consensus_label: np.ndarray\\n        An array of shape ``(N,)`` with the majority vote label aggregated from all annotators.\\n\\n        In the event of majority vote ties, ties are broken in the following order:\\n        using the model ``pred_probs`` (if provided) and selecting the class with highest predicted probability,\\n        using the empirical class frequencies and selecting the class with highest frequency,\\n        using an initial annotator quality score and selecting the class that has been labeled by annotators with higher quality,\\n        and lastly by random selection.\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    if verbose:\n        assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, annotator_ids=annotator_ids)\n    if pred_probs is not None:\n        num_classes = pred_probs.shape[1]\n    else:\n        num_classes = int(np.nanmax(labels_multiannotator) + 1)\n\n    def get_labels_mode(label_count, num_classes):\n        max_count_idx = np.where(label_count == np.nanmax(label_count))[0].astype(float)\n        return np.pad(max_count_idx, (0, num_classes - len(max_count_idx)), 'constant', constant_values=np.NaN)\n    majority_vote_label = np.full(len(labels_multiannotator), np.nan)\n    label_count = np.apply_along_axis(lambda s: np.bincount(s[~np.isnan(s)].astype(int), minlength=num_classes), axis=1, arr=labels_multiannotator)\n    mode_labels_multiannotator = np.apply_along_axis(get_labels_mode, axis=1, arr=label_count, num_classes=num_classes)\n    nontied_idx = []\n    tied_idx = dict()\n    for (idx, label_mode) in enumerate(mode_labels_multiannotator):\n        label_mode = label_mode[~np.isnan(label_mode)].astype(int)\n        if len(label_mode) == 1:\n            majority_vote_label[idx] = label_mode[0]\n            nontied_idx.append(idx)\n        else:\n            tied_idx[idx] = label_mode\n    if pred_probs is not None and len(tied_idx) > 0:\n        for (idx, label_mode) in tied_idx.copy().items():\n            max_pred_probs = np.where(pred_probs[idx, label_mode] == np.max(pred_probs[idx, label_mode]))[0]\n            if len(max_pred_probs) == 1:\n                majority_vote_label[idx] = label_mode[max_pred_probs[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[max_pred_probs]\n    if len(tied_idx) > 0:\n        class_frequencies = label_count.sum(axis=0)\n        for (idx, label_mode) in tied_idx.copy().items():\n            min_frequency = np.where(class_frequencies[label_mode] == np.min(class_frequencies[label_mode]))[0]\n            if len(min_frequency) == 1:\n                majority_vote_label[idx] = label_mode[min_frequency[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[min_frequency]\n    if len(tied_idx) > 0:\n        nontied_majority_vote_label = majority_vote_label[nontied_idx]\n        nontied_labels_multiannotator = labels_multiannotator[nontied_idx]\n        annotator_agreement_with_consensus = np.zeros(nontied_labels_multiannotator.shape[1])\n        for i in range(len(annotator_agreement_with_consensus)):\n            labels = nontied_labels_multiannotator[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_agreement_with_consensus[i] = np.NaN\n            else:\n                annotator_agreement_with_consensus[i] = np.mean(labels[labels_mask] == nontied_majority_vote_label[labels_mask])\n        nan_mask = np.isnan(annotator_agreement_with_consensus)\n        avg_annotator_agreement = np.mean(annotator_agreement_with_consensus[~nan_mask])\n        annotator_agreement_with_consensus[nan_mask] = avg_annotator_agreement\n        for (idx, label_mode) in tied_idx.copy().items():\n            label_quality_score = np.array([np.mean(annotator_agreement_with_consensus[np.where(labels_multiannotator[idx] == label)[0]]) for label in label_mode])\n            max_score = np.where(label_quality_score == label_quality_score.max())[0]\n            if len(max_score) == 1:\n                majority_vote_label[idx] = label_mode[max_score[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[max_score]\n    if len(tied_idx) > 0:\n        warnings.warn(f'breaking ties of examples {list(tied_idx.keys())} by random selection, you may want to set seed for reproducability')\n        for (idx, label_mode) in tied_idx.items():\n            majority_vote_label[idx] = np.random.choice(label_mode)\n    if verbose:\n        check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=majority_vote_label, consensus_method='majority_vote')\n    return majority_vote_label.astype(int)",
            "def get_majority_vote_label(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: Optional[np.ndarray]=None, verbose: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the majority vote label for each example, aggregated from the labels given by multiple annotators.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray, optional\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    verbose : bool, optional\\n        Important warnings and other printed statements may be suppressed if ``verbose`` is set to ``False``.\\n    Returns\\n    -------\\n    consensus_label: np.ndarray\\n        An array of shape ``(N,)`` with the majority vote label aggregated from all annotators.\\n\\n        In the event of majority vote ties, ties are broken in the following order:\\n        using the model ``pred_probs`` (if provided) and selecting the class with highest predicted probability,\\n        using the empirical class frequencies and selecting the class with highest frequency,\\n        using an initial annotator quality score and selecting the class that has been labeled by annotators with higher quality,\\n        and lastly by random selection.\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    if verbose:\n        assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, annotator_ids=annotator_ids)\n    if pred_probs is not None:\n        num_classes = pred_probs.shape[1]\n    else:\n        num_classes = int(np.nanmax(labels_multiannotator) + 1)\n\n    def get_labels_mode(label_count, num_classes):\n        max_count_idx = np.where(label_count == np.nanmax(label_count))[0].astype(float)\n        return np.pad(max_count_idx, (0, num_classes - len(max_count_idx)), 'constant', constant_values=np.NaN)\n    majority_vote_label = np.full(len(labels_multiannotator), np.nan)\n    label_count = np.apply_along_axis(lambda s: np.bincount(s[~np.isnan(s)].astype(int), minlength=num_classes), axis=1, arr=labels_multiannotator)\n    mode_labels_multiannotator = np.apply_along_axis(get_labels_mode, axis=1, arr=label_count, num_classes=num_classes)\n    nontied_idx = []\n    tied_idx = dict()\n    for (idx, label_mode) in enumerate(mode_labels_multiannotator):\n        label_mode = label_mode[~np.isnan(label_mode)].astype(int)\n        if len(label_mode) == 1:\n            majority_vote_label[idx] = label_mode[0]\n            nontied_idx.append(idx)\n        else:\n            tied_idx[idx] = label_mode\n    if pred_probs is not None and len(tied_idx) > 0:\n        for (idx, label_mode) in tied_idx.copy().items():\n            max_pred_probs = np.where(pred_probs[idx, label_mode] == np.max(pred_probs[idx, label_mode]))[0]\n            if len(max_pred_probs) == 1:\n                majority_vote_label[idx] = label_mode[max_pred_probs[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[max_pred_probs]\n    if len(tied_idx) > 0:\n        class_frequencies = label_count.sum(axis=0)\n        for (idx, label_mode) in tied_idx.copy().items():\n            min_frequency = np.where(class_frequencies[label_mode] == np.min(class_frequencies[label_mode]))[0]\n            if len(min_frequency) == 1:\n                majority_vote_label[idx] = label_mode[min_frequency[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[min_frequency]\n    if len(tied_idx) > 0:\n        nontied_majority_vote_label = majority_vote_label[nontied_idx]\n        nontied_labels_multiannotator = labels_multiannotator[nontied_idx]\n        annotator_agreement_with_consensus = np.zeros(nontied_labels_multiannotator.shape[1])\n        for i in range(len(annotator_agreement_with_consensus)):\n            labels = nontied_labels_multiannotator[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_agreement_with_consensus[i] = np.NaN\n            else:\n                annotator_agreement_with_consensus[i] = np.mean(labels[labels_mask] == nontied_majority_vote_label[labels_mask])\n        nan_mask = np.isnan(annotator_agreement_with_consensus)\n        avg_annotator_agreement = np.mean(annotator_agreement_with_consensus[~nan_mask])\n        annotator_agreement_with_consensus[nan_mask] = avg_annotator_agreement\n        for (idx, label_mode) in tied_idx.copy().items():\n            label_quality_score = np.array([np.mean(annotator_agreement_with_consensus[np.where(labels_multiannotator[idx] == label)[0]]) for label in label_mode])\n            max_score = np.where(label_quality_score == label_quality_score.max())[0]\n            if len(max_score) == 1:\n                majority_vote_label[idx] = label_mode[max_score[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[max_score]\n    if len(tied_idx) > 0:\n        warnings.warn(f'breaking ties of examples {list(tied_idx.keys())} by random selection, you may want to set seed for reproducability')\n        for (idx, label_mode) in tied_idx.items():\n            majority_vote_label[idx] = np.random.choice(label_mode)\n    if verbose:\n        check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=majority_vote_label, consensus_method='majority_vote')\n    return majority_vote_label.astype(int)",
            "def get_majority_vote_label(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: Optional[np.ndarray]=None, verbose: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the majority vote label for each example, aggregated from the labels given by multiple annotators.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray, optional\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    verbose : bool, optional\\n        Important warnings and other printed statements may be suppressed if ``verbose`` is set to ``False``.\\n    Returns\\n    -------\\n    consensus_label: np.ndarray\\n        An array of shape ``(N,)`` with the majority vote label aggregated from all annotators.\\n\\n        In the event of majority vote ties, ties are broken in the following order:\\n        using the model ``pred_probs`` (if provided) and selecting the class with highest predicted probability,\\n        using the empirical class frequencies and selecting the class with highest frequency,\\n        using an initial annotator quality score and selecting the class that has been labeled by annotators with higher quality,\\n        and lastly by random selection.\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    if verbose:\n        assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, annotator_ids=annotator_ids)\n    if pred_probs is not None:\n        num_classes = pred_probs.shape[1]\n    else:\n        num_classes = int(np.nanmax(labels_multiannotator) + 1)\n\n    def get_labels_mode(label_count, num_classes):\n        max_count_idx = np.where(label_count == np.nanmax(label_count))[0].astype(float)\n        return np.pad(max_count_idx, (0, num_classes - len(max_count_idx)), 'constant', constant_values=np.NaN)\n    majority_vote_label = np.full(len(labels_multiannotator), np.nan)\n    label_count = np.apply_along_axis(lambda s: np.bincount(s[~np.isnan(s)].astype(int), minlength=num_classes), axis=1, arr=labels_multiannotator)\n    mode_labels_multiannotator = np.apply_along_axis(get_labels_mode, axis=1, arr=label_count, num_classes=num_classes)\n    nontied_idx = []\n    tied_idx = dict()\n    for (idx, label_mode) in enumerate(mode_labels_multiannotator):\n        label_mode = label_mode[~np.isnan(label_mode)].astype(int)\n        if len(label_mode) == 1:\n            majority_vote_label[idx] = label_mode[0]\n            nontied_idx.append(idx)\n        else:\n            tied_idx[idx] = label_mode\n    if pred_probs is not None and len(tied_idx) > 0:\n        for (idx, label_mode) in tied_idx.copy().items():\n            max_pred_probs = np.where(pred_probs[idx, label_mode] == np.max(pred_probs[idx, label_mode]))[0]\n            if len(max_pred_probs) == 1:\n                majority_vote_label[idx] = label_mode[max_pred_probs[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[max_pred_probs]\n    if len(tied_idx) > 0:\n        class_frequencies = label_count.sum(axis=0)\n        for (idx, label_mode) in tied_idx.copy().items():\n            min_frequency = np.where(class_frequencies[label_mode] == np.min(class_frequencies[label_mode]))[0]\n            if len(min_frequency) == 1:\n                majority_vote_label[idx] = label_mode[min_frequency[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[min_frequency]\n    if len(tied_idx) > 0:\n        nontied_majority_vote_label = majority_vote_label[nontied_idx]\n        nontied_labels_multiannotator = labels_multiannotator[nontied_idx]\n        annotator_agreement_with_consensus = np.zeros(nontied_labels_multiannotator.shape[1])\n        for i in range(len(annotator_agreement_with_consensus)):\n            labels = nontied_labels_multiannotator[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_agreement_with_consensus[i] = np.NaN\n            else:\n                annotator_agreement_with_consensus[i] = np.mean(labels[labels_mask] == nontied_majority_vote_label[labels_mask])\n        nan_mask = np.isnan(annotator_agreement_with_consensus)\n        avg_annotator_agreement = np.mean(annotator_agreement_with_consensus[~nan_mask])\n        annotator_agreement_with_consensus[nan_mask] = avg_annotator_agreement\n        for (idx, label_mode) in tied_idx.copy().items():\n            label_quality_score = np.array([np.mean(annotator_agreement_with_consensus[np.where(labels_multiannotator[idx] == label)[0]]) for label in label_mode])\n            max_score = np.where(label_quality_score == label_quality_score.max())[0]\n            if len(max_score) == 1:\n                majority_vote_label[idx] = label_mode[max_score[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[max_score]\n    if len(tied_idx) > 0:\n        warnings.warn(f'breaking ties of examples {list(tied_idx.keys())} by random selection, you may want to set seed for reproducability')\n        for (idx, label_mode) in tied_idx.items():\n            majority_vote_label[idx] = np.random.choice(label_mode)\n    if verbose:\n        check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=majority_vote_label, consensus_method='majority_vote')\n    return majority_vote_label.astype(int)",
            "def get_majority_vote_label(labels_multiannotator: Union[pd.DataFrame, np.ndarray], pred_probs: Optional[np.ndarray]=None, verbose: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the majority vote label for each example, aggregated from the labels given by multiple annotators.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : pd.DataFrame or np.ndarray\\n        2D pandas DataFrame or array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray, optional\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    verbose : bool, optional\\n        Important warnings and other printed statements may be suppressed if ``verbose`` is set to ``False``.\\n    Returns\\n    -------\\n    consensus_label: np.ndarray\\n        An array of shape ``(N,)`` with the majority vote label aggregated from all annotators.\\n\\n        In the event of majority vote ties, ties are broken in the following order:\\n        using the model ``pred_probs`` (if provided) and selecting the class with highest predicted probability,\\n        using the empirical class frequencies and selecting the class with highest frequency,\\n        using an initial annotator quality score and selecting the class that has been labeled by annotators with higher quality,\\n        and lastly by random selection.\\n    '\n    if isinstance(labels_multiannotator, pd.DataFrame):\n        annotator_ids = labels_multiannotator.columns\n        labels_multiannotator = labels_multiannotator.replace({pd.NA: np.NaN}).astype(float).to_numpy()\n    elif isinstance(labels_multiannotator, np.ndarray):\n        annotator_ids = None\n    else:\n        raise ValueError('labels_multiannotator must be either a NumPy array or Pandas DataFrame.')\n    if verbose:\n        assert_valid_inputs_multiannotator(labels_multiannotator, pred_probs, annotator_ids=annotator_ids)\n    if pred_probs is not None:\n        num_classes = pred_probs.shape[1]\n    else:\n        num_classes = int(np.nanmax(labels_multiannotator) + 1)\n\n    def get_labels_mode(label_count, num_classes):\n        max_count_idx = np.where(label_count == np.nanmax(label_count))[0].astype(float)\n        return np.pad(max_count_idx, (0, num_classes - len(max_count_idx)), 'constant', constant_values=np.NaN)\n    majority_vote_label = np.full(len(labels_multiannotator), np.nan)\n    label_count = np.apply_along_axis(lambda s: np.bincount(s[~np.isnan(s)].astype(int), minlength=num_classes), axis=1, arr=labels_multiannotator)\n    mode_labels_multiannotator = np.apply_along_axis(get_labels_mode, axis=1, arr=label_count, num_classes=num_classes)\n    nontied_idx = []\n    tied_idx = dict()\n    for (idx, label_mode) in enumerate(mode_labels_multiannotator):\n        label_mode = label_mode[~np.isnan(label_mode)].astype(int)\n        if len(label_mode) == 1:\n            majority_vote_label[idx] = label_mode[0]\n            nontied_idx.append(idx)\n        else:\n            tied_idx[idx] = label_mode\n    if pred_probs is not None and len(tied_idx) > 0:\n        for (idx, label_mode) in tied_idx.copy().items():\n            max_pred_probs = np.where(pred_probs[idx, label_mode] == np.max(pred_probs[idx, label_mode]))[0]\n            if len(max_pred_probs) == 1:\n                majority_vote_label[idx] = label_mode[max_pred_probs[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[max_pred_probs]\n    if len(tied_idx) > 0:\n        class_frequencies = label_count.sum(axis=0)\n        for (idx, label_mode) in tied_idx.copy().items():\n            min_frequency = np.where(class_frequencies[label_mode] == np.min(class_frequencies[label_mode]))[0]\n            if len(min_frequency) == 1:\n                majority_vote_label[idx] = label_mode[min_frequency[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[min_frequency]\n    if len(tied_idx) > 0:\n        nontied_majority_vote_label = majority_vote_label[nontied_idx]\n        nontied_labels_multiannotator = labels_multiannotator[nontied_idx]\n        annotator_agreement_with_consensus = np.zeros(nontied_labels_multiannotator.shape[1])\n        for i in range(len(annotator_agreement_with_consensus)):\n            labels = nontied_labels_multiannotator[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_agreement_with_consensus[i] = np.NaN\n            else:\n                annotator_agreement_with_consensus[i] = np.mean(labels[labels_mask] == nontied_majority_vote_label[labels_mask])\n        nan_mask = np.isnan(annotator_agreement_with_consensus)\n        avg_annotator_agreement = np.mean(annotator_agreement_with_consensus[~nan_mask])\n        annotator_agreement_with_consensus[nan_mask] = avg_annotator_agreement\n        for (idx, label_mode) in tied_idx.copy().items():\n            label_quality_score = np.array([np.mean(annotator_agreement_with_consensus[np.where(labels_multiannotator[idx] == label)[0]]) for label in label_mode])\n            max_score = np.where(label_quality_score == label_quality_score.max())[0]\n            if len(max_score) == 1:\n                majority_vote_label[idx] = label_mode[max_score[0]]\n                del tied_idx[idx]\n            else:\n                tied_idx[idx] = label_mode[max_score]\n    if len(tied_idx) > 0:\n        warnings.warn(f'breaking ties of examples {list(tied_idx.keys())} by random selection, you may want to set seed for reproducability')\n        for (idx, label_mode) in tied_idx.items():\n            majority_vote_label[idx] = np.random.choice(label_mode)\n    if verbose:\n        check_consensus_label_classes(labels_multiannotator=labels_multiannotator, consensus_label=majority_vote_label, consensus_method='majority_vote')\n    return majority_vote_label.astype(int)"
        ]
    },
    {
        "func_name": "convert_long_to_wide_dataset",
        "original": "def convert_long_to_wide_dataset(labels_multiannotator_long: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Converts a long format dataset to wide format which is suitable for passing into\n    :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n\n    Dataframe must contain three columns named:\n\n    #. ``task`` representing each example labeled by the annotators\n    #. ``annotator`` representing each annotator\n    #. ``label`` representing the label given by an annotator for the corresponding task (i.e. example)\n\n    Parameters\n    ----------\n    labels_multiannotator_long : pd.DataFrame\n        pandas DataFrame in long format with three columns named ``task``, ``annotator`` and ``label``\n\n    Returns\n    -------\n    labels_multiannotator_wide : pd.DataFrame\n        pandas DataFrame of the proper format to be passed as ``labels_multiannotator`` for the other ``cleanlab.multiannotator`` functions.\n    \"\"\"\n    labels_multiannotator_wide = labels_multiannotator_long.pivot(index='task', columns='annotator', values='label')\n    labels_multiannotator_wide.index.name = None\n    labels_multiannotator_wide.columns.name = None\n    return labels_multiannotator_wide",
        "mutated": [
            "def convert_long_to_wide_dataset(labels_multiannotator_long: pd.DataFrame) -> pd.DataFrame:\n    if False:\n        i = 10\n    'Converts a long format dataset to wide format which is suitable for passing into\\n    :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    Dataframe must contain three columns named:\\n\\n    #. ``task`` representing each example labeled by the annotators\\n    #. ``annotator`` representing each annotator\\n    #. ``label`` representing the label given by an annotator for the corresponding task (i.e. example)\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator_long : pd.DataFrame\\n        pandas DataFrame in long format with three columns named ``task``, ``annotator`` and ``label``\\n\\n    Returns\\n    -------\\n    labels_multiannotator_wide : pd.DataFrame\\n        pandas DataFrame of the proper format to be passed as ``labels_multiannotator`` for the other ``cleanlab.multiannotator`` functions.\\n    '\n    labels_multiannotator_wide = labels_multiannotator_long.pivot(index='task', columns='annotator', values='label')\n    labels_multiannotator_wide.index.name = None\n    labels_multiannotator_wide.columns.name = None\n    return labels_multiannotator_wide",
            "def convert_long_to_wide_dataset(labels_multiannotator_long: pd.DataFrame) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Converts a long format dataset to wide format which is suitable for passing into\\n    :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    Dataframe must contain three columns named:\\n\\n    #. ``task`` representing each example labeled by the annotators\\n    #. ``annotator`` representing each annotator\\n    #. ``label`` representing the label given by an annotator for the corresponding task (i.e. example)\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator_long : pd.DataFrame\\n        pandas DataFrame in long format with three columns named ``task``, ``annotator`` and ``label``\\n\\n    Returns\\n    -------\\n    labels_multiannotator_wide : pd.DataFrame\\n        pandas DataFrame of the proper format to be passed as ``labels_multiannotator`` for the other ``cleanlab.multiannotator`` functions.\\n    '\n    labels_multiannotator_wide = labels_multiannotator_long.pivot(index='task', columns='annotator', values='label')\n    labels_multiannotator_wide.index.name = None\n    labels_multiannotator_wide.columns.name = None\n    return labels_multiannotator_wide",
            "def convert_long_to_wide_dataset(labels_multiannotator_long: pd.DataFrame) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Converts a long format dataset to wide format which is suitable for passing into\\n    :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    Dataframe must contain three columns named:\\n\\n    #. ``task`` representing each example labeled by the annotators\\n    #. ``annotator`` representing each annotator\\n    #. ``label`` representing the label given by an annotator for the corresponding task (i.e. example)\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator_long : pd.DataFrame\\n        pandas DataFrame in long format with three columns named ``task``, ``annotator`` and ``label``\\n\\n    Returns\\n    -------\\n    labels_multiannotator_wide : pd.DataFrame\\n        pandas DataFrame of the proper format to be passed as ``labels_multiannotator`` for the other ``cleanlab.multiannotator`` functions.\\n    '\n    labels_multiannotator_wide = labels_multiannotator_long.pivot(index='task', columns='annotator', values='label')\n    labels_multiannotator_wide.index.name = None\n    labels_multiannotator_wide.columns.name = None\n    return labels_multiannotator_wide",
            "def convert_long_to_wide_dataset(labels_multiannotator_long: pd.DataFrame) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Converts a long format dataset to wide format which is suitable for passing into\\n    :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    Dataframe must contain three columns named:\\n\\n    #. ``task`` representing each example labeled by the annotators\\n    #. ``annotator`` representing each annotator\\n    #. ``label`` representing the label given by an annotator for the corresponding task (i.e. example)\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator_long : pd.DataFrame\\n        pandas DataFrame in long format with three columns named ``task``, ``annotator`` and ``label``\\n\\n    Returns\\n    -------\\n    labels_multiannotator_wide : pd.DataFrame\\n        pandas DataFrame of the proper format to be passed as ``labels_multiannotator`` for the other ``cleanlab.multiannotator`` functions.\\n    '\n    labels_multiannotator_wide = labels_multiannotator_long.pivot(index='task', columns='annotator', values='label')\n    labels_multiannotator_wide.index.name = None\n    labels_multiannotator_wide.columns.name = None\n    return labels_multiannotator_wide",
            "def convert_long_to_wide_dataset(labels_multiannotator_long: pd.DataFrame) -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Converts a long format dataset to wide format which is suitable for passing into\\n    :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n\\n    Dataframe must contain three columns named:\\n\\n    #. ``task`` representing each example labeled by the annotators\\n    #. ``annotator`` representing each annotator\\n    #. ``label`` representing the label given by an annotator for the corresponding task (i.e. example)\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator_long : pd.DataFrame\\n        pandas DataFrame in long format with three columns named ``task``, ``annotator`` and ``label``\\n\\n    Returns\\n    -------\\n    labels_multiannotator_wide : pd.DataFrame\\n        pandas DataFrame of the proper format to be passed as ``labels_multiannotator`` for the other ``cleanlab.multiannotator`` functions.\\n    '\n    labels_multiannotator_wide = labels_multiannotator_long.pivot(index='task', columns='annotator', values='label')\n    labels_multiannotator_wide.index.name = None\n    labels_multiannotator_wide.columns.name = None\n    return labels_multiannotator_wide"
        ]
    },
    {
        "func_name": "_get_consensus_stats",
        "original": "def _get_consensus_stats(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, num_annotations: np.ndarray, consensus_label: np.ndarray, quality_method: str='crowdlab', verbose: bool=True, ensemble: bool=False, label_quality_score_kwargs: dict={}) -> tuple:\n    \"\"\"Returns a tuple containing the consensus labels, annotator agreement scores, and quality of consensus\n\n    Parameters\n    ----------\n    labels_multiannotator : np.ndarray\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\n        where N is the number of examples and M is the number of annotators.\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    pred_probs : np.ndarray\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    num_annotations : np.ndarray\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\n    consensus_label : np.ndarray\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\n        Specifies the method used to calculate the quality of the consensus label.\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\n    label_quality_score_kwargs : dict, optional\n        Keyword arguments to pass into ``get_label_quality_scores()``.\n    verbose : bool, default = True\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\n    ensemble : bool, default = False\n        Boolean flag to indicate whether the pred_probs passed are from ensemble models.\n\n    Returns\n    ------\n    stats : tuple\n        A tuple of (consensus_label, annotator_agreement, consensus_quality_score, post_pred_probs).\n    \"\"\"\n    annotator_agreement = _get_annotator_agreement_with_consensus(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label)\n    if ensemble:\n        (post_pred_probs, model_weight, annotator_weight) = _get_post_pred_probs_and_weights_ensemble(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, prior_pred_probs=pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, verbose=verbose)\n    else:\n        (post_pred_probs, model_weight, annotator_weight) = _get_post_pred_probs_and_weights(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, prior_pred_probs=pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, verbose=verbose)\n    consensus_quality_score = _get_consensus_quality_score(consensus_label=consensus_label, pred_probs=post_pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, label_quality_score_kwargs=label_quality_score_kwargs)\n    return (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight)",
        "mutated": [
            "def _get_consensus_stats(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, num_annotations: np.ndarray, consensus_label: np.ndarray, quality_method: str='crowdlab', verbose: bool=True, ensemble: bool=False, label_quality_score_kwargs: dict={}) -> tuple:\n    if False:\n        i = 10\n    'Returns a tuple containing the consensus labels, annotator agreement scores, and quality of consensus\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments to pass into ``get_label_quality_scores()``.\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n    ensemble : bool, default = False\\n        Boolean flag to indicate whether the pred_probs passed are from ensemble models.\\n\\n    Returns\\n    ------\\n    stats : tuple\\n        A tuple of (consensus_label, annotator_agreement, consensus_quality_score, post_pred_probs).\\n    '\n    annotator_agreement = _get_annotator_agreement_with_consensus(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label)\n    if ensemble:\n        (post_pred_probs, model_weight, annotator_weight) = _get_post_pred_probs_and_weights_ensemble(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, prior_pred_probs=pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, verbose=verbose)\n    else:\n        (post_pred_probs, model_weight, annotator_weight) = _get_post_pred_probs_and_weights(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, prior_pred_probs=pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, verbose=verbose)\n    consensus_quality_score = _get_consensus_quality_score(consensus_label=consensus_label, pred_probs=post_pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, label_quality_score_kwargs=label_quality_score_kwargs)\n    return (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight)",
            "def _get_consensus_stats(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, num_annotations: np.ndarray, consensus_label: np.ndarray, quality_method: str='crowdlab', verbose: bool=True, ensemble: bool=False, label_quality_score_kwargs: dict={}) -> tuple:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns a tuple containing the consensus labels, annotator agreement scores, and quality of consensus\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments to pass into ``get_label_quality_scores()``.\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n    ensemble : bool, default = False\\n        Boolean flag to indicate whether the pred_probs passed are from ensemble models.\\n\\n    Returns\\n    ------\\n    stats : tuple\\n        A tuple of (consensus_label, annotator_agreement, consensus_quality_score, post_pred_probs).\\n    '\n    annotator_agreement = _get_annotator_agreement_with_consensus(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label)\n    if ensemble:\n        (post_pred_probs, model_weight, annotator_weight) = _get_post_pred_probs_and_weights_ensemble(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, prior_pred_probs=pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, verbose=verbose)\n    else:\n        (post_pred_probs, model_weight, annotator_weight) = _get_post_pred_probs_and_weights(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, prior_pred_probs=pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, verbose=verbose)\n    consensus_quality_score = _get_consensus_quality_score(consensus_label=consensus_label, pred_probs=post_pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, label_quality_score_kwargs=label_quality_score_kwargs)\n    return (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight)",
            "def _get_consensus_stats(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, num_annotations: np.ndarray, consensus_label: np.ndarray, quality_method: str='crowdlab', verbose: bool=True, ensemble: bool=False, label_quality_score_kwargs: dict={}) -> tuple:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns a tuple containing the consensus labels, annotator agreement scores, and quality of consensus\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments to pass into ``get_label_quality_scores()``.\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n    ensemble : bool, default = False\\n        Boolean flag to indicate whether the pred_probs passed are from ensemble models.\\n\\n    Returns\\n    ------\\n    stats : tuple\\n        A tuple of (consensus_label, annotator_agreement, consensus_quality_score, post_pred_probs).\\n    '\n    annotator_agreement = _get_annotator_agreement_with_consensus(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label)\n    if ensemble:\n        (post_pred_probs, model_weight, annotator_weight) = _get_post_pred_probs_and_weights_ensemble(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, prior_pred_probs=pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, verbose=verbose)\n    else:\n        (post_pred_probs, model_weight, annotator_weight) = _get_post_pred_probs_and_weights(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, prior_pred_probs=pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, verbose=verbose)\n    consensus_quality_score = _get_consensus_quality_score(consensus_label=consensus_label, pred_probs=post_pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, label_quality_score_kwargs=label_quality_score_kwargs)\n    return (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight)",
            "def _get_consensus_stats(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, num_annotations: np.ndarray, consensus_label: np.ndarray, quality_method: str='crowdlab', verbose: bool=True, ensemble: bool=False, label_quality_score_kwargs: dict={}) -> tuple:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns a tuple containing the consensus labels, annotator agreement scores, and quality of consensus\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments to pass into ``get_label_quality_scores()``.\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n    ensemble : bool, default = False\\n        Boolean flag to indicate whether the pred_probs passed are from ensemble models.\\n\\n    Returns\\n    ------\\n    stats : tuple\\n        A tuple of (consensus_label, annotator_agreement, consensus_quality_score, post_pred_probs).\\n    '\n    annotator_agreement = _get_annotator_agreement_with_consensus(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label)\n    if ensemble:\n        (post_pred_probs, model_weight, annotator_weight) = _get_post_pred_probs_and_weights_ensemble(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, prior_pred_probs=pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, verbose=verbose)\n    else:\n        (post_pred_probs, model_weight, annotator_weight) = _get_post_pred_probs_and_weights(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, prior_pred_probs=pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, verbose=verbose)\n    consensus_quality_score = _get_consensus_quality_score(consensus_label=consensus_label, pred_probs=post_pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, label_quality_score_kwargs=label_quality_score_kwargs)\n    return (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight)",
            "def _get_consensus_stats(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, num_annotations: np.ndarray, consensus_label: np.ndarray, quality_method: str='crowdlab', verbose: bool=True, ensemble: bool=False, label_quality_score_kwargs: dict={}) -> tuple:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns a tuple containing the consensus labels, annotator agreement scores, and quality of consensus\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    label_quality_score_kwargs : dict, optional\\n        Keyword arguments to pass into ``get_label_quality_scores()``.\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n    ensemble : bool, default = False\\n        Boolean flag to indicate whether the pred_probs passed are from ensemble models.\\n\\n    Returns\\n    ------\\n    stats : tuple\\n        A tuple of (consensus_label, annotator_agreement, consensus_quality_score, post_pred_probs).\\n    '\n    annotator_agreement = _get_annotator_agreement_with_consensus(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label)\n    if ensemble:\n        (post_pred_probs, model_weight, annotator_weight) = _get_post_pred_probs_and_weights_ensemble(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, prior_pred_probs=pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, verbose=verbose)\n    else:\n        (post_pred_probs, model_weight, annotator_weight) = _get_post_pred_probs_and_weights(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, prior_pred_probs=pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, verbose=verbose)\n    consensus_quality_score = _get_consensus_quality_score(consensus_label=consensus_label, pred_probs=post_pred_probs, num_annotations=num_annotations, annotator_agreement=annotator_agreement, quality_method=quality_method, label_quality_score_kwargs=label_quality_score_kwargs)\n    return (annotator_agreement, consensus_quality_score, post_pred_probs, model_weight, annotator_weight)"
        ]
    },
    {
        "func_name": "_get_annotator_stats",
        "original": "def _get_annotator_stats(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, consensus_label: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, model_weight: np.ndarray, annotator_weight: np.ndarray, consensus_quality_score: np.ndarray, detailed_label_quality: Optional[np.ndarray]=None, annotator_ids: Optional[pd.Index]=None, quality_method: str='crowdlab') -> pd.DataFrame:\n    \"\"\"Returns a dictionary containing overall statistics about each annotator.\n\n    Parameters\n    ----------\n    labels_multiannotator : np.ndarray\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\n        where N is the number of examples and M is the number of annotators.\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    pred_probs : np.ndarray\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    consensus_label : np.ndarray\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\n    num_annotations : np.ndarray\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\n    annotator_agreement : np.ndarray\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\n    model_weight : float\n        float specifying the model weight used in weighted averages,\n        None if model weight is not used to compute quality scores\n    annotator_weight : np.ndarray\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\n        None if annotator weights are not used to compute quality scores\n    consensus_quality_score : np.ndarray\n        An array of shape ``(N,)`` with the quality score of the consensus.\n    detailed_label_quality :\n        pandas DataFrame containing the detailed label quality scores for all examples and annotators\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\n        Specifies the method used to calculate the quality of the consensus label.\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\n\n    Returns\n    -------\n    annotator_stats : pd.DataFrame\n        Overall statistics about each annotator.\n        For details, see the documentation of :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    \"\"\"\n    annotator_quality = _get_annotator_quality(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=model_weight, annotator_weight=annotator_weight, detailed_label_quality=detailed_label_quality, quality_method=quality_method)\n    num_examples_labeled = np.sum(~np.isnan(labels_multiannotator), axis=0)\n    agreement_with_consensus = np.zeros(labels_multiannotator.shape[1])\n    for i in range(len(agreement_with_consensus)):\n        labels = labels_multiannotator[:, i]\n        labels_mask = ~np.isnan(labels)\n        agreement_with_consensus[i] = np.mean(labels[labels_mask] == consensus_label[labels_mask])\n    worst_class = _get_annotator_worst_class(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_quality_score=consensus_quality_score)\n    annotator_stats = pd.DataFrame({'annotator_quality': annotator_quality, 'agreement_with_consensus': agreement_with_consensus, 'worst_class': worst_class, 'num_examples_labeled': num_examples_labeled}, index=annotator_ids)\n    return annotator_stats.sort_values(by=['annotator_quality', 'agreement_with_consensus'])",
        "mutated": [
            "def _get_annotator_stats(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, consensus_label: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, model_weight: np.ndarray, annotator_weight: np.ndarray, consensus_quality_score: np.ndarray, detailed_label_quality: Optional[np.ndarray]=None, annotator_ids: Optional[pd.Index]=None, quality_method: str='crowdlab') -> pd.DataFrame:\n    if False:\n        i = 10\n    'Returns a dictionary containing overall statistics about each annotator.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    model_weight : float\\n        float specifying the model weight used in weighted averages,\\n        None if model weight is not used to compute quality scores\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        None if annotator weights are not used to compute quality scores\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n    detailed_label_quality :\\n        pandas DataFrame containing the detailed label quality scores for all examples and annotators\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    annotator_stats : pd.DataFrame\\n        Overall statistics about each annotator.\\n        For details, see the documentation of :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    '\n    annotator_quality = _get_annotator_quality(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=model_weight, annotator_weight=annotator_weight, detailed_label_quality=detailed_label_quality, quality_method=quality_method)\n    num_examples_labeled = np.sum(~np.isnan(labels_multiannotator), axis=0)\n    agreement_with_consensus = np.zeros(labels_multiannotator.shape[1])\n    for i in range(len(agreement_with_consensus)):\n        labels = labels_multiannotator[:, i]\n        labels_mask = ~np.isnan(labels)\n        agreement_with_consensus[i] = np.mean(labels[labels_mask] == consensus_label[labels_mask])\n    worst_class = _get_annotator_worst_class(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_quality_score=consensus_quality_score)\n    annotator_stats = pd.DataFrame({'annotator_quality': annotator_quality, 'agreement_with_consensus': agreement_with_consensus, 'worst_class': worst_class, 'num_examples_labeled': num_examples_labeled}, index=annotator_ids)\n    return annotator_stats.sort_values(by=['annotator_quality', 'agreement_with_consensus'])",
            "def _get_annotator_stats(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, consensus_label: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, model_weight: np.ndarray, annotator_weight: np.ndarray, consensus_quality_score: np.ndarray, detailed_label_quality: Optional[np.ndarray]=None, annotator_ids: Optional[pd.Index]=None, quality_method: str='crowdlab') -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns a dictionary containing overall statistics about each annotator.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    model_weight : float\\n        float specifying the model weight used in weighted averages,\\n        None if model weight is not used to compute quality scores\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        None if annotator weights are not used to compute quality scores\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n    detailed_label_quality :\\n        pandas DataFrame containing the detailed label quality scores for all examples and annotators\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    annotator_stats : pd.DataFrame\\n        Overall statistics about each annotator.\\n        For details, see the documentation of :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    '\n    annotator_quality = _get_annotator_quality(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=model_weight, annotator_weight=annotator_weight, detailed_label_quality=detailed_label_quality, quality_method=quality_method)\n    num_examples_labeled = np.sum(~np.isnan(labels_multiannotator), axis=0)\n    agreement_with_consensus = np.zeros(labels_multiannotator.shape[1])\n    for i in range(len(agreement_with_consensus)):\n        labels = labels_multiannotator[:, i]\n        labels_mask = ~np.isnan(labels)\n        agreement_with_consensus[i] = np.mean(labels[labels_mask] == consensus_label[labels_mask])\n    worst_class = _get_annotator_worst_class(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_quality_score=consensus_quality_score)\n    annotator_stats = pd.DataFrame({'annotator_quality': annotator_quality, 'agreement_with_consensus': agreement_with_consensus, 'worst_class': worst_class, 'num_examples_labeled': num_examples_labeled}, index=annotator_ids)\n    return annotator_stats.sort_values(by=['annotator_quality', 'agreement_with_consensus'])",
            "def _get_annotator_stats(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, consensus_label: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, model_weight: np.ndarray, annotator_weight: np.ndarray, consensus_quality_score: np.ndarray, detailed_label_quality: Optional[np.ndarray]=None, annotator_ids: Optional[pd.Index]=None, quality_method: str='crowdlab') -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns a dictionary containing overall statistics about each annotator.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    model_weight : float\\n        float specifying the model weight used in weighted averages,\\n        None if model weight is not used to compute quality scores\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        None if annotator weights are not used to compute quality scores\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n    detailed_label_quality :\\n        pandas DataFrame containing the detailed label quality scores for all examples and annotators\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    annotator_stats : pd.DataFrame\\n        Overall statistics about each annotator.\\n        For details, see the documentation of :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    '\n    annotator_quality = _get_annotator_quality(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=model_weight, annotator_weight=annotator_weight, detailed_label_quality=detailed_label_quality, quality_method=quality_method)\n    num_examples_labeled = np.sum(~np.isnan(labels_multiannotator), axis=0)\n    agreement_with_consensus = np.zeros(labels_multiannotator.shape[1])\n    for i in range(len(agreement_with_consensus)):\n        labels = labels_multiannotator[:, i]\n        labels_mask = ~np.isnan(labels)\n        agreement_with_consensus[i] = np.mean(labels[labels_mask] == consensus_label[labels_mask])\n    worst_class = _get_annotator_worst_class(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_quality_score=consensus_quality_score)\n    annotator_stats = pd.DataFrame({'annotator_quality': annotator_quality, 'agreement_with_consensus': agreement_with_consensus, 'worst_class': worst_class, 'num_examples_labeled': num_examples_labeled}, index=annotator_ids)\n    return annotator_stats.sort_values(by=['annotator_quality', 'agreement_with_consensus'])",
            "def _get_annotator_stats(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, consensus_label: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, model_weight: np.ndarray, annotator_weight: np.ndarray, consensus_quality_score: np.ndarray, detailed_label_quality: Optional[np.ndarray]=None, annotator_ids: Optional[pd.Index]=None, quality_method: str='crowdlab') -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns a dictionary containing overall statistics about each annotator.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    model_weight : float\\n        float specifying the model weight used in weighted averages,\\n        None if model weight is not used to compute quality scores\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        None if annotator weights are not used to compute quality scores\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n    detailed_label_quality :\\n        pandas DataFrame containing the detailed label quality scores for all examples and annotators\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    annotator_stats : pd.DataFrame\\n        Overall statistics about each annotator.\\n        For details, see the documentation of :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    '\n    annotator_quality = _get_annotator_quality(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=model_weight, annotator_weight=annotator_weight, detailed_label_quality=detailed_label_quality, quality_method=quality_method)\n    num_examples_labeled = np.sum(~np.isnan(labels_multiannotator), axis=0)\n    agreement_with_consensus = np.zeros(labels_multiannotator.shape[1])\n    for i in range(len(agreement_with_consensus)):\n        labels = labels_multiannotator[:, i]\n        labels_mask = ~np.isnan(labels)\n        agreement_with_consensus[i] = np.mean(labels[labels_mask] == consensus_label[labels_mask])\n    worst_class = _get_annotator_worst_class(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_quality_score=consensus_quality_score)\n    annotator_stats = pd.DataFrame({'annotator_quality': annotator_quality, 'agreement_with_consensus': agreement_with_consensus, 'worst_class': worst_class, 'num_examples_labeled': num_examples_labeled}, index=annotator_ids)\n    return annotator_stats.sort_values(by=['annotator_quality', 'agreement_with_consensus'])",
            "def _get_annotator_stats(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, consensus_label: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, model_weight: np.ndarray, annotator_weight: np.ndarray, consensus_quality_score: np.ndarray, detailed_label_quality: Optional[np.ndarray]=None, annotator_ids: Optional[pd.Index]=None, quality_method: str='crowdlab') -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns a dictionary containing overall statistics about each annotator.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    model_weight : float\\n        float specifying the model weight used in weighted averages,\\n        None if model weight is not used to compute quality scores\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        None if annotator weights are not used to compute quality scores\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n    detailed_label_quality :\\n        pandas DataFrame containing the detailed label quality scores for all examples and annotators\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    annotator_stats : pd.DataFrame\\n        Overall statistics about each annotator.\\n        For details, see the documentation of :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    '\n    annotator_quality = _get_annotator_quality(labels_multiannotator=labels_multiannotator, pred_probs=pred_probs, consensus_label=consensus_label, num_annotations=num_annotations, annotator_agreement=annotator_agreement, model_weight=model_weight, annotator_weight=annotator_weight, detailed_label_quality=detailed_label_quality, quality_method=quality_method)\n    num_examples_labeled = np.sum(~np.isnan(labels_multiannotator), axis=0)\n    agreement_with_consensus = np.zeros(labels_multiannotator.shape[1])\n    for i in range(len(agreement_with_consensus)):\n        labels = labels_multiannotator[:, i]\n        labels_mask = ~np.isnan(labels)\n        agreement_with_consensus[i] = np.mean(labels[labels_mask] == consensus_label[labels_mask])\n    worst_class = _get_annotator_worst_class(labels_multiannotator=labels_multiannotator, consensus_label=consensus_label, consensus_quality_score=consensus_quality_score)\n    annotator_stats = pd.DataFrame({'annotator_quality': annotator_quality, 'agreement_with_consensus': agreement_with_consensus, 'worst_class': worst_class, 'num_examples_labeled': num_examples_labeled}, index=annotator_ids)\n    return annotator_stats.sort_values(by=['annotator_quality', 'agreement_with_consensus'])"
        ]
    },
    {
        "func_name": "_get_annotator_agreement_with_consensus",
        "original": "def _get_annotator_agreement_with_consensus(labels_multiannotator: np.ndarray, consensus_label: np.ndarray) -> np.ndarray:\n    \"\"\"Returns the fractions of annotators that agree with the consensus label per example. Note that the\n    fraction for each example only considers the annotators that labeled that particular example.\n\n    Parameters\n    ----------\n    labels_multiannotator : np.ndarray\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\n        where N is the number of examples and M is the number of annotators.\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    consensus_label : np.ndarray\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\n\n    Returns\n    -------\n    annotator_agreement : np.ndarray\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\n    \"\"\"\n    annotator_agreement = np.zeros(len(labels_multiannotator))\n    for (i, labels) in enumerate(labels_multiannotator):\n        annotator_agreement[i] = np.mean(labels[~np.isnan(labels)] == consensus_label[i])\n    return annotator_agreement",
        "mutated": [
            "def _get_annotator_agreement_with_consensus(labels_multiannotator: np.ndarray, consensus_label: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n    'Returns the fractions of annotators that agree with the consensus label per example. Note that the\\n    fraction for each example only considers the annotators that labeled that particular example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n\\n    Returns\\n    -------\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    '\n    annotator_agreement = np.zeros(len(labels_multiannotator))\n    for (i, labels) in enumerate(labels_multiannotator):\n        annotator_agreement[i] = np.mean(labels[~np.isnan(labels)] == consensus_label[i])\n    return annotator_agreement",
            "def _get_annotator_agreement_with_consensus(labels_multiannotator: np.ndarray, consensus_label: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the fractions of annotators that agree with the consensus label per example. Note that the\\n    fraction for each example only considers the annotators that labeled that particular example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n\\n    Returns\\n    -------\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    '\n    annotator_agreement = np.zeros(len(labels_multiannotator))\n    for (i, labels) in enumerate(labels_multiannotator):\n        annotator_agreement[i] = np.mean(labels[~np.isnan(labels)] == consensus_label[i])\n    return annotator_agreement",
            "def _get_annotator_agreement_with_consensus(labels_multiannotator: np.ndarray, consensus_label: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the fractions of annotators that agree with the consensus label per example. Note that the\\n    fraction for each example only considers the annotators that labeled that particular example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n\\n    Returns\\n    -------\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    '\n    annotator_agreement = np.zeros(len(labels_multiannotator))\n    for (i, labels) in enumerate(labels_multiannotator):\n        annotator_agreement[i] = np.mean(labels[~np.isnan(labels)] == consensus_label[i])\n    return annotator_agreement",
            "def _get_annotator_agreement_with_consensus(labels_multiannotator: np.ndarray, consensus_label: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the fractions of annotators that agree with the consensus label per example. Note that the\\n    fraction for each example only considers the annotators that labeled that particular example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n\\n    Returns\\n    -------\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    '\n    annotator_agreement = np.zeros(len(labels_multiannotator))\n    for (i, labels) in enumerate(labels_multiannotator):\n        annotator_agreement[i] = np.mean(labels[~np.isnan(labels)] == consensus_label[i])\n    return annotator_agreement",
            "def _get_annotator_agreement_with_consensus(labels_multiannotator: np.ndarray, consensus_label: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the fractions of annotators that agree with the consensus label per example. Note that the\\n    fraction for each example only considers the annotators that labeled that particular example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n\\n    Returns\\n    -------\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    '\n    annotator_agreement = np.zeros(len(labels_multiannotator))\n    for (i, labels) in enumerate(labels_multiannotator):\n        annotator_agreement[i] = np.mean(labels[~np.isnan(labels)] == consensus_label[i])\n    return annotator_agreement"
        ]
    },
    {
        "func_name": "_get_annotator_agreement_with_annotators",
        "original": "def _get_annotator_agreement_with_annotators(labels_multiannotator: np.ndarray, num_annotations: np.ndarray, verbose: bool=True) -> np.ndarray:\n    \"\"\"Returns the average agreement of each annotator with other annotators that label the same example.\n\n    Parameters\n    ----------\n    labels_multiannotator : np.ndarray\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\n        where N is the number of examples and M is the number of annotators.\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    consensus_label : np.ndarray\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\n    verbose : bool, default = True\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\n\n    Returns\n    -------\n    annotator_agreement : np.ndarray\n        An array of shape ``(M,)`` where M is the number of annotators, with the agreement of each annotator with other\n        annotators that labeled the same examples.\n    \"\"\"\n    annotator_agreement_with_annotators = np.zeros(labels_multiannotator.shape[1])\n    for i in range(len(annotator_agreement_with_annotators)):\n        annotator_labels = labels_multiannotator[:, i]\n        annotator_labels_mask = ~np.isnan(annotator_labels)\n        annotator_agreement_with_annotators[i] = _get_single_annotator_agreement(labels_multiannotator[annotator_labels_mask], num_annotations[annotator_labels_mask], i)\n    non_overlap_mask = np.isnan(annotator_agreement_with_annotators)\n    if np.sum(non_overlap_mask) > 0:\n        if verbose:\n            print(f\"Annotator(s) {list(np.where(non_overlap_mask)[0])} did not annotate any examples that overlap with other annotators,                 \\nusing the average annotator agreeement among other annotators as this annotator's agreement.\")\n        avg_annotator_agreement = np.mean(annotator_agreement_with_annotators[~non_overlap_mask])\n        annotator_agreement_with_annotators[non_overlap_mask] = avg_annotator_agreement\n    return annotator_agreement_with_annotators",
        "mutated": [
            "def _get_annotator_agreement_with_annotators(labels_multiannotator: np.ndarray, num_annotations: np.ndarray, verbose: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n    'Returns the average agreement of each annotator with other annotators that label the same example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, with the agreement of each annotator with other\\n        annotators that labeled the same examples.\\n    '\n    annotator_agreement_with_annotators = np.zeros(labels_multiannotator.shape[1])\n    for i in range(len(annotator_agreement_with_annotators)):\n        annotator_labels = labels_multiannotator[:, i]\n        annotator_labels_mask = ~np.isnan(annotator_labels)\n        annotator_agreement_with_annotators[i] = _get_single_annotator_agreement(labels_multiannotator[annotator_labels_mask], num_annotations[annotator_labels_mask], i)\n    non_overlap_mask = np.isnan(annotator_agreement_with_annotators)\n    if np.sum(non_overlap_mask) > 0:\n        if verbose:\n            print(f\"Annotator(s) {list(np.where(non_overlap_mask)[0])} did not annotate any examples that overlap with other annotators,                 \\nusing the average annotator agreeement among other annotators as this annotator's agreement.\")\n        avg_annotator_agreement = np.mean(annotator_agreement_with_annotators[~non_overlap_mask])\n        annotator_agreement_with_annotators[non_overlap_mask] = avg_annotator_agreement\n    return annotator_agreement_with_annotators",
            "def _get_annotator_agreement_with_annotators(labels_multiannotator: np.ndarray, num_annotations: np.ndarray, verbose: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the average agreement of each annotator with other annotators that label the same example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, with the agreement of each annotator with other\\n        annotators that labeled the same examples.\\n    '\n    annotator_agreement_with_annotators = np.zeros(labels_multiannotator.shape[1])\n    for i in range(len(annotator_agreement_with_annotators)):\n        annotator_labels = labels_multiannotator[:, i]\n        annotator_labels_mask = ~np.isnan(annotator_labels)\n        annotator_agreement_with_annotators[i] = _get_single_annotator_agreement(labels_multiannotator[annotator_labels_mask], num_annotations[annotator_labels_mask], i)\n    non_overlap_mask = np.isnan(annotator_agreement_with_annotators)\n    if np.sum(non_overlap_mask) > 0:\n        if verbose:\n            print(f\"Annotator(s) {list(np.where(non_overlap_mask)[0])} did not annotate any examples that overlap with other annotators,                 \\nusing the average annotator agreeement among other annotators as this annotator's agreement.\")\n        avg_annotator_agreement = np.mean(annotator_agreement_with_annotators[~non_overlap_mask])\n        annotator_agreement_with_annotators[non_overlap_mask] = avg_annotator_agreement\n    return annotator_agreement_with_annotators",
            "def _get_annotator_agreement_with_annotators(labels_multiannotator: np.ndarray, num_annotations: np.ndarray, verbose: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the average agreement of each annotator with other annotators that label the same example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, with the agreement of each annotator with other\\n        annotators that labeled the same examples.\\n    '\n    annotator_agreement_with_annotators = np.zeros(labels_multiannotator.shape[1])\n    for i in range(len(annotator_agreement_with_annotators)):\n        annotator_labels = labels_multiannotator[:, i]\n        annotator_labels_mask = ~np.isnan(annotator_labels)\n        annotator_agreement_with_annotators[i] = _get_single_annotator_agreement(labels_multiannotator[annotator_labels_mask], num_annotations[annotator_labels_mask], i)\n    non_overlap_mask = np.isnan(annotator_agreement_with_annotators)\n    if np.sum(non_overlap_mask) > 0:\n        if verbose:\n            print(f\"Annotator(s) {list(np.where(non_overlap_mask)[0])} did not annotate any examples that overlap with other annotators,                 \\nusing the average annotator agreeement among other annotators as this annotator's agreement.\")\n        avg_annotator_agreement = np.mean(annotator_agreement_with_annotators[~non_overlap_mask])\n        annotator_agreement_with_annotators[non_overlap_mask] = avg_annotator_agreement\n    return annotator_agreement_with_annotators",
            "def _get_annotator_agreement_with_annotators(labels_multiannotator: np.ndarray, num_annotations: np.ndarray, verbose: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the average agreement of each annotator with other annotators that label the same example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, with the agreement of each annotator with other\\n        annotators that labeled the same examples.\\n    '\n    annotator_agreement_with_annotators = np.zeros(labels_multiannotator.shape[1])\n    for i in range(len(annotator_agreement_with_annotators)):\n        annotator_labels = labels_multiannotator[:, i]\n        annotator_labels_mask = ~np.isnan(annotator_labels)\n        annotator_agreement_with_annotators[i] = _get_single_annotator_agreement(labels_multiannotator[annotator_labels_mask], num_annotations[annotator_labels_mask], i)\n    non_overlap_mask = np.isnan(annotator_agreement_with_annotators)\n    if np.sum(non_overlap_mask) > 0:\n        if verbose:\n            print(f\"Annotator(s) {list(np.where(non_overlap_mask)[0])} did not annotate any examples that overlap with other annotators,                 \\nusing the average annotator agreeement among other annotators as this annotator's agreement.\")\n        avg_annotator_agreement = np.mean(annotator_agreement_with_annotators[~non_overlap_mask])\n        annotator_agreement_with_annotators[non_overlap_mask] = avg_annotator_agreement\n    return annotator_agreement_with_annotators",
            "def _get_annotator_agreement_with_annotators(labels_multiannotator: np.ndarray, num_annotations: np.ndarray, verbose: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the average agreement of each annotator with other annotators that label the same example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, with the agreement of each annotator with other\\n        annotators that labeled the same examples.\\n    '\n    annotator_agreement_with_annotators = np.zeros(labels_multiannotator.shape[1])\n    for i in range(len(annotator_agreement_with_annotators)):\n        annotator_labels = labels_multiannotator[:, i]\n        annotator_labels_mask = ~np.isnan(annotator_labels)\n        annotator_agreement_with_annotators[i] = _get_single_annotator_agreement(labels_multiannotator[annotator_labels_mask], num_annotations[annotator_labels_mask], i)\n    non_overlap_mask = np.isnan(annotator_agreement_with_annotators)\n    if np.sum(non_overlap_mask) > 0:\n        if verbose:\n            print(f\"Annotator(s) {list(np.where(non_overlap_mask)[0])} did not annotate any examples that overlap with other annotators,                 \\nusing the average annotator agreeement among other annotators as this annotator's agreement.\")\n        avg_annotator_agreement = np.mean(annotator_agreement_with_annotators[~non_overlap_mask])\n        annotator_agreement_with_annotators[non_overlap_mask] = avg_annotator_agreement\n    return annotator_agreement_with_annotators"
        ]
    },
    {
        "func_name": "_get_single_annotator_agreement",
        "original": "def _get_single_annotator_agreement(labels_multiannotator: np.ndarray, num_annotations: np.ndarray, annotator_idx: int) -> float:\n    \"\"\"Returns the average agreement of a given annotator other annotators that label the same example.\n\n    Parameters\n    ----------\n    labels_multiannotator : np.ndarray\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\n        where N is the number of examples and M is the number of annotators.\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    num_annotations : np.ndarray\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\n    annotator_idx : int\n        The index of the annotator we want to compute the annotator agreement for.\n\n    Returns\n    -------\n    annotator_agreement : float\n        An float repesenting the agreement of each annotator with other annotators that labeled the same examples.\n    \"\"\"\n    annotator_agreement_per_example = np.zeros(len(labels_multiannotator))\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_subset = labels[~np.isnan(labels)]\n        examples_num_annotators = len(labels_subset)\n        if examples_num_annotators > 1:\n            annotator_agreement_per_example[i] = (np.sum(labels_subset == labels[annotator_idx]) - 1) / (examples_num_annotators - 1)\n    adjusted_num_annotations = num_annotations - 1\n    if np.sum(adjusted_num_annotations) == 0:\n        annotator_agreement = np.NaN\n    else:\n        annotator_agreement = np.average(annotator_agreement_per_example, weights=num_annotations - 1)\n    return annotator_agreement",
        "mutated": [
            "def _get_single_annotator_agreement(labels_multiannotator: np.ndarray, num_annotations: np.ndarray, annotator_idx: int) -> float:\n    if False:\n        i = 10\n    'Returns the average agreement of a given annotator other annotators that label the same example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_idx : int\\n        The index of the annotator we want to compute the annotator agreement for.\\n\\n    Returns\\n    -------\\n    annotator_agreement : float\\n        An float repesenting the agreement of each annotator with other annotators that labeled the same examples.\\n    '\n    annotator_agreement_per_example = np.zeros(len(labels_multiannotator))\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_subset = labels[~np.isnan(labels)]\n        examples_num_annotators = len(labels_subset)\n        if examples_num_annotators > 1:\n            annotator_agreement_per_example[i] = (np.sum(labels_subset == labels[annotator_idx]) - 1) / (examples_num_annotators - 1)\n    adjusted_num_annotations = num_annotations - 1\n    if np.sum(adjusted_num_annotations) == 0:\n        annotator_agreement = np.NaN\n    else:\n        annotator_agreement = np.average(annotator_agreement_per_example, weights=num_annotations - 1)\n    return annotator_agreement",
            "def _get_single_annotator_agreement(labels_multiannotator: np.ndarray, num_annotations: np.ndarray, annotator_idx: int) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the average agreement of a given annotator other annotators that label the same example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_idx : int\\n        The index of the annotator we want to compute the annotator agreement for.\\n\\n    Returns\\n    -------\\n    annotator_agreement : float\\n        An float repesenting the agreement of each annotator with other annotators that labeled the same examples.\\n    '\n    annotator_agreement_per_example = np.zeros(len(labels_multiannotator))\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_subset = labels[~np.isnan(labels)]\n        examples_num_annotators = len(labels_subset)\n        if examples_num_annotators > 1:\n            annotator_agreement_per_example[i] = (np.sum(labels_subset == labels[annotator_idx]) - 1) / (examples_num_annotators - 1)\n    adjusted_num_annotations = num_annotations - 1\n    if np.sum(adjusted_num_annotations) == 0:\n        annotator_agreement = np.NaN\n    else:\n        annotator_agreement = np.average(annotator_agreement_per_example, weights=num_annotations - 1)\n    return annotator_agreement",
            "def _get_single_annotator_agreement(labels_multiannotator: np.ndarray, num_annotations: np.ndarray, annotator_idx: int) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the average agreement of a given annotator other annotators that label the same example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_idx : int\\n        The index of the annotator we want to compute the annotator agreement for.\\n\\n    Returns\\n    -------\\n    annotator_agreement : float\\n        An float repesenting the agreement of each annotator with other annotators that labeled the same examples.\\n    '\n    annotator_agreement_per_example = np.zeros(len(labels_multiannotator))\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_subset = labels[~np.isnan(labels)]\n        examples_num_annotators = len(labels_subset)\n        if examples_num_annotators > 1:\n            annotator_agreement_per_example[i] = (np.sum(labels_subset == labels[annotator_idx]) - 1) / (examples_num_annotators - 1)\n    adjusted_num_annotations = num_annotations - 1\n    if np.sum(adjusted_num_annotations) == 0:\n        annotator_agreement = np.NaN\n    else:\n        annotator_agreement = np.average(annotator_agreement_per_example, weights=num_annotations - 1)\n    return annotator_agreement",
            "def _get_single_annotator_agreement(labels_multiannotator: np.ndarray, num_annotations: np.ndarray, annotator_idx: int) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the average agreement of a given annotator other annotators that label the same example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_idx : int\\n        The index of the annotator we want to compute the annotator agreement for.\\n\\n    Returns\\n    -------\\n    annotator_agreement : float\\n        An float repesenting the agreement of each annotator with other annotators that labeled the same examples.\\n    '\n    annotator_agreement_per_example = np.zeros(len(labels_multiannotator))\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_subset = labels[~np.isnan(labels)]\n        examples_num_annotators = len(labels_subset)\n        if examples_num_annotators > 1:\n            annotator_agreement_per_example[i] = (np.sum(labels_subset == labels[annotator_idx]) - 1) / (examples_num_annotators - 1)\n    adjusted_num_annotations = num_annotations - 1\n    if np.sum(adjusted_num_annotations) == 0:\n        annotator_agreement = np.NaN\n    else:\n        annotator_agreement = np.average(annotator_agreement_per_example, weights=num_annotations - 1)\n    return annotator_agreement",
            "def _get_single_annotator_agreement(labels_multiannotator: np.ndarray, num_annotations: np.ndarray, annotator_idx: int) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the average agreement of a given annotator other annotators that label the same example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_idx : int\\n        The index of the annotator we want to compute the annotator agreement for.\\n\\n    Returns\\n    -------\\n    annotator_agreement : float\\n        An float repesenting the agreement of each annotator with other annotators that labeled the same examples.\\n    '\n    annotator_agreement_per_example = np.zeros(len(labels_multiannotator))\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_subset = labels[~np.isnan(labels)]\n        examples_num_annotators = len(labels_subset)\n        if examples_num_annotators > 1:\n            annotator_agreement_per_example[i] = (np.sum(labels_subset == labels[annotator_idx]) - 1) / (examples_num_annotators - 1)\n    adjusted_num_annotations = num_annotations - 1\n    if np.sum(adjusted_num_annotations) == 0:\n        annotator_agreement = np.NaN\n    else:\n        annotator_agreement = np.average(annotator_agreement_per_example, weights=num_annotations - 1)\n    return annotator_agreement"
        ]
    },
    {
        "func_name": "_get_post_pred_probs_and_weights",
        "original": "def _get_post_pred_probs_and_weights(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, prior_pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', verbose: bool=True) -> Tuple[np.ndarray, Optional[float], Optional[np.ndarray]]:\n    \"\"\"Return the posterior predicted probabilities of each example given a specified quality method.\n\n    Parameters\n    ----------\n    labels_multiannotator : np.ndarray\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\n        where N is the number of examples and M is the number of annotators.\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    consensus_label : np.ndarray\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\n    prior_pred_probs : np.ndarray\n        An array of shape ``(N, K)`` of prior predicted probabilities, ``P(label=k|x)``, usually the out-of-sample predicted probability computed by a model.\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    num_annotations : np.ndarray\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\n    annotator_agreement : np.ndarray\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\n    quality_method : default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\n        Specifies the method used to calculate the quality of the consensus label.\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\n    verbose : default = True\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\n\n    Returns\n    -------\n    post_pred_probs : np.ndarray\n        An array of shape ``(N, K)`` with the posterior predicted probabilities.\n\n    model_weight : float\n        float specifying the model weight used in weighted averages,\n        None if model weight is not used to compute quality scores\n\n    annotator_weight : np.ndarray\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\n        None if annotator weights are not used to compute quality scores\n\n    \"\"\"\n    valid_methods = ['crowdlab', 'agreement']\n    return_model_weight = None\n    return_annotator_weight = None\n    if quality_method == 'crowdlab':\n        num_classes = get_num_classes(pred_probs=prior_pred_probs)\n        consensus_likelihood = np.mean(annotator_agreement[num_annotations != 1])\n        non_consensus_likelihood = (1 - consensus_likelihood) / (num_classes - 1)\n        mask = num_annotations != 1\n        consensus_label_subset = consensus_label[mask]\n        prior_pred_probs_subset = prior_pred_probs[mask]\n        most_likely_class_error = np.clip(np.mean(consensus_label_subset != np.argmax(np.bincount(consensus_label_subset, minlength=num_classes))), a_min=CLIPPING_LOWER_BOUND, a_max=None)\n        annotator_agreement_with_annotators = _get_annotator_agreement_with_annotators(labels_multiannotator, num_annotations, verbose)\n        annotator_error = 1 - annotator_agreement_with_annotators\n        adjusted_annotator_agreement = np.clip(1 - annotator_error / most_likely_class_error, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n        model_error = np.mean(np.argmax(prior_pred_probs_subset, axis=1) != consensus_label_subset)\n        model_weight = np.max([1 - model_error / most_likely_class_error, CLIPPING_LOWER_BOUND]) * np.sqrt(np.mean(num_annotations))\n        post_pred_probs = np.full(prior_pred_probs.shape, np.nan)\n        for (i, labels) in enumerate(labels_multiannotator):\n            labels_mask = ~np.isnan(labels)\n            labels_subset = labels[labels_mask]\n            post_pred_probs[i] = [np.average([prior_pred_probs[i, true_label]] + [consensus_likelihood if annotator_label == true_label else non_consensus_likelihood for annotator_label in labels_subset], weights=np.concatenate(([model_weight], adjusted_annotator_agreement[labels_mask]))) for true_label in range(num_classes)]\n        return_model_weight = model_weight\n        return_annotator_weight = adjusted_annotator_agreement\n    elif quality_method == 'agreement':\n        num_classes = get_num_classes(pred_probs=prior_pred_probs)\n        label_counts = np.full((len(labels_multiannotator), num_classes), np.NaN)\n        for (i, labels) in enumerate(labels_multiannotator):\n            label_counts[i, :] = value_counts(labels[~np.isnan(labels)], num_classes=num_classes)\n        post_pred_probs = label_counts / num_annotations.reshape(-1, 1)\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return (post_pred_probs, return_model_weight, return_annotator_weight)",
        "mutated": [
            "def _get_post_pred_probs_and_weights(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, prior_pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', verbose: bool=True) -> Tuple[np.ndarray, Optional[float], Optional[np.ndarray]]:\n    if False:\n        i = 10\n    'Return the posterior predicted probabilities of each example given a specified quality method.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    prior_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of prior predicted probabilities, ``P(label=k|x)``, usually the out-of-sample predicted probability computed by a model.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    verbose : default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    post_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` with the posterior predicted probabilities.\\n\\n    model_weight : float\\n        float specifying the model weight used in weighted averages,\\n        None if model weight is not used to compute quality scores\\n\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        None if annotator weights are not used to compute quality scores\\n\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    return_model_weight = None\n    return_annotator_weight = None\n    if quality_method == 'crowdlab':\n        num_classes = get_num_classes(pred_probs=prior_pred_probs)\n        consensus_likelihood = np.mean(annotator_agreement[num_annotations != 1])\n        non_consensus_likelihood = (1 - consensus_likelihood) / (num_classes - 1)\n        mask = num_annotations != 1\n        consensus_label_subset = consensus_label[mask]\n        prior_pred_probs_subset = prior_pred_probs[mask]\n        most_likely_class_error = np.clip(np.mean(consensus_label_subset != np.argmax(np.bincount(consensus_label_subset, minlength=num_classes))), a_min=CLIPPING_LOWER_BOUND, a_max=None)\n        annotator_agreement_with_annotators = _get_annotator_agreement_with_annotators(labels_multiannotator, num_annotations, verbose)\n        annotator_error = 1 - annotator_agreement_with_annotators\n        adjusted_annotator_agreement = np.clip(1 - annotator_error / most_likely_class_error, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n        model_error = np.mean(np.argmax(prior_pred_probs_subset, axis=1) != consensus_label_subset)\n        model_weight = np.max([1 - model_error / most_likely_class_error, CLIPPING_LOWER_BOUND]) * np.sqrt(np.mean(num_annotations))\n        post_pred_probs = np.full(prior_pred_probs.shape, np.nan)\n        for (i, labels) in enumerate(labels_multiannotator):\n            labels_mask = ~np.isnan(labels)\n            labels_subset = labels[labels_mask]\n            post_pred_probs[i] = [np.average([prior_pred_probs[i, true_label]] + [consensus_likelihood if annotator_label == true_label else non_consensus_likelihood for annotator_label in labels_subset], weights=np.concatenate(([model_weight], adjusted_annotator_agreement[labels_mask]))) for true_label in range(num_classes)]\n        return_model_weight = model_weight\n        return_annotator_weight = adjusted_annotator_agreement\n    elif quality_method == 'agreement':\n        num_classes = get_num_classes(pred_probs=prior_pred_probs)\n        label_counts = np.full((len(labels_multiannotator), num_classes), np.NaN)\n        for (i, labels) in enumerate(labels_multiannotator):\n            label_counts[i, :] = value_counts(labels[~np.isnan(labels)], num_classes=num_classes)\n        post_pred_probs = label_counts / num_annotations.reshape(-1, 1)\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return (post_pred_probs, return_model_weight, return_annotator_weight)",
            "def _get_post_pred_probs_and_weights(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, prior_pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', verbose: bool=True) -> Tuple[np.ndarray, Optional[float], Optional[np.ndarray]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return the posterior predicted probabilities of each example given a specified quality method.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    prior_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of prior predicted probabilities, ``P(label=k|x)``, usually the out-of-sample predicted probability computed by a model.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    verbose : default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    post_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` with the posterior predicted probabilities.\\n\\n    model_weight : float\\n        float specifying the model weight used in weighted averages,\\n        None if model weight is not used to compute quality scores\\n\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        None if annotator weights are not used to compute quality scores\\n\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    return_model_weight = None\n    return_annotator_weight = None\n    if quality_method == 'crowdlab':\n        num_classes = get_num_classes(pred_probs=prior_pred_probs)\n        consensus_likelihood = np.mean(annotator_agreement[num_annotations != 1])\n        non_consensus_likelihood = (1 - consensus_likelihood) / (num_classes - 1)\n        mask = num_annotations != 1\n        consensus_label_subset = consensus_label[mask]\n        prior_pred_probs_subset = prior_pred_probs[mask]\n        most_likely_class_error = np.clip(np.mean(consensus_label_subset != np.argmax(np.bincount(consensus_label_subset, minlength=num_classes))), a_min=CLIPPING_LOWER_BOUND, a_max=None)\n        annotator_agreement_with_annotators = _get_annotator_agreement_with_annotators(labels_multiannotator, num_annotations, verbose)\n        annotator_error = 1 - annotator_agreement_with_annotators\n        adjusted_annotator_agreement = np.clip(1 - annotator_error / most_likely_class_error, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n        model_error = np.mean(np.argmax(prior_pred_probs_subset, axis=1) != consensus_label_subset)\n        model_weight = np.max([1 - model_error / most_likely_class_error, CLIPPING_LOWER_BOUND]) * np.sqrt(np.mean(num_annotations))\n        post_pred_probs = np.full(prior_pred_probs.shape, np.nan)\n        for (i, labels) in enumerate(labels_multiannotator):\n            labels_mask = ~np.isnan(labels)\n            labels_subset = labels[labels_mask]\n            post_pred_probs[i] = [np.average([prior_pred_probs[i, true_label]] + [consensus_likelihood if annotator_label == true_label else non_consensus_likelihood for annotator_label in labels_subset], weights=np.concatenate(([model_weight], adjusted_annotator_agreement[labels_mask]))) for true_label in range(num_classes)]\n        return_model_weight = model_weight\n        return_annotator_weight = adjusted_annotator_agreement\n    elif quality_method == 'agreement':\n        num_classes = get_num_classes(pred_probs=prior_pred_probs)\n        label_counts = np.full((len(labels_multiannotator), num_classes), np.NaN)\n        for (i, labels) in enumerate(labels_multiannotator):\n            label_counts[i, :] = value_counts(labels[~np.isnan(labels)], num_classes=num_classes)\n        post_pred_probs = label_counts / num_annotations.reshape(-1, 1)\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return (post_pred_probs, return_model_weight, return_annotator_weight)",
            "def _get_post_pred_probs_and_weights(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, prior_pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', verbose: bool=True) -> Tuple[np.ndarray, Optional[float], Optional[np.ndarray]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return the posterior predicted probabilities of each example given a specified quality method.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    prior_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of prior predicted probabilities, ``P(label=k|x)``, usually the out-of-sample predicted probability computed by a model.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    verbose : default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    post_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` with the posterior predicted probabilities.\\n\\n    model_weight : float\\n        float specifying the model weight used in weighted averages,\\n        None if model weight is not used to compute quality scores\\n\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        None if annotator weights are not used to compute quality scores\\n\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    return_model_weight = None\n    return_annotator_weight = None\n    if quality_method == 'crowdlab':\n        num_classes = get_num_classes(pred_probs=prior_pred_probs)\n        consensus_likelihood = np.mean(annotator_agreement[num_annotations != 1])\n        non_consensus_likelihood = (1 - consensus_likelihood) / (num_classes - 1)\n        mask = num_annotations != 1\n        consensus_label_subset = consensus_label[mask]\n        prior_pred_probs_subset = prior_pred_probs[mask]\n        most_likely_class_error = np.clip(np.mean(consensus_label_subset != np.argmax(np.bincount(consensus_label_subset, minlength=num_classes))), a_min=CLIPPING_LOWER_BOUND, a_max=None)\n        annotator_agreement_with_annotators = _get_annotator_agreement_with_annotators(labels_multiannotator, num_annotations, verbose)\n        annotator_error = 1 - annotator_agreement_with_annotators\n        adjusted_annotator_agreement = np.clip(1 - annotator_error / most_likely_class_error, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n        model_error = np.mean(np.argmax(prior_pred_probs_subset, axis=1) != consensus_label_subset)\n        model_weight = np.max([1 - model_error / most_likely_class_error, CLIPPING_LOWER_BOUND]) * np.sqrt(np.mean(num_annotations))\n        post_pred_probs = np.full(prior_pred_probs.shape, np.nan)\n        for (i, labels) in enumerate(labels_multiannotator):\n            labels_mask = ~np.isnan(labels)\n            labels_subset = labels[labels_mask]\n            post_pred_probs[i] = [np.average([prior_pred_probs[i, true_label]] + [consensus_likelihood if annotator_label == true_label else non_consensus_likelihood for annotator_label in labels_subset], weights=np.concatenate(([model_weight], adjusted_annotator_agreement[labels_mask]))) for true_label in range(num_classes)]\n        return_model_weight = model_weight\n        return_annotator_weight = adjusted_annotator_agreement\n    elif quality_method == 'agreement':\n        num_classes = get_num_classes(pred_probs=prior_pred_probs)\n        label_counts = np.full((len(labels_multiannotator), num_classes), np.NaN)\n        for (i, labels) in enumerate(labels_multiannotator):\n            label_counts[i, :] = value_counts(labels[~np.isnan(labels)], num_classes=num_classes)\n        post_pred_probs = label_counts / num_annotations.reshape(-1, 1)\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return (post_pred_probs, return_model_weight, return_annotator_weight)",
            "def _get_post_pred_probs_and_weights(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, prior_pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', verbose: bool=True) -> Tuple[np.ndarray, Optional[float], Optional[np.ndarray]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return the posterior predicted probabilities of each example given a specified quality method.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    prior_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of prior predicted probabilities, ``P(label=k|x)``, usually the out-of-sample predicted probability computed by a model.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    verbose : default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    post_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` with the posterior predicted probabilities.\\n\\n    model_weight : float\\n        float specifying the model weight used in weighted averages,\\n        None if model weight is not used to compute quality scores\\n\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        None if annotator weights are not used to compute quality scores\\n\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    return_model_weight = None\n    return_annotator_weight = None\n    if quality_method == 'crowdlab':\n        num_classes = get_num_classes(pred_probs=prior_pred_probs)\n        consensus_likelihood = np.mean(annotator_agreement[num_annotations != 1])\n        non_consensus_likelihood = (1 - consensus_likelihood) / (num_classes - 1)\n        mask = num_annotations != 1\n        consensus_label_subset = consensus_label[mask]\n        prior_pred_probs_subset = prior_pred_probs[mask]\n        most_likely_class_error = np.clip(np.mean(consensus_label_subset != np.argmax(np.bincount(consensus_label_subset, minlength=num_classes))), a_min=CLIPPING_LOWER_BOUND, a_max=None)\n        annotator_agreement_with_annotators = _get_annotator_agreement_with_annotators(labels_multiannotator, num_annotations, verbose)\n        annotator_error = 1 - annotator_agreement_with_annotators\n        adjusted_annotator_agreement = np.clip(1 - annotator_error / most_likely_class_error, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n        model_error = np.mean(np.argmax(prior_pred_probs_subset, axis=1) != consensus_label_subset)\n        model_weight = np.max([1 - model_error / most_likely_class_error, CLIPPING_LOWER_BOUND]) * np.sqrt(np.mean(num_annotations))\n        post_pred_probs = np.full(prior_pred_probs.shape, np.nan)\n        for (i, labels) in enumerate(labels_multiannotator):\n            labels_mask = ~np.isnan(labels)\n            labels_subset = labels[labels_mask]\n            post_pred_probs[i] = [np.average([prior_pred_probs[i, true_label]] + [consensus_likelihood if annotator_label == true_label else non_consensus_likelihood for annotator_label in labels_subset], weights=np.concatenate(([model_weight], adjusted_annotator_agreement[labels_mask]))) for true_label in range(num_classes)]\n        return_model_weight = model_weight\n        return_annotator_weight = adjusted_annotator_agreement\n    elif quality_method == 'agreement':\n        num_classes = get_num_classes(pred_probs=prior_pred_probs)\n        label_counts = np.full((len(labels_multiannotator), num_classes), np.NaN)\n        for (i, labels) in enumerate(labels_multiannotator):\n            label_counts[i, :] = value_counts(labels[~np.isnan(labels)], num_classes=num_classes)\n        post_pred_probs = label_counts / num_annotations.reshape(-1, 1)\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return (post_pred_probs, return_model_weight, return_annotator_weight)",
            "def _get_post_pred_probs_and_weights(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, prior_pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', verbose: bool=True) -> Tuple[np.ndarray, Optional[float], Optional[np.ndarray]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return the posterior predicted probabilities of each example given a specified quality method.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    prior_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of prior predicted probabilities, ``P(label=k|x)``, usually the out-of-sample predicted probability computed by a model.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    verbose : default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    post_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` with the posterior predicted probabilities.\\n\\n    model_weight : float\\n        float specifying the model weight used in weighted averages,\\n        None if model weight is not used to compute quality scores\\n\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        None if annotator weights are not used to compute quality scores\\n\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    return_model_weight = None\n    return_annotator_weight = None\n    if quality_method == 'crowdlab':\n        num_classes = get_num_classes(pred_probs=prior_pred_probs)\n        consensus_likelihood = np.mean(annotator_agreement[num_annotations != 1])\n        non_consensus_likelihood = (1 - consensus_likelihood) / (num_classes - 1)\n        mask = num_annotations != 1\n        consensus_label_subset = consensus_label[mask]\n        prior_pred_probs_subset = prior_pred_probs[mask]\n        most_likely_class_error = np.clip(np.mean(consensus_label_subset != np.argmax(np.bincount(consensus_label_subset, minlength=num_classes))), a_min=CLIPPING_LOWER_BOUND, a_max=None)\n        annotator_agreement_with_annotators = _get_annotator_agreement_with_annotators(labels_multiannotator, num_annotations, verbose)\n        annotator_error = 1 - annotator_agreement_with_annotators\n        adjusted_annotator_agreement = np.clip(1 - annotator_error / most_likely_class_error, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n        model_error = np.mean(np.argmax(prior_pred_probs_subset, axis=1) != consensus_label_subset)\n        model_weight = np.max([1 - model_error / most_likely_class_error, CLIPPING_LOWER_BOUND]) * np.sqrt(np.mean(num_annotations))\n        post_pred_probs = np.full(prior_pred_probs.shape, np.nan)\n        for (i, labels) in enumerate(labels_multiannotator):\n            labels_mask = ~np.isnan(labels)\n            labels_subset = labels[labels_mask]\n            post_pred_probs[i] = [np.average([prior_pred_probs[i, true_label]] + [consensus_likelihood if annotator_label == true_label else non_consensus_likelihood for annotator_label in labels_subset], weights=np.concatenate(([model_weight], adjusted_annotator_agreement[labels_mask]))) for true_label in range(num_classes)]\n        return_model_weight = model_weight\n        return_annotator_weight = adjusted_annotator_agreement\n    elif quality_method == 'agreement':\n        num_classes = get_num_classes(pred_probs=prior_pred_probs)\n        label_counts = np.full((len(labels_multiannotator), num_classes), np.NaN)\n        for (i, labels) in enumerate(labels_multiannotator):\n            label_counts[i, :] = value_counts(labels[~np.isnan(labels)], num_classes=num_classes)\n        post_pred_probs = label_counts / num_annotations.reshape(-1, 1)\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return (post_pred_probs, return_model_weight, return_annotator_weight)"
        ]
    },
    {
        "func_name": "_get_post_pred_probs_and_weights_ensemble",
        "original": "def _get_post_pred_probs_and_weights_ensemble(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, prior_pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', verbose: bool=True) -> Tuple[np.ndarray, Any, Any]:\n    \"\"\"Return the posterior predicted class probabilites of each example given a specified quality method and prior predicted class probabilities from an ensemble of multiple classifier models.\n\n    Parameters\n    ----------\n    labels_multiannotator : np.ndarray\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\n        where N is the number of examples and M is the number of annotators.\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    consensus_label : np.ndarray\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\n    prior_pred_probs : np.ndarray\n        An array of shape ``(N, K)`` of prior predicted probabilities, ``P(label=k|x)``, usually the out-of-sample predicted probability computed by a model.\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    num_annotations : np.ndarray\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\n    annotator_agreement : np.ndarray\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\n        Specifies the method used to calculate the quality of the consensus label.\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\n    verbose : bool, default = True\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\n\n    Returns\n    -------\n    post_pred_probs : np.ndarray\n        An array of shape ``(N, K)`` with the posterior predicted probabilities.\n\n    model_weight : np.ndarray\n        An array of shape ``(P,)`` where P is the number of models in this ensemble, specifying the model weight used in weighted averages,\n        ``None`` if model weight is not used to compute quality scores\n\n    annotator_weight : np.ndarray\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\n        ``None`` if annotator weights are not used to compute quality scores\n\n    \"\"\"\n    num_classes = get_num_classes(pred_probs=prior_pred_probs[0])\n    consensus_likelihood = np.mean(annotator_agreement[num_annotations != 1])\n    non_consensus_likelihood = (1 - consensus_likelihood) / (num_classes - 1)\n    mask = num_annotations != 1\n    consensus_label_subset = consensus_label[mask]\n    most_likely_class_error = np.clip(np.mean(consensus_label_subset != np.argmax(np.bincount(consensus_label_subset, minlength=num_classes))), a_min=CLIPPING_LOWER_BOUND, a_max=None)\n    annotator_agreement_with_annotators = _get_annotator_agreement_with_annotators(labels_multiannotator, num_annotations, verbose)\n    annotator_error = 1 - annotator_agreement_with_annotators\n    adjusted_annotator_agreement = np.clip(1 - annotator_error / most_likely_class_error, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n    model_weight = np.full(prior_pred_probs.shape[0], np.nan)\n    for idx in range(prior_pred_probs.shape[0]):\n        prior_pred_probs_subset = prior_pred_probs[idx][mask]\n        model_error = np.mean(np.argmax(prior_pred_probs_subset, axis=1) != consensus_label_subset)\n        model_weight[idx] = np.max([1 - model_error / most_likely_class_error, CLIPPING_LOWER_BOUND]) * np.sqrt(np.mean(num_annotations))\n    post_pred_probs = np.full(prior_pred_probs[0].shape, np.nan)\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_mask = ~np.isnan(labels)\n        labels_subset = labels[labels_mask]\n        post_pred_probs[i] = [np.average([prior_pred_probs[ind][i, true_label] for ind in range(prior_pred_probs.shape[0])] + [consensus_likelihood if annotator_label == true_label else non_consensus_likelihood for annotator_label in labels_subset], weights=np.concatenate((model_weight, adjusted_annotator_agreement[labels_mask]))) for true_label in range(num_classes)]\n    return_model_weight = model_weight\n    return_annotator_weight = adjusted_annotator_agreement\n    return (post_pred_probs, return_model_weight, return_annotator_weight)",
        "mutated": [
            "def _get_post_pred_probs_and_weights_ensemble(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, prior_pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', verbose: bool=True) -> Tuple[np.ndarray, Any, Any]:\n    if False:\n        i = 10\n    'Return the posterior predicted class probabilites of each example given a specified quality method and prior predicted class probabilities from an ensemble of multiple classifier models.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    prior_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of prior predicted probabilities, ``P(label=k|x)``, usually the out-of-sample predicted probability computed by a model.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    post_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` with the posterior predicted probabilities.\\n\\n    model_weight : np.ndarray\\n        An array of shape ``(P,)`` where P is the number of models in this ensemble, specifying the model weight used in weighted averages,\\n        ``None`` if model weight is not used to compute quality scores\\n\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        ``None`` if annotator weights are not used to compute quality scores\\n\\n    '\n    num_classes = get_num_classes(pred_probs=prior_pred_probs[0])\n    consensus_likelihood = np.mean(annotator_agreement[num_annotations != 1])\n    non_consensus_likelihood = (1 - consensus_likelihood) / (num_classes - 1)\n    mask = num_annotations != 1\n    consensus_label_subset = consensus_label[mask]\n    most_likely_class_error = np.clip(np.mean(consensus_label_subset != np.argmax(np.bincount(consensus_label_subset, minlength=num_classes))), a_min=CLIPPING_LOWER_BOUND, a_max=None)\n    annotator_agreement_with_annotators = _get_annotator_agreement_with_annotators(labels_multiannotator, num_annotations, verbose)\n    annotator_error = 1 - annotator_agreement_with_annotators\n    adjusted_annotator_agreement = np.clip(1 - annotator_error / most_likely_class_error, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n    model_weight = np.full(prior_pred_probs.shape[0], np.nan)\n    for idx in range(prior_pred_probs.shape[0]):\n        prior_pred_probs_subset = prior_pred_probs[idx][mask]\n        model_error = np.mean(np.argmax(prior_pred_probs_subset, axis=1) != consensus_label_subset)\n        model_weight[idx] = np.max([1 - model_error / most_likely_class_error, CLIPPING_LOWER_BOUND]) * np.sqrt(np.mean(num_annotations))\n    post_pred_probs = np.full(prior_pred_probs[0].shape, np.nan)\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_mask = ~np.isnan(labels)\n        labels_subset = labels[labels_mask]\n        post_pred_probs[i] = [np.average([prior_pred_probs[ind][i, true_label] for ind in range(prior_pred_probs.shape[0])] + [consensus_likelihood if annotator_label == true_label else non_consensus_likelihood for annotator_label in labels_subset], weights=np.concatenate((model_weight, adjusted_annotator_agreement[labels_mask]))) for true_label in range(num_classes)]\n    return_model_weight = model_weight\n    return_annotator_weight = adjusted_annotator_agreement\n    return (post_pred_probs, return_model_weight, return_annotator_weight)",
            "def _get_post_pred_probs_and_weights_ensemble(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, prior_pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', verbose: bool=True) -> Tuple[np.ndarray, Any, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return the posterior predicted class probabilites of each example given a specified quality method and prior predicted class probabilities from an ensemble of multiple classifier models.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    prior_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of prior predicted probabilities, ``P(label=k|x)``, usually the out-of-sample predicted probability computed by a model.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    post_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` with the posterior predicted probabilities.\\n\\n    model_weight : np.ndarray\\n        An array of shape ``(P,)`` where P is the number of models in this ensemble, specifying the model weight used in weighted averages,\\n        ``None`` if model weight is not used to compute quality scores\\n\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        ``None`` if annotator weights are not used to compute quality scores\\n\\n    '\n    num_classes = get_num_classes(pred_probs=prior_pred_probs[0])\n    consensus_likelihood = np.mean(annotator_agreement[num_annotations != 1])\n    non_consensus_likelihood = (1 - consensus_likelihood) / (num_classes - 1)\n    mask = num_annotations != 1\n    consensus_label_subset = consensus_label[mask]\n    most_likely_class_error = np.clip(np.mean(consensus_label_subset != np.argmax(np.bincount(consensus_label_subset, minlength=num_classes))), a_min=CLIPPING_LOWER_BOUND, a_max=None)\n    annotator_agreement_with_annotators = _get_annotator_agreement_with_annotators(labels_multiannotator, num_annotations, verbose)\n    annotator_error = 1 - annotator_agreement_with_annotators\n    adjusted_annotator_agreement = np.clip(1 - annotator_error / most_likely_class_error, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n    model_weight = np.full(prior_pred_probs.shape[0], np.nan)\n    for idx in range(prior_pred_probs.shape[0]):\n        prior_pred_probs_subset = prior_pred_probs[idx][mask]\n        model_error = np.mean(np.argmax(prior_pred_probs_subset, axis=1) != consensus_label_subset)\n        model_weight[idx] = np.max([1 - model_error / most_likely_class_error, CLIPPING_LOWER_BOUND]) * np.sqrt(np.mean(num_annotations))\n    post_pred_probs = np.full(prior_pred_probs[0].shape, np.nan)\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_mask = ~np.isnan(labels)\n        labels_subset = labels[labels_mask]\n        post_pred_probs[i] = [np.average([prior_pred_probs[ind][i, true_label] for ind in range(prior_pred_probs.shape[0])] + [consensus_likelihood if annotator_label == true_label else non_consensus_likelihood for annotator_label in labels_subset], weights=np.concatenate((model_weight, adjusted_annotator_agreement[labels_mask]))) for true_label in range(num_classes)]\n    return_model_weight = model_weight\n    return_annotator_weight = adjusted_annotator_agreement\n    return (post_pred_probs, return_model_weight, return_annotator_weight)",
            "def _get_post_pred_probs_and_weights_ensemble(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, prior_pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', verbose: bool=True) -> Tuple[np.ndarray, Any, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return the posterior predicted class probabilites of each example given a specified quality method and prior predicted class probabilities from an ensemble of multiple classifier models.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    prior_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of prior predicted probabilities, ``P(label=k|x)``, usually the out-of-sample predicted probability computed by a model.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    post_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` with the posterior predicted probabilities.\\n\\n    model_weight : np.ndarray\\n        An array of shape ``(P,)`` where P is the number of models in this ensemble, specifying the model weight used in weighted averages,\\n        ``None`` if model weight is not used to compute quality scores\\n\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        ``None`` if annotator weights are not used to compute quality scores\\n\\n    '\n    num_classes = get_num_classes(pred_probs=prior_pred_probs[0])\n    consensus_likelihood = np.mean(annotator_agreement[num_annotations != 1])\n    non_consensus_likelihood = (1 - consensus_likelihood) / (num_classes - 1)\n    mask = num_annotations != 1\n    consensus_label_subset = consensus_label[mask]\n    most_likely_class_error = np.clip(np.mean(consensus_label_subset != np.argmax(np.bincount(consensus_label_subset, minlength=num_classes))), a_min=CLIPPING_LOWER_BOUND, a_max=None)\n    annotator_agreement_with_annotators = _get_annotator_agreement_with_annotators(labels_multiannotator, num_annotations, verbose)\n    annotator_error = 1 - annotator_agreement_with_annotators\n    adjusted_annotator_agreement = np.clip(1 - annotator_error / most_likely_class_error, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n    model_weight = np.full(prior_pred_probs.shape[0], np.nan)\n    for idx in range(prior_pred_probs.shape[0]):\n        prior_pred_probs_subset = prior_pred_probs[idx][mask]\n        model_error = np.mean(np.argmax(prior_pred_probs_subset, axis=1) != consensus_label_subset)\n        model_weight[idx] = np.max([1 - model_error / most_likely_class_error, CLIPPING_LOWER_BOUND]) * np.sqrt(np.mean(num_annotations))\n    post_pred_probs = np.full(prior_pred_probs[0].shape, np.nan)\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_mask = ~np.isnan(labels)\n        labels_subset = labels[labels_mask]\n        post_pred_probs[i] = [np.average([prior_pred_probs[ind][i, true_label] for ind in range(prior_pred_probs.shape[0])] + [consensus_likelihood if annotator_label == true_label else non_consensus_likelihood for annotator_label in labels_subset], weights=np.concatenate((model_weight, adjusted_annotator_agreement[labels_mask]))) for true_label in range(num_classes)]\n    return_model_weight = model_weight\n    return_annotator_weight = adjusted_annotator_agreement\n    return (post_pred_probs, return_model_weight, return_annotator_weight)",
            "def _get_post_pred_probs_and_weights_ensemble(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, prior_pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', verbose: bool=True) -> Tuple[np.ndarray, Any, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return the posterior predicted class probabilites of each example given a specified quality method and prior predicted class probabilities from an ensemble of multiple classifier models.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    prior_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of prior predicted probabilities, ``P(label=k|x)``, usually the out-of-sample predicted probability computed by a model.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    post_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` with the posterior predicted probabilities.\\n\\n    model_weight : np.ndarray\\n        An array of shape ``(P,)`` where P is the number of models in this ensemble, specifying the model weight used in weighted averages,\\n        ``None`` if model weight is not used to compute quality scores\\n\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        ``None`` if annotator weights are not used to compute quality scores\\n\\n    '\n    num_classes = get_num_classes(pred_probs=prior_pred_probs[0])\n    consensus_likelihood = np.mean(annotator_agreement[num_annotations != 1])\n    non_consensus_likelihood = (1 - consensus_likelihood) / (num_classes - 1)\n    mask = num_annotations != 1\n    consensus_label_subset = consensus_label[mask]\n    most_likely_class_error = np.clip(np.mean(consensus_label_subset != np.argmax(np.bincount(consensus_label_subset, minlength=num_classes))), a_min=CLIPPING_LOWER_BOUND, a_max=None)\n    annotator_agreement_with_annotators = _get_annotator_agreement_with_annotators(labels_multiannotator, num_annotations, verbose)\n    annotator_error = 1 - annotator_agreement_with_annotators\n    adjusted_annotator_agreement = np.clip(1 - annotator_error / most_likely_class_error, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n    model_weight = np.full(prior_pred_probs.shape[0], np.nan)\n    for idx in range(prior_pred_probs.shape[0]):\n        prior_pred_probs_subset = prior_pred_probs[idx][mask]\n        model_error = np.mean(np.argmax(prior_pred_probs_subset, axis=1) != consensus_label_subset)\n        model_weight[idx] = np.max([1 - model_error / most_likely_class_error, CLIPPING_LOWER_BOUND]) * np.sqrt(np.mean(num_annotations))\n    post_pred_probs = np.full(prior_pred_probs[0].shape, np.nan)\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_mask = ~np.isnan(labels)\n        labels_subset = labels[labels_mask]\n        post_pred_probs[i] = [np.average([prior_pred_probs[ind][i, true_label] for ind in range(prior_pred_probs.shape[0])] + [consensus_likelihood if annotator_label == true_label else non_consensus_likelihood for annotator_label in labels_subset], weights=np.concatenate((model_weight, adjusted_annotator_agreement[labels_mask]))) for true_label in range(num_classes)]\n    return_model_weight = model_weight\n    return_annotator_weight = adjusted_annotator_agreement\n    return (post_pred_probs, return_model_weight, return_annotator_weight)",
            "def _get_post_pred_probs_and_weights_ensemble(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, prior_pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', verbose: bool=True) -> Tuple[np.ndarray, Any, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return the posterior predicted class probabilites of each example given a specified quality method and prior predicted class probabilities from an ensemble of multiple classifier models.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(P, N, K)`` where P is the number of models, consisting of predicted class probabilities from the ensemble models.\\n        Each set of predicted probabilities with shape ``(N, K)`` is in the same format expected by the :py:func:`get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n    prior_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of prior predicted probabilities, ``P(label=k|x)``, usually the out-of-sample predicted probability computed by a model.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n    verbose : bool, default = True\\n        Certain warnings and notes will be printed if ``verbose`` is set to ``True``.\\n\\n    Returns\\n    -------\\n    post_pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` with the posterior predicted probabilities.\\n\\n    model_weight : np.ndarray\\n        An array of shape ``(P,)`` where P is the number of models in this ensemble, specifying the model weight used in weighted averages,\\n        ``None`` if model weight is not used to compute quality scores\\n\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        ``None`` if annotator weights are not used to compute quality scores\\n\\n    '\n    num_classes = get_num_classes(pred_probs=prior_pred_probs[0])\n    consensus_likelihood = np.mean(annotator_agreement[num_annotations != 1])\n    non_consensus_likelihood = (1 - consensus_likelihood) / (num_classes - 1)\n    mask = num_annotations != 1\n    consensus_label_subset = consensus_label[mask]\n    most_likely_class_error = np.clip(np.mean(consensus_label_subset != np.argmax(np.bincount(consensus_label_subset, minlength=num_classes))), a_min=CLIPPING_LOWER_BOUND, a_max=None)\n    annotator_agreement_with_annotators = _get_annotator_agreement_with_annotators(labels_multiannotator, num_annotations, verbose)\n    annotator_error = 1 - annotator_agreement_with_annotators\n    adjusted_annotator_agreement = np.clip(1 - annotator_error / most_likely_class_error, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n    model_weight = np.full(prior_pred_probs.shape[0], np.nan)\n    for idx in range(prior_pred_probs.shape[0]):\n        prior_pred_probs_subset = prior_pred_probs[idx][mask]\n        model_error = np.mean(np.argmax(prior_pred_probs_subset, axis=1) != consensus_label_subset)\n        model_weight[idx] = np.max([1 - model_error / most_likely_class_error, CLIPPING_LOWER_BOUND]) * np.sqrt(np.mean(num_annotations))\n    post_pred_probs = np.full(prior_pred_probs[0].shape, np.nan)\n    for (i, labels) in enumerate(labels_multiannotator):\n        labels_mask = ~np.isnan(labels)\n        labels_subset = labels[labels_mask]\n        post_pred_probs[i] = [np.average([prior_pred_probs[ind][i, true_label] for ind in range(prior_pred_probs.shape[0])] + [consensus_likelihood if annotator_label == true_label else non_consensus_likelihood for annotator_label in labels_subset], weights=np.concatenate((model_weight, adjusted_annotator_agreement[labels_mask]))) for true_label in range(num_classes)]\n    return_model_weight = model_weight\n    return_annotator_weight = adjusted_annotator_agreement\n    return (post_pred_probs, return_model_weight, return_annotator_weight)"
        ]
    },
    {
        "func_name": "_get_consensus_quality_score",
        "original": "def _get_consensus_quality_score(consensus_label: np.ndarray, pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', label_quality_score_kwargs: dict={}) -> np.ndarray:\n    \"\"\"Return scores representing quality of the consensus label for each example.\n\n    Parameters\n    ----------\n    labels_multiannotator : np.ndarray\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\n        where N is the number of examples and M is the number of annotators.\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    consensus_label : np.ndarray\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\n    pred_probs : np.ndarray\n        An array of shape ``(N, K)`` of posterior predicted probabilities, ``P(label=k|x)``.\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    num_annotations : np.ndarray\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\n    annotator_agreement : np.ndarray\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\n        Specifies the method used to calculate the quality of the consensus label.\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\n\n    Returns\n    -------\n    consensus_quality_score : np.ndarray\n        An array of shape ``(N,)`` with the quality score of the consensus.\n    \"\"\"\n    valid_methods = ['crowdlab', 'agreement']\n    if quality_method == 'crowdlab':\n        consensus_quality_score = get_label_quality_scores(consensus_label, pred_probs, **label_quality_score_kwargs)\n    elif quality_method == 'agreement':\n        consensus_quality_score = annotator_agreement\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid consensus quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return consensus_quality_score",
        "mutated": [
            "def _get_consensus_quality_score(consensus_label: np.ndarray, pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', label_quality_score_kwargs: dict={}) -> np.ndarray:\n    if False:\n        i = 10\n    'Return scores representing quality of the consensus label for each example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of posterior predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    if quality_method == 'crowdlab':\n        consensus_quality_score = get_label_quality_scores(consensus_label, pred_probs, **label_quality_score_kwargs)\n    elif quality_method == 'agreement':\n        consensus_quality_score = annotator_agreement\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid consensus quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return consensus_quality_score",
            "def _get_consensus_quality_score(consensus_label: np.ndarray, pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', label_quality_score_kwargs: dict={}) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Return scores representing quality of the consensus label for each example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of posterior predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    if quality_method == 'crowdlab':\n        consensus_quality_score = get_label_quality_scores(consensus_label, pred_probs, **label_quality_score_kwargs)\n    elif quality_method == 'agreement':\n        consensus_quality_score = annotator_agreement\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid consensus quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return consensus_quality_score",
            "def _get_consensus_quality_score(consensus_label: np.ndarray, pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', label_quality_score_kwargs: dict={}) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Return scores representing quality of the consensus label for each example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of posterior predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    if quality_method == 'crowdlab':\n        consensus_quality_score = get_label_quality_scores(consensus_label, pred_probs, **label_quality_score_kwargs)\n    elif quality_method == 'agreement':\n        consensus_quality_score = annotator_agreement\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid consensus quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return consensus_quality_score",
            "def _get_consensus_quality_score(consensus_label: np.ndarray, pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', label_quality_score_kwargs: dict={}) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Return scores representing quality of the consensus label for each example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of posterior predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    if quality_method == 'crowdlab':\n        consensus_quality_score = get_label_quality_scores(consensus_label, pred_probs, **label_quality_score_kwargs)\n    elif quality_method == 'agreement':\n        consensus_quality_score = annotator_agreement\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid consensus quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return consensus_quality_score",
            "def _get_consensus_quality_score(consensus_label: np.ndarray, pred_probs: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, quality_method: str='crowdlab', label_quality_score_kwargs: dict={}) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Return scores representing quality of the consensus label for each example.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of posterior predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the consensus label.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    if quality_method == 'crowdlab':\n        consensus_quality_score = get_label_quality_scores(consensus_label, pred_probs, **label_quality_score_kwargs)\n    elif quality_method == 'agreement':\n        consensus_quality_score = annotator_agreement\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid consensus quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return consensus_quality_score"
        ]
    },
    {
        "func_name": "_get_annotator_label_quality_score",
        "original": "def _get_annotator_label_quality_score(annotator_label: np.ndarray, pred_probs: np.ndarray, label_quality_score_kwargs: dict={}) -> np.ndarray:\n    \"\"\"Returns quality scores for each datapoint.\n    Very similar functionality as ``_get_consensus_quality_score`` with additional support for annotator labels that contain NaN values.\n    For more info about parameters and returns, see the docstring of :py:func:`_get_consensus_quality_score <cleanlab.multiannotator._get_consensus_quality_score>`.\n    \"\"\"\n    mask = ~np.isnan(annotator_label)\n    annotator_label_quality_score_subset = get_label_quality_scores(labels=annotator_label[mask].astype(int), pred_probs=pred_probs[mask], **label_quality_score_kwargs)\n    annotator_label_quality_score = np.full(len(annotator_label), np.nan)\n    annotator_label_quality_score[mask] = annotator_label_quality_score_subset\n    return annotator_label_quality_score",
        "mutated": [
            "def _get_annotator_label_quality_score(annotator_label: np.ndarray, pred_probs: np.ndarray, label_quality_score_kwargs: dict={}) -> np.ndarray:\n    if False:\n        i = 10\n    'Returns quality scores for each datapoint.\\n    Very similar functionality as ``_get_consensus_quality_score`` with additional support for annotator labels that contain NaN values.\\n    For more info about parameters and returns, see the docstring of :py:func:`_get_consensus_quality_score <cleanlab.multiannotator._get_consensus_quality_score>`.\\n    '\n    mask = ~np.isnan(annotator_label)\n    annotator_label_quality_score_subset = get_label_quality_scores(labels=annotator_label[mask].astype(int), pred_probs=pred_probs[mask], **label_quality_score_kwargs)\n    annotator_label_quality_score = np.full(len(annotator_label), np.nan)\n    annotator_label_quality_score[mask] = annotator_label_quality_score_subset\n    return annotator_label_quality_score",
            "def _get_annotator_label_quality_score(annotator_label: np.ndarray, pred_probs: np.ndarray, label_quality_score_kwargs: dict={}) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns quality scores for each datapoint.\\n    Very similar functionality as ``_get_consensus_quality_score`` with additional support for annotator labels that contain NaN values.\\n    For more info about parameters and returns, see the docstring of :py:func:`_get_consensus_quality_score <cleanlab.multiannotator._get_consensus_quality_score>`.\\n    '\n    mask = ~np.isnan(annotator_label)\n    annotator_label_quality_score_subset = get_label_quality_scores(labels=annotator_label[mask].astype(int), pred_probs=pred_probs[mask], **label_quality_score_kwargs)\n    annotator_label_quality_score = np.full(len(annotator_label), np.nan)\n    annotator_label_quality_score[mask] = annotator_label_quality_score_subset\n    return annotator_label_quality_score",
            "def _get_annotator_label_quality_score(annotator_label: np.ndarray, pred_probs: np.ndarray, label_quality_score_kwargs: dict={}) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns quality scores for each datapoint.\\n    Very similar functionality as ``_get_consensus_quality_score`` with additional support for annotator labels that contain NaN values.\\n    For more info about parameters and returns, see the docstring of :py:func:`_get_consensus_quality_score <cleanlab.multiannotator._get_consensus_quality_score>`.\\n    '\n    mask = ~np.isnan(annotator_label)\n    annotator_label_quality_score_subset = get_label_quality_scores(labels=annotator_label[mask].astype(int), pred_probs=pred_probs[mask], **label_quality_score_kwargs)\n    annotator_label_quality_score = np.full(len(annotator_label), np.nan)\n    annotator_label_quality_score[mask] = annotator_label_quality_score_subset\n    return annotator_label_quality_score",
            "def _get_annotator_label_quality_score(annotator_label: np.ndarray, pred_probs: np.ndarray, label_quality_score_kwargs: dict={}) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns quality scores for each datapoint.\\n    Very similar functionality as ``_get_consensus_quality_score`` with additional support for annotator labels that contain NaN values.\\n    For more info about parameters and returns, see the docstring of :py:func:`_get_consensus_quality_score <cleanlab.multiannotator._get_consensus_quality_score>`.\\n    '\n    mask = ~np.isnan(annotator_label)\n    annotator_label_quality_score_subset = get_label_quality_scores(labels=annotator_label[mask].astype(int), pred_probs=pred_probs[mask], **label_quality_score_kwargs)\n    annotator_label_quality_score = np.full(len(annotator_label), np.nan)\n    annotator_label_quality_score[mask] = annotator_label_quality_score_subset\n    return annotator_label_quality_score",
            "def _get_annotator_label_quality_score(annotator_label: np.ndarray, pred_probs: np.ndarray, label_quality_score_kwargs: dict={}) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns quality scores for each datapoint.\\n    Very similar functionality as ``_get_consensus_quality_score`` with additional support for annotator labels that contain NaN values.\\n    For more info about parameters and returns, see the docstring of :py:func:`_get_consensus_quality_score <cleanlab.multiannotator._get_consensus_quality_score>`.\\n    '\n    mask = ~np.isnan(annotator_label)\n    annotator_label_quality_score_subset = get_label_quality_scores(labels=annotator_label[mask].astype(int), pred_probs=pred_probs[mask], **label_quality_score_kwargs)\n    annotator_label_quality_score = np.full(len(annotator_label), np.nan)\n    annotator_label_quality_score[mask] = annotator_label_quality_score_subset\n    return annotator_label_quality_score"
        ]
    },
    {
        "func_name": "_get_annotator_quality",
        "original": "def _get_annotator_quality(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, consensus_label: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, model_weight: np.ndarray, annotator_weight: np.ndarray, detailed_label_quality: Optional[np.ndarray]=None, quality_method: str='crowdlab') -> pd.DataFrame:\n    \"\"\"Returns annotator quality score for each annotator.\n\n    Parameters\n    ----------\n    labels_multiannotator : np.ndarray\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\n        where N is the number of examples and M is the number of annotators.\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    pred_probs : np.ndarray\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    consensus_label : np.ndarray\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\n    num_annotations : np.ndarray\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\n    annotator_agreement : np.ndarray\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\n    model_weight : float\n        An array of shape ``(P,)`` where P is the number of models in this ensemble, specifying the model weight used in weighted averages,\n        ``None`` if model weight is not used to compute quality scores\n    annotator_weight : np.ndarray\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\n        ``None`` if annotator weights are not used to compute quality scores\n    detailed_label_quality :\n        pandas DataFrame containing the detailed label quality scores for all examples and annotators\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\n        Specifies the method used to calculate the quality of the annotators.\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\n\n    Returns\n    -------\n    annotator_quality : np.ndarray\n        Quality scores of a given annotator's labels\n    \"\"\"\n    valid_methods = ['crowdlab', 'agreement']\n    if quality_method == 'crowdlab':\n        if detailed_label_quality is None:\n            annotator_lqs = np.zeros(labels_multiannotator.shape[1])\n            for i in range(len(annotator_lqs)):\n                labels = labels_multiannotator[:, i]\n                labels_mask = ~np.isnan(labels)\n                annotator_lqs[i] = np.mean(get_label_quality_scores(labels[labels_mask].astype(int), pred_probs[labels_mask]))\n        else:\n            annotator_lqs = np.nanmean(detailed_label_quality, axis=0)\n        mask = num_annotations != 1\n        labels_multiannotator_subset = labels_multiannotator[mask]\n        consensus_label_subset = consensus_label[mask]\n        annotator_agreement = np.zeros(labels_multiannotator_subset.shape[1])\n        for i in range(len(annotator_agreement)):\n            labels = labels_multiannotator_subset[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_agreement[i] = np.NaN\n            else:\n                annotator_agreement[i] = np.mean(labels[labels_mask] == consensus_label_subset[labels_mask])\n        avg_num_annotations_frac = np.mean(num_annotations) / len(annotator_weight)\n        annotator_weight_adjusted = np.sum(annotator_weight) * avg_num_annotations_frac\n        w = model_weight / (model_weight + annotator_weight_adjusted)\n        annotator_quality = w * annotator_lqs + (1 - w) * annotator_agreement\n    elif quality_method == 'agreement':\n        mask = num_annotations != 1\n        labels_multiannotator_subset = labels_multiannotator[mask]\n        consensus_label_subset = consensus_label[mask]\n        annotator_quality = np.zeros(labels_multiannotator_subset.shape[1])\n        for i in range(len(annotator_quality)):\n            labels = labels_multiannotator_subset[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_quality[i] = np.NaN\n            else:\n                annotator_quality[i] = np.mean(labels[labels_mask] == consensus_label_subset[labels_mask])\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid annotator quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return annotator_quality",
        "mutated": [
            "def _get_annotator_quality(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, consensus_label: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, model_weight: np.ndarray, annotator_weight: np.ndarray, detailed_label_quality: Optional[np.ndarray]=None, quality_method: str='crowdlab') -> pd.DataFrame:\n    if False:\n        i = 10\n    'Returns annotator quality score for each annotator.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    model_weight : float\\n        An array of shape ``(P,)`` where P is the number of models in this ensemble, specifying the model weight used in weighted averages,\\n        ``None`` if model weight is not used to compute quality scores\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        ``None`` if annotator weights are not used to compute quality scores\\n    detailed_label_quality :\\n        pandas DataFrame containing the detailed label quality scores for all examples and annotators\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the annotators.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    annotator_quality : np.ndarray\\n        Quality scores of a given annotator\\'s labels\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    if quality_method == 'crowdlab':\n        if detailed_label_quality is None:\n            annotator_lqs = np.zeros(labels_multiannotator.shape[1])\n            for i in range(len(annotator_lqs)):\n                labels = labels_multiannotator[:, i]\n                labels_mask = ~np.isnan(labels)\n                annotator_lqs[i] = np.mean(get_label_quality_scores(labels[labels_mask].astype(int), pred_probs[labels_mask]))\n        else:\n            annotator_lqs = np.nanmean(detailed_label_quality, axis=0)\n        mask = num_annotations != 1\n        labels_multiannotator_subset = labels_multiannotator[mask]\n        consensus_label_subset = consensus_label[mask]\n        annotator_agreement = np.zeros(labels_multiannotator_subset.shape[1])\n        for i in range(len(annotator_agreement)):\n            labels = labels_multiannotator_subset[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_agreement[i] = np.NaN\n            else:\n                annotator_agreement[i] = np.mean(labels[labels_mask] == consensus_label_subset[labels_mask])\n        avg_num_annotations_frac = np.mean(num_annotations) / len(annotator_weight)\n        annotator_weight_adjusted = np.sum(annotator_weight) * avg_num_annotations_frac\n        w = model_weight / (model_weight + annotator_weight_adjusted)\n        annotator_quality = w * annotator_lqs + (1 - w) * annotator_agreement\n    elif quality_method == 'agreement':\n        mask = num_annotations != 1\n        labels_multiannotator_subset = labels_multiannotator[mask]\n        consensus_label_subset = consensus_label[mask]\n        annotator_quality = np.zeros(labels_multiannotator_subset.shape[1])\n        for i in range(len(annotator_quality)):\n            labels = labels_multiannotator_subset[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_quality[i] = np.NaN\n            else:\n                annotator_quality[i] = np.mean(labels[labels_mask] == consensus_label_subset[labels_mask])\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid annotator quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return annotator_quality",
            "def _get_annotator_quality(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, consensus_label: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, model_weight: np.ndarray, annotator_weight: np.ndarray, detailed_label_quality: Optional[np.ndarray]=None, quality_method: str='crowdlab') -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns annotator quality score for each annotator.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    model_weight : float\\n        An array of shape ``(P,)`` where P is the number of models in this ensemble, specifying the model weight used in weighted averages,\\n        ``None`` if model weight is not used to compute quality scores\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        ``None`` if annotator weights are not used to compute quality scores\\n    detailed_label_quality :\\n        pandas DataFrame containing the detailed label quality scores for all examples and annotators\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the annotators.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    annotator_quality : np.ndarray\\n        Quality scores of a given annotator\\'s labels\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    if quality_method == 'crowdlab':\n        if detailed_label_quality is None:\n            annotator_lqs = np.zeros(labels_multiannotator.shape[1])\n            for i in range(len(annotator_lqs)):\n                labels = labels_multiannotator[:, i]\n                labels_mask = ~np.isnan(labels)\n                annotator_lqs[i] = np.mean(get_label_quality_scores(labels[labels_mask].astype(int), pred_probs[labels_mask]))\n        else:\n            annotator_lqs = np.nanmean(detailed_label_quality, axis=0)\n        mask = num_annotations != 1\n        labels_multiannotator_subset = labels_multiannotator[mask]\n        consensus_label_subset = consensus_label[mask]\n        annotator_agreement = np.zeros(labels_multiannotator_subset.shape[1])\n        for i in range(len(annotator_agreement)):\n            labels = labels_multiannotator_subset[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_agreement[i] = np.NaN\n            else:\n                annotator_agreement[i] = np.mean(labels[labels_mask] == consensus_label_subset[labels_mask])\n        avg_num_annotations_frac = np.mean(num_annotations) / len(annotator_weight)\n        annotator_weight_adjusted = np.sum(annotator_weight) * avg_num_annotations_frac\n        w = model_weight / (model_weight + annotator_weight_adjusted)\n        annotator_quality = w * annotator_lqs + (1 - w) * annotator_agreement\n    elif quality_method == 'agreement':\n        mask = num_annotations != 1\n        labels_multiannotator_subset = labels_multiannotator[mask]\n        consensus_label_subset = consensus_label[mask]\n        annotator_quality = np.zeros(labels_multiannotator_subset.shape[1])\n        for i in range(len(annotator_quality)):\n            labels = labels_multiannotator_subset[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_quality[i] = np.NaN\n            else:\n                annotator_quality[i] = np.mean(labels[labels_mask] == consensus_label_subset[labels_mask])\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid annotator quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return annotator_quality",
            "def _get_annotator_quality(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, consensus_label: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, model_weight: np.ndarray, annotator_weight: np.ndarray, detailed_label_quality: Optional[np.ndarray]=None, quality_method: str='crowdlab') -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns annotator quality score for each annotator.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    model_weight : float\\n        An array of shape ``(P,)`` where P is the number of models in this ensemble, specifying the model weight used in weighted averages,\\n        ``None`` if model weight is not used to compute quality scores\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        ``None`` if annotator weights are not used to compute quality scores\\n    detailed_label_quality :\\n        pandas DataFrame containing the detailed label quality scores for all examples and annotators\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the annotators.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    annotator_quality : np.ndarray\\n        Quality scores of a given annotator\\'s labels\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    if quality_method == 'crowdlab':\n        if detailed_label_quality is None:\n            annotator_lqs = np.zeros(labels_multiannotator.shape[1])\n            for i in range(len(annotator_lqs)):\n                labels = labels_multiannotator[:, i]\n                labels_mask = ~np.isnan(labels)\n                annotator_lqs[i] = np.mean(get_label_quality_scores(labels[labels_mask].astype(int), pred_probs[labels_mask]))\n        else:\n            annotator_lqs = np.nanmean(detailed_label_quality, axis=0)\n        mask = num_annotations != 1\n        labels_multiannotator_subset = labels_multiannotator[mask]\n        consensus_label_subset = consensus_label[mask]\n        annotator_agreement = np.zeros(labels_multiannotator_subset.shape[1])\n        for i in range(len(annotator_agreement)):\n            labels = labels_multiannotator_subset[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_agreement[i] = np.NaN\n            else:\n                annotator_agreement[i] = np.mean(labels[labels_mask] == consensus_label_subset[labels_mask])\n        avg_num_annotations_frac = np.mean(num_annotations) / len(annotator_weight)\n        annotator_weight_adjusted = np.sum(annotator_weight) * avg_num_annotations_frac\n        w = model_weight / (model_weight + annotator_weight_adjusted)\n        annotator_quality = w * annotator_lqs + (1 - w) * annotator_agreement\n    elif quality_method == 'agreement':\n        mask = num_annotations != 1\n        labels_multiannotator_subset = labels_multiannotator[mask]\n        consensus_label_subset = consensus_label[mask]\n        annotator_quality = np.zeros(labels_multiannotator_subset.shape[1])\n        for i in range(len(annotator_quality)):\n            labels = labels_multiannotator_subset[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_quality[i] = np.NaN\n            else:\n                annotator_quality[i] = np.mean(labels[labels_mask] == consensus_label_subset[labels_mask])\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid annotator quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return annotator_quality",
            "def _get_annotator_quality(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, consensus_label: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, model_weight: np.ndarray, annotator_weight: np.ndarray, detailed_label_quality: Optional[np.ndarray]=None, quality_method: str='crowdlab') -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns annotator quality score for each annotator.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    model_weight : float\\n        An array of shape ``(P,)`` where P is the number of models in this ensemble, specifying the model weight used in weighted averages,\\n        ``None`` if model weight is not used to compute quality scores\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        ``None`` if annotator weights are not used to compute quality scores\\n    detailed_label_quality :\\n        pandas DataFrame containing the detailed label quality scores for all examples and annotators\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the annotators.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    annotator_quality : np.ndarray\\n        Quality scores of a given annotator\\'s labels\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    if quality_method == 'crowdlab':\n        if detailed_label_quality is None:\n            annotator_lqs = np.zeros(labels_multiannotator.shape[1])\n            for i in range(len(annotator_lqs)):\n                labels = labels_multiannotator[:, i]\n                labels_mask = ~np.isnan(labels)\n                annotator_lqs[i] = np.mean(get_label_quality_scores(labels[labels_mask].astype(int), pred_probs[labels_mask]))\n        else:\n            annotator_lqs = np.nanmean(detailed_label_quality, axis=0)\n        mask = num_annotations != 1\n        labels_multiannotator_subset = labels_multiannotator[mask]\n        consensus_label_subset = consensus_label[mask]\n        annotator_agreement = np.zeros(labels_multiannotator_subset.shape[1])\n        for i in range(len(annotator_agreement)):\n            labels = labels_multiannotator_subset[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_agreement[i] = np.NaN\n            else:\n                annotator_agreement[i] = np.mean(labels[labels_mask] == consensus_label_subset[labels_mask])\n        avg_num_annotations_frac = np.mean(num_annotations) / len(annotator_weight)\n        annotator_weight_adjusted = np.sum(annotator_weight) * avg_num_annotations_frac\n        w = model_weight / (model_weight + annotator_weight_adjusted)\n        annotator_quality = w * annotator_lqs + (1 - w) * annotator_agreement\n    elif quality_method == 'agreement':\n        mask = num_annotations != 1\n        labels_multiannotator_subset = labels_multiannotator[mask]\n        consensus_label_subset = consensus_label[mask]\n        annotator_quality = np.zeros(labels_multiannotator_subset.shape[1])\n        for i in range(len(annotator_quality)):\n            labels = labels_multiannotator_subset[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_quality[i] = np.NaN\n            else:\n                annotator_quality[i] = np.mean(labels[labels_mask] == consensus_label_subset[labels_mask])\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid annotator quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return annotator_quality",
            "def _get_annotator_quality(labels_multiannotator: np.ndarray, pred_probs: np.ndarray, consensus_label: np.ndarray, num_annotations: np.ndarray, annotator_agreement: np.ndarray, model_weight: np.ndarray, annotator_weight: np.ndarray, detailed_label_quality: Optional[np.ndarray]=None, quality_method: str='crowdlab') -> pd.DataFrame:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns annotator quality score for each annotator.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D numpy array of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    pred_probs : np.ndarray\\n        An array of shape ``(N, K)`` of model-predicted probabilities, ``P(label=k|x)``.\\n        For details, predicted probabilities in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    num_annotations : np.ndarray\\n        An array of shape ``(N,)`` with the number of annotators that have labeled each example.\\n    annotator_agreement : np.ndarray\\n        An array of shape ``(N,)`` with the fraction of annotators that agree with each consensus label.\\n    model_weight : float\\n        An array of shape ``(P,)`` where P is the number of models in this ensemble, specifying the model weight used in weighted averages,\\n        ``None`` if model weight is not used to compute quality scores\\n    annotator_weight : np.ndarray\\n        An array of shape ``(M,)`` where M is the number of annotators, specifying the annotator weights used in weighted averages,\\n        ``None`` if annotator weights are not used to compute quality scores\\n    detailed_label_quality :\\n        pandas DataFrame containing the detailed label quality scores for all examples and annotators\\n    quality_method : str, default = \"crowdlab\" (Options: [\"crowdlab\", \"agreement\"])\\n        Specifies the method used to calculate the quality of the annotators.\\n        For valid quality methods, view :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`\\n\\n    Returns\\n    -------\\n    annotator_quality : np.ndarray\\n        Quality scores of a given annotator\\'s labels\\n    '\n    valid_methods = ['crowdlab', 'agreement']\n    if quality_method == 'crowdlab':\n        if detailed_label_quality is None:\n            annotator_lqs = np.zeros(labels_multiannotator.shape[1])\n            for i in range(len(annotator_lqs)):\n                labels = labels_multiannotator[:, i]\n                labels_mask = ~np.isnan(labels)\n                annotator_lqs[i] = np.mean(get_label_quality_scores(labels[labels_mask].astype(int), pred_probs[labels_mask]))\n        else:\n            annotator_lqs = np.nanmean(detailed_label_quality, axis=0)\n        mask = num_annotations != 1\n        labels_multiannotator_subset = labels_multiannotator[mask]\n        consensus_label_subset = consensus_label[mask]\n        annotator_agreement = np.zeros(labels_multiannotator_subset.shape[1])\n        for i in range(len(annotator_agreement)):\n            labels = labels_multiannotator_subset[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_agreement[i] = np.NaN\n            else:\n                annotator_agreement[i] = np.mean(labels[labels_mask] == consensus_label_subset[labels_mask])\n        avg_num_annotations_frac = np.mean(num_annotations) / len(annotator_weight)\n        annotator_weight_adjusted = np.sum(annotator_weight) * avg_num_annotations_frac\n        w = model_weight / (model_weight + annotator_weight_adjusted)\n        annotator_quality = w * annotator_lqs + (1 - w) * annotator_agreement\n    elif quality_method == 'agreement':\n        mask = num_annotations != 1\n        labels_multiannotator_subset = labels_multiannotator[mask]\n        consensus_label_subset = consensus_label[mask]\n        annotator_quality = np.zeros(labels_multiannotator_subset.shape[1])\n        for i in range(len(annotator_quality)):\n            labels = labels_multiannotator_subset[:, i]\n            labels_mask = ~np.isnan(labels)\n            if np.sum(labels_mask) == 0:\n                annotator_quality[i] = np.NaN\n            else:\n                annotator_quality[i] = np.mean(labels[labels_mask] == consensus_label_subset[labels_mask])\n    else:\n        raise ValueError(f'\\n            {quality_method} is not a valid annotator quality method!\\n            Please choose a valid quality_method: {valid_methods}\\n            ')\n    return annotator_quality"
        ]
    },
    {
        "func_name": "_get_annotator_worst_class",
        "original": "def _get_annotator_worst_class(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, consensus_quality_score: np.ndarray) -> np.ndarray:\n    \"\"\"Returns the class which each annotator makes the most errors in.\n\n    Parameters\n    ----------\n    labels_multiannotator : np.ndarray\n        2D pandas DataFrame of multiple given labels for each example with shape ``(N, M)``,\n        where N is the number of examples and M is the number of annotators.\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\n    consensus_label : np.ndarray\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\n    consensus_quality_score : np.ndarray\n        An array of shape ``(N,)`` with the quality score of the consensus.\n\n    Returns\n    -------\n    worst_class : np.ndarray\n        The class that is most frequently mislabeled by a given annotator.\n    \"\"\"\n    worst_class = np.apply_along_axis(_get_single_annotator_worst_class, axis=0, arr=labels_multiannotator, consensus_label=consensus_label, consensus_quality_score=consensus_quality_score).astype(int)\n    return worst_class",
        "mutated": [
            "def _get_annotator_worst_class(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, consensus_quality_score: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n    'Returns the class which each annotator makes the most errors in.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D pandas DataFrame of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n\\n    Returns\\n    -------\\n    worst_class : np.ndarray\\n        The class that is most frequently mislabeled by a given annotator.\\n    '\n    worst_class = np.apply_along_axis(_get_single_annotator_worst_class, axis=0, arr=labels_multiannotator, consensus_label=consensus_label, consensus_quality_score=consensus_quality_score).astype(int)\n    return worst_class",
            "def _get_annotator_worst_class(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, consensus_quality_score: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the class which each annotator makes the most errors in.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D pandas DataFrame of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n\\n    Returns\\n    -------\\n    worst_class : np.ndarray\\n        The class that is most frequently mislabeled by a given annotator.\\n    '\n    worst_class = np.apply_along_axis(_get_single_annotator_worst_class, axis=0, arr=labels_multiannotator, consensus_label=consensus_label, consensus_quality_score=consensus_quality_score).astype(int)\n    return worst_class",
            "def _get_annotator_worst_class(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, consensus_quality_score: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the class which each annotator makes the most errors in.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D pandas DataFrame of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n\\n    Returns\\n    -------\\n    worst_class : np.ndarray\\n        The class that is most frequently mislabeled by a given annotator.\\n    '\n    worst_class = np.apply_along_axis(_get_single_annotator_worst_class, axis=0, arr=labels_multiannotator, consensus_label=consensus_label, consensus_quality_score=consensus_quality_score).astype(int)\n    return worst_class",
            "def _get_annotator_worst_class(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, consensus_quality_score: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the class which each annotator makes the most errors in.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D pandas DataFrame of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n\\n    Returns\\n    -------\\n    worst_class : np.ndarray\\n        The class that is most frequently mislabeled by a given annotator.\\n    '\n    worst_class = np.apply_along_axis(_get_single_annotator_worst_class, axis=0, arr=labels_multiannotator, consensus_label=consensus_label, consensus_quality_score=consensus_quality_score).astype(int)\n    return worst_class",
            "def _get_annotator_worst_class(labels_multiannotator: np.ndarray, consensus_label: np.ndarray, consensus_quality_score: np.ndarray) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the class which each annotator makes the most errors in.\\n\\n    Parameters\\n    ----------\\n    labels_multiannotator : np.ndarray\\n        2D pandas DataFrame of multiple given labels for each example with shape ``(N, M)``,\\n        where N is the number of examples and M is the number of annotators.\\n        For more details, labels in the same format expected by the :py:func:`get_label_quality_multiannotator <cleanlab.multiannotator.get_label_quality_multiannotator>`.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n\\n    Returns\\n    -------\\n    worst_class : np.ndarray\\n        The class that is most frequently mislabeled by a given annotator.\\n    '\n    worst_class = np.apply_along_axis(_get_single_annotator_worst_class, axis=0, arr=labels_multiannotator, consensus_label=consensus_label, consensus_quality_score=consensus_quality_score).astype(int)\n    return worst_class"
        ]
    },
    {
        "func_name": "_get_single_annotator_worst_class",
        "original": "def _get_single_annotator_worst_class(labels: np.ndarray, consensus_label: np.ndarray, consensus_quality_score: np.ndarray) -> int:\n    \"\"\"Returns the class a given annotator makes the most errors in.\n\n    Parameters\n    ----------\n    labels : np.ndarray\n        An array of shape ``(N,)`` with the labels from the annotator we want to evaluate.\n    consensus_label : np.ndarray\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\n    consensus_quality_score : np.ndarray\n        An array of shape ``(N,)`` with the quality score of the consensus.\n\n    Returns\n    -------\n    worst_class : int\n        The class that is most frequently mislabeled by the given annotator.\n    \"\"\"\n    labels = pd.Series(labels)\n    labels_mask = pd.notna(labels)\n    class_accuracies = (labels[labels_mask] == consensus_label[labels_mask]).groupby(labels).mean()\n    accuracy_min_idx = class_accuracies[class_accuracies == class_accuracies.min()].index.values\n    if len(accuracy_min_idx) == 1:\n        return accuracy_min_idx[0]\n    class_count = labels[labels_mask].groupby(labels).count()[accuracy_min_idx]\n    count_max_idx = class_count[class_count == class_count.max()].index.values\n    if len(count_max_idx) == 1:\n        return count_max_idx[0]\n    avg_consensus_quality = pd.DataFrame({'annotator_label': labels, 'consensus_quality_score': consensus_quality_score})[labels_mask].groupby('annotator_label').mean()['consensus_quality_score'][count_max_idx]\n    quality_max_idx = avg_consensus_quality[avg_consensus_quality == avg_consensus_quality.max()].index.values\n    return quality_max_idx[0]",
        "mutated": [
            "def _get_single_annotator_worst_class(labels: np.ndarray, consensus_label: np.ndarray, consensus_quality_score: np.ndarray) -> int:\n    if False:\n        i = 10\n    'Returns the class a given annotator makes the most errors in.\\n\\n    Parameters\\n    ----------\\n    labels : np.ndarray\\n        An array of shape ``(N,)`` with the labels from the annotator we want to evaluate.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n\\n    Returns\\n    -------\\n    worst_class : int\\n        The class that is most frequently mislabeled by the given annotator.\\n    '\n    labels = pd.Series(labels)\n    labels_mask = pd.notna(labels)\n    class_accuracies = (labels[labels_mask] == consensus_label[labels_mask]).groupby(labels).mean()\n    accuracy_min_idx = class_accuracies[class_accuracies == class_accuracies.min()].index.values\n    if len(accuracy_min_idx) == 1:\n        return accuracy_min_idx[0]\n    class_count = labels[labels_mask].groupby(labels).count()[accuracy_min_idx]\n    count_max_idx = class_count[class_count == class_count.max()].index.values\n    if len(count_max_idx) == 1:\n        return count_max_idx[0]\n    avg_consensus_quality = pd.DataFrame({'annotator_label': labels, 'consensus_quality_score': consensus_quality_score})[labels_mask].groupby('annotator_label').mean()['consensus_quality_score'][count_max_idx]\n    quality_max_idx = avg_consensus_quality[avg_consensus_quality == avg_consensus_quality.max()].index.values\n    return quality_max_idx[0]",
            "def _get_single_annotator_worst_class(labels: np.ndarray, consensus_label: np.ndarray, consensus_quality_score: np.ndarray) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the class a given annotator makes the most errors in.\\n\\n    Parameters\\n    ----------\\n    labels : np.ndarray\\n        An array of shape ``(N,)`` with the labels from the annotator we want to evaluate.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n\\n    Returns\\n    -------\\n    worst_class : int\\n        The class that is most frequently mislabeled by the given annotator.\\n    '\n    labels = pd.Series(labels)\n    labels_mask = pd.notna(labels)\n    class_accuracies = (labels[labels_mask] == consensus_label[labels_mask]).groupby(labels).mean()\n    accuracy_min_idx = class_accuracies[class_accuracies == class_accuracies.min()].index.values\n    if len(accuracy_min_idx) == 1:\n        return accuracy_min_idx[0]\n    class_count = labels[labels_mask].groupby(labels).count()[accuracy_min_idx]\n    count_max_idx = class_count[class_count == class_count.max()].index.values\n    if len(count_max_idx) == 1:\n        return count_max_idx[0]\n    avg_consensus_quality = pd.DataFrame({'annotator_label': labels, 'consensus_quality_score': consensus_quality_score})[labels_mask].groupby('annotator_label').mean()['consensus_quality_score'][count_max_idx]\n    quality_max_idx = avg_consensus_quality[avg_consensus_quality == avg_consensus_quality.max()].index.values\n    return quality_max_idx[0]",
            "def _get_single_annotator_worst_class(labels: np.ndarray, consensus_label: np.ndarray, consensus_quality_score: np.ndarray) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the class a given annotator makes the most errors in.\\n\\n    Parameters\\n    ----------\\n    labels : np.ndarray\\n        An array of shape ``(N,)`` with the labels from the annotator we want to evaluate.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n\\n    Returns\\n    -------\\n    worst_class : int\\n        The class that is most frequently mislabeled by the given annotator.\\n    '\n    labels = pd.Series(labels)\n    labels_mask = pd.notna(labels)\n    class_accuracies = (labels[labels_mask] == consensus_label[labels_mask]).groupby(labels).mean()\n    accuracy_min_idx = class_accuracies[class_accuracies == class_accuracies.min()].index.values\n    if len(accuracy_min_idx) == 1:\n        return accuracy_min_idx[0]\n    class_count = labels[labels_mask].groupby(labels).count()[accuracy_min_idx]\n    count_max_idx = class_count[class_count == class_count.max()].index.values\n    if len(count_max_idx) == 1:\n        return count_max_idx[0]\n    avg_consensus_quality = pd.DataFrame({'annotator_label': labels, 'consensus_quality_score': consensus_quality_score})[labels_mask].groupby('annotator_label').mean()['consensus_quality_score'][count_max_idx]\n    quality_max_idx = avg_consensus_quality[avg_consensus_quality == avg_consensus_quality.max()].index.values\n    return quality_max_idx[0]",
            "def _get_single_annotator_worst_class(labels: np.ndarray, consensus_label: np.ndarray, consensus_quality_score: np.ndarray) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the class a given annotator makes the most errors in.\\n\\n    Parameters\\n    ----------\\n    labels : np.ndarray\\n        An array of shape ``(N,)`` with the labels from the annotator we want to evaluate.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n\\n    Returns\\n    -------\\n    worst_class : int\\n        The class that is most frequently mislabeled by the given annotator.\\n    '\n    labels = pd.Series(labels)\n    labels_mask = pd.notna(labels)\n    class_accuracies = (labels[labels_mask] == consensus_label[labels_mask]).groupby(labels).mean()\n    accuracy_min_idx = class_accuracies[class_accuracies == class_accuracies.min()].index.values\n    if len(accuracy_min_idx) == 1:\n        return accuracy_min_idx[0]\n    class_count = labels[labels_mask].groupby(labels).count()[accuracy_min_idx]\n    count_max_idx = class_count[class_count == class_count.max()].index.values\n    if len(count_max_idx) == 1:\n        return count_max_idx[0]\n    avg_consensus_quality = pd.DataFrame({'annotator_label': labels, 'consensus_quality_score': consensus_quality_score})[labels_mask].groupby('annotator_label').mean()['consensus_quality_score'][count_max_idx]\n    quality_max_idx = avg_consensus_quality[avg_consensus_quality == avg_consensus_quality.max()].index.values\n    return quality_max_idx[0]",
            "def _get_single_annotator_worst_class(labels: np.ndarray, consensus_label: np.ndarray, consensus_quality_score: np.ndarray) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the class a given annotator makes the most errors in.\\n\\n    Parameters\\n    ----------\\n    labels : np.ndarray\\n        An array of shape ``(N,)`` with the labels from the annotator we want to evaluate.\\n    consensus_label : np.ndarray\\n        An array of shape ``(N,)`` with the consensus labels aggregated from all annotators.\\n    consensus_quality_score : np.ndarray\\n        An array of shape ``(N,)`` with the quality score of the consensus.\\n\\n    Returns\\n    -------\\n    worst_class : int\\n        The class that is most frequently mislabeled by the given annotator.\\n    '\n    labels = pd.Series(labels)\n    labels_mask = pd.notna(labels)\n    class_accuracies = (labels[labels_mask] == consensus_label[labels_mask]).groupby(labels).mean()\n    accuracy_min_idx = class_accuracies[class_accuracies == class_accuracies.min()].index.values\n    if len(accuracy_min_idx) == 1:\n        return accuracy_min_idx[0]\n    class_count = labels[labels_mask].groupby(labels).count()[accuracy_min_idx]\n    count_max_idx = class_count[class_count == class_count.max()].index.values\n    if len(count_max_idx) == 1:\n        return count_max_idx[0]\n    avg_consensus_quality = pd.DataFrame({'annotator_label': labels, 'consensus_quality_score': consensus_quality_score})[labels_mask].groupby('annotator_label').mean()['consensus_quality_score'][count_max_idx]\n    quality_max_idx = avg_consensus_quality[avg_consensus_quality == avg_consensus_quality.max()].index.values\n    return quality_max_idx[0]"
        ]
    }
]