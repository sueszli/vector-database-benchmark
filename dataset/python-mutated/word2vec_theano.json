[
    {
        "func_name": "remove_punctuation_2",
        "original": "def remove_punctuation_2(s):\n    return s.translate(None, string.punctuation)",
        "mutated": [
            "def remove_punctuation_2(s):\n    if False:\n        i = 10\n    return s.translate(None, string.punctuation)",
            "def remove_punctuation_2(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return s.translate(None, string.punctuation)",
            "def remove_punctuation_2(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return s.translate(None, string.punctuation)",
            "def remove_punctuation_2(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return s.translate(None, string.punctuation)",
            "def remove_punctuation_2(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return s.translate(None, string.punctuation)"
        ]
    },
    {
        "func_name": "remove_punctuation_3",
        "original": "def remove_punctuation_3(s):\n    return s.translate(str.maketrans('', '', string.punctuation))",
        "mutated": [
            "def remove_punctuation_3(s):\n    if False:\n        i = 10\n    return s.translate(str.maketrans('', '', string.punctuation))",
            "def remove_punctuation_3(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return s.translate(str.maketrans('', '', string.punctuation))",
            "def remove_punctuation_3(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return s.translate(str.maketrans('', '', string.punctuation))",
            "def remove_punctuation_3(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return s.translate(str.maketrans('', '', string.punctuation))",
            "def remove_punctuation_3(s):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return s.translate(str.maketrans('', '', string.punctuation))"
        ]
    },
    {
        "func_name": "get_wiki",
        "original": "def get_wiki():\n    V = 20000\n    files = glob('../large_files/enwiki*.txt')\n    all_word_counts = {}\n    for f in files:\n        for line in open(f):\n            if line and line[0] not in '[*-|=\\\\{\\\\}':\n                s = remove_punctuation(line).lower().split()\n                if len(s) > 1:\n                    for word in s:\n                        if word not in all_word_counts:\n                            all_word_counts[word] = 0\n                        all_word_counts[word] += 1\n    print('finished counting')\n    V = min(V, len(all_word_counts))\n    all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n    top_words = [w for (w, count) in all_word_counts[:V - 1]] + ['<UNK>']\n    word2idx = {w: i for (i, w) in enumerate(top_words)}\n    unk = word2idx['<UNK>']\n    sents = []\n    for f in files:\n        for line in open(f):\n            if line and line[0] not in '[*-|=\\\\{\\\\}':\n                s = remove_punctuation(line).lower().split()\n                if len(s) > 1:\n                    sent = [word2idx[w] if w in word2idx else unk for w in s]\n                    sents.append(sent)\n    return (sents, word2idx)",
        "mutated": [
            "def get_wiki():\n    if False:\n        i = 10\n    V = 20000\n    files = glob('../large_files/enwiki*.txt')\n    all_word_counts = {}\n    for f in files:\n        for line in open(f):\n            if line and line[0] not in '[*-|=\\\\{\\\\}':\n                s = remove_punctuation(line).lower().split()\n                if len(s) > 1:\n                    for word in s:\n                        if word not in all_word_counts:\n                            all_word_counts[word] = 0\n                        all_word_counts[word] += 1\n    print('finished counting')\n    V = min(V, len(all_word_counts))\n    all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n    top_words = [w for (w, count) in all_word_counts[:V - 1]] + ['<UNK>']\n    word2idx = {w: i for (i, w) in enumerate(top_words)}\n    unk = word2idx['<UNK>']\n    sents = []\n    for f in files:\n        for line in open(f):\n            if line and line[0] not in '[*-|=\\\\{\\\\}':\n                s = remove_punctuation(line).lower().split()\n                if len(s) > 1:\n                    sent = [word2idx[w] if w in word2idx else unk for w in s]\n                    sents.append(sent)\n    return (sents, word2idx)",
            "def get_wiki():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    V = 20000\n    files = glob('../large_files/enwiki*.txt')\n    all_word_counts = {}\n    for f in files:\n        for line in open(f):\n            if line and line[0] not in '[*-|=\\\\{\\\\}':\n                s = remove_punctuation(line).lower().split()\n                if len(s) > 1:\n                    for word in s:\n                        if word not in all_word_counts:\n                            all_word_counts[word] = 0\n                        all_word_counts[word] += 1\n    print('finished counting')\n    V = min(V, len(all_word_counts))\n    all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n    top_words = [w for (w, count) in all_word_counts[:V - 1]] + ['<UNK>']\n    word2idx = {w: i for (i, w) in enumerate(top_words)}\n    unk = word2idx['<UNK>']\n    sents = []\n    for f in files:\n        for line in open(f):\n            if line and line[0] not in '[*-|=\\\\{\\\\}':\n                s = remove_punctuation(line).lower().split()\n                if len(s) > 1:\n                    sent = [word2idx[w] if w in word2idx else unk for w in s]\n                    sents.append(sent)\n    return (sents, word2idx)",
            "def get_wiki():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    V = 20000\n    files = glob('../large_files/enwiki*.txt')\n    all_word_counts = {}\n    for f in files:\n        for line in open(f):\n            if line and line[0] not in '[*-|=\\\\{\\\\}':\n                s = remove_punctuation(line).lower().split()\n                if len(s) > 1:\n                    for word in s:\n                        if word not in all_word_counts:\n                            all_word_counts[word] = 0\n                        all_word_counts[word] += 1\n    print('finished counting')\n    V = min(V, len(all_word_counts))\n    all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n    top_words = [w for (w, count) in all_word_counts[:V - 1]] + ['<UNK>']\n    word2idx = {w: i for (i, w) in enumerate(top_words)}\n    unk = word2idx['<UNK>']\n    sents = []\n    for f in files:\n        for line in open(f):\n            if line and line[0] not in '[*-|=\\\\{\\\\}':\n                s = remove_punctuation(line).lower().split()\n                if len(s) > 1:\n                    sent = [word2idx[w] if w in word2idx else unk for w in s]\n                    sents.append(sent)\n    return (sents, word2idx)",
            "def get_wiki():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    V = 20000\n    files = glob('../large_files/enwiki*.txt')\n    all_word_counts = {}\n    for f in files:\n        for line in open(f):\n            if line and line[0] not in '[*-|=\\\\{\\\\}':\n                s = remove_punctuation(line).lower().split()\n                if len(s) > 1:\n                    for word in s:\n                        if word not in all_word_counts:\n                            all_word_counts[word] = 0\n                        all_word_counts[word] += 1\n    print('finished counting')\n    V = min(V, len(all_word_counts))\n    all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n    top_words = [w for (w, count) in all_word_counts[:V - 1]] + ['<UNK>']\n    word2idx = {w: i for (i, w) in enumerate(top_words)}\n    unk = word2idx['<UNK>']\n    sents = []\n    for f in files:\n        for line in open(f):\n            if line and line[0] not in '[*-|=\\\\{\\\\}':\n                s = remove_punctuation(line).lower().split()\n                if len(s) > 1:\n                    sent = [word2idx[w] if w in word2idx else unk for w in s]\n                    sents.append(sent)\n    return (sents, word2idx)",
            "def get_wiki():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    V = 20000\n    files = glob('../large_files/enwiki*.txt')\n    all_word_counts = {}\n    for f in files:\n        for line in open(f):\n            if line and line[0] not in '[*-|=\\\\{\\\\}':\n                s = remove_punctuation(line).lower().split()\n                if len(s) > 1:\n                    for word in s:\n                        if word not in all_word_counts:\n                            all_word_counts[word] = 0\n                        all_word_counts[word] += 1\n    print('finished counting')\n    V = min(V, len(all_word_counts))\n    all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n    top_words = [w for (w, count) in all_word_counts[:V - 1]] + ['<UNK>']\n    word2idx = {w: i for (i, w) in enumerate(top_words)}\n    unk = word2idx['<UNK>']\n    sents = []\n    for f in files:\n        for line in open(f):\n            if line and line[0] not in '[*-|=\\\\{\\\\}':\n                s = remove_punctuation(line).lower().split()\n                if len(s) > 1:\n                    sent = [word2idx[w] if w in word2idx else unk for w in s]\n                    sents.append(sent)\n    return (sents, word2idx)"
        ]
    },
    {
        "func_name": "train_model",
        "original": "def train_model(savedir):\n    (sentences, word2idx) = get_wiki()\n    vocab_size = len(word2idx)\n    window_size = 5\n    learning_rate = 0.025 * 128\n    final_learning_rate = 0.0001 * 128\n    num_negatives = 5\n    samples_per_epoch = int(100000.0)\n    epochs = 1\n    D = 50\n    learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n    W = np.random.randn(vocab_size, D) / np.sqrt(D + vocab_size)\n    V = np.random.randn(D, vocab_size) / np.sqrt(D + vocab_size)\n    thW = theano.shared(W)\n    thV = theano.shared(V)\n    th_pos_word = T.ivector('pos_word')\n    th_neg_word = T.ivector('neg_word')\n    th_context = T.ivector('context')\n    th_lr = T.scalar('learning_rate')\n    input_words = T.concatenate([th_pos_word, th_neg_word])\n    W_subset = thW[input_words]\n    dbl_context = T.concatenate([th_context, th_context])\n    V_subset = thV[:, dbl_context]\n    logits = W_subset.dot(V_subset)\n    out = T.nnet.sigmoid(logits)\n    n = th_pos_word.shape[0]\n    th_cost = -T.log(out[:n]).mean() - T.log(1 - out[n:]).mean()\n    gW = T.grad(th_cost, W_subset)\n    gV = T.grad(th_cost, V_subset)\n    W_update = T.inc_subtensor(W_subset, -th_lr * gW)\n    V_update = T.inc_subtensor(V_subset, -th_lr * gV)\n    updates = [(thW, W_update), (thV, V_update)]\n    cost_op = theano.function(inputs=[th_pos_word, th_neg_word, th_context], outputs=th_cost)\n    cost_train_op = theano.function(inputs=[th_pos_word, th_neg_word, th_context, th_lr], outputs=th_cost, updates=updates)\n    p_neg = get_negative_sampling_distribution(sentences, vocab_size)\n    costs = []\n    total_words = sum((len(sentence) for sentence in sentences))\n    print('total number of words in corpus:', total_words)\n    threshold = 1e-05\n    p_drop = 1 - np.sqrt(threshold / p_neg)\n    for epoch in range(epochs):\n        np.random.shuffle(sentences)\n        cost = 0\n        counter = 0\n        inputs = []\n        targets = []\n        negwords = []\n        t0 = datetime.now()\n        for sentence in sentences:\n            sentence = [w for w in sentence if np.random.random() < 1 - p_drop[w]]\n            if len(sentence) < 2:\n                continue\n            randomly_ordered_positions = np.random.choice(len(sentence), size=len(sentence), replace=False)\n            for pos in randomly_ordered_positions:\n                word = sentence[pos]\n                context_words = get_context(pos, sentence, window_size)\n                neg_word = np.random.choice(vocab_size, p=p_neg)\n                n = len(context_words)\n                inputs += [word] * n\n                negwords += [neg_word] * n\n                targets += context_words\n                if len(inputs) >= 128:\n                    c = cost_train_op(inputs, negwords, targets, learning_rate)\n                    cost += c\n                    if np.isnan(c):\n                        print('c is nan:', c)\n                        exit()\n                    inputs = []\n                    targets = []\n                    negwords = []\n            counter += 1\n            if counter % 100 == 0:\n                sys.stdout.write('processed %s / %s, cost: %s\\r' % (counter, len(sentences), c))\n                sys.stdout.flush()\n        dt = datetime.now() - t0\n        print('epoch complete:', epoch, 'cost:', cost, 'dt:', dt)\n        costs.append(cost)\n        learning_rate -= learning_rate_delta\n    plt.plot(costs)\n    plt.show()\n    if not os.path.exists(savedir):\n        os.mkdir(savedir)\n    with open('%s/word2idx.json' % savedir, 'w') as f:\n        json.dump(word2idx, f)\n    (W, V) = (thW.get_value(), thV.get_value())\n    np.savez('%s/weights.npz' % savedir, W, V)\n    return (word2idx, W, V)",
        "mutated": [
            "def train_model(savedir):\n    if False:\n        i = 10\n    (sentences, word2idx) = get_wiki()\n    vocab_size = len(word2idx)\n    window_size = 5\n    learning_rate = 0.025 * 128\n    final_learning_rate = 0.0001 * 128\n    num_negatives = 5\n    samples_per_epoch = int(100000.0)\n    epochs = 1\n    D = 50\n    learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n    W = np.random.randn(vocab_size, D) / np.sqrt(D + vocab_size)\n    V = np.random.randn(D, vocab_size) / np.sqrt(D + vocab_size)\n    thW = theano.shared(W)\n    thV = theano.shared(V)\n    th_pos_word = T.ivector('pos_word')\n    th_neg_word = T.ivector('neg_word')\n    th_context = T.ivector('context')\n    th_lr = T.scalar('learning_rate')\n    input_words = T.concatenate([th_pos_word, th_neg_word])\n    W_subset = thW[input_words]\n    dbl_context = T.concatenate([th_context, th_context])\n    V_subset = thV[:, dbl_context]\n    logits = W_subset.dot(V_subset)\n    out = T.nnet.sigmoid(logits)\n    n = th_pos_word.shape[0]\n    th_cost = -T.log(out[:n]).mean() - T.log(1 - out[n:]).mean()\n    gW = T.grad(th_cost, W_subset)\n    gV = T.grad(th_cost, V_subset)\n    W_update = T.inc_subtensor(W_subset, -th_lr * gW)\n    V_update = T.inc_subtensor(V_subset, -th_lr * gV)\n    updates = [(thW, W_update), (thV, V_update)]\n    cost_op = theano.function(inputs=[th_pos_word, th_neg_word, th_context], outputs=th_cost)\n    cost_train_op = theano.function(inputs=[th_pos_word, th_neg_word, th_context, th_lr], outputs=th_cost, updates=updates)\n    p_neg = get_negative_sampling_distribution(sentences, vocab_size)\n    costs = []\n    total_words = sum((len(sentence) for sentence in sentences))\n    print('total number of words in corpus:', total_words)\n    threshold = 1e-05\n    p_drop = 1 - np.sqrt(threshold / p_neg)\n    for epoch in range(epochs):\n        np.random.shuffle(sentences)\n        cost = 0\n        counter = 0\n        inputs = []\n        targets = []\n        negwords = []\n        t0 = datetime.now()\n        for sentence in sentences:\n            sentence = [w for w in sentence if np.random.random() < 1 - p_drop[w]]\n            if len(sentence) < 2:\n                continue\n            randomly_ordered_positions = np.random.choice(len(sentence), size=len(sentence), replace=False)\n            for pos in randomly_ordered_positions:\n                word = sentence[pos]\n                context_words = get_context(pos, sentence, window_size)\n                neg_word = np.random.choice(vocab_size, p=p_neg)\n                n = len(context_words)\n                inputs += [word] * n\n                negwords += [neg_word] * n\n                targets += context_words\n                if len(inputs) >= 128:\n                    c = cost_train_op(inputs, negwords, targets, learning_rate)\n                    cost += c\n                    if np.isnan(c):\n                        print('c is nan:', c)\n                        exit()\n                    inputs = []\n                    targets = []\n                    negwords = []\n            counter += 1\n            if counter % 100 == 0:\n                sys.stdout.write('processed %s / %s, cost: %s\\r' % (counter, len(sentences), c))\n                sys.stdout.flush()\n        dt = datetime.now() - t0\n        print('epoch complete:', epoch, 'cost:', cost, 'dt:', dt)\n        costs.append(cost)\n        learning_rate -= learning_rate_delta\n    plt.plot(costs)\n    plt.show()\n    if not os.path.exists(savedir):\n        os.mkdir(savedir)\n    with open('%s/word2idx.json' % savedir, 'w') as f:\n        json.dump(word2idx, f)\n    (W, V) = (thW.get_value(), thV.get_value())\n    np.savez('%s/weights.npz' % savedir, W, V)\n    return (word2idx, W, V)",
            "def train_model(savedir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (sentences, word2idx) = get_wiki()\n    vocab_size = len(word2idx)\n    window_size = 5\n    learning_rate = 0.025 * 128\n    final_learning_rate = 0.0001 * 128\n    num_negatives = 5\n    samples_per_epoch = int(100000.0)\n    epochs = 1\n    D = 50\n    learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n    W = np.random.randn(vocab_size, D) / np.sqrt(D + vocab_size)\n    V = np.random.randn(D, vocab_size) / np.sqrt(D + vocab_size)\n    thW = theano.shared(W)\n    thV = theano.shared(V)\n    th_pos_word = T.ivector('pos_word')\n    th_neg_word = T.ivector('neg_word')\n    th_context = T.ivector('context')\n    th_lr = T.scalar('learning_rate')\n    input_words = T.concatenate([th_pos_word, th_neg_word])\n    W_subset = thW[input_words]\n    dbl_context = T.concatenate([th_context, th_context])\n    V_subset = thV[:, dbl_context]\n    logits = W_subset.dot(V_subset)\n    out = T.nnet.sigmoid(logits)\n    n = th_pos_word.shape[0]\n    th_cost = -T.log(out[:n]).mean() - T.log(1 - out[n:]).mean()\n    gW = T.grad(th_cost, W_subset)\n    gV = T.grad(th_cost, V_subset)\n    W_update = T.inc_subtensor(W_subset, -th_lr * gW)\n    V_update = T.inc_subtensor(V_subset, -th_lr * gV)\n    updates = [(thW, W_update), (thV, V_update)]\n    cost_op = theano.function(inputs=[th_pos_word, th_neg_word, th_context], outputs=th_cost)\n    cost_train_op = theano.function(inputs=[th_pos_word, th_neg_word, th_context, th_lr], outputs=th_cost, updates=updates)\n    p_neg = get_negative_sampling_distribution(sentences, vocab_size)\n    costs = []\n    total_words = sum((len(sentence) for sentence in sentences))\n    print('total number of words in corpus:', total_words)\n    threshold = 1e-05\n    p_drop = 1 - np.sqrt(threshold / p_neg)\n    for epoch in range(epochs):\n        np.random.shuffle(sentences)\n        cost = 0\n        counter = 0\n        inputs = []\n        targets = []\n        negwords = []\n        t0 = datetime.now()\n        for sentence in sentences:\n            sentence = [w for w in sentence if np.random.random() < 1 - p_drop[w]]\n            if len(sentence) < 2:\n                continue\n            randomly_ordered_positions = np.random.choice(len(sentence), size=len(sentence), replace=False)\n            for pos in randomly_ordered_positions:\n                word = sentence[pos]\n                context_words = get_context(pos, sentence, window_size)\n                neg_word = np.random.choice(vocab_size, p=p_neg)\n                n = len(context_words)\n                inputs += [word] * n\n                negwords += [neg_word] * n\n                targets += context_words\n                if len(inputs) >= 128:\n                    c = cost_train_op(inputs, negwords, targets, learning_rate)\n                    cost += c\n                    if np.isnan(c):\n                        print('c is nan:', c)\n                        exit()\n                    inputs = []\n                    targets = []\n                    negwords = []\n            counter += 1\n            if counter % 100 == 0:\n                sys.stdout.write('processed %s / %s, cost: %s\\r' % (counter, len(sentences), c))\n                sys.stdout.flush()\n        dt = datetime.now() - t0\n        print('epoch complete:', epoch, 'cost:', cost, 'dt:', dt)\n        costs.append(cost)\n        learning_rate -= learning_rate_delta\n    plt.plot(costs)\n    plt.show()\n    if not os.path.exists(savedir):\n        os.mkdir(savedir)\n    with open('%s/word2idx.json' % savedir, 'w') as f:\n        json.dump(word2idx, f)\n    (W, V) = (thW.get_value(), thV.get_value())\n    np.savez('%s/weights.npz' % savedir, W, V)\n    return (word2idx, W, V)",
            "def train_model(savedir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (sentences, word2idx) = get_wiki()\n    vocab_size = len(word2idx)\n    window_size = 5\n    learning_rate = 0.025 * 128\n    final_learning_rate = 0.0001 * 128\n    num_negatives = 5\n    samples_per_epoch = int(100000.0)\n    epochs = 1\n    D = 50\n    learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n    W = np.random.randn(vocab_size, D) / np.sqrt(D + vocab_size)\n    V = np.random.randn(D, vocab_size) / np.sqrt(D + vocab_size)\n    thW = theano.shared(W)\n    thV = theano.shared(V)\n    th_pos_word = T.ivector('pos_word')\n    th_neg_word = T.ivector('neg_word')\n    th_context = T.ivector('context')\n    th_lr = T.scalar('learning_rate')\n    input_words = T.concatenate([th_pos_word, th_neg_word])\n    W_subset = thW[input_words]\n    dbl_context = T.concatenate([th_context, th_context])\n    V_subset = thV[:, dbl_context]\n    logits = W_subset.dot(V_subset)\n    out = T.nnet.sigmoid(logits)\n    n = th_pos_word.shape[0]\n    th_cost = -T.log(out[:n]).mean() - T.log(1 - out[n:]).mean()\n    gW = T.grad(th_cost, W_subset)\n    gV = T.grad(th_cost, V_subset)\n    W_update = T.inc_subtensor(W_subset, -th_lr * gW)\n    V_update = T.inc_subtensor(V_subset, -th_lr * gV)\n    updates = [(thW, W_update), (thV, V_update)]\n    cost_op = theano.function(inputs=[th_pos_word, th_neg_word, th_context], outputs=th_cost)\n    cost_train_op = theano.function(inputs=[th_pos_word, th_neg_word, th_context, th_lr], outputs=th_cost, updates=updates)\n    p_neg = get_negative_sampling_distribution(sentences, vocab_size)\n    costs = []\n    total_words = sum((len(sentence) for sentence in sentences))\n    print('total number of words in corpus:', total_words)\n    threshold = 1e-05\n    p_drop = 1 - np.sqrt(threshold / p_neg)\n    for epoch in range(epochs):\n        np.random.shuffle(sentences)\n        cost = 0\n        counter = 0\n        inputs = []\n        targets = []\n        negwords = []\n        t0 = datetime.now()\n        for sentence in sentences:\n            sentence = [w for w in sentence if np.random.random() < 1 - p_drop[w]]\n            if len(sentence) < 2:\n                continue\n            randomly_ordered_positions = np.random.choice(len(sentence), size=len(sentence), replace=False)\n            for pos in randomly_ordered_positions:\n                word = sentence[pos]\n                context_words = get_context(pos, sentence, window_size)\n                neg_word = np.random.choice(vocab_size, p=p_neg)\n                n = len(context_words)\n                inputs += [word] * n\n                negwords += [neg_word] * n\n                targets += context_words\n                if len(inputs) >= 128:\n                    c = cost_train_op(inputs, negwords, targets, learning_rate)\n                    cost += c\n                    if np.isnan(c):\n                        print('c is nan:', c)\n                        exit()\n                    inputs = []\n                    targets = []\n                    negwords = []\n            counter += 1\n            if counter % 100 == 0:\n                sys.stdout.write('processed %s / %s, cost: %s\\r' % (counter, len(sentences), c))\n                sys.stdout.flush()\n        dt = datetime.now() - t0\n        print('epoch complete:', epoch, 'cost:', cost, 'dt:', dt)\n        costs.append(cost)\n        learning_rate -= learning_rate_delta\n    plt.plot(costs)\n    plt.show()\n    if not os.path.exists(savedir):\n        os.mkdir(savedir)\n    with open('%s/word2idx.json' % savedir, 'w') as f:\n        json.dump(word2idx, f)\n    (W, V) = (thW.get_value(), thV.get_value())\n    np.savez('%s/weights.npz' % savedir, W, V)\n    return (word2idx, W, V)",
            "def train_model(savedir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (sentences, word2idx) = get_wiki()\n    vocab_size = len(word2idx)\n    window_size = 5\n    learning_rate = 0.025 * 128\n    final_learning_rate = 0.0001 * 128\n    num_negatives = 5\n    samples_per_epoch = int(100000.0)\n    epochs = 1\n    D = 50\n    learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n    W = np.random.randn(vocab_size, D) / np.sqrt(D + vocab_size)\n    V = np.random.randn(D, vocab_size) / np.sqrt(D + vocab_size)\n    thW = theano.shared(W)\n    thV = theano.shared(V)\n    th_pos_word = T.ivector('pos_word')\n    th_neg_word = T.ivector('neg_word')\n    th_context = T.ivector('context')\n    th_lr = T.scalar('learning_rate')\n    input_words = T.concatenate([th_pos_word, th_neg_word])\n    W_subset = thW[input_words]\n    dbl_context = T.concatenate([th_context, th_context])\n    V_subset = thV[:, dbl_context]\n    logits = W_subset.dot(V_subset)\n    out = T.nnet.sigmoid(logits)\n    n = th_pos_word.shape[0]\n    th_cost = -T.log(out[:n]).mean() - T.log(1 - out[n:]).mean()\n    gW = T.grad(th_cost, W_subset)\n    gV = T.grad(th_cost, V_subset)\n    W_update = T.inc_subtensor(W_subset, -th_lr * gW)\n    V_update = T.inc_subtensor(V_subset, -th_lr * gV)\n    updates = [(thW, W_update), (thV, V_update)]\n    cost_op = theano.function(inputs=[th_pos_word, th_neg_word, th_context], outputs=th_cost)\n    cost_train_op = theano.function(inputs=[th_pos_word, th_neg_word, th_context, th_lr], outputs=th_cost, updates=updates)\n    p_neg = get_negative_sampling_distribution(sentences, vocab_size)\n    costs = []\n    total_words = sum((len(sentence) for sentence in sentences))\n    print('total number of words in corpus:', total_words)\n    threshold = 1e-05\n    p_drop = 1 - np.sqrt(threshold / p_neg)\n    for epoch in range(epochs):\n        np.random.shuffle(sentences)\n        cost = 0\n        counter = 0\n        inputs = []\n        targets = []\n        negwords = []\n        t0 = datetime.now()\n        for sentence in sentences:\n            sentence = [w for w in sentence if np.random.random() < 1 - p_drop[w]]\n            if len(sentence) < 2:\n                continue\n            randomly_ordered_positions = np.random.choice(len(sentence), size=len(sentence), replace=False)\n            for pos in randomly_ordered_positions:\n                word = sentence[pos]\n                context_words = get_context(pos, sentence, window_size)\n                neg_word = np.random.choice(vocab_size, p=p_neg)\n                n = len(context_words)\n                inputs += [word] * n\n                negwords += [neg_word] * n\n                targets += context_words\n                if len(inputs) >= 128:\n                    c = cost_train_op(inputs, negwords, targets, learning_rate)\n                    cost += c\n                    if np.isnan(c):\n                        print('c is nan:', c)\n                        exit()\n                    inputs = []\n                    targets = []\n                    negwords = []\n            counter += 1\n            if counter % 100 == 0:\n                sys.stdout.write('processed %s / %s, cost: %s\\r' % (counter, len(sentences), c))\n                sys.stdout.flush()\n        dt = datetime.now() - t0\n        print('epoch complete:', epoch, 'cost:', cost, 'dt:', dt)\n        costs.append(cost)\n        learning_rate -= learning_rate_delta\n    plt.plot(costs)\n    plt.show()\n    if not os.path.exists(savedir):\n        os.mkdir(savedir)\n    with open('%s/word2idx.json' % savedir, 'w') as f:\n        json.dump(word2idx, f)\n    (W, V) = (thW.get_value(), thV.get_value())\n    np.savez('%s/weights.npz' % savedir, W, V)\n    return (word2idx, W, V)",
            "def train_model(savedir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (sentences, word2idx) = get_wiki()\n    vocab_size = len(word2idx)\n    window_size = 5\n    learning_rate = 0.025 * 128\n    final_learning_rate = 0.0001 * 128\n    num_negatives = 5\n    samples_per_epoch = int(100000.0)\n    epochs = 1\n    D = 50\n    learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n    W = np.random.randn(vocab_size, D) / np.sqrt(D + vocab_size)\n    V = np.random.randn(D, vocab_size) / np.sqrt(D + vocab_size)\n    thW = theano.shared(W)\n    thV = theano.shared(V)\n    th_pos_word = T.ivector('pos_word')\n    th_neg_word = T.ivector('neg_word')\n    th_context = T.ivector('context')\n    th_lr = T.scalar('learning_rate')\n    input_words = T.concatenate([th_pos_word, th_neg_word])\n    W_subset = thW[input_words]\n    dbl_context = T.concatenate([th_context, th_context])\n    V_subset = thV[:, dbl_context]\n    logits = W_subset.dot(V_subset)\n    out = T.nnet.sigmoid(logits)\n    n = th_pos_word.shape[0]\n    th_cost = -T.log(out[:n]).mean() - T.log(1 - out[n:]).mean()\n    gW = T.grad(th_cost, W_subset)\n    gV = T.grad(th_cost, V_subset)\n    W_update = T.inc_subtensor(W_subset, -th_lr * gW)\n    V_update = T.inc_subtensor(V_subset, -th_lr * gV)\n    updates = [(thW, W_update), (thV, V_update)]\n    cost_op = theano.function(inputs=[th_pos_word, th_neg_word, th_context], outputs=th_cost)\n    cost_train_op = theano.function(inputs=[th_pos_word, th_neg_word, th_context, th_lr], outputs=th_cost, updates=updates)\n    p_neg = get_negative_sampling_distribution(sentences, vocab_size)\n    costs = []\n    total_words = sum((len(sentence) for sentence in sentences))\n    print('total number of words in corpus:', total_words)\n    threshold = 1e-05\n    p_drop = 1 - np.sqrt(threshold / p_neg)\n    for epoch in range(epochs):\n        np.random.shuffle(sentences)\n        cost = 0\n        counter = 0\n        inputs = []\n        targets = []\n        negwords = []\n        t0 = datetime.now()\n        for sentence in sentences:\n            sentence = [w for w in sentence if np.random.random() < 1 - p_drop[w]]\n            if len(sentence) < 2:\n                continue\n            randomly_ordered_positions = np.random.choice(len(sentence), size=len(sentence), replace=False)\n            for pos in randomly_ordered_positions:\n                word = sentence[pos]\n                context_words = get_context(pos, sentence, window_size)\n                neg_word = np.random.choice(vocab_size, p=p_neg)\n                n = len(context_words)\n                inputs += [word] * n\n                negwords += [neg_word] * n\n                targets += context_words\n                if len(inputs) >= 128:\n                    c = cost_train_op(inputs, negwords, targets, learning_rate)\n                    cost += c\n                    if np.isnan(c):\n                        print('c is nan:', c)\n                        exit()\n                    inputs = []\n                    targets = []\n                    negwords = []\n            counter += 1\n            if counter % 100 == 0:\n                sys.stdout.write('processed %s / %s, cost: %s\\r' % (counter, len(sentences), c))\n                sys.stdout.flush()\n        dt = datetime.now() - t0\n        print('epoch complete:', epoch, 'cost:', cost, 'dt:', dt)\n        costs.append(cost)\n        learning_rate -= learning_rate_delta\n    plt.plot(costs)\n    plt.show()\n    if not os.path.exists(savedir):\n        os.mkdir(savedir)\n    with open('%s/word2idx.json' % savedir, 'w') as f:\n        json.dump(word2idx, f)\n    (W, V) = (thW.get_value(), thV.get_value())\n    np.savez('%s/weights.npz' % savedir, W, V)\n    return (word2idx, W, V)"
        ]
    },
    {
        "func_name": "get_negative_sampling_distribution",
        "original": "def get_negative_sampling_distribution(sentences, vocab_size):\n    word_freq = np.zeros(vocab_size)\n    word_count = sum((len(sentence) for sentence in sentences))\n    for sentence in sentences:\n        for word in sentence:\n            word_freq[word] += 1\n    p_neg = word_freq ** 0.75\n    p_neg = p_neg / p_neg.sum()\n    assert np.all(p_neg > 0)\n    return p_neg",
        "mutated": [
            "def get_negative_sampling_distribution(sentences, vocab_size):\n    if False:\n        i = 10\n    word_freq = np.zeros(vocab_size)\n    word_count = sum((len(sentence) for sentence in sentences))\n    for sentence in sentences:\n        for word in sentence:\n            word_freq[word] += 1\n    p_neg = word_freq ** 0.75\n    p_neg = p_neg / p_neg.sum()\n    assert np.all(p_neg > 0)\n    return p_neg",
            "def get_negative_sampling_distribution(sentences, vocab_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    word_freq = np.zeros(vocab_size)\n    word_count = sum((len(sentence) for sentence in sentences))\n    for sentence in sentences:\n        for word in sentence:\n            word_freq[word] += 1\n    p_neg = word_freq ** 0.75\n    p_neg = p_neg / p_neg.sum()\n    assert np.all(p_neg > 0)\n    return p_neg",
            "def get_negative_sampling_distribution(sentences, vocab_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    word_freq = np.zeros(vocab_size)\n    word_count = sum((len(sentence) for sentence in sentences))\n    for sentence in sentences:\n        for word in sentence:\n            word_freq[word] += 1\n    p_neg = word_freq ** 0.75\n    p_neg = p_neg / p_neg.sum()\n    assert np.all(p_neg > 0)\n    return p_neg",
            "def get_negative_sampling_distribution(sentences, vocab_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    word_freq = np.zeros(vocab_size)\n    word_count = sum((len(sentence) for sentence in sentences))\n    for sentence in sentences:\n        for word in sentence:\n            word_freq[word] += 1\n    p_neg = word_freq ** 0.75\n    p_neg = p_neg / p_neg.sum()\n    assert np.all(p_neg > 0)\n    return p_neg",
            "def get_negative_sampling_distribution(sentences, vocab_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    word_freq = np.zeros(vocab_size)\n    word_count = sum((len(sentence) for sentence in sentences))\n    for sentence in sentences:\n        for word in sentence:\n            word_freq[word] += 1\n    p_neg = word_freq ** 0.75\n    p_neg = p_neg / p_neg.sum()\n    assert np.all(p_neg > 0)\n    return p_neg"
        ]
    },
    {
        "func_name": "get_context",
        "original": "def get_context(pos, sentence, window_size):\n    start = max(0, pos - window_size)\n    end_ = min(len(sentence), pos + window_size)\n    context = []\n    for (ctx_pos, ctx_word_idx) in enumerate(sentence[start:end_], start=start):\n        if ctx_pos != pos:\n            context.append(ctx_word_idx)\n    return context",
        "mutated": [
            "def get_context(pos, sentence, window_size):\n    if False:\n        i = 10\n    start = max(0, pos - window_size)\n    end_ = min(len(sentence), pos + window_size)\n    context = []\n    for (ctx_pos, ctx_word_idx) in enumerate(sentence[start:end_], start=start):\n        if ctx_pos != pos:\n            context.append(ctx_word_idx)\n    return context",
            "def get_context(pos, sentence, window_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    start = max(0, pos - window_size)\n    end_ = min(len(sentence), pos + window_size)\n    context = []\n    for (ctx_pos, ctx_word_idx) in enumerate(sentence[start:end_], start=start):\n        if ctx_pos != pos:\n            context.append(ctx_word_idx)\n    return context",
            "def get_context(pos, sentence, window_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    start = max(0, pos - window_size)\n    end_ = min(len(sentence), pos + window_size)\n    context = []\n    for (ctx_pos, ctx_word_idx) in enumerate(sentence[start:end_], start=start):\n        if ctx_pos != pos:\n            context.append(ctx_word_idx)\n    return context",
            "def get_context(pos, sentence, window_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    start = max(0, pos - window_size)\n    end_ = min(len(sentence), pos + window_size)\n    context = []\n    for (ctx_pos, ctx_word_idx) in enumerate(sentence[start:end_], start=start):\n        if ctx_pos != pos:\n            context.append(ctx_word_idx)\n    return context",
            "def get_context(pos, sentence, window_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    start = max(0, pos - window_size)\n    end_ = min(len(sentence), pos + window_size)\n    context = []\n    for (ctx_pos, ctx_word_idx) in enumerate(sentence[start:end_], start=start):\n        if ctx_pos != pos:\n            context.append(ctx_word_idx)\n    return context"
        ]
    },
    {
        "func_name": "load_model",
        "original": "def load_model(savedir):\n    with open('%s/word2idx.json' % savedir) as f:\n        word2idx = json.load(f)\n    npz = np.load('%s/weights.npz' % savedir)\n    W = npz['arr_0']\n    V = npz['arr_1']\n    return (word2idx, W, V)",
        "mutated": [
            "def load_model(savedir):\n    if False:\n        i = 10\n    with open('%s/word2idx.json' % savedir) as f:\n        word2idx = json.load(f)\n    npz = np.load('%s/weights.npz' % savedir)\n    W = npz['arr_0']\n    V = npz['arr_1']\n    return (word2idx, W, V)",
            "def load_model(savedir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with open('%s/word2idx.json' % savedir) as f:\n        word2idx = json.load(f)\n    npz = np.load('%s/weights.npz' % savedir)\n    W = npz['arr_0']\n    V = npz['arr_1']\n    return (word2idx, W, V)",
            "def load_model(savedir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with open('%s/word2idx.json' % savedir) as f:\n        word2idx = json.load(f)\n    npz = np.load('%s/weights.npz' % savedir)\n    W = npz['arr_0']\n    V = npz['arr_1']\n    return (word2idx, W, V)",
            "def load_model(savedir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with open('%s/word2idx.json' % savedir) as f:\n        word2idx = json.load(f)\n    npz = np.load('%s/weights.npz' % savedir)\n    W = npz['arr_0']\n    V = npz['arr_1']\n    return (word2idx, W, V)",
            "def load_model(savedir):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with open('%s/word2idx.json' % savedir) as f:\n        word2idx = json.load(f)\n    npz = np.load('%s/weights.npz' % savedir)\n    W = npz['arr_0']\n    V = npz['arr_1']\n    return (word2idx, W, V)"
        ]
    },
    {
        "func_name": "analogy",
        "original": "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word, W):\n    (V, D) = W.shape\n    print('testing: %s - %s = %s - %s' % (pos1, neg1, pos2, neg2))\n    for w in (pos1, neg1, pos2, neg2):\n        if w not in word2idx:\n            print('Sorry, %s not in word2idx' % w)\n            return\n    p1 = W[word2idx[pos1]]\n    n1 = W[word2idx[neg1]]\n    p2 = W[word2idx[pos2]]\n    n2 = W[word2idx[neg2]]\n    vec = p1 - n1 + n2\n    distances = pairwise_distances(vec.reshape(1, D), W, metric='cosine').reshape(V)\n    idx = distances.argsort()[:10]\n    best_idx = -1\n    keep_out = [word2idx[w] for w in (pos1, neg1, neg2)]\n    for i in idx:\n        if i not in keep_out:\n            best_idx = i\n            break\n    print('got: %s - %s = %s - %s' % (pos1, neg1, idx2word[best_idx], neg2))\n    print('closest 10:')\n    for i in idx:\n        print(idx2word[i], distances[i])\n    print('dist to %s:' % pos2, cos_dist(p2, vec))",
        "mutated": [
            "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word, W):\n    if False:\n        i = 10\n    (V, D) = W.shape\n    print('testing: %s - %s = %s - %s' % (pos1, neg1, pos2, neg2))\n    for w in (pos1, neg1, pos2, neg2):\n        if w not in word2idx:\n            print('Sorry, %s not in word2idx' % w)\n            return\n    p1 = W[word2idx[pos1]]\n    n1 = W[word2idx[neg1]]\n    p2 = W[word2idx[pos2]]\n    n2 = W[word2idx[neg2]]\n    vec = p1 - n1 + n2\n    distances = pairwise_distances(vec.reshape(1, D), W, metric='cosine').reshape(V)\n    idx = distances.argsort()[:10]\n    best_idx = -1\n    keep_out = [word2idx[w] for w in (pos1, neg1, neg2)]\n    for i in idx:\n        if i not in keep_out:\n            best_idx = i\n            break\n    print('got: %s - %s = %s - %s' % (pos1, neg1, idx2word[best_idx], neg2))\n    print('closest 10:')\n    for i in idx:\n        print(idx2word[i], distances[i])\n    print('dist to %s:' % pos2, cos_dist(p2, vec))",
            "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word, W):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (V, D) = W.shape\n    print('testing: %s - %s = %s - %s' % (pos1, neg1, pos2, neg2))\n    for w in (pos1, neg1, pos2, neg2):\n        if w not in word2idx:\n            print('Sorry, %s not in word2idx' % w)\n            return\n    p1 = W[word2idx[pos1]]\n    n1 = W[word2idx[neg1]]\n    p2 = W[word2idx[pos2]]\n    n2 = W[word2idx[neg2]]\n    vec = p1 - n1 + n2\n    distances = pairwise_distances(vec.reshape(1, D), W, metric='cosine').reshape(V)\n    idx = distances.argsort()[:10]\n    best_idx = -1\n    keep_out = [word2idx[w] for w in (pos1, neg1, neg2)]\n    for i in idx:\n        if i not in keep_out:\n            best_idx = i\n            break\n    print('got: %s - %s = %s - %s' % (pos1, neg1, idx2word[best_idx], neg2))\n    print('closest 10:')\n    for i in idx:\n        print(idx2word[i], distances[i])\n    print('dist to %s:' % pos2, cos_dist(p2, vec))",
            "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word, W):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (V, D) = W.shape\n    print('testing: %s - %s = %s - %s' % (pos1, neg1, pos2, neg2))\n    for w in (pos1, neg1, pos2, neg2):\n        if w not in word2idx:\n            print('Sorry, %s not in word2idx' % w)\n            return\n    p1 = W[word2idx[pos1]]\n    n1 = W[word2idx[neg1]]\n    p2 = W[word2idx[pos2]]\n    n2 = W[word2idx[neg2]]\n    vec = p1 - n1 + n2\n    distances = pairwise_distances(vec.reshape(1, D), W, metric='cosine').reshape(V)\n    idx = distances.argsort()[:10]\n    best_idx = -1\n    keep_out = [word2idx[w] for w in (pos1, neg1, neg2)]\n    for i in idx:\n        if i not in keep_out:\n            best_idx = i\n            break\n    print('got: %s - %s = %s - %s' % (pos1, neg1, idx2word[best_idx], neg2))\n    print('closest 10:')\n    for i in idx:\n        print(idx2word[i], distances[i])\n    print('dist to %s:' % pos2, cos_dist(p2, vec))",
            "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word, W):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (V, D) = W.shape\n    print('testing: %s - %s = %s - %s' % (pos1, neg1, pos2, neg2))\n    for w in (pos1, neg1, pos2, neg2):\n        if w not in word2idx:\n            print('Sorry, %s not in word2idx' % w)\n            return\n    p1 = W[word2idx[pos1]]\n    n1 = W[word2idx[neg1]]\n    p2 = W[word2idx[pos2]]\n    n2 = W[word2idx[neg2]]\n    vec = p1 - n1 + n2\n    distances = pairwise_distances(vec.reshape(1, D), W, metric='cosine').reshape(V)\n    idx = distances.argsort()[:10]\n    best_idx = -1\n    keep_out = [word2idx[w] for w in (pos1, neg1, neg2)]\n    for i in idx:\n        if i not in keep_out:\n            best_idx = i\n            break\n    print('got: %s - %s = %s - %s' % (pos1, neg1, idx2word[best_idx], neg2))\n    print('closest 10:')\n    for i in idx:\n        print(idx2word[i], distances[i])\n    print('dist to %s:' % pos2, cos_dist(p2, vec))",
            "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word, W):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (V, D) = W.shape\n    print('testing: %s - %s = %s - %s' % (pos1, neg1, pos2, neg2))\n    for w in (pos1, neg1, pos2, neg2):\n        if w not in word2idx:\n            print('Sorry, %s not in word2idx' % w)\n            return\n    p1 = W[word2idx[pos1]]\n    n1 = W[word2idx[neg1]]\n    p2 = W[word2idx[pos2]]\n    n2 = W[word2idx[neg2]]\n    vec = p1 - n1 + n2\n    distances = pairwise_distances(vec.reshape(1, D), W, metric='cosine').reshape(V)\n    idx = distances.argsort()[:10]\n    best_idx = -1\n    keep_out = [word2idx[w] for w in (pos1, neg1, neg2)]\n    for i in idx:\n        if i not in keep_out:\n            best_idx = i\n            break\n    print('got: %s - %s = %s - %s' % (pos1, neg1, idx2word[best_idx], neg2))\n    print('closest 10:')\n    for i in idx:\n        print(idx2word[i], distances[i])\n    print('dist to %s:' % pos2, cos_dist(p2, vec))"
        ]
    },
    {
        "func_name": "test_model",
        "original": "def test_model(word2idx, W, V):\n    idx2word = {i: w for (w, i) in word2idx.items()}\n    for We in (W, (W + V.T) / 2):\n        print('**********')\n        analogy('king', 'man', 'queen', 'woman', word2idx, idx2word, We)\n        analogy('king', 'prince', 'queen', 'princess', word2idx, idx2word, We)\n        analogy('miami', 'florida', 'dallas', 'texas', word2idx, idx2word, We)\n        analogy('einstein', 'scientist', 'picasso', 'painter', word2idx, idx2word, We)\n        analogy('japan', 'sushi', 'germany', 'bratwurst', word2idx, idx2word, We)\n        analogy('man', 'woman', 'he', 'she', word2idx, idx2word, We)\n        analogy('man', 'woman', 'uncle', 'aunt', word2idx, idx2word, We)\n        analogy('man', 'woman', 'brother', 'sister', word2idx, idx2word, We)\n        analogy('man', 'woman', 'husband', 'wife', word2idx, idx2word, We)\n        analogy('man', 'woman', 'actor', 'actress', word2idx, idx2word, We)\n        analogy('man', 'woman', 'father', 'mother', word2idx, idx2word, We)\n        analogy('heir', 'heiress', 'prince', 'princess', word2idx, idx2word, We)\n        analogy('nephew', 'niece', 'uncle', 'aunt', word2idx, idx2word, We)\n        analogy('france', 'paris', 'japan', 'tokyo', word2idx, idx2word, We)\n        analogy('france', 'paris', 'china', 'beijing', word2idx, idx2word, We)\n        analogy('february', 'january', 'december', 'november', word2idx, idx2word, We)\n        analogy('france', 'paris', 'germany', 'berlin', word2idx, idx2word, We)\n        analogy('week', 'day', 'year', 'month', word2idx, idx2word, We)\n        analogy('week', 'day', 'hour', 'minute', word2idx, idx2word, We)\n        analogy('france', 'paris', 'italy', 'rome', word2idx, idx2word, We)\n        analogy('paris', 'france', 'rome', 'italy', word2idx, idx2word, We)\n        analogy('france', 'french', 'england', 'english', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'china', 'chinese', word2idx, idx2word, We)\n        analogy('china', 'chinese', 'america', 'american', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'italy', 'italian', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'australia', 'australian', word2idx, idx2word, We)\n        analogy('walk', 'walking', 'swim', 'swimming', word2idx, idx2word, We)",
        "mutated": [
            "def test_model(word2idx, W, V):\n    if False:\n        i = 10\n    idx2word = {i: w for (w, i) in word2idx.items()}\n    for We in (W, (W + V.T) / 2):\n        print('**********')\n        analogy('king', 'man', 'queen', 'woman', word2idx, idx2word, We)\n        analogy('king', 'prince', 'queen', 'princess', word2idx, idx2word, We)\n        analogy('miami', 'florida', 'dallas', 'texas', word2idx, idx2word, We)\n        analogy('einstein', 'scientist', 'picasso', 'painter', word2idx, idx2word, We)\n        analogy('japan', 'sushi', 'germany', 'bratwurst', word2idx, idx2word, We)\n        analogy('man', 'woman', 'he', 'she', word2idx, idx2word, We)\n        analogy('man', 'woman', 'uncle', 'aunt', word2idx, idx2word, We)\n        analogy('man', 'woman', 'brother', 'sister', word2idx, idx2word, We)\n        analogy('man', 'woman', 'husband', 'wife', word2idx, idx2word, We)\n        analogy('man', 'woman', 'actor', 'actress', word2idx, idx2word, We)\n        analogy('man', 'woman', 'father', 'mother', word2idx, idx2word, We)\n        analogy('heir', 'heiress', 'prince', 'princess', word2idx, idx2word, We)\n        analogy('nephew', 'niece', 'uncle', 'aunt', word2idx, idx2word, We)\n        analogy('france', 'paris', 'japan', 'tokyo', word2idx, idx2word, We)\n        analogy('france', 'paris', 'china', 'beijing', word2idx, idx2word, We)\n        analogy('february', 'january', 'december', 'november', word2idx, idx2word, We)\n        analogy('france', 'paris', 'germany', 'berlin', word2idx, idx2word, We)\n        analogy('week', 'day', 'year', 'month', word2idx, idx2word, We)\n        analogy('week', 'day', 'hour', 'minute', word2idx, idx2word, We)\n        analogy('france', 'paris', 'italy', 'rome', word2idx, idx2word, We)\n        analogy('paris', 'france', 'rome', 'italy', word2idx, idx2word, We)\n        analogy('france', 'french', 'england', 'english', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'china', 'chinese', word2idx, idx2word, We)\n        analogy('china', 'chinese', 'america', 'american', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'italy', 'italian', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'australia', 'australian', word2idx, idx2word, We)\n        analogy('walk', 'walking', 'swim', 'swimming', word2idx, idx2word, We)",
            "def test_model(word2idx, W, V):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    idx2word = {i: w for (w, i) in word2idx.items()}\n    for We in (W, (W + V.T) / 2):\n        print('**********')\n        analogy('king', 'man', 'queen', 'woman', word2idx, idx2word, We)\n        analogy('king', 'prince', 'queen', 'princess', word2idx, idx2word, We)\n        analogy('miami', 'florida', 'dallas', 'texas', word2idx, idx2word, We)\n        analogy('einstein', 'scientist', 'picasso', 'painter', word2idx, idx2word, We)\n        analogy('japan', 'sushi', 'germany', 'bratwurst', word2idx, idx2word, We)\n        analogy('man', 'woman', 'he', 'she', word2idx, idx2word, We)\n        analogy('man', 'woman', 'uncle', 'aunt', word2idx, idx2word, We)\n        analogy('man', 'woman', 'brother', 'sister', word2idx, idx2word, We)\n        analogy('man', 'woman', 'husband', 'wife', word2idx, idx2word, We)\n        analogy('man', 'woman', 'actor', 'actress', word2idx, idx2word, We)\n        analogy('man', 'woman', 'father', 'mother', word2idx, idx2word, We)\n        analogy('heir', 'heiress', 'prince', 'princess', word2idx, idx2word, We)\n        analogy('nephew', 'niece', 'uncle', 'aunt', word2idx, idx2word, We)\n        analogy('france', 'paris', 'japan', 'tokyo', word2idx, idx2word, We)\n        analogy('france', 'paris', 'china', 'beijing', word2idx, idx2word, We)\n        analogy('february', 'january', 'december', 'november', word2idx, idx2word, We)\n        analogy('france', 'paris', 'germany', 'berlin', word2idx, idx2word, We)\n        analogy('week', 'day', 'year', 'month', word2idx, idx2word, We)\n        analogy('week', 'day', 'hour', 'minute', word2idx, idx2word, We)\n        analogy('france', 'paris', 'italy', 'rome', word2idx, idx2word, We)\n        analogy('paris', 'france', 'rome', 'italy', word2idx, idx2word, We)\n        analogy('france', 'french', 'england', 'english', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'china', 'chinese', word2idx, idx2word, We)\n        analogy('china', 'chinese', 'america', 'american', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'italy', 'italian', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'australia', 'australian', word2idx, idx2word, We)\n        analogy('walk', 'walking', 'swim', 'swimming', word2idx, idx2word, We)",
            "def test_model(word2idx, W, V):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    idx2word = {i: w for (w, i) in word2idx.items()}\n    for We in (W, (W + V.T) / 2):\n        print('**********')\n        analogy('king', 'man', 'queen', 'woman', word2idx, idx2word, We)\n        analogy('king', 'prince', 'queen', 'princess', word2idx, idx2word, We)\n        analogy('miami', 'florida', 'dallas', 'texas', word2idx, idx2word, We)\n        analogy('einstein', 'scientist', 'picasso', 'painter', word2idx, idx2word, We)\n        analogy('japan', 'sushi', 'germany', 'bratwurst', word2idx, idx2word, We)\n        analogy('man', 'woman', 'he', 'she', word2idx, idx2word, We)\n        analogy('man', 'woman', 'uncle', 'aunt', word2idx, idx2word, We)\n        analogy('man', 'woman', 'brother', 'sister', word2idx, idx2word, We)\n        analogy('man', 'woman', 'husband', 'wife', word2idx, idx2word, We)\n        analogy('man', 'woman', 'actor', 'actress', word2idx, idx2word, We)\n        analogy('man', 'woman', 'father', 'mother', word2idx, idx2word, We)\n        analogy('heir', 'heiress', 'prince', 'princess', word2idx, idx2word, We)\n        analogy('nephew', 'niece', 'uncle', 'aunt', word2idx, idx2word, We)\n        analogy('france', 'paris', 'japan', 'tokyo', word2idx, idx2word, We)\n        analogy('france', 'paris', 'china', 'beijing', word2idx, idx2word, We)\n        analogy('february', 'january', 'december', 'november', word2idx, idx2word, We)\n        analogy('france', 'paris', 'germany', 'berlin', word2idx, idx2word, We)\n        analogy('week', 'day', 'year', 'month', word2idx, idx2word, We)\n        analogy('week', 'day', 'hour', 'minute', word2idx, idx2word, We)\n        analogy('france', 'paris', 'italy', 'rome', word2idx, idx2word, We)\n        analogy('paris', 'france', 'rome', 'italy', word2idx, idx2word, We)\n        analogy('france', 'french', 'england', 'english', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'china', 'chinese', word2idx, idx2word, We)\n        analogy('china', 'chinese', 'america', 'american', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'italy', 'italian', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'australia', 'australian', word2idx, idx2word, We)\n        analogy('walk', 'walking', 'swim', 'swimming', word2idx, idx2word, We)",
            "def test_model(word2idx, W, V):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    idx2word = {i: w for (w, i) in word2idx.items()}\n    for We in (W, (W + V.T) / 2):\n        print('**********')\n        analogy('king', 'man', 'queen', 'woman', word2idx, idx2word, We)\n        analogy('king', 'prince', 'queen', 'princess', word2idx, idx2word, We)\n        analogy('miami', 'florida', 'dallas', 'texas', word2idx, idx2word, We)\n        analogy('einstein', 'scientist', 'picasso', 'painter', word2idx, idx2word, We)\n        analogy('japan', 'sushi', 'germany', 'bratwurst', word2idx, idx2word, We)\n        analogy('man', 'woman', 'he', 'she', word2idx, idx2word, We)\n        analogy('man', 'woman', 'uncle', 'aunt', word2idx, idx2word, We)\n        analogy('man', 'woman', 'brother', 'sister', word2idx, idx2word, We)\n        analogy('man', 'woman', 'husband', 'wife', word2idx, idx2word, We)\n        analogy('man', 'woman', 'actor', 'actress', word2idx, idx2word, We)\n        analogy('man', 'woman', 'father', 'mother', word2idx, idx2word, We)\n        analogy('heir', 'heiress', 'prince', 'princess', word2idx, idx2word, We)\n        analogy('nephew', 'niece', 'uncle', 'aunt', word2idx, idx2word, We)\n        analogy('france', 'paris', 'japan', 'tokyo', word2idx, idx2word, We)\n        analogy('france', 'paris', 'china', 'beijing', word2idx, idx2word, We)\n        analogy('february', 'january', 'december', 'november', word2idx, idx2word, We)\n        analogy('france', 'paris', 'germany', 'berlin', word2idx, idx2word, We)\n        analogy('week', 'day', 'year', 'month', word2idx, idx2word, We)\n        analogy('week', 'day', 'hour', 'minute', word2idx, idx2word, We)\n        analogy('france', 'paris', 'italy', 'rome', word2idx, idx2word, We)\n        analogy('paris', 'france', 'rome', 'italy', word2idx, idx2word, We)\n        analogy('france', 'french', 'england', 'english', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'china', 'chinese', word2idx, idx2word, We)\n        analogy('china', 'chinese', 'america', 'american', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'italy', 'italian', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'australia', 'australian', word2idx, idx2word, We)\n        analogy('walk', 'walking', 'swim', 'swimming', word2idx, idx2word, We)",
            "def test_model(word2idx, W, V):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    idx2word = {i: w for (w, i) in word2idx.items()}\n    for We in (W, (W + V.T) / 2):\n        print('**********')\n        analogy('king', 'man', 'queen', 'woman', word2idx, idx2word, We)\n        analogy('king', 'prince', 'queen', 'princess', word2idx, idx2word, We)\n        analogy('miami', 'florida', 'dallas', 'texas', word2idx, idx2word, We)\n        analogy('einstein', 'scientist', 'picasso', 'painter', word2idx, idx2word, We)\n        analogy('japan', 'sushi', 'germany', 'bratwurst', word2idx, idx2word, We)\n        analogy('man', 'woman', 'he', 'she', word2idx, idx2word, We)\n        analogy('man', 'woman', 'uncle', 'aunt', word2idx, idx2word, We)\n        analogy('man', 'woman', 'brother', 'sister', word2idx, idx2word, We)\n        analogy('man', 'woman', 'husband', 'wife', word2idx, idx2word, We)\n        analogy('man', 'woman', 'actor', 'actress', word2idx, idx2word, We)\n        analogy('man', 'woman', 'father', 'mother', word2idx, idx2word, We)\n        analogy('heir', 'heiress', 'prince', 'princess', word2idx, idx2word, We)\n        analogy('nephew', 'niece', 'uncle', 'aunt', word2idx, idx2word, We)\n        analogy('france', 'paris', 'japan', 'tokyo', word2idx, idx2word, We)\n        analogy('france', 'paris', 'china', 'beijing', word2idx, idx2word, We)\n        analogy('february', 'january', 'december', 'november', word2idx, idx2word, We)\n        analogy('france', 'paris', 'germany', 'berlin', word2idx, idx2word, We)\n        analogy('week', 'day', 'year', 'month', word2idx, idx2word, We)\n        analogy('week', 'day', 'hour', 'minute', word2idx, idx2word, We)\n        analogy('france', 'paris', 'italy', 'rome', word2idx, idx2word, We)\n        analogy('paris', 'france', 'rome', 'italy', word2idx, idx2word, We)\n        analogy('france', 'french', 'england', 'english', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'china', 'chinese', word2idx, idx2word, We)\n        analogy('china', 'chinese', 'america', 'american', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'italy', 'italian', word2idx, idx2word, We)\n        analogy('japan', 'japanese', 'australia', 'australian', word2idx, idx2word, We)\n        analogy('walk', 'walking', 'swim', 'swimming', word2idx, idx2word, We)"
        ]
    }
]