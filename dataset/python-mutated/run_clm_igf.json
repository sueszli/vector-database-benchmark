[
    {
        "func_name": "generate_n_pairs",
        "original": "def generate_n_pairs(context_len=32, max_steps=10, size_objective_set=100, min_len=1026, trim=True, data_file='data/tokenized_stories_train_wikitext103.jbl', igf_data_file='igf_context_pairs.jbl'):\n    \"\"\"\n    Collecting *n* pairs for training the secondary learner\n    Args:\n        context_len: The maximum total input sequence length after tokenization. Sequences longer\n                    than this will be truncated, sequences shorter will be padded\n        max_steps: To calculate training epochs of secondary learner\n        size_objective_set: size of objective data set used to create (X,IG(X)) pairs which is the training data for secondary learner\n        min_len: The minimum length of the article to be used as objective set\n        trim: If True truncate the context if it exceeds context length\n        data_file: Tokenized data set split for training and evaluation of model\n        igf_data_file: file to store (I,IG(X)) paired data set to train secondary learner\n\n    Returns:\n        Data stored in igf_data_file\n\n    \"\"\"\n    set_seed(3)\n    (train_data, objective_set) = generate_datasets(context_len, data_file, number=size_objective_set, min_len=1026, trim=True)\n    set_seed(4)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model = load_gpt2('gpt2').to(device)\n    print('computing perplexity on objective set')\n    orig_perp = compute_perplexity(model, objective_set, context_len).item()\n    print('perplexity on objective set:', orig_perp)\n    collect_objective_set(model, orig_perp, context_len, train_data, objective_set, max_steps, device, igf_data_file)\n    del model, train_data, objective_set\n    torch.cuda.empty_cache()",
        "mutated": [
            "def generate_n_pairs(context_len=32, max_steps=10, size_objective_set=100, min_len=1026, trim=True, data_file='data/tokenized_stories_train_wikitext103.jbl', igf_data_file='igf_context_pairs.jbl'):\n    if False:\n        i = 10\n    '\\n    Collecting *n* pairs for training the secondary learner\\n    Args:\\n        context_len: The maximum total input sequence length after tokenization. Sequences longer\\n                    than this will be truncated, sequences shorter will be padded\\n        max_steps: To calculate training epochs of secondary learner\\n        size_objective_set: size of objective data set used to create (X,IG(X)) pairs which is the training data for secondary learner\\n        min_len: The minimum length of the article to be used as objective set\\n        trim: If True truncate the context if it exceeds context length\\n        data_file: Tokenized data set split for training and evaluation of model\\n        igf_data_file: file to store (I,IG(X)) paired data set to train secondary learner\\n\\n    Returns:\\n        Data stored in igf_data_file\\n\\n    '\n    set_seed(3)\n    (train_data, objective_set) = generate_datasets(context_len, data_file, number=size_objective_set, min_len=1026, trim=True)\n    set_seed(4)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model = load_gpt2('gpt2').to(device)\n    print('computing perplexity on objective set')\n    orig_perp = compute_perplexity(model, objective_set, context_len).item()\n    print('perplexity on objective set:', orig_perp)\n    collect_objective_set(model, orig_perp, context_len, train_data, objective_set, max_steps, device, igf_data_file)\n    del model, train_data, objective_set\n    torch.cuda.empty_cache()",
            "def generate_n_pairs(context_len=32, max_steps=10, size_objective_set=100, min_len=1026, trim=True, data_file='data/tokenized_stories_train_wikitext103.jbl', igf_data_file='igf_context_pairs.jbl'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Collecting *n* pairs for training the secondary learner\\n    Args:\\n        context_len: The maximum total input sequence length after tokenization. Sequences longer\\n                    than this will be truncated, sequences shorter will be padded\\n        max_steps: To calculate training epochs of secondary learner\\n        size_objective_set: size of objective data set used to create (X,IG(X)) pairs which is the training data for secondary learner\\n        min_len: The minimum length of the article to be used as objective set\\n        trim: If True truncate the context if it exceeds context length\\n        data_file: Tokenized data set split for training and evaluation of model\\n        igf_data_file: file to store (I,IG(X)) paired data set to train secondary learner\\n\\n    Returns:\\n        Data stored in igf_data_file\\n\\n    '\n    set_seed(3)\n    (train_data, objective_set) = generate_datasets(context_len, data_file, number=size_objective_set, min_len=1026, trim=True)\n    set_seed(4)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model = load_gpt2('gpt2').to(device)\n    print('computing perplexity on objective set')\n    orig_perp = compute_perplexity(model, objective_set, context_len).item()\n    print('perplexity on objective set:', orig_perp)\n    collect_objective_set(model, orig_perp, context_len, train_data, objective_set, max_steps, device, igf_data_file)\n    del model, train_data, objective_set\n    torch.cuda.empty_cache()",
            "def generate_n_pairs(context_len=32, max_steps=10, size_objective_set=100, min_len=1026, trim=True, data_file='data/tokenized_stories_train_wikitext103.jbl', igf_data_file='igf_context_pairs.jbl'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Collecting *n* pairs for training the secondary learner\\n    Args:\\n        context_len: The maximum total input sequence length after tokenization. Sequences longer\\n                    than this will be truncated, sequences shorter will be padded\\n        max_steps: To calculate training epochs of secondary learner\\n        size_objective_set: size of objective data set used to create (X,IG(X)) pairs which is the training data for secondary learner\\n        min_len: The minimum length of the article to be used as objective set\\n        trim: If True truncate the context if it exceeds context length\\n        data_file: Tokenized data set split for training and evaluation of model\\n        igf_data_file: file to store (I,IG(X)) paired data set to train secondary learner\\n\\n    Returns:\\n        Data stored in igf_data_file\\n\\n    '\n    set_seed(3)\n    (train_data, objective_set) = generate_datasets(context_len, data_file, number=size_objective_set, min_len=1026, trim=True)\n    set_seed(4)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model = load_gpt2('gpt2').to(device)\n    print('computing perplexity on objective set')\n    orig_perp = compute_perplexity(model, objective_set, context_len).item()\n    print('perplexity on objective set:', orig_perp)\n    collect_objective_set(model, orig_perp, context_len, train_data, objective_set, max_steps, device, igf_data_file)\n    del model, train_data, objective_set\n    torch.cuda.empty_cache()",
            "def generate_n_pairs(context_len=32, max_steps=10, size_objective_set=100, min_len=1026, trim=True, data_file='data/tokenized_stories_train_wikitext103.jbl', igf_data_file='igf_context_pairs.jbl'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Collecting *n* pairs for training the secondary learner\\n    Args:\\n        context_len: The maximum total input sequence length after tokenization. Sequences longer\\n                    than this will be truncated, sequences shorter will be padded\\n        max_steps: To calculate training epochs of secondary learner\\n        size_objective_set: size of objective data set used to create (X,IG(X)) pairs which is the training data for secondary learner\\n        min_len: The minimum length of the article to be used as objective set\\n        trim: If True truncate the context if it exceeds context length\\n        data_file: Tokenized data set split for training and evaluation of model\\n        igf_data_file: file to store (I,IG(X)) paired data set to train secondary learner\\n\\n    Returns:\\n        Data stored in igf_data_file\\n\\n    '\n    set_seed(3)\n    (train_data, objective_set) = generate_datasets(context_len, data_file, number=size_objective_set, min_len=1026, trim=True)\n    set_seed(4)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model = load_gpt2('gpt2').to(device)\n    print('computing perplexity on objective set')\n    orig_perp = compute_perplexity(model, objective_set, context_len).item()\n    print('perplexity on objective set:', orig_perp)\n    collect_objective_set(model, orig_perp, context_len, train_data, objective_set, max_steps, device, igf_data_file)\n    del model, train_data, objective_set\n    torch.cuda.empty_cache()",
            "def generate_n_pairs(context_len=32, max_steps=10, size_objective_set=100, min_len=1026, trim=True, data_file='data/tokenized_stories_train_wikitext103.jbl', igf_data_file='igf_context_pairs.jbl'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Collecting *n* pairs for training the secondary learner\\n    Args:\\n        context_len: The maximum total input sequence length after tokenization. Sequences longer\\n                    than this will be truncated, sequences shorter will be padded\\n        max_steps: To calculate training epochs of secondary learner\\n        size_objective_set: size of objective data set used to create (X,IG(X)) pairs which is the training data for secondary learner\\n        min_len: The minimum length of the article to be used as objective set\\n        trim: If True truncate the context if it exceeds context length\\n        data_file: Tokenized data set split for training and evaluation of model\\n        igf_data_file: file to store (I,IG(X)) paired data set to train secondary learner\\n\\n    Returns:\\n        Data stored in igf_data_file\\n\\n    '\n    set_seed(3)\n    (train_data, objective_set) = generate_datasets(context_len, data_file, number=size_objective_set, min_len=1026, trim=True)\n    set_seed(4)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model = load_gpt2('gpt2').to(device)\n    print('computing perplexity on objective set')\n    orig_perp = compute_perplexity(model, objective_set, context_len).item()\n    print('perplexity on objective set:', orig_perp)\n    collect_objective_set(model, orig_perp, context_len, train_data, objective_set, max_steps, device, igf_data_file)\n    del model, train_data, objective_set\n    torch.cuda.empty_cache()"
        ]
    },
    {
        "func_name": "training_secondary_learner",
        "original": "def training_secondary_learner(secondary_learner_train_data, secondary_learner_max_epochs=15, secondary_learner_batch_size=128, eval_freq=100, igf_model_path='igf_model.pt'):\n    \"\"\"\n    Train the secondary learner\n\n    Args:\n        secondary_learner_train_data: Data set with (X,IG(X)) pairs to train secondary learner where IG(X) - measure of informativeness and X- context\n        secondary_learner_max_epochs: Number of epochs to train secondary learner\n        secondary_learner_batch_size: Batch size to train secondary learner\n        eval_freq (object): secondary model evaluation can be triggered at eval_freq\n        igf_model_path: path to store trained secondary learner\n\n    Returns:\n        Trained secondary learner\n    \"\"\"\n    set_seed(42)\n    model = GPT2LMHeadModel.from_pretrained('gpt2')\n    secondary_learner = SecondaryLearner(model)\n    secondary_learner = train_secondary_learner(secondary_learner, secondary_learner_train_data, max_epochs=secondary_learner_max_epochs, batch_size=secondary_learner_batch_size, eval_freq=100, igf_model_path=igf_model_path)\n    del model, secondary_learner_train_data\n    torch.cuda.empty_cache()\n    return secondary_learner",
        "mutated": [
            "def training_secondary_learner(secondary_learner_train_data, secondary_learner_max_epochs=15, secondary_learner_batch_size=128, eval_freq=100, igf_model_path='igf_model.pt'):\n    if False:\n        i = 10\n    '\\n    Train the secondary learner\\n\\n    Args:\\n        secondary_learner_train_data: Data set with (X,IG(X)) pairs to train secondary learner where IG(X) - measure of informativeness and X- context\\n        secondary_learner_max_epochs: Number of epochs to train secondary learner\\n        secondary_learner_batch_size: Batch size to train secondary learner\\n        eval_freq (object): secondary model evaluation can be triggered at eval_freq\\n        igf_model_path: path to store trained secondary learner\\n\\n    Returns:\\n        Trained secondary learner\\n    '\n    set_seed(42)\n    model = GPT2LMHeadModel.from_pretrained('gpt2')\n    secondary_learner = SecondaryLearner(model)\n    secondary_learner = train_secondary_learner(secondary_learner, secondary_learner_train_data, max_epochs=secondary_learner_max_epochs, batch_size=secondary_learner_batch_size, eval_freq=100, igf_model_path=igf_model_path)\n    del model, secondary_learner_train_data\n    torch.cuda.empty_cache()\n    return secondary_learner",
            "def training_secondary_learner(secondary_learner_train_data, secondary_learner_max_epochs=15, secondary_learner_batch_size=128, eval_freq=100, igf_model_path='igf_model.pt'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Train the secondary learner\\n\\n    Args:\\n        secondary_learner_train_data: Data set with (X,IG(X)) pairs to train secondary learner where IG(X) - measure of informativeness and X- context\\n        secondary_learner_max_epochs: Number of epochs to train secondary learner\\n        secondary_learner_batch_size: Batch size to train secondary learner\\n        eval_freq (object): secondary model evaluation can be triggered at eval_freq\\n        igf_model_path: path to store trained secondary learner\\n\\n    Returns:\\n        Trained secondary learner\\n    '\n    set_seed(42)\n    model = GPT2LMHeadModel.from_pretrained('gpt2')\n    secondary_learner = SecondaryLearner(model)\n    secondary_learner = train_secondary_learner(secondary_learner, secondary_learner_train_data, max_epochs=secondary_learner_max_epochs, batch_size=secondary_learner_batch_size, eval_freq=100, igf_model_path=igf_model_path)\n    del model, secondary_learner_train_data\n    torch.cuda.empty_cache()\n    return secondary_learner",
            "def training_secondary_learner(secondary_learner_train_data, secondary_learner_max_epochs=15, secondary_learner_batch_size=128, eval_freq=100, igf_model_path='igf_model.pt'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Train the secondary learner\\n\\n    Args:\\n        secondary_learner_train_data: Data set with (X,IG(X)) pairs to train secondary learner where IG(X) - measure of informativeness and X- context\\n        secondary_learner_max_epochs: Number of epochs to train secondary learner\\n        secondary_learner_batch_size: Batch size to train secondary learner\\n        eval_freq (object): secondary model evaluation can be triggered at eval_freq\\n        igf_model_path: path to store trained secondary learner\\n\\n    Returns:\\n        Trained secondary learner\\n    '\n    set_seed(42)\n    model = GPT2LMHeadModel.from_pretrained('gpt2')\n    secondary_learner = SecondaryLearner(model)\n    secondary_learner = train_secondary_learner(secondary_learner, secondary_learner_train_data, max_epochs=secondary_learner_max_epochs, batch_size=secondary_learner_batch_size, eval_freq=100, igf_model_path=igf_model_path)\n    del model, secondary_learner_train_data\n    torch.cuda.empty_cache()\n    return secondary_learner",
            "def training_secondary_learner(secondary_learner_train_data, secondary_learner_max_epochs=15, secondary_learner_batch_size=128, eval_freq=100, igf_model_path='igf_model.pt'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Train the secondary learner\\n\\n    Args:\\n        secondary_learner_train_data: Data set with (X,IG(X)) pairs to train secondary learner where IG(X) - measure of informativeness and X- context\\n        secondary_learner_max_epochs: Number of epochs to train secondary learner\\n        secondary_learner_batch_size: Batch size to train secondary learner\\n        eval_freq (object): secondary model evaluation can be triggered at eval_freq\\n        igf_model_path: path to store trained secondary learner\\n\\n    Returns:\\n        Trained secondary learner\\n    '\n    set_seed(42)\n    model = GPT2LMHeadModel.from_pretrained('gpt2')\n    secondary_learner = SecondaryLearner(model)\n    secondary_learner = train_secondary_learner(secondary_learner, secondary_learner_train_data, max_epochs=secondary_learner_max_epochs, batch_size=secondary_learner_batch_size, eval_freq=100, igf_model_path=igf_model_path)\n    del model, secondary_learner_train_data\n    torch.cuda.empty_cache()\n    return secondary_learner",
            "def training_secondary_learner(secondary_learner_train_data, secondary_learner_max_epochs=15, secondary_learner_batch_size=128, eval_freq=100, igf_model_path='igf_model.pt'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Train the secondary learner\\n\\n    Args:\\n        secondary_learner_train_data: Data set with (X,IG(X)) pairs to train secondary learner where IG(X) - measure of informativeness and X- context\\n        secondary_learner_max_epochs: Number of epochs to train secondary learner\\n        secondary_learner_batch_size: Batch size to train secondary learner\\n        eval_freq (object): secondary model evaluation can be triggered at eval_freq\\n        igf_model_path: path to store trained secondary learner\\n\\n    Returns:\\n        Trained secondary learner\\n    '\n    set_seed(42)\n    model = GPT2LMHeadModel.from_pretrained('gpt2')\n    secondary_learner = SecondaryLearner(model)\n    secondary_learner = train_secondary_learner(secondary_learner, secondary_learner_train_data, max_epochs=secondary_learner_max_epochs, batch_size=secondary_learner_batch_size, eval_freq=100, igf_model_path=igf_model_path)\n    del model, secondary_learner_train_data\n    torch.cuda.empty_cache()\n    return secondary_learner"
        ]
    },
    {
        "func_name": "finetune",
        "original": "def finetune(model, train_dataset, test_dataset, context_len=32, max_steps=1000, batch_size=16, threshold=1.0, recopy_model=recopy_gpt2, secondary_learner=None, eval_interval=10, finetuned_model_name='gpt2_finetuned.pt'):\n    \"\"\"\n    fine-tune with IGF if secondary_learner is not None, else standard fine-tuning\n\n    Args:\n        model: pre-trained GPT-2 model\n        train_dataset: Data set to train GPT-2 model\n        test_dataset: Evaluate GPT-2 model\n        context_len: The maximum total input sequence length after tokenization. Sequences longer\n                    than this will be truncated, sequences shorter will be padded\n        max_steps: To calculate training epochs\n        batch_size: Batch size to train GPT-2 model\n        threshold: The threshold value used by secondary learner to filter the train_data and allow only\"\n                    informative data as input to the model\n        recopy_model: Reset the model to the original pretrained GPT-2 weights after each iteration\n        secondary_learner: Selection of IGF as fine-tuning method if not None\n        eval_interval: number of batches after which decay the selectivity of our secondary learner filter from\n                        1 standard deviation above average to 1 below average\n        fine-tuned_model_name: name of the final final-tuned GPT-2 model\n\n    Returns:\n        Fine-tuned GPT-2 model\n\n    \"\"\"\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler)\n    num_train_epochs = max_steps // len(train_dataset) + 1\n    global_step = 0\n    context = torch.zeros((1, context_len), dtype=torch.long, device=device)\n    (model, lm_optimizer, lm_scheduler) = recopy_model(model, device, max_steps)\n    model.train()\n    if secondary_learner is not None:\n        secondary_learner.to(device)\n        secondary_learner.eval()\n    contexts = []\n    examples = 0\n    observed_qs = []\n    test_perps = []\n    real_perp = compute_perplexity(model, test_dataset, context_len)\n    test_perps.append(real_perp)\n    print('Test perplexity, step', global_step, ':', real_perp)\n    for epoch in range(int(num_train_epochs)):\n        for (step, example) in enumerate(train_dataloader):\n            torch.cuda.empty_cache()\n            start = random.randint(0, example.size(2) - context_len - 1)\n            context[0, :] = example[0, 0, start:start + context_len]\n            lm_optimizer.zero_grad()\n            outputs = model(context, labels=context)\n            do_backprop = True\n            if secondary_learner is not None:\n                predicted_q = secondary_learner.forward(torch.tensor(context, dtype=torch.long, device=device).unsqueeze(0))[0].item()\n                observed_qs.append(float(predicted_q))\n                if global_step == 10:\n                    threshold = -1\n                if predicted_q < threshold:\n                    do_backprop = False\n            if do_backprop:\n                contexts.append(np.array(context.cpu()))\n                lm_loss = outputs[0]\n                lm_loss.backward()\n                examples += 1\n            del outputs\n            if examples == batch_size:\n                torch.cuda.empty_cache()\n                examples = 0\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n                lm_optimizer.step()\n                lm_scheduler.step()\n                global_step += 1\n                if global_step % eval_interval == 0:\n                    real_perp = compute_perplexity(model, test_dataset, context_len)\n                    test_perps.append(real_perp)\n                    print('Test perplexity, step', global_step, ':', real_perp)\n            if max_steps > 0 and global_step > 60:\n                break\n        if max_steps > 0 and global_step > 60:\n            break\n    torch.save(model.state_dict(), finetuned_model_name)\n    torch.cuda.empty_cache()\n    del lm_optimizer\n    del lm_scheduler\n    return model",
        "mutated": [
            "def finetune(model, train_dataset, test_dataset, context_len=32, max_steps=1000, batch_size=16, threshold=1.0, recopy_model=recopy_gpt2, secondary_learner=None, eval_interval=10, finetuned_model_name='gpt2_finetuned.pt'):\n    if False:\n        i = 10\n    '\\n    fine-tune with IGF if secondary_learner is not None, else standard fine-tuning\\n\\n    Args:\\n        model: pre-trained GPT-2 model\\n        train_dataset: Data set to train GPT-2 model\\n        test_dataset: Evaluate GPT-2 model\\n        context_len: The maximum total input sequence length after tokenization. Sequences longer\\n                    than this will be truncated, sequences shorter will be padded\\n        max_steps: To calculate training epochs\\n        batch_size: Batch size to train GPT-2 model\\n        threshold: The threshold value used by secondary learner to filter the train_data and allow only\"\\n                    informative data as input to the model\\n        recopy_model: Reset the model to the original pretrained GPT-2 weights after each iteration\\n        secondary_learner: Selection of IGF as fine-tuning method if not None\\n        eval_interval: number of batches after which decay the selectivity of our secondary learner filter from\\n                        1 standard deviation above average to 1 below average\\n        fine-tuned_model_name: name of the final final-tuned GPT-2 model\\n\\n    Returns:\\n        Fine-tuned GPT-2 model\\n\\n    '\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler)\n    num_train_epochs = max_steps // len(train_dataset) + 1\n    global_step = 0\n    context = torch.zeros((1, context_len), dtype=torch.long, device=device)\n    (model, lm_optimizer, lm_scheduler) = recopy_model(model, device, max_steps)\n    model.train()\n    if secondary_learner is not None:\n        secondary_learner.to(device)\n        secondary_learner.eval()\n    contexts = []\n    examples = 0\n    observed_qs = []\n    test_perps = []\n    real_perp = compute_perplexity(model, test_dataset, context_len)\n    test_perps.append(real_perp)\n    print('Test perplexity, step', global_step, ':', real_perp)\n    for epoch in range(int(num_train_epochs)):\n        for (step, example) in enumerate(train_dataloader):\n            torch.cuda.empty_cache()\n            start = random.randint(0, example.size(2) - context_len - 1)\n            context[0, :] = example[0, 0, start:start + context_len]\n            lm_optimizer.zero_grad()\n            outputs = model(context, labels=context)\n            do_backprop = True\n            if secondary_learner is not None:\n                predicted_q = secondary_learner.forward(torch.tensor(context, dtype=torch.long, device=device).unsqueeze(0))[0].item()\n                observed_qs.append(float(predicted_q))\n                if global_step == 10:\n                    threshold = -1\n                if predicted_q < threshold:\n                    do_backprop = False\n            if do_backprop:\n                contexts.append(np.array(context.cpu()))\n                lm_loss = outputs[0]\n                lm_loss.backward()\n                examples += 1\n            del outputs\n            if examples == batch_size:\n                torch.cuda.empty_cache()\n                examples = 0\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n                lm_optimizer.step()\n                lm_scheduler.step()\n                global_step += 1\n                if global_step % eval_interval == 0:\n                    real_perp = compute_perplexity(model, test_dataset, context_len)\n                    test_perps.append(real_perp)\n                    print('Test perplexity, step', global_step, ':', real_perp)\n            if max_steps > 0 and global_step > 60:\n                break\n        if max_steps > 0 and global_step > 60:\n            break\n    torch.save(model.state_dict(), finetuned_model_name)\n    torch.cuda.empty_cache()\n    del lm_optimizer\n    del lm_scheduler\n    return model",
            "def finetune(model, train_dataset, test_dataset, context_len=32, max_steps=1000, batch_size=16, threshold=1.0, recopy_model=recopy_gpt2, secondary_learner=None, eval_interval=10, finetuned_model_name='gpt2_finetuned.pt'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    fine-tune with IGF if secondary_learner is not None, else standard fine-tuning\\n\\n    Args:\\n        model: pre-trained GPT-2 model\\n        train_dataset: Data set to train GPT-2 model\\n        test_dataset: Evaluate GPT-2 model\\n        context_len: The maximum total input sequence length after tokenization. Sequences longer\\n                    than this will be truncated, sequences shorter will be padded\\n        max_steps: To calculate training epochs\\n        batch_size: Batch size to train GPT-2 model\\n        threshold: The threshold value used by secondary learner to filter the train_data and allow only\"\\n                    informative data as input to the model\\n        recopy_model: Reset the model to the original pretrained GPT-2 weights after each iteration\\n        secondary_learner: Selection of IGF as fine-tuning method if not None\\n        eval_interval: number of batches after which decay the selectivity of our secondary learner filter from\\n                        1 standard deviation above average to 1 below average\\n        fine-tuned_model_name: name of the final final-tuned GPT-2 model\\n\\n    Returns:\\n        Fine-tuned GPT-2 model\\n\\n    '\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler)\n    num_train_epochs = max_steps // len(train_dataset) + 1\n    global_step = 0\n    context = torch.zeros((1, context_len), dtype=torch.long, device=device)\n    (model, lm_optimizer, lm_scheduler) = recopy_model(model, device, max_steps)\n    model.train()\n    if secondary_learner is not None:\n        secondary_learner.to(device)\n        secondary_learner.eval()\n    contexts = []\n    examples = 0\n    observed_qs = []\n    test_perps = []\n    real_perp = compute_perplexity(model, test_dataset, context_len)\n    test_perps.append(real_perp)\n    print('Test perplexity, step', global_step, ':', real_perp)\n    for epoch in range(int(num_train_epochs)):\n        for (step, example) in enumerate(train_dataloader):\n            torch.cuda.empty_cache()\n            start = random.randint(0, example.size(2) - context_len - 1)\n            context[0, :] = example[0, 0, start:start + context_len]\n            lm_optimizer.zero_grad()\n            outputs = model(context, labels=context)\n            do_backprop = True\n            if secondary_learner is not None:\n                predicted_q = secondary_learner.forward(torch.tensor(context, dtype=torch.long, device=device).unsqueeze(0))[0].item()\n                observed_qs.append(float(predicted_q))\n                if global_step == 10:\n                    threshold = -1\n                if predicted_q < threshold:\n                    do_backprop = False\n            if do_backprop:\n                contexts.append(np.array(context.cpu()))\n                lm_loss = outputs[0]\n                lm_loss.backward()\n                examples += 1\n            del outputs\n            if examples == batch_size:\n                torch.cuda.empty_cache()\n                examples = 0\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n                lm_optimizer.step()\n                lm_scheduler.step()\n                global_step += 1\n                if global_step % eval_interval == 0:\n                    real_perp = compute_perplexity(model, test_dataset, context_len)\n                    test_perps.append(real_perp)\n                    print('Test perplexity, step', global_step, ':', real_perp)\n            if max_steps > 0 and global_step > 60:\n                break\n        if max_steps > 0 and global_step > 60:\n            break\n    torch.save(model.state_dict(), finetuned_model_name)\n    torch.cuda.empty_cache()\n    del lm_optimizer\n    del lm_scheduler\n    return model",
            "def finetune(model, train_dataset, test_dataset, context_len=32, max_steps=1000, batch_size=16, threshold=1.0, recopy_model=recopy_gpt2, secondary_learner=None, eval_interval=10, finetuned_model_name='gpt2_finetuned.pt'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    fine-tune with IGF if secondary_learner is not None, else standard fine-tuning\\n\\n    Args:\\n        model: pre-trained GPT-2 model\\n        train_dataset: Data set to train GPT-2 model\\n        test_dataset: Evaluate GPT-2 model\\n        context_len: The maximum total input sequence length after tokenization. Sequences longer\\n                    than this will be truncated, sequences shorter will be padded\\n        max_steps: To calculate training epochs\\n        batch_size: Batch size to train GPT-2 model\\n        threshold: The threshold value used by secondary learner to filter the train_data and allow only\"\\n                    informative data as input to the model\\n        recopy_model: Reset the model to the original pretrained GPT-2 weights after each iteration\\n        secondary_learner: Selection of IGF as fine-tuning method if not None\\n        eval_interval: number of batches after which decay the selectivity of our secondary learner filter from\\n                        1 standard deviation above average to 1 below average\\n        fine-tuned_model_name: name of the final final-tuned GPT-2 model\\n\\n    Returns:\\n        Fine-tuned GPT-2 model\\n\\n    '\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler)\n    num_train_epochs = max_steps // len(train_dataset) + 1\n    global_step = 0\n    context = torch.zeros((1, context_len), dtype=torch.long, device=device)\n    (model, lm_optimizer, lm_scheduler) = recopy_model(model, device, max_steps)\n    model.train()\n    if secondary_learner is not None:\n        secondary_learner.to(device)\n        secondary_learner.eval()\n    contexts = []\n    examples = 0\n    observed_qs = []\n    test_perps = []\n    real_perp = compute_perplexity(model, test_dataset, context_len)\n    test_perps.append(real_perp)\n    print('Test perplexity, step', global_step, ':', real_perp)\n    for epoch in range(int(num_train_epochs)):\n        for (step, example) in enumerate(train_dataloader):\n            torch.cuda.empty_cache()\n            start = random.randint(0, example.size(2) - context_len - 1)\n            context[0, :] = example[0, 0, start:start + context_len]\n            lm_optimizer.zero_grad()\n            outputs = model(context, labels=context)\n            do_backprop = True\n            if secondary_learner is not None:\n                predicted_q = secondary_learner.forward(torch.tensor(context, dtype=torch.long, device=device).unsqueeze(0))[0].item()\n                observed_qs.append(float(predicted_q))\n                if global_step == 10:\n                    threshold = -1\n                if predicted_q < threshold:\n                    do_backprop = False\n            if do_backprop:\n                contexts.append(np.array(context.cpu()))\n                lm_loss = outputs[0]\n                lm_loss.backward()\n                examples += 1\n            del outputs\n            if examples == batch_size:\n                torch.cuda.empty_cache()\n                examples = 0\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n                lm_optimizer.step()\n                lm_scheduler.step()\n                global_step += 1\n                if global_step % eval_interval == 0:\n                    real_perp = compute_perplexity(model, test_dataset, context_len)\n                    test_perps.append(real_perp)\n                    print('Test perplexity, step', global_step, ':', real_perp)\n            if max_steps > 0 and global_step > 60:\n                break\n        if max_steps > 0 and global_step > 60:\n            break\n    torch.save(model.state_dict(), finetuned_model_name)\n    torch.cuda.empty_cache()\n    del lm_optimizer\n    del lm_scheduler\n    return model",
            "def finetune(model, train_dataset, test_dataset, context_len=32, max_steps=1000, batch_size=16, threshold=1.0, recopy_model=recopy_gpt2, secondary_learner=None, eval_interval=10, finetuned_model_name='gpt2_finetuned.pt'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    fine-tune with IGF if secondary_learner is not None, else standard fine-tuning\\n\\n    Args:\\n        model: pre-trained GPT-2 model\\n        train_dataset: Data set to train GPT-2 model\\n        test_dataset: Evaluate GPT-2 model\\n        context_len: The maximum total input sequence length after tokenization. Sequences longer\\n                    than this will be truncated, sequences shorter will be padded\\n        max_steps: To calculate training epochs\\n        batch_size: Batch size to train GPT-2 model\\n        threshold: The threshold value used by secondary learner to filter the train_data and allow only\"\\n                    informative data as input to the model\\n        recopy_model: Reset the model to the original pretrained GPT-2 weights after each iteration\\n        secondary_learner: Selection of IGF as fine-tuning method if not None\\n        eval_interval: number of batches after which decay the selectivity of our secondary learner filter from\\n                        1 standard deviation above average to 1 below average\\n        fine-tuned_model_name: name of the final final-tuned GPT-2 model\\n\\n    Returns:\\n        Fine-tuned GPT-2 model\\n\\n    '\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler)\n    num_train_epochs = max_steps // len(train_dataset) + 1\n    global_step = 0\n    context = torch.zeros((1, context_len), dtype=torch.long, device=device)\n    (model, lm_optimizer, lm_scheduler) = recopy_model(model, device, max_steps)\n    model.train()\n    if secondary_learner is not None:\n        secondary_learner.to(device)\n        secondary_learner.eval()\n    contexts = []\n    examples = 0\n    observed_qs = []\n    test_perps = []\n    real_perp = compute_perplexity(model, test_dataset, context_len)\n    test_perps.append(real_perp)\n    print('Test perplexity, step', global_step, ':', real_perp)\n    for epoch in range(int(num_train_epochs)):\n        for (step, example) in enumerate(train_dataloader):\n            torch.cuda.empty_cache()\n            start = random.randint(0, example.size(2) - context_len - 1)\n            context[0, :] = example[0, 0, start:start + context_len]\n            lm_optimizer.zero_grad()\n            outputs = model(context, labels=context)\n            do_backprop = True\n            if secondary_learner is not None:\n                predicted_q = secondary_learner.forward(torch.tensor(context, dtype=torch.long, device=device).unsqueeze(0))[0].item()\n                observed_qs.append(float(predicted_q))\n                if global_step == 10:\n                    threshold = -1\n                if predicted_q < threshold:\n                    do_backprop = False\n            if do_backprop:\n                contexts.append(np.array(context.cpu()))\n                lm_loss = outputs[0]\n                lm_loss.backward()\n                examples += 1\n            del outputs\n            if examples == batch_size:\n                torch.cuda.empty_cache()\n                examples = 0\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n                lm_optimizer.step()\n                lm_scheduler.step()\n                global_step += 1\n                if global_step % eval_interval == 0:\n                    real_perp = compute_perplexity(model, test_dataset, context_len)\n                    test_perps.append(real_perp)\n                    print('Test perplexity, step', global_step, ':', real_perp)\n            if max_steps > 0 and global_step > 60:\n                break\n        if max_steps > 0 and global_step > 60:\n            break\n    torch.save(model.state_dict(), finetuned_model_name)\n    torch.cuda.empty_cache()\n    del lm_optimizer\n    del lm_scheduler\n    return model",
            "def finetune(model, train_dataset, test_dataset, context_len=32, max_steps=1000, batch_size=16, threshold=1.0, recopy_model=recopy_gpt2, secondary_learner=None, eval_interval=10, finetuned_model_name='gpt2_finetuned.pt'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    fine-tune with IGF if secondary_learner is not None, else standard fine-tuning\\n\\n    Args:\\n        model: pre-trained GPT-2 model\\n        train_dataset: Data set to train GPT-2 model\\n        test_dataset: Evaluate GPT-2 model\\n        context_len: The maximum total input sequence length after tokenization. Sequences longer\\n                    than this will be truncated, sequences shorter will be padded\\n        max_steps: To calculate training epochs\\n        batch_size: Batch size to train GPT-2 model\\n        threshold: The threshold value used by secondary learner to filter the train_data and allow only\"\\n                    informative data as input to the model\\n        recopy_model: Reset the model to the original pretrained GPT-2 weights after each iteration\\n        secondary_learner: Selection of IGF as fine-tuning method if not None\\n        eval_interval: number of batches after which decay the selectivity of our secondary learner filter from\\n                        1 standard deviation above average to 1 below average\\n        fine-tuned_model_name: name of the final final-tuned GPT-2 model\\n\\n    Returns:\\n        Fine-tuned GPT-2 model\\n\\n    '\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler)\n    num_train_epochs = max_steps // len(train_dataset) + 1\n    global_step = 0\n    context = torch.zeros((1, context_len), dtype=torch.long, device=device)\n    (model, lm_optimizer, lm_scheduler) = recopy_model(model, device, max_steps)\n    model.train()\n    if secondary_learner is not None:\n        secondary_learner.to(device)\n        secondary_learner.eval()\n    contexts = []\n    examples = 0\n    observed_qs = []\n    test_perps = []\n    real_perp = compute_perplexity(model, test_dataset, context_len)\n    test_perps.append(real_perp)\n    print('Test perplexity, step', global_step, ':', real_perp)\n    for epoch in range(int(num_train_epochs)):\n        for (step, example) in enumerate(train_dataloader):\n            torch.cuda.empty_cache()\n            start = random.randint(0, example.size(2) - context_len - 1)\n            context[0, :] = example[0, 0, start:start + context_len]\n            lm_optimizer.zero_grad()\n            outputs = model(context, labels=context)\n            do_backprop = True\n            if secondary_learner is not None:\n                predicted_q = secondary_learner.forward(torch.tensor(context, dtype=torch.long, device=device).unsqueeze(0))[0].item()\n                observed_qs.append(float(predicted_q))\n                if global_step == 10:\n                    threshold = -1\n                if predicted_q < threshold:\n                    do_backprop = False\n            if do_backprop:\n                contexts.append(np.array(context.cpu()))\n                lm_loss = outputs[0]\n                lm_loss.backward()\n                examples += 1\n            del outputs\n            if examples == batch_size:\n                torch.cuda.empty_cache()\n                examples = 0\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n                lm_optimizer.step()\n                lm_scheduler.step()\n                global_step += 1\n                if global_step % eval_interval == 0:\n                    real_perp = compute_perplexity(model, test_dataset, context_len)\n                    test_perps.append(real_perp)\n                    print('Test perplexity, step', global_step, ':', real_perp)\n            if max_steps > 0 and global_step > 60:\n                break\n        if max_steps > 0 and global_step > 60:\n            break\n    torch.save(model.state_dict(), finetuned_model_name)\n    torch.cuda.empty_cache()\n    del lm_optimizer\n    del lm_scheduler\n    return model"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    parser = argparse.ArgumentParser(description='Fine-tune a transformer model with IGF on a language modeling task')\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain data files for WikiText.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--data_file', type=str, default=None, help='A jbl file containing tokenized data which can be split as objective dataset, train_dataset and test_dataset.')\n    parser.add_argument('--igf_data_file', type=str, default=None, help='A jbl file containing the context and information gain pairs to train secondary learner.')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the final fine-tuned model is stored.')\n    parser.add_argument('--tokenizer_name', default=None, type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--context_len', default=32, type=int, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.')\n    parser.add_argument('--size_objective_set', default=100, type=int, help='number of articles that are long enough to be used as our objective set')\n    parser.add_argument('--eval_freq', default=100, type=int, help='secondary model evaluation is triggered at eval_freq')\n    parser.add_argument('--max_steps', default=1000, type=int, help='To calculate training epochs')\n    parser.add_argument('--secondary_learner_batch_size', default=128, type=int, help='batch size of training data for secondary learner')\n    parser.add_argument('--batch_size', default=16, type=int, help='batch size of training data of language model(gpt2) ')\n    parser.add_argument('--eval_interval', default=10, type=int, help='decay the selectivity of our secondary learner filter from 1 standard deviation above average to 1 below average after 10 batches')\n    parser.add_argument('--number', default=100, type=int, help='The number of examples split to be used as objective_set/test_data')\n    parser.add_argument('--min_len', default=1026, type=int, help='The minimum length of the article to be used as objective set')\n    parser.add_argument('--secondary_learner_max_epochs', default=15, type=int, help='number of epochs to train secondary learner')\n    parser.add_argument('--trim', default=True, type=bool, help='truncate the example if it exceeds context length')\n    parser.add_argument('--threshold', default=1.0, type=float, help='The threshold value used by secondary learner to filter the train_data and allow only informative data as input to the model')\n    parser.add_argument('--finetuned_model_name', default='gpt2_finetuned.pt', type=str, help='finetuned_model_name')\n    parser.add_argument('--recopy_model', default=recopy_gpt2, type=str, help='Reset the model to the original pretrained GPT-2 weights after each iteration')\n    generate_n_pairs(context_len=32, max_steps=10, size_objective_set=100, min_len=1026, trim=True, data_file='data/tokenized_stories_train_wikitext103.jbl', igf_data_file='igf_context_pairs.jbl')\n    secondary_learner_train_data = joblib.load('data/IGF_values.jbl')\n    secondary_learner = training_secondary_learner(secondary_learner_train_data, secondary_learner_max_epochs=15, secondary_learner_batch_size=128, eval_freq=100, igf_model_path='igf_model.pt')\n    model = GPT2LMHeadModel.from_pretrained('gpt2')\n    set_seed(42)\n    (train_dataset, test_dataset) = generate_datasets(context_len=32, file='data/tokenized_stories_train_wikitext103.jbl', number=100, min_len=1026, trim=True)\n    finetune(model, train_dataset, test_dataset, context_len=32, max_steps=1000, batch_size=16, threshold=1.0, recopy_model=recopy_gpt2, secondary_learner=secondary_learner, eval_interval=10, finetuned_model_name='gpt2_finetuned.pt')",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser(description='Fine-tune a transformer model with IGF on a language modeling task')\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain data files for WikiText.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--data_file', type=str, default=None, help='A jbl file containing tokenized data which can be split as objective dataset, train_dataset and test_dataset.')\n    parser.add_argument('--igf_data_file', type=str, default=None, help='A jbl file containing the context and information gain pairs to train secondary learner.')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the final fine-tuned model is stored.')\n    parser.add_argument('--tokenizer_name', default=None, type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--context_len', default=32, type=int, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.')\n    parser.add_argument('--size_objective_set', default=100, type=int, help='number of articles that are long enough to be used as our objective set')\n    parser.add_argument('--eval_freq', default=100, type=int, help='secondary model evaluation is triggered at eval_freq')\n    parser.add_argument('--max_steps', default=1000, type=int, help='To calculate training epochs')\n    parser.add_argument('--secondary_learner_batch_size', default=128, type=int, help='batch size of training data for secondary learner')\n    parser.add_argument('--batch_size', default=16, type=int, help='batch size of training data of language model(gpt2) ')\n    parser.add_argument('--eval_interval', default=10, type=int, help='decay the selectivity of our secondary learner filter from 1 standard deviation above average to 1 below average after 10 batches')\n    parser.add_argument('--number', default=100, type=int, help='The number of examples split to be used as objective_set/test_data')\n    parser.add_argument('--min_len', default=1026, type=int, help='The minimum length of the article to be used as objective set')\n    parser.add_argument('--secondary_learner_max_epochs', default=15, type=int, help='number of epochs to train secondary learner')\n    parser.add_argument('--trim', default=True, type=bool, help='truncate the example if it exceeds context length')\n    parser.add_argument('--threshold', default=1.0, type=float, help='The threshold value used by secondary learner to filter the train_data and allow only informative data as input to the model')\n    parser.add_argument('--finetuned_model_name', default='gpt2_finetuned.pt', type=str, help='finetuned_model_name')\n    parser.add_argument('--recopy_model', default=recopy_gpt2, type=str, help='Reset the model to the original pretrained GPT-2 weights after each iteration')\n    generate_n_pairs(context_len=32, max_steps=10, size_objective_set=100, min_len=1026, trim=True, data_file='data/tokenized_stories_train_wikitext103.jbl', igf_data_file='igf_context_pairs.jbl')\n    secondary_learner_train_data = joblib.load('data/IGF_values.jbl')\n    secondary_learner = training_secondary_learner(secondary_learner_train_data, secondary_learner_max_epochs=15, secondary_learner_batch_size=128, eval_freq=100, igf_model_path='igf_model.pt')\n    model = GPT2LMHeadModel.from_pretrained('gpt2')\n    set_seed(42)\n    (train_dataset, test_dataset) = generate_datasets(context_len=32, file='data/tokenized_stories_train_wikitext103.jbl', number=100, min_len=1026, trim=True)\n    finetune(model, train_dataset, test_dataset, context_len=32, max_steps=1000, batch_size=16, threshold=1.0, recopy_model=recopy_gpt2, secondary_learner=secondary_learner, eval_interval=10, finetuned_model_name='gpt2_finetuned.pt')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser(description='Fine-tune a transformer model with IGF on a language modeling task')\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain data files for WikiText.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--data_file', type=str, default=None, help='A jbl file containing tokenized data which can be split as objective dataset, train_dataset and test_dataset.')\n    parser.add_argument('--igf_data_file', type=str, default=None, help='A jbl file containing the context and information gain pairs to train secondary learner.')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the final fine-tuned model is stored.')\n    parser.add_argument('--tokenizer_name', default=None, type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--context_len', default=32, type=int, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.')\n    parser.add_argument('--size_objective_set', default=100, type=int, help='number of articles that are long enough to be used as our objective set')\n    parser.add_argument('--eval_freq', default=100, type=int, help='secondary model evaluation is triggered at eval_freq')\n    parser.add_argument('--max_steps', default=1000, type=int, help='To calculate training epochs')\n    parser.add_argument('--secondary_learner_batch_size', default=128, type=int, help='batch size of training data for secondary learner')\n    parser.add_argument('--batch_size', default=16, type=int, help='batch size of training data of language model(gpt2) ')\n    parser.add_argument('--eval_interval', default=10, type=int, help='decay the selectivity of our secondary learner filter from 1 standard deviation above average to 1 below average after 10 batches')\n    parser.add_argument('--number', default=100, type=int, help='The number of examples split to be used as objective_set/test_data')\n    parser.add_argument('--min_len', default=1026, type=int, help='The minimum length of the article to be used as objective set')\n    parser.add_argument('--secondary_learner_max_epochs', default=15, type=int, help='number of epochs to train secondary learner')\n    parser.add_argument('--trim', default=True, type=bool, help='truncate the example if it exceeds context length')\n    parser.add_argument('--threshold', default=1.0, type=float, help='The threshold value used by secondary learner to filter the train_data and allow only informative data as input to the model')\n    parser.add_argument('--finetuned_model_name', default='gpt2_finetuned.pt', type=str, help='finetuned_model_name')\n    parser.add_argument('--recopy_model', default=recopy_gpt2, type=str, help='Reset the model to the original pretrained GPT-2 weights after each iteration')\n    generate_n_pairs(context_len=32, max_steps=10, size_objective_set=100, min_len=1026, trim=True, data_file='data/tokenized_stories_train_wikitext103.jbl', igf_data_file='igf_context_pairs.jbl')\n    secondary_learner_train_data = joblib.load('data/IGF_values.jbl')\n    secondary_learner = training_secondary_learner(secondary_learner_train_data, secondary_learner_max_epochs=15, secondary_learner_batch_size=128, eval_freq=100, igf_model_path='igf_model.pt')\n    model = GPT2LMHeadModel.from_pretrained('gpt2')\n    set_seed(42)\n    (train_dataset, test_dataset) = generate_datasets(context_len=32, file='data/tokenized_stories_train_wikitext103.jbl', number=100, min_len=1026, trim=True)\n    finetune(model, train_dataset, test_dataset, context_len=32, max_steps=1000, batch_size=16, threshold=1.0, recopy_model=recopy_gpt2, secondary_learner=secondary_learner, eval_interval=10, finetuned_model_name='gpt2_finetuned.pt')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser(description='Fine-tune a transformer model with IGF on a language modeling task')\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain data files for WikiText.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--data_file', type=str, default=None, help='A jbl file containing tokenized data which can be split as objective dataset, train_dataset and test_dataset.')\n    parser.add_argument('--igf_data_file', type=str, default=None, help='A jbl file containing the context and information gain pairs to train secondary learner.')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the final fine-tuned model is stored.')\n    parser.add_argument('--tokenizer_name', default=None, type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--context_len', default=32, type=int, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.')\n    parser.add_argument('--size_objective_set', default=100, type=int, help='number of articles that are long enough to be used as our objective set')\n    parser.add_argument('--eval_freq', default=100, type=int, help='secondary model evaluation is triggered at eval_freq')\n    parser.add_argument('--max_steps', default=1000, type=int, help='To calculate training epochs')\n    parser.add_argument('--secondary_learner_batch_size', default=128, type=int, help='batch size of training data for secondary learner')\n    parser.add_argument('--batch_size', default=16, type=int, help='batch size of training data of language model(gpt2) ')\n    parser.add_argument('--eval_interval', default=10, type=int, help='decay the selectivity of our secondary learner filter from 1 standard deviation above average to 1 below average after 10 batches')\n    parser.add_argument('--number', default=100, type=int, help='The number of examples split to be used as objective_set/test_data')\n    parser.add_argument('--min_len', default=1026, type=int, help='The minimum length of the article to be used as objective set')\n    parser.add_argument('--secondary_learner_max_epochs', default=15, type=int, help='number of epochs to train secondary learner')\n    parser.add_argument('--trim', default=True, type=bool, help='truncate the example if it exceeds context length')\n    parser.add_argument('--threshold', default=1.0, type=float, help='The threshold value used by secondary learner to filter the train_data and allow only informative data as input to the model')\n    parser.add_argument('--finetuned_model_name', default='gpt2_finetuned.pt', type=str, help='finetuned_model_name')\n    parser.add_argument('--recopy_model', default=recopy_gpt2, type=str, help='Reset the model to the original pretrained GPT-2 weights after each iteration')\n    generate_n_pairs(context_len=32, max_steps=10, size_objective_set=100, min_len=1026, trim=True, data_file='data/tokenized_stories_train_wikitext103.jbl', igf_data_file='igf_context_pairs.jbl')\n    secondary_learner_train_data = joblib.load('data/IGF_values.jbl')\n    secondary_learner = training_secondary_learner(secondary_learner_train_data, secondary_learner_max_epochs=15, secondary_learner_batch_size=128, eval_freq=100, igf_model_path='igf_model.pt')\n    model = GPT2LMHeadModel.from_pretrained('gpt2')\n    set_seed(42)\n    (train_dataset, test_dataset) = generate_datasets(context_len=32, file='data/tokenized_stories_train_wikitext103.jbl', number=100, min_len=1026, trim=True)\n    finetune(model, train_dataset, test_dataset, context_len=32, max_steps=1000, batch_size=16, threshold=1.0, recopy_model=recopy_gpt2, secondary_learner=secondary_learner, eval_interval=10, finetuned_model_name='gpt2_finetuned.pt')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser(description='Fine-tune a transformer model with IGF on a language modeling task')\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain data files for WikiText.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--data_file', type=str, default=None, help='A jbl file containing tokenized data which can be split as objective dataset, train_dataset and test_dataset.')\n    parser.add_argument('--igf_data_file', type=str, default=None, help='A jbl file containing the context and information gain pairs to train secondary learner.')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the final fine-tuned model is stored.')\n    parser.add_argument('--tokenizer_name', default=None, type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--context_len', default=32, type=int, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.')\n    parser.add_argument('--size_objective_set', default=100, type=int, help='number of articles that are long enough to be used as our objective set')\n    parser.add_argument('--eval_freq', default=100, type=int, help='secondary model evaluation is triggered at eval_freq')\n    parser.add_argument('--max_steps', default=1000, type=int, help='To calculate training epochs')\n    parser.add_argument('--secondary_learner_batch_size', default=128, type=int, help='batch size of training data for secondary learner')\n    parser.add_argument('--batch_size', default=16, type=int, help='batch size of training data of language model(gpt2) ')\n    parser.add_argument('--eval_interval', default=10, type=int, help='decay the selectivity of our secondary learner filter from 1 standard deviation above average to 1 below average after 10 batches')\n    parser.add_argument('--number', default=100, type=int, help='The number of examples split to be used as objective_set/test_data')\n    parser.add_argument('--min_len', default=1026, type=int, help='The minimum length of the article to be used as objective set')\n    parser.add_argument('--secondary_learner_max_epochs', default=15, type=int, help='number of epochs to train secondary learner')\n    parser.add_argument('--trim', default=True, type=bool, help='truncate the example if it exceeds context length')\n    parser.add_argument('--threshold', default=1.0, type=float, help='The threshold value used by secondary learner to filter the train_data and allow only informative data as input to the model')\n    parser.add_argument('--finetuned_model_name', default='gpt2_finetuned.pt', type=str, help='finetuned_model_name')\n    parser.add_argument('--recopy_model', default=recopy_gpt2, type=str, help='Reset the model to the original pretrained GPT-2 weights after each iteration')\n    generate_n_pairs(context_len=32, max_steps=10, size_objective_set=100, min_len=1026, trim=True, data_file='data/tokenized_stories_train_wikitext103.jbl', igf_data_file='igf_context_pairs.jbl')\n    secondary_learner_train_data = joblib.load('data/IGF_values.jbl')\n    secondary_learner = training_secondary_learner(secondary_learner_train_data, secondary_learner_max_epochs=15, secondary_learner_batch_size=128, eval_freq=100, igf_model_path='igf_model.pt')\n    model = GPT2LMHeadModel.from_pretrained('gpt2')\n    set_seed(42)\n    (train_dataset, test_dataset) = generate_datasets(context_len=32, file='data/tokenized_stories_train_wikitext103.jbl', number=100, min_len=1026, trim=True)\n    finetune(model, train_dataset, test_dataset, context_len=32, max_steps=1000, batch_size=16, threshold=1.0, recopy_model=recopy_gpt2, secondary_learner=secondary_learner, eval_interval=10, finetuned_model_name='gpt2_finetuned.pt')",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser(description='Fine-tune a transformer model with IGF on a language modeling task')\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain data files for WikiText.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--data_file', type=str, default=None, help='A jbl file containing tokenized data which can be split as objective dataset, train_dataset and test_dataset.')\n    parser.add_argument('--igf_data_file', type=str, default=None, help='A jbl file containing the context and information gain pairs to train secondary learner.')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the final fine-tuned model is stored.')\n    parser.add_argument('--tokenizer_name', default=None, type=str, help='Pretrained tokenizer name or path if not the same as model_name')\n    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')\n    parser.add_argument('--context_len', default=32, type=int, help='The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.')\n    parser.add_argument('--size_objective_set', default=100, type=int, help='number of articles that are long enough to be used as our objective set')\n    parser.add_argument('--eval_freq', default=100, type=int, help='secondary model evaluation is triggered at eval_freq')\n    parser.add_argument('--max_steps', default=1000, type=int, help='To calculate training epochs')\n    parser.add_argument('--secondary_learner_batch_size', default=128, type=int, help='batch size of training data for secondary learner')\n    parser.add_argument('--batch_size', default=16, type=int, help='batch size of training data of language model(gpt2) ')\n    parser.add_argument('--eval_interval', default=10, type=int, help='decay the selectivity of our secondary learner filter from 1 standard deviation above average to 1 below average after 10 batches')\n    parser.add_argument('--number', default=100, type=int, help='The number of examples split to be used as objective_set/test_data')\n    parser.add_argument('--min_len', default=1026, type=int, help='The minimum length of the article to be used as objective set')\n    parser.add_argument('--secondary_learner_max_epochs', default=15, type=int, help='number of epochs to train secondary learner')\n    parser.add_argument('--trim', default=True, type=bool, help='truncate the example if it exceeds context length')\n    parser.add_argument('--threshold', default=1.0, type=float, help='The threshold value used by secondary learner to filter the train_data and allow only informative data as input to the model')\n    parser.add_argument('--finetuned_model_name', default='gpt2_finetuned.pt', type=str, help='finetuned_model_name')\n    parser.add_argument('--recopy_model', default=recopy_gpt2, type=str, help='Reset the model to the original pretrained GPT-2 weights after each iteration')\n    generate_n_pairs(context_len=32, max_steps=10, size_objective_set=100, min_len=1026, trim=True, data_file='data/tokenized_stories_train_wikitext103.jbl', igf_data_file='igf_context_pairs.jbl')\n    secondary_learner_train_data = joblib.load('data/IGF_values.jbl')\n    secondary_learner = training_secondary_learner(secondary_learner_train_data, secondary_learner_max_epochs=15, secondary_learner_batch_size=128, eval_freq=100, igf_model_path='igf_model.pt')\n    model = GPT2LMHeadModel.from_pretrained('gpt2')\n    set_seed(42)\n    (train_dataset, test_dataset) = generate_datasets(context_len=32, file='data/tokenized_stories_train_wikitext103.jbl', number=100, min_len=1026, trim=True)\n    finetune(model, train_dataset, test_dataset, context_len=32, max_steps=1000, batch_size=16, threshold=1.0, recopy_model=recopy_gpt2, secondary_learner=secondary_learner, eval_interval=10, finetuned_model_name='gpt2_finetuned.pt')"
        ]
    }
]