[
    {
        "func_name": "__init__",
        "original": "def __init__(self, observation_space, action_space, config, existing_model=None, existing_inputs=None):\n    base.enable_eager_execution_if_necessary()\n    if not config.get('_enable_new_api_stack', False):\n        VTraceClipGradients.__init__(self)\n        VTraceOptimizer.__init__(self)\n    base.__init__(self, observation_space, action_space, config, existing_inputs=existing_inputs, existing_model=existing_model)\n    LearningRateSchedule.__init__(self, config['lr'], config['lr_schedule'])\n    EntropyCoeffSchedule.__init__(self, config['entropy_coeff'], config['entropy_coeff_schedule'])\n    ValueNetworkMixin.__init__(self, config)\n    KLCoeffMixin.__init__(self, config)\n    if not config.get('_enable_new_api_stack', False):\n        GradStatsMixin.__init__(self)\n    self.maybe_initialize_optimizer_and_loss()\n    TargetNetworkMixin.__init__(self)",
        "mutated": [
            "def __init__(self, observation_space, action_space, config, existing_model=None, existing_inputs=None):\n    if False:\n        i = 10\n    base.enable_eager_execution_if_necessary()\n    if not config.get('_enable_new_api_stack', False):\n        VTraceClipGradients.__init__(self)\n        VTraceOptimizer.__init__(self)\n    base.__init__(self, observation_space, action_space, config, existing_inputs=existing_inputs, existing_model=existing_model)\n    LearningRateSchedule.__init__(self, config['lr'], config['lr_schedule'])\n    EntropyCoeffSchedule.__init__(self, config['entropy_coeff'], config['entropy_coeff_schedule'])\n    ValueNetworkMixin.__init__(self, config)\n    KLCoeffMixin.__init__(self, config)\n    if not config.get('_enable_new_api_stack', False):\n        GradStatsMixin.__init__(self)\n    self.maybe_initialize_optimizer_and_loss()\n    TargetNetworkMixin.__init__(self)",
            "def __init__(self, observation_space, action_space, config, existing_model=None, existing_inputs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    base.enable_eager_execution_if_necessary()\n    if not config.get('_enable_new_api_stack', False):\n        VTraceClipGradients.__init__(self)\n        VTraceOptimizer.__init__(self)\n    base.__init__(self, observation_space, action_space, config, existing_inputs=existing_inputs, existing_model=existing_model)\n    LearningRateSchedule.__init__(self, config['lr'], config['lr_schedule'])\n    EntropyCoeffSchedule.__init__(self, config['entropy_coeff'], config['entropy_coeff_schedule'])\n    ValueNetworkMixin.__init__(self, config)\n    KLCoeffMixin.__init__(self, config)\n    if not config.get('_enable_new_api_stack', False):\n        GradStatsMixin.__init__(self)\n    self.maybe_initialize_optimizer_and_loss()\n    TargetNetworkMixin.__init__(self)",
            "def __init__(self, observation_space, action_space, config, existing_model=None, existing_inputs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    base.enable_eager_execution_if_necessary()\n    if not config.get('_enable_new_api_stack', False):\n        VTraceClipGradients.__init__(self)\n        VTraceOptimizer.__init__(self)\n    base.__init__(self, observation_space, action_space, config, existing_inputs=existing_inputs, existing_model=existing_model)\n    LearningRateSchedule.__init__(self, config['lr'], config['lr_schedule'])\n    EntropyCoeffSchedule.__init__(self, config['entropy_coeff'], config['entropy_coeff_schedule'])\n    ValueNetworkMixin.__init__(self, config)\n    KLCoeffMixin.__init__(self, config)\n    if not config.get('_enable_new_api_stack', False):\n        GradStatsMixin.__init__(self)\n    self.maybe_initialize_optimizer_and_loss()\n    TargetNetworkMixin.__init__(self)",
            "def __init__(self, observation_space, action_space, config, existing_model=None, existing_inputs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    base.enable_eager_execution_if_necessary()\n    if not config.get('_enable_new_api_stack', False):\n        VTraceClipGradients.__init__(self)\n        VTraceOptimizer.__init__(self)\n    base.__init__(self, observation_space, action_space, config, existing_inputs=existing_inputs, existing_model=existing_model)\n    LearningRateSchedule.__init__(self, config['lr'], config['lr_schedule'])\n    EntropyCoeffSchedule.__init__(self, config['entropy_coeff'], config['entropy_coeff_schedule'])\n    ValueNetworkMixin.__init__(self, config)\n    KLCoeffMixin.__init__(self, config)\n    if not config.get('_enable_new_api_stack', False):\n        GradStatsMixin.__init__(self)\n    self.maybe_initialize_optimizer_and_loss()\n    TargetNetworkMixin.__init__(self)",
            "def __init__(self, observation_space, action_space, config, existing_model=None, existing_inputs=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    base.enable_eager_execution_if_necessary()\n    if not config.get('_enable_new_api_stack', False):\n        VTraceClipGradients.__init__(self)\n        VTraceOptimizer.__init__(self)\n    base.__init__(self, observation_space, action_space, config, existing_inputs=existing_inputs, existing_model=existing_model)\n    LearningRateSchedule.__init__(self, config['lr'], config['lr_schedule'])\n    EntropyCoeffSchedule.__init__(self, config['entropy_coeff'], config['entropy_coeff_schedule'])\n    ValueNetworkMixin.__init__(self, config)\n    KLCoeffMixin.__init__(self, config)\n    if not config.get('_enable_new_api_stack', False):\n        GradStatsMixin.__init__(self)\n    self.maybe_initialize_optimizer_and_loss()\n    TargetNetworkMixin.__init__(self)"
        ]
    },
    {
        "func_name": "make_model",
        "original": "@override(base)\ndef make_model(self) -> ModelV2:\n    return make_appo_models(self)",
        "mutated": [
            "@override(base)\ndef make_model(self) -> ModelV2:\n    if False:\n        i = 10\n    return make_appo_models(self)",
            "@override(base)\ndef make_model(self) -> ModelV2:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return make_appo_models(self)",
            "@override(base)\ndef make_model(self) -> ModelV2:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return make_appo_models(self)",
            "@override(base)\ndef make_model(self) -> ModelV2:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return make_appo_models(self)",
            "@override(base)\ndef make_model(self) -> ModelV2:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return make_appo_models(self)"
        ]
    },
    {
        "func_name": "make_time_major",
        "original": "def make_time_major(*args, **kw):\n    return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)",
        "mutated": [
            "def make_time_major(*args, **kw):\n    if False:\n        i = 10\n    return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)",
            "def make_time_major(*args, **kw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)",
            "def make_time_major(*args, **kw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)",
            "def make_time_major(*args, **kw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)",
            "def make_time_major(*args, **kw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)"
        ]
    },
    {
        "func_name": "reduce_mean_valid",
        "original": "def reduce_mean_valid(t):\n    return tf.reduce_mean(tf.boolean_mask(t, mask))",
        "mutated": [
            "def reduce_mean_valid(t):\n    if False:\n        i = 10\n    return tf.reduce_mean(tf.boolean_mask(t, mask))",
            "def reduce_mean_valid(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return tf.reduce_mean(tf.boolean_mask(t, mask))",
            "def reduce_mean_valid(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return tf.reduce_mean(tf.boolean_mask(t, mask))",
            "def reduce_mean_valid(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return tf.reduce_mean(tf.boolean_mask(t, mask))",
            "def reduce_mean_valid(t):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return tf.reduce_mean(tf.boolean_mask(t, mask))"
        ]
    },
    {
        "func_name": "loss",
        "original": "@override(base)\ndef loss(self, model: Union[ModelV2, 'tf.keras.Model'], dist_class: Type[TFActionDistribution], train_batch: SampleBatch) -> Union[TensorType, List[TensorType]]:\n    (model_out, _) = model(train_batch)\n    action_dist = dist_class(model_out, model)\n    if isinstance(self.action_space, gym.spaces.Discrete):\n        is_multidiscrete = False\n        output_hidden_shape = [self.action_space.n]\n    elif isinstance(self.action_space, gym.spaces.multi_discrete.MultiDiscrete):\n        is_multidiscrete = True\n        output_hidden_shape = self.action_space.nvec.astype(np.int32)\n    else:\n        is_multidiscrete = False\n        output_hidden_shape = 1\n\n    def make_time_major(*args, **kw):\n        return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)\n    actions = train_batch[SampleBatch.ACTIONS]\n    dones = train_batch[SampleBatch.TERMINATEDS]\n    rewards = train_batch[SampleBatch.REWARDS]\n    behaviour_logits = train_batch[SampleBatch.ACTION_DIST_INPUTS]\n    (target_model_out, _) = self.target_model(train_batch)\n    prev_action_dist = dist_class(behaviour_logits, self.model)\n    values = self.model.value_function()\n    values_time_major = make_time_major(values)\n    bootstrap_values_time_major = make_time_major(train_batch[SampleBatch.VALUES_BOOTSTRAPPED])\n    bootstrap_value = bootstrap_values_time_major[-1]\n    if self.is_recurrent():\n        max_seq_len = tf.reduce_max(train_batch[SampleBatch.SEQ_LENS])\n        mask = tf.sequence_mask(train_batch[SampleBatch.SEQ_LENS], max_seq_len)\n        mask = tf.reshape(mask, [-1])\n        mask = make_time_major(mask)\n\n        def reduce_mean_valid(t):\n            return tf.reduce_mean(tf.boolean_mask(t, mask))\n    else:\n        reduce_mean_valid = tf.reduce_mean\n    if self.config['vtrace']:\n        logger.debug('Using V-Trace surrogate loss (vtrace=True)')\n        loss_actions = actions if is_multidiscrete else tf.expand_dims(actions, axis=1)\n        old_policy_behaviour_logits = tf.stop_gradient(target_model_out)\n        old_policy_action_dist = dist_class(old_policy_behaviour_logits, model)\n        mean_kl = make_time_major(old_policy_action_dist.multi_kl(action_dist))\n        unpacked_behaviour_logits = tf.split(behaviour_logits, output_hidden_shape, axis=1)\n        unpacked_old_policy_behaviour_logits = tf.split(old_policy_behaviour_logits, output_hidden_shape, axis=1)\n        with tf.device('/cpu:0'):\n            vtrace_returns = vtrace.multi_from_logits(behaviour_policy_logits=make_time_major(unpacked_behaviour_logits), target_policy_logits=make_time_major(unpacked_old_policy_behaviour_logits), actions=tf.unstack(make_time_major(loss_actions), axis=2), discounts=tf.cast(~make_time_major(tf.cast(dones, tf.bool)), tf.float32) * self.config['gamma'], rewards=make_time_major(rewards), values=values_time_major, bootstrap_value=bootstrap_value, dist_class=Categorical if is_multidiscrete else dist_class, model=model, clip_rho_threshold=tf.cast(self.config['vtrace_clip_rho_threshold'], tf.float32), clip_pg_rho_threshold=tf.cast(self.config['vtrace_clip_pg_rho_threshold'], tf.float32))\n        actions_logp = make_time_major(action_dist.logp(actions))\n        prev_actions_logp = make_time_major(prev_action_dist.logp(actions))\n        old_policy_actions_logp = make_time_major(old_policy_action_dist.logp(actions))\n        is_ratio = tf.clip_by_value(tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)\n        logp_ratio = is_ratio * tf.exp(actions_logp - prev_actions_logp)\n        self._is_ratio = is_ratio\n        advantages = vtrace_returns.pg_advantages\n        surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n        action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n        mean_kl_loss = reduce_mean_valid(action_kl)\n        mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n        value_targets = vtrace_returns.vs\n        delta = values_time_major - value_targets\n        mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n        actions_entropy = make_time_major(action_dist.multi_entropy())\n        mean_entropy = reduce_mean_valid(actions_entropy)\n    else:\n        logger.debug('Using PPO surrogate loss (vtrace=False)')\n        mean_kl = make_time_major(prev_action_dist.multi_kl(action_dist))\n        logp_ratio = tf.math.exp(make_time_major(action_dist.logp(actions)) - make_time_major(prev_action_dist.logp(actions)))\n        advantages = make_time_major(train_batch[Postprocessing.ADVANTAGES])\n        surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n        action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n        mean_kl_loss = reduce_mean_valid(action_kl)\n        mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n        value_targets = make_time_major(train_batch[Postprocessing.VALUE_TARGETS])\n        delta = values_time_major - value_targets\n        mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n        mean_entropy = reduce_mean_valid(make_time_major(action_dist.multi_entropy()))\n    total_loss = mean_policy_loss - mean_entropy * self.entropy_coeff\n    if self.config['use_kl_loss']:\n        total_loss += self.kl_coeff * mean_kl_loss\n    loss_wo_vf = total_loss\n    if not self.config['_separate_vf_optimizer']:\n        total_loss += mean_vf_loss * self.config['vf_loss_coeff']\n    self._total_loss = total_loss\n    self._loss_wo_vf = loss_wo_vf\n    self._mean_policy_loss = mean_policy_loss\n    self._mean_kl_loss = self._mean_kl = mean_kl_loss\n    self._mean_vf_loss = mean_vf_loss\n    self._mean_entropy = mean_entropy\n    self._value_targets = value_targets\n    if self.config['_separate_vf_optimizer']:\n        return (loss_wo_vf, mean_vf_loss)\n    else:\n        return total_loss",
        "mutated": [
            "@override(base)\ndef loss(self, model: Union[ModelV2, 'tf.keras.Model'], dist_class: Type[TFActionDistribution], train_batch: SampleBatch) -> Union[TensorType, List[TensorType]]:\n    if False:\n        i = 10\n    (model_out, _) = model(train_batch)\n    action_dist = dist_class(model_out, model)\n    if isinstance(self.action_space, gym.spaces.Discrete):\n        is_multidiscrete = False\n        output_hidden_shape = [self.action_space.n]\n    elif isinstance(self.action_space, gym.spaces.multi_discrete.MultiDiscrete):\n        is_multidiscrete = True\n        output_hidden_shape = self.action_space.nvec.astype(np.int32)\n    else:\n        is_multidiscrete = False\n        output_hidden_shape = 1\n\n    def make_time_major(*args, **kw):\n        return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)\n    actions = train_batch[SampleBatch.ACTIONS]\n    dones = train_batch[SampleBatch.TERMINATEDS]\n    rewards = train_batch[SampleBatch.REWARDS]\n    behaviour_logits = train_batch[SampleBatch.ACTION_DIST_INPUTS]\n    (target_model_out, _) = self.target_model(train_batch)\n    prev_action_dist = dist_class(behaviour_logits, self.model)\n    values = self.model.value_function()\n    values_time_major = make_time_major(values)\n    bootstrap_values_time_major = make_time_major(train_batch[SampleBatch.VALUES_BOOTSTRAPPED])\n    bootstrap_value = bootstrap_values_time_major[-1]\n    if self.is_recurrent():\n        max_seq_len = tf.reduce_max(train_batch[SampleBatch.SEQ_LENS])\n        mask = tf.sequence_mask(train_batch[SampleBatch.SEQ_LENS], max_seq_len)\n        mask = tf.reshape(mask, [-1])\n        mask = make_time_major(mask)\n\n        def reduce_mean_valid(t):\n            return tf.reduce_mean(tf.boolean_mask(t, mask))\n    else:\n        reduce_mean_valid = tf.reduce_mean\n    if self.config['vtrace']:\n        logger.debug('Using V-Trace surrogate loss (vtrace=True)')\n        loss_actions = actions if is_multidiscrete else tf.expand_dims(actions, axis=1)\n        old_policy_behaviour_logits = tf.stop_gradient(target_model_out)\n        old_policy_action_dist = dist_class(old_policy_behaviour_logits, model)\n        mean_kl = make_time_major(old_policy_action_dist.multi_kl(action_dist))\n        unpacked_behaviour_logits = tf.split(behaviour_logits, output_hidden_shape, axis=1)\n        unpacked_old_policy_behaviour_logits = tf.split(old_policy_behaviour_logits, output_hidden_shape, axis=1)\n        with tf.device('/cpu:0'):\n            vtrace_returns = vtrace.multi_from_logits(behaviour_policy_logits=make_time_major(unpacked_behaviour_logits), target_policy_logits=make_time_major(unpacked_old_policy_behaviour_logits), actions=tf.unstack(make_time_major(loss_actions), axis=2), discounts=tf.cast(~make_time_major(tf.cast(dones, tf.bool)), tf.float32) * self.config['gamma'], rewards=make_time_major(rewards), values=values_time_major, bootstrap_value=bootstrap_value, dist_class=Categorical if is_multidiscrete else dist_class, model=model, clip_rho_threshold=tf.cast(self.config['vtrace_clip_rho_threshold'], tf.float32), clip_pg_rho_threshold=tf.cast(self.config['vtrace_clip_pg_rho_threshold'], tf.float32))\n        actions_logp = make_time_major(action_dist.logp(actions))\n        prev_actions_logp = make_time_major(prev_action_dist.logp(actions))\n        old_policy_actions_logp = make_time_major(old_policy_action_dist.logp(actions))\n        is_ratio = tf.clip_by_value(tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)\n        logp_ratio = is_ratio * tf.exp(actions_logp - prev_actions_logp)\n        self._is_ratio = is_ratio\n        advantages = vtrace_returns.pg_advantages\n        surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n        action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n        mean_kl_loss = reduce_mean_valid(action_kl)\n        mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n        value_targets = vtrace_returns.vs\n        delta = values_time_major - value_targets\n        mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n        actions_entropy = make_time_major(action_dist.multi_entropy())\n        mean_entropy = reduce_mean_valid(actions_entropy)\n    else:\n        logger.debug('Using PPO surrogate loss (vtrace=False)')\n        mean_kl = make_time_major(prev_action_dist.multi_kl(action_dist))\n        logp_ratio = tf.math.exp(make_time_major(action_dist.logp(actions)) - make_time_major(prev_action_dist.logp(actions)))\n        advantages = make_time_major(train_batch[Postprocessing.ADVANTAGES])\n        surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n        action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n        mean_kl_loss = reduce_mean_valid(action_kl)\n        mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n        value_targets = make_time_major(train_batch[Postprocessing.VALUE_TARGETS])\n        delta = values_time_major - value_targets\n        mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n        mean_entropy = reduce_mean_valid(make_time_major(action_dist.multi_entropy()))\n    total_loss = mean_policy_loss - mean_entropy * self.entropy_coeff\n    if self.config['use_kl_loss']:\n        total_loss += self.kl_coeff * mean_kl_loss\n    loss_wo_vf = total_loss\n    if not self.config['_separate_vf_optimizer']:\n        total_loss += mean_vf_loss * self.config['vf_loss_coeff']\n    self._total_loss = total_loss\n    self._loss_wo_vf = loss_wo_vf\n    self._mean_policy_loss = mean_policy_loss\n    self._mean_kl_loss = self._mean_kl = mean_kl_loss\n    self._mean_vf_loss = mean_vf_loss\n    self._mean_entropy = mean_entropy\n    self._value_targets = value_targets\n    if self.config['_separate_vf_optimizer']:\n        return (loss_wo_vf, mean_vf_loss)\n    else:\n        return total_loss",
            "@override(base)\ndef loss(self, model: Union[ModelV2, 'tf.keras.Model'], dist_class: Type[TFActionDistribution], train_batch: SampleBatch) -> Union[TensorType, List[TensorType]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (model_out, _) = model(train_batch)\n    action_dist = dist_class(model_out, model)\n    if isinstance(self.action_space, gym.spaces.Discrete):\n        is_multidiscrete = False\n        output_hidden_shape = [self.action_space.n]\n    elif isinstance(self.action_space, gym.spaces.multi_discrete.MultiDiscrete):\n        is_multidiscrete = True\n        output_hidden_shape = self.action_space.nvec.astype(np.int32)\n    else:\n        is_multidiscrete = False\n        output_hidden_shape = 1\n\n    def make_time_major(*args, **kw):\n        return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)\n    actions = train_batch[SampleBatch.ACTIONS]\n    dones = train_batch[SampleBatch.TERMINATEDS]\n    rewards = train_batch[SampleBatch.REWARDS]\n    behaviour_logits = train_batch[SampleBatch.ACTION_DIST_INPUTS]\n    (target_model_out, _) = self.target_model(train_batch)\n    prev_action_dist = dist_class(behaviour_logits, self.model)\n    values = self.model.value_function()\n    values_time_major = make_time_major(values)\n    bootstrap_values_time_major = make_time_major(train_batch[SampleBatch.VALUES_BOOTSTRAPPED])\n    bootstrap_value = bootstrap_values_time_major[-1]\n    if self.is_recurrent():\n        max_seq_len = tf.reduce_max(train_batch[SampleBatch.SEQ_LENS])\n        mask = tf.sequence_mask(train_batch[SampleBatch.SEQ_LENS], max_seq_len)\n        mask = tf.reshape(mask, [-1])\n        mask = make_time_major(mask)\n\n        def reduce_mean_valid(t):\n            return tf.reduce_mean(tf.boolean_mask(t, mask))\n    else:\n        reduce_mean_valid = tf.reduce_mean\n    if self.config['vtrace']:\n        logger.debug('Using V-Trace surrogate loss (vtrace=True)')\n        loss_actions = actions if is_multidiscrete else tf.expand_dims(actions, axis=1)\n        old_policy_behaviour_logits = tf.stop_gradient(target_model_out)\n        old_policy_action_dist = dist_class(old_policy_behaviour_logits, model)\n        mean_kl = make_time_major(old_policy_action_dist.multi_kl(action_dist))\n        unpacked_behaviour_logits = tf.split(behaviour_logits, output_hidden_shape, axis=1)\n        unpacked_old_policy_behaviour_logits = tf.split(old_policy_behaviour_logits, output_hidden_shape, axis=1)\n        with tf.device('/cpu:0'):\n            vtrace_returns = vtrace.multi_from_logits(behaviour_policy_logits=make_time_major(unpacked_behaviour_logits), target_policy_logits=make_time_major(unpacked_old_policy_behaviour_logits), actions=tf.unstack(make_time_major(loss_actions), axis=2), discounts=tf.cast(~make_time_major(tf.cast(dones, tf.bool)), tf.float32) * self.config['gamma'], rewards=make_time_major(rewards), values=values_time_major, bootstrap_value=bootstrap_value, dist_class=Categorical if is_multidiscrete else dist_class, model=model, clip_rho_threshold=tf.cast(self.config['vtrace_clip_rho_threshold'], tf.float32), clip_pg_rho_threshold=tf.cast(self.config['vtrace_clip_pg_rho_threshold'], tf.float32))\n        actions_logp = make_time_major(action_dist.logp(actions))\n        prev_actions_logp = make_time_major(prev_action_dist.logp(actions))\n        old_policy_actions_logp = make_time_major(old_policy_action_dist.logp(actions))\n        is_ratio = tf.clip_by_value(tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)\n        logp_ratio = is_ratio * tf.exp(actions_logp - prev_actions_logp)\n        self._is_ratio = is_ratio\n        advantages = vtrace_returns.pg_advantages\n        surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n        action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n        mean_kl_loss = reduce_mean_valid(action_kl)\n        mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n        value_targets = vtrace_returns.vs\n        delta = values_time_major - value_targets\n        mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n        actions_entropy = make_time_major(action_dist.multi_entropy())\n        mean_entropy = reduce_mean_valid(actions_entropy)\n    else:\n        logger.debug('Using PPO surrogate loss (vtrace=False)')\n        mean_kl = make_time_major(prev_action_dist.multi_kl(action_dist))\n        logp_ratio = tf.math.exp(make_time_major(action_dist.logp(actions)) - make_time_major(prev_action_dist.logp(actions)))\n        advantages = make_time_major(train_batch[Postprocessing.ADVANTAGES])\n        surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n        action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n        mean_kl_loss = reduce_mean_valid(action_kl)\n        mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n        value_targets = make_time_major(train_batch[Postprocessing.VALUE_TARGETS])\n        delta = values_time_major - value_targets\n        mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n        mean_entropy = reduce_mean_valid(make_time_major(action_dist.multi_entropy()))\n    total_loss = mean_policy_loss - mean_entropy * self.entropy_coeff\n    if self.config['use_kl_loss']:\n        total_loss += self.kl_coeff * mean_kl_loss\n    loss_wo_vf = total_loss\n    if not self.config['_separate_vf_optimizer']:\n        total_loss += mean_vf_loss * self.config['vf_loss_coeff']\n    self._total_loss = total_loss\n    self._loss_wo_vf = loss_wo_vf\n    self._mean_policy_loss = mean_policy_loss\n    self._mean_kl_loss = self._mean_kl = mean_kl_loss\n    self._mean_vf_loss = mean_vf_loss\n    self._mean_entropy = mean_entropy\n    self._value_targets = value_targets\n    if self.config['_separate_vf_optimizer']:\n        return (loss_wo_vf, mean_vf_loss)\n    else:\n        return total_loss",
            "@override(base)\ndef loss(self, model: Union[ModelV2, 'tf.keras.Model'], dist_class: Type[TFActionDistribution], train_batch: SampleBatch) -> Union[TensorType, List[TensorType]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (model_out, _) = model(train_batch)\n    action_dist = dist_class(model_out, model)\n    if isinstance(self.action_space, gym.spaces.Discrete):\n        is_multidiscrete = False\n        output_hidden_shape = [self.action_space.n]\n    elif isinstance(self.action_space, gym.spaces.multi_discrete.MultiDiscrete):\n        is_multidiscrete = True\n        output_hidden_shape = self.action_space.nvec.astype(np.int32)\n    else:\n        is_multidiscrete = False\n        output_hidden_shape = 1\n\n    def make_time_major(*args, **kw):\n        return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)\n    actions = train_batch[SampleBatch.ACTIONS]\n    dones = train_batch[SampleBatch.TERMINATEDS]\n    rewards = train_batch[SampleBatch.REWARDS]\n    behaviour_logits = train_batch[SampleBatch.ACTION_DIST_INPUTS]\n    (target_model_out, _) = self.target_model(train_batch)\n    prev_action_dist = dist_class(behaviour_logits, self.model)\n    values = self.model.value_function()\n    values_time_major = make_time_major(values)\n    bootstrap_values_time_major = make_time_major(train_batch[SampleBatch.VALUES_BOOTSTRAPPED])\n    bootstrap_value = bootstrap_values_time_major[-1]\n    if self.is_recurrent():\n        max_seq_len = tf.reduce_max(train_batch[SampleBatch.SEQ_LENS])\n        mask = tf.sequence_mask(train_batch[SampleBatch.SEQ_LENS], max_seq_len)\n        mask = tf.reshape(mask, [-1])\n        mask = make_time_major(mask)\n\n        def reduce_mean_valid(t):\n            return tf.reduce_mean(tf.boolean_mask(t, mask))\n    else:\n        reduce_mean_valid = tf.reduce_mean\n    if self.config['vtrace']:\n        logger.debug('Using V-Trace surrogate loss (vtrace=True)')\n        loss_actions = actions if is_multidiscrete else tf.expand_dims(actions, axis=1)\n        old_policy_behaviour_logits = tf.stop_gradient(target_model_out)\n        old_policy_action_dist = dist_class(old_policy_behaviour_logits, model)\n        mean_kl = make_time_major(old_policy_action_dist.multi_kl(action_dist))\n        unpacked_behaviour_logits = tf.split(behaviour_logits, output_hidden_shape, axis=1)\n        unpacked_old_policy_behaviour_logits = tf.split(old_policy_behaviour_logits, output_hidden_shape, axis=1)\n        with tf.device('/cpu:0'):\n            vtrace_returns = vtrace.multi_from_logits(behaviour_policy_logits=make_time_major(unpacked_behaviour_logits), target_policy_logits=make_time_major(unpacked_old_policy_behaviour_logits), actions=tf.unstack(make_time_major(loss_actions), axis=2), discounts=tf.cast(~make_time_major(tf.cast(dones, tf.bool)), tf.float32) * self.config['gamma'], rewards=make_time_major(rewards), values=values_time_major, bootstrap_value=bootstrap_value, dist_class=Categorical if is_multidiscrete else dist_class, model=model, clip_rho_threshold=tf.cast(self.config['vtrace_clip_rho_threshold'], tf.float32), clip_pg_rho_threshold=tf.cast(self.config['vtrace_clip_pg_rho_threshold'], tf.float32))\n        actions_logp = make_time_major(action_dist.logp(actions))\n        prev_actions_logp = make_time_major(prev_action_dist.logp(actions))\n        old_policy_actions_logp = make_time_major(old_policy_action_dist.logp(actions))\n        is_ratio = tf.clip_by_value(tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)\n        logp_ratio = is_ratio * tf.exp(actions_logp - prev_actions_logp)\n        self._is_ratio = is_ratio\n        advantages = vtrace_returns.pg_advantages\n        surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n        action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n        mean_kl_loss = reduce_mean_valid(action_kl)\n        mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n        value_targets = vtrace_returns.vs\n        delta = values_time_major - value_targets\n        mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n        actions_entropy = make_time_major(action_dist.multi_entropy())\n        mean_entropy = reduce_mean_valid(actions_entropy)\n    else:\n        logger.debug('Using PPO surrogate loss (vtrace=False)')\n        mean_kl = make_time_major(prev_action_dist.multi_kl(action_dist))\n        logp_ratio = tf.math.exp(make_time_major(action_dist.logp(actions)) - make_time_major(prev_action_dist.logp(actions)))\n        advantages = make_time_major(train_batch[Postprocessing.ADVANTAGES])\n        surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n        action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n        mean_kl_loss = reduce_mean_valid(action_kl)\n        mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n        value_targets = make_time_major(train_batch[Postprocessing.VALUE_TARGETS])\n        delta = values_time_major - value_targets\n        mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n        mean_entropy = reduce_mean_valid(make_time_major(action_dist.multi_entropy()))\n    total_loss = mean_policy_loss - mean_entropy * self.entropy_coeff\n    if self.config['use_kl_loss']:\n        total_loss += self.kl_coeff * mean_kl_loss\n    loss_wo_vf = total_loss\n    if not self.config['_separate_vf_optimizer']:\n        total_loss += mean_vf_loss * self.config['vf_loss_coeff']\n    self._total_loss = total_loss\n    self._loss_wo_vf = loss_wo_vf\n    self._mean_policy_loss = mean_policy_loss\n    self._mean_kl_loss = self._mean_kl = mean_kl_loss\n    self._mean_vf_loss = mean_vf_loss\n    self._mean_entropy = mean_entropy\n    self._value_targets = value_targets\n    if self.config['_separate_vf_optimizer']:\n        return (loss_wo_vf, mean_vf_loss)\n    else:\n        return total_loss",
            "@override(base)\ndef loss(self, model: Union[ModelV2, 'tf.keras.Model'], dist_class: Type[TFActionDistribution], train_batch: SampleBatch) -> Union[TensorType, List[TensorType]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (model_out, _) = model(train_batch)\n    action_dist = dist_class(model_out, model)\n    if isinstance(self.action_space, gym.spaces.Discrete):\n        is_multidiscrete = False\n        output_hidden_shape = [self.action_space.n]\n    elif isinstance(self.action_space, gym.spaces.multi_discrete.MultiDiscrete):\n        is_multidiscrete = True\n        output_hidden_shape = self.action_space.nvec.astype(np.int32)\n    else:\n        is_multidiscrete = False\n        output_hidden_shape = 1\n\n    def make_time_major(*args, **kw):\n        return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)\n    actions = train_batch[SampleBatch.ACTIONS]\n    dones = train_batch[SampleBatch.TERMINATEDS]\n    rewards = train_batch[SampleBatch.REWARDS]\n    behaviour_logits = train_batch[SampleBatch.ACTION_DIST_INPUTS]\n    (target_model_out, _) = self.target_model(train_batch)\n    prev_action_dist = dist_class(behaviour_logits, self.model)\n    values = self.model.value_function()\n    values_time_major = make_time_major(values)\n    bootstrap_values_time_major = make_time_major(train_batch[SampleBatch.VALUES_BOOTSTRAPPED])\n    bootstrap_value = bootstrap_values_time_major[-1]\n    if self.is_recurrent():\n        max_seq_len = tf.reduce_max(train_batch[SampleBatch.SEQ_LENS])\n        mask = tf.sequence_mask(train_batch[SampleBatch.SEQ_LENS], max_seq_len)\n        mask = tf.reshape(mask, [-1])\n        mask = make_time_major(mask)\n\n        def reduce_mean_valid(t):\n            return tf.reduce_mean(tf.boolean_mask(t, mask))\n    else:\n        reduce_mean_valid = tf.reduce_mean\n    if self.config['vtrace']:\n        logger.debug('Using V-Trace surrogate loss (vtrace=True)')\n        loss_actions = actions if is_multidiscrete else tf.expand_dims(actions, axis=1)\n        old_policy_behaviour_logits = tf.stop_gradient(target_model_out)\n        old_policy_action_dist = dist_class(old_policy_behaviour_logits, model)\n        mean_kl = make_time_major(old_policy_action_dist.multi_kl(action_dist))\n        unpacked_behaviour_logits = tf.split(behaviour_logits, output_hidden_shape, axis=1)\n        unpacked_old_policy_behaviour_logits = tf.split(old_policy_behaviour_logits, output_hidden_shape, axis=1)\n        with tf.device('/cpu:0'):\n            vtrace_returns = vtrace.multi_from_logits(behaviour_policy_logits=make_time_major(unpacked_behaviour_logits), target_policy_logits=make_time_major(unpacked_old_policy_behaviour_logits), actions=tf.unstack(make_time_major(loss_actions), axis=2), discounts=tf.cast(~make_time_major(tf.cast(dones, tf.bool)), tf.float32) * self.config['gamma'], rewards=make_time_major(rewards), values=values_time_major, bootstrap_value=bootstrap_value, dist_class=Categorical if is_multidiscrete else dist_class, model=model, clip_rho_threshold=tf.cast(self.config['vtrace_clip_rho_threshold'], tf.float32), clip_pg_rho_threshold=tf.cast(self.config['vtrace_clip_pg_rho_threshold'], tf.float32))\n        actions_logp = make_time_major(action_dist.logp(actions))\n        prev_actions_logp = make_time_major(prev_action_dist.logp(actions))\n        old_policy_actions_logp = make_time_major(old_policy_action_dist.logp(actions))\n        is_ratio = tf.clip_by_value(tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)\n        logp_ratio = is_ratio * tf.exp(actions_logp - prev_actions_logp)\n        self._is_ratio = is_ratio\n        advantages = vtrace_returns.pg_advantages\n        surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n        action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n        mean_kl_loss = reduce_mean_valid(action_kl)\n        mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n        value_targets = vtrace_returns.vs\n        delta = values_time_major - value_targets\n        mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n        actions_entropy = make_time_major(action_dist.multi_entropy())\n        mean_entropy = reduce_mean_valid(actions_entropy)\n    else:\n        logger.debug('Using PPO surrogate loss (vtrace=False)')\n        mean_kl = make_time_major(prev_action_dist.multi_kl(action_dist))\n        logp_ratio = tf.math.exp(make_time_major(action_dist.logp(actions)) - make_time_major(prev_action_dist.logp(actions)))\n        advantages = make_time_major(train_batch[Postprocessing.ADVANTAGES])\n        surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n        action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n        mean_kl_loss = reduce_mean_valid(action_kl)\n        mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n        value_targets = make_time_major(train_batch[Postprocessing.VALUE_TARGETS])\n        delta = values_time_major - value_targets\n        mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n        mean_entropy = reduce_mean_valid(make_time_major(action_dist.multi_entropy()))\n    total_loss = mean_policy_loss - mean_entropy * self.entropy_coeff\n    if self.config['use_kl_loss']:\n        total_loss += self.kl_coeff * mean_kl_loss\n    loss_wo_vf = total_loss\n    if not self.config['_separate_vf_optimizer']:\n        total_loss += mean_vf_loss * self.config['vf_loss_coeff']\n    self._total_loss = total_loss\n    self._loss_wo_vf = loss_wo_vf\n    self._mean_policy_loss = mean_policy_loss\n    self._mean_kl_loss = self._mean_kl = mean_kl_loss\n    self._mean_vf_loss = mean_vf_loss\n    self._mean_entropy = mean_entropy\n    self._value_targets = value_targets\n    if self.config['_separate_vf_optimizer']:\n        return (loss_wo_vf, mean_vf_loss)\n    else:\n        return total_loss",
            "@override(base)\ndef loss(self, model: Union[ModelV2, 'tf.keras.Model'], dist_class: Type[TFActionDistribution], train_batch: SampleBatch) -> Union[TensorType, List[TensorType]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (model_out, _) = model(train_batch)\n    action_dist = dist_class(model_out, model)\n    if isinstance(self.action_space, gym.spaces.Discrete):\n        is_multidiscrete = False\n        output_hidden_shape = [self.action_space.n]\n    elif isinstance(self.action_space, gym.spaces.multi_discrete.MultiDiscrete):\n        is_multidiscrete = True\n        output_hidden_shape = self.action_space.nvec.astype(np.int32)\n    else:\n        is_multidiscrete = False\n        output_hidden_shape = 1\n\n    def make_time_major(*args, **kw):\n        return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)\n    actions = train_batch[SampleBatch.ACTIONS]\n    dones = train_batch[SampleBatch.TERMINATEDS]\n    rewards = train_batch[SampleBatch.REWARDS]\n    behaviour_logits = train_batch[SampleBatch.ACTION_DIST_INPUTS]\n    (target_model_out, _) = self.target_model(train_batch)\n    prev_action_dist = dist_class(behaviour_logits, self.model)\n    values = self.model.value_function()\n    values_time_major = make_time_major(values)\n    bootstrap_values_time_major = make_time_major(train_batch[SampleBatch.VALUES_BOOTSTRAPPED])\n    bootstrap_value = bootstrap_values_time_major[-1]\n    if self.is_recurrent():\n        max_seq_len = tf.reduce_max(train_batch[SampleBatch.SEQ_LENS])\n        mask = tf.sequence_mask(train_batch[SampleBatch.SEQ_LENS], max_seq_len)\n        mask = tf.reshape(mask, [-1])\n        mask = make_time_major(mask)\n\n        def reduce_mean_valid(t):\n            return tf.reduce_mean(tf.boolean_mask(t, mask))\n    else:\n        reduce_mean_valid = tf.reduce_mean\n    if self.config['vtrace']:\n        logger.debug('Using V-Trace surrogate loss (vtrace=True)')\n        loss_actions = actions if is_multidiscrete else tf.expand_dims(actions, axis=1)\n        old_policy_behaviour_logits = tf.stop_gradient(target_model_out)\n        old_policy_action_dist = dist_class(old_policy_behaviour_logits, model)\n        mean_kl = make_time_major(old_policy_action_dist.multi_kl(action_dist))\n        unpacked_behaviour_logits = tf.split(behaviour_logits, output_hidden_shape, axis=1)\n        unpacked_old_policy_behaviour_logits = tf.split(old_policy_behaviour_logits, output_hidden_shape, axis=1)\n        with tf.device('/cpu:0'):\n            vtrace_returns = vtrace.multi_from_logits(behaviour_policy_logits=make_time_major(unpacked_behaviour_logits), target_policy_logits=make_time_major(unpacked_old_policy_behaviour_logits), actions=tf.unstack(make_time_major(loss_actions), axis=2), discounts=tf.cast(~make_time_major(tf.cast(dones, tf.bool)), tf.float32) * self.config['gamma'], rewards=make_time_major(rewards), values=values_time_major, bootstrap_value=bootstrap_value, dist_class=Categorical if is_multidiscrete else dist_class, model=model, clip_rho_threshold=tf.cast(self.config['vtrace_clip_rho_threshold'], tf.float32), clip_pg_rho_threshold=tf.cast(self.config['vtrace_clip_pg_rho_threshold'], tf.float32))\n        actions_logp = make_time_major(action_dist.logp(actions))\n        prev_actions_logp = make_time_major(prev_action_dist.logp(actions))\n        old_policy_actions_logp = make_time_major(old_policy_action_dist.logp(actions))\n        is_ratio = tf.clip_by_value(tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)\n        logp_ratio = is_ratio * tf.exp(actions_logp - prev_actions_logp)\n        self._is_ratio = is_ratio\n        advantages = vtrace_returns.pg_advantages\n        surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n        action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n        mean_kl_loss = reduce_mean_valid(action_kl)\n        mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n        value_targets = vtrace_returns.vs\n        delta = values_time_major - value_targets\n        mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n        actions_entropy = make_time_major(action_dist.multi_entropy())\n        mean_entropy = reduce_mean_valid(actions_entropy)\n    else:\n        logger.debug('Using PPO surrogate loss (vtrace=False)')\n        mean_kl = make_time_major(prev_action_dist.multi_kl(action_dist))\n        logp_ratio = tf.math.exp(make_time_major(action_dist.logp(actions)) - make_time_major(prev_action_dist.logp(actions)))\n        advantages = make_time_major(train_batch[Postprocessing.ADVANTAGES])\n        surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n        action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n        mean_kl_loss = reduce_mean_valid(action_kl)\n        mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n        value_targets = make_time_major(train_batch[Postprocessing.VALUE_TARGETS])\n        delta = values_time_major - value_targets\n        mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n        mean_entropy = reduce_mean_valid(make_time_major(action_dist.multi_entropy()))\n    total_loss = mean_policy_loss - mean_entropy * self.entropy_coeff\n    if self.config['use_kl_loss']:\n        total_loss += self.kl_coeff * mean_kl_loss\n    loss_wo_vf = total_loss\n    if not self.config['_separate_vf_optimizer']:\n        total_loss += mean_vf_loss * self.config['vf_loss_coeff']\n    self._total_loss = total_loss\n    self._loss_wo_vf = loss_wo_vf\n    self._mean_policy_loss = mean_policy_loss\n    self._mean_kl_loss = self._mean_kl = mean_kl_loss\n    self._mean_vf_loss = mean_vf_loss\n    self._mean_entropy = mean_entropy\n    self._value_targets = value_targets\n    if self.config['_separate_vf_optimizer']:\n        return (loss_wo_vf, mean_vf_loss)\n    else:\n        return total_loss"
        ]
    },
    {
        "func_name": "stats_fn",
        "original": "@override(base)\ndef stats_fn(self, train_batch: SampleBatch) -> Dict[str, TensorType]:\n    values_batched = _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), self.model.value_function())\n    stats_dict = {'cur_lr': tf.cast(self.cur_lr, tf.float64), 'total_loss': self._total_loss, 'policy_loss': self._mean_policy_loss, 'entropy': self._mean_entropy, 'var_gnorm': tf.linalg.global_norm(self.model.trainable_variables()), 'vf_loss': self._mean_vf_loss, 'vf_explained_var': explained_variance(tf.reshape(self._value_targets, [-1]), tf.reshape(values_batched, [-1])), 'entropy_coeff': tf.cast(self.entropy_coeff, tf.float64)}\n    if self.config['vtrace']:\n        (is_stat_mean, is_stat_var) = tf.nn.moments(self._is_ratio, [0, 1])\n        stats_dict['mean_IS'] = is_stat_mean\n        stats_dict['var_IS'] = is_stat_var\n    if self.config['use_kl_loss']:\n        stats_dict['kl'] = self._mean_kl_loss\n        stats_dict['KL_Coeff'] = self.kl_coeff\n    return stats_dict",
        "mutated": [
            "@override(base)\ndef stats_fn(self, train_batch: SampleBatch) -> Dict[str, TensorType]:\n    if False:\n        i = 10\n    values_batched = _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), self.model.value_function())\n    stats_dict = {'cur_lr': tf.cast(self.cur_lr, tf.float64), 'total_loss': self._total_loss, 'policy_loss': self._mean_policy_loss, 'entropy': self._mean_entropy, 'var_gnorm': tf.linalg.global_norm(self.model.trainable_variables()), 'vf_loss': self._mean_vf_loss, 'vf_explained_var': explained_variance(tf.reshape(self._value_targets, [-1]), tf.reshape(values_batched, [-1])), 'entropy_coeff': tf.cast(self.entropy_coeff, tf.float64)}\n    if self.config['vtrace']:\n        (is_stat_mean, is_stat_var) = tf.nn.moments(self._is_ratio, [0, 1])\n        stats_dict['mean_IS'] = is_stat_mean\n        stats_dict['var_IS'] = is_stat_var\n    if self.config['use_kl_loss']:\n        stats_dict['kl'] = self._mean_kl_loss\n        stats_dict['KL_Coeff'] = self.kl_coeff\n    return stats_dict",
            "@override(base)\ndef stats_fn(self, train_batch: SampleBatch) -> Dict[str, TensorType]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    values_batched = _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), self.model.value_function())\n    stats_dict = {'cur_lr': tf.cast(self.cur_lr, tf.float64), 'total_loss': self._total_loss, 'policy_loss': self._mean_policy_loss, 'entropy': self._mean_entropy, 'var_gnorm': tf.linalg.global_norm(self.model.trainable_variables()), 'vf_loss': self._mean_vf_loss, 'vf_explained_var': explained_variance(tf.reshape(self._value_targets, [-1]), tf.reshape(values_batched, [-1])), 'entropy_coeff': tf.cast(self.entropy_coeff, tf.float64)}\n    if self.config['vtrace']:\n        (is_stat_mean, is_stat_var) = tf.nn.moments(self._is_ratio, [0, 1])\n        stats_dict['mean_IS'] = is_stat_mean\n        stats_dict['var_IS'] = is_stat_var\n    if self.config['use_kl_loss']:\n        stats_dict['kl'] = self._mean_kl_loss\n        stats_dict['KL_Coeff'] = self.kl_coeff\n    return stats_dict",
            "@override(base)\ndef stats_fn(self, train_batch: SampleBatch) -> Dict[str, TensorType]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    values_batched = _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), self.model.value_function())\n    stats_dict = {'cur_lr': tf.cast(self.cur_lr, tf.float64), 'total_loss': self._total_loss, 'policy_loss': self._mean_policy_loss, 'entropy': self._mean_entropy, 'var_gnorm': tf.linalg.global_norm(self.model.trainable_variables()), 'vf_loss': self._mean_vf_loss, 'vf_explained_var': explained_variance(tf.reshape(self._value_targets, [-1]), tf.reshape(values_batched, [-1])), 'entropy_coeff': tf.cast(self.entropy_coeff, tf.float64)}\n    if self.config['vtrace']:\n        (is_stat_mean, is_stat_var) = tf.nn.moments(self._is_ratio, [0, 1])\n        stats_dict['mean_IS'] = is_stat_mean\n        stats_dict['var_IS'] = is_stat_var\n    if self.config['use_kl_loss']:\n        stats_dict['kl'] = self._mean_kl_loss\n        stats_dict['KL_Coeff'] = self.kl_coeff\n    return stats_dict",
            "@override(base)\ndef stats_fn(self, train_batch: SampleBatch) -> Dict[str, TensorType]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    values_batched = _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), self.model.value_function())\n    stats_dict = {'cur_lr': tf.cast(self.cur_lr, tf.float64), 'total_loss': self._total_loss, 'policy_loss': self._mean_policy_loss, 'entropy': self._mean_entropy, 'var_gnorm': tf.linalg.global_norm(self.model.trainable_variables()), 'vf_loss': self._mean_vf_loss, 'vf_explained_var': explained_variance(tf.reshape(self._value_targets, [-1]), tf.reshape(values_batched, [-1])), 'entropy_coeff': tf.cast(self.entropy_coeff, tf.float64)}\n    if self.config['vtrace']:\n        (is_stat_mean, is_stat_var) = tf.nn.moments(self._is_ratio, [0, 1])\n        stats_dict['mean_IS'] = is_stat_mean\n        stats_dict['var_IS'] = is_stat_var\n    if self.config['use_kl_loss']:\n        stats_dict['kl'] = self._mean_kl_loss\n        stats_dict['KL_Coeff'] = self.kl_coeff\n    return stats_dict",
            "@override(base)\ndef stats_fn(self, train_batch: SampleBatch) -> Dict[str, TensorType]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    values_batched = _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), self.model.value_function())\n    stats_dict = {'cur_lr': tf.cast(self.cur_lr, tf.float64), 'total_loss': self._total_loss, 'policy_loss': self._mean_policy_loss, 'entropy': self._mean_entropy, 'var_gnorm': tf.linalg.global_norm(self.model.trainable_variables()), 'vf_loss': self._mean_vf_loss, 'vf_explained_var': explained_variance(tf.reshape(self._value_targets, [-1]), tf.reshape(values_batched, [-1])), 'entropy_coeff': tf.cast(self.entropy_coeff, tf.float64)}\n    if self.config['vtrace']:\n        (is_stat_mean, is_stat_var) = tf.nn.moments(self._is_ratio, [0, 1])\n        stats_dict['mean_IS'] = is_stat_mean\n        stats_dict['var_IS'] = is_stat_var\n    if self.config['use_kl_loss']:\n        stats_dict['kl'] = self._mean_kl_loss\n        stats_dict['KL_Coeff'] = self.kl_coeff\n    return stats_dict"
        ]
    },
    {
        "func_name": "postprocess_trajectory",
        "original": "@override(base)\ndef postprocess_trajectory(self, sample_batch: SampleBatch, other_agent_batches: Optional[SampleBatch]=None, episode: Optional['Episode']=None):\n    if not self.config['vtrace']:\n        sample_batch = compute_gae_for_sample_batch(self, sample_batch, other_agent_batches, episode)\n    else:\n        sample_batch = compute_bootstrap_value(sample_batch, self)\n    return sample_batch",
        "mutated": [
            "@override(base)\ndef postprocess_trajectory(self, sample_batch: SampleBatch, other_agent_batches: Optional[SampleBatch]=None, episode: Optional['Episode']=None):\n    if False:\n        i = 10\n    if not self.config['vtrace']:\n        sample_batch = compute_gae_for_sample_batch(self, sample_batch, other_agent_batches, episode)\n    else:\n        sample_batch = compute_bootstrap_value(sample_batch, self)\n    return sample_batch",
            "@override(base)\ndef postprocess_trajectory(self, sample_batch: SampleBatch, other_agent_batches: Optional[SampleBatch]=None, episode: Optional['Episode']=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not self.config['vtrace']:\n        sample_batch = compute_gae_for_sample_batch(self, sample_batch, other_agent_batches, episode)\n    else:\n        sample_batch = compute_bootstrap_value(sample_batch, self)\n    return sample_batch",
            "@override(base)\ndef postprocess_trajectory(self, sample_batch: SampleBatch, other_agent_batches: Optional[SampleBatch]=None, episode: Optional['Episode']=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not self.config['vtrace']:\n        sample_batch = compute_gae_for_sample_batch(self, sample_batch, other_agent_batches, episode)\n    else:\n        sample_batch = compute_bootstrap_value(sample_batch, self)\n    return sample_batch",
            "@override(base)\ndef postprocess_trajectory(self, sample_batch: SampleBatch, other_agent_batches: Optional[SampleBatch]=None, episode: Optional['Episode']=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not self.config['vtrace']:\n        sample_batch = compute_gae_for_sample_batch(self, sample_batch, other_agent_batches, episode)\n    else:\n        sample_batch = compute_bootstrap_value(sample_batch, self)\n    return sample_batch",
            "@override(base)\ndef postprocess_trajectory(self, sample_batch: SampleBatch, other_agent_batches: Optional[SampleBatch]=None, episode: Optional['Episode']=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not self.config['vtrace']:\n        sample_batch = compute_gae_for_sample_batch(self, sample_batch, other_agent_batches, episode)\n    else:\n        sample_batch = compute_bootstrap_value(sample_batch, self)\n    return sample_batch"
        ]
    },
    {
        "func_name": "get_batch_divisibility_req",
        "original": "@override(base)\ndef get_batch_divisibility_req(self) -> int:\n    return self.config['rollout_fragment_length']",
        "mutated": [
            "@override(base)\ndef get_batch_divisibility_req(self) -> int:\n    if False:\n        i = 10\n    return self.config['rollout_fragment_length']",
            "@override(base)\ndef get_batch_divisibility_req(self) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self.config['rollout_fragment_length']",
            "@override(base)\ndef get_batch_divisibility_req(self) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self.config['rollout_fragment_length']",
            "@override(base)\ndef get_batch_divisibility_req(self) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self.config['rollout_fragment_length']",
            "@override(base)\ndef get_batch_divisibility_req(self) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self.config['rollout_fragment_length']"
        ]
    },
    {
        "func_name": "get_appo_tf_policy",
        "original": "def get_appo_tf_policy(name: str, base: type) -> type:\n    \"\"\"Construct an APPOTFPolicy inheriting either dynamic or eager base policies.\n\n    Args:\n        base: Base class for this policy. DynamicTFPolicyV2 or EagerTFPolicyV2.\n\n    Returns:\n        A TF Policy to be used with Impala.\n    \"\"\"\n\n    class APPOTFPolicy(VTraceClipGradients, VTraceOptimizer, LearningRateSchedule, KLCoeffMixin, EntropyCoeffSchedule, ValueNetworkMixin, TargetNetworkMixin, GradStatsMixin, base):\n\n        def __init__(self, observation_space, action_space, config, existing_model=None, existing_inputs=None):\n            base.enable_eager_execution_if_necessary()\n            if not config.get('_enable_new_api_stack', False):\n                VTraceClipGradients.__init__(self)\n                VTraceOptimizer.__init__(self)\n            base.__init__(self, observation_space, action_space, config, existing_inputs=existing_inputs, existing_model=existing_model)\n            LearningRateSchedule.__init__(self, config['lr'], config['lr_schedule'])\n            EntropyCoeffSchedule.__init__(self, config['entropy_coeff'], config['entropy_coeff_schedule'])\n            ValueNetworkMixin.__init__(self, config)\n            KLCoeffMixin.__init__(self, config)\n            if not config.get('_enable_new_api_stack', False):\n                GradStatsMixin.__init__(self)\n            self.maybe_initialize_optimizer_and_loss()\n            TargetNetworkMixin.__init__(self)\n\n        @override(base)\n        def make_model(self) -> ModelV2:\n            return make_appo_models(self)\n\n        @override(base)\n        def loss(self, model: Union[ModelV2, 'tf.keras.Model'], dist_class: Type[TFActionDistribution], train_batch: SampleBatch) -> Union[TensorType, List[TensorType]]:\n            (model_out, _) = model(train_batch)\n            action_dist = dist_class(model_out, model)\n            if isinstance(self.action_space, gym.spaces.Discrete):\n                is_multidiscrete = False\n                output_hidden_shape = [self.action_space.n]\n            elif isinstance(self.action_space, gym.spaces.multi_discrete.MultiDiscrete):\n                is_multidiscrete = True\n                output_hidden_shape = self.action_space.nvec.astype(np.int32)\n            else:\n                is_multidiscrete = False\n                output_hidden_shape = 1\n\n            def make_time_major(*args, **kw):\n                return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)\n            actions = train_batch[SampleBatch.ACTIONS]\n            dones = train_batch[SampleBatch.TERMINATEDS]\n            rewards = train_batch[SampleBatch.REWARDS]\n            behaviour_logits = train_batch[SampleBatch.ACTION_DIST_INPUTS]\n            (target_model_out, _) = self.target_model(train_batch)\n            prev_action_dist = dist_class(behaviour_logits, self.model)\n            values = self.model.value_function()\n            values_time_major = make_time_major(values)\n            bootstrap_values_time_major = make_time_major(train_batch[SampleBatch.VALUES_BOOTSTRAPPED])\n            bootstrap_value = bootstrap_values_time_major[-1]\n            if self.is_recurrent():\n                max_seq_len = tf.reduce_max(train_batch[SampleBatch.SEQ_LENS])\n                mask = tf.sequence_mask(train_batch[SampleBatch.SEQ_LENS], max_seq_len)\n                mask = tf.reshape(mask, [-1])\n                mask = make_time_major(mask)\n\n                def reduce_mean_valid(t):\n                    return tf.reduce_mean(tf.boolean_mask(t, mask))\n            else:\n                reduce_mean_valid = tf.reduce_mean\n            if self.config['vtrace']:\n                logger.debug('Using V-Trace surrogate loss (vtrace=True)')\n                loss_actions = actions if is_multidiscrete else tf.expand_dims(actions, axis=1)\n                old_policy_behaviour_logits = tf.stop_gradient(target_model_out)\n                old_policy_action_dist = dist_class(old_policy_behaviour_logits, model)\n                mean_kl = make_time_major(old_policy_action_dist.multi_kl(action_dist))\n                unpacked_behaviour_logits = tf.split(behaviour_logits, output_hidden_shape, axis=1)\n                unpacked_old_policy_behaviour_logits = tf.split(old_policy_behaviour_logits, output_hidden_shape, axis=1)\n                with tf.device('/cpu:0'):\n                    vtrace_returns = vtrace.multi_from_logits(behaviour_policy_logits=make_time_major(unpacked_behaviour_logits), target_policy_logits=make_time_major(unpacked_old_policy_behaviour_logits), actions=tf.unstack(make_time_major(loss_actions), axis=2), discounts=tf.cast(~make_time_major(tf.cast(dones, tf.bool)), tf.float32) * self.config['gamma'], rewards=make_time_major(rewards), values=values_time_major, bootstrap_value=bootstrap_value, dist_class=Categorical if is_multidiscrete else dist_class, model=model, clip_rho_threshold=tf.cast(self.config['vtrace_clip_rho_threshold'], tf.float32), clip_pg_rho_threshold=tf.cast(self.config['vtrace_clip_pg_rho_threshold'], tf.float32))\n                actions_logp = make_time_major(action_dist.logp(actions))\n                prev_actions_logp = make_time_major(prev_action_dist.logp(actions))\n                old_policy_actions_logp = make_time_major(old_policy_action_dist.logp(actions))\n                is_ratio = tf.clip_by_value(tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)\n                logp_ratio = is_ratio * tf.exp(actions_logp - prev_actions_logp)\n                self._is_ratio = is_ratio\n                advantages = vtrace_returns.pg_advantages\n                surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n                action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n                mean_kl_loss = reduce_mean_valid(action_kl)\n                mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n                value_targets = vtrace_returns.vs\n                delta = values_time_major - value_targets\n                mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n                actions_entropy = make_time_major(action_dist.multi_entropy())\n                mean_entropy = reduce_mean_valid(actions_entropy)\n            else:\n                logger.debug('Using PPO surrogate loss (vtrace=False)')\n                mean_kl = make_time_major(prev_action_dist.multi_kl(action_dist))\n                logp_ratio = tf.math.exp(make_time_major(action_dist.logp(actions)) - make_time_major(prev_action_dist.logp(actions)))\n                advantages = make_time_major(train_batch[Postprocessing.ADVANTAGES])\n                surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n                action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n                mean_kl_loss = reduce_mean_valid(action_kl)\n                mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n                value_targets = make_time_major(train_batch[Postprocessing.VALUE_TARGETS])\n                delta = values_time_major - value_targets\n                mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n                mean_entropy = reduce_mean_valid(make_time_major(action_dist.multi_entropy()))\n            total_loss = mean_policy_loss - mean_entropy * self.entropy_coeff\n            if self.config['use_kl_loss']:\n                total_loss += self.kl_coeff * mean_kl_loss\n            loss_wo_vf = total_loss\n            if not self.config['_separate_vf_optimizer']:\n                total_loss += mean_vf_loss * self.config['vf_loss_coeff']\n            self._total_loss = total_loss\n            self._loss_wo_vf = loss_wo_vf\n            self._mean_policy_loss = mean_policy_loss\n            self._mean_kl_loss = self._mean_kl = mean_kl_loss\n            self._mean_vf_loss = mean_vf_loss\n            self._mean_entropy = mean_entropy\n            self._value_targets = value_targets\n            if self.config['_separate_vf_optimizer']:\n                return (loss_wo_vf, mean_vf_loss)\n            else:\n                return total_loss\n\n        @override(base)\n        def stats_fn(self, train_batch: SampleBatch) -> Dict[str, TensorType]:\n            values_batched = _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), self.model.value_function())\n            stats_dict = {'cur_lr': tf.cast(self.cur_lr, tf.float64), 'total_loss': self._total_loss, 'policy_loss': self._mean_policy_loss, 'entropy': self._mean_entropy, 'var_gnorm': tf.linalg.global_norm(self.model.trainable_variables()), 'vf_loss': self._mean_vf_loss, 'vf_explained_var': explained_variance(tf.reshape(self._value_targets, [-1]), tf.reshape(values_batched, [-1])), 'entropy_coeff': tf.cast(self.entropy_coeff, tf.float64)}\n            if self.config['vtrace']:\n                (is_stat_mean, is_stat_var) = tf.nn.moments(self._is_ratio, [0, 1])\n                stats_dict['mean_IS'] = is_stat_mean\n                stats_dict['var_IS'] = is_stat_var\n            if self.config['use_kl_loss']:\n                stats_dict['kl'] = self._mean_kl_loss\n                stats_dict['KL_Coeff'] = self.kl_coeff\n            return stats_dict\n\n        @override(base)\n        def postprocess_trajectory(self, sample_batch: SampleBatch, other_agent_batches: Optional[SampleBatch]=None, episode: Optional['Episode']=None):\n            if not self.config['vtrace']:\n                sample_batch = compute_gae_for_sample_batch(self, sample_batch, other_agent_batches, episode)\n            else:\n                sample_batch = compute_bootstrap_value(sample_batch, self)\n            return sample_batch\n\n        @override(base)\n        def get_batch_divisibility_req(self) -> int:\n            return self.config['rollout_fragment_length']\n    APPOTFPolicy.__name__ = name\n    APPOTFPolicy.__qualname__ = name\n    return APPOTFPolicy",
        "mutated": [
            "def get_appo_tf_policy(name: str, base: type) -> type:\n    if False:\n        i = 10\n    'Construct an APPOTFPolicy inheriting either dynamic or eager base policies.\\n\\n    Args:\\n        base: Base class for this policy. DynamicTFPolicyV2 or EagerTFPolicyV2.\\n\\n    Returns:\\n        A TF Policy to be used with Impala.\\n    '\n\n    class APPOTFPolicy(VTraceClipGradients, VTraceOptimizer, LearningRateSchedule, KLCoeffMixin, EntropyCoeffSchedule, ValueNetworkMixin, TargetNetworkMixin, GradStatsMixin, base):\n\n        def __init__(self, observation_space, action_space, config, existing_model=None, existing_inputs=None):\n            base.enable_eager_execution_if_necessary()\n            if not config.get('_enable_new_api_stack', False):\n                VTraceClipGradients.__init__(self)\n                VTraceOptimizer.__init__(self)\n            base.__init__(self, observation_space, action_space, config, existing_inputs=existing_inputs, existing_model=existing_model)\n            LearningRateSchedule.__init__(self, config['lr'], config['lr_schedule'])\n            EntropyCoeffSchedule.__init__(self, config['entropy_coeff'], config['entropy_coeff_schedule'])\n            ValueNetworkMixin.__init__(self, config)\n            KLCoeffMixin.__init__(self, config)\n            if not config.get('_enable_new_api_stack', False):\n                GradStatsMixin.__init__(self)\n            self.maybe_initialize_optimizer_and_loss()\n            TargetNetworkMixin.__init__(self)\n\n        @override(base)\n        def make_model(self) -> ModelV2:\n            return make_appo_models(self)\n\n        @override(base)\n        def loss(self, model: Union[ModelV2, 'tf.keras.Model'], dist_class: Type[TFActionDistribution], train_batch: SampleBatch) -> Union[TensorType, List[TensorType]]:\n            (model_out, _) = model(train_batch)\n            action_dist = dist_class(model_out, model)\n            if isinstance(self.action_space, gym.spaces.Discrete):\n                is_multidiscrete = False\n                output_hidden_shape = [self.action_space.n]\n            elif isinstance(self.action_space, gym.spaces.multi_discrete.MultiDiscrete):\n                is_multidiscrete = True\n                output_hidden_shape = self.action_space.nvec.astype(np.int32)\n            else:\n                is_multidiscrete = False\n                output_hidden_shape = 1\n\n            def make_time_major(*args, **kw):\n                return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)\n            actions = train_batch[SampleBatch.ACTIONS]\n            dones = train_batch[SampleBatch.TERMINATEDS]\n            rewards = train_batch[SampleBatch.REWARDS]\n            behaviour_logits = train_batch[SampleBatch.ACTION_DIST_INPUTS]\n            (target_model_out, _) = self.target_model(train_batch)\n            prev_action_dist = dist_class(behaviour_logits, self.model)\n            values = self.model.value_function()\n            values_time_major = make_time_major(values)\n            bootstrap_values_time_major = make_time_major(train_batch[SampleBatch.VALUES_BOOTSTRAPPED])\n            bootstrap_value = bootstrap_values_time_major[-1]\n            if self.is_recurrent():\n                max_seq_len = tf.reduce_max(train_batch[SampleBatch.SEQ_LENS])\n                mask = tf.sequence_mask(train_batch[SampleBatch.SEQ_LENS], max_seq_len)\n                mask = tf.reshape(mask, [-1])\n                mask = make_time_major(mask)\n\n                def reduce_mean_valid(t):\n                    return tf.reduce_mean(tf.boolean_mask(t, mask))\n            else:\n                reduce_mean_valid = tf.reduce_mean\n            if self.config['vtrace']:\n                logger.debug('Using V-Trace surrogate loss (vtrace=True)')\n                loss_actions = actions if is_multidiscrete else tf.expand_dims(actions, axis=1)\n                old_policy_behaviour_logits = tf.stop_gradient(target_model_out)\n                old_policy_action_dist = dist_class(old_policy_behaviour_logits, model)\n                mean_kl = make_time_major(old_policy_action_dist.multi_kl(action_dist))\n                unpacked_behaviour_logits = tf.split(behaviour_logits, output_hidden_shape, axis=1)\n                unpacked_old_policy_behaviour_logits = tf.split(old_policy_behaviour_logits, output_hidden_shape, axis=1)\n                with tf.device('/cpu:0'):\n                    vtrace_returns = vtrace.multi_from_logits(behaviour_policy_logits=make_time_major(unpacked_behaviour_logits), target_policy_logits=make_time_major(unpacked_old_policy_behaviour_logits), actions=tf.unstack(make_time_major(loss_actions), axis=2), discounts=tf.cast(~make_time_major(tf.cast(dones, tf.bool)), tf.float32) * self.config['gamma'], rewards=make_time_major(rewards), values=values_time_major, bootstrap_value=bootstrap_value, dist_class=Categorical if is_multidiscrete else dist_class, model=model, clip_rho_threshold=tf.cast(self.config['vtrace_clip_rho_threshold'], tf.float32), clip_pg_rho_threshold=tf.cast(self.config['vtrace_clip_pg_rho_threshold'], tf.float32))\n                actions_logp = make_time_major(action_dist.logp(actions))\n                prev_actions_logp = make_time_major(prev_action_dist.logp(actions))\n                old_policy_actions_logp = make_time_major(old_policy_action_dist.logp(actions))\n                is_ratio = tf.clip_by_value(tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)\n                logp_ratio = is_ratio * tf.exp(actions_logp - prev_actions_logp)\n                self._is_ratio = is_ratio\n                advantages = vtrace_returns.pg_advantages\n                surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n                action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n                mean_kl_loss = reduce_mean_valid(action_kl)\n                mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n                value_targets = vtrace_returns.vs\n                delta = values_time_major - value_targets\n                mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n                actions_entropy = make_time_major(action_dist.multi_entropy())\n                mean_entropy = reduce_mean_valid(actions_entropy)\n            else:\n                logger.debug('Using PPO surrogate loss (vtrace=False)')\n                mean_kl = make_time_major(prev_action_dist.multi_kl(action_dist))\n                logp_ratio = tf.math.exp(make_time_major(action_dist.logp(actions)) - make_time_major(prev_action_dist.logp(actions)))\n                advantages = make_time_major(train_batch[Postprocessing.ADVANTAGES])\n                surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n                action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n                mean_kl_loss = reduce_mean_valid(action_kl)\n                mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n                value_targets = make_time_major(train_batch[Postprocessing.VALUE_TARGETS])\n                delta = values_time_major - value_targets\n                mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n                mean_entropy = reduce_mean_valid(make_time_major(action_dist.multi_entropy()))\n            total_loss = mean_policy_loss - mean_entropy * self.entropy_coeff\n            if self.config['use_kl_loss']:\n                total_loss += self.kl_coeff * mean_kl_loss\n            loss_wo_vf = total_loss\n            if not self.config['_separate_vf_optimizer']:\n                total_loss += mean_vf_loss * self.config['vf_loss_coeff']\n            self._total_loss = total_loss\n            self._loss_wo_vf = loss_wo_vf\n            self._mean_policy_loss = mean_policy_loss\n            self._mean_kl_loss = self._mean_kl = mean_kl_loss\n            self._mean_vf_loss = mean_vf_loss\n            self._mean_entropy = mean_entropy\n            self._value_targets = value_targets\n            if self.config['_separate_vf_optimizer']:\n                return (loss_wo_vf, mean_vf_loss)\n            else:\n                return total_loss\n\n        @override(base)\n        def stats_fn(self, train_batch: SampleBatch) -> Dict[str, TensorType]:\n            values_batched = _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), self.model.value_function())\n            stats_dict = {'cur_lr': tf.cast(self.cur_lr, tf.float64), 'total_loss': self._total_loss, 'policy_loss': self._mean_policy_loss, 'entropy': self._mean_entropy, 'var_gnorm': tf.linalg.global_norm(self.model.trainable_variables()), 'vf_loss': self._mean_vf_loss, 'vf_explained_var': explained_variance(tf.reshape(self._value_targets, [-1]), tf.reshape(values_batched, [-1])), 'entropy_coeff': tf.cast(self.entropy_coeff, tf.float64)}\n            if self.config['vtrace']:\n                (is_stat_mean, is_stat_var) = tf.nn.moments(self._is_ratio, [0, 1])\n                stats_dict['mean_IS'] = is_stat_mean\n                stats_dict['var_IS'] = is_stat_var\n            if self.config['use_kl_loss']:\n                stats_dict['kl'] = self._mean_kl_loss\n                stats_dict['KL_Coeff'] = self.kl_coeff\n            return stats_dict\n\n        @override(base)\n        def postprocess_trajectory(self, sample_batch: SampleBatch, other_agent_batches: Optional[SampleBatch]=None, episode: Optional['Episode']=None):\n            if not self.config['vtrace']:\n                sample_batch = compute_gae_for_sample_batch(self, sample_batch, other_agent_batches, episode)\n            else:\n                sample_batch = compute_bootstrap_value(sample_batch, self)\n            return sample_batch\n\n        @override(base)\n        def get_batch_divisibility_req(self) -> int:\n            return self.config['rollout_fragment_length']\n    APPOTFPolicy.__name__ = name\n    APPOTFPolicy.__qualname__ = name\n    return APPOTFPolicy",
            "def get_appo_tf_policy(name: str, base: type) -> type:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Construct an APPOTFPolicy inheriting either dynamic or eager base policies.\\n\\n    Args:\\n        base: Base class for this policy. DynamicTFPolicyV2 or EagerTFPolicyV2.\\n\\n    Returns:\\n        A TF Policy to be used with Impala.\\n    '\n\n    class APPOTFPolicy(VTraceClipGradients, VTraceOptimizer, LearningRateSchedule, KLCoeffMixin, EntropyCoeffSchedule, ValueNetworkMixin, TargetNetworkMixin, GradStatsMixin, base):\n\n        def __init__(self, observation_space, action_space, config, existing_model=None, existing_inputs=None):\n            base.enable_eager_execution_if_necessary()\n            if not config.get('_enable_new_api_stack', False):\n                VTraceClipGradients.__init__(self)\n                VTraceOptimizer.__init__(self)\n            base.__init__(self, observation_space, action_space, config, existing_inputs=existing_inputs, existing_model=existing_model)\n            LearningRateSchedule.__init__(self, config['lr'], config['lr_schedule'])\n            EntropyCoeffSchedule.__init__(self, config['entropy_coeff'], config['entropy_coeff_schedule'])\n            ValueNetworkMixin.__init__(self, config)\n            KLCoeffMixin.__init__(self, config)\n            if not config.get('_enable_new_api_stack', False):\n                GradStatsMixin.__init__(self)\n            self.maybe_initialize_optimizer_and_loss()\n            TargetNetworkMixin.__init__(self)\n\n        @override(base)\n        def make_model(self) -> ModelV2:\n            return make_appo_models(self)\n\n        @override(base)\n        def loss(self, model: Union[ModelV2, 'tf.keras.Model'], dist_class: Type[TFActionDistribution], train_batch: SampleBatch) -> Union[TensorType, List[TensorType]]:\n            (model_out, _) = model(train_batch)\n            action_dist = dist_class(model_out, model)\n            if isinstance(self.action_space, gym.spaces.Discrete):\n                is_multidiscrete = False\n                output_hidden_shape = [self.action_space.n]\n            elif isinstance(self.action_space, gym.spaces.multi_discrete.MultiDiscrete):\n                is_multidiscrete = True\n                output_hidden_shape = self.action_space.nvec.astype(np.int32)\n            else:\n                is_multidiscrete = False\n                output_hidden_shape = 1\n\n            def make_time_major(*args, **kw):\n                return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)\n            actions = train_batch[SampleBatch.ACTIONS]\n            dones = train_batch[SampleBatch.TERMINATEDS]\n            rewards = train_batch[SampleBatch.REWARDS]\n            behaviour_logits = train_batch[SampleBatch.ACTION_DIST_INPUTS]\n            (target_model_out, _) = self.target_model(train_batch)\n            prev_action_dist = dist_class(behaviour_logits, self.model)\n            values = self.model.value_function()\n            values_time_major = make_time_major(values)\n            bootstrap_values_time_major = make_time_major(train_batch[SampleBatch.VALUES_BOOTSTRAPPED])\n            bootstrap_value = bootstrap_values_time_major[-1]\n            if self.is_recurrent():\n                max_seq_len = tf.reduce_max(train_batch[SampleBatch.SEQ_LENS])\n                mask = tf.sequence_mask(train_batch[SampleBatch.SEQ_LENS], max_seq_len)\n                mask = tf.reshape(mask, [-1])\n                mask = make_time_major(mask)\n\n                def reduce_mean_valid(t):\n                    return tf.reduce_mean(tf.boolean_mask(t, mask))\n            else:\n                reduce_mean_valid = tf.reduce_mean\n            if self.config['vtrace']:\n                logger.debug('Using V-Trace surrogate loss (vtrace=True)')\n                loss_actions = actions if is_multidiscrete else tf.expand_dims(actions, axis=1)\n                old_policy_behaviour_logits = tf.stop_gradient(target_model_out)\n                old_policy_action_dist = dist_class(old_policy_behaviour_logits, model)\n                mean_kl = make_time_major(old_policy_action_dist.multi_kl(action_dist))\n                unpacked_behaviour_logits = tf.split(behaviour_logits, output_hidden_shape, axis=1)\n                unpacked_old_policy_behaviour_logits = tf.split(old_policy_behaviour_logits, output_hidden_shape, axis=1)\n                with tf.device('/cpu:0'):\n                    vtrace_returns = vtrace.multi_from_logits(behaviour_policy_logits=make_time_major(unpacked_behaviour_logits), target_policy_logits=make_time_major(unpacked_old_policy_behaviour_logits), actions=tf.unstack(make_time_major(loss_actions), axis=2), discounts=tf.cast(~make_time_major(tf.cast(dones, tf.bool)), tf.float32) * self.config['gamma'], rewards=make_time_major(rewards), values=values_time_major, bootstrap_value=bootstrap_value, dist_class=Categorical if is_multidiscrete else dist_class, model=model, clip_rho_threshold=tf.cast(self.config['vtrace_clip_rho_threshold'], tf.float32), clip_pg_rho_threshold=tf.cast(self.config['vtrace_clip_pg_rho_threshold'], tf.float32))\n                actions_logp = make_time_major(action_dist.logp(actions))\n                prev_actions_logp = make_time_major(prev_action_dist.logp(actions))\n                old_policy_actions_logp = make_time_major(old_policy_action_dist.logp(actions))\n                is_ratio = tf.clip_by_value(tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)\n                logp_ratio = is_ratio * tf.exp(actions_logp - prev_actions_logp)\n                self._is_ratio = is_ratio\n                advantages = vtrace_returns.pg_advantages\n                surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n                action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n                mean_kl_loss = reduce_mean_valid(action_kl)\n                mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n                value_targets = vtrace_returns.vs\n                delta = values_time_major - value_targets\n                mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n                actions_entropy = make_time_major(action_dist.multi_entropy())\n                mean_entropy = reduce_mean_valid(actions_entropy)\n            else:\n                logger.debug('Using PPO surrogate loss (vtrace=False)')\n                mean_kl = make_time_major(prev_action_dist.multi_kl(action_dist))\n                logp_ratio = tf.math.exp(make_time_major(action_dist.logp(actions)) - make_time_major(prev_action_dist.logp(actions)))\n                advantages = make_time_major(train_batch[Postprocessing.ADVANTAGES])\n                surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n                action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n                mean_kl_loss = reduce_mean_valid(action_kl)\n                mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n                value_targets = make_time_major(train_batch[Postprocessing.VALUE_TARGETS])\n                delta = values_time_major - value_targets\n                mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n                mean_entropy = reduce_mean_valid(make_time_major(action_dist.multi_entropy()))\n            total_loss = mean_policy_loss - mean_entropy * self.entropy_coeff\n            if self.config['use_kl_loss']:\n                total_loss += self.kl_coeff * mean_kl_loss\n            loss_wo_vf = total_loss\n            if not self.config['_separate_vf_optimizer']:\n                total_loss += mean_vf_loss * self.config['vf_loss_coeff']\n            self._total_loss = total_loss\n            self._loss_wo_vf = loss_wo_vf\n            self._mean_policy_loss = mean_policy_loss\n            self._mean_kl_loss = self._mean_kl = mean_kl_loss\n            self._mean_vf_loss = mean_vf_loss\n            self._mean_entropy = mean_entropy\n            self._value_targets = value_targets\n            if self.config['_separate_vf_optimizer']:\n                return (loss_wo_vf, mean_vf_loss)\n            else:\n                return total_loss\n\n        @override(base)\n        def stats_fn(self, train_batch: SampleBatch) -> Dict[str, TensorType]:\n            values_batched = _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), self.model.value_function())\n            stats_dict = {'cur_lr': tf.cast(self.cur_lr, tf.float64), 'total_loss': self._total_loss, 'policy_loss': self._mean_policy_loss, 'entropy': self._mean_entropy, 'var_gnorm': tf.linalg.global_norm(self.model.trainable_variables()), 'vf_loss': self._mean_vf_loss, 'vf_explained_var': explained_variance(tf.reshape(self._value_targets, [-1]), tf.reshape(values_batched, [-1])), 'entropy_coeff': tf.cast(self.entropy_coeff, tf.float64)}\n            if self.config['vtrace']:\n                (is_stat_mean, is_stat_var) = tf.nn.moments(self._is_ratio, [0, 1])\n                stats_dict['mean_IS'] = is_stat_mean\n                stats_dict['var_IS'] = is_stat_var\n            if self.config['use_kl_loss']:\n                stats_dict['kl'] = self._mean_kl_loss\n                stats_dict['KL_Coeff'] = self.kl_coeff\n            return stats_dict\n\n        @override(base)\n        def postprocess_trajectory(self, sample_batch: SampleBatch, other_agent_batches: Optional[SampleBatch]=None, episode: Optional['Episode']=None):\n            if not self.config['vtrace']:\n                sample_batch = compute_gae_for_sample_batch(self, sample_batch, other_agent_batches, episode)\n            else:\n                sample_batch = compute_bootstrap_value(sample_batch, self)\n            return sample_batch\n\n        @override(base)\n        def get_batch_divisibility_req(self) -> int:\n            return self.config['rollout_fragment_length']\n    APPOTFPolicy.__name__ = name\n    APPOTFPolicy.__qualname__ = name\n    return APPOTFPolicy",
            "def get_appo_tf_policy(name: str, base: type) -> type:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Construct an APPOTFPolicy inheriting either dynamic or eager base policies.\\n\\n    Args:\\n        base: Base class for this policy. DynamicTFPolicyV2 or EagerTFPolicyV2.\\n\\n    Returns:\\n        A TF Policy to be used with Impala.\\n    '\n\n    class APPOTFPolicy(VTraceClipGradients, VTraceOptimizer, LearningRateSchedule, KLCoeffMixin, EntropyCoeffSchedule, ValueNetworkMixin, TargetNetworkMixin, GradStatsMixin, base):\n\n        def __init__(self, observation_space, action_space, config, existing_model=None, existing_inputs=None):\n            base.enable_eager_execution_if_necessary()\n            if not config.get('_enable_new_api_stack', False):\n                VTraceClipGradients.__init__(self)\n                VTraceOptimizer.__init__(self)\n            base.__init__(self, observation_space, action_space, config, existing_inputs=existing_inputs, existing_model=existing_model)\n            LearningRateSchedule.__init__(self, config['lr'], config['lr_schedule'])\n            EntropyCoeffSchedule.__init__(self, config['entropy_coeff'], config['entropy_coeff_schedule'])\n            ValueNetworkMixin.__init__(self, config)\n            KLCoeffMixin.__init__(self, config)\n            if not config.get('_enable_new_api_stack', False):\n                GradStatsMixin.__init__(self)\n            self.maybe_initialize_optimizer_and_loss()\n            TargetNetworkMixin.__init__(self)\n\n        @override(base)\n        def make_model(self) -> ModelV2:\n            return make_appo_models(self)\n\n        @override(base)\n        def loss(self, model: Union[ModelV2, 'tf.keras.Model'], dist_class: Type[TFActionDistribution], train_batch: SampleBatch) -> Union[TensorType, List[TensorType]]:\n            (model_out, _) = model(train_batch)\n            action_dist = dist_class(model_out, model)\n            if isinstance(self.action_space, gym.spaces.Discrete):\n                is_multidiscrete = False\n                output_hidden_shape = [self.action_space.n]\n            elif isinstance(self.action_space, gym.spaces.multi_discrete.MultiDiscrete):\n                is_multidiscrete = True\n                output_hidden_shape = self.action_space.nvec.astype(np.int32)\n            else:\n                is_multidiscrete = False\n                output_hidden_shape = 1\n\n            def make_time_major(*args, **kw):\n                return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)\n            actions = train_batch[SampleBatch.ACTIONS]\n            dones = train_batch[SampleBatch.TERMINATEDS]\n            rewards = train_batch[SampleBatch.REWARDS]\n            behaviour_logits = train_batch[SampleBatch.ACTION_DIST_INPUTS]\n            (target_model_out, _) = self.target_model(train_batch)\n            prev_action_dist = dist_class(behaviour_logits, self.model)\n            values = self.model.value_function()\n            values_time_major = make_time_major(values)\n            bootstrap_values_time_major = make_time_major(train_batch[SampleBatch.VALUES_BOOTSTRAPPED])\n            bootstrap_value = bootstrap_values_time_major[-1]\n            if self.is_recurrent():\n                max_seq_len = tf.reduce_max(train_batch[SampleBatch.SEQ_LENS])\n                mask = tf.sequence_mask(train_batch[SampleBatch.SEQ_LENS], max_seq_len)\n                mask = tf.reshape(mask, [-1])\n                mask = make_time_major(mask)\n\n                def reduce_mean_valid(t):\n                    return tf.reduce_mean(tf.boolean_mask(t, mask))\n            else:\n                reduce_mean_valid = tf.reduce_mean\n            if self.config['vtrace']:\n                logger.debug('Using V-Trace surrogate loss (vtrace=True)')\n                loss_actions = actions if is_multidiscrete else tf.expand_dims(actions, axis=1)\n                old_policy_behaviour_logits = tf.stop_gradient(target_model_out)\n                old_policy_action_dist = dist_class(old_policy_behaviour_logits, model)\n                mean_kl = make_time_major(old_policy_action_dist.multi_kl(action_dist))\n                unpacked_behaviour_logits = tf.split(behaviour_logits, output_hidden_shape, axis=1)\n                unpacked_old_policy_behaviour_logits = tf.split(old_policy_behaviour_logits, output_hidden_shape, axis=1)\n                with tf.device('/cpu:0'):\n                    vtrace_returns = vtrace.multi_from_logits(behaviour_policy_logits=make_time_major(unpacked_behaviour_logits), target_policy_logits=make_time_major(unpacked_old_policy_behaviour_logits), actions=tf.unstack(make_time_major(loss_actions), axis=2), discounts=tf.cast(~make_time_major(tf.cast(dones, tf.bool)), tf.float32) * self.config['gamma'], rewards=make_time_major(rewards), values=values_time_major, bootstrap_value=bootstrap_value, dist_class=Categorical if is_multidiscrete else dist_class, model=model, clip_rho_threshold=tf.cast(self.config['vtrace_clip_rho_threshold'], tf.float32), clip_pg_rho_threshold=tf.cast(self.config['vtrace_clip_pg_rho_threshold'], tf.float32))\n                actions_logp = make_time_major(action_dist.logp(actions))\n                prev_actions_logp = make_time_major(prev_action_dist.logp(actions))\n                old_policy_actions_logp = make_time_major(old_policy_action_dist.logp(actions))\n                is_ratio = tf.clip_by_value(tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)\n                logp_ratio = is_ratio * tf.exp(actions_logp - prev_actions_logp)\n                self._is_ratio = is_ratio\n                advantages = vtrace_returns.pg_advantages\n                surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n                action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n                mean_kl_loss = reduce_mean_valid(action_kl)\n                mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n                value_targets = vtrace_returns.vs\n                delta = values_time_major - value_targets\n                mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n                actions_entropy = make_time_major(action_dist.multi_entropy())\n                mean_entropy = reduce_mean_valid(actions_entropy)\n            else:\n                logger.debug('Using PPO surrogate loss (vtrace=False)')\n                mean_kl = make_time_major(prev_action_dist.multi_kl(action_dist))\n                logp_ratio = tf.math.exp(make_time_major(action_dist.logp(actions)) - make_time_major(prev_action_dist.logp(actions)))\n                advantages = make_time_major(train_batch[Postprocessing.ADVANTAGES])\n                surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n                action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n                mean_kl_loss = reduce_mean_valid(action_kl)\n                mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n                value_targets = make_time_major(train_batch[Postprocessing.VALUE_TARGETS])\n                delta = values_time_major - value_targets\n                mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n                mean_entropy = reduce_mean_valid(make_time_major(action_dist.multi_entropy()))\n            total_loss = mean_policy_loss - mean_entropy * self.entropy_coeff\n            if self.config['use_kl_loss']:\n                total_loss += self.kl_coeff * mean_kl_loss\n            loss_wo_vf = total_loss\n            if not self.config['_separate_vf_optimizer']:\n                total_loss += mean_vf_loss * self.config['vf_loss_coeff']\n            self._total_loss = total_loss\n            self._loss_wo_vf = loss_wo_vf\n            self._mean_policy_loss = mean_policy_loss\n            self._mean_kl_loss = self._mean_kl = mean_kl_loss\n            self._mean_vf_loss = mean_vf_loss\n            self._mean_entropy = mean_entropy\n            self._value_targets = value_targets\n            if self.config['_separate_vf_optimizer']:\n                return (loss_wo_vf, mean_vf_loss)\n            else:\n                return total_loss\n\n        @override(base)\n        def stats_fn(self, train_batch: SampleBatch) -> Dict[str, TensorType]:\n            values_batched = _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), self.model.value_function())\n            stats_dict = {'cur_lr': tf.cast(self.cur_lr, tf.float64), 'total_loss': self._total_loss, 'policy_loss': self._mean_policy_loss, 'entropy': self._mean_entropy, 'var_gnorm': tf.linalg.global_norm(self.model.trainable_variables()), 'vf_loss': self._mean_vf_loss, 'vf_explained_var': explained_variance(tf.reshape(self._value_targets, [-1]), tf.reshape(values_batched, [-1])), 'entropy_coeff': tf.cast(self.entropy_coeff, tf.float64)}\n            if self.config['vtrace']:\n                (is_stat_mean, is_stat_var) = tf.nn.moments(self._is_ratio, [0, 1])\n                stats_dict['mean_IS'] = is_stat_mean\n                stats_dict['var_IS'] = is_stat_var\n            if self.config['use_kl_loss']:\n                stats_dict['kl'] = self._mean_kl_loss\n                stats_dict['KL_Coeff'] = self.kl_coeff\n            return stats_dict\n\n        @override(base)\n        def postprocess_trajectory(self, sample_batch: SampleBatch, other_agent_batches: Optional[SampleBatch]=None, episode: Optional['Episode']=None):\n            if not self.config['vtrace']:\n                sample_batch = compute_gae_for_sample_batch(self, sample_batch, other_agent_batches, episode)\n            else:\n                sample_batch = compute_bootstrap_value(sample_batch, self)\n            return sample_batch\n\n        @override(base)\n        def get_batch_divisibility_req(self) -> int:\n            return self.config['rollout_fragment_length']\n    APPOTFPolicy.__name__ = name\n    APPOTFPolicy.__qualname__ = name\n    return APPOTFPolicy",
            "def get_appo_tf_policy(name: str, base: type) -> type:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Construct an APPOTFPolicy inheriting either dynamic or eager base policies.\\n\\n    Args:\\n        base: Base class for this policy. DynamicTFPolicyV2 or EagerTFPolicyV2.\\n\\n    Returns:\\n        A TF Policy to be used with Impala.\\n    '\n\n    class APPOTFPolicy(VTraceClipGradients, VTraceOptimizer, LearningRateSchedule, KLCoeffMixin, EntropyCoeffSchedule, ValueNetworkMixin, TargetNetworkMixin, GradStatsMixin, base):\n\n        def __init__(self, observation_space, action_space, config, existing_model=None, existing_inputs=None):\n            base.enable_eager_execution_if_necessary()\n            if not config.get('_enable_new_api_stack', False):\n                VTraceClipGradients.__init__(self)\n                VTraceOptimizer.__init__(self)\n            base.__init__(self, observation_space, action_space, config, existing_inputs=existing_inputs, existing_model=existing_model)\n            LearningRateSchedule.__init__(self, config['lr'], config['lr_schedule'])\n            EntropyCoeffSchedule.__init__(self, config['entropy_coeff'], config['entropy_coeff_schedule'])\n            ValueNetworkMixin.__init__(self, config)\n            KLCoeffMixin.__init__(self, config)\n            if not config.get('_enable_new_api_stack', False):\n                GradStatsMixin.__init__(self)\n            self.maybe_initialize_optimizer_and_loss()\n            TargetNetworkMixin.__init__(self)\n\n        @override(base)\n        def make_model(self) -> ModelV2:\n            return make_appo_models(self)\n\n        @override(base)\n        def loss(self, model: Union[ModelV2, 'tf.keras.Model'], dist_class: Type[TFActionDistribution], train_batch: SampleBatch) -> Union[TensorType, List[TensorType]]:\n            (model_out, _) = model(train_batch)\n            action_dist = dist_class(model_out, model)\n            if isinstance(self.action_space, gym.spaces.Discrete):\n                is_multidiscrete = False\n                output_hidden_shape = [self.action_space.n]\n            elif isinstance(self.action_space, gym.spaces.multi_discrete.MultiDiscrete):\n                is_multidiscrete = True\n                output_hidden_shape = self.action_space.nvec.astype(np.int32)\n            else:\n                is_multidiscrete = False\n                output_hidden_shape = 1\n\n            def make_time_major(*args, **kw):\n                return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)\n            actions = train_batch[SampleBatch.ACTIONS]\n            dones = train_batch[SampleBatch.TERMINATEDS]\n            rewards = train_batch[SampleBatch.REWARDS]\n            behaviour_logits = train_batch[SampleBatch.ACTION_DIST_INPUTS]\n            (target_model_out, _) = self.target_model(train_batch)\n            prev_action_dist = dist_class(behaviour_logits, self.model)\n            values = self.model.value_function()\n            values_time_major = make_time_major(values)\n            bootstrap_values_time_major = make_time_major(train_batch[SampleBatch.VALUES_BOOTSTRAPPED])\n            bootstrap_value = bootstrap_values_time_major[-1]\n            if self.is_recurrent():\n                max_seq_len = tf.reduce_max(train_batch[SampleBatch.SEQ_LENS])\n                mask = tf.sequence_mask(train_batch[SampleBatch.SEQ_LENS], max_seq_len)\n                mask = tf.reshape(mask, [-1])\n                mask = make_time_major(mask)\n\n                def reduce_mean_valid(t):\n                    return tf.reduce_mean(tf.boolean_mask(t, mask))\n            else:\n                reduce_mean_valid = tf.reduce_mean\n            if self.config['vtrace']:\n                logger.debug('Using V-Trace surrogate loss (vtrace=True)')\n                loss_actions = actions if is_multidiscrete else tf.expand_dims(actions, axis=1)\n                old_policy_behaviour_logits = tf.stop_gradient(target_model_out)\n                old_policy_action_dist = dist_class(old_policy_behaviour_logits, model)\n                mean_kl = make_time_major(old_policy_action_dist.multi_kl(action_dist))\n                unpacked_behaviour_logits = tf.split(behaviour_logits, output_hidden_shape, axis=1)\n                unpacked_old_policy_behaviour_logits = tf.split(old_policy_behaviour_logits, output_hidden_shape, axis=1)\n                with tf.device('/cpu:0'):\n                    vtrace_returns = vtrace.multi_from_logits(behaviour_policy_logits=make_time_major(unpacked_behaviour_logits), target_policy_logits=make_time_major(unpacked_old_policy_behaviour_logits), actions=tf.unstack(make_time_major(loss_actions), axis=2), discounts=tf.cast(~make_time_major(tf.cast(dones, tf.bool)), tf.float32) * self.config['gamma'], rewards=make_time_major(rewards), values=values_time_major, bootstrap_value=bootstrap_value, dist_class=Categorical if is_multidiscrete else dist_class, model=model, clip_rho_threshold=tf.cast(self.config['vtrace_clip_rho_threshold'], tf.float32), clip_pg_rho_threshold=tf.cast(self.config['vtrace_clip_pg_rho_threshold'], tf.float32))\n                actions_logp = make_time_major(action_dist.logp(actions))\n                prev_actions_logp = make_time_major(prev_action_dist.logp(actions))\n                old_policy_actions_logp = make_time_major(old_policy_action_dist.logp(actions))\n                is_ratio = tf.clip_by_value(tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)\n                logp_ratio = is_ratio * tf.exp(actions_logp - prev_actions_logp)\n                self._is_ratio = is_ratio\n                advantages = vtrace_returns.pg_advantages\n                surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n                action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n                mean_kl_loss = reduce_mean_valid(action_kl)\n                mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n                value_targets = vtrace_returns.vs\n                delta = values_time_major - value_targets\n                mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n                actions_entropy = make_time_major(action_dist.multi_entropy())\n                mean_entropy = reduce_mean_valid(actions_entropy)\n            else:\n                logger.debug('Using PPO surrogate loss (vtrace=False)')\n                mean_kl = make_time_major(prev_action_dist.multi_kl(action_dist))\n                logp_ratio = tf.math.exp(make_time_major(action_dist.logp(actions)) - make_time_major(prev_action_dist.logp(actions)))\n                advantages = make_time_major(train_batch[Postprocessing.ADVANTAGES])\n                surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n                action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n                mean_kl_loss = reduce_mean_valid(action_kl)\n                mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n                value_targets = make_time_major(train_batch[Postprocessing.VALUE_TARGETS])\n                delta = values_time_major - value_targets\n                mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n                mean_entropy = reduce_mean_valid(make_time_major(action_dist.multi_entropy()))\n            total_loss = mean_policy_loss - mean_entropy * self.entropy_coeff\n            if self.config['use_kl_loss']:\n                total_loss += self.kl_coeff * mean_kl_loss\n            loss_wo_vf = total_loss\n            if not self.config['_separate_vf_optimizer']:\n                total_loss += mean_vf_loss * self.config['vf_loss_coeff']\n            self._total_loss = total_loss\n            self._loss_wo_vf = loss_wo_vf\n            self._mean_policy_loss = mean_policy_loss\n            self._mean_kl_loss = self._mean_kl = mean_kl_loss\n            self._mean_vf_loss = mean_vf_loss\n            self._mean_entropy = mean_entropy\n            self._value_targets = value_targets\n            if self.config['_separate_vf_optimizer']:\n                return (loss_wo_vf, mean_vf_loss)\n            else:\n                return total_loss\n\n        @override(base)\n        def stats_fn(self, train_batch: SampleBatch) -> Dict[str, TensorType]:\n            values_batched = _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), self.model.value_function())\n            stats_dict = {'cur_lr': tf.cast(self.cur_lr, tf.float64), 'total_loss': self._total_loss, 'policy_loss': self._mean_policy_loss, 'entropy': self._mean_entropy, 'var_gnorm': tf.linalg.global_norm(self.model.trainable_variables()), 'vf_loss': self._mean_vf_loss, 'vf_explained_var': explained_variance(tf.reshape(self._value_targets, [-1]), tf.reshape(values_batched, [-1])), 'entropy_coeff': tf.cast(self.entropy_coeff, tf.float64)}\n            if self.config['vtrace']:\n                (is_stat_mean, is_stat_var) = tf.nn.moments(self._is_ratio, [0, 1])\n                stats_dict['mean_IS'] = is_stat_mean\n                stats_dict['var_IS'] = is_stat_var\n            if self.config['use_kl_loss']:\n                stats_dict['kl'] = self._mean_kl_loss\n                stats_dict['KL_Coeff'] = self.kl_coeff\n            return stats_dict\n\n        @override(base)\n        def postprocess_trajectory(self, sample_batch: SampleBatch, other_agent_batches: Optional[SampleBatch]=None, episode: Optional['Episode']=None):\n            if not self.config['vtrace']:\n                sample_batch = compute_gae_for_sample_batch(self, sample_batch, other_agent_batches, episode)\n            else:\n                sample_batch = compute_bootstrap_value(sample_batch, self)\n            return sample_batch\n\n        @override(base)\n        def get_batch_divisibility_req(self) -> int:\n            return self.config['rollout_fragment_length']\n    APPOTFPolicy.__name__ = name\n    APPOTFPolicy.__qualname__ = name\n    return APPOTFPolicy",
            "def get_appo_tf_policy(name: str, base: type) -> type:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Construct an APPOTFPolicy inheriting either dynamic or eager base policies.\\n\\n    Args:\\n        base: Base class for this policy. DynamicTFPolicyV2 or EagerTFPolicyV2.\\n\\n    Returns:\\n        A TF Policy to be used with Impala.\\n    '\n\n    class APPOTFPolicy(VTraceClipGradients, VTraceOptimizer, LearningRateSchedule, KLCoeffMixin, EntropyCoeffSchedule, ValueNetworkMixin, TargetNetworkMixin, GradStatsMixin, base):\n\n        def __init__(self, observation_space, action_space, config, existing_model=None, existing_inputs=None):\n            base.enable_eager_execution_if_necessary()\n            if not config.get('_enable_new_api_stack', False):\n                VTraceClipGradients.__init__(self)\n                VTraceOptimizer.__init__(self)\n            base.__init__(self, observation_space, action_space, config, existing_inputs=existing_inputs, existing_model=existing_model)\n            LearningRateSchedule.__init__(self, config['lr'], config['lr_schedule'])\n            EntropyCoeffSchedule.__init__(self, config['entropy_coeff'], config['entropy_coeff_schedule'])\n            ValueNetworkMixin.__init__(self, config)\n            KLCoeffMixin.__init__(self, config)\n            if not config.get('_enable_new_api_stack', False):\n                GradStatsMixin.__init__(self)\n            self.maybe_initialize_optimizer_and_loss()\n            TargetNetworkMixin.__init__(self)\n\n        @override(base)\n        def make_model(self) -> ModelV2:\n            return make_appo_models(self)\n\n        @override(base)\n        def loss(self, model: Union[ModelV2, 'tf.keras.Model'], dist_class: Type[TFActionDistribution], train_batch: SampleBatch) -> Union[TensorType, List[TensorType]]:\n            (model_out, _) = model(train_batch)\n            action_dist = dist_class(model_out, model)\n            if isinstance(self.action_space, gym.spaces.Discrete):\n                is_multidiscrete = False\n                output_hidden_shape = [self.action_space.n]\n            elif isinstance(self.action_space, gym.spaces.multi_discrete.MultiDiscrete):\n                is_multidiscrete = True\n                output_hidden_shape = self.action_space.nvec.astype(np.int32)\n            else:\n                is_multidiscrete = False\n                output_hidden_shape = 1\n\n            def make_time_major(*args, **kw):\n                return _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), *args, **kw)\n            actions = train_batch[SampleBatch.ACTIONS]\n            dones = train_batch[SampleBatch.TERMINATEDS]\n            rewards = train_batch[SampleBatch.REWARDS]\n            behaviour_logits = train_batch[SampleBatch.ACTION_DIST_INPUTS]\n            (target_model_out, _) = self.target_model(train_batch)\n            prev_action_dist = dist_class(behaviour_logits, self.model)\n            values = self.model.value_function()\n            values_time_major = make_time_major(values)\n            bootstrap_values_time_major = make_time_major(train_batch[SampleBatch.VALUES_BOOTSTRAPPED])\n            bootstrap_value = bootstrap_values_time_major[-1]\n            if self.is_recurrent():\n                max_seq_len = tf.reduce_max(train_batch[SampleBatch.SEQ_LENS])\n                mask = tf.sequence_mask(train_batch[SampleBatch.SEQ_LENS], max_seq_len)\n                mask = tf.reshape(mask, [-1])\n                mask = make_time_major(mask)\n\n                def reduce_mean_valid(t):\n                    return tf.reduce_mean(tf.boolean_mask(t, mask))\n            else:\n                reduce_mean_valid = tf.reduce_mean\n            if self.config['vtrace']:\n                logger.debug('Using V-Trace surrogate loss (vtrace=True)')\n                loss_actions = actions if is_multidiscrete else tf.expand_dims(actions, axis=1)\n                old_policy_behaviour_logits = tf.stop_gradient(target_model_out)\n                old_policy_action_dist = dist_class(old_policy_behaviour_logits, model)\n                mean_kl = make_time_major(old_policy_action_dist.multi_kl(action_dist))\n                unpacked_behaviour_logits = tf.split(behaviour_logits, output_hidden_shape, axis=1)\n                unpacked_old_policy_behaviour_logits = tf.split(old_policy_behaviour_logits, output_hidden_shape, axis=1)\n                with tf.device('/cpu:0'):\n                    vtrace_returns = vtrace.multi_from_logits(behaviour_policy_logits=make_time_major(unpacked_behaviour_logits), target_policy_logits=make_time_major(unpacked_old_policy_behaviour_logits), actions=tf.unstack(make_time_major(loss_actions), axis=2), discounts=tf.cast(~make_time_major(tf.cast(dones, tf.bool)), tf.float32) * self.config['gamma'], rewards=make_time_major(rewards), values=values_time_major, bootstrap_value=bootstrap_value, dist_class=Categorical if is_multidiscrete else dist_class, model=model, clip_rho_threshold=tf.cast(self.config['vtrace_clip_rho_threshold'], tf.float32), clip_pg_rho_threshold=tf.cast(self.config['vtrace_clip_pg_rho_threshold'], tf.float32))\n                actions_logp = make_time_major(action_dist.logp(actions))\n                prev_actions_logp = make_time_major(prev_action_dist.logp(actions))\n                old_policy_actions_logp = make_time_major(old_policy_action_dist.logp(actions))\n                is_ratio = tf.clip_by_value(tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)\n                logp_ratio = is_ratio * tf.exp(actions_logp - prev_actions_logp)\n                self._is_ratio = is_ratio\n                advantages = vtrace_returns.pg_advantages\n                surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n                action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n                mean_kl_loss = reduce_mean_valid(action_kl)\n                mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n                value_targets = vtrace_returns.vs\n                delta = values_time_major - value_targets\n                mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n                actions_entropy = make_time_major(action_dist.multi_entropy())\n                mean_entropy = reduce_mean_valid(actions_entropy)\n            else:\n                logger.debug('Using PPO surrogate loss (vtrace=False)')\n                mean_kl = make_time_major(prev_action_dist.multi_kl(action_dist))\n                logp_ratio = tf.math.exp(make_time_major(action_dist.logp(actions)) - make_time_major(prev_action_dist.logp(actions)))\n                advantages = make_time_major(train_batch[Postprocessing.ADVANTAGES])\n                surrogate_loss = tf.minimum(advantages * logp_ratio, advantages * tf.clip_by_value(logp_ratio, 1 - self.config['clip_param'], 1 + self.config['clip_param']))\n                action_kl = tf.reduce_mean(mean_kl, axis=0) if is_multidiscrete else mean_kl\n                mean_kl_loss = reduce_mean_valid(action_kl)\n                mean_policy_loss = -reduce_mean_valid(surrogate_loss)\n                value_targets = make_time_major(train_batch[Postprocessing.VALUE_TARGETS])\n                delta = values_time_major - value_targets\n                mean_vf_loss = 0.5 * reduce_mean_valid(tf.math.square(delta))\n                mean_entropy = reduce_mean_valid(make_time_major(action_dist.multi_entropy()))\n            total_loss = mean_policy_loss - mean_entropy * self.entropy_coeff\n            if self.config['use_kl_loss']:\n                total_loss += self.kl_coeff * mean_kl_loss\n            loss_wo_vf = total_loss\n            if not self.config['_separate_vf_optimizer']:\n                total_loss += mean_vf_loss * self.config['vf_loss_coeff']\n            self._total_loss = total_loss\n            self._loss_wo_vf = loss_wo_vf\n            self._mean_policy_loss = mean_policy_loss\n            self._mean_kl_loss = self._mean_kl = mean_kl_loss\n            self._mean_vf_loss = mean_vf_loss\n            self._mean_entropy = mean_entropy\n            self._value_targets = value_targets\n            if self.config['_separate_vf_optimizer']:\n                return (loss_wo_vf, mean_vf_loss)\n            else:\n                return total_loss\n\n        @override(base)\n        def stats_fn(self, train_batch: SampleBatch) -> Dict[str, TensorType]:\n            values_batched = _make_time_major(self, train_batch.get(SampleBatch.SEQ_LENS), self.model.value_function())\n            stats_dict = {'cur_lr': tf.cast(self.cur_lr, tf.float64), 'total_loss': self._total_loss, 'policy_loss': self._mean_policy_loss, 'entropy': self._mean_entropy, 'var_gnorm': tf.linalg.global_norm(self.model.trainable_variables()), 'vf_loss': self._mean_vf_loss, 'vf_explained_var': explained_variance(tf.reshape(self._value_targets, [-1]), tf.reshape(values_batched, [-1])), 'entropy_coeff': tf.cast(self.entropy_coeff, tf.float64)}\n            if self.config['vtrace']:\n                (is_stat_mean, is_stat_var) = tf.nn.moments(self._is_ratio, [0, 1])\n                stats_dict['mean_IS'] = is_stat_mean\n                stats_dict['var_IS'] = is_stat_var\n            if self.config['use_kl_loss']:\n                stats_dict['kl'] = self._mean_kl_loss\n                stats_dict['KL_Coeff'] = self.kl_coeff\n            return stats_dict\n\n        @override(base)\n        def postprocess_trajectory(self, sample_batch: SampleBatch, other_agent_batches: Optional[SampleBatch]=None, episode: Optional['Episode']=None):\n            if not self.config['vtrace']:\n                sample_batch = compute_gae_for_sample_batch(self, sample_batch, other_agent_batches, episode)\n            else:\n                sample_batch = compute_bootstrap_value(sample_batch, self)\n            return sample_batch\n\n        @override(base)\n        def get_batch_divisibility_req(self) -> int:\n            return self.config['rollout_fragment_length']\n    APPOTFPolicy.__name__ = name\n    APPOTFPolicy.__qualname__ = name\n    return APPOTFPolicy"
        ]
    }
]