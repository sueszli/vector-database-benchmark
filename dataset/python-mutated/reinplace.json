[
    {
        "func_name": "_is_view_op",
        "original": "def _is_view_op(tgt):\n    if tgt is not None and isinstance(tgt, torch._ops.OpOverload):\n        schema = tgt._schema\n        if len(schema.arguments) > 0:\n            first_arg = schema.arguments[0]\n            return first_arg.alias_info is not None and (not first_arg.alias_info.is_write)",
        "mutated": [
            "def _is_view_op(tgt):\n    if False:\n        i = 10\n    if tgt is not None and isinstance(tgt, torch._ops.OpOverload):\n        schema = tgt._schema\n        if len(schema.arguments) > 0:\n            first_arg = schema.arguments[0]\n            return first_arg.alias_info is not None and (not first_arg.alias_info.is_write)",
            "def _is_view_op(tgt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if tgt is not None and isinstance(tgt, torch._ops.OpOverload):\n        schema = tgt._schema\n        if len(schema.arguments) > 0:\n            first_arg = schema.arguments[0]\n            return first_arg.alias_info is not None and (not first_arg.alias_info.is_write)",
            "def _is_view_op(tgt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if tgt is not None and isinstance(tgt, torch._ops.OpOverload):\n        schema = tgt._schema\n        if len(schema.arguments) > 0:\n            first_arg = schema.arguments[0]\n            return first_arg.alias_info is not None and (not first_arg.alias_info.is_write)",
            "def _is_view_op(tgt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if tgt is not None and isinstance(tgt, torch._ops.OpOverload):\n        schema = tgt._schema\n        if len(schema.arguments) > 0:\n            first_arg = schema.arguments[0]\n            return first_arg.alias_info is not None and (not first_arg.alias_info.is_write)",
            "def _is_view_op(tgt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if tgt is not None and isinstance(tgt, torch._ops.OpOverload):\n        schema = tgt._schema\n        if len(schema.arguments) > 0:\n            first_arg = schema.arguments[0]\n            return first_arg.alias_info is not None and (not first_arg.alias_info.is_write)"
        ]
    },
    {
        "func_name": "_get_view_type",
        "original": "def _get_view_type(tgt) -> _ViewType:\n    if tgt is not None and isinstance(tgt, torch._ops.OpOverload):\n        schema = tgt._schema\n        if len(schema.arguments) > 0:\n            first_arg = schema.arguments[0]\n            if first_arg.alias_info is not None and (not first_arg.alias_info.is_write):\n                if '*' in first_arg.alias_info.after_set:\n                    return _ViewType.MultiOutputView\n                else:\n                    return _ViewType.SingleOutputView\n    return _ViewType.NonView",
        "mutated": [
            "def _get_view_type(tgt) -> _ViewType:\n    if False:\n        i = 10\n    if tgt is not None and isinstance(tgt, torch._ops.OpOverload):\n        schema = tgt._schema\n        if len(schema.arguments) > 0:\n            first_arg = schema.arguments[0]\n            if first_arg.alias_info is not None and (not first_arg.alias_info.is_write):\n                if '*' in first_arg.alias_info.after_set:\n                    return _ViewType.MultiOutputView\n                else:\n                    return _ViewType.SingleOutputView\n    return _ViewType.NonView",
            "def _get_view_type(tgt) -> _ViewType:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if tgt is not None and isinstance(tgt, torch._ops.OpOverload):\n        schema = tgt._schema\n        if len(schema.arguments) > 0:\n            first_arg = schema.arguments[0]\n            if first_arg.alias_info is not None and (not first_arg.alias_info.is_write):\n                if '*' in first_arg.alias_info.after_set:\n                    return _ViewType.MultiOutputView\n                else:\n                    return _ViewType.SingleOutputView\n    return _ViewType.NonView",
            "def _get_view_type(tgt) -> _ViewType:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if tgt is not None and isinstance(tgt, torch._ops.OpOverload):\n        schema = tgt._schema\n        if len(schema.arguments) > 0:\n            first_arg = schema.arguments[0]\n            if first_arg.alias_info is not None and (not first_arg.alias_info.is_write):\n                if '*' in first_arg.alias_info.after_set:\n                    return _ViewType.MultiOutputView\n                else:\n                    return _ViewType.SingleOutputView\n    return _ViewType.NonView",
            "def _get_view_type(tgt) -> _ViewType:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if tgt is not None and isinstance(tgt, torch._ops.OpOverload):\n        schema = tgt._schema\n        if len(schema.arguments) > 0:\n            first_arg = schema.arguments[0]\n            if first_arg.alias_info is not None and (not first_arg.alias_info.is_write):\n                if '*' in first_arg.alias_info.after_set:\n                    return _ViewType.MultiOutputView\n                else:\n                    return _ViewType.SingleOutputView\n    return _ViewType.NonView",
            "def _get_view_type(tgt) -> _ViewType:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if tgt is not None and isinstance(tgt, torch._ops.OpOverload):\n        schema = tgt._schema\n        if len(schema.arguments) > 0:\n            first_arg = schema.arguments[0]\n            if first_arg.alias_info is not None and (not first_arg.alias_info.is_write):\n                if '*' in first_arg.alias_info.after_set:\n                    return _ViewType.MultiOutputView\n                else:\n                    return _ViewType.SingleOutputView\n    return _ViewType.NonView"
        ]
    },
    {
        "func_name": "run_node",
        "original": "def run_node(self, node: Node):\n    self.node_counter += 1\n    result = super().run_node(node)\n    node.meta['fake_result'] = result\n    node.meta['node_idx'] = self.node_counter\n    node_args = node.args\n    if node.target is torch.ops.aten.copy_.default:\n        node_args = node_args[1:]\n    if node.op == 'call_function':\n        view_type = _get_view_type(node.target)\n        if view_type == _ViewType.SingleOutputView:\n            assert isinstance(node.args[0], Node)\n            node.meta['view_of'] = node.args[0]\n        elif view_type == _ViewType.MultiOutputView:\n            self.multi_output_view_nodes[node] = node.args[0]\n        elif node.target is _operator.getitem:\n            list_arg = node.args[0]\n            maybe_base_of_view = self.multi_output_view_nodes.get(list_arg, None)\n            if maybe_base_of_view is not None:\n                assert isinstance(maybe_base_of_view, Node)\n                node.meta['view_of'] = maybe_base_of_view\n    if 'view_of' in node.meta:\n        assert isinstance(node.meta['fake_result'], FakeTensor)\n        assert isinstance(node.meta['view_of'].meta['fake_result'], FakeTensor)\n        view_storage = StorageWeakRef(node.meta['fake_result']._typed_storage())\n        base_storage = StorageWeakRef(node.meta['view_of'].meta['fake_result']._typed_storage())\n        assert view_storage == base_storage\n    return result",
        "mutated": [
            "def run_node(self, node: Node):\n    if False:\n        i = 10\n    self.node_counter += 1\n    result = super().run_node(node)\n    node.meta['fake_result'] = result\n    node.meta['node_idx'] = self.node_counter\n    node_args = node.args\n    if node.target is torch.ops.aten.copy_.default:\n        node_args = node_args[1:]\n    if node.op == 'call_function':\n        view_type = _get_view_type(node.target)\n        if view_type == _ViewType.SingleOutputView:\n            assert isinstance(node.args[0], Node)\n            node.meta['view_of'] = node.args[0]\n        elif view_type == _ViewType.MultiOutputView:\n            self.multi_output_view_nodes[node] = node.args[0]\n        elif node.target is _operator.getitem:\n            list_arg = node.args[0]\n            maybe_base_of_view = self.multi_output_view_nodes.get(list_arg, None)\n            if maybe_base_of_view is not None:\n                assert isinstance(maybe_base_of_view, Node)\n                node.meta['view_of'] = maybe_base_of_view\n    if 'view_of' in node.meta:\n        assert isinstance(node.meta['fake_result'], FakeTensor)\n        assert isinstance(node.meta['view_of'].meta['fake_result'], FakeTensor)\n        view_storage = StorageWeakRef(node.meta['fake_result']._typed_storage())\n        base_storage = StorageWeakRef(node.meta['view_of'].meta['fake_result']._typed_storage())\n        assert view_storage == base_storage\n    return result",
            "def run_node(self, node: Node):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.node_counter += 1\n    result = super().run_node(node)\n    node.meta['fake_result'] = result\n    node.meta['node_idx'] = self.node_counter\n    node_args = node.args\n    if node.target is torch.ops.aten.copy_.default:\n        node_args = node_args[1:]\n    if node.op == 'call_function':\n        view_type = _get_view_type(node.target)\n        if view_type == _ViewType.SingleOutputView:\n            assert isinstance(node.args[0], Node)\n            node.meta['view_of'] = node.args[0]\n        elif view_type == _ViewType.MultiOutputView:\n            self.multi_output_view_nodes[node] = node.args[0]\n        elif node.target is _operator.getitem:\n            list_arg = node.args[0]\n            maybe_base_of_view = self.multi_output_view_nodes.get(list_arg, None)\n            if maybe_base_of_view is not None:\n                assert isinstance(maybe_base_of_view, Node)\n                node.meta['view_of'] = maybe_base_of_view\n    if 'view_of' in node.meta:\n        assert isinstance(node.meta['fake_result'], FakeTensor)\n        assert isinstance(node.meta['view_of'].meta['fake_result'], FakeTensor)\n        view_storage = StorageWeakRef(node.meta['fake_result']._typed_storage())\n        base_storage = StorageWeakRef(node.meta['view_of'].meta['fake_result']._typed_storage())\n        assert view_storage == base_storage\n    return result",
            "def run_node(self, node: Node):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.node_counter += 1\n    result = super().run_node(node)\n    node.meta['fake_result'] = result\n    node.meta['node_idx'] = self.node_counter\n    node_args = node.args\n    if node.target is torch.ops.aten.copy_.default:\n        node_args = node_args[1:]\n    if node.op == 'call_function':\n        view_type = _get_view_type(node.target)\n        if view_type == _ViewType.SingleOutputView:\n            assert isinstance(node.args[0], Node)\n            node.meta['view_of'] = node.args[0]\n        elif view_type == _ViewType.MultiOutputView:\n            self.multi_output_view_nodes[node] = node.args[0]\n        elif node.target is _operator.getitem:\n            list_arg = node.args[0]\n            maybe_base_of_view = self.multi_output_view_nodes.get(list_arg, None)\n            if maybe_base_of_view is not None:\n                assert isinstance(maybe_base_of_view, Node)\n                node.meta['view_of'] = maybe_base_of_view\n    if 'view_of' in node.meta:\n        assert isinstance(node.meta['fake_result'], FakeTensor)\n        assert isinstance(node.meta['view_of'].meta['fake_result'], FakeTensor)\n        view_storage = StorageWeakRef(node.meta['fake_result']._typed_storage())\n        base_storage = StorageWeakRef(node.meta['view_of'].meta['fake_result']._typed_storage())\n        assert view_storage == base_storage\n    return result",
            "def run_node(self, node: Node):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.node_counter += 1\n    result = super().run_node(node)\n    node.meta['fake_result'] = result\n    node.meta['node_idx'] = self.node_counter\n    node_args = node.args\n    if node.target is torch.ops.aten.copy_.default:\n        node_args = node_args[1:]\n    if node.op == 'call_function':\n        view_type = _get_view_type(node.target)\n        if view_type == _ViewType.SingleOutputView:\n            assert isinstance(node.args[0], Node)\n            node.meta['view_of'] = node.args[0]\n        elif view_type == _ViewType.MultiOutputView:\n            self.multi_output_view_nodes[node] = node.args[0]\n        elif node.target is _operator.getitem:\n            list_arg = node.args[0]\n            maybe_base_of_view = self.multi_output_view_nodes.get(list_arg, None)\n            if maybe_base_of_view is not None:\n                assert isinstance(maybe_base_of_view, Node)\n                node.meta['view_of'] = maybe_base_of_view\n    if 'view_of' in node.meta:\n        assert isinstance(node.meta['fake_result'], FakeTensor)\n        assert isinstance(node.meta['view_of'].meta['fake_result'], FakeTensor)\n        view_storage = StorageWeakRef(node.meta['fake_result']._typed_storage())\n        base_storage = StorageWeakRef(node.meta['view_of'].meta['fake_result']._typed_storage())\n        assert view_storage == base_storage\n    return result",
            "def run_node(self, node: Node):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.node_counter += 1\n    result = super().run_node(node)\n    node.meta['fake_result'] = result\n    node.meta['node_idx'] = self.node_counter\n    node_args = node.args\n    if node.target is torch.ops.aten.copy_.default:\n        node_args = node_args[1:]\n    if node.op == 'call_function':\n        view_type = _get_view_type(node.target)\n        if view_type == _ViewType.SingleOutputView:\n            assert isinstance(node.args[0], Node)\n            node.meta['view_of'] = node.args[0]\n        elif view_type == _ViewType.MultiOutputView:\n            self.multi_output_view_nodes[node] = node.args[0]\n        elif node.target is _operator.getitem:\n            list_arg = node.args[0]\n            maybe_base_of_view = self.multi_output_view_nodes.get(list_arg, None)\n            if maybe_base_of_view is not None:\n                assert isinstance(maybe_base_of_view, Node)\n                node.meta['view_of'] = maybe_base_of_view\n    if 'view_of' in node.meta:\n        assert isinstance(node.meta['fake_result'], FakeTensor)\n        assert isinstance(node.meta['view_of'].meta['fake_result'], FakeTensor)\n        view_storage = StorageWeakRef(node.meta['fake_result']._typed_storage())\n        base_storage = StorageWeakRef(node.meta['view_of'].meta['fake_result']._typed_storage())\n        assert view_storage == base_storage\n    return result"
        ]
    },
    {
        "func_name": "propagate",
        "original": "def propagate(self, *args):\n    self.multi_output_view_nodes = {}\n    self.node_counter = -1\n    with FakeTensorMode() as mode:\n        fake_args = [mode.from_tensor(a) for a in args]\n        return super().run(*fake_args)",
        "mutated": [
            "def propagate(self, *args):\n    if False:\n        i = 10\n    self.multi_output_view_nodes = {}\n    self.node_counter = -1\n    with FakeTensorMode() as mode:\n        fake_args = [mode.from_tensor(a) for a in args]\n        return super().run(*fake_args)",
            "def propagate(self, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.multi_output_view_nodes = {}\n    self.node_counter = -1\n    with FakeTensorMode() as mode:\n        fake_args = [mode.from_tensor(a) for a in args]\n        return super().run(*fake_args)",
            "def propagate(self, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.multi_output_view_nodes = {}\n    self.node_counter = -1\n    with FakeTensorMode() as mode:\n        fake_args = [mode.from_tensor(a) for a in args]\n        return super().run(*fake_args)",
            "def propagate(self, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.multi_output_view_nodes = {}\n    self.node_counter = -1\n    with FakeTensorMode() as mode:\n        fake_args = [mode.from_tensor(a) for a in args]\n        return super().run(*fake_args)",
            "def propagate(self, *args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.multi_output_view_nodes = {}\n    self.node_counter = -1\n    with FakeTensorMode() as mode:\n        fake_args = [mode.from_tensor(a) for a in args]\n        return super().run(*fake_args)"
        ]
    },
    {
        "func_name": "_schemas_match",
        "original": "def _schemas_match(functional_schema, inplace_schema):\n    names_match = inplace_schema.name.endswith('_') and inplace_schema.name[:-1] == functional_schema.name\n    arg_types_match = len(functional_schema.arguments) == len(inplace_schema.arguments) and all((a1.type == a2.type for (a1, a2) in zip(functional_schema.arguments, inplace_schema.arguments)))\n    assert inplace_schema.arguments[0].alias_info is not None and inplace_schema.arguments[0].alias_info.is_write\n    assert all((a.alias_info is None for a in inplace_schema.arguments[1:]))\n    return names_match and arg_types_match",
        "mutated": [
            "def _schemas_match(functional_schema, inplace_schema):\n    if False:\n        i = 10\n    names_match = inplace_schema.name.endswith('_') and inplace_schema.name[:-1] == functional_schema.name\n    arg_types_match = len(functional_schema.arguments) == len(inplace_schema.arguments) and all((a1.type == a2.type for (a1, a2) in zip(functional_schema.arguments, inplace_schema.arguments)))\n    assert inplace_schema.arguments[0].alias_info is not None and inplace_schema.arguments[0].alias_info.is_write\n    assert all((a.alias_info is None for a in inplace_schema.arguments[1:]))\n    return names_match and arg_types_match",
            "def _schemas_match(functional_schema, inplace_schema):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    names_match = inplace_schema.name.endswith('_') and inplace_schema.name[:-1] == functional_schema.name\n    arg_types_match = len(functional_schema.arguments) == len(inplace_schema.arguments) and all((a1.type == a2.type for (a1, a2) in zip(functional_schema.arguments, inplace_schema.arguments)))\n    assert inplace_schema.arguments[0].alias_info is not None and inplace_schema.arguments[0].alias_info.is_write\n    assert all((a.alias_info is None for a in inplace_schema.arguments[1:]))\n    return names_match and arg_types_match",
            "def _schemas_match(functional_schema, inplace_schema):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    names_match = inplace_schema.name.endswith('_') and inplace_schema.name[:-1] == functional_schema.name\n    arg_types_match = len(functional_schema.arguments) == len(inplace_schema.arguments) and all((a1.type == a2.type for (a1, a2) in zip(functional_schema.arguments, inplace_schema.arguments)))\n    assert inplace_schema.arguments[0].alias_info is not None and inplace_schema.arguments[0].alias_info.is_write\n    assert all((a.alias_info is None for a in inplace_schema.arguments[1:]))\n    return names_match and arg_types_match",
            "def _schemas_match(functional_schema, inplace_schema):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    names_match = inplace_schema.name.endswith('_') and inplace_schema.name[:-1] == functional_schema.name\n    arg_types_match = len(functional_schema.arguments) == len(inplace_schema.arguments) and all((a1.type == a2.type for (a1, a2) in zip(functional_schema.arguments, inplace_schema.arguments)))\n    assert inplace_schema.arguments[0].alias_info is not None and inplace_schema.arguments[0].alias_info.is_write\n    assert all((a.alias_info is None for a in inplace_schema.arguments[1:]))\n    return names_match and arg_types_match",
            "def _schemas_match(functional_schema, inplace_schema):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    names_match = inplace_schema.name.endswith('_') and inplace_schema.name[:-1] == functional_schema.name\n    arg_types_match = len(functional_schema.arguments) == len(inplace_schema.arguments) and all((a1.type == a2.type for (a1, a2) in zip(functional_schema.arguments, inplace_schema.arguments)))\n    assert inplace_schema.arguments[0].alias_info is not None and inplace_schema.arguments[0].alias_info.is_write\n    assert all((a.alias_info is None for a in inplace_schema.arguments[1:]))\n    return names_match and arg_types_match"
        ]
    },
    {
        "func_name": "_maybe_get_inplace_op",
        "original": "def _maybe_get_inplace_op(op):\n    if not isinstance(op, torch._ops.OpOverload):\n        return None\n    if _is_view_op(op):\n        return None\n    op_namespace = op.__module__.split('.')[-1]\n    op_base_name = op.overloadpacket.__name__\n    maybe_namespace_module = getattr(torch.ops, op_namespace)\n    maybe_inplace_op = None if maybe_namespace_module is None else getattr(maybe_namespace_module, f'{op_base_name}_', None)\n    if maybe_inplace_op is None:\n        return None\n    inplace_overloads = [getattr(maybe_inplace_op, overload_name) for overload_name in maybe_inplace_op.overloads()]\n    inplace_overloads_with_matching_schemas = [f for f in inplace_overloads if _schemas_match(op._schema, f._schema)]\n    if len(inplace_overloads_with_matching_schemas) == 0:\n        return None\n    assert len(inplace_overloads_with_matching_schemas) == 1\n    inplace_op = inplace_overloads_with_matching_schemas[0]\n    return inplace_op",
        "mutated": [
            "def _maybe_get_inplace_op(op):\n    if False:\n        i = 10\n    if not isinstance(op, torch._ops.OpOverload):\n        return None\n    if _is_view_op(op):\n        return None\n    op_namespace = op.__module__.split('.')[-1]\n    op_base_name = op.overloadpacket.__name__\n    maybe_namespace_module = getattr(torch.ops, op_namespace)\n    maybe_inplace_op = None if maybe_namespace_module is None else getattr(maybe_namespace_module, f'{op_base_name}_', None)\n    if maybe_inplace_op is None:\n        return None\n    inplace_overloads = [getattr(maybe_inplace_op, overload_name) for overload_name in maybe_inplace_op.overloads()]\n    inplace_overloads_with_matching_schemas = [f for f in inplace_overloads if _schemas_match(op._schema, f._schema)]\n    if len(inplace_overloads_with_matching_schemas) == 0:\n        return None\n    assert len(inplace_overloads_with_matching_schemas) == 1\n    inplace_op = inplace_overloads_with_matching_schemas[0]\n    return inplace_op",
            "def _maybe_get_inplace_op(op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not isinstance(op, torch._ops.OpOverload):\n        return None\n    if _is_view_op(op):\n        return None\n    op_namespace = op.__module__.split('.')[-1]\n    op_base_name = op.overloadpacket.__name__\n    maybe_namespace_module = getattr(torch.ops, op_namespace)\n    maybe_inplace_op = None if maybe_namespace_module is None else getattr(maybe_namespace_module, f'{op_base_name}_', None)\n    if maybe_inplace_op is None:\n        return None\n    inplace_overloads = [getattr(maybe_inplace_op, overload_name) for overload_name in maybe_inplace_op.overloads()]\n    inplace_overloads_with_matching_schemas = [f for f in inplace_overloads if _schemas_match(op._schema, f._schema)]\n    if len(inplace_overloads_with_matching_schemas) == 0:\n        return None\n    assert len(inplace_overloads_with_matching_schemas) == 1\n    inplace_op = inplace_overloads_with_matching_schemas[0]\n    return inplace_op",
            "def _maybe_get_inplace_op(op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not isinstance(op, torch._ops.OpOverload):\n        return None\n    if _is_view_op(op):\n        return None\n    op_namespace = op.__module__.split('.')[-1]\n    op_base_name = op.overloadpacket.__name__\n    maybe_namespace_module = getattr(torch.ops, op_namespace)\n    maybe_inplace_op = None if maybe_namespace_module is None else getattr(maybe_namespace_module, f'{op_base_name}_', None)\n    if maybe_inplace_op is None:\n        return None\n    inplace_overloads = [getattr(maybe_inplace_op, overload_name) for overload_name in maybe_inplace_op.overloads()]\n    inplace_overloads_with_matching_schemas = [f for f in inplace_overloads if _schemas_match(op._schema, f._schema)]\n    if len(inplace_overloads_with_matching_schemas) == 0:\n        return None\n    assert len(inplace_overloads_with_matching_schemas) == 1\n    inplace_op = inplace_overloads_with_matching_schemas[0]\n    return inplace_op",
            "def _maybe_get_inplace_op(op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not isinstance(op, torch._ops.OpOverload):\n        return None\n    if _is_view_op(op):\n        return None\n    op_namespace = op.__module__.split('.')[-1]\n    op_base_name = op.overloadpacket.__name__\n    maybe_namespace_module = getattr(torch.ops, op_namespace)\n    maybe_inplace_op = None if maybe_namespace_module is None else getattr(maybe_namespace_module, f'{op_base_name}_', None)\n    if maybe_inplace_op is None:\n        return None\n    inplace_overloads = [getattr(maybe_inplace_op, overload_name) for overload_name in maybe_inplace_op.overloads()]\n    inplace_overloads_with_matching_schemas = [f for f in inplace_overloads if _schemas_match(op._schema, f._schema)]\n    if len(inplace_overloads_with_matching_schemas) == 0:\n        return None\n    assert len(inplace_overloads_with_matching_schemas) == 1\n    inplace_op = inplace_overloads_with_matching_schemas[0]\n    return inplace_op",
            "def _maybe_get_inplace_op(op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not isinstance(op, torch._ops.OpOverload):\n        return None\n    if _is_view_op(op):\n        return None\n    op_namespace = op.__module__.split('.')[-1]\n    op_base_name = op.overloadpacket.__name__\n    maybe_namespace_module = getattr(torch.ops, op_namespace)\n    maybe_inplace_op = None if maybe_namespace_module is None else getattr(maybe_namespace_module, f'{op_base_name}_', None)\n    if maybe_inplace_op is None:\n        return None\n    inplace_overloads = [getattr(maybe_inplace_op, overload_name) for overload_name in maybe_inplace_op.overloads()]\n    inplace_overloads_with_matching_schemas = [f for f in inplace_overloads if _schemas_match(op._schema, f._schema)]\n    if len(inplace_overloads_with_matching_schemas) == 0:\n        return None\n    assert len(inplace_overloads_with_matching_schemas) == 1\n    inplace_op = inplace_overloads_with_matching_schemas[0]\n    return inplace_op"
        ]
    },
    {
        "func_name": "_add_if_tensor",
        "original": "def _add_if_tensor(x, set_):\n    if isinstance(x, FakeTensor):\n        set_.add(StorageWeakRef(x._typed_storage()))",
        "mutated": [
            "def _add_if_tensor(x, set_):\n    if False:\n        i = 10\n    if isinstance(x, FakeTensor):\n        set_.add(StorageWeakRef(x._typed_storage()))",
            "def _add_if_tensor(x, set_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if isinstance(x, FakeTensor):\n        set_.add(StorageWeakRef(x._typed_storage()))",
            "def _add_if_tensor(x, set_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if isinstance(x, FakeTensor):\n        set_.add(StorageWeakRef(x._typed_storage()))",
            "def _add_if_tensor(x, set_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if isinstance(x, FakeTensor):\n        set_.add(StorageWeakRef(x._typed_storage()))",
            "def _add_if_tensor(x, set_):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if isinstance(x, FakeTensor):\n        set_.add(StorageWeakRef(x._typed_storage()))"
        ]
    },
    {
        "func_name": "_get_all_later_node_usages",
        "original": "def _get_all_later_node_usages(tensor_aliases: Set[Node], op_index: int):\n\n    def _add_if_tensor(x, set_):\n        if isinstance(x, FakeTensor):\n            set_.add(StorageWeakRef(x._typed_storage()))\n    nodes_used_after = set()\n    for t in tensor_aliases:\n        usage_nodes = t.users\n        for n in usage_nodes:\n            if 'node_idx' not in n.meta or n.meta['node_idx'] <= op_index:\n                continue\n            if n in tensor_aliases:\n                if isinstance(n.target, torch._ops.OpOverload) or n.target == _operator.getitem:\n                    continue\n            nodes_used_after.add(n)\n    return nodes_used_after",
        "mutated": [
            "def _get_all_later_node_usages(tensor_aliases: Set[Node], op_index: int):\n    if False:\n        i = 10\n\n    def _add_if_tensor(x, set_):\n        if isinstance(x, FakeTensor):\n            set_.add(StorageWeakRef(x._typed_storage()))\n    nodes_used_after = set()\n    for t in tensor_aliases:\n        usage_nodes = t.users\n        for n in usage_nodes:\n            if 'node_idx' not in n.meta or n.meta['node_idx'] <= op_index:\n                continue\n            if n in tensor_aliases:\n                if isinstance(n.target, torch._ops.OpOverload) or n.target == _operator.getitem:\n                    continue\n            nodes_used_after.add(n)\n    return nodes_used_after",
            "def _get_all_later_node_usages(tensor_aliases: Set[Node], op_index: int):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def _add_if_tensor(x, set_):\n        if isinstance(x, FakeTensor):\n            set_.add(StorageWeakRef(x._typed_storage()))\n    nodes_used_after = set()\n    for t in tensor_aliases:\n        usage_nodes = t.users\n        for n in usage_nodes:\n            if 'node_idx' not in n.meta or n.meta['node_idx'] <= op_index:\n                continue\n            if n in tensor_aliases:\n                if isinstance(n.target, torch._ops.OpOverload) or n.target == _operator.getitem:\n                    continue\n            nodes_used_after.add(n)\n    return nodes_used_after",
            "def _get_all_later_node_usages(tensor_aliases: Set[Node], op_index: int):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def _add_if_tensor(x, set_):\n        if isinstance(x, FakeTensor):\n            set_.add(StorageWeakRef(x._typed_storage()))\n    nodes_used_after = set()\n    for t in tensor_aliases:\n        usage_nodes = t.users\n        for n in usage_nodes:\n            if 'node_idx' not in n.meta or n.meta['node_idx'] <= op_index:\n                continue\n            if n in tensor_aliases:\n                if isinstance(n.target, torch._ops.OpOverload) or n.target == _operator.getitem:\n                    continue\n            nodes_used_after.add(n)\n    return nodes_used_after",
            "def _get_all_later_node_usages(tensor_aliases: Set[Node], op_index: int):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def _add_if_tensor(x, set_):\n        if isinstance(x, FakeTensor):\n            set_.add(StorageWeakRef(x._typed_storage()))\n    nodes_used_after = set()\n    for t in tensor_aliases:\n        usage_nodes = t.users\n        for n in usage_nodes:\n            if 'node_idx' not in n.meta or n.meta['node_idx'] <= op_index:\n                continue\n            if n in tensor_aliases:\n                if isinstance(n.target, torch._ops.OpOverload) or n.target == _operator.getitem:\n                    continue\n            nodes_used_after.add(n)\n    return nodes_used_after",
            "def _get_all_later_node_usages(tensor_aliases: Set[Node], op_index: int):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def _add_if_tensor(x, set_):\n        if isinstance(x, FakeTensor):\n            set_.add(StorageWeakRef(x._typed_storage()))\n    nodes_used_after = set()\n    for t in tensor_aliases:\n        usage_nodes = t.users\n        for n in usage_nodes:\n            if 'node_idx' not in n.meta or n.meta['node_idx'] <= op_index:\n                continue\n            if n in tensor_aliases:\n                if isinstance(n.target, torch._ops.OpOverload) or n.target == _operator.getitem:\n                    continue\n            nodes_used_after.add(n)\n    return nodes_used_after"
        ]
    },
    {
        "func_name": "matching_view_metadata",
        "original": "def matching_view_metadata(a, b):\n    return a.size() == b.size() and a.stride() == b.stride() and (a.storage_offset() == b.storage_offset())",
        "mutated": [
            "def matching_view_metadata(a, b):\n    if False:\n        i = 10\n    return a.size() == b.size() and a.stride() == b.stride() and (a.storage_offset() == b.storage_offset())",
            "def matching_view_metadata(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return a.size() == b.size() and a.stride() == b.stride() and (a.storage_offset() == b.storage_offset())",
            "def matching_view_metadata(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return a.size() == b.size() and a.stride() == b.stride() and (a.storage_offset() == b.storage_offset())",
            "def matching_view_metadata(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return a.size() == b.size() and a.stride() == b.stride() and (a.storage_offset() == b.storage_offset())",
            "def matching_view_metadata(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return a.size() == b.size() and a.stride() == b.stride() and (a.storage_offset() == b.storage_offset())"
        ]
    },
    {
        "func_name": "_get_view_inverse_node_usages",
        "original": "def _get_view_inverse_node_usages(later_node_usages: Set[Node], self_aliases: Set[Node]) -> Set[Node]:\n\n    def matching_view_metadata(a, b):\n        return a.size() == b.size() and a.stride() == b.stride() and (a.storage_offset() == b.storage_offset())\n    view_inverse_nodes = set()\n    for n in sorted(later_node_usages, key=lambda x: x.meta['node_idx']):\n        if n.target not in _VIEW_INVERSE_MAP:\n            continue\n        base = n.args[0]\n        mutated_view = n.args[1]\n        assert isinstance(base, Node)\n        assert isinstance(base.meta['fake_result'], FakeTensor)\n        assert isinstance(mutated_view, Node)\n        assert isinstance(mutated_view.meta['fake_result'], FakeTensor)\n        original_view = _VIEW_INVERSE_MAP[n.target]\n        for self_alias in self_aliases:\n            if 'view_of' not in self_alias.meta:\n                continue\n            self_alias_base = self_alias.meta['view_of']\n            try:\n                view_replay_metadata = original_view(self_alias_base.meta['fake_result'], *n.args[2:], **n.kwargs)\n                expected_metadata = self_alias.meta['fake_result']\n                if matching_view_metadata(self_alias_base.meta['fake_result'], base.meta['fake_result']) and matching_view_metadata(view_replay_metadata, expected_metadata):\n                    view_inverse_nodes.add(n)\n            except Exception:\n                continue\n    return view_inverse_nodes",
        "mutated": [
            "def _get_view_inverse_node_usages(later_node_usages: Set[Node], self_aliases: Set[Node]) -> Set[Node]:\n    if False:\n        i = 10\n\n    def matching_view_metadata(a, b):\n        return a.size() == b.size() and a.stride() == b.stride() and (a.storage_offset() == b.storage_offset())\n    view_inverse_nodes = set()\n    for n in sorted(later_node_usages, key=lambda x: x.meta['node_idx']):\n        if n.target not in _VIEW_INVERSE_MAP:\n            continue\n        base = n.args[0]\n        mutated_view = n.args[1]\n        assert isinstance(base, Node)\n        assert isinstance(base.meta['fake_result'], FakeTensor)\n        assert isinstance(mutated_view, Node)\n        assert isinstance(mutated_view.meta['fake_result'], FakeTensor)\n        original_view = _VIEW_INVERSE_MAP[n.target]\n        for self_alias in self_aliases:\n            if 'view_of' not in self_alias.meta:\n                continue\n            self_alias_base = self_alias.meta['view_of']\n            try:\n                view_replay_metadata = original_view(self_alias_base.meta['fake_result'], *n.args[2:], **n.kwargs)\n                expected_metadata = self_alias.meta['fake_result']\n                if matching_view_metadata(self_alias_base.meta['fake_result'], base.meta['fake_result']) and matching_view_metadata(view_replay_metadata, expected_metadata):\n                    view_inverse_nodes.add(n)\n            except Exception:\n                continue\n    return view_inverse_nodes",
            "def _get_view_inverse_node_usages(later_node_usages: Set[Node], self_aliases: Set[Node]) -> Set[Node]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def matching_view_metadata(a, b):\n        return a.size() == b.size() and a.stride() == b.stride() and (a.storage_offset() == b.storage_offset())\n    view_inverse_nodes = set()\n    for n in sorted(later_node_usages, key=lambda x: x.meta['node_idx']):\n        if n.target not in _VIEW_INVERSE_MAP:\n            continue\n        base = n.args[0]\n        mutated_view = n.args[1]\n        assert isinstance(base, Node)\n        assert isinstance(base.meta['fake_result'], FakeTensor)\n        assert isinstance(mutated_view, Node)\n        assert isinstance(mutated_view.meta['fake_result'], FakeTensor)\n        original_view = _VIEW_INVERSE_MAP[n.target]\n        for self_alias in self_aliases:\n            if 'view_of' not in self_alias.meta:\n                continue\n            self_alias_base = self_alias.meta['view_of']\n            try:\n                view_replay_metadata = original_view(self_alias_base.meta['fake_result'], *n.args[2:], **n.kwargs)\n                expected_metadata = self_alias.meta['fake_result']\n                if matching_view_metadata(self_alias_base.meta['fake_result'], base.meta['fake_result']) and matching_view_metadata(view_replay_metadata, expected_metadata):\n                    view_inverse_nodes.add(n)\n            except Exception:\n                continue\n    return view_inverse_nodes",
            "def _get_view_inverse_node_usages(later_node_usages: Set[Node], self_aliases: Set[Node]) -> Set[Node]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def matching_view_metadata(a, b):\n        return a.size() == b.size() and a.stride() == b.stride() and (a.storage_offset() == b.storage_offset())\n    view_inverse_nodes = set()\n    for n in sorted(later_node_usages, key=lambda x: x.meta['node_idx']):\n        if n.target not in _VIEW_INVERSE_MAP:\n            continue\n        base = n.args[0]\n        mutated_view = n.args[1]\n        assert isinstance(base, Node)\n        assert isinstance(base.meta['fake_result'], FakeTensor)\n        assert isinstance(mutated_view, Node)\n        assert isinstance(mutated_view.meta['fake_result'], FakeTensor)\n        original_view = _VIEW_INVERSE_MAP[n.target]\n        for self_alias in self_aliases:\n            if 'view_of' not in self_alias.meta:\n                continue\n            self_alias_base = self_alias.meta['view_of']\n            try:\n                view_replay_metadata = original_view(self_alias_base.meta['fake_result'], *n.args[2:], **n.kwargs)\n                expected_metadata = self_alias.meta['fake_result']\n                if matching_view_metadata(self_alias_base.meta['fake_result'], base.meta['fake_result']) and matching_view_metadata(view_replay_metadata, expected_metadata):\n                    view_inverse_nodes.add(n)\n            except Exception:\n                continue\n    return view_inverse_nodes",
            "def _get_view_inverse_node_usages(later_node_usages: Set[Node], self_aliases: Set[Node]) -> Set[Node]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def matching_view_metadata(a, b):\n        return a.size() == b.size() and a.stride() == b.stride() and (a.storage_offset() == b.storage_offset())\n    view_inverse_nodes = set()\n    for n in sorted(later_node_usages, key=lambda x: x.meta['node_idx']):\n        if n.target not in _VIEW_INVERSE_MAP:\n            continue\n        base = n.args[0]\n        mutated_view = n.args[1]\n        assert isinstance(base, Node)\n        assert isinstance(base.meta['fake_result'], FakeTensor)\n        assert isinstance(mutated_view, Node)\n        assert isinstance(mutated_view.meta['fake_result'], FakeTensor)\n        original_view = _VIEW_INVERSE_MAP[n.target]\n        for self_alias in self_aliases:\n            if 'view_of' not in self_alias.meta:\n                continue\n            self_alias_base = self_alias.meta['view_of']\n            try:\n                view_replay_metadata = original_view(self_alias_base.meta['fake_result'], *n.args[2:], **n.kwargs)\n                expected_metadata = self_alias.meta['fake_result']\n                if matching_view_metadata(self_alias_base.meta['fake_result'], base.meta['fake_result']) and matching_view_metadata(view_replay_metadata, expected_metadata):\n                    view_inverse_nodes.add(n)\n            except Exception:\n                continue\n    return view_inverse_nodes",
            "def _get_view_inverse_node_usages(later_node_usages: Set[Node], self_aliases: Set[Node]) -> Set[Node]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def matching_view_metadata(a, b):\n        return a.size() == b.size() and a.stride() == b.stride() and (a.storage_offset() == b.storage_offset())\n    view_inverse_nodes = set()\n    for n in sorted(later_node_usages, key=lambda x: x.meta['node_idx']):\n        if n.target not in _VIEW_INVERSE_MAP:\n            continue\n        base = n.args[0]\n        mutated_view = n.args[1]\n        assert isinstance(base, Node)\n        assert isinstance(base.meta['fake_result'], FakeTensor)\n        assert isinstance(mutated_view, Node)\n        assert isinstance(mutated_view.meta['fake_result'], FakeTensor)\n        original_view = _VIEW_INVERSE_MAP[n.target]\n        for self_alias in self_aliases:\n            if 'view_of' not in self_alias.meta:\n                continue\n            self_alias_base = self_alias.meta['view_of']\n            try:\n                view_replay_metadata = original_view(self_alias_base.meta['fake_result'], *n.args[2:], **n.kwargs)\n                expected_metadata = self_alias.meta['fake_result']\n                if matching_view_metadata(self_alias_base.meta['fake_result'], base.meta['fake_result']) and matching_view_metadata(view_replay_metadata, expected_metadata):\n                    view_inverse_nodes.add(n)\n            except Exception:\n                continue\n    return view_inverse_nodes"
        ]
    },
    {
        "func_name": "_add_to_map",
        "original": "def _add_to_map(x):\n    if isinstance(x, FakeTensor):\n        storage_to_nodes[StorageWeakRef(x._typed_storage())].add(n)",
        "mutated": [
            "def _add_to_map(x):\n    if False:\n        i = 10\n    if isinstance(x, FakeTensor):\n        storage_to_nodes[StorageWeakRef(x._typed_storage())].add(n)",
            "def _add_to_map(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if isinstance(x, FakeTensor):\n        storage_to_nodes[StorageWeakRef(x._typed_storage())].add(n)",
            "def _add_to_map(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if isinstance(x, FakeTensor):\n        storage_to_nodes[StorageWeakRef(x._typed_storage())].add(n)",
            "def _add_to_map(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if isinstance(x, FakeTensor):\n        storage_to_nodes[StorageWeakRef(x._typed_storage())].add(n)",
            "def _add_to_map(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if isinstance(x, FakeTensor):\n        storage_to_nodes[StorageWeakRef(x._typed_storage())].add(n)"
        ]
    },
    {
        "func_name": "replace_arg",
        "original": "def replace_arg(a):\n    if a == old:\n        return new\n    return a",
        "mutated": [
            "def replace_arg(a):\n    if False:\n        i = 10\n    if a == old:\n        return new\n    return a",
            "def replace_arg(a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if a == old:\n        return new\n    return a",
            "def replace_arg(a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if a == old:\n        return new\n    return a",
            "def replace_arg(a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if a == old:\n        return new\n    return a",
            "def replace_arg(a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if a == old:\n        return new\n    return a"
        ]
    },
    {
        "func_name": "reinplace",
        "original": "@compatibility(is_backward_compatible=True)\ndef reinplace(gm, *sample_args):\n    \"\"\"\n    Given an fx.GraphModule, modifies it to perform \"reinplacing\",\n    mutating the nodes of the graph.\n    We look for out-of-place op call sites like `b = a.add(...)`,\n    and convert them to be inplace (`b = a.add_(...)`),\n    as long as the input to the current operator (\"a\") isn't re-used\n    anywhere later in the graph.\n\n    This pass currently expects to operate on a **functional, ATen** graph.\n    This can be obtained by running `make_fx(functionalize(f))`.\n\n    Sample inputs are needed to determine aliasing relationships of the inputs.\n    In general, we can't reinplace node `b = a.add(...)` if \"a\" aliases any of the\n    inputs to the program.\n\n    Given a node \"b = foo(a, args...) the algorithm for re-inplacing is as follows:\n\n    (1) Perform some initial checks on the metadata of \"a\" and \"args...\"\n        that can disqualify them from being reinplaced.\n\n      (1a) Check that the self argument we're attempting to reinplace\n           has acceptable dtype/size metadata to reinplace with.\n\n           For example, if we have:\n             a = torch.ones(1)\n             b = torch.ones(10)\n             out = torch.add(a, b)\n           We can't turn that into\n             a.add_(b)\n           Because that would require resizing \"a\".\n\n           Similarly, we can't convert torch.ge(a, b) into a.ge_(b),\n           because that would require changing a's dtype (from e.g. float32 to bool).\n           Note that in this specific example, we could technically do better..\n\n           If we see the pattern:\n             a_1 = a.ge(b)\n             a_2 = aten._to_copy(a_1, a.dtype)\n           Then we this should be valid to completely re-inplace\n           (this is exactly what functionalization will emit when it sees a.ge_(b)).\n\n           This optimization is only really important for user programs\n           that directly use inplace comparison ops though.\n\n           We also cannot re-inplace on tensors that have overlapping memory,\n           e.g. torch.ones(1).expand(4, 4).add_(1)\n\n      (1b) Check if \"a\" is an alias of any of the program inputs.\n\n          If it is, skip and move to the next node.\n          Inplace'ing an op that would cause it to mutate a program is not sound,\n          because that would be a side effect visible to the user.\n\n          NOTE: there's a future optimization that we should make:\n          if \"a\" is a (alias of a)  program input, but later in the program\n          there is a node that looks like \"a.copy_(...)\",\n          Then re-inplacing is ok to do - we are temporarily re-using a's buffer,\n          which will later be overwritten by the copy_() call.\n\n          This will be an important optimization to have for programs that mutate\n          their inputs. It currently isn't implemented though.\n\n      (1c) Check if \"a\" and \"args...\" alias\n\n          For example, re-inplacing to create code like the below\n          isn't guaranteed to be sound:\n\n            aten.mul_(a, a)\n\n    (2) Check that \"a\" and all of its outstanding aliases are not used anywhere\n        later in the graph. If this is the case, then it's safe to re-inplace\n        to \"b = foo_(a)\".\n\n        There are a few caveats to this, explained in more detail below:\n        (a) If \"a\" is used later as an argument to a view op, that is okay.\n            It's only a problem if \"a\" (or that view) is later passed\n            into a normal operator, or if it is returned as the program output.\n        (b) If \"a\" is a repeat argument in `foo()`, then don't reinplace.\n            Most ATen kernels don't make any guarantees that this is sound,\n            e.g. if you do aten.mul_(a, a).\n            So we'll just ban re-inplacing in this case.\n            It's only a problem if \"a\" (or that view) is later passed\n        (c) If \"a\" is used as an input into a view \"inverse\" / \"scatter\"\n            operator, it is potentially fine to re-inplace\n            (and remove that scatter operator from the graph).\n            See below for a more detailed example.\n\n        NOTE: there is an optimization in this step that is crucial\n        to fully recovering performance from functionalization.\n\n        Given this program:\n        def f(x):\n            a = torch.ops.aten.add(x, x)\n            b = torch.ops.aten.diagonal(a)\n            torch.ops.aten.fill_(b, 0)\n            return d\n\n        Functionalization will emit the following:\n        def f(x):\n            a = torch.ops.aten.add(x, x)\n            b = torch.ops.aten.diagonal(a, 0, 1)\n            b_updated = torch.ops.aten.fill(b, 0)\n            a_updated = torch.ops.aten.diagonal_scatter(a, b_updated, 0, 1)\n            return a_updated\n\n        Ordinarily, we would not be able to reinplace the fill,\n        because \"b\" aliases with \"a\" which is used by the diagonal_scatter call.\n\n        \"re-inplacing\" is on the hook for figuring out that it is ok to\n        completely, the expensive diagonal_scatter call, if we re-inplace the add().\n\n        So, for every `alias in alias_set(a)`, instead of checking\n        that \"alias\" is not used anywhere later in the graph,\n        we check that\n            EITHER:\n          (a) alias is not used anywhere later in the graph\n            OR:\n          (b) alias is used exactly once later on in the graph,\n              in the following op:\n\n                out = foo_scatter(alias, x, args...)\n\n              where the following must hold:\n                (i) \"foo_scatter\" is the \"inverse\" operator for foo.\n                    This only applies to \"foo\" ops that are view operators,\n                    which view into a subset of the original tensor's memory.\n                    In practice, there are ~4 operators where this applies:\n                      diagonal -> diagonal_scatter\n                      slice -> slice_scatter\n                      select -> select_scatter\n                      as_strided -> as_strided_scatter\n                (ii) \"args...\" are the same between the foo() and foo_scatter() calls.\n\n    (3) Perform the actual re-inplacing on foo!\n\n      (3b) is the common case, but special care is needed for {view}_scatter (3a)\n\n      (3a) {view}_scatter ops.\n\n        Consider this program:\n          a = torch.zeros(2, 2)\n          b = torch.ones(2)\n          a[0] = b\n\n        Post functionalization, that will look like:\n          a = torch.zeros(2)\n          b = torch.ones(1)\n          a_updated = torch.select_scatter(a, b, 0, 0)\n\n        In this case though, there is no \"functional\" op to re-inplace!\n        Instead, we'd like to directly remove toe select_scatter call.\n        We already know from (3) that this is valid,\n        because \"a\" has no later usages in the graph.\n\n        We perform the re-inplacing on the {view}_scatter op like so\n        Before:\n          a_updated = torch.select_scatter(a, b, args...)\n        After:\n          a_slice = a.select(a, args...)\n          a_slice.copy_(b)\n\n      (3b) Otherwise, replace the functional op with its inplace variant.\n        Before:\n          b = foo(a, args...)\n        After:\n          a.foo_(args...)\n\n    (4) Finally, after converting either:\n          Before:\n            b = foo(a)\n          After:\n            foo_(a)\n        or\n          Before:\n            b = {slice}_scatter(a, mutated_slice, args...)\n          After:\n            slice = {slice}(a, args...)\n            slice.copy_(mutated_slice)\n\n        We now need to find all later nodes that use \"b\" as an argument\n        and update them to take in \"a\" instead.\n\n        Note that for the majority of inplace ops, this isn't actually necessary\n        (because most inplace ops return \"self\" as their output).\n        This isn't generally true for all mutable ops though, which is why\n        we need to actually replace all of the arguments.\n\n        We also need to update our metadata of Dict[StorageWeakRef, Set[Node]],\n        That maps a given tensor storage to the set of all nodes that take in that storage\n        as an input.\n        Specifically, re-inplacing `b = foo(a)` causes \"a\" and \"b\"'s sets to get fused\n        together.\n\n    (5) Any \"view_inverse/scatter\" nodes that were identified as \"it's ok to ignore them\"\n        during step (3) get manually deleted from the graph.\n        Their outputs are no longer used, so technically standard DCE would be able\n        to do this, but we can no longer run FX's DCE pass now that we have mutable\n        ops in the graph.\n    \"\"\"\n    _FunctionalizationMetadataProp(gm).propagate(*sample_args)\n    input_storages = {StorageWeakRef(node.meta['fake_result']._typed_storage()) for node in gm.graph.nodes if node.op == 'placeholder'}\n    storage_to_nodes: Dict[StorageWeakRef, Set[Node]] = defaultdict(set)\n    for n in gm.graph.nodes:\n        if 'fake_result' in n.meta:\n\n            def _add_to_map(x):\n                if isinstance(x, FakeTensor):\n                    storage_to_nodes[StorageWeakRef(x._typed_storage())].add(n)\n            pytree.tree_map_(_add_to_map, n.meta['fake_result'])\n    all_later_view_inverse_nodes_to_delete = set()\n    for (idx, node) in enumerate(gm.graph.nodes):\n        if node.op == 'call_function':\n            if not isinstance(node.target, torch._ops.OpOverload):\n                continue\n            if len(node.target._schema.arguments) < 1:\n                continue\n            if type(node.target._schema.arguments[0].type) != torch.TensorType:\n                continue\n            self_arg = node.args[0]\n            self_flattened = pytree.tree_leaves(self_arg.meta['fake_result'])\n            node_flattened = pytree.tree_leaves(node.meta['fake_result'])\n            self_has_wrong_metadata = False\n            if len(self_flattened) == len(node_flattened):\n                for (self_meta, node_meta) in zip(self_flattened, node_flattened):\n                    if self_meta.numel() != node_meta.numel():\n                        self_has_wrong_metadata = True\n                    if self_meta.dtype != node_meta.dtype:\n                        self_has_wrong_metadata = True\n                    if torch._debug_has_internal_overlap(self_meta) == 1:\n                        self_has_wrong_metadata = True\n            if self_has_wrong_metadata and node.target != torch.ops.aten.resize.default:\n                continue\n            self_arg_name = self_arg.name\n            self_arg_storage = StorageWeakRef(self_arg.meta['fake_result']._typed_storage())\n            if self_arg_storage in input_storages:\n                continue\n            if len([x for x in node.args if x is self_arg]) > 1:\n                continue\n            self_arg_storage = StorageWeakRef(self_arg.meta['fake_result']._typed_storage())\n            self_aliases = storage_to_nodes[self_arg_storage]\n            later_node_usages = _get_all_later_node_usages(self_aliases, node.meta['node_idx'])\n            later_view_inverse_node_usages = _get_view_inverse_node_usages(later_node_usages, self_aliases)\n            can_reinplace = len(later_node_usages - later_view_inverse_node_usages) == 0\n            if not can_reinplace:\n                continue\n            if node.target in _VIEW_INVERSE_MAP and node not in all_later_view_inverse_nodes_to_delete:\n                view_op = _VIEW_INVERSE_MAP[node.target]\n                with gm.graph.inserting_before(node):\n                    mutated_slice_node = node.args[1]\n                    remaining_slice_args = node.args[2:]\n                    slice_node = gm.graph.create_node('call_function', view_op, (self_arg,) + tuple(remaining_slice_args), node.kwargs)\n                    copy_node = gm.graph.create_node('call_function', torch.ops.aten.copy_.default, (slice_node, mutated_slice_node), {})\n                all_later_view_inverse_nodes_to_delete.add(node)\n            else:\n                maybe_inplace_op = _maybe_get_inplace_op(node.target)\n                if maybe_inplace_op is None:\n                    continue\n                node.target = maybe_inplace_op\n            curr_node_storage = StorageWeakRef(node.meta['fake_result']._typed_storage())\n            storage_to_nodes[self_arg_storage].update(storage_to_nodes[curr_node_storage])\n            storage_to_nodes[curr_node_storage].update(storage_to_nodes[self_arg_storage])\n            all_later_view_inverse_nodes_to_delete.update(later_view_inverse_node_usages)\n            for old in itertools.chain([node], later_view_inverse_node_usages):\n                new = old.args[0]\n                nodes_to_update = [n for n in old.users if n.meta['node_idx'] > node.meta['node_idx']]\n                for node_to_update in nodes_to_update:\n                    new_args = []\n                    args = node_to_update.args\n\n                    def replace_arg(a):\n                        if a == old:\n                            return new\n                        return a\n                    node_to_update.args = tree_map_only(Node, replace_arg, node_to_update.args)\n                    node_to_update.kwargs = tree_map_only(Node, replace_arg, node_to_update.kwargs)\n                    old_flattened_res = pytree.tree_leaves(old.meta['fake_result'])\n                    node_flattened_res = pytree.tree_leaves(node_to_update.meta['fake_result'])\n                    old_res_storage = {StorageWeakRef(x._typed_storage()) for x in old_flattened_res if isinstance(x, FakeTensor)}\n                    node_res_storage = {StorageWeakRef(x._typed_storage()) for x in node_flattened_res if isinstance(x, FakeTensor)}\n                    if len(old_res_storage) == 1 and len(node_res_storage) == 1 and (old_res_storage == node_res_storage):\n                        new_flattened_res = pytree.tree_leaves(new.meta['fake_result'])\n                        new_res_storage = {StorageWeakRef(x._typed_storage()) for x in new_flattened_res if isinstance(x, FakeTensor)}\n                        assert len(new_res_storage) == 1\n                        (old_ref,) = old_res_storage\n                        (new_ref,) = new_res_storage\n                        (node_ref,) = node_res_storage\n                        storage_to_nodes[node_ref].update(storage_to_nodes[new_ref])\n                        storage_to_nodes[new_ref].update(storage_to_nodes[node_ref])\n    for to_delete in all_later_view_inverse_nodes_to_delete:\n        gm.graph.erase_node(to_delete)\n    gm.recompile()\n    return gm",
        "mutated": [
            "@compatibility(is_backward_compatible=True)\ndef reinplace(gm, *sample_args):\n    if False:\n        i = 10\n    '\\n    Given an fx.GraphModule, modifies it to perform \"reinplacing\",\\n    mutating the nodes of the graph.\\n    We look for out-of-place op call sites like `b = a.add(...)`,\\n    and convert them to be inplace (`b = a.add_(...)`),\\n    as long as the input to the current operator (\"a\") isn\\'t re-used\\n    anywhere later in the graph.\\n\\n    This pass currently expects to operate on a **functional, ATen** graph.\\n    This can be obtained by running `make_fx(functionalize(f))`.\\n\\n    Sample inputs are needed to determine aliasing relationships of the inputs.\\n    In general, we can\\'t reinplace node `b = a.add(...)` if \"a\" aliases any of the\\n    inputs to the program.\\n\\n    Given a node \"b = foo(a, args...) the algorithm for re-inplacing is as follows:\\n\\n    (1) Perform some initial checks on the metadata of \"a\" and \"args...\"\\n        that can disqualify them from being reinplaced.\\n\\n      (1a) Check that the self argument we\\'re attempting to reinplace\\n           has acceptable dtype/size metadata to reinplace with.\\n\\n           For example, if we have:\\n             a = torch.ones(1)\\n             b = torch.ones(10)\\n             out = torch.add(a, b)\\n           We can\\'t turn that into\\n             a.add_(b)\\n           Because that would require resizing \"a\".\\n\\n           Similarly, we can\\'t convert torch.ge(a, b) into a.ge_(b),\\n           because that would require changing a\\'s dtype (from e.g. float32 to bool).\\n           Note that in this specific example, we could technically do better..\\n\\n           If we see the pattern:\\n             a_1 = a.ge(b)\\n             a_2 = aten._to_copy(a_1, a.dtype)\\n           Then we this should be valid to completely re-inplace\\n           (this is exactly what functionalization will emit when it sees a.ge_(b)).\\n\\n           This optimization is only really important for user programs\\n           that directly use inplace comparison ops though.\\n\\n           We also cannot re-inplace on tensors that have overlapping memory,\\n           e.g. torch.ones(1).expand(4, 4).add_(1)\\n\\n      (1b) Check if \"a\" is an alias of any of the program inputs.\\n\\n          If it is, skip and move to the next node.\\n          Inplace\\'ing an op that would cause it to mutate a program is not sound,\\n          because that would be a side effect visible to the user.\\n\\n          NOTE: there\\'s a future optimization that we should make:\\n          if \"a\" is a (alias of a)  program input, but later in the program\\n          there is a node that looks like \"a.copy_(...)\",\\n          Then re-inplacing is ok to do - we are temporarily re-using a\\'s buffer,\\n          which will later be overwritten by the copy_() call.\\n\\n          This will be an important optimization to have for programs that mutate\\n          their inputs. It currently isn\\'t implemented though.\\n\\n      (1c) Check if \"a\" and \"args...\" alias\\n\\n          For example, re-inplacing to create code like the below\\n          isn\\'t guaranteed to be sound:\\n\\n            aten.mul_(a, a)\\n\\n    (2) Check that \"a\" and all of its outstanding aliases are not used anywhere\\n        later in the graph. If this is the case, then it\\'s safe to re-inplace\\n        to \"b = foo_(a)\".\\n\\n        There are a few caveats to this, explained in more detail below:\\n        (a) If \"a\" is used later as an argument to a view op, that is okay.\\n            It\\'s only a problem if \"a\" (or that view) is later passed\\n            into a normal operator, or if it is returned as the program output.\\n        (b) If \"a\" is a repeat argument in `foo()`, then don\\'t reinplace.\\n            Most ATen kernels don\\'t make any guarantees that this is sound,\\n            e.g. if you do aten.mul_(a, a).\\n            So we\\'ll just ban re-inplacing in this case.\\n            It\\'s only a problem if \"a\" (or that view) is later passed\\n        (c) If \"a\" is used as an input into a view \"inverse\" / \"scatter\"\\n            operator, it is potentially fine to re-inplace\\n            (and remove that scatter operator from the graph).\\n            See below for a more detailed example.\\n\\n        NOTE: there is an optimization in this step that is crucial\\n        to fully recovering performance from functionalization.\\n\\n        Given this program:\\n        def f(x):\\n            a = torch.ops.aten.add(x, x)\\n            b = torch.ops.aten.diagonal(a)\\n            torch.ops.aten.fill_(b, 0)\\n            return d\\n\\n        Functionalization will emit the following:\\n        def f(x):\\n            a = torch.ops.aten.add(x, x)\\n            b = torch.ops.aten.diagonal(a, 0, 1)\\n            b_updated = torch.ops.aten.fill(b, 0)\\n            a_updated = torch.ops.aten.diagonal_scatter(a, b_updated, 0, 1)\\n            return a_updated\\n\\n        Ordinarily, we would not be able to reinplace the fill,\\n        because \"b\" aliases with \"a\" which is used by the diagonal_scatter call.\\n\\n        \"re-inplacing\" is on the hook for figuring out that it is ok to\\n        completely, the expensive diagonal_scatter call, if we re-inplace the add().\\n\\n        So, for every `alias in alias_set(a)`, instead of checking\\n        that \"alias\" is not used anywhere later in the graph,\\n        we check that\\n            EITHER:\\n          (a) alias is not used anywhere later in the graph\\n            OR:\\n          (b) alias is used exactly once later on in the graph,\\n              in the following op:\\n\\n                out = foo_scatter(alias, x, args...)\\n\\n              where the following must hold:\\n                (i) \"foo_scatter\" is the \"inverse\" operator for foo.\\n                    This only applies to \"foo\" ops that are view operators,\\n                    which view into a subset of the original tensor\\'s memory.\\n                    In practice, there are ~4 operators where this applies:\\n                      diagonal -> diagonal_scatter\\n                      slice -> slice_scatter\\n                      select -> select_scatter\\n                      as_strided -> as_strided_scatter\\n                (ii) \"args...\" are the same between the foo() and foo_scatter() calls.\\n\\n    (3) Perform the actual re-inplacing on foo!\\n\\n      (3b) is the common case, but special care is needed for {view}_scatter (3a)\\n\\n      (3a) {view}_scatter ops.\\n\\n        Consider this program:\\n          a = torch.zeros(2, 2)\\n          b = torch.ones(2)\\n          a[0] = b\\n\\n        Post functionalization, that will look like:\\n          a = torch.zeros(2)\\n          b = torch.ones(1)\\n          a_updated = torch.select_scatter(a, b, 0, 0)\\n\\n        In this case though, there is no \"functional\" op to re-inplace!\\n        Instead, we\\'d like to directly remove toe select_scatter call.\\n        We already know from (3) that this is valid,\\n        because \"a\" has no later usages in the graph.\\n\\n        We perform the re-inplacing on the {view}_scatter op like so\\n        Before:\\n          a_updated = torch.select_scatter(a, b, args...)\\n        After:\\n          a_slice = a.select(a, args...)\\n          a_slice.copy_(b)\\n\\n      (3b) Otherwise, replace the functional op with its inplace variant.\\n        Before:\\n          b = foo(a, args...)\\n        After:\\n          a.foo_(args...)\\n\\n    (4) Finally, after converting either:\\n          Before:\\n            b = foo(a)\\n          After:\\n            foo_(a)\\n        or\\n          Before:\\n            b = {slice}_scatter(a, mutated_slice, args...)\\n          After:\\n            slice = {slice}(a, args...)\\n            slice.copy_(mutated_slice)\\n\\n        We now need to find all later nodes that use \"b\" as an argument\\n        and update them to take in \"a\" instead.\\n\\n        Note that for the majority of inplace ops, this isn\\'t actually necessary\\n        (because most inplace ops return \"self\" as their output).\\n        This isn\\'t generally true for all mutable ops though, which is why\\n        we need to actually replace all of the arguments.\\n\\n        We also need to update our metadata of Dict[StorageWeakRef, Set[Node]],\\n        That maps a given tensor storage to the set of all nodes that take in that storage\\n        as an input.\\n        Specifically, re-inplacing `b = foo(a)` causes \"a\" and \"b\"\\'s sets to get fused\\n        together.\\n\\n    (5) Any \"view_inverse/scatter\" nodes that were identified as \"it\\'s ok to ignore them\"\\n        during step (3) get manually deleted from the graph.\\n        Their outputs are no longer used, so technically standard DCE would be able\\n        to do this, but we can no longer run FX\\'s DCE pass now that we have mutable\\n        ops in the graph.\\n    '\n    _FunctionalizationMetadataProp(gm).propagate(*sample_args)\n    input_storages = {StorageWeakRef(node.meta['fake_result']._typed_storage()) for node in gm.graph.nodes if node.op == 'placeholder'}\n    storage_to_nodes: Dict[StorageWeakRef, Set[Node]] = defaultdict(set)\n    for n in gm.graph.nodes:\n        if 'fake_result' in n.meta:\n\n            def _add_to_map(x):\n                if isinstance(x, FakeTensor):\n                    storage_to_nodes[StorageWeakRef(x._typed_storage())].add(n)\n            pytree.tree_map_(_add_to_map, n.meta['fake_result'])\n    all_later_view_inverse_nodes_to_delete = set()\n    for (idx, node) in enumerate(gm.graph.nodes):\n        if node.op == 'call_function':\n            if not isinstance(node.target, torch._ops.OpOverload):\n                continue\n            if len(node.target._schema.arguments) < 1:\n                continue\n            if type(node.target._schema.arguments[0].type) != torch.TensorType:\n                continue\n            self_arg = node.args[0]\n            self_flattened = pytree.tree_leaves(self_arg.meta['fake_result'])\n            node_flattened = pytree.tree_leaves(node.meta['fake_result'])\n            self_has_wrong_metadata = False\n            if len(self_flattened) == len(node_flattened):\n                for (self_meta, node_meta) in zip(self_flattened, node_flattened):\n                    if self_meta.numel() != node_meta.numel():\n                        self_has_wrong_metadata = True\n                    if self_meta.dtype != node_meta.dtype:\n                        self_has_wrong_metadata = True\n                    if torch._debug_has_internal_overlap(self_meta) == 1:\n                        self_has_wrong_metadata = True\n            if self_has_wrong_metadata and node.target != torch.ops.aten.resize.default:\n                continue\n            self_arg_name = self_arg.name\n            self_arg_storage = StorageWeakRef(self_arg.meta['fake_result']._typed_storage())\n            if self_arg_storage in input_storages:\n                continue\n            if len([x for x in node.args if x is self_arg]) > 1:\n                continue\n            self_arg_storage = StorageWeakRef(self_arg.meta['fake_result']._typed_storage())\n            self_aliases = storage_to_nodes[self_arg_storage]\n            later_node_usages = _get_all_later_node_usages(self_aliases, node.meta['node_idx'])\n            later_view_inverse_node_usages = _get_view_inverse_node_usages(later_node_usages, self_aliases)\n            can_reinplace = len(later_node_usages - later_view_inverse_node_usages) == 0\n            if not can_reinplace:\n                continue\n            if node.target in _VIEW_INVERSE_MAP and node not in all_later_view_inverse_nodes_to_delete:\n                view_op = _VIEW_INVERSE_MAP[node.target]\n                with gm.graph.inserting_before(node):\n                    mutated_slice_node = node.args[1]\n                    remaining_slice_args = node.args[2:]\n                    slice_node = gm.graph.create_node('call_function', view_op, (self_arg,) + tuple(remaining_slice_args), node.kwargs)\n                    copy_node = gm.graph.create_node('call_function', torch.ops.aten.copy_.default, (slice_node, mutated_slice_node), {})\n                all_later_view_inverse_nodes_to_delete.add(node)\n            else:\n                maybe_inplace_op = _maybe_get_inplace_op(node.target)\n                if maybe_inplace_op is None:\n                    continue\n                node.target = maybe_inplace_op\n            curr_node_storage = StorageWeakRef(node.meta['fake_result']._typed_storage())\n            storage_to_nodes[self_arg_storage].update(storage_to_nodes[curr_node_storage])\n            storage_to_nodes[curr_node_storage].update(storage_to_nodes[self_arg_storage])\n            all_later_view_inverse_nodes_to_delete.update(later_view_inverse_node_usages)\n            for old in itertools.chain([node], later_view_inverse_node_usages):\n                new = old.args[0]\n                nodes_to_update = [n for n in old.users if n.meta['node_idx'] > node.meta['node_idx']]\n                for node_to_update in nodes_to_update:\n                    new_args = []\n                    args = node_to_update.args\n\n                    def replace_arg(a):\n                        if a == old:\n                            return new\n                        return a\n                    node_to_update.args = tree_map_only(Node, replace_arg, node_to_update.args)\n                    node_to_update.kwargs = tree_map_only(Node, replace_arg, node_to_update.kwargs)\n                    old_flattened_res = pytree.tree_leaves(old.meta['fake_result'])\n                    node_flattened_res = pytree.tree_leaves(node_to_update.meta['fake_result'])\n                    old_res_storage = {StorageWeakRef(x._typed_storage()) for x in old_flattened_res if isinstance(x, FakeTensor)}\n                    node_res_storage = {StorageWeakRef(x._typed_storage()) for x in node_flattened_res if isinstance(x, FakeTensor)}\n                    if len(old_res_storage) == 1 and len(node_res_storage) == 1 and (old_res_storage == node_res_storage):\n                        new_flattened_res = pytree.tree_leaves(new.meta['fake_result'])\n                        new_res_storage = {StorageWeakRef(x._typed_storage()) for x in new_flattened_res if isinstance(x, FakeTensor)}\n                        assert len(new_res_storage) == 1\n                        (old_ref,) = old_res_storage\n                        (new_ref,) = new_res_storage\n                        (node_ref,) = node_res_storage\n                        storage_to_nodes[node_ref].update(storage_to_nodes[new_ref])\n                        storage_to_nodes[new_ref].update(storage_to_nodes[node_ref])\n    for to_delete in all_later_view_inverse_nodes_to_delete:\n        gm.graph.erase_node(to_delete)\n    gm.recompile()\n    return gm",
            "@compatibility(is_backward_compatible=True)\ndef reinplace(gm, *sample_args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Given an fx.GraphModule, modifies it to perform \"reinplacing\",\\n    mutating the nodes of the graph.\\n    We look for out-of-place op call sites like `b = a.add(...)`,\\n    and convert them to be inplace (`b = a.add_(...)`),\\n    as long as the input to the current operator (\"a\") isn\\'t re-used\\n    anywhere later in the graph.\\n\\n    This pass currently expects to operate on a **functional, ATen** graph.\\n    This can be obtained by running `make_fx(functionalize(f))`.\\n\\n    Sample inputs are needed to determine aliasing relationships of the inputs.\\n    In general, we can\\'t reinplace node `b = a.add(...)` if \"a\" aliases any of the\\n    inputs to the program.\\n\\n    Given a node \"b = foo(a, args...) the algorithm for re-inplacing is as follows:\\n\\n    (1) Perform some initial checks on the metadata of \"a\" and \"args...\"\\n        that can disqualify them from being reinplaced.\\n\\n      (1a) Check that the self argument we\\'re attempting to reinplace\\n           has acceptable dtype/size metadata to reinplace with.\\n\\n           For example, if we have:\\n             a = torch.ones(1)\\n             b = torch.ones(10)\\n             out = torch.add(a, b)\\n           We can\\'t turn that into\\n             a.add_(b)\\n           Because that would require resizing \"a\".\\n\\n           Similarly, we can\\'t convert torch.ge(a, b) into a.ge_(b),\\n           because that would require changing a\\'s dtype (from e.g. float32 to bool).\\n           Note that in this specific example, we could technically do better..\\n\\n           If we see the pattern:\\n             a_1 = a.ge(b)\\n             a_2 = aten._to_copy(a_1, a.dtype)\\n           Then we this should be valid to completely re-inplace\\n           (this is exactly what functionalization will emit when it sees a.ge_(b)).\\n\\n           This optimization is only really important for user programs\\n           that directly use inplace comparison ops though.\\n\\n           We also cannot re-inplace on tensors that have overlapping memory,\\n           e.g. torch.ones(1).expand(4, 4).add_(1)\\n\\n      (1b) Check if \"a\" is an alias of any of the program inputs.\\n\\n          If it is, skip and move to the next node.\\n          Inplace\\'ing an op that would cause it to mutate a program is not sound,\\n          because that would be a side effect visible to the user.\\n\\n          NOTE: there\\'s a future optimization that we should make:\\n          if \"a\" is a (alias of a)  program input, but later in the program\\n          there is a node that looks like \"a.copy_(...)\",\\n          Then re-inplacing is ok to do - we are temporarily re-using a\\'s buffer,\\n          which will later be overwritten by the copy_() call.\\n\\n          This will be an important optimization to have for programs that mutate\\n          their inputs. It currently isn\\'t implemented though.\\n\\n      (1c) Check if \"a\" and \"args...\" alias\\n\\n          For example, re-inplacing to create code like the below\\n          isn\\'t guaranteed to be sound:\\n\\n            aten.mul_(a, a)\\n\\n    (2) Check that \"a\" and all of its outstanding aliases are not used anywhere\\n        later in the graph. If this is the case, then it\\'s safe to re-inplace\\n        to \"b = foo_(a)\".\\n\\n        There are a few caveats to this, explained in more detail below:\\n        (a) If \"a\" is used later as an argument to a view op, that is okay.\\n            It\\'s only a problem if \"a\" (or that view) is later passed\\n            into a normal operator, or if it is returned as the program output.\\n        (b) If \"a\" is a repeat argument in `foo()`, then don\\'t reinplace.\\n            Most ATen kernels don\\'t make any guarantees that this is sound,\\n            e.g. if you do aten.mul_(a, a).\\n            So we\\'ll just ban re-inplacing in this case.\\n            It\\'s only a problem if \"a\" (or that view) is later passed\\n        (c) If \"a\" is used as an input into a view \"inverse\" / \"scatter\"\\n            operator, it is potentially fine to re-inplace\\n            (and remove that scatter operator from the graph).\\n            See below for a more detailed example.\\n\\n        NOTE: there is an optimization in this step that is crucial\\n        to fully recovering performance from functionalization.\\n\\n        Given this program:\\n        def f(x):\\n            a = torch.ops.aten.add(x, x)\\n            b = torch.ops.aten.diagonal(a)\\n            torch.ops.aten.fill_(b, 0)\\n            return d\\n\\n        Functionalization will emit the following:\\n        def f(x):\\n            a = torch.ops.aten.add(x, x)\\n            b = torch.ops.aten.diagonal(a, 0, 1)\\n            b_updated = torch.ops.aten.fill(b, 0)\\n            a_updated = torch.ops.aten.diagonal_scatter(a, b_updated, 0, 1)\\n            return a_updated\\n\\n        Ordinarily, we would not be able to reinplace the fill,\\n        because \"b\" aliases with \"a\" which is used by the diagonal_scatter call.\\n\\n        \"re-inplacing\" is on the hook for figuring out that it is ok to\\n        completely, the expensive diagonal_scatter call, if we re-inplace the add().\\n\\n        So, for every `alias in alias_set(a)`, instead of checking\\n        that \"alias\" is not used anywhere later in the graph,\\n        we check that\\n            EITHER:\\n          (a) alias is not used anywhere later in the graph\\n            OR:\\n          (b) alias is used exactly once later on in the graph,\\n              in the following op:\\n\\n                out = foo_scatter(alias, x, args...)\\n\\n              where the following must hold:\\n                (i) \"foo_scatter\" is the \"inverse\" operator for foo.\\n                    This only applies to \"foo\" ops that are view operators,\\n                    which view into a subset of the original tensor\\'s memory.\\n                    In practice, there are ~4 operators where this applies:\\n                      diagonal -> diagonal_scatter\\n                      slice -> slice_scatter\\n                      select -> select_scatter\\n                      as_strided -> as_strided_scatter\\n                (ii) \"args...\" are the same between the foo() and foo_scatter() calls.\\n\\n    (3) Perform the actual re-inplacing on foo!\\n\\n      (3b) is the common case, but special care is needed for {view}_scatter (3a)\\n\\n      (3a) {view}_scatter ops.\\n\\n        Consider this program:\\n          a = torch.zeros(2, 2)\\n          b = torch.ones(2)\\n          a[0] = b\\n\\n        Post functionalization, that will look like:\\n          a = torch.zeros(2)\\n          b = torch.ones(1)\\n          a_updated = torch.select_scatter(a, b, 0, 0)\\n\\n        In this case though, there is no \"functional\" op to re-inplace!\\n        Instead, we\\'d like to directly remove toe select_scatter call.\\n        We already know from (3) that this is valid,\\n        because \"a\" has no later usages in the graph.\\n\\n        We perform the re-inplacing on the {view}_scatter op like so\\n        Before:\\n          a_updated = torch.select_scatter(a, b, args...)\\n        After:\\n          a_slice = a.select(a, args...)\\n          a_slice.copy_(b)\\n\\n      (3b) Otherwise, replace the functional op with its inplace variant.\\n        Before:\\n          b = foo(a, args...)\\n        After:\\n          a.foo_(args...)\\n\\n    (4) Finally, after converting either:\\n          Before:\\n            b = foo(a)\\n          After:\\n            foo_(a)\\n        or\\n          Before:\\n            b = {slice}_scatter(a, mutated_slice, args...)\\n          After:\\n            slice = {slice}(a, args...)\\n            slice.copy_(mutated_slice)\\n\\n        We now need to find all later nodes that use \"b\" as an argument\\n        and update them to take in \"a\" instead.\\n\\n        Note that for the majority of inplace ops, this isn\\'t actually necessary\\n        (because most inplace ops return \"self\" as their output).\\n        This isn\\'t generally true for all mutable ops though, which is why\\n        we need to actually replace all of the arguments.\\n\\n        We also need to update our metadata of Dict[StorageWeakRef, Set[Node]],\\n        That maps a given tensor storage to the set of all nodes that take in that storage\\n        as an input.\\n        Specifically, re-inplacing `b = foo(a)` causes \"a\" and \"b\"\\'s sets to get fused\\n        together.\\n\\n    (5) Any \"view_inverse/scatter\" nodes that were identified as \"it\\'s ok to ignore them\"\\n        during step (3) get manually deleted from the graph.\\n        Their outputs are no longer used, so technically standard DCE would be able\\n        to do this, but we can no longer run FX\\'s DCE pass now that we have mutable\\n        ops in the graph.\\n    '\n    _FunctionalizationMetadataProp(gm).propagate(*sample_args)\n    input_storages = {StorageWeakRef(node.meta['fake_result']._typed_storage()) for node in gm.graph.nodes if node.op == 'placeholder'}\n    storage_to_nodes: Dict[StorageWeakRef, Set[Node]] = defaultdict(set)\n    for n in gm.graph.nodes:\n        if 'fake_result' in n.meta:\n\n            def _add_to_map(x):\n                if isinstance(x, FakeTensor):\n                    storage_to_nodes[StorageWeakRef(x._typed_storage())].add(n)\n            pytree.tree_map_(_add_to_map, n.meta['fake_result'])\n    all_later_view_inverse_nodes_to_delete = set()\n    for (idx, node) in enumerate(gm.graph.nodes):\n        if node.op == 'call_function':\n            if not isinstance(node.target, torch._ops.OpOverload):\n                continue\n            if len(node.target._schema.arguments) < 1:\n                continue\n            if type(node.target._schema.arguments[0].type) != torch.TensorType:\n                continue\n            self_arg = node.args[0]\n            self_flattened = pytree.tree_leaves(self_arg.meta['fake_result'])\n            node_flattened = pytree.tree_leaves(node.meta['fake_result'])\n            self_has_wrong_metadata = False\n            if len(self_flattened) == len(node_flattened):\n                for (self_meta, node_meta) in zip(self_flattened, node_flattened):\n                    if self_meta.numel() != node_meta.numel():\n                        self_has_wrong_metadata = True\n                    if self_meta.dtype != node_meta.dtype:\n                        self_has_wrong_metadata = True\n                    if torch._debug_has_internal_overlap(self_meta) == 1:\n                        self_has_wrong_metadata = True\n            if self_has_wrong_metadata and node.target != torch.ops.aten.resize.default:\n                continue\n            self_arg_name = self_arg.name\n            self_arg_storage = StorageWeakRef(self_arg.meta['fake_result']._typed_storage())\n            if self_arg_storage in input_storages:\n                continue\n            if len([x for x in node.args if x is self_arg]) > 1:\n                continue\n            self_arg_storage = StorageWeakRef(self_arg.meta['fake_result']._typed_storage())\n            self_aliases = storage_to_nodes[self_arg_storage]\n            later_node_usages = _get_all_later_node_usages(self_aliases, node.meta['node_idx'])\n            later_view_inverse_node_usages = _get_view_inverse_node_usages(later_node_usages, self_aliases)\n            can_reinplace = len(later_node_usages - later_view_inverse_node_usages) == 0\n            if not can_reinplace:\n                continue\n            if node.target in _VIEW_INVERSE_MAP and node not in all_later_view_inverse_nodes_to_delete:\n                view_op = _VIEW_INVERSE_MAP[node.target]\n                with gm.graph.inserting_before(node):\n                    mutated_slice_node = node.args[1]\n                    remaining_slice_args = node.args[2:]\n                    slice_node = gm.graph.create_node('call_function', view_op, (self_arg,) + tuple(remaining_slice_args), node.kwargs)\n                    copy_node = gm.graph.create_node('call_function', torch.ops.aten.copy_.default, (slice_node, mutated_slice_node), {})\n                all_later_view_inverse_nodes_to_delete.add(node)\n            else:\n                maybe_inplace_op = _maybe_get_inplace_op(node.target)\n                if maybe_inplace_op is None:\n                    continue\n                node.target = maybe_inplace_op\n            curr_node_storage = StorageWeakRef(node.meta['fake_result']._typed_storage())\n            storage_to_nodes[self_arg_storage].update(storage_to_nodes[curr_node_storage])\n            storage_to_nodes[curr_node_storage].update(storage_to_nodes[self_arg_storage])\n            all_later_view_inverse_nodes_to_delete.update(later_view_inverse_node_usages)\n            for old in itertools.chain([node], later_view_inverse_node_usages):\n                new = old.args[0]\n                nodes_to_update = [n for n in old.users if n.meta['node_idx'] > node.meta['node_idx']]\n                for node_to_update in nodes_to_update:\n                    new_args = []\n                    args = node_to_update.args\n\n                    def replace_arg(a):\n                        if a == old:\n                            return new\n                        return a\n                    node_to_update.args = tree_map_only(Node, replace_arg, node_to_update.args)\n                    node_to_update.kwargs = tree_map_only(Node, replace_arg, node_to_update.kwargs)\n                    old_flattened_res = pytree.tree_leaves(old.meta['fake_result'])\n                    node_flattened_res = pytree.tree_leaves(node_to_update.meta['fake_result'])\n                    old_res_storage = {StorageWeakRef(x._typed_storage()) for x in old_flattened_res if isinstance(x, FakeTensor)}\n                    node_res_storage = {StorageWeakRef(x._typed_storage()) for x in node_flattened_res if isinstance(x, FakeTensor)}\n                    if len(old_res_storage) == 1 and len(node_res_storage) == 1 and (old_res_storage == node_res_storage):\n                        new_flattened_res = pytree.tree_leaves(new.meta['fake_result'])\n                        new_res_storage = {StorageWeakRef(x._typed_storage()) for x in new_flattened_res if isinstance(x, FakeTensor)}\n                        assert len(new_res_storage) == 1\n                        (old_ref,) = old_res_storage\n                        (new_ref,) = new_res_storage\n                        (node_ref,) = node_res_storage\n                        storage_to_nodes[node_ref].update(storage_to_nodes[new_ref])\n                        storage_to_nodes[new_ref].update(storage_to_nodes[node_ref])\n    for to_delete in all_later_view_inverse_nodes_to_delete:\n        gm.graph.erase_node(to_delete)\n    gm.recompile()\n    return gm",
            "@compatibility(is_backward_compatible=True)\ndef reinplace(gm, *sample_args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Given an fx.GraphModule, modifies it to perform \"reinplacing\",\\n    mutating the nodes of the graph.\\n    We look for out-of-place op call sites like `b = a.add(...)`,\\n    and convert them to be inplace (`b = a.add_(...)`),\\n    as long as the input to the current operator (\"a\") isn\\'t re-used\\n    anywhere later in the graph.\\n\\n    This pass currently expects to operate on a **functional, ATen** graph.\\n    This can be obtained by running `make_fx(functionalize(f))`.\\n\\n    Sample inputs are needed to determine aliasing relationships of the inputs.\\n    In general, we can\\'t reinplace node `b = a.add(...)` if \"a\" aliases any of the\\n    inputs to the program.\\n\\n    Given a node \"b = foo(a, args...) the algorithm for re-inplacing is as follows:\\n\\n    (1) Perform some initial checks on the metadata of \"a\" and \"args...\"\\n        that can disqualify them from being reinplaced.\\n\\n      (1a) Check that the self argument we\\'re attempting to reinplace\\n           has acceptable dtype/size metadata to reinplace with.\\n\\n           For example, if we have:\\n             a = torch.ones(1)\\n             b = torch.ones(10)\\n             out = torch.add(a, b)\\n           We can\\'t turn that into\\n             a.add_(b)\\n           Because that would require resizing \"a\".\\n\\n           Similarly, we can\\'t convert torch.ge(a, b) into a.ge_(b),\\n           because that would require changing a\\'s dtype (from e.g. float32 to bool).\\n           Note that in this specific example, we could technically do better..\\n\\n           If we see the pattern:\\n             a_1 = a.ge(b)\\n             a_2 = aten._to_copy(a_1, a.dtype)\\n           Then we this should be valid to completely re-inplace\\n           (this is exactly what functionalization will emit when it sees a.ge_(b)).\\n\\n           This optimization is only really important for user programs\\n           that directly use inplace comparison ops though.\\n\\n           We also cannot re-inplace on tensors that have overlapping memory,\\n           e.g. torch.ones(1).expand(4, 4).add_(1)\\n\\n      (1b) Check if \"a\" is an alias of any of the program inputs.\\n\\n          If it is, skip and move to the next node.\\n          Inplace\\'ing an op that would cause it to mutate a program is not sound,\\n          because that would be a side effect visible to the user.\\n\\n          NOTE: there\\'s a future optimization that we should make:\\n          if \"a\" is a (alias of a)  program input, but later in the program\\n          there is a node that looks like \"a.copy_(...)\",\\n          Then re-inplacing is ok to do - we are temporarily re-using a\\'s buffer,\\n          which will later be overwritten by the copy_() call.\\n\\n          This will be an important optimization to have for programs that mutate\\n          their inputs. It currently isn\\'t implemented though.\\n\\n      (1c) Check if \"a\" and \"args...\" alias\\n\\n          For example, re-inplacing to create code like the below\\n          isn\\'t guaranteed to be sound:\\n\\n            aten.mul_(a, a)\\n\\n    (2) Check that \"a\" and all of its outstanding aliases are not used anywhere\\n        later in the graph. If this is the case, then it\\'s safe to re-inplace\\n        to \"b = foo_(a)\".\\n\\n        There are a few caveats to this, explained in more detail below:\\n        (a) If \"a\" is used later as an argument to a view op, that is okay.\\n            It\\'s only a problem if \"a\" (or that view) is later passed\\n            into a normal operator, or if it is returned as the program output.\\n        (b) If \"a\" is a repeat argument in `foo()`, then don\\'t reinplace.\\n            Most ATen kernels don\\'t make any guarantees that this is sound,\\n            e.g. if you do aten.mul_(a, a).\\n            So we\\'ll just ban re-inplacing in this case.\\n            It\\'s only a problem if \"a\" (or that view) is later passed\\n        (c) If \"a\" is used as an input into a view \"inverse\" / \"scatter\"\\n            operator, it is potentially fine to re-inplace\\n            (and remove that scatter operator from the graph).\\n            See below for a more detailed example.\\n\\n        NOTE: there is an optimization in this step that is crucial\\n        to fully recovering performance from functionalization.\\n\\n        Given this program:\\n        def f(x):\\n            a = torch.ops.aten.add(x, x)\\n            b = torch.ops.aten.diagonal(a)\\n            torch.ops.aten.fill_(b, 0)\\n            return d\\n\\n        Functionalization will emit the following:\\n        def f(x):\\n            a = torch.ops.aten.add(x, x)\\n            b = torch.ops.aten.diagonal(a, 0, 1)\\n            b_updated = torch.ops.aten.fill(b, 0)\\n            a_updated = torch.ops.aten.diagonal_scatter(a, b_updated, 0, 1)\\n            return a_updated\\n\\n        Ordinarily, we would not be able to reinplace the fill,\\n        because \"b\" aliases with \"a\" which is used by the diagonal_scatter call.\\n\\n        \"re-inplacing\" is on the hook for figuring out that it is ok to\\n        completely, the expensive diagonal_scatter call, if we re-inplace the add().\\n\\n        So, for every `alias in alias_set(a)`, instead of checking\\n        that \"alias\" is not used anywhere later in the graph,\\n        we check that\\n            EITHER:\\n          (a) alias is not used anywhere later in the graph\\n            OR:\\n          (b) alias is used exactly once later on in the graph,\\n              in the following op:\\n\\n                out = foo_scatter(alias, x, args...)\\n\\n              where the following must hold:\\n                (i) \"foo_scatter\" is the \"inverse\" operator for foo.\\n                    This only applies to \"foo\" ops that are view operators,\\n                    which view into a subset of the original tensor\\'s memory.\\n                    In practice, there are ~4 operators where this applies:\\n                      diagonal -> diagonal_scatter\\n                      slice -> slice_scatter\\n                      select -> select_scatter\\n                      as_strided -> as_strided_scatter\\n                (ii) \"args...\" are the same between the foo() and foo_scatter() calls.\\n\\n    (3) Perform the actual re-inplacing on foo!\\n\\n      (3b) is the common case, but special care is needed for {view}_scatter (3a)\\n\\n      (3a) {view}_scatter ops.\\n\\n        Consider this program:\\n          a = torch.zeros(2, 2)\\n          b = torch.ones(2)\\n          a[0] = b\\n\\n        Post functionalization, that will look like:\\n          a = torch.zeros(2)\\n          b = torch.ones(1)\\n          a_updated = torch.select_scatter(a, b, 0, 0)\\n\\n        In this case though, there is no \"functional\" op to re-inplace!\\n        Instead, we\\'d like to directly remove toe select_scatter call.\\n        We already know from (3) that this is valid,\\n        because \"a\" has no later usages in the graph.\\n\\n        We perform the re-inplacing on the {view}_scatter op like so\\n        Before:\\n          a_updated = torch.select_scatter(a, b, args...)\\n        After:\\n          a_slice = a.select(a, args...)\\n          a_slice.copy_(b)\\n\\n      (3b) Otherwise, replace the functional op with its inplace variant.\\n        Before:\\n          b = foo(a, args...)\\n        After:\\n          a.foo_(args...)\\n\\n    (4) Finally, after converting either:\\n          Before:\\n            b = foo(a)\\n          After:\\n            foo_(a)\\n        or\\n          Before:\\n            b = {slice}_scatter(a, mutated_slice, args...)\\n          After:\\n            slice = {slice}(a, args...)\\n            slice.copy_(mutated_slice)\\n\\n        We now need to find all later nodes that use \"b\" as an argument\\n        and update them to take in \"a\" instead.\\n\\n        Note that for the majority of inplace ops, this isn\\'t actually necessary\\n        (because most inplace ops return \"self\" as their output).\\n        This isn\\'t generally true for all mutable ops though, which is why\\n        we need to actually replace all of the arguments.\\n\\n        We also need to update our metadata of Dict[StorageWeakRef, Set[Node]],\\n        That maps a given tensor storage to the set of all nodes that take in that storage\\n        as an input.\\n        Specifically, re-inplacing `b = foo(a)` causes \"a\" and \"b\"\\'s sets to get fused\\n        together.\\n\\n    (5) Any \"view_inverse/scatter\" nodes that were identified as \"it\\'s ok to ignore them\"\\n        during step (3) get manually deleted from the graph.\\n        Their outputs are no longer used, so technically standard DCE would be able\\n        to do this, but we can no longer run FX\\'s DCE pass now that we have mutable\\n        ops in the graph.\\n    '\n    _FunctionalizationMetadataProp(gm).propagate(*sample_args)\n    input_storages = {StorageWeakRef(node.meta['fake_result']._typed_storage()) for node in gm.graph.nodes if node.op == 'placeholder'}\n    storage_to_nodes: Dict[StorageWeakRef, Set[Node]] = defaultdict(set)\n    for n in gm.graph.nodes:\n        if 'fake_result' in n.meta:\n\n            def _add_to_map(x):\n                if isinstance(x, FakeTensor):\n                    storage_to_nodes[StorageWeakRef(x._typed_storage())].add(n)\n            pytree.tree_map_(_add_to_map, n.meta['fake_result'])\n    all_later_view_inverse_nodes_to_delete = set()\n    for (idx, node) in enumerate(gm.graph.nodes):\n        if node.op == 'call_function':\n            if not isinstance(node.target, torch._ops.OpOverload):\n                continue\n            if len(node.target._schema.arguments) < 1:\n                continue\n            if type(node.target._schema.arguments[0].type) != torch.TensorType:\n                continue\n            self_arg = node.args[0]\n            self_flattened = pytree.tree_leaves(self_arg.meta['fake_result'])\n            node_flattened = pytree.tree_leaves(node.meta['fake_result'])\n            self_has_wrong_metadata = False\n            if len(self_flattened) == len(node_flattened):\n                for (self_meta, node_meta) in zip(self_flattened, node_flattened):\n                    if self_meta.numel() != node_meta.numel():\n                        self_has_wrong_metadata = True\n                    if self_meta.dtype != node_meta.dtype:\n                        self_has_wrong_metadata = True\n                    if torch._debug_has_internal_overlap(self_meta) == 1:\n                        self_has_wrong_metadata = True\n            if self_has_wrong_metadata and node.target != torch.ops.aten.resize.default:\n                continue\n            self_arg_name = self_arg.name\n            self_arg_storage = StorageWeakRef(self_arg.meta['fake_result']._typed_storage())\n            if self_arg_storage in input_storages:\n                continue\n            if len([x for x in node.args if x is self_arg]) > 1:\n                continue\n            self_arg_storage = StorageWeakRef(self_arg.meta['fake_result']._typed_storage())\n            self_aliases = storage_to_nodes[self_arg_storage]\n            later_node_usages = _get_all_later_node_usages(self_aliases, node.meta['node_idx'])\n            later_view_inverse_node_usages = _get_view_inverse_node_usages(later_node_usages, self_aliases)\n            can_reinplace = len(later_node_usages - later_view_inverse_node_usages) == 0\n            if not can_reinplace:\n                continue\n            if node.target in _VIEW_INVERSE_MAP and node not in all_later_view_inverse_nodes_to_delete:\n                view_op = _VIEW_INVERSE_MAP[node.target]\n                with gm.graph.inserting_before(node):\n                    mutated_slice_node = node.args[1]\n                    remaining_slice_args = node.args[2:]\n                    slice_node = gm.graph.create_node('call_function', view_op, (self_arg,) + tuple(remaining_slice_args), node.kwargs)\n                    copy_node = gm.graph.create_node('call_function', torch.ops.aten.copy_.default, (slice_node, mutated_slice_node), {})\n                all_later_view_inverse_nodes_to_delete.add(node)\n            else:\n                maybe_inplace_op = _maybe_get_inplace_op(node.target)\n                if maybe_inplace_op is None:\n                    continue\n                node.target = maybe_inplace_op\n            curr_node_storage = StorageWeakRef(node.meta['fake_result']._typed_storage())\n            storage_to_nodes[self_arg_storage].update(storage_to_nodes[curr_node_storage])\n            storage_to_nodes[curr_node_storage].update(storage_to_nodes[self_arg_storage])\n            all_later_view_inverse_nodes_to_delete.update(later_view_inverse_node_usages)\n            for old in itertools.chain([node], later_view_inverse_node_usages):\n                new = old.args[0]\n                nodes_to_update = [n for n in old.users if n.meta['node_idx'] > node.meta['node_idx']]\n                for node_to_update in nodes_to_update:\n                    new_args = []\n                    args = node_to_update.args\n\n                    def replace_arg(a):\n                        if a == old:\n                            return new\n                        return a\n                    node_to_update.args = tree_map_only(Node, replace_arg, node_to_update.args)\n                    node_to_update.kwargs = tree_map_only(Node, replace_arg, node_to_update.kwargs)\n                    old_flattened_res = pytree.tree_leaves(old.meta['fake_result'])\n                    node_flattened_res = pytree.tree_leaves(node_to_update.meta['fake_result'])\n                    old_res_storage = {StorageWeakRef(x._typed_storage()) for x in old_flattened_res if isinstance(x, FakeTensor)}\n                    node_res_storage = {StorageWeakRef(x._typed_storage()) for x in node_flattened_res if isinstance(x, FakeTensor)}\n                    if len(old_res_storage) == 1 and len(node_res_storage) == 1 and (old_res_storage == node_res_storage):\n                        new_flattened_res = pytree.tree_leaves(new.meta['fake_result'])\n                        new_res_storage = {StorageWeakRef(x._typed_storage()) for x in new_flattened_res if isinstance(x, FakeTensor)}\n                        assert len(new_res_storage) == 1\n                        (old_ref,) = old_res_storage\n                        (new_ref,) = new_res_storage\n                        (node_ref,) = node_res_storage\n                        storage_to_nodes[node_ref].update(storage_to_nodes[new_ref])\n                        storage_to_nodes[new_ref].update(storage_to_nodes[node_ref])\n    for to_delete in all_later_view_inverse_nodes_to_delete:\n        gm.graph.erase_node(to_delete)\n    gm.recompile()\n    return gm",
            "@compatibility(is_backward_compatible=True)\ndef reinplace(gm, *sample_args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Given an fx.GraphModule, modifies it to perform \"reinplacing\",\\n    mutating the nodes of the graph.\\n    We look for out-of-place op call sites like `b = a.add(...)`,\\n    and convert them to be inplace (`b = a.add_(...)`),\\n    as long as the input to the current operator (\"a\") isn\\'t re-used\\n    anywhere later in the graph.\\n\\n    This pass currently expects to operate on a **functional, ATen** graph.\\n    This can be obtained by running `make_fx(functionalize(f))`.\\n\\n    Sample inputs are needed to determine aliasing relationships of the inputs.\\n    In general, we can\\'t reinplace node `b = a.add(...)` if \"a\" aliases any of the\\n    inputs to the program.\\n\\n    Given a node \"b = foo(a, args...) the algorithm for re-inplacing is as follows:\\n\\n    (1) Perform some initial checks on the metadata of \"a\" and \"args...\"\\n        that can disqualify them from being reinplaced.\\n\\n      (1a) Check that the self argument we\\'re attempting to reinplace\\n           has acceptable dtype/size metadata to reinplace with.\\n\\n           For example, if we have:\\n             a = torch.ones(1)\\n             b = torch.ones(10)\\n             out = torch.add(a, b)\\n           We can\\'t turn that into\\n             a.add_(b)\\n           Because that would require resizing \"a\".\\n\\n           Similarly, we can\\'t convert torch.ge(a, b) into a.ge_(b),\\n           because that would require changing a\\'s dtype (from e.g. float32 to bool).\\n           Note that in this specific example, we could technically do better..\\n\\n           If we see the pattern:\\n             a_1 = a.ge(b)\\n             a_2 = aten._to_copy(a_1, a.dtype)\\n           Then we this should be valid to completely re-inplace\\n           (this is exactly what functionalization will emit when it sees a.ge_(b)).\\n\\n           This optimization is only really important for user programs\\n           that directly use inplace comparison ops though.\\n\\n           We also cannot re-inplace on tensors that have overlapping memory,\\n           e.g. torch.ones(1).expand(4, 4).add_(1)\\n\\n      (1b) Check if \"a\" is an alias of any of the program inputs.\\n\\n          If it is, skip and move to the next node.\\n          Inplace\\'ing an op that would cause it to mutate a program is not sound,\\n          because that would be a side effect visible to the user.\\n\\n          NOTE: there\\'s a future optimization that we should make:\\n          if \"a\" is a (alias of a)  program input, but later in the program\\n          there is a node that looks like \"a.copy_(...)\",\\n          Then re-inplacing is ok to do - we are temporarily re-using a\\'s buffer,\\n          which will later be overwritten by the copy_() call.\\n\\n          This will be an important optimization to have for programs that mutate\\n          their inputs. It currently isn\\'t implemented though.\\n\\n      (1c) Check if \"a\" and \"args...\" alias\\n\\n          For example, re-inplacing to create code like the below\\n          isn\\'t guaranteed to be sound:\\n\\n            aten.mul_(a, a)\\n\\n    (2) Check that \"a\" and all of its outstanding aliases are not used anywhere\\n        later in the graph. If this is the case, then it\\'s safe to re-inplace\\n        to \"b = foo_(a)\".\\n\\n        There are a few caveats to this, explained in more detail below:\\n        (a) If \"a\" is used later as an argument to a view op, that is okay.\\n            It\\'s only a problem if \"a\" (or that view) is later passed\\n            into a normal operator, or if it is returned as the program output.\\n        (b) If \"a\" is a repeat argument in `foo()`, then don\\'t reinplace.\\n            Most ATen kernels don\\'t make any guarantees that this is sound,\\n            e.g. if you do aten.mul_(a, a).\\n            So we\\'ll just ban re-inplacing in this case.\\n            It\\'s only a problem if \"a\" (or that view) is later passed\\n        (c) If \"a\" is used as an input into a view \"inverse\" / \"scatter\"\\n            operator, it is potentially fine to re-inplace\\n            (and remove that scatter operator from the graph).\\n            See below for a more detailed example.\\n\\n        NOTE: there is an optimization in this step that is crucial\\n        to fully recovering performance from functionalization.\\n\\n        Given this program:\\n        def f(x):\\n            a = torch.ops.aten.add(x, x)\\n            b = torch.ops.aten.diagonal(a)\\n            torch.ops.aten.fill_(b, 0)\\n            return d\\n\\n        Functionalization will emit the following:\\n        def f(x):\\n            a = torch.ops.aten.add(x, x)\\n            b = torch.ops.aten.diagonal(a, 0, 1)\\n            b_updated = torch.ops.aten.fill(b, 0)\\n            a_updated = torch.ops.aten.diagonal_scatter(a, b_updated, 0, 1)\\n            return a_updated\\n\\n        Ordinarily, we would not be able to reinplace the fill,\\n        because \"b\" aliases with \"a\" which is used by the diagonal_scatter call.\\n\\n        \"re-inplacing\" is on the hook for figuring out that it is ok to\\n        completely, the expensive diagonal_scatter call, if we re-inplace the add().\\n\\n        So, for every `alias in alias_set(a)`, instead of checking\\n        that \"alias\" is not used anywhere later in the graph,\\n        we check that\\n            EITHER:\\n          (a) alias is not used anywhere later in the graph\\n            OR:\\n          (b) alias is used exactly once later on in the graph,\\n              in the following op:\\n\\n                out = foo_scatter(alias, x, args...)\\n\\n              where the following must hold:\\n                (i) \"foo_scatter\" is the \"inverse\" operator for foo.\\n                    This only applies to \"foo\" ops that are view operators,\\n                    which view into a subset of the original tensor\\'s memory.\\n                    In practice, there are ~4 operators where this applies:\\n                      diagonal -> diagonal_scatter\\n                      slice -> slice_scatter\\n                      select -> select_scatter\\n                      as_strided -> as_strided_scatter\\n                (ii) \"args...\" are the same between the foo() and foo_scatter() calls.\\n\\n    (3) Perform the actual re-inplacing on foo!\\n\\n      (3b) is the common case, but special care is needed for {view}_scatter (3a)\\n\\n      (3a) {view}_scatter ops.\\n\\n        Consider this program:\\n          a = torch.zeros(2, 2)\\n          b = torch.ones(2)\\n          a[0] = b\\n\\n        Post functionalization, that will look like:\\n          a = torch.zeros(2)\\n          b = torch.ones(1)\\n          a_updated = torch.select_scatter(a, b, 0, 0)\\n\\n        In this case though, there is no \"functional\" op to re-inplace!\\n        Instead, we\\'d like to directly remove toe select_scatter call.\\n        We already know from (3) that this is valid,\\n        because \"a\" has no later usages in the graph.\\n\\n        We perform the re-inplacing on the {view}_scatter op like so\\n        Before:\\n          a_updated = torch.select_scatter(a, b, args...)\\n        After:\\n          a_slice = a.select(a, args...)\\n          a_slice.copy_(b)\\n\\n      (3b) Otherwise, replace the functional op with its inplace variant.\\n        Before:\\n          b = foo(a, args...)\\n        After:\\n          a.foo_(args...)\\n\\n    (4) Finally, after converting either:\\n          Before:\\n            b = foo(a)\\n          After:\\n            foo_(a)\\n        or\\n          Before:\\n            b = {slice}_scatter(a, mutated_slice, args...)\\n          After:\\n            slice = {slice}(a, args...)\\n            slice.copy_(mutated_slice)\\n\\n        We now need to find all later nodes that use \"b\" as an argument\\n        and update them to take in \"a\" instead.\\n\\n        Note that for the majority of inplace ops, this isn\\'t actually necessary\\n        (because most inplace ops return \"self\" as their output).\\n        This isn\\'t generally true for all mutable ops though, which is why\\n        we need to actually replace all of the arguments.\\n\\n        We also need to update our metadata of Dict[StorageWeakRef, Set[Node]],\\n        That maps a given tensor storage to the set of all nodes that take in that storage\\n        as an input.\\n        Specifically, re-inplacing `b = foo(a)` causes \"a\" and \"b\"\\'s sets to get fused\\n        together.\\n\\n    (5) Any \"view_inverse/scatter\" nodes that were identified as \"it\\'s ok to ignore them\"\\n        during step (3) get manually deleted from the graph.\\n        Their outputs are no longer used, so technically standard DCE would be able\\n        to do this, but we can no longer run FX\\'s DCE pass now that we have mutable\\n        ops in the graph.\\n    '\n    _FunctionalizationMetadataProp(gm).propagate(*sample_args)\n    input_storages = {StorageWeakRef(node.meta['fake_result']._typed_storage()) for node in gm.graph.nodes if node.op == 'placeholder'}\n    storage_to_nodes: Dict[StorageWeakRef, Set[Node]] = defaultdict(set)\n    for n in gm.graph.nodes:\n        if 'fake_result' in n.meta:\n\n            def _add_to_map(x):\n                if isinstance(x, FakeTensor):\n                    storage_to_nodes[StorageWeakRef(x._typed_storage())].add(n)\n            pytree.tree_map_(_add_to_map, n.meta['fake_result'])\n    all_later_view_inverse_nodes_to_delete = set()\n    for (idx, node) in enumerate(gm.graph.nodes):\n        if node.op == 'call_function':\n            if not isinstance(node.target, torch._ops.OpOverload):\n                continue\n            if len(node.target._schema.arguments) < 1:\n                continue\n            if type(node.target._schema.arguments[0].type) != torch.TensorType:\n                continue\n            self_arg = node.args[0]\n            self_flattened = pytree.tree_leaves(self_arg.meta['fake_result'])\n            node_flattened = pytree.tree_leaves(node.meta['fake_result'])\n            self_has_wrong_metadata = False\n            if len(self_flattened) == len(node_flattened):\n                for (self_meta, node_meta) in zip(self_flattened, node_flattened):\n                    if self_meta.numel() != node_meta.numel():\n                        self_has_wrong_metadata = True\n                    if self_meta.dtype != node_meta.dtype:\n                        self_has_wrong_metadata = True\n                    if torch._debug_has_internal_overlap(self_meta) == 1:\n                        self_has_wrong_metadata = True\n            if self_has_wrong_metadata and node.target != torch.ops.aten.resize.default:\n                continue\n            self_arg_name = self_arg.name\n            self_arg_storage = StorageWeakRef(self_arg.meta['fake_result']._typed_storage())\n            if self_arg_storage in input_storages:\n                continue\n            if len([x for x in node.args if x is self_arg]) > 1:\n                continue\n            self_arg_storage = StorageWeakRef(self_arg.meta['fake_result']._typed_storage())\n            self_aliases = storage_to_nodes[self_arg_storage]\n            later_node_usages = _get_all_later_node_usages(self_aliases, node.meta['node_idx'])\n            later_view_inverse_node_usages = _get_view_inverse_node_usages(later_node_usages, self_aliases)\n            can_reinplace = len(later_node_usages - later_view_inverse_node_usages) == 0\n            if not can_reinplace:\n                continue\n            if node.target in _VIEW_INVERSE_MAP and node not in all_later_view_inverse_nodes_to_delete:\n                view_op = _VIEW_INVERSE_MAP[node.target]\n                with gm.graph.inserting_before(node):\n                    mutated_slice_node = node.args[1]\n                    remaining_slice_args = node.args[2:]\n                    slice_node = gm.graph.create_node('call_function', view_op, (self_arg,) + tuple(remaining_slice_args), node.kwargs)\n                    copy_node = gm.graph.create_node('call_function', torch.ops.aten.copy_.default, (slice_node, mutated_slice_node), {})\n                all_later_view_inverse_nodes_to_delete.add(node)\n            else:\n                maybe_inplace_op = _maybe_get_inplace_op(node.target)\n                if maybe_inplace_op is None:\n                    continue\n                node.target = maybe_inplace_op\n            curr_node_storage = StorageWeakRef(node.meta['fake_result']._typed_storage())\n            storage_to_nodes[self_arg_storage].update(storage_to_nodes[curr_node_storage])\n            storage_to_nodes[curr_node_storage].update(storage_to_nodes[self_arg_storage])\n            all_later_view_inverse_nodes_to_delete.update(later_view_inverse_node_usages)\n            for old in itertools.chain([node], later_view_inverse_node_usages):\n                new = old.args[0]\n                nodes_to_update = [n for n in old.users if n.meta['node_idx'] > node.meta['node_idx']]\n                for node_to_update in nodes_to_update:\n                    new_args = []\n                    args = node_to_update.args\n\n                    def replace_arg(a):\n                        if a == old:\n                            return new\n                        return a\n                    node_to_update.args = tree_map_only(Node, replace_arg, node_to_update.args)\n                    node_to_update.kwargs = tree_map_only(Node, replace_arg, node_to_update.kwargs)\n                    old_flattened_res = pytree.tree_leaves(old.meta['fake_result'])\n                    node_flattened_res = pytree.tree_leaves(node_to_update.meta['fake_result'])\n                    old_res_storage = {StorageWeakRef(x._typed_storage()) for x in old_flattened_res if isinstance(x, FakeTensor)}\n                    node_res_storage = {StorageWeakRef(x._typed_storage()) for x in node_flattened_res if isinstance(x, FakeTensor)}\n                    if len(old_res_storage) == 1 and len(node_res_storage) == 1 and (old_res_storage == node_res_storage):\n                        new_flattened_res = pytree.tree_leaves(new.meta['fake_result'])\n                        new_res_storage = {StorageWeakRef(x._typed_storage()) for x in new_flattened_res if isinstance(x, FakeTensor)}\n                        assert len(new_res_storage) == 1\n                        (old_ref,) = old_res_storage\n                        (new_ref,) = new_res_storage\n                        (node_ref,) = node_res_storage\n                        storage_to_nodes[node_ref].update(storage_to_nodes[new_ref])\n                        storage_to_nodes[new_ref].update(storage_to_nodes[node_ref])\n    for to_delete in all_later_view_inverse_nodes_to_delete:\n        gm.graph.erase_node(to_delete)\n    gm.recompile()\n    return gm",
            "@compatibility(is_backward_compatible=True)\ndef reinplace(gm, *sample_args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Given an fx.GraphModule, modifies it to perform \"reinplacing\",\\n    mutating the nodes of the graph.\\n    We look for out-of-place op call sites like `b = a.add(...)`,\\n    and convert them to be inplace (`b = a.add_(...)`),\\n    as long as the input to the current operator (\"a\") isn\\'t re-used\\n    anywhere later in the graph.\\n\\n    This pass currently expects to operate on a **functional, ATen** graph.\\n    This can be obtained by running `make_fx(functionalize(f))`.\\n\\n    Sample inputs are needed to determine aliasing relationships of the inputs.\\n    In general, we can\\'t reinplace node `b = a.add(...)` if \"a\" aliases any of the\\n    inputs to the program.\\n\\n    Given a node \"b = foo(a, args...) the algorithm for re-inplacing is as follows:\\n\\n    (1) Perform some initial checks on the metadata of \"a\" and \"args...\"\\n        that can disqualify them from being reinplaced.\\n\\n      (1a) Check that the self argument we\\'re attempting to reinplace\\n           has acceptable dtype/size metadata to reinplace with.\\n\\n           For example, if we have:\\n             a = torch.ones(1)\\n             b = torch.ones(10)\\n             out = torch.add(a, b)\\n           We can\\'t turn that into\\n             a.add_(b)\\n           Because that would require resizing \"a\".\\n\\n           Similarly, we can\\'t convert torch.ge(a, b) into a.ge_(b),\\n           because that would require changing a\\'s dtype (from e.g. float32 to bool).\\n           Note that in this specific example, we could technically do better..\\n\\n           If we see the pattern:\\n             a_1 = a.ge(b)\\n             a_2 = aten._to_copy(a_1, a.dtype)\\n           Then we this should be valid to completely re-inplace\\n           (this is exactly what functionalization will emit when it sees a.ge_(b)).\\n\\n           This optimization is only really important for user programs\\n           that directly use inplace comparison ops though.\\n\\n           We also cannot re-inplace on tensors that have overlapping memory,\\n           e.g. torch.ones(1).expand(4, 4).add_(1)\\n\\n      (1b) Check if \"a\" is an alias of any of the program inputs.\\n\\n          If it is, skip and move to the next node.\\n          Inplace\\'ing an op that would cause it to mutate a program is not sound,\\n          because that would be a side effect visible to the user.\\n\\n          NOTE: there\\'s a future optimization that we should make:\\n          if \"a\" is a (alias of a)  program input, but later in the program\\n          there is a node that looks like \"a.copy_(...)\",\\n          Then re-inplacing is ok to do - we are temporarily re-using a\\'s buffer,\\n          which will later be overwritten by the copy_() call.\\n\\n          This will be an important optimization to have for programs that mutate\\n          their inputs. It currently isn\\'t implemented though.\\n\\n      (1c) Check if \"a\" and \"args...\" alias\\n\\n          For example, re-inplacing to create code like the below\\n          isn\\'t guaranteed to be sound:\\n\\n            aten.mul_(a, a)\\n\\n    (2) Check that \"a\" and all of its outstanding aliases are not used anywhere\\n        later in the graph. If this is the case, then it\\'s safe to re-inplace\\n        to \"b = foo_(a)\".\\n\\n        There are a few caveats to this, explained in more detail below:\\n        (a) If \"a\" is used later as an argument to a view op, that is okay.\\n            It\\'s only a problem if \"a\" (or that view) is later passed\\n            into a normal operator, or if it is returned as the program output.\\n        (b) If \"a\" is a repeat argument in `foo()`, then don\\'t reinplace.\\n            Most ATen kernels don\\'t make any guarantees that this is sound,\\n            e.g. if you do aten.mul_(a, a).\\n            So we\\'ll just ban re-inplacing in this case.\\n            It\\'s only a problem if \"a\" (or that view) is later passed\\n        (c) If \"a\" is used as an input into a view \"inverse\" / \"scatter\"\\n            operator, it is potentially fine to re-inplace\\n            (and remove that scatter operator from the graph).\\n            See below for a more detailed example.\\n\\n        NOTE: there is an optimization in this step that is crucial\\n        to fully recovering performance from functionalization.\\n\\n        Given this program:\\n        def f(x):\\n            a = torch.ops.aten.add(x, x)\\n            b = torch.ops.aten.diagonal(a)\\n            torch.ops.aten.fill_(b, 0)\\n            return d\\n\\n        Functionalization will emit the following:\\n        def f(x):\\n            a = torch.ops.aten.add(x, x)\\n            b = torch.ops.aten.diagonal(a, 0, 1)\\n            b_updated = torch.ops.aten.fill(b, 0)\\n            a_updated = torch.ops.aten.diagonal_scatter(a, b_updated, 0, 1)\\n            return a_updated\\n\\n        Ordinarily, we would not be able to reinplace the fill,\\n        because \"b\" aliases with \"a\" which is used by the diagonal_scatter call.\\n\\n        \"re-inplacing\" is on the hook for figuring out that it is ok to\\n        completely, the expensive diagonal_scatter call, if we re-inplace the add().\\n\\n        So, for every `alias in alias_set(a)`, instead of checking\\n        that \"alias\" is not used anywhere later in the graph,\\n        we check that\\n            EITHER:\\n          (a) alias is not used anywhere later in the graph\\n            OR:\\n          (b) alias is used exactly once later on in the graph,\\n              in the following op:\\n\\n                out = foo_scatter(alias, x, args...)\\n\\n              where the following must hold:\\n                (i) \"foo_scatter\" is the \"inverse\" operator for foo.\\n                    This only applies to \"foo\" ops that are view operators,\\n                    which view into a subset of the original tensor\\'s memory.\\n                    In practice, there are ~4 operators where this applies:\\n                      diagonal -> diagonal_scatter\\n                      slice -> slice_scatter\\n                      select -> select_scatter\\n                      as_strided -> as_strided_scatter\\n                (ii) \"args...\" are the same between the foo() and foo_scatter() calls.\\n\\n    (3) Perform the actual re-inplacing on foo!\\n\\n      (3b) is the common case, but special care is needed for {view}_scatter (3a)\\n\\n      (3a) {view}_scatter ops.\\n\\n        Consider this program:\\n          a = torch.zeros(2, 2)\\n          b = torch.ones(2)\\n          a[0] = b\\n\\n        Post functionalization, that will look like:\\n          a = torch.zeros(2)\\n          b = torch.ones(1)\\n          a_updated = torch.select_scatter(a, b, 0, 0)\\n\\n        In this case though, there is no \"functional\" op to re-inplace!\\n        Instead, we\\'d like to directly remove toe select_scatter call.\\n        We already know from (3) that this is valid,\\n        because \"a\" has no later usages in the graph.\\n\\n        We perform the re-inplacing on the {view}_scatter op like so\\n        Before:\\n          a_updated = torch.select_scatter(a, b, args...)\\n        After:\\n          a_slice = a.select(a, args...)\\n          a_slice.copy_(b)\\n\\n      (3b) Otherwise, replace the functional op with its inplace variant.\\n        Before:\\n          b = foo(a, args...)\\n        After:\\n          a.foo_(args...)\\n\\n    (4) Finally, after converting either:\\n          Before:\\n            b = foo(a)\\n          After:\\n            foo_(a)\\n        or\\n          Before:\\n            b = {slice}_scatter(a, mutated_slice, args...)\\n          After:\\n            slice = {slice}(a, args...)\\n            slice.copy_(mutated_slice)\\n\\n        We now need to find all later nodes that use \"b\" as an argument\\n        and update them to take in \"a\" instead.\\n\\n        Note that for the majority of inplace ops, this isn\\'t actually necessary\\n        (because most inplace ops return \"self\" as their output).\\n        This isn\\'t generally true for all mutable ops though, which is why\\n        we need to actually replace all of the arguments.\\n\\n        We also need to update our metadata of Dict[StorageWeakRef, Set[Node]],\\n        That maps a given tensor storage to the set of all nodes that take in that storage\\n        as an input.\\n        Specifically, re-inplacing `b = foo(a)` causes \"a\" and \"b\"\\'s sets to get fused\\n        together.\\n\\n    (5) Any \"view_inverse/scatter\" nodes that were identified as \"it\\'s ok to ignore them\"\\n        during step (3) get manually deleted from the graph.\\n        Their outputs are no longer used, so technically standard DCE would be able\\n        to do this, but we can no longer run FX\\'s DCE pass now that we have mutable\\n        ops in the graph.\\n    '\n    _FunctionalizationMetadataProp(gm).propagate(*sample_args)\n    input_storages = {StorageWeakRef(node.meta['fake_result']._typed_storage()) for node in gm.graph.nodes if node.op == 'placeholder'}\n    storage_to_nodes: Dict[StorageWeakRef, Set[Node]] = defaultdict(set)\n    for n in gm.graph.nodes:\n        if 'fake_result' in n.meta:\n\n            def _add_to_map(x):\n                if isinstance(x, FakeTensor):\n                    storage_to_nodes[StorageWeakRef(x._typed_storage())].add(n)\n            pytree.tree_map_(_add_to_map, n.meta['fake_result'])\n    all_later_view_inverse_nodes_to_delete = set()\n    for (idx, node) in enumerate(gm.graph.nodes):\n        if node.op == 'call_function':\n            if not isinstance(node.target, torch._ops.OpOverload):\n                continue\n            if len(node.target._schema.arguments) < 1:\n                continue\n            if type(node.target._schema.arguments[0].type) != torch.TensorType:\n                continue\n            self_arg = node.args[0]\n            self_flattened = pytree.tree_leaves(self_arg.meta['fake_result'])\n            node_flattened = pytree.tree_leaves(node.meta['fake_result'])\n            self_has_wrong_metadata = False\n            if len(self_flattened) == len(node_flattened):\n                for (self_meta, node_meta) in zip(self_flattened, node_flattened):\n                    if self_meta.numel() != node_meta.numel():\n                        self_has_wrong_metadata = True\n                    if self_meta.dtype != node_meta.dtype:\n                        self_has_wrong_metadata = True\n                    if torch._debug_has_internal_overlap(self_meta) == 1:\n                        self_has_wrong_metadata = True\n            if self_has_wrong_metadata and node.target != torch.ops.aten.resize.default:\n                continue\n            self_arg_name = self_arg.name\n            self_arg_storage = StorageWeakRef(self_arg.meta['fake_result']._typed_storage())\n            if self_arg_storage in input_storages:\n                continue\n            if len([x for x in node.args if x is self_arg]) > 1:\n                continue\n            self_arg_storage = StorageWeakRef(self_arg.meta['fake_result']._typed_storage())\n            self_aliases = storage_to_nodes[self_arg_storage]\n            later_node_usages = _get_all_later_node_usages(self_aliases, node.meta['node_idx'])\n            later_view_inverse_node_usages = _get_view_inverse_node_usages(later_node_usages, self_aliases)\n            can_reinplace = len(later_node_usages - later_view_inverse_node_usages) == 0\n            if not can_reinplace:\n                continue\n            if node.target in _VIEW_INVERSE_MAP and node not in all_later_view_inverse_nodes_to_delete:\n                view_op = _VIEW_INVERSE_MAP[node.target]\n                with gm.graph.inserting_before(node):\n                    mutated_slice_node = node.args[1]\n                    remaining_slice_args = node.args[2:]\n                    slice_node = gm.graph.create_node('call_function', view_op, (self_arg,) + tuple(remaining_slice_args), node.kwargs)\n                    copy_node = gm.graph.create_node('call_function', torch.ops.aten.copy_.default, (slice_node, mutated_slice_node), {})\n                all_later_view_inverse_nodes_to_delete.add(node)\n            else:\n                maybe_inplace_op = _maybe_get_inplace_op(node.target)\n                if maybe_inplace_op is None:\n                    continue\n                node.target = maybe_inplace_op\n            curr_node_storage = StorageWeakRef(node.meta['fake_result']._typed_storage())\n            storage_to_nodes[self_arg_storage].update(storage_to_nodes[curr_node_storage])\n            storage_to_nodes[curr_node_storage].update(storage_to_nodes[self_arg_storage])\n            all_later_view_inverse_nodes_to_delete.update(later_view_inverse_node_usages)\n            for old in itertools.chain([node], later_view_inverse_node_usages):\n                new = old.args[0]\n                nodes_to_update = [n for n in old.users if n.meta['node_idx'] > node.meta['node_idx']]\n                for node_to_update in nodes_to_update:\n                    new_args = []\n                    args = node_to_update.args\n\n                    def replace_arg(a):\n                        if a == old:\n                            return new\n                        return a\n                    node_to_update.args = tree_map_only(Node, replace_arg, node_to_update.args)\n                    node_to_update.kwargs = tree_map_only(Node, replace_arg, node_to_update.kwargs)\n                    old_flattened_res = pytree.tree_leaves(old.meta['fake_result'])\n                    node_flattened_res = pytree.tree_leaves(node_to_update.meta['fake_result'])\n                    old_res_storage = {StorageWeakRef(x._typed_storage()) for x in old_flattened_res if isinstance(x, FakeTensor)}\n                    node_res_storage = {StorageWeakRef(x._typed_storage()) for x in node_flattened_res if isinstance(x, FakeTensor)}\n                    if len(old_res_storage) == 1 and len(node_res_storage) == 1 and (old_res_storage == node_res_storage):\n                        new_flattened_res = pytree.tree_leaves(new.meta['fake_result'])\n                        new_res_storage = {StorageWeakRef(x._typed_storage()) for x in new_flattened_res if isinstance(x, FakeTensor)}\n                        assert len(new_res_storage) == 1\n                        (old_ref,) = old_res_storage\n                        (new_ref,) = new_res_storage\n                        (node_ref,) = node_res_storage\n                        storage_to_nodes[node_ref].update(storage_to_nodes[new_ref])\n                        storage_to_nodes[new_ref].update(storage_to_nodes[node_ref])\n    for to_delete in all_later_view_inverse_nodes_to_delete:\n        gm.graph.erase_node(to_delete)\n    gm.recompile()\n    return gm"
        ]
    }
]