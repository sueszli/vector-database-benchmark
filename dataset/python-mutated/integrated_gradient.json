[
    {
        "func_name": "saliency_interpret_from_json",
        "original": "def saliency_interpret_from_json(self, inputs: JsonDict) -> JsonDict:\n    labeled_instances = self.predictor.json_to_labeled_instances(inputs)\n    instances_with_grads = dict()\n    for (idx, instance) in enumerate(labeled_instances):\n        grads = self._integrate_gradients(instance)\n        for (key, grad) in grads.items():\n            embedding_grad = numpy.sum(grad[0], axis=1)\n            norm = numpy.linalg.norm(embedding_grad, ord=1)\n            normalized_grad = [math.fabs(e) / norm for e in embedding_grad]\n            grads[key] = normalized_grad\n        instances_with_grads['instance_' + str(idx + 1)] = grads\n    return sanitize(instances_with_grads)",
        "mutated": [
            "def saliency_interpret_from_json(self, inputs: JsonDict) -> JsonDict:\n    if False:\n        i = 10\n    labeled_instances = self.predictor.json_to_labeled_instances(inputs)\n    instances_with_grads = dict()\n    for (idx, instance) in enumerate(labeled_instances):\n        grads = self._integrate_gradients(instance)\n        for (key, grad) in grads.items():\n            embedding_grad = numpy.sum(grad[0], axis=1)\n            norm = numpy.linalg.norm(embedding_grad, ord=1)\n            normalized_grad = [math.fabs(e) / norm for e in embedding_grad]\n            grads[key] = normalized_grad\n        instances_with_grads['instance_' + str(idx + 1)] = grads\n    return sanitize(instances_with_grads)",
            "def saliency_interpret_from_json(self, inputs: JsonDict) -> JsonDict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    labeled_instances = self.predictor.json_to_labeled_instances(inputs)\n    instances_with_grads = dict()\n    for (idx, instance) in enumerate(labeled_instances):\n        grads = self._integrate_gradients(instance)\n        for (key, grad) in grads.items():\n            embedding_grad = numpy.sum(grad[0], axis=1)\n            norm = numpy.linalg.norm(embedding_grad, ord=1)\n            normalized_grad = [math.fabs(e) / norm for e in embedding_grad]\n            grads[key] = normalized_grad\n        instances_with_grads['instance_' + str(idx + 1)] = grads\n    return sanitize(instances_with_grads)",
            "def saliency_interpret_from_json(self, inputs: JsonDict) -> JsonDict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    labeled_instances = self.predictor.json_to_labeled_instances(inputs)\n    instances_with_grads = dict()\n    for (idx, instance) in enumerate(labeled_instances):\n        grads = self._integrate_gradients(instance)\n        for (key, grad) in grads.items():\n            embedding_grad = numpy.sum(grad[0], axis=1)\n            norm = numpy.linalg.norm(embedding_grad, ord=1)\n            normalized_grad = [math.fabs(e) / norm for e in embedding_grad]\n            grads[key] = normalized_grad\n        instances_with_grads['instance_' + str(idx + 1)] = grads\n    return sanitize(instances_with_grads)",
            "def saliency_interpret_from_json(self, inputs: JsonDict) -> JsonDict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    labeled_instances = self.predictor.json_to_labeled_instances(inputs)\n    instances_with_grads = dict()\n    for (idx, instance) in enumerate(labeled_instances):\n        grads = self._integrate_gradients(instance)\n        for (key, grad) in grads.items():\n            embedding_grad = numpy.sum(grad[0], axis=1)\n            norm = numpy.linalg.norm(embedding_grad, ord=1)\n            normalized_grad = [math.fabs(e) / norm for e in embedding_grad]\n            grads[key] = normalized_grad\n        instances_with_grads['instance_' + str(idx + 1)] = grads\n    return sanitize(instances_with_grads)",
            "def saliency_interpret_from_json(self, inputs: JsonDict) -> JsonDict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    labeled_instances = self.predictor.json_to_labeled_instances(inputs)\n    instances_with_grads = dict()\n    for (idx, instance) in enumerate(labeled_instances):\n        grads = self._integrate_gradients(instance)\n        for (key, grad) in grads.items():\n            embedding_grad = numpy.sum(grad[0], axis=1)\n            norm = numpy.linalg.norm(embedding_grad, ord=1)\n            normalized_grad = [math.fabs(e) / norm for e in embedding_grad]\n            grads[key] = normalized_grad\n        instances_with_grads['instance_' + str(idx + 1)] = grads\n    return sanitize(instances_with_grads)"
        ]
    },
    {
        "func_name": "forward_hook",
        "original": "def forward_hook(module, inputs, output):\n    if alpha == 0:\n        embeddings_list.append(output.squeeze(0).clone().detach())\n    output.mul_(alpha)",
        "mutated": [
            "def forward_hook(module, inputs, output):\n    if False:\n        i = 10\n    if alpha == 0:\n        embeddings_list.append(output.squeeze(0).clone().detach())\n    output.mul_(alpha)",
            "def forward_hook(module, inputs, output):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if alpha == 0:\n        embeddings_list.append(output.squeeze(0).clone().detach())\n    output.mul_(alpha)",
            "def forward_hook(module, inputs, output):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if alpha == 0:\n        embeddings_list.append(output.squeeze(0).clone().detach())\n    output.mul_(alpha)",
            "def forward_hook(module, inputs, output):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if alpha == 0:\n        embeddings_list.append(output.squeeze(0).clone().detach())\n    output.mul_(alpha)",
            "def forward_hook(module, inputs, output):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if alpha == 0:\n        embeddings_list.append(output.squeeze(0).clone().detach())\n    output.mul_(alpha)"
        ]
    },
    {
        "func_name": "get_token_offsets",
        "original": "def get_token_offsets(module, inputs, outputs):\n    offsets = util.get_token_offsets_from_text_field_inputs(inputs)\n    if offsets is not None:\n        token_offsets.append(offsets)",
        "mutated": [
            "def get_token_offsets(module, inputs, outputs):\n    if False:\n        i = 10\n    offsets = util.get_token_offsets_from_text_field_inputs(inputs)\n    if offsets is not None:\n        token_offsets.append(offsets)",
            "def get_token_offsets(module, inputs, outputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    offsets = util.get_token_offsets_from_text_field_inputs(inputs)\n    if offsets is not None:\n        token_offsets.append(offsets)",
            "def get_token_offsets(module, inputs, outputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    offsets = util.get_token_offsets_from_text_field_inputs(inputs)\n    if offsets is not None:\n        token_offsets.append(offsets)",
            "def get_token_offsets(module, inputs, outputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    offsets = util.get_token_offsets_from_text_field_inputs(inputs)\n    if offsets is not None:\n        token_offsets.append(offsets)",
            "def get_token_offsets(module, inputs, outputs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    offsets = util.get_token_offsets_from_text_field_inputs(inputs)\n    if offsets is not None:\n        token_offsets.append(offsets)"
        ]
    },
    {
        "func_name": "_register_hooks",
        "original": "def _register_hooks(self, alpha: int, embeddings_list: List, token_offsets: List):\n    \"\"\"\n        Register a forward hook on the embedding layer which scales the embeddings by alpha. Used\n        for one term in the Integrated Gradients sum.\n\n        We store the embedding output into the embeddings_list when alpha is zero.  This is used\n        later to element-wise multiply the input by the averaged gradients.\n        \"\"\"\n\n    def forward_hook(module, inputs, output):\n        if alpha == 0:\n            embeddings_list.append(output.squeeze(0).clone().detach())\n        output.mul_(alpha)\n\n    def get_token_offsets(module, inputs, outputs):\n        offsets = util.get_token_offsets_from_text_field_inputs(inputs)\n        if offsets is not None:\n            token_offsets.append(offsets)\n    handles = []\n    embedding_layer = self.predictor.get_interpretable_layer()\n    handles.append(embedding_layer.register_forward_hook(forward_hook))\n    text_field_embedder = self.predictor.get_interpretable_text_field_embedder()\n    handles.append(text_field_embedder.register_forward_hook(get_token_offsets))\n    return handles",
        "mutated": [
            "def _register_hooks(self, alpha: int, embeddings_list: List, token_offsets: List):\n    if False:\n        i = 10\n    '\\n        Register a forward hook on the embedding layer which scales the embeddings by alpha. Used\\n        for one term in the Integrated Gradients sum.\\n\\n        We store the embedding output into the embeddings_list when alpha is zero.  This is used\\n        later to element-wise multiply the input by the averaged gradients.\\n        '\n\n    def forward_hook(module, inputs, output):\n        if alpha == 0:\n            embeddings_list.append(output.squeeze(0).clone().detach())\n        output.mul_(alpha)\n\n    def get_token_offsets(module, inputs, outputs):\n        offsets = util.get_token_offsets_from_text_field_inputs(inputs)\n        if offsets is not None:\n            token_offsets.append(offsets)\n    handles = []\n    embedding_layer = self.predictor.get_interpretable_layer()\n    handles.append(embedding_layer.register_forward_hook(forward_hook))\n    text_field_embedder = self.predictor.get_interpretable_text_field_embedder()\n    handles.append(text_field_embedder.register_forward_hook(get_token_offsets))\n    return handles",
            "def _register_hooks(self, alpha: int, embeddings_list: List, token_offsets: List):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Register a forward hook on the embedding layer which scales the embeddings by alpha. Used\\n        for one term in the Integrated Gradients sum.\\n\\n        We store the embedding output into the embeddings_list when alpha is zero.  This is used\\n        later to element-wise multiply the input by the averaged gradients.\\n        '\n\n    def forward_hook(module, inputs, output):\n        if alpha == 0:\n            embeddings_list.append(output.squeeze(0).clone().detach())\n        output.mul_(alpha)\n\n    def get_token_offsets(module, inputs, outputs):\n        offsets = util.get_token_offsets_from_text_field_inputs(inputs)\n        if offsets is not None:\n            token_offsets.append(offsets)\n    handles = []\n    embedding_layer = self.predictor.get_interpretable_layer()\n    handles.append(embedding_layer.register_forward_hook(forward_hook))\n    text_field_embedder = self.predictor.get_interpretable_text_field_embedder()\n    handles.append(text_field_embedder.register_forward_hook(get_token_offsets))\n    return handles",
            "def _register_hooks(self, alpha: int, embeddings_list: List, token_offsets: List):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Register a forward hook on the embedding layer which scales the embeddings by alpha. Used\\n        for one term in the Integrated Gradients sum.\\n\\n        We store the embedding output into the embeddings_list when alpha is zero.  This is used\\n        later to element-wise multiply the input by the averaged gradients.\\n        '\n\n    def forward_hook(module, inputs, output):\n        if alpha == 0:\n            embeddings_list.append(output.squeeze(0).clone().detach())\n        output.mul_(alpha)\n\n    def get_token_offsets(module, inputs, outputs):\n        offsets = util.get_token_offsets_from_text_field_inputs(inputs)\n        if offsets is not None:\n            token_offsets.append(offsets)\n    handles = []\n    embedding_layer = self.predictor.get_interpretable_layer()\n    handles.append(embedding_layer.register_forward_hook(forward_hook))\n    text_field_embedder = self.predictor.get_interpretable_text_field_embedder()\n    handles.append(text_field_embedder.register_forward_hook(get_token_offsets))\n    return handles",
            "def _register_hooks(self, alpha: int, embeddings_list: List, token_offsets: List):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Register a forward hook on the embedding layer which scales the embeddings by alpha. Used\\n        for one term in the Integrated Gradients sum.\\n\\n        We store the embedding output into the embeddings_list when alpha is zero.  This is used\\n        later to element-wise multiply the input by the averaged gradients.\\n        '\n\n    def forward_hook(module, inputs, output):\n        if alpha == 0:\n            embeddings_list.append(output.squeeze(0).clone().detach())\n        output.mul_(alpha)\n\n    def get_token_offsets(module, inputs, outputs):\n        offsets = util.get_token_offsets_from_text_field_inputs(inputs)\n        if offsets is not None:\n            token_offsets.append(offsets)\n    handles = []\n    embedding_layer = self.predictor.get_interpretable_layer()\n    handles.append(embedding_layer.register_forward_hook(forward_hook))\n    text_field_embedder = self.predictor.get_interpretable_text_field_embedder()\n    handles.append(text_field_embedder.register_forward_hook(get_token_offsets))\n    return handles",
            "def _register_hooks(self, alpha: int, embeddings_list: List, token_offsets: List):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Register a forward hook on the embedding layer which scales the embeddings by alpha. Used\\n        for one term in the Integrated Gradients sum.\\n\\n        We store the embedding output into the embeddings_list when alpha is zero.  This is used\\n        later to element-wise multiply the input by the averaged gradients.\\n        '\n\n    def forward_hook(module, inputs, output):\n        if alpha == 0:\n            embeddings_list.append(output.squeeze(0).clone().detach())\n        output.mul_(alpha)\n\n    def get_token_offsets(module, inputs, outputs):\n        offsets = util.get_token_offsets_from_text_field_inputs(inputs)\n        if offsets is not None:\n            token_offsets.append(offsets)\n    handles = []\n    embedding_layer = self.predictor.get_interpretable_layer()\n    handles.append(embedding_layer.register_forward_hook(forward_hook))\n    text_field_embedder = self.predictor.get_interpretable_text_field_embedder()\n    handles.append(text_field_embedder.register_forward_hook(get_token_offsets))\n    return handles"
        ]
    },
    {
        "func_name": "_integrate_gradients",
        "original": "def _integrate_gradients(self, instance: Instance) -> Dict[str, numpy.ndarray]:\n    \"\"\"\n        Returns integrated gradients for the given [`Instance`](../../data/instance.md)\n        \"\"\"\n    ig_grads: Dict[str, Any] = {}\n    embeddings_list: List[torch.Tensor] = []\n    token_offsets: List[torch.Tensor] = []\n    steps = 10\n    for alpha in numpy.linspace(0, 1.0, num=steps, endpoint=False):\n        handles = []\n        handles = self._register_hooks(alpha, embeddings_list, token_offsets)\n        try:\n            grads = self.predictor.get_gradients([instance])[0]\n        finally:\n            for handle in handles:\n                handle.remove()\n        if ig_grads == {}:\n            ig_grads = grads\n        else:\n            for key in grads.keys():\n                ig_grads[key] += grads[key]\n    for key in ig_grads.keys():\n        ig_grads[key] /= steps\n    embeddings_list.reverse()\n    token_offsets.reverse()\n    embeddings_list = self._aggregate_token_embeddings(embeddings_list, token_offsets)\n    for (idx, input_embedding) in enumerate(embeddings_list):\n        key = 'grad_input_' + str(idx + 1)\n        ig_grads[key] *= input_embedding\n    return ig_grads",
        "mutated": [
            "def _integrate_gradients(self, instance: Instance) -> Dict[str, numpy.ndarray]:\n    if False:\n        i = 10\n    '\\n        Returns integrated gradients for the given [`Instance`](../../data/instance.md)\\n        '\n    ig_grads: Dict[str, Any] = {}\n    embeddings_list: List[torch.Tensor] = []\n    token_offsets: List[torch.Tensor] = []\n    steps = 10\n    for alpha in numpy.linspace(0, 1.0, num=steps, endpoint=False):\n        handles = []\n        handles = self._register_hooks(alpha, embeddings_list, token_offsets)\n        try:\n            grads = self.predictor.get_gradients([instance])[0]\n        finally:\n            for handle in handles:\n                handle.remove()\n        if ig_grads == {}:\n            ig_grads = grads\n        else:\n            for key in grads.keys():\n                ig_grads[key] += grads[key]\n    for key in ig_grads.keys():\n        ig_grads[key] /= steps\n    embeddings_list.reverse()\n    token_offsets.reverse()\n    embeddings_list = self._aggregate_token_embeddings(embeddings_list, token_offsets)\n    for (idx, input_embedding) in enumerate(embeddings_list):\n        key = 'grad_input_' + str(idx + 1)\n        ig_grads[key] *= input_embedding\n    return ig_grads",
            "def _integrate_gradients(self, instance: Instance) -> Dict[str, numpy.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns integrated gradients for the given [`Instance`](../../data/instance.md)\\n        '\n    ig_grads: Dict[str, Any] = {}\n    embeddings_list: List[torch.Tensor] = []\n    token_offsets: List[torch.Tensor] = []\n    steps = 10\n    for alpha in numpy.linspace(0, 1.0, num=steps, endpoint=False):\n        handles = []\n        handles = self._register_hooks(alpha, embeddings_list, token_offsets)\n        try:\n            grads = self.predictor.get_gradients([instance])[0]\n        finally:\n            for handle in handles:\n                handle.remove()\n        if ig_grads == {}:\n            ig_grads = grads\n        else:\n            for key in grads.keys():\n                ig_grads[key] += grads[key]\n    for key in ig_grads.keys():\n        ig_grads[key] /= steps\n    embeddings_list.reverse()\n    token_offsets.reverse()\n    embeddings_list = self._aggregate_token_embeddings(embeddings_list, token_offsets)\n    for (idx, input_embedding) in enumerate(embeddings_list):\n        key = 'grad_input_' + str(idx + 1)\n        ig_grads[key] *= input_embedding\n    return ig_grads",
            "def _integrate_gradients(self, instance: Instance) -> Dict[str, numpy.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns integrated gradients for the given [`Instance`](../../data/instance.md)\\n        '\n    ig_grads: Dict[str, Any] = {}\n    embeddings_list: List[torch.Tensor] = []\n    token_offsets: List[torch.Tensor] = []\n    steps = 10\n    for alpha in numpy.linspace(0, 1.0, num=steps, endpoint=False):\n        handles = []\n        handles = self._register_hooks(alpha, embeddings_list, token_offsets)\n        try:\n            grads = self.predictor.get_gradients([instance])[0]\n        finally:\n            for handle in handles:\n                handle.remove()\n        if ig_grads == {}:\n            ig_grads = grads\n        else:\n            for key in grads.keys():\n                ig_grads[key] += grads[key]\n    for key in ig_grads.keys():\n        ig_grads[key] /= steps\n    embeddings_list.reverse()\n    token_offsets.reverse()\n    embeddings_list = self._aggregate_token_embeddings(embeddings_list, token_offsets)\n    for (idx, input_embedding) in enumerate(embeddings_list):\n        key = 'grad_input_' + str(idx + 1)\n        ig_grads[key] *= input_embedding\n    return ig_grads",
            "def _integrate_gradients(self, instance: Instance) -> Dict[str, numpy.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns integrated gradients for the given [`Instance`](../../data/instance.md)\\n        '\n    ig_grads: Dict[str, Any] = {}\n    embeddings_list: List[torch.Tensor] = []\n    token_offsets: List[torch.Tensor] = []\n    steps = 10\n    for alpha in numpy.linspace(0, 1.0, num=steps, endpoint=False):\n        handles = []\n        handles = self._register_hooks(alpha, embeddings_list, token_offsets)\n        try:\n            grads = self.predictor.get_gradients([instance])[0]\n        finally:\n            for handle in handles:\n                handle.remove()\n        if ig_grads == {}:\n            ig_grads = grads\n        else:\n            for key in grads.keys():\n                ig_grads[key] += grads[key]\n    for key in ig_grads.keys():\n        ig_grads[key] /= steps\n    embeddings_list.reverse()\n    token_offsets.reverse()\n    embeddings_list = self._aggregate_token_embeddings(embeddings_list, token_offsets)\n    for (idx, input_embedding) in enumerate(embeddings_list):\n        key = 'grad_input_' + str(idx + 1)\n        ig_grads[key] *= input_embedding\n    return ig_grads",
            "def _integrate_gradients(self, instance: Instance) -> Dict[str, numpy.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns integrated gradients for the given [`Instance`](../../data/instance.md)\\n        '\n    ig_grads: Dict[str, Any] = {}\n    embeddings_list: List[torch.Tensor] = []\n    token_offsets: List[torch.Tensor] = []\n    steps = 10\n    for alpha in numpy.linspace(0, 1.0, num=steps, endpoint=False):\n        handles = []\n        handles = self._register_hooks(alpha, embeddings_list, token_offsets)\n        try:\n            grads = self.predictor.get_gradients([instance])[0]\n        finally:\n            for handle in handles:\n                handle.remove()\n        if ig_grads == {}:\n            ig_grads = grads\n        else:\n            for key in grads.keys():\n                ig_grads[key] += grads[key]\n    for key in ig_grads.keys():\n        ig_grads[key] /= steps\n    embeddings_list.reverse()\n    token_offsets.reverse()\n    embeddings_list = self._aggregate_token_embeddings(embeddings_list, token_offsets)\n    for (idx, input_embedding) in enumerate(embeddings_list):\n        key = 'grad_input_' + str(idx + 1)\n        ig_grads[key] *= input_embedding\n    return ig_grads"
        ]
    }
]