[
    {
        "func_name": "__init__",
        "original": "def __init__(self, data_root, ann_file, split, pts_prefix='velodyne', pipeline=None, classes=None, modality=None, box_type_3d='LiDAR', filter_empty_gt=True, test_mode=False, load_interval=1, pcd_limit_range=[-85, -85, -5, 85, 85, 5], **kwargs):\n    super().__init__(data_root=data_root, ann_file=ann_file, split=split, pts_prefix=pts_prefix, pipeline=pipeline, classes=classes, modality=modality, box_type_3d=box_type_3d, filter_empty_gt=filter_empty_gt, test_mode=test_mode, pcd_limit_range=pcd_limit_range, **kwargs)\n    self.data_infos = self.data_infos[::load_interval]\n    if hasattr(self, 'flag'):\n        self.flag = self.flag[::load_interval]",
        "mutated": [
            "def __init__(self, data_root, ann_file, split, pts_prefix='velodyne', pipeline=None, classes=None, modality=None, box_type_3d='LiDAR', filter_empty_gt=True, test_mode=False, load_interval=1, pcd_limit_range=[-85, -85, -5, 85, 85, 5], **kwargs):\n    if False:\n        i = 10\n    super().__init__(data_root=data_root, ann_file=ann_file, split=split, pts_prefix=pts_prefix, pipeline=pipeline, classes=classes, modality=modality, box_type_3d=box_type_3d, filter_empty_gt=filter_empty_gt, test_mode=test_mode, pcd_limit_range=pcd_limit_range, **kwargs)\n    self.data_infos = self.data_infos[::load_interval]\n    if hasattr(self, 'flag'):\n        self.flag = self.flag[::load_interval]",
            "def __init__(self, data_root, ann_file, split, pts_prefix='velodyne', pipeline=None, classes=None, modality=None, box_type_3d='LiDAR', filter_empty_gt=True, test_mode=False, load_interval=1, pcd_limit_range=[-85, -85, -5, 85, 85, 5], **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(data_root=data_root, ann_file=ann_file, split=split, pts_prefix=pts_prefix, pipeline=pipeline, classes=classes, modality=modality, box_type_3d=box_type_3d, filter_empty_gt=filter_empty_gt, test_mode=test_mode, pcd_limit_range=pcd_limit_range, **kwargs)\n    self.data_infos = self.data_infos[::load_interval]\n    if hasattr(self, 'flag'):\n        self.flag = self.flag[::load_interval]",
            "def __init__(self, data_root, ann_file, split, pts_prefix='velodyne', pipeline=None, classes=None, modality=None, box_type_3d='LiDAR', filter_empty_gt=True, test_mode=False, load_interval=1, pcd_limit_range=[-85, -85, -5, 85, 85, 5], **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(data_root=data_root, ann_file=ann_file, split=split, pts_prefix=pts_prefix, pipeline=pipeline, classes=classes, modality=modality, box_type_3d=box_type_3d, filter_empty_gt=filter_empty_gt, test_mode=test_mode, pcd_limit_range=pcd_limit_range, **kwargs)\n    self.data_infos = self.data_infos[::load_interval]\n    if hasattr(self, 'flag'):\n        self.flag = self.flag[::load_interval]",
            "def __init__(self, data_root, ann_file, split, pts_prefix='velodyne', pipeline=None, classes=None, modality=None, box_type_3d='LiDAR', filter_empty_gt=True, test_mode=False, load_interval=1, pcd_limit_range=[-85, -85, -5, 85, 85, 5], **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(data_root=data_root, ann_file=ann_file, split=split, pts_prefix=pts_prefix, pipeline=pipeline, classes=classes, modality=modality, box_type_3d=box_type_3d, filter_empty_gt=filter_empty_gt, test_mode=test_mode, pcd_limit_range=pcd_limit_range, **kwargs)\n    self.data_infos = self.data_infos[::load_interval]\n    if hasattr(self, 'flag'):\n        self.flag = self.flag[::load_interval]",
            "def __init__(self, data_root, ann_file, split, pts_prefix='velodyne', pipeline=None, classes=None, modality=None, box_type_3d='LiDAR', filter_empty_gt=True, test_mode=False, load_interval=1, pcd_limit_range=[-85, -85, -5, 85, 85, 5], **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(data_root=data_root, ann_file=ann_file, split=split, pts_prefix=pts_prefix, pipeline=pipeline, classes=classes, modality=modality, box_type_3d=box_type_3d, filter_empty_gt=filter_empty_gt, test_mode=test_mode, pcd_limit_range=pcd_limit_range, **kwargs)\n    self.data_infos = self.data_infos[::load_interval]\n    if hasattr(self, 'flag'):\n        self.flag = self.flag[::load_interval]"
        ]
    },
    {
        "func_name": "_get_pts_filename",
        "original": "def _get_pts_filename(self, idx):\n    pts_filename = osp.join(self.root_split, self.pts_prefix, f'{idx:07d}.bin')\n    return pts_filename",
        "mutated": [
            "def _get_pts_filename(self, idx):\n    if False:\n        i = 10\n    pts_filename = osp.join(self.root_split, self.pts_prefix, f'{idx:07d}.bin')\n    return pts_filename",
            "def _get_pts_filename(self, idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pts_filename = osp.join(self.root_split, self.pts_prefix, f'{idx:07d}.bin')\n    return pts_filename",
            "def _get_pts_filename(self, idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pts_filename = osp.join(self.root_split, self.pts_prefix, f'{idx:07d}.bin')\n    return pts_filename",
            "def _get_pts_filename(self, idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pts_filename = osp.join(self.root_split, self.pts_prefix, f'{idx:07d}.bin')\n    return pts_filename",
            "def _get_pts_filename(self, idx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pts_filename = osp.join(self.root_split, self.pts_prefix, f'{idx:07d}.bin')\n    return pts_filename"
        ]
    },
    {
        "func_name": "get_data_info",
        "original": "def get_data_info(self, index):\n    \"\"\"Get data info according to the given index.\n\n        Args:\n            index (int): Index of the sample data to get.\n\n        Returns:\n            dict: Standard input_dict consists of the\n                data information.\n\n                - sample_idx (str): sample index\n                - pts_filename (str): filename of point clouds\n                - img_prefix (str): prefix of image files\n                - img_info (dict): image info\n                - lidar2img (list[np.ndarray], optional): transformations from\n                    lidar to different cameras\n                - ann_info (dict): annotation info\n        \"\"\"\n    info = self.data_infos[index]\n    sample_idx = info['image']['image_idx']\n    img_filename = os.path.join(self.data_root, info['image']['image_path'])\n    rect = info['calib']['R0_rect'].astype(np.float32)\n    Trv2c = info['calib']['Tr_velo_to_cam'].astype(np.float32)\n    P0 = info['calib']['P0'].astype(np.float32)\n    lidar2img = P0 @ rect @ Trv2c\n    pts_filename = self._get_pts_filename(sample_idx)\n    input_dict = dict(sample_idx=sample_idx, pts_filename=pts_filename, img_prefix=None, img_info=dict(filename=img_filename), lidar2img=lidar2img)\n    if not self.test_mode:\n        annos = self.get_ann_info(index)\n        input_dict['ann_info'] = annos\n    return input_dict",
        "mutated": [
            "def get_data_info(self, index):\n    if False:\n        i = 10\n    'Get data info according to the given index.\\n\\n        Args:\\n            index (int): Index of the sample data to get.\\n\\n        Returns:\\n            dict: Standard input_dict consists of the\\n                data information.\\n\\n                - sample_idx (str): sample index\\n                - pts_filename (str): filename of point clouds\\n                - img_prefix (str): prefix of image files\\n                - img_info (dict): image info\\n                - lidar2img (list[np.ndarray], optional): transformations from\\n                    lidar to different cameras\\n                - ann_info (dict): annotation info\\n        '\n    info = self.data_infos[index]\n    sample_idx = info['image']['image_idx']\n    img_filename = os.path.join(self.data_root, info['image']['image_path'])\n    rect = info['calib']['R0_rect'].astype(np.float32)\n    Trv2c = info['calib']['Tr_velo_to_cam'].astype(np.float32)\n    P0 = info['calib']['P0'].astype(np.float32)\n    lidar2img = P0 @ rect @ Trv2c\n    pts_filename = self._get_pts_filename(sample_idx)\n    input_dict = dict(sample_idx=sample_idx, pts_filename=pts_filename, img_prefix=None, img_info=dict(filename=img_filename), lidar2img=lidar2img)\n    if not self.test_mode:\n        annos = self.get_ann_info(index)\n        input_dict['ann_info'] = annos\n    return input_dict",
            "def get_data_info(self, index):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get data info according to the given index.\\n\\n        Args:\\n            index (int): Index of the sample data to get.\\n\\n        Returns:\\n            dict: Standard input_dict consists of the\\n                data information.\\n\\n                - sample_idx (str): sample index\\n                - pts_filename (str): filename of point clouds\\n                - img_prefix (str): prefix of image files\\n                - img_info (dict): image info\\n                - lidar2img (list[np.ndarray], optional): transformations from\\n                    lidar to different cameras\\n                - ann_info (dict): annotation info\\n        '\n    info = self.data_infos[index]\n    sample_idx = info['image']['image_idx']\n    img_filename = os.path.join(self.data_root, info['image']['image_path'])\n    rect = info['calib']['R0_rect'].astype(np.float32)\n    Trv2c = info['calib']['Tr_velo_to_cam'].astype(np.float32)\n    P0 = info['calib']['P0'].astype(np.float32)\n    lidar2img = P0 @ rect @ Trv2c\n    pts_filename = self._get_pts_filename(sample_idx)\n    input_dict = dict(sample_idx=sample_idx, pts_filename=pts_filename, img_prefix=None, img_info=dict(filename=img_filename), lidar2img=lidar2img)\n    if not self.test_mode:\n        annos = self.get_ann_info(index)\n        input_dict['ann_info'] = annos\n    return input_dict",
            "def get_data_info(self, index):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get data info according to the given index.\\n\\n        Args:\\n            index (int): Index of the sample data to get.\\n\\n        Returns:\\n            dict: Standard input_dict consists of the\\n                data information.\\n\\n                - sample_idx (str): sample index\\n                - pts_filename (str): filename of point clouds\\n                - img_prefix (str): prefix of image files\\n                - img_info (dict): image info\\n                - lidar2img (list[np.ndarray], optional): transformations from\\n                    lidar to different cameras\\n                - ann_info (dict): annotation info\\n        '\n    info = self.data_infos[index]\n    sample_idx = info['image']['image_idx']\n    img_filename = os.path.join(self.data_root, info['image']['image_path'])\n    rect = info['calib']['R0_rect'].astype(np.float32)\n    Trv2c = info['calib']['Tr_velo_to_cam'].astype(np.float32)\n    P0 = info['calib']['P0'].astype(np.float32)\n    lidar2img = P0 @ rect @ Trv2c\n    pts_filename = self._get_pts_filename(sample_idx)\n    input_dict = dict(sample_idx=sample_idx, pts_filename=pts_filename, img_prefix=None, img_info=dict(filename=img_filename), lidar2img=lidar2img)\n    if not self.test_mode:\n        annos = self.get_ann_info(index)\n        input_dict['ann_info'] = annos\n    return input_dict",
            "def get_data_info(self, index):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get data info according to the given index.\\n\\n        Args:\\n            index (int): Index of the sample data to get.\\n\\n        Returns:\\n            dict: Standard input_dict consists of the\\n                data information.\\n\\n                - sample_idx (str): sample index\\n                - pts_filename (str): filename of point clouds\\n                - img_prefix (str): prefix of image files\\n                - img_info (dict): image info\\n                - lidar2img (list[np.ndarray], optional): transformations from\\n                    lidar to different cameras\\n                - ann_info (dict): annotation info\\n        '\n    info = self.data_infos[index]\n    sample_idx = info['image']['image_idx']\n    img_filename = os.path.join(self.data_root, info['image']['image_path'])\n    rect = info['calib']['R0_rect'].astype(np.float32)\n    Trv2c = info['calib']['Tr_velo_to_cam'].astype(np.float32)\n    P0 = info['calib']['P0'].astype(np.float32)\n    lidar2img = P0 @ rect @ Trv2c\n    pts_filename = self._get_pts_filename(sample_idx)\n    input_dict = dict(sample_idx=sample_idx, pts_filename=pts_filename, img_prefix=None, img_info=dict(filename=img_filename), lidar2img=lidar2img)\n    if not self.test_mode:\n        annos = self.get_ann_info(index)\n        input_dict['ann_info'] = annos\n    return input_dict",
            "def get_data_info(self, index):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get data info according to the given index.\\n\\n        Args:\\n            index (int): Index of the sample data to get.\\n\\n        Returns:\\n            dict: Standard input_dict consists of the\\n                data information.\\n\\n                - sample_idx (str): sample index\\n                - pts_filename (str): filename of point clouds\\n                - img_prefix (str): prefix of image files\\n                - img_info (dict): image info\\n                - lidar2img (list[np.ndarray], optional): transformations from\\n                    lidar to different cameras\\n                - ann_info (dict): annotation info\\n        '\n    info = self.data_infos[index]\n    sample_idx = info['image']['image_idx']\n    img_filename = os.path.join(self.data_root, info['image']['image_path'])\n    rect = info['calib']['R0_rect'].astype(np.float32)\n    Trv2c = info['calib']['Tr_velo_to_cam'].astype(np.float32)\n    P0 = info['calib']['P0'].astype(np.float32)\n    lidar2img = P0 @ rect @ Trv2c\n    pts_filename = self._get_pts_filename(sample_idx)\n    input_dict = dict(sample_idx=sample_idx, pts_filename=pts_filename, img_prefix=None, img_info=dict(filename=img_filename), lidar2img=lidar2img)\n    if not self.test_mode:\n        annos = self.get_ann_info(index)\n        input_dict['ann_info'] = annos\n    return input_dict"
        ]
    },
    {
        "func_name": "format_results",
        "original": "def format_results(self, outputs, pklfile_prefix=None, submission_prefix=None, data_format='waymo'):\n    \"\"\"Format the results to pkl file.\n\n        Args:\n            outputs (list[dict]): Testing results of the dataset.\n            pklfile_prefix (str): The prefix of pkl files. It includes\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\n                If not specified, a temp file will be created. Default: None.\n            submission_prefix (str): The prefix of submitted files. It\n                includes the file path and the prefix of filename, e.g.,\n                \"a/b/prefix\". If not specified, a temp file will be created.\n                Default: None.\n            data_format (str, optional): Output data format.\n                Default: 'waymo'. Another supported choice is 'kitti'.\n\n        Returns:\n            tuple: (result_files, tmp_dir), result_files is a dict containing\n                the json filepaths, tmp_dir is the temporal directory created\n                for saving json files when jsonfile_prefix is not specified.\n        \"\"\"\n    if pklfile_prefix is None:\n        tmp_dir = tempfile.TemporaryDirectory()\n        pklfile_prefix = osp.join(tmp_dir.name, 'results')\n    else:\n        tmp_dir = None\n    assert 'waymo' in data_format or 'kitti' in data_format, f'invalid data_format {data_format}'\n    if not isinstance(outputs[0], dict) or 'img_bbox' in outputs[0]:\n        raise TypeError('Not supported type for reformat results.')\n    elif 'pts_bbox' in outputs[0]:\n        result_files = dict()\n        for name in outputs[0]:\n            results_ = [out[name] for out in outputs]\n            pklfile_prefix_ = pklfile_prefix + name\n            if submission_prefix is not None:\n                submission_prefix_ = f'{submission_prefix}_{name}'\n            else:\n                submission_prefix_ = None\n            result_files_ = self.bbox2result_kitti(results_, self.CLASSES, pklfile_prefix_, submission_prefix_)\n            result_files[name] = result_files_\n    else:\n        result_files = self.bbox2result_kitti(outputs, self.CLASSES, pklfile_prefix, submission_prefix)\n    if 'waymo' in data_format:\n        from ..core.evaluation.waymo_utils.prediction_kitti_to_waymo import KITTI2Waymo\n        waymo_root = osp.join(self.data_root.split('kitti_format')[0], 'waymo_format')\n        if self.split == 'training':\n            waymo_tfrecords_dir = osp.join(waymo_root, 'validation')\n            prefix = '1'\n        elif self.split == 'testing':\n            waymo_tfrecords_dir = osp.join(waymo_root, 'testing')\n            prefix = '2'\n        else:\n            raise ValueError('Not supported split value.')\n        save_tmp_dir = tempfile.TemporaryDirectory()\n        waymo_results_save_dir = save_tmp_dir.name\n        waymo_results_final_path = f'{pklfile_prefix}.bin'\n        if 'pts_bbox' in result_files:\n            converter = KITTI2Waymo(result_files['pts_bbox'], waymo_tfrecords_dir, waymo_results_save_dir, waymo_results_final_path, prefix)\n        else:\n            converter = KITTI2Waymo(result_files, waymo_tfrecords_dir, waymo_results_save_dir, waymo_results_final_path, prefix)\n        converter.convert()\n        save_tmp_dir.cleanup()\n    return (result_files, tmp_dir)",
        "mutated": [
            "def format_results(self, outputs, pklfile_prefix=None, submission_prefix=None, data_format='waymo'):\n    if False:\n        i = 10\n    'Format the results to pkl file.\\n\\n        Args:\\n            outputs (list[dict]): Testing results of the dataset.\\n            pklfile_prefix (str): The prefix of pkl files. It includes\\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\\n                If not specified, a temp file will be created. Default: None.\\n            submission_prefix (str): The prefix of submitted files. It\\n                includes the file path and the prefix of filename, e.g.,\\n                \"a/b/prefix\". If not specified, a temp file will be created.\\n                Default: None.\\n            data_format (str, optional): Output data format.\\n                Default: \\'waymo\\'. Another supported choice is \\'kitti\\'.\\n\\n        Returns:\\n            tuple: (result_files, tmp_dir), result_files is a dict containing\\n                the json filepaths, tmp_dir is the temporal directory created\\n                for saving json files when jsonfile_prefix is not specified.\\n        '\n    if pklfile_prefix is None:\n        tmp_dir = tempfile.TemporaryDirectory()\n        pklfile_prefix = osp.join(tmp_dir.name, 'results')\n    else:\n        tmp_dir = None\n    assert 'waymo' in data_format or 'kitti' in data_format, f'invalid data_format {data_format}'\n    if not isinstance(outputs[0], dict) or 'img_bbox' in outputs[0]:\n        raise TypeError('Not supported type for reformat results.')\n    elif 'pts_bbox' in outputs[0]:\n        result_files = dict()\n        for name in outputs[0]:\n            results_ = [out[name] for out in outputs]\n            pklfile_prefix_ = pklfile_prefix + name\n            if submission_prefix is not None:\n                submission_prefix_ = f'{submission_prefix}_{name}'\n            else:\n                submission_prefix_ = None\n            result_files_ = self.bbox2result_kitti(results_, self.CLASSES, pklfile_prefix_, submission_prefix_)\n            result_files[name] = result_files_\n    else:\n        result_files = self.bbox2result_kitti(outputs, self.CLASSES, pklfile_prefix, submission_prefix)\n    if 'waymo' in data_format:\n        from ..core.evaluation.waymo_utils.prediction_kitti_to_waymo import KITTI2Waymo\n        waymo_root = osp.join(self.data_root.split('kitti_format')[0], 'waymo_format')\n        if self.split == 'training':\n            waymo_tfrecords_dir = osp.join(waymo_root, 'validation')\n            prefix = '1'\n        elif self.split == 'testing':\n            waymo_tfrecords_dir = osp.join(waymo_root, 'testing')\n            prefix = '2'\n        else:\n            raise ValueError('Not supported split value.')\n        save_tmp_dir = tempfile.TemporaryDirectory()\n        waymo_results_save_dir = save_tmp_dir.name\n        waymo_results_final_path = f'{pklfile_prefix}.bin'\n        if 'pts_bbox' in result_files:\n            converter = KITTI2Waymo(result_files['pts_bbox'], waymo_tfrecords_dir, waymo_results_save_dir, waymo_results_final_path, prefix)\n        else:\n            converter = KITTI2Waymo(result_files, waymo_tfrecords_dir, waymo_results_save_dir, waymo_results_final_path, prefix)\n        converter.convert()\n        save_tmp_dir.cleanup()\n    return (result_files, tmp_dir)",
            "def format_results(self, outputs, pklfile_prefix=None, submission_prefix=None, data_format='waymo'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Format the results to pkl file.\\n\\n        Args:\\n            outputs (list[dict]): Testing results of the dataset.\\n            pklfile_prefix (str): The prefix of pkl files. It includes\\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\\n                If not specified, a temp file will be created. Default: None.\\n            submission_prefix (str): The prefix of submitted files. It\\n                includes the file path and the prefix of filename, e.g.,\\n                \"a/b/prefix\". If not specified, a temp file will be created.\\n                Default: None.\\n            data_format (str, optional): Output data format.\\n                Default: \\'waymo\\'. Another supported choice is \\'kitti\\'.\\n\\n        Returns:\\n            tuple: (result_files, tmp_dir), result_files is a dict containing\\n                the json filepaths, tmp_dir is the temporal directory created\\n                for saving json files when jsonfile_prefix is not specified.\\n        '\n    if pklfile_prefix is None:\n        tmp_dir = tempfile.TemporaryDirectory()\n        pklfile_prefix = osp.join(tmp_dir.name, 'results')\n    else:\n        tmp_dir = None\n    assert 'waymo' in data_format or 'kitti' in data_format, f'invalid data_format {data_format}'\n    if not isinstance(outputs[0], dict) or 'img_bbox' in outputs[0]:\n        raise TypeError('Not supported type for reformat results.')\n    elif 'pts_bbox' in outputs[0]:\n        result_files = dict()\n        for name in outputs[0]:\n            results_ = [out[name] for out in outputs]\n            pklfile_prefix_ = pklfile_prefix + name\n            if submission_prefix is not None:\n                submission_prefix_ = f'{submission_prefix}_{name}'\n            else:\n                submission_prefix_ = None\n            result_files_ = self.bbox2result_kitti(results_, self.CLASSES, pklfile_prefix_, submission_prefix_)\n            result_files[name] = result_files_\n    else:\n        result_files = self.bbox2result_kitti(outputs, self.CLASSES, pklfile_prefix, submission_prefix)\n    if 'waymo' in data_format:\n        from ..core.evaluation.waymo_utils.prediction_kitti_to_waymo import KITTI2Waymo\n        waymo_root = osp.join(self.data_root.split('kitti_format')[0], 'waymo_format')\n        if self.split == 'training':\n            waymo_tfrecords_dir = osp.join(waymo_root, 'validation')\n            prefix = '1'\n        elif self.split == 'testing':\n            waymo_tfrecords_dir = osp.join(waymo_root, 'testing')\n            prefix = '2'\n        else:\n            raise ValueError('Not supported split value.')\n        save_tmp_dir = tempfile.TemporaryDirectory()\n        waymo_results_save_dir = save_tmp_dir.name\n        waymo_results_final_path = f'{pklfile_prefix}.bin'\n        if 'pts_bbox' in result_files:\n            converter = KITTI2Waymo(result_files['pts_bbox'], waymo_tfrecords_dir, waymo_results_save_dir, waymo_results_final_path, prefix)\n        else:\n            converter = KITTI2Waymo(result_files, waymo_tfrecords_dir, waymo_results_save_dir, waymo_results_final_path, prefix)\n        converter.convert()\n        save_tmp_dir.cleanup()\n    return (result_files, tmp_dir)",
            "def format_results(self, outputs, pklfile_prefix=None, submission_prefix=None, data_format='waymo'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Format the results to pkl file.\\n\\n        Args:\\n            outputs (list[dict]): Testing results of the dataset.\\n            pklfile_prefix (str): The prefix of pkl files. It includes\\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\\n                If not specified, a temp file will be created. Default: None.\\n            submission_prefix (str): The prefix of submitted files. It\\n                includes the file path and the prefix of filename, e.g.,\\n                \"a/b/prefix\". If not specified, a temp file will be created.\\n                Default: None.\\n            data_format (str, optional): Output data format.\\n                Default: \\'waymo\\'. Another supported choice is \\'kitti\\'.\\n\\n        Returns:\\n            tuple: (result_files, tmp_dir), result_files is a dict containing\\n                the json filepaths, tmp_dir is the temporal directory created\\n                for saving json files when jsonfile_prefix is not specified.\\n        '\n    if pklfile_prefix is None:\n        tmp_dir = tempfile.TemporaryDirectory()\n        pklfile_prefix = osp.join(tmp_dir.name, 'results')\n    else:\n        tmp_dir = None\n    assert 'waymo' in data_format or 'kitti' in data_format, f'invalid data_format {data_format}'\n    if not isinstance(outputs[0], dict) or 'img_bbox' in outputs[0]:\n        raise TypeError('Not supported type for reformat results.')\n    elif 'pts_bbox' in outputs[0]:\n        result_files = dict()\n        for name in outputs[0]:\n            results_ = [out[name] for out in outputs]\n            pklfile_prefix_ = pklfile_prefix + name\n            if submission_prefix is not None:\n                submission_prefix_ = f'{submission_prefix}_{name}'\n            else:\n                submission_prefix_ = None\n            result_files_ = self.bbox2result_kitti(results_, self.CLASSES, pklfile_prefix_, submission_prefix_)\n            result_files[name] = result_files_\n    else:\n        result_files = self.bbox2result_kitti(outputs, self.CLASSES, pklfile_prefix, submission_prefix)\n    if 'waymo' in data_format:\n        from ..core.evaluation.waymo_utils.prediction_kitti_to_waymo import KITTI2Waymo\n        waymo_root = osp.join(self.data_root.split('kitti_format')[0], 'waymo_format')\n        if self.split == 'training':\n            waymo_tfrecords_dir = osp.join(waymo_root, 'validation')\n            prefix = '1'\n        elif self.split == 'testing':\n            waymo_tfrecords_dir = osp.join(waymo_root, 'testing')\n            prefix = '2'\n        else:\n            raise ValueError('Not supported split value.')\n        save_tmp_dir = tempfile.TemporaryDirectory()\n        waymo_results_save_dir = save_tmp_dir.name\n        waymo_results_final_path = f'{pklfile_prefix}.bin'\n        if 'pts_bbox' in result_files:\n            converter = KITTI2Waymo(result_files['pts_bbox'], waymo_tfrecords_dir, waymo_results_save_dir, waymo_results_final_path, prefix)\n        else:\n            converter = KITTI2Waymo(result_files, waymo_tfrecords_dir, waymo_results_save_dir, waymo_results_final_path, prefix)\n        converter.convert()\n        save_tmp_dir.cleanup()\n    return (result_files, tmp_dir)",
            "def format_results(self, outputs, pklfile_prefix=None, submission_prefix=None, data_format='waymo'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Format the results to pkl file.\\n\\n        Args:\\n            outputs (list[dict]): Testing results of the dataset.\\n            pklfile_prefix (str): The prefix of pkl files. It includes\\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\\n                If not specified, a temp file will be created. Default: None.\\n            submission_prefix (str): The prefix of submitted files. It\\n                includes the file path and the prefix of filename, e.g.,\\n                \"a/b/prefix\". If not specified, a temp file will be created.\\n                Default: None.\\n            data_format (str, optional): Output data format.\\n                Default: \\'waymo\\'. Another supported choice is \\'kitti\\'.\\n\\n        Returns:\\n            tuple: (result_files, tmp_dir), result_files is a dict containing\\n                the json filepaths, tmp_dir is the temporal directory created\\n                for saving json files when jsonfile_prefix is not specified.\\n        '\n    if pklfile_prefix is None:\n        tmp_dir = tempfile.TemporaryDirectory()\n        pklfile_prefix = osp.join(tmp_dir.name, 'results')\n    else:\n        tmp_dir = None\n    assert 'waymo' in data_format or 'kitti' in data_format, f'invalid data_format {data_format}'\n    if not isinstance(outputs[0], dict) or 'img_bbox' in outputs[0]:\n        raise TypeError('Not supported type for reformat results.')\n    elif 'pts_bbox' in outputs[0]:\n        result_files = dict()\n        for name in outputs[0]:\n            results_ = [out[name] for out in outputs]\n            pklfile_prefix_ = pklfile_prefix + name\n            if submission_prefix is not None:\n                submission_prefix_ = f'{submission_prefix}_{name}'\n            else:\n                submission_prefix_ = None\n            result_files_ = self.bbox2result_kitti(results_, self.CLASSES, pklfile_prefix_, submission_prefix_)\n            result_files[name] = result_files_\n    else:\n        result_files = self.bbox2result_kitti(outputs, self.CLASSES, pklfile_prefix, submission_prefix)\n    if 'waymo' in data_format:\n        from ..core.evaluation.waymo_utils.prediction_kitti_to_waymo import KITTI2Waymo\n        waymo_root = osp.join(self.data_root.split('kitti_format')[0], 'waymo_format')\n        if self.split == 'training':\n            waymo_tfrecords_dir = osp.join(waymo_root, 'validation')\n            prefix = '1'\n        elif self.split == 'testing':\n            waymo_tfrecords_dir = osp.join(waymo_root, 'testing')\n            prefix = '2'\n        else:\n            raise ValueError('Not supported split value.')\n        save_tmp_dir = tempfile.TemporaryDirectory()\n        waymo_results_save_dir = save_tmp_dir.name\n        waymo_results_final_path = f'{pklfile_prefix}.bin'\n        if 'pts_bbox' in result_files:\n            converter = KITTI2Waymo(result_files['pts_bbox'], waymo_tfrecords_dir, waymo_results_save_dir, waymo_results_final_path, prefix)\n        else:\n            converter = KITTI2Waymo(result_files, waymo_tfrecords_dir, waymo_results_save_dir, waymo_results_final_path, prefix)\n        converter.convert()\n        save_tmp_dir.cleanup()\n    return (result_files, tmp_dir)",
            "def format_results(self, outputs, pklfile_prefix=None, submission_prefix=None, data_format='waymo'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Format the results to pkl file.\\n\\n        Args:\\n            outputs (list[dict]): Testing results of the dataset.\\n            pklfile_prefix (str): The prefix of pkl files. It includes\\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\\n                If not specified, a temp file will be created. Default: None.\\n            submission_prefix (str): The prefix of submitted files. It\\n                includes the file path and the prefix of filename, e.g.,\\n                \"a/b/prefix\". If not specified, a temp file will be created.\\n                Default: None.\\n            data_format (str, optional): Output data format.\\n                Default: \\'waymo\\'. Another supported choice is \\'kitti\\'.\\n\\n        Returns:\\n            tuple: (result_files, tmp_dir), result_files is a dict containing\\n                the json filepaths, tmp_dir is the temporal directory created\\n                for saving json files when jsonfile_prefix is not specified.\\n        '\n    if pklfile_prefix is None:\n        tmp_dir = tempfile.TemporaryDirectory()\n        pklfile_prefix = osp.join(tmp_dir.name, 'results')\n    else:\n        tmp_dir = None\n    assert 'waymo' in data_format or 'kitti' in data_format, f'invalid data_format {data_format}'\n    if not isinstance(outputs[0], dict) or 'img_bbox' in outputs[0]:\n        raise TypeError('Not supported type for reformat results.')\n    elif 'pts_bbox' in outputs[0]:\n        result_files = dict()\n        for name in outputs[0]:\n            results_ = [out[name] for out in outputs]\n            pklfile_prefix_ = pklfile_prefix + name\n            if submission_prefix is not None:\n                submission_prefix_ = f'{submission_prefix}_{name}'\n            else:\n                submission_prefix_ = None\n            result_files_ = self.bbox2result_kitti(results_, self.CLASSES, pklfile_prefix_, submission_prefix_)\n            result_files[name] = result_files_\n    else:\n        result_files = self.bbox2result_kitti(outputs, self.CLASSES, pklfile_prefix, submission_prefix)\n    if 'waymo' in data_format:\n        from ..core.evaluation.waymo_utils.prediction_kitti_to_waymo import KITTI2Waymo\n        waymo_root = osp.join(self.data_root.split('kitti_format')[0], 'waymo_format')\n        if self.split == 'training':\n            waymo_tfrecords_dir = osp.join(waymo_root, 'validation')\n            prefix = '1'\n        elif self.split == 'testing':\n            waymo_tfrecords_dir = osp.join(waymo_root, 'testing')\n            prefix = '2'\n        else:\n            raise ValueError('Not supported split value.')\n        save_tmp_dir = tempfile.TemporaryDirectory()\n        waymo_results_save_dir = save_tmp_dir.name\n        waymo_results_final_path = f'{pklfile_prefix}.bin'\n        if 'pts_bbox' in result_files:\n            converter = KITTI2Waymo(result_files['pts_bbox'], waymo_tfrecords_dir, waymo_results_save_dir, waymo_results_final_path, prefix)\n        else:\n            converter = KITTI2Waymo(result_files, waymo_tfrecords_dir, waymo_results_save_dir, waymo_results_final_path, prefix)\n        converter.convert()\n        save_tmp_dir.cleanup()\n    return (result_files, tmp_dir)"
        ]
    },
    {
        "func_name": "evaluate",
        "original": "def evaluate(self, results, metric='waymo', logger=None, pklfile_prefix=None, submission_prefix=None, show=False, out_dir=None, pipeline=None):\n    \"\"\"Evaluation in KITTI protocol.\n\n        Args:\n            results (list[dict]): Testing results of the dataset.\n            metric (str | list[str], optional): Metrics to be evaluated.\n                Default: 'waymo'. Another supported metric is 'kitti'.\n            logger (logging.Logger | str, optional): Logger used for printing\n                related information during evaluation. Default: None.\n            pklfile_prefix (str, optional): The prefix of pkl files including\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\n                If not specified, a temp file will be created. Default: None.\n            submission_prefix (str, optional): The prefix of submission data.\n                If not specified, the submission data will not be generated.\n            show (bool, optional): Whether to visualize.\n                Default: False.\n            out_dir (str, optional): Path to save the visualization results.\n                Default: None.\n            pipeline (list[dict], optional): raw data loading for showing.\n                Default: None.\n\n        Returns:\n            dict[str: float]: results of each evaluation metric\n        \"\"\"\n    assert 'waymo' in metric or 'kitti' in metric, f'invalid metric {metric}'\n    if 'kitti' in metric:\n        (result_files, tmp_dir) = self.format_results(results, pklfile_prefix, submission_prefix, data_format='kitti')\n        from mmdet3d.core.evaluation import kitti_eval\n        gt_annos = [info['annos'] for info in self.data_infos]\n        if isinstance(result_files, dict):\n            ap_dict = dict()\n            for (name, result_files_) in result_files.items():\n                eval_types = ['bev', '3d']\n                (ap_result_str, ap_dict_) = kitti_eval(gt_annos, result_files_, self.CLASSES, eval_types=eval_types)\n                for (ap_type, ap) in ap_dict_.items():\n                    ap_dict[f'{name}/{ap_type}'] = float('{:.4f}'.format(ap))\n                print_log(f'Results of {name}:\\n' + ap_result_str, logger=logger)\n        else:\n            (ap_result_str, ap_dict) = kitti_eval(gt_annos, result_files, self.CLASSES, eval_types=['bev', '3d'])\n            print_log('\\n' + ap_result_str, logger=logger)\n    if 'waymo' in metric:\n        waymo_root = osp.join(self.data_root.split('kitti_format')[0], 'waymo_format')\n        if pklfile_prefix is None:\n            eval_tmp_dir = tempfile.TemporaryDirectory()\n            pklfile_prefix = osp.join(eval_tmp_dir.name, 'results')\n        else:\n            eval_tmp_dir = None\n        (result_files, tmp_dir) = self.format_results(results, pklfile_prefix, submission_prefix, data_format='waymo')\n        import subprocess\n        ret_bytes = subprocess.check_output('mmdet3d/core/evaluation/waymo_utils/' + f'compute_detection_metrics_main {pklfile_prefix}.bin ' + f'{waymo_root}/gt.bin', shell=True)\n        ret_texts = ret_bytes.decode('utf-8')\n        print_log(ret_texts)\n        ap_dict = {'Vehicle/L1 mAP': 0, 'Vehicle/L1 mAPH': 0, 'Vehicle/L2 mAP': 0, 'Vehicle/L2 mAPH': 0, 'Pedestrian/L1 mAP': 0, 'Pedestrian/L1 mAPH': 0, 'Pedestrian/L2 mAP': 0, 'Pedestrian/L2 mAPH': 0, 'Sign/L1 mAP': 0, 'Sign/L1 mAPH': 0, 'Sign/L2 mAP': 0, 'Sign/L2 mAPH': 0, 'Cyclist/L1 mAP': 0, 'Cyclist/L1 mAPH': 0, 'Cyclist/L2 mAP': 0, 'Cyclist/L2 mAPH': 0, 'Overall/L1 mAP': 0, 'Overall/L1 mAPH': 0, 'Overall/L2 mAP': 0, 'Overall/L2 mAPH': 0}\n        mAP_splits = ret_texts.split('mAP ')\n        mAPH_splits = ret_texts.split('mAPH ')\n        for (idx, key) in enumerate(ap_dict.keys()):\n            split_idx = int(idx / 2) + 1\n            if idx % 2 == 0:\n                ap_dict[key] = float(mAP_splits[split_idx].split(']')[0])\n            else:\n                ap_dict[key] = float(mAPH_splits[split_idx].split(']')[0])\n        ap_dict['Overall/L1 mAP'] = (ap_dict['Vehicle/L1 mAP'] + ap_dict['Pedestrian/L1 mAP'] + ap_dict['Cyclist/L1 mAP']) / 3\n        ap_dict['Overall/L1 mAPH'] = (ap_dict['Vehicle/L1 mAPH'] + ap_dict['Pedestrian/L1 mAPH'] + ap_dict['Cyclist/L1 mAPH']) / 3\n        ap_dict['Overall/L2 mAP'] = (ap_dict['Vehicle/L2 mAP'] + ap_dict['Pedestrian/L2 mAP'] + ap_dict['Cyclist/L2 mAP']) / 3\n        ap_dict['Overall/L2 mAPH'] = (ap_dict['Vehicle/L2 mAPH'] + ap_dict['Pedestrian/L2 mAPH'] + ap_dict['Cyclist/L2 mAPH']) / 3\n        if eval_tmp_dir is not None:\n            eval_tmp_dir.cleanup()\n    if tmp_dir is not None:\n        tmp_dir.cleanup()\n    if show or out_dir:\n        self.show(results, out_dir, show=show, pipeline=pipeline)\n    return ap_dict",
        "mutated": [
            "def evaluate(self, results, metric='waymo', logger=None, pklfile_prefix=None, submission_prefix=None, show=False, out_dir=None, pipeline=None):\n    if False:\n        i = 10\n    'Evaluation in KITTI protocol.\\n\\n        Args:\\n            results (list[dict]): Testing results of the dataset.\\n            metric (str | list[str], optional): Metrics to be evaluated.\\n                Default: \\'waymo\\'. Another supported metric is \\'kitti\\'.\\n            logger (logging.Logger | str, optional): Logger used for printing\\n                related information during evaluation. Default: None.\\n            pklfile_prefix (str, optional): The prefix of pkl files including\\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\\n                If not specified, a temp file will be created. Default: None.\\n            submission_prefix (str, optional): The prefix of submission data.\\n                If not specified, the submission data will not be generated.\\n            show (bool, optional): Whether to visualize.\\n                Default: False.\\n            out_dir (str, optional): Path to save the visualization results.\\n                Default: None.\\n            pipeline (list[dict], optional): raw data loading for showing.\\n                Default: None.\\n\\n        Returns:\\n            dict[str: float]: results of each evaluation metric\\n        '\n    assert 'waymo' in metric or 'kitti' in metric, f'invalid metric {metric}'\n    if 'kitti' in metric:\n        (result_files, tmp_dir) = self.format_results(results, pklfile_prefix, submission_prefix, data_format='kitti')\n        from mmdet3d.core.evaluation import kitti_eval\n        gt_annos = [info['annos'] for info in self.data_infos]\n        if isinstance(result_files, dict):\n            ap_dict = dict()\n            for (name, result_files_) in result_files.items():\n                eval_types = ['bev', '3d']\n                (ap_result_str, ap_dict_) = kitti_eval(gt_annos, result_files_, self.CLASSES, eval_types=eval_types)\n                for (ap_type, ap) in ap_dict_.items():\n                    ap_dict[f'{name}/{ap_type}'] = float('{:.4f}'.format(ap))\n                print_log(f'Results of {name}:\\n' + ap_result_str, logger=logger)\n        else:\n            (ap_result_str, ap_dict) = kitti_eval(gt_annos, result_files, self.CLASSES, eval_types=['bev', '3d'])\n            print_log('\\n' + ap_result_str, logger=logger)\n    if 'waymo' in metric:\n        waymo_root = osp.join(self.data_root.split('kitti_format')[0], 'waymo_format')\n        if pklfile_prefix is None:\n            eval_tmp_dir = tempfile.TemporaryDirectory()\n            pklfile_prefix = osp.join(eval_tmp_dir.name, 'results')\n        else:\n            eval_tmp_dir = None\n        (result_files, tmp_dir) = self.format_results(results, pklfile_prefix, submission_prefix, data_format='waymo')\n        import subprocess\n        ret_bytes = subprocess.check_output('mmdet3d/core/evaluation/waymo_utils/' + f'compute_detection_metrics_main {pklfile_prefix}.bin ' + f'{waymo_root}/gt.bin', shell=True)\n        ret_texts = ret_bytes.decode('utf-8')\n        print_log(ret_texts)\n        ap_dict = {'Vehicle/L1 mAP': 0, 'Vehicle/L1 mAPH': 0, 'Vehicle/L2 mAP': 0, 'Vehicle/L2 mAPH': 0, 'Pedestrian/L1 mAP': 0, 'Pedestrian/L1 mAPH': 0, 'Pedestrian/L2 mAP': 0, 'Pedestrian/L2 mAPH': 0, 'Sign/L1 mAP': 0, 'Sign/L1 mAPH': 0, 'Sign/L2 mAP': 0, 'Sign/L2 mAPH': 0, 'Cyclist/L1 mAP': 0, 'Cyclist/L1 mAPH': 0, 'Cyclist/L2 mAP': 0, 'Cyclist/L2 mAPH': 0, 'Overall/L1 mAP': 0, 'Overall/L1 mAPH': 0, 'Overall/L2 mAP': 0, 'Overall/L2 mAPH': 0}\n        mAP_splits = ret_texts.split('mAP ')\n        mAPH_splits = ret_texts.split('mAPH ')\n        for (idx, key) in enumerate(ap_dict.keys()):\n            split_idx = int(idx / 2) + 1\n            if idx % 2 == 0:\n                ap_dict[key] = float(mAP_splits[split_idx].split(']')[0])\n            else:\n                ap_dict[key] = float(mAPH_splits[split_idx].split(']')[0])\n        ap_dict['Overall/L1 mAP'] = (ap_dict['Vehicle/L1 mAP'] + ap_dict['Pedestrian/L1 mAP'] + ap_dict['Cyclist/L1 mAP']) / 3\n        ap_dict['Overall/L1 mAPH'] = (ap_dict['Vehicle/L1 mAPH'] + ap_dict['Pedestrian/L1 mAPH'] + ap_dict['Cyclist/L1 mAPH']) / 3\n        ap_dict['Overall/L2 mAP'] = (ap_dict['Vehicle/L2 mAP'] + ap_dict['Pedestrian/L2 mAP'] + ap_dict['Cyclist/L2 mAP']) / 3\n        ap_dict['Overall/L2 mAPH'] = (ap_dict['Vehicle/L2 mAPH'] + ap_dict['Pedestrian/L2 mAPH'] + ap_dict['Cyclist/L2 mAPH']) / 3\n        if eval_tmp_dir is not None:\n            eval_tmp_dir.cleanup()\n    if tmp_dir is not None:\n        tmp_dir.cleanup()\n    if show or out_dir:\n        self.show(results, out_dir, show=show, pipeline=pipeline)\n    return ap_dict",
            "def evaluate(self, results, metric='waymo', logger=None, pklfile_prefix=None, submission_prefix=None, show=False, out_dir=None, pipeline=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Evaluation in KITTI protocol.\\n\\n        Args:\\n            results (list[dict]): Testing results of the dataset.\\n            metric (str | list[str], optional): Metrics to be evaluated.\\n                Default: \\'waymo\\'. Another supported metric is \\'kitti\\'.\\n            logger (logging.Logger | str, optional): Logger used for printing\\n                related information during evaluation. Default: None.\\n            pklfile_prefix (str, optional): The prefix of pkl files including\\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\\n                If not specified, a temp file will be created. Default: None.\\n            submission_prefix (str, optional): The prefix of submission data.\\n                If not specified, the submission data will not be generated.\\n            show (bool, optional): Whether to visualize.\\n                Default: False.\\n            out_dir (str, optional): Path to save the visualization results.\\n                Default: None.\\n            pipeline (list[dict], optional): raw data loading for showing.\\n                Default: None.\\n\\n        Returns:\\n            dict[str: float]: results of each evaluation metric\\n        '\n    assert 'waymo' in metric or 'kitti' in metric, f'invalid metric {metric}'\n    if 'kitti' in metric:\n        (result_files, tmp_dir) = self.format_results(results, pklfile_prefix, submission_prefix, data_format='kitti')\n        from mmdet3d.core.evaluation import kitti_eval\n        gt_annos = [info['annos'] for info in self.data_infos]\n        if isinstance(result_files, dict):\n            ap_dict = dict()\n            for (name, result_files_) in result_files.items():\n                eval_types = ['bev', '3d']\n                (ap_result_str, ap_dict_) = kitti_eval(gt_annos, result_files_, self.CLASSES, eval_types=eval_types)\n                for (ap_type, ap) in ap_dict_.items():\n                    ap_dict[f'{name}/{ap_type}'] = float('{:.4f}'.format(ap))\n                print_log(f'Results of {name}:\\n' + ap_result_str, logger=logger)\n        else:\n            (ap_result_str, ap_dict) = kitti_eval(gt_annos, result_files, self.CLASSES, eval_types=['bev', '3d'])\n            print_log('\\n' + ap_result_str, logger=logger)\n    if 'waymo' in metric:\n        waymo_root = osp.join(self.data_root.split('kitti_format')[0], 'waymo_format')\n        if pklfile_prefix is None:\n            eval_tmp_dir = tempfile.TemporaryDirectory()\n            pklfile_prefix = osp.join(eval_tmp_dir.name, 'results')\n        else:\n            eval_tmp_dir = None\n        (result_files, tmp_dir) = self.format_results(results, pklfile_prefix, submission_prefix, data_format='waymo')\n        import subprocess\n        ret_bytes = subprocess.check_output('mmdet3d/core/evaluation/waymo_utils/' + f'compute_detection_metrics_main {pklfile_prefix}.bin ' + f'{waymo_root}/gt.bin', shell=True)\n        ret_texts = ret_bytes.decode('utf-8')\n        print_log(ret_texts)\n        ap_dict = {'Vehicle/L1 mAP': 0, 'Vehicle/L1 mAPH': 0, 'Vehicle/L2 mAP': 0, 'Vehicle/L2 mAPH': 0, 'Pedestrian/L1 mAP': 0, 'Pedestrian/L1 mAPH': 0, 'Pedestrian/L2 mAP': 0, 'Pedestrian/L2 mAPH': 0, 'Sign/L1 mAP': 0, 'Sign/L1 mAPH': 0, 'Sign/L2 mAP': 0, 'Sign/L2 mAPH': 0, 'Cyclist/L1 mAP': 0, 'Cyclist/L1 mAPH': 0, 'Cyclist/L2 mAP': 0, 'Cyclist/L2 mAPH': 0, 'Overall/L1 mAP': 0, 'Overall/L1 mAPH': 0, 'Overall/L2 mAP': 0, 'Overall/L2 mAPH': 0}\n        mAP_splits = ret_texts.split('mAP ')\n        mAPH_splits = ret_texts.split('mAPH ')\n        for (idx, key) in enumerate(ap_dict.keys()):\n            split_idx = int(idx / 2) + 1\n            if idx % 2 == 0:\n                ap_dict[key] = float(mAP_splits[split_idx].split(']')[0])\n            else:\n                ap_dict[key] = float(mAPH_splits[split_idx].split(']')[0])\n        ap_dict['Overall/L1 mAP'] = (ap_dict['Vehicle/L1 mAP'] + ap_dict['Pedestrian/L1 mAP'] + ap_dict['Cyclist/L1 mAP']) / 3\n        ap_dict['Overall/L1 mAPH'] = (ap_dict['Vehicle/L1 mAPH'] + ap_dict['Pedestrian/L1 mAPH'] + ap_dict['Cyclist/L1 mAPH']) / 3\n        ap_dict['Overall/L2 mAP'] = (ap_dict['Vehicle/L2 mAP'] + ap_dict['Pedestrian/L2 mAP'] + ap_dict['Cyclist/L2 mAP']) / 3\n        ap_dict['Overall/L2 mAPH'] = (ap_dict['Vehicle/L2 mAPH'] + ap_dict['Pedestrian/L2 mAPH'] + ap_dict['Cyclist/L2 mAPH']) / 3\n        if eval_tmp_dir is not None:\n            eval_tmp_dir.cleanup()\n    if tmp_dir is not None:\n        tmp_dir.cleanup()\n    if show or out_dir:\n        self.show(results, out_dir, show=show, pipeline=pipeline)\n    return ap_dict",
            "def evaluate(self, results, metric='waymo', logger=None, pklfile_prefix=None, submission_prefix=None, show=False, out_dir=None, pipeline=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Evaluation in KITTI protocol.\\n\\n        Args:\\n            results (list[dict]): Testing results of the dataset.\\n            metric (str | list[str], optional): Metrics to be evaluated.\\n                Default: \\'waymo\\'. Another supported metric is \\'kitti\\'.\\n            logger (logging.Logger | str, optional): Logger used for printing\\n                related information during evaluation. Default: None.\\n            pklfile_prefix (str, optional): The prefix of pkl files including\\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\\n                If not specified, a temp file will be created. Default: None.\\n            submission_prefix (str, optional): The prefix of submission data.\\n                If not specified, the submission data will not be generated.\\n            show (bool, optional): Whether to visualize.\\n                Default: False.\\n            out_dir (str, optional): Path to save the visualization results.\\n                Default: None.\\n            pipeline (list[dict], optional): raw data loading for showing.\\n                Default: None.\\n\\n        Returns:\\n            dict[str: float]: results of each evaluation metric\\n        '\n    assert 'waymo' in metric or 'kitti' in metric, f'invalid metric {metric}'\n    if 'kitti' in metric:\n        (result_files, tmp_dir) = self.format_results(results, pklfile_prefix, submission_prefix, data_format='kitti')\n        from mmdet3d.core.evaluation import kitti_eval\n        gt_annos = [info['annos'] for info in self.data_infos]\n        if isinstance(result_files, dict):\n            ap_dict = dict()\n            for (name, result_files_) in result_files.items():\n                eval_types = ['bev', '3d']\n                (ap_result_str, ap_dict_) = kitti_eval(gt_annos, result_files_, self.CLASSES, eval_types=eval_types)\n                for (ap_type, ap) in ap_dict_.items():\n                    ap_dict[f'{name}/{ap_type}'] = float('{:.4f}'.format(ap))\n                print_log(f'Results of {name}:\\n' + ap_result_str, logger=logger)\n        else:\n            (ap_result_str, ap_dict) = kitti_eval(gt_annos, result_files, self.CLASSES, eval_types=['bev', '3d'])\n            print_log('\\n' + ap_result_str, logger=logger)\n    if 'waymo' in metric:\n        waymo_root = osp.join(self.data_root.split('kitti_format')[0], 'waymo_format')\n        if pklfile_prefix is None:\n            eval_tmp_dir = tempfile.TemporaryDirectory()\n            pklfile_prefix = osp.join(eval_tmp_dir.name, 'results')\n        else:\n            eval_tmp_dir = None\n        (result_files, tmp_dir) = self.format_results(results, pklfile_prefix, submission_prefix, data_format='waymo')\n        import subprocess\n        ret_bytes = subprocess.check_output('mmdet3d/core/evaluation/waymo_utils/' + f'compute_detection_metrics_main {pklfile_prefix}.bin ' + f'{waymo_root}/gt.bin', shell=True)\n        ret_texts = ret_bytes.decode('utf-8')\n        print_log(ret_texts)\n        ap_dict = {'Vehicle/L1 mAP': 0, 'Vehicle/L1 mAPH': 0, 'Vehicle/L2 mAP': 0, 'Vehicle/L2 mAPH': 0, 'Pedestrian/L1 mAP': 0, 'Pedestrian/L1 mAPH': 0, 'Pedestrian/L2 mAP': 0, 'Pedestrian/L2 mAPH': 0, 'Sign/L1 mAP': 0, 'Sign/L1 mAPH': 0, 'Sign/L2 mAP': 0, 'Sign/L2 mAPH': 0, 'Cyclist/L1 mAP': 0, 'Cyclist/L1 mAPH': 0, 'Cyclist/L2 mAP': 0, 'Cyclist/L2 mAPH': 0, 'Overall/L1 mAP': 0, 'Overall/L1 mAPH': 0, 'Overall/L2 mAP': 0, 'Overall/L2 mAPH': 0}\n        mAP_splits = ret_texts.split('mAP ')\n        mAPH_splits = ret_texts.split('mAPH ')\n        for (idx, key) in enumerate(ap_dict.keys()):\n            split_idx = int(idx / 2) + 1\n            if idx % 2 == 0:\n                ap_dict[key] = float(mAP_splits[split_idx].split(']')[0])\n            else:\n                ap_dict[key] = float(mAPH_splits[split_idx].split(']')[0])\n        ap_dict['Overall/L1 mAP'] = (ap_dict['Vehicle/L1 mAP'] + ap_dict['Pedestrian/L1 mAP'] + ap_dict['Cyclist/L1 mAP']) / 3\n        ap_dict['Overall/L1 mAPH'] = (ap_dict['Vehicle/L1 mAPH'] + ap_dict['Pedestrian/L1 mAPH'] + ap_dict['Cyclist/L1 mAPH']) / 3\n        ap_dict['Overall/L2 mAP'] = (ap_dict['Vehicle/L2 mAP'] + ap_dict['Pedestrian/L2 mAP'] + ap_dict['Cyclist/L2 mAP']) / 3\n        ap_dict['Overall/L2 mAPH'] = (ap_dict['Vehicle/L2 mAPH'] + ap_dict['Pedestrian/L2 mAPH'] + ap_dict['Cyclist/L2 mAPH']) / 3\n        if eval_tmp_dir is not None:\n            eval_tmp_dir.cleanup()\n    if tmp_dir is not None:\n        tmp_dir.cleanup()\n    if show or out_dir:\n        self.show(results, out_dir, show=show, pipeline=pipeline)\n    return ap_dict",
            "def evaluate(self, results, metric='waymo', logger=None, pklfile_prefix=None, submission_prefix=None, show=False, out_dir=None, pipeline=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Evaluation in KITTI protocol.\\n\\n        Args:\\n            results (list[dict]): Testing results of the dataset.\\n            metric (str | list[str], optional): Metrics to be evaluated.\\n                Default: \\'waymo\\'. Another supported metric is \\'kitti\\'.\\n            logger (logging.Logger | str, optional): Logger used for printing\\n                related information during evaluation. Default: None.\\n            pklfile_prefix (str, optional): The prefix of pkl files including\\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\\n                If not specified, a temp file will be created. Default: None.\\n            submission_prefix (str, optional): The prefix of submission data.\\n                If not specified, the submission data will not be generated.\\n            show (bool, optional): Whether to visualize.\\n                Default: False.\\n            out_dir (str, optional): Path to save the visualization results.\\n                Default: None.\\n            pipeline (list[dict], optional): raw data loading for showing.\\n                Default: None.\\n\\n        Returns:\\n            dict[str: float]: results of each evaluation metric\\n        '\n    assert 'waymo' in metric or 'kitti' in metric, f'invalid metric {metric}'\n    if 'kitti' in metric:\n        (result_files, tmp_dir) = self.format_results(results, pklfile_prefix, submission_prefix, data_format='kitti')\n        from mmdet3d.core.evaluation import kitti_eval\n        gt_annos = [info['annos'] for info in self.data_infos]\n        if isinstance(result_files, dict):\n            ap_dict = dict()\n            for (name, result_files_) in result_files.items():\n                eval_types = ['bev', '3d']\n                (ap_result_str, ap_dict_) = kitti_eval(gt_annos, result_files_, self.CLASSES, eval_types=eval_types)\n                for (ap_type, ap) in ap_dict_.items():\n                    ap_dict[f'{name}/{ap_type}'] = float('{:.4f}'.format(ap))\n                print_log(f'Results of {name}:\\n' + ap_result_str, logger=logger)\n        else:\n            (ap_result_str, ap_dict) = kitti_eval(gt_annos, result_files, self.CLASSES, eval_types=['bev', '3d'])\n            print_log('\\n' + ap_result_str, logger=logger)\n    if 'waymo' in metric:\n        waymo_root = osp.join(self.data_root.split('kitti_format')[0], 'waymo_format')\n        if pklfile_prefix is None:\n            eval_tmp_dir = tempfile.TemporaryDirectory()\n            pklfile_prefix = osp.join(eval_tmp_dir.name, 'results')\n        else:\n            eval_tmp_dir = None\n        (result_files, tmp_dir) = self.format_results(results, pklfile_prefix, submission_prefix, data_format='waymo')\n        import subprocess\n        ret_bytes = subprocess.check_output('mmdet3d/core/evaluation/waymo_utils/' + f'compute_detection_metrics_main {pklfile_prefix}.bin ' + f'{waymo_root}/gt.bin', shell=True)\n        ret_texts = ret_bytes.decode('utf-8')\n        print_log(ret_texts)\n        ap_dict = {'Vehicle/L1 mAP': 0, 'Vehicle/L1 mAPH': 0, 'Vehicle/L2 mAP': 0, 'Vehicle/L2 mAPH': 0, 'Pedestrian/L1 mAP': 0, 'Pedestrian/L1 mAPH': 0, 'Pedestrian/L2 mAP': 0, 'Pedestrian/L2 mAPH': 0, 'Sign/L1 mAP': 0, 'Sign/L1 mAPH': 0, 'Sign/L2 mAP': 0, 'Sign/L2 mAPH': 0, 'Cyclist/L1 mAP': 0, 'Cyclist/L1 mAPH': 0, 'Cyclist/L2 mAP': 0, 'Cyclist/L2 mAPH': 0, 'Overall/L1 mAP': 0, 'Overall/L1 mAPH': 0, 'Overall/L2 mAP': 0, 'Overall/L2 mAPH': 0}\n        mAP_splits = ret_texts.split('mAP ')\n        mAPH_splits = ret_texts.split('mAPH ')\n        for (idx, key) in enumerate(ap_dict.keys()):\n            split_idx = int(idx / 2) + 1\n            if idx % 2 == 0:\n                ap_dict[key] = float(mAP_splits[split_idx].split(']')[0])\n            else:\n                ap_dict[key] = float(mAPH_splits[split_idx].split(']')[0])\n        ap_dict['Overall/L1 mAP'] = (ap_dict['Vehicle/L1 mAP'] + ap_dict['Pedestrian/L1 mAP'] + ap_dict['Cyclist/L1 mAP']) / 3\n        ap_dict['Overall/L1 mAPH'] = (ap_dict['Vehicle/L1 mAPH'] + ap_dict['Pedestrian/L1 mAPH'] + ap_dict['Cyclist/L1 mAPH']) / 3\n        ap_dict['Overall/L2 mAP'] = (ap_dict['Vehicle/L2 mAP'] + ap_dict['Pedestrian/L2 mAP'] + ap_dict['Cyclist/L2 mAP']) / 3\n        ap_dict['Overall/L2 mAPH'] = (ap_dict['Vehicle/L2 mAPH'] + ap_dict['Pedestrian/L2 mAPH'] + ap_dict['Cyclist/L2 mAPH']) / 3\n        if eval_tmp_dir is not None:\n            eval_tmp_dir.cleanup()\n    if tmp_dir is not None:\n        tmp_dir.cleanup()\n    if show or out_dir:\n        self.show(results, out_dir, show=show, pipeline=pipeline)\n    return ap_dict",
            "def evaluate(self, results, metric='waymo', logger=None, pklfile_prefix=None, submission_prefix=None, show=False, out_dir=None, pipeline=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Evaluation in KITTI protocol.\\n\\n        Args:\\n            results (list[dict]): Testing results of the dataset.\\n            metric (str | list[str], optional): Metrics to be evaluated.\\n                Default: \\'waymo\\'. Another supported metric is \\'kitti\\'.\\n            logger (logging.Logger | str, optional): Logger used for printing\\n                related information during evaluation. Default: None.\\n            pklfile_prefix (str, optional): The prefix of pkl files including\\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\\n                If not specified, a temp file will be created. Default: None.\\n            submission_prefix (str, optional): The prefix of submission data.\\n                If not specified, the submission data will not be generated.\\n            show (bool, optional): Whether to visualize.\\n                Default: False.\\n            out_dir (str, optional): Path to save the visualization results.\\n                Default: None.\\n            pipeline (list[dict], optional): raw data loading for showing.\\n                Default: None.\\n\\n        Returns:\\n            dict[str: float]: results of each evaluation metric\\n        '\n    assert 'waymo' in metric or 'kitti' in metric, f'invalid metric {metric}'\n    if 'kitti' in metric:\n        (result_files, tmp_dir) = self.format_results(results, pklfile_prefix, submission_prefix, data_format='kitti')\n        from mmdet3d.core.evaluation import kitti_eval\n        gt_annos = [info['annos'] for info in self.data_infos]\n        if isinstance(result_files, dict):\n            ap_dict = dict()\n            for (name, result_files_) in result_files.items():\n                eval_types = ['bev', '3d']\n                (ap_result_str, ap_dict_) = kitti_eval(gt_annos, result_files_, self.CLASSES, eval_types=eval_types)\n                for (ap_type, ap) in ap_dict_.items():\n                    ap_dict[f'{name}/{ap_type}'] = float('{:.4f}'.format(ap))\n                print_log(f'Results of {name}:\\n' + ap_result_str, logger=logger)\n        else:\n            (ap_result_str, ap_dict) = kitti_eval(gt_annos, result_files, self.CLASSES, eval_types=['bev', '3d'])\n            print_log('\\n' + ap_result_str, logger=logger)\n    if 'waymo' in metric:\n        waymo_root = osp.join(self.data_root.split('kitti_format')[0], 'waymo_format')\n        if pklfile_prefix is None:\n            eval_tmp_dir = tempfile.TemporaryDirectory()\n            pklfile_prefix = osp.join(eval_tmp_dir.name, 'results')\n        else:\n            eval_tmp_dir = None\n        (result_files, tmp_dir) = self.format_results(results, pklfile_prefix, submission_prefix, data_format='waymo')\n        import subprocess\n        ret_bytes = subprocess.check_output('mmdet3d/core/evaluation/waymo_utils/' + f'compute_detection_metrics_main {pklfile_prefix}.bin ' + f'{waymo_root}/gt.bin', shell=True)\n        ret_texts = ret_bytes.decode('utf-8')\n        print_log(ret_texts)\n        ap_dict = {'Vehicle/L1 mAP': 0, 'Vehicle/L1 mAPH': 0, 'Vehicle/L2 mAP': 0, 'Vehicle/L2 mAPH': 0, 'Pedestrian/L1 mAP': 0, 'Pedestrian/L1 mAPH': 0, 'Pedestrian/L2 mAP': 0, 'Pedestrian/L2 mAPH': 0, 'Sign/L1 mAP': 0, 'Sign/L1 mAPH': 0, 'Sign/L2 mAP': 0, 'Sign/L2 mAPH': 0, 'Cyclist/L1 mAP': 0, 'Cyclist/L1 mAPH': 0, 'Cyclist/L2 mAP': 0, 'Cyclist/L2 mAPH': 0, 'Overall/L1 mAP': 0, 'Overall/L1 mAPH': 0, 'Overall/L2 mAP': 0, 'Overall/L2 mAPH': 0}\n        mAP_splits = ret_texts.split('mAP ')\n        mAPH_splits = ret_texts.split('mAPH ')\n        for (idx, key) in enumerate(ap_dict.keys()):\n            split_idx = int(idx / 2) + 1\n            if idx % 2 == 0:\n                ap_dict[key] = float(mAP_splits[split_idx].split(']')[0])\n            else:\n                ap_dict[key] = float(mAPH_splits[split_idx].split(']')[0])\n        ap_dict['Overall/L1 mAP'] = (ap_dict['Vehicle/L1 mAP'] + ap_dict['Pedestrian/L1 mAP'] + ap_dict['Cyclist/L1 mAP']) / 3\n        ap_dict['Overall/L1 mAPH'] = (ap_dict['Vehicle/L1 mAPH'] + ap_dict['Pedestrian/L1 mAPH'] + ap_dict['Cyclist/L1 mAPH']) / 3\n        ap_dict['Overall/L2 mAP'] = (ap_dict['Vehicle/L2 mAP'] + ap_dict['Pedestrian/L2 mAP'] + ap_dict['Cyclist/L2 mAP']) / 3\n        ap_dict['Overall/L2 mAPH'] = (ap_dict['Vehicle/L2 mAPH'] + ap_dict['Pedestrian/L2 mAPH'] + ap_dict['Cyclist/L2 mAPH']) / 3\n        if eval_tmp_dir is not None:\n            eval_tmp_dir.cleanup()\n    if tmp_dir is not None:\n        tmp_dir.cleanup()\n    if show or out_dir:\n        self.show(results, out_dir, show=show, pipeline=pipeline)\n    return ap_dict"
        ]
    },
    {
        "func_name": "bbox2result_kitti",
        "original": "def bbox2result_kitti(self, net_outputs, class_names, pklfile_prefix=None, submission_prefix=None):\n    \"\"\"Convert results to kitti format for evaluation and test submission.\n\n        Args:\n            net_outputs (List[np.ndarray]): list of array storing the\n                bbox and score\n            class_nanes (List[String]): A list of class names\n            pklfile_prefix (str): The prefix of pkl file.\n            submission_prefix (str): The prefix of submission file.\n\n        Returns:\n            List[dict]: A list of dict have the kitti 3d format\n        \"\"\"\n    assert len(net_outputs) == len(self.data_infos), 'invalid list length of network outputs'\n    if submission_prefix is not None:\n        mmcv.mkdir_or_exist(submission_prefix)\n    det_annos = []\n    print('\\nConverting prediction to KITTI format')\n    for (idx, pred_dicts) in enumerate(mmcv.track_iter_progress(net_outputs)):\n        annos = []\n        info = self.data_infos[idx]\n        sample_idx = info['image']['image_idx']\n        image_shape = info['image']['image_shape'][:2]\n        box_dict = self.convert_valid_bboxes(pred_dicts, info)\n        if len(box_dict['bbox']) > 0:\n            box_2d_preds = box_dict['bbox']\n            box_preds = box_dict['box3d_camera']\n            scores = box_dict['scores']\n            box_preds_lidar = box_dict['box3d_lidar']\n            label_preds = box_dict['label_preds']\n            anno = {'name': [], 'truncated': [], 'occluded': [], 'alpha': [], 'bbox': [], 'dimensions': [], 'location': [], 'rotation_y': [], 'score': []}\n            for (box, box_lidar, bbox, score, label) in zip(box_preds, box_preds_lidar, box_2d_preds, scores, label_preds):\n                bbox[2:] = np.minimum(bbox[2:], image_shape[::-1])\n                bbox[:2] = np.maximum(bbox[:2], [0, 0])\n                anno['name'].append(class_names[int(label)])\n                anno['truncated'].append(0.0)\n                anno['occluded'].append(0)\n                anno['alpha'].append(-np.arctan2(-box_lidar[1], box_lidar[0]) + box[6])\n                anno['bbox'].append(bbox)\n                anno['dimensions'].append(box[3:6])\n                anno['location'].append(box[:3])\n                anno['rotation_y'].append(box[6])\n                anno['score'].append(score)\n            anno = {k: np.stack(v) for (k, v) in anno.items()}\n            annos.append(anno)\n            if submission_prefix is not None:\n                curr_file = f'{submission_prefix}/{sample_idx:07d}.txt'\n                with open(curr_file, 'w') as f:\n                    bbox = anno['bbox']\n                    loc = anno['location']\n                    dims = anno['dimensions']\n                    for idx in range(len(bbox)):\n                        print('{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(anno['name'][idx], anno['alpha'][idx], bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3], dims[idx][1], dims[idx][2], dims[idx][0], loc[idx][0], loc[idx][1], loc[idx][2], anno['rotation_y'][idx], anno['score'][idx]), file=f)\n        else:\n            annos.append({'name': np.array([]), 'truncated': np.array([]), 'occluded': np.array([]), 'alpha': np.array([]), 'bbox': np.zeros([0, 4]), 'dimensions': np.zeros([0, 3]), 'location': np.zeros([0, 3]), 'rotation_y': np.array([]), 'score': np.array([])})\n        annos[-1]['sample_idx'] = np.array([sample_idx] * len(annos[-1]['score']), dtype=np.int64)\n        det_annos += annos\n    if pklfile_prefix is not None:\n        if not pklfile_prefix.endswith(('.pkl', '.pickle')):\n            out = f'{pklfile_prefix}.pkl'\n        mmcv.dump(det_annos, out)\n        print(f'Result is saved to {out}.')\n    return det_annos",
        "mutated": [
            "def bbox2result_kitti(self, net_outputs, class_names, pklfile_prefix=None, submission_prefix=None):\n    if False:\n        i = 10\n    'Convert results to kitti format for evaluation and test submission.\\n\\n        Args:\\n            net_outputs (List[np.ndarray]): list of array storing the\\n                bbox and score\\n            class_nanes (List[String]): A list of class names\\n            pklfile_prefix (str): The prefix of pkl file.\\n            submission_prefix (str): The prefix of submission file.\\n\\n        Returns:\\n            List[dict]: A list of dict have the kitti 3d format\\n        '\n    assert len(net_outputs) == len(self.data_infos), 'invalid list length of network outputs'\n    if submission_prefix is not None:\n        mmcv.mkdir_or_exist(submission_prefix)\n    det_annos = []\n    print('\\nConverting prediction to KITTI format')\n    for (idx, pred_dicts) in enumerate(mmcv.track_iter_progress(net_outputs)):\n        annos = []\n        info = self.data_infos[idx]\n        sample_idx = info['image']['image_idx']\n        image_shape = info['image']['image_shape'][:2]\n        box_dict = self.convert_valid_bboxes(pred_dicts, info)\n        if len(box_dict['bbox']) > 0:\n            box_2d_preds = box_dict['bbox']\n            box_preds = box_dict['box3d_camera']\n            scores = box_dict['scores']\n            box_preds_lidar = box_dict['box3d_lidar']\n            label_preds = box_dict['label_preds']\n            anno = {'name': [], 'truncated': [], 'occluded': [], 'alpha': [], 'bbox': [], 'dimensions': [], 'location': [], 'rotation_y': [], 'score': []}\n            for (box, box_lidar, bbox, score, label) in zip(box_preds, box_preds_lidar, box_2d_preds, scores, label_preds):\n                bbox[2:] = np.minimum(bbox[2:], image_shape[::-1])\n                bbox[:2] = np.maximum(bbox[:2], [0, 0])\n                anno['name'].append(class_names[int(label)])\n                anno['truncated'].append(0.0)\n                anno['occluded'].append(0)\n                anno['alpha'].append(-np.arctan2(-box_lidar[1], box_lidar[0]) + box[6])\n                anno['bbox'].append(bbox)\n                anno['dimensions'].append(box[3:6])\n                anno['location'].append(box[:3])\n                anno['rotation_y'].append(box[6])\n                anno['score'].append(score)\n            anno = {k: np.stack(v) for (k, v) in anno.items()}\n            annos.append(anno)\n            if submission_prefix is not None:\n                curr_file = f'{submission_prefix}/{sample_idx:07d}.txt'\n                with open(curr_file, 'w') as f:\n                    bbox = anno['bbox']\n                    loc = anno['location']\n                    dims = anno['dimensions']\n                    for idx in range(len(bbox)):\n                        print('{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(anno['name'][idx], anno['alpha'][idx], bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3], dims[idx][1], dims[idx][2], dims[idx][0], loc[idx][0], loc[idx][1], loc[idx][2], anno['rotation_y'][idx], anno['score'][idx]), file=f)\n        else:\n            annos.append({'name': np.array([]), 'truncated': np.array([]), 'occluded': np.array([]), 'alpha': np.array([]), 'bbox': np.zeros([0, 4]), 'dimensions': np.zeros([0, 3]), 'location': np.zeros([0, 3]), 'rotation_y': np.array([]), 'score': np.array([])})\n        annos[-1]['sample_idx'] = np.array([sample_idx] * len(annos[-1]['score']), dtype=np.int64)\n        det_annos += annos\n    if pklfile_prefix is not None:\n        if not pklfile_prefix.endswith(('.pkl', '.pickle')):\n            out = f'{pklfile_prefix}.pkl'\n        mmcv.dump(det_annos, out)\n        print(f'Result is saved to {out}.')\n    return det_annos",
            "def bbox2result_kitti(self, net_outputs, class_names, pklfile_prefix=None, submission_prefix=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Convert results to kitti format for evaluation and test submission.\\n\\n        Args:\\n            net_outputs (List[np.ndarray]): list of array storing the\\n                bbox and score\\n            class_nanes (List[String]): A list of class names\\n            pklfile_prefix (str): The prefix of pkl file.\\n            submission_prefix (str): The prefix of submission file.\\n\\n        Returns:\\n            List[dict]: A list of dict have the kitti 3d format\\n        '\n    assert len(net_outputs) == len(self.data_infos), 'invalid list length of network outputs'\n    if submission_prefix is not None:\n        mmcv.mkdir_or_exist(submission_prefix)\n    det_annos = []\n    print('\\nConverting prediction to KITTI format')\n    for (idx, pred_dicts) in enumerate(mmcv.track_iter_progress(net_outputs)):\n        annos = []\n        info = self.data_infos[idx]\n        sample_idx = info['image']['image_idx']\n        image_shape = info['image']['image_shape'][:2]\n        box_dict = self.convert_valid_bboxes(pred_dicts, info)\n        if len(box_dict['bbox']) > 0:\n            box_2d_preds = box_dict['bbox']\n            box_preds = box_dict['box3d_camera']\n            scores = box_dict['scores']\n            box_preds_lidar = box_dict['box3d_lidar']\n            label_preds = box_dict['label_preds']\n            anno = {'name': [], 'truncated': [], 'occluded': [], 'alpha': [], 'bbox': [], 'dimensions': [], 'location': [], 'rotation_y': [], 'score': []}\n            for (box, box_lidar, bbox, score, label) in zip(box_preds, box_preds_lidar, box_2d_preds, scores, label_preds):\n                bbox[2:] = np.minimum(bbox[2:], image_shape[::-1])\n                bbox[:2] = np.maximum(bbox[:2], [0, 0])\n                anno['name'].append(class_names[int(label)])\n                anno['truncated'].append(0.0)\n                anno['occluded'].append(0)\n                anno['alpha'].append(-np.arctan2(-box_lidar[1], box_lidar[0]) + box[6])\n                anno['bbox'].append(bbox)\n                anno['dimensions'].append(box[3:6])\n                anno['location'].append(box[:3])\n                anno['rotation_y'].append(box[6])\n                anno['score'].append(score)\n            anno = {k: np.stack(v) for (k, v) in anno.items()}\n            annos.append(anno)\n            if submission_prefix is not None:\n                curr_file = f'{submission_prefix}/{sample_idx:07d}.txt'\n                with open(curr_file, 'w') as f:\n                    bbox = anno['bbox']\n                    loc = anno['location']\n                    dims = anno['dimensions']\n                    for idx in range(len(bbox)):\n                        print('{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(anno['name'][idx], anno['alpha'][idx], bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3], dims[idx][1], dims[idx][2], dims[idx][0], loc[idx][0], loc[idx][1], loc[idx][2], anno['rotation_y'][idx], anno['score'][idx]), file=f)\n        else:\n            annos.append({'name': np.array([]), 'truncated': np.array([]), 'occluded': np.array([]), 'alpha': np.array([]), 'bbox': np.zeros([0, 4]), 'dimensions': np.zeros([0, 3]), 'location': np.zeros([0, 3]), 'rotation_y': np.array([]), 'score': np.array([])})\n        annos[-1]['sample_idx'] = np.array([sample_idx] * len(annos[-1]['score']), dtype=np.int64)\n        det_annos += annos\n    if pklfile_prefix is not None:\n        if not pklfile_prefix.endswith(('.pkl', '.pickle')):\n            out = f'{pklfile_prefix}.pkl'\n        mmcv.dump(det_annos, out)\n        print(f'Result is saved to {out}.')\n    return det_annos",
            "def bbox2result_kitti(self, net_outputs, class_names, pklfile_prefix=None, submission_prefix=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Convert results to kitti format for evaluation and test submission.\\n\\n        Args:\\n            net_outputs (List[np.ndarray]): list of array storing the\\n                bbox and score\\n            class_nanes (List[String]): A list of class names\\n            pklfile_prefix (str): The prefix of pkl file.\\n            submission_prefix (str): The prefix of submission file.\\n\\n        Returns:\\n            List[dict]: A list of dict have the kitti 3d format\\n        '\n    assert len(net_outputs) == len(self.data_infos), 'invalid list length of network outputs'\n    if submission_prefix is not None:\n        mmcv.mkdir_or_exist(submission_prefix)\n    det_annos = []\n    print('\\nConverting prediction to KITTI format')\n    for (idx, pred_dicts) in enumerate(mmcv.track_iter_progress(net_outputs)):\n        annos = []\n        info = self.data_infos[idx]\n        sample_idx = info['image']['image_idx']\n        image_shape = info['image']['image_shape'][:2]\n        box_dict = self.convert_valid_bboxes(pred_dicts, info)\n        if len(box_dict['bbox']) > 0:\n            box_2d_preds = box_dict['bbox']\n            box_preds = box_dict['box3d_camera']\n            scores = box_dict['scores']\n            box_preds_lidar = box_dict['box3d_lidar']\n            label_preds = box_dict['label_preds']\n            anno = {'name': [], 'truncated': [], 'occluded': [], 'alpha': [], 'bbox': [], 'dimensions': [], 'location': [], 'rotation_y': [], 'score': []}\n            for (box, box_lidar, bbox, score, label) in zip(box_preds, box_preds_lidar, box_2d_preds, scores, label_preds):\n                bbox[2:] = np.minimum(bbox[2:], image_shape[::-1])\n                bbox[:2] = np.maximum(bbox[:2], [0, 0])\n                anno['name'].append(class_names[int(label)])\n                anno['truncated'].append(0.0)\n                anno['occluded'].append(0)\n                anno['alpha'].append(-np.arctan2(-box_lidar[1], box_lidar[0]) + box[6])\n                anno['bbox'].append(bbox)\n                anno['dimensions'].append(box[3:6])\n                anno['location'].append(box[:3])\n                anno['rotation_y'].append(box[6])\n                anno['score'].append(score)\n            anno = {k: np.stack(v) for (k, v) in anno.items()}\n            annos.append(anno)\n            if submission_prefix is not None:\n                curr_file = f'{submission_prefix}/{sample_idx:07d}.txt'\n                with open(curr_file, 'w') as f:\n                    bbox = anno['bbox']\n                    loc = anno['location']\n                    dims = anno['dimensions']\n                    for idx in range(len(bbox)):\n                        print('{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(anno['name'][idx], anno['alpha'][idx], bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3], dims[idx][1], dims[idx][2], dims[idx][0], loc[idx][0], loc[idx][1], loc[idx][2], anno['rotation_y'][idx], anno['score'][idx]), file=f)\n        else:\n            annos.append({'name': np.array([]), 'truncated': np.array([]), 'occluded': np.array([]), 'alpha': np.array([]), 'bbox': np.zeros([0, 4]), 'dimensions': np.zeros([0, 3]), 'location': np.zeros([0, 3]), 'rotation_y': np.array([]), 'score': np.array([])})\n        annos[-1]['sample_idx'] = np.array([sample_idx] * len(annos[-1]['score']), dtype=np.int64)\n        det_annos += annos\n    if pklfile_prefix is not None:\n        if not pklfile_prefix.endswith(('.pkl', '.pickle')):\n            out = f'{pklfile_prefix}.pkl'\n        mmcv.dump(det_annos, out)\n        print(f'Result is saved to {out}.')\n    return det_annos",
            "def bbox2result_kitti(self, net_outputs, class_names, pklfile_prefix=None, submission_prefix=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Convert results to kitti format for evaluation and test submission.\\n\\n        Args:\\n            net_outputs (List[np.ndarray]): list of array storing the\\n                bbox and score\\n            class_nanes (List[String]): A list of class names\\n            pklfile_prefix (str): The prefix of pkl file.\\n            submission_prefix (str): The prefix of submission file.\\n\\n        Returns:\\n            List[dict]: A list of dict have the kitti 3d format\\n        '\n    assert len(net_outputs) == len(self.data_infos), 'invalid list length of network outputs'\n    if submission_prefix is not None:\n        mmcv.mkdir_or_exist(submission_prefix)\n    det_annos = []\n    print('\\nConverting prediction to KITTI format')\n    for (idx, pred_dicts) in enumerate(mmcv.track_iter_progress(net_outputs)):\n        annos = []\n        info = self.data_infos[idx]\n        sample_idx = info['image']['image_idx']\n        image_shape = info['image']['image_shape'][:2]\n        box_dict = self.convert_valid_bboxes(pred_dicts, info)\n        if len(box_dict['bbox']) > 0:\n            box_2d_preds = box_dict['bbox']\n            box_preds = box_dict['box3d_camera']\n            scores = box_dict['scores']\n            box_preds_lidar = box_dict['box3d_lidar']\n            label_preds = box_dict['label_preds']\n            anno = {'name': [], 'truncated': [], 'occluded': [], 'alpha': [], 'bbox': [], 'dimensions': [], 'location': [], 'rotation_y': [], 'score': []}\n            for (box, box_lidar, bbox, score, label) in zip(box_preds, box_preds_lidar, box_2d_preds, scores, label_preds):\n                bbox[2:] = np.minimum(bbox[2:], image_shape[::-1])\n                bbox[:2] = np.maximum(bbox[:2], [0, 0])\n                anno['name'].append(class_names[int(label)])\n                anno['truncated'].append(0.0)\n                anno['occluded'].append(0)\n                anno['alpha'].append(-np.arctan2(-box_lidar[1], box_lidar[0]) + box[6])\n                anno['bbox'].append(bbox)\n                anno['dimensions'].append(box[3:6])\n                anno['location'].append(box[:3])\n                anno['rotation_y'].append(box[6])\n                anno['score'].append(score)\n            anno = {k: np.stack(v) for (k, v) in anno.items()}\n            annos.append(anno)\n            if submission_prefix is not None:\n                curr_file = f'{submission_prefix}/{sample_idx:07d}.txt'\n                with open(curr_file, 'w') as f:\n                    bbox = anno['bbox']\n                    loc = anno['location']\n                    dims = anno['dimensions']\n                    for idx in range(len(bbox)):\n                        print('{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(anno['name'][idx], anno['alpha'][idx], bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3], dims[idx][1], dims[idx][2], dims[idx][0], loc[idx][0], loc[idx][1], loc[idx][2], anno['rotation_y'][idx], anno['score'][idx]), file=f)\n        else:\n            annos.append({'name': np.array([]), 'truncated': np.array([]), 'occluded': np.array([]), 'alpha': np.array([]), 'bbox': np.zeros([0, 4]), 'dimensions': np.zeros([0, 3]), 'location': np.zeros([0, 3]), 'rotation_y': np.array([]), 'score': np.array([])})\n        annos[-1]['sample_idx'] = np.array([sample_idx] * len(annos[-1]['score']), dtype=np.int64)\n        det_annos += annos\n    if pklfile_prefix is not None:\n        if not pklfile_prefix.endswith(('.pkl', '.pickle')):\n            out = f'{pklfile_prefix}.pkl'\n        mmcv.dump(det_annos, out)\n        print(f'Result is saved to {out}.')\n    return det_annos",
            "def bbox2result_kitti(self, net_outputs, class_names, pklfile_prefix=None, submission_prefix=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Convert results to kitti format for evaluation and test submission.\\n\\n        Args:\\n            net_outputs (List[np.ndarray]): list of array storing the\\n                bbox and score\\n            class_nanes (List[String]): A list of class names\\n            pklfile_prefix (str): The prefix of pkl file.\\n            submission_prefix (str): The prefix of submission file.\\n\\n        Returns:\\n            List[dict]: A list of dict have the kitti 3d format\\n        '\n    assert len(net_outputs) == len(self.data_infos), 'invalid list length of network outputs'\n    if submission_prefix is not None:\n        mmcv.mkdir_or_exist(submission_prefix)\n    det_annos = []\n    print('\\nConverting prediction to KITTI format')\n    for (idx, pred_dicts) in enumerate(mmcv.track_iter_progress(net_outputs)):\n        annos = []\n        info = self.data_infos[idx]\n        sample_idx = info['image']['image_idx']\n        image_shape = info['image']['image_shape'][:2]\n        box_dict = self.convert_valid_bboxes(pred_dicts, info)\n        if len(box_dict['bbox']) > 0:\n            box_2d_preds = box_dict['bbox']\n            box_preds = box_dict['box3d_camera']\n            scores = box_dict['scores']\n            box_preds_lidar = box_dict['box3d_lidar']\n            label_preds = box_dict['label_preds']\n            anno = {'name': [], 'truncated': [], 'occluded': [], 'alpha': [], 'bbox': [], 'dimensions': [], 'location': [], 'rotation_y': [], 'score': []}\n            for (box, box_lidar, bbox, score, label) in zip(box_preds, box_preds_lidar, box_2d_preds, scores, label_preds):\n                bbox[2:] = np.minimum(bbox[2:], image_shape[::-1])\n                bbox[:2] = np.maximum(bbox[:2], [0, 0])\n                anno['name'].append(class_names[int(label)])\n                anno['truncated'].append(0.0)\n                anno['occluded'].append(0)\n                anno['alpha'].append(-np.arctan2(-box_lidar[1], box_lidar[0]) + box[6])\n                anno['bbox'].append(bbox)\n                anno['dimensions'].append(box[3:6])\n                anno['location'].append(box[:3])\n                anno['rotation_y'].append(box[6])\n                anno['score'].append(score)\n            anno = {k: np.stack(v) for (k, v) in anno.items()}\n            annos.append(anno)\n            if submission_prefix is not None:\n                curr_file = f'{submission_prefix}/{sample_idx:07d}.txt'\n                with open(curr_file, 'w') as f:\n                    bbox = anno['bbox']\n                    loc = anno['location']\n                    dims = anno['dimensions']\n                    for idx in range(len(bbox)):\n                        print('{} -1 -1 {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(anno['name'][idx], anno['alpha'][idx], bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3], dims[idx][1], dims[idx][2], dims[idx][0], loc[idx][0], loc[idx][1], loc[idx][2], anno['rotation_y'][idx], anno['score'][idx]), file=f)\n        else:\n            annos.append({'name': np.array([]), 'truncated': np.array([]), 'occluded': np.array([]), 'alpha': np.array([]), 'bbox': np.zeros([0, 4]), 'dimensions': np.zeros([0, 3]), 'location': np.zeros([0, 3]), 'rotation_y': np.array([]), 'score': np.array([])})\n        annos[-1]['sample_idx'] = np.array([sample_idx] * len(annos[-1]['score']), dtype=np.int64)\n        det_annos += annos\n    if pklfile_prefix is not None:\n        if not pklfile_prefix.endswith(('.pkl', '.pickle')):\n            out = f'{pklfile_prefix}.pkl'\n        mmcv.dump(det_annos, out)\n        print(f'Result is saved to {out}.')\n    return det_annos"
        ]
    },
    {
        "func_name": "convert_valid_bboxes",
        "original": "def convert_valid_bboxes(self, box_dict, info):\n    \"\"\"Convert the boxes into valid format.\n\n        Args:\n            box_dict (dict): Bounding boxes to be converted.\n\n                - boxes_3d (:obj:``LiDARInstance3DBoxes``): 3D bounding boxes.\n                - scores_3d (np.ndarray): Scores of predicted boxes.\n                - labels_3d (np.ndarray): Class labels of predicted boxes.\n            info (dict): Dataset information dictionary.\n\n        Returns:\n            dict: Valid boxes after conversion.\n\n                - bbox (np.ndarray): 2D bounding boxes (in camera 0).\n                - box3d_camera (np.ndarray): 3D boxes in camera coordinates.\n                - box3d_lidar (np.ndarray): 3D boxes in lidar coordinates.\n                - scores (np.ndarray): Scores of predicted boxes.\n                - label_preds (np.ndarray): Class labels of predicted boxes.\n                - sample_idx (np.ndarray): Sample index.\n        \"\"\"\n    box_preds = box_dict['boxes_3d']\n    scores = box_dict['scores_3d']\n    labels = box_dict['labels_3d']\n    sample_idx = info['image']['image_idx']\n    box_preds.limit_yaw(offset=0.5, period=np.pi * 2)\n    if len(box_preds) == 0:\n        return dict(bbox=np.zeros([0, 4]), box3d_camera=np.zeros([0, 7]), box3d_lidar=np.zeros([0, 7]), scores=np.zeros([0]), label_preds=np.zeros([0, 4]), sample_idx=sample_idx)\n    rect = info['calib']['R0_rect'].astype(np.float32)\n    Trv2c = info['calib']['Tr_velo_to_cam'].astype(np.float32)\n    P0 = info['calib']['P0'].astype(np.float32)\n    P0 = box_preds.tensor.new_tensor(P0)\n    box_preds_camera = box_preds.convert_to(Box3DMode.CAM, rect @ Trv2c)\n    box_corners = box_preds_camera.corners\n    box_corners_in_image = points_cam2img(box_corners, P0)\n    minxy = torch.min(box_corners_in_image, dim=1)[0]\n    maxxy = torch.max(box_corners_in_image, dim=1)[0]\n    box_2d_preds = torch.cat([minxy, maxxy], dim=1)\n    limit_range = box_preds.tensor.new_tensor(self.pcd_limit_range)\n    valid_pcd_inds = (box_preds.center > limit_range[:3]) & (box_preds.center < limit_range[3:])\n    valid_inds = valid_pcd_inds.all(-1)\n    if valid_inds.sum() > 0:\n        return dict(bbox=box_2d_preds[valid_inds, :].numpy(), box3d_camera=box_preds_camera[valid_inds].tensor.numpy(), box3d_lidar=box_preds[valid_inds].tensor.numpy(), scores=scores[valid_inds].numpy(), label_preds=labels[valid_inds].numpy(), sample_idx=sample_idx)\n    else:\n        return dict(bbox=np.zeros([0, 4]), box3d_camera=np.zeros([0, 7]), box3d_lidar=np.zeros([0, 7]), scores=np.zeros([0]), label_preds=np.zeros([0, 4]), sample_idx=sample_idx)",
        "mutated": [
            "def convert_valid_bboxes(self, box_dict, info):\n    if False:\n        i = 10\n    'Convert the boxes into valid format.\\n\\n        Args:\\n            box_dict (dict): Bounding boxes to be converted.\\n\\n                - boxes_3d (:obj:``LiDARInstance3DBoxes``): 3D bounding boxes.\\n                - scores_3d (np.ndarray): Scores of predicted boxes.\\n                - labels_3d (np.ndarray): Class labels of predicted boxes.\\n            info (dict): Dataset information dictionary.\\n\\n        Returns:\\n            dict: Valid boxes after conversion.\\n\\n                - bbox (np.ndarray): 2D bounding boxes (in camera 0).\\n                - box3d_camera (np.ndarray): 3D boxes in camera coordinates.\\n                - box3d_lidar (np.ndarray): 3D boxes in lidar coordinates.\\n                - scores (np.ndarray): Scores of predicted boxes.\\n                - label_preds (np.ndarray): Class labels of predicted boxes.\\n                - sample_idx (np.ndarray): Sample index.\\n        '\n    box_preds = box_dict['boxes_3d']\n    scores = box_dict['scores_3d']\n    labels = box_dict['labels_3d']\n    sample_idx = info['image']['image_idx']\n    box_preds.limit_yaw(offset=0.5, period=np.pi * 2)\n    if len(box_preds) == 0:\n        return dict(bbox=np.zeros([0, 4]), box3d_camera=np.zeros([0, 7]), box3d_lidar=np.zeros([0, 7]), scores=np.zeros([0]), label_preds=np.zeros([0, 4]), sample_idx=sample_idx)\n    rect = info['calib']['R0_rect'].astype(np.float32)\n    Trv2c = info['calib']['Tr_velo_to_cam'].astype(np.float32)\n    P0 = info['calib']['P0'].astype(np.float32)\n    P0 = box_preds.tensor.new_tensor(P0)\n    box_preds_camera = box_preds.convert_to(Box3DMode.CAM, rect @ Trv2c)\n    box_corners = box_preds_camera.corners\n    box_corners_in_image = points_cam2img(box_corners, P0)\n    minxy = torch.min(box_corners_in_image, dim=1)[0]\n    maxxy = torch.max(box_corners_in_image, dim=1)[0]\n    box_2d_preds = torch.cat([minxy, maxxy], dim=1)\n    limit_range = box_preds.tensor.new_tensor(self.pcd_limit_range)\n    valid_pcd_inds = (box_preds.center > limit_range[:3]) & (box_preds.center < limit_range[3:])\n    valid_inds = valid_pcd_inds.all(-1)\n    if valid_inds.sum() > 0:\n        return dict(bbox=box_2d_preds[valid_inds, :].numpy(), box3d_camera=box_preds_camera[valid_inds].tensor.numpy(), box3d_lidar=box_preds[valid_inds].tensor.numpy(), scores=scores[valid_inds].numpy(), label_preds=labels[valid_inds].numpy(), sample_idx=sample_idx)\n    else:\n        return dict(bbox=np.zeros([0, 4]), box3d_camera=np.zeros([0, 7]), box3d_lidar=np.zeros([0, 7]), scores=np.zeros([0]), label_preds=np.zeros([0, 4]), sample_idx=sample_idx)",
            "def convert_valid_bboxes(self, box_dict, info):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Convert the boxes into valid format.\\n\\n        Args:\\n            box_dict (dict): Bounding boxes to be converted.\\n\\n                - boxes_3d (:obj:``LiDARInstance3DBoxes``): 3D bounding boxes.\\n                - scores_3d (np.ndarray): Scores of predicted boxes.\\n                - labels_3d (np.ndarray): Class labels of predicted boxes.\\n            info (dict): Dataset information dictionary.\\n\\n        Returns:\\n            dict: Valid boxes after conversion.\\n\\n                - bbox (np.ndarray): 2D bounding boxes (in camera 0).\\n                - box3d_camera (np.ndarray): 3D boxes in camera coordinates.\\n                - box3d_lidar (np.ndarray): 3D boxes in lidar coordinates.\\n                - scores (np.ndarray): Scores of predicted boxes.\\n                - label_preds (np.ndarray): Class labels of predicted boxes.\\n                - sample_idx (np.ndarray): Sample index.\\n        '\n    box_preds = box_dict['boxes_3d']\n    scores = box_dict['scores_3d']\n    labels = box_dict['labels_3d']\n    sample_idx = info['image']['image_idx']\n    box_preds.limit_yaw(offset=0.5, period=np.pi * 2)\n    if len(box_preds) == 0:\n        return dict(bbox=np.zeros([0, 4]), box3d_camera=np.zeros([0, 7]), box3d_lidar=np.zeros([0, 7]), scores=np.zeros([0]), label_preds=np.zeros([0, 4]), sample_idx=sample_idx)\n    rect = info['calib']['R0_rect'].astype(np.float32)\n    Trv2c = info['calib']['Tr_velo_to_cam'].astype(np.float32)\n    P0 = info['calib']['P0'].astype(np.float32)\n    P0 = box_preds.tensor.new_tensor(P0)\n    box_preds_camera = box_preds.convert_to(Box3DMode.CAM, rect @ Trv2c)\n    box_corners = box_preds_camera.corners\n    box_corners_in_image = points_cam2img(box_corners, P0)\n    minxy = torch.min(box_corners_in_image, dim=1)[0]\n    maxxy = torch.max(box_corners_in_image, dim=1)[0]\n    box_2d_preds = torch.cat([minxy, maxxy], dim=1)\n    limit_range = box_preds.tensor.new_tensor(self.pcd_limit_range)\n    valid_pcd_inds = (box_preds.center > limit_range[:3]) & (box_preds.center < limit_range[3:])\n    valid_inds = valid_pcd_inds.all(-1)\n    if valid_inds.sum() > 0:\n        return dict(bbox=box_2d_preds[valid_inds, :].numpy(), box3d_camera=box_preds_camera[valid_inds].tensor.numpy(), box3d_lidar=box_preds[valid_inds].tensor.numpy(), scores=scores[valid_inds].numpy(), label_preds=labels[valid_inds].numpy(), sample_idx=sample_idx)\n    else:\n        return dict(bbox=np.zeros([0, 4]), box3d_camera=np.zeros([0, 7]), box3d_lidar=np.zeros([0, 7]), scores=np.zeros([0]), label_preds=np.zeros([0, 4]), sample_idx=sample_idx)",
            "def convert_valid_bboxes(self, box_dict, info):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Convert the boxes into valid format.\\n\\n        Args:\\n            box_dict (dict): Bounding boxes to be converted.\\n\\n                - boxes_3d (:obj:``LiDARInstance3DBoxes``): 3D bounding boxes.\\n                - scores_3d (np.ndarray): Scores of predicted boxes.\\n                - labels_3d (np.ndarray): Class labels of predicted boxes.\\n            info (dict): Dataset information dictionary.\\n\\n        Returns:\\n            dict: Valid boxes after conversion.\\n\\n                - bbox (np.ndarray): 2D bounding boxes (in camera 0).\\n                - box3d_camera (np.ndarray): 3D boxes in camera coordinates.\\n                - box3d_lidar (np.ndarray): 3D boxes in lidar coordinates.\\n                - scores (np.ndarray): Scores of predicted boxes.\\n                - label_preds (np.ndarray): Class labels of predicted boxes.\\n                - sample_idx (np.ndarray): Sample index.\\n        '\n    box_preds = box_dict['boxes_3d']\n    scores = box_dict['scores_3d']\n    labels = box_dict['labels_3d']\n    sample_idx = info['image']['image_idx']\n    box_preds.limit_yaw(offset=0.5, period=np.pi * 2)\n    if len(box_preds) == 0:\n        return dict(bbox=np.zeros([0, 4]), box3d_camera=np.zeros([0, 7]), box3d_lidar=np.zeros([0, 7]), scores=np.zeros([0]), label_preds=np.zeros([0, 4]), sample_idx=sample_idx)\n    rect = info['calib']['R0_rect'].astype(np.float32)\n    Trv2c = info['calib']['Tr_velo_to_cam'].astype(np.float32)\n    P0 = info['calib']['P0'].astype(np.float32)\n    P0 = box_preds.tensor.new_tensor(P0)\n    box_preds_camera = box_preds.convert_to(Box3DMode.CAM, rect @ Trv2c)\n    box_corners = box_preds_camera.corners\n    box_corners_in_image = points_cam2img(box_corners, P0)\n    minxy = torch.min(box_corners_in_image, dim=1)[0]\n    maxxy = torch.max(box_corners_in_image, dim=1)[0]\n    box_2d_preds = torch.cat([minxy, maxxy], dim=1)\n    limit_range = box_preds.tensor.new_tensor(self.pcd_limit_range)\n    valid_pcd_inds = (box_preds.center > limit_range[:3]) & (box_preds.center < limit_range[3:])\n    valid_inds = valid_pcd_inds.all(-1)\n    if valid_inds.sum() > 0:\n        return dict(bbox=box_2d_preds[valid_inds, :].numpy(), box3d_camera=box_preds_camera[valid_inds].tensor.numpy(), box3d_lidar=box_preds[valid_inds].tensor.numpy(), scores=scores[valid_inds].numpy(), label_preds=labels[valid_inds].numpy(), sample_idx=sample_idx)\n    else:\n        return dict(bbox=np.zeros([0, 4]), box3d_camera=np.zeros([0, 7]), box3d_lidar=np.zeros([0, 7]), scores=np.zeros([0]), label_preds=np.zeros([0, 4]), sample_idx=sample_idx)",
            "def convert_valid_bboxes(self, box_dict, info):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Convert the boxes into valid format.\\n\\n        Args:\\n            box_dict (dict): Bounding boxes to be converted.\\n\\n                - boxes_3d (:obj:``LiDARInstance3DBoxes``): 3D bounding boxes.\\n                - scores_3d (np.ndarray): Scores of predicted boxes.\\n                - labels_3d (np.ndarray): Class labels of predicted boxes.\\n            info (dict): Dataset information dictionary.\\n\\n        Returns:\\n            dict: Valid boxes after conversion.\\n\\n                - bbox (np.ndarray): 2D bounding boxes (in camera 0).\\n                - box3d_camera (np.ndarray): 3D boxes in camera coordinates.\\n                - box3d_lidar (np.ndarray): 3D boxes in lidar coordinates.\\n                - scores (np.ndarray): Scores of predicted boxes.\\n                - label_preds (np.ndarray): Class labels of predicted boxes.\\n                - sample_idx (np.ndarray): Sample index.\\n        '\n    box_preds = box_dict['boxes_3d']\n    scores = box_dict['scores_3d']\n    labels = box_dict['labels_3d']\n    sample_idx = info['image']['image_idx']\n    box_preds.limit_yaw(offset=0.5, period=np.pi * 2)\n    if len(box_preds) == 0:\n        return dict(bbox=np.zeros([0, 4]), box3d_camera=np.zeros([0, 7]), box3d_lidar=np.zeros([0, 7]), scores=np.zeros([0]), label_preds=np.zeros([0, 4]), sample_idx=sample_idx)\n    rect = info['calib']['R0_rect'].astype(np.float32)\n    Trv2c = info['calib']['Tr_velo_to_cam'].astype(np.float32)\n    P0 = info['calib']['P0'].astype(np.float32)\n    P0 = box_preds.tensor.new_tensor(P0)\n    box_preds_camera = box_preds.convert_to(Box3DMode.CAM, rect @ Trv2c)\n    box_corners = box_preds_camera.corners\n    box_corners_in_image = points_cam2img(box_corners, P0)\n    minxy = torch.min(box_corners_in_image, dim=1)[0]\n    maxxy = torch.max(box_corners_in_image, dim=1)[0]\n    box_2d_preds = torch.cat([minxy, maxxy], dim=1)\n    limit_range = box_preds.tensor.new_tensor(self.pcd_limit_range)\n    valid_pcd_inds = (box_preds.center > limit_range[:3]) & (box_preds.center < limit_range[3:])\n    valid_inds = valid_pcd_inds.all(-1)\n    if valid_inds.sum() > 0:\n        return dict(bbox=box_2d_preds[valid_inds, :].numpy(), box3d_camera=box_preds_camera[valid_inds].tensor.numpy(), box3d_lidar=box_preds[valid_inds].tensor.numpy(), scores=scores[valid_inds].numpy(), label_preds=labels[valid_inds].numpy(), sample_idx=sample_idx)\n    else:\n        return dict(bbox=np.zeros([0, 4]), box3d_camera=np.zeros([0, 7]), box3d_lidar=np.zeros([0, 7]), scores=np.zeros([0]), label_preds=np.zeros([0, 4]), sample_idx=sample_idx)",
            "def convert_valid_bboxes(self, box_dict, info):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Convert the boxes into valid format.\\n\\n        Args:\\n            box_dict (dict): Bounding boxes to be converted.\\n\\n                - boxes_3d (:obj:``LiDARInstance3DBoxes``): 3D bounding boxes.\\n                - scores_3d (np.ndarray): Scores of predicted boxes.\\n                - labels_3d (np.ndarray): Class labels of predicted boxes.\\n            info (dict): Dataset information dictionary.\\n\\n        Returns:\\n            dict: Valid boxes after conversion.\\n\\n                - bbox (np.ndarray): 2D bounding boxes (in camera 0).\\n                - box3d_camera (np.ndarray): 3D boxes in camera coordinates.\\n                - box3d_lidar (np.ndarray): 3D boxes in lidar coordinates.\\n                - scores (np.ndarray): Scores of predicted boxes.\\n                - label_preds (np.ndarray): Class labels of predicted boxes.\\n                - sample_idx (np.ndarray): Sample index.\\n        '\n    box_preds = box_dict['boxes_3d']\n    scores = box_dict['scores_3d']\n    labels = box_dict['labels_3d']\n    sample_idx = info['image']['image_idx']\n    box_preds.limit_yaw(offset=0.5, period=np.pi * 2)\n    if len(box_preds) == 0:\n        return dict(bbox=np.zeros([0, 4]), box3d_camera=np.zeros([0, 7]), box3d_lidar=np.zeros([0, 7]), scores=np.zeros([0]), label_preds=np.zeros([0, 4]), sample_idx=sample_idx)\n    rect = info['calib']['R0_rect'].astype(np.float32)\n    Trv2c = info['calib']['Tr_velo_to_cam'].astype(np.float32)\n    P0 = info['calib']['P0'].astype(np.float32)\n    P0 = box_preds.tensor.new_tensor(P0)\n    box_preds_camera = box_preds.convert_to(Box3DMode.CAM, rect @ Trv2c)\n    box_corners = box_preds_camera.corners\n    box_corners_in_image = points_cam2img(box_corners, P0)\n    minxy = torch.min(box_corners_in_image, dim=1)[0]\n    maxxy = torch.max(box_corners_in_image, dim=1)[0]\n    box_2d_preds = torch.cat([minxy, maxxy], dim=1)\n    limit_range = box_preds.tensor.new_tensor(self.pcd_limit_range)\n    valid_pcd_inds = (box_preds.center > limit_range[:3]) & (box_preds.center < limit_range[3:])\n    valid_inds = valid_pcd_inds.all(-1)\n    if valid_inds.sum() > 0:\n        return dict(bbox=box_2d_preds[valid_inds, :].numpy(), box3d_camera=box_preds_camera[valid_inds].tensor.numpy(), box3d_lidar=box_preds[valid_inds].tensor.numpy(), scores=scores[valid_inds].numpy(), label_preds=labels[valid_inds].numpy(), sample_idx=sample_idx)\n    else:\n        return dict(bbox=np.zeros([0, 4]), box3d_camera=np.zeros([0, 7]), box3d_lidar=np.zeros([0, 7]), scores=np.zeros([0]), label_preds=np.zeros([0, 4]), sample_idx=sample_idx)"
        ]
    }
]