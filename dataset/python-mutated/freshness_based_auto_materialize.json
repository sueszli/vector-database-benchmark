[
    {
        "func_name": "get_execution_period_for_policy",
        "original": "def get_execution_period_for_policy(freshness_policy: FreshnessPolicy, effective_data_time: Optional[datetime.datetime], current_time: datetime.datetime) -> pendulum.Period:\n    if freshness_policy.cron_schedule:\n        tick_iterator = cron_string_iterator(start_timestamp=current_time.timestamp(), cron_string=freshness_policy.cron_schedule, execution_timezone=freshness_policy.cron_schedule_timezone)\n        while True:\n            tick = next(tick_iterator)\n            required_data_time = tick - freshness_policy.maximum_lag_delta\n            if effective_data_time is None or effective_data_time < required_data_time:\n                return pendulum.Period(start=required_data_time, end=tick)\n    else:\n        if effective_data_time is None:\n            return pendulum.Period(start=current_time - freshness_policy.maximum_lag_delta, end=current_time)\n        return pendulum.Period(start=effective_data_time + 0.9 * freshness_policy.maximum_lag_delta, end=max(effective_data_time + freshness_policy.maximum_lag_delta, current_time))",
        "mutated": [
            "def get_execution_period_for_policy(freshness_policy: FreshnessPolicy, effective_data_time: Optional[datetime.datetime], current_time: datetime.datetime) -> pendulum.Period:\n    if False:\n        i = 10\n    if freshness_policy.cron_schedule:\n        tick_iterator = cron_string_iterator(start_timestamp=current_time.timestamp(), cron_string=freshness_policy.cron_schedule, execution_timezone=freshness_policy.cron_schedule_timezone)\n        while True:\n            tick = next(tick_iterator)\n            required_data_time = tick - freshness_policy.maximum_lag_delta\n            if effective_data_time is None or effective_data_time < required_data_time:\n                return pendulum.Period(start=required_data_time, end=tick)\n    else:\n        if effective_data_time is None:\n            return pendulum.Period(start=current_time - freshness_policy.maximum_lag_delta, end=current_time)\n        return pendulum.Period(start=effective_data_time + 0.9 * freshness_policy.maximum_lag_delta, end=max(effective_data_time + freshness_policy.maximum_lag_delta, current_time))",
            "def get_execution_period_for_policy(freshness_policy: FreshnessPolicy, effective_data_time: Optional[datetime.datetime], current_time: datetime.datetime) -> pendulum.Period:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if freshness_policy.cron_schedule:\n        tick_iterator = cron_string_iterator(start_timestamp=current_time.timestamp(), cron_string=freshness_policy.cron_schedule, execution_timezone=freshness_policy.cron_schedule_timezone)\n        while True:\n            tick = next(tick_iterator)\n            required_data_time = tick - freshness_policy.maximum_lag_delta\n            if effective_data_time is None or effective_data_time < required_data_time:\n                return pendulum.Period(start=required_data_time, end=tick)\n    else:\n        if effective_data_time is None:\n            return pendulum.Period(start=current_time - freshness_policy.maximum_lag_delta, end=current_time)\n        return pendulum.Period(start=effective_data_time + 0.9 * freshness_policy.maximum_lag_delta, end=max(effective_data_time + freshness_policy.maximum_lag_delta, current_time))",
            "def get_execution_period_for_policy(freshness_policy: FreshnessPolicy, effective_data_time: Optional[datetime.datetime], current_time: datetime.datetime) -> pendulum.Period:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if freshness_policy.cron_schedule:\n        tick_iterator = cron_string_iterator(start_timestamp=current_time.timestamp(), cron_string=freshness_policy.cron_schedule, execution_timezone=freshness_policy.cron_schedule_timezone)\n        while True:\n            tick = next(tick_iterator)\n            required_data_time = tick - freshness_policy.maximum_lag_delta\n            if effective_data_time is None or effective_data_time < required_data_time:\n                return pendulum.Period(start=required_data_time, end=tick)\n    else:\n        if effective_data_time is None:\n            return pendulum.Period(start=current_time - freshness_policy.maximum_lag_delta, end=current_time)\n        return pendulum.Period(start=effective_data_time + 0.9 * freshness_policy.maximum_lag_delta, end=max(effective_data_time + freshness_policy.maximum_lag_delta, current_time))",
            "def get_execution_period_for_policy(freshness_policy: FreshnessPolicy, effective_data_time: Optional[datetime.datetime], current_time: datetime.datetime) -> pendulum.Period:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if freshness_policy.cron_schedule:\n        tick_iterator = cron_string_iterator(start_timestamp=current_time.timestamp(), cron_string=freshness_policy.cron_schedule, execution_timezone=freshness_policy.cron_schedule_timezone)\n        while True:\n            tick = next(tick_iterator)\n            required_data_time = tick - freshness_policy.maximum_lag_delta\n            if effective_data_time is None or effective_data_time < required_data_time:\n                return pendulum.Period(start=required_data_time, end=tick)\n    else:\n        if effective_data_time is None:\n            return pendulum.Period(start=current_time - freshness_policy.maximum_lag_delta, end=current_time)\n        return pendulum.Period(start=effective_data_time + 0.9 * freshness_policy.maximum_lag_delta, end=max(effective_data_time + freshness_policy.maximum_lag_delta, current_time))",
            "def get_execution_period_for_policy(freshness_policy: FreshnessPolicy, effective_data_time: Optional[datetime.datetime], current_time: datetime.datetime) -> pendulum.Period:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if freshness_policy.cron_schedule:\n        tick_iterator = cron_string_iterator(start_timestamp=current_time.timestamp(), cron_string=freshness_policy.cron_schedule, execution_timezone=freshness_policy.cron_schedule_timezone)\n        while True:\n            tick = next(tick_iterator)\n            required_data_time = tick - freshness_policy.maximum_lag_delta\n            if effective_data_time is None or effective_data_time < required_data_time:\n                return pendulum.Period(start=required_data_time, end=tick)\n    else:\n        if effective_data_time is None:\n            return pendulum.Period(start=current_time - freshness_policy.maximum_lag_delta, end=current_time)\n        return pendulum.Period(start=effective_data_time + 0.9 * freshness_policy.maximum_lag_delta, end=max(effective_data_time + freshness_policy.maximum_lag_delta, current_time))"
        ]
    },
    {
        "func_name": "get_execution_period_and_evaluation_data_for_policies",
        "original": "def get_execution_period_and_evaluation_data_for_policies(local_policy: Optional[FreshnessPolicy], policies: AbstractSet[FreshnessPolicy], effective_data_time: Optional[datetime.datetime], current_time: datetime.datetime) -> Tuple[Optional[pendulum.Period], Optional['TextRuleEvaluationData']]:\n    \"\"\"Determines a range of times for which you can kick off an execution of this asset to solve\n    the most pressing constraint, alongside a maximum number of additional constraints.\n    \"\"\"\n    from .auto_materialize_rule_evaluation import TextRuleEvaluationData\n    merged_period = None\n    contains_local = False\n    contains_downstream = False\n    for (period, policy) in sorted(((get_execution_period_for_policy(policy, effective_data_time, current_time), policy) for policy in policies), key=lambda pp: pp[0].end):\n        if merged_period is None:\n            merged_period = period\n        elif period.start <= merged_period.end:\n            merged_period = pendulum.Period(start=max(period.start, merged_period.start), end=period.end)\n        else:\n            break\n        if policy == local_policy:\n            contains_local = True\n        else:\n            contains_downstream = True\n    if not contains_local and (not contains_downstream):\n        evaluation_data = None\n    elif not contains_local:\n        evaluation_data = TextRuleEvaluationData(\"Required by downstream asset's policy\")\n    elif not contains_downstream:\n        evaluation_data = TextRuleEvaluationData(\"Required by this asset's policy\")\n    else:\n        evaluation_data = TextRuleEvaluationData(\"Required by this asset's policy and downstream asset's policy\")\n    return (merged_period, evaluation_data)",
        "mutated": [
            "def get_execution_period_and_evaluation_data_for_policies(local_policy: Optional[FreshnessPolicy], policies: AbstractSet[FreshnessPolicy], effective_data_time: Optional[datetime.datetime], current_time: datetime.datetime) -> Tuple[Optional[pendulum.Period], Optional['TextRuleEvaluationData']]:\n    if False:\n        i = 10\n    'Determines a range of times for which you can kick off an execution of this asset to solve\\n    the most pressing constraint, alongside a maximum number of additional constraints.\\n    '\n    from .auto_materialize_rule_evaluation import TextRuleEvaluationData\n    merged_period = None\n    contains_local = False\n    contains_downstream = False\n    for (period, policy) in sorted(((get_execution_period_for_policy(policy, effective_data_time, current_time), policy) for policy in policies), key=lambda pp: pp[0].end):\n        if merged_period is None:\n            merged_period = period\n        elif period.start <= merged_period.end:\n            merged_period = pendulum.Period(start=max(period.start, merged_period.start), end=period.end)\n        else:\n            break\n        if policy == local_policy:\n            contains_local = True\n        else:\n            contains_downstream = True\n    if not contains_local and (not contains_downstream):\n        evaluation_data = None\n    elif not contains_local:\n        evaluation_data = TextRuleEvaluationData(\"Required by downstream asset's policy\")\n    elif not contains_downstream:\n        evaluation_data = TextRuleEvaluationData(\"Required by this asset's policy\")\n    else:\n        evaluation_data = TextRuleEvaluationData(\"Required by this asset's policy and downstream asset's policy\")\n    return (merged_period, evaluation_data)",
            "def get_execution_period_and_evaluation_data_for_policies(local_policy: Optional[FreshnessPolicy], policies: AbstractSet[FreshnessPolicy], effective_data_time: Optional[datetime.datetime], current_time: datetime.datetime) -> Tuple[Optional[pendulum.Period], Optional['TextRuleEvaluationData']]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Determines a range of times for which you can kick off an execution of this asset to solve\\n    the most pressing constraint, alongside a maximum number of additional constraints.\\n    '\n    from .auto_materialize_rule_evaluation import TextRuleEvaluationData\n    merged_period = None\n    contains_local = False\n    contains_downstream = False\n    for (period, policy) in sorted(((get_execution_period_for_policy(policy, effective_data_time, current_time), policy) for policy in policies), key=lambda pp: pp[0].end):\n        if merged_period is None:\n            merged_period = period\n        elif period.start <= merged_period.end:\n            merged_period = pendulum.Period(start=max(period.start, merged_period.start), end=period.end)\n        else:\n            break\n        if policy == local_policy:\n            contains_local = True\n        else:\n            contains_downstream = True\n    if not contains_local and (not contains_downstream):\n        evaluation_data = None\n    elif not contains_local:\n        evaluation_data = TextRuleEvaluationData(\"Required by downstream asset's policy\")\n    elif not contains_downstream:\n        evaluation_data = TextRuleEvaluationData(\"Required by this asset's policy\")\n    else:\n        evaluation_data = TextRuleEvaluationData(\"Required by this asset's policy and downstream asset's policy\")\n    return (merged_period, evaluation_data)",
            "def get_execution_period_and_evaluation_data_for_policies(local_policy: Optional[FreshnessPolicy], policies: AbstractSet[FreshnessPolicy], effective_data_time: Optional[datetime.datetime], current_time: datetime.datetime) -> Tuple[Optional[pendulum.Period], Optional['TextRuleEvaluationData']]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Determines a range of times for which you can kick off an execution of this asset to solve\\n    the most pressing constraint, alongside a maximum number of additional constraints.\\n    '\n    from .auto_materialize_rule_evaluation import TextRuleEvaluationData\n    merged_period = None\n    contains_local = False\n    contains_downstream = False\n    for (period, policy) in sorted(((get_execution_period_for_policy(policy, effective_data_time, current_time), policy) for policy in policies), key=lambda pp: pp[0].end):\n        if merged_period is None:\n            merged_period = period\n        elif period.start <= merged_period.end:\n            merged_period = pendulum.Period(start=max(period.start, merged_period.start), end=period.end)\n        else:\n            break\n        if policy == local_policy:\n            contains_local = True\n        else:\n            contains_downstream = True\n    if not contains_local and (not contains_downstream):\n        evaluation_data = None\n    elif not contains_local:\n        evaluation_data = TextRuleEvaluationData(\"Required by downstream asset's policy\")\n    elif not contains_downstream:\n        evaluation_data = TextRuleEvaluationData(\"Required by this asset's policy\")\n    else:\n        evaluation_data = TextRuleEvaluationData(\"Required by this asset's policy and downstream asset's policy\")\n    return (merged_period, evaluation_data)",
            "def get_execution_period_and_evaluation_data_for_policies(local_policy: Optional[FreshnessPolicy], policies: AbstractSet[FreshnessPolicy], effective_data_time: Optional[datetime.datetime], current_time: datetime.datetime) -> Tuple[Optional[pendulum.Period], Optional['TextRuleEvaluationData']]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Determines a range of times for which you can kick off an execution of this asset to solve\\n    the most pressing constraint, alongside a maximum number of additional constraints.\\n    '\n    from .auto_materialize_rule_evaluation import TextRuleEvaluationData\n    merged_period = None\n    contains_local = False\n    contains_downstream = False\n    for (period, policy) in sorted(((get_execution_period_for_policy(policy, effective_data_time, current_time), policy) for policy in policies), key=lambda pp: pp[0].end):\n        if merged_period is None:\n            merged_period = period\n        elif period.start <= merged_period.end:\n            merged_period = pendulum.Period(start=max(period.start, merged_period.start), end=period.end)\n        else:\n            break\n        if policy == local_policy:\n            contains_local = True\n        else:\n            contains_downstream = True\n    if not contains_local and (not contains_downstream):\n        evaluation_data = None\n    elif not contains_local:\n        evaluation_data = TextRuleEvaluationData(\"Required by downstream asset's policy\")\n    elif not contains_downstream:\n        evaluation_data = TextRuleEvaluationData(\"Required by this asset's policy\")\n    else:\n        evaluation_data = TextRuleEvaluationData(\"Required by this asset's policy and downstream asset's policy\")\n    return (merged_period, evaluation_data)",
            "def get_execution_period_and_evaluation_data_for_policies(local_policy: Optional[FreshnessPolicy], policies: AbstractSet[FreshnessPolicy], effective_data_time: Optional[datetime.datetime], current_time: datetime.datetime) -> Tuple[Optional[pendulum.Period], Optional['TextRuleEvaluationData']]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Determines a range of times for which you can kick off an execution of this asset to solve\\n    the most pressing constraint, alongside a maximum number of additional constraints.\\n    '\n    from .auto_materialize_rule_evaluation import TextRuleEvaluationData\n    merged_period = None\n    contains_local = False\n    contains_downstream = False\n    for (period, policy) in sorted(((get_execution_period_for_policy(policy, effective_data_time, current_time), policy) for policy in policies), key=lambda pp: pp[0].end):\n        if merged_period is None:\n            merged_period = period\n        elif period.start <= merged_period.end:\n            merged_period = pendulum.Period(start=max(period.start, merged_period.start), end=period.end)\n        else:\n            break\n        if policy == local_policy:\n            contains_local = True\n        else:\n            contains_downstream = True\n    if not contains_local and (not contains_downstream):\n        evaluation_data = None\n    elif not contains_local:\n        evaluation_data = TextRuleEvaluationData(\"Required by downstream asset's policy\")\n    elif not contains_downstream:\n        evaluation_data = TextRuleEvaluationData(\"Required by this asset's policy\")\n    else:\n        evaluation_data = TextRuleEvaluationData(\"Required by this asset's policy and downstream asset's policy\")\n    return (merged_period, evaluation_data)"
        ]
    },
    {
        "func_name": "get_expected_data_time_for_asset_key",
        "original": "def get_expected_data_time_for_asset_key(asset_graph: AssetGraph, asset_key: AssetKey, will_materialize_mapping: Mapping[AssetKey, AbstractSet[AssetKeyPartitionKey]], expected_data_time_mapping: Mapping[AssetKey, Optional[datetime.datetime]], data_time_resolver: 'CachingDataTimeResolver', current_time: datetime.datetime, will_materialize: bool) -> Optional[datetime.datetime]:\n    \"\"\"Returns the data time that you would expect this asset to have if you were to execute it\n    on this tick.\n    \"\"\"\n    from dagster._core.definitions.external_asset_graph import ExternalAssetGraph\n    if not asset_graph.get_downstream_freshness_policies(asset_key=asset_key):\n        return None\n    elif not will_materialize:\n        return data_time_resolver.get_current_data_time(asset_key, current_time)\n    elif asset_graph.has_non_source_parents(asset_key):\n        expected_data_time = None\n        for parent_key in asset_graph.get_parents(asset_key):\n            if isinstance(asset_graph, ExternalAssetGraph) and AssetKeyPartitionKey(parent_key) in will_materialize_mapping[parent_key]:\n                parent_repo = asset_graph.get_repository_handle(parent_key)\n                if parent_repo != asset_graph.get_repository_handle(asset_key):\n                    return data_time_resolver.get_current_data_time(asset_key, current_time)\n            parent_expected_data_time = expected_data_time_mapping.get(parent_key) or data_time_resolver.get_current_data_time(parent_key, current_time)\n            expected_data_time = min(filter(None, [expected_data_time, parent_expected_data_time]), default=None)\n        return expected_data_time\n    else:\n        return current_time",
        "mutated": [
            "def get_expected_data_time_for_asset_key(asset_graph: AssetGraph, asset_key: AssetKey, will_materialize_mapping: Mapping[AssetKey, AbstractSet[AssetKeyPartitionKey]], expected_data_time_mapping: Mapping[AssetKey, Optional[datetime.datetime]], data_time_resolver: 'CachingDataTimeResolver', current_time: datetime.datetime, will_materialize: bool) -> Optional[datetime.datetime]:\n    if False:\n        i = 10\n    'Returns the data time that you would expect this asset to have if you were to execute it\\n    on this tick.\\n    '\n    from dagster._core.definitions.external_asset_graph import ExternalAssetGraph\n    if not asset_graph.get_downstream_freshness_policies(asset_key=asset_key):\n        return None\n    elif not will_materialize:\n        return data_time_resolver.get_current_data_time(asset_key, current_time)\n    elif asset_graph.has_non_source_parents(asset_key):\n        expected_data_time = None\n        for parent_key in asset_graph.get_parents(asset_key):\n            if isinstance(asset_graph, ExternalAssetGraph) and AssetKeyPartitionKey(parent_key) in will_materialize_mapping[parent_key]:\n                parent_repo = asset_graph.get_repository_handle(parent_key)\n                if parent_repo != asset_graph.get_repository_handle(asset_key):\n                    return data_time_resolver.get_current_data_time(asset_key, current_time)\n            parent_expected_data_time = expected_data_time_mapping.get(parent_key) or data_time_resolver.get_current_data_time(parent_key, current_time)\n            expected_data_time = min(filter(None, [expected_data_time, parent_expected_data_time]), default=None)\n        return expected_data_time\n    else:\n        return current_time",
            "def get_expected_data_time_for_asset_key(asset_graph: AssetGraph, asset_key: AssetKey, will_materialize_mapping: Mapping[AssetKey, AbstractSet[AssetKeyPartitionKey]], expected_data_time_mapping: Mapping[AssetKey, Optional[datetime.datetime]], data_time_resolver: 'CachingDataTimeResolver', current_time: datetime.datetime, will_materialize: bool) -> Optional[datetime.datetime]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns the data time that you would expect this asset to have if you were to execute it\\n    on this tick.\\n    '\n    from dagster._core.definitions.external_asset_graph import ExternalAssetGraph\n    if not asset_graph.get_downstream_freshness_policies(asset_key=asset_key):\n        return None\n    elif not will_materialize:\n        return data_time_resolver.get_current_data_time(asset_key, current_time)\n    elif asset_graph.has_non_source_parents(asset_key):\n        expected_data_time = None\n        for parent_key in asset_graph.get_parents(asset_key):\n            if isinstance(asset_graph, ExternalAssetGraph) and AssetKeyPartitionKey(parent_key) in will_materialize_mapping[parent_key]:\n                parent_repo = asset_graph.get_repository_handle(parent_key)\n                if parent_repo != asset_graph.get_repository_handle(asset_key):\n                    return data_time_resolver.get_current_data_time(asset_key, current_time)\n            parent_expected_data_time = expected_data_time_mapping.get(parent_key) or data_time_resolver.get_current_data_time(parent_key, current_time)\n            expected_data_time = min(filter(None, [expected_data_time, parent_expected_data_time]), default=None)\n        return expected_data_time\n    else:\n        return current_time",
            "def get_expected_data_time_for_asset_key(asset_graph: AssetGraph, asset_key: AssetKey, will_materialize_mapping: Mapping[AssetKey, AbstractSet[AssetKeyPartitionKey]], expected_data_time_mapping: Mapping[AssetKey, Optional[datetime.datetime]], data_time_resolver: 'CachingDataTimeResolver', current_time: datetime.datetime, will_materialize: bool) -> Optional[datetime.datetime]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns the data time that you would expect this asset to have if you were to execute it\\n    on this tick.\\n    '\n    from dagster._core.definitions.external_asset_graph import ExternalAssetGraph\n    if not asset_graph.get_downstream_freshness_policies(asset_key=asset_key):\n        return None\n    elif not will_materialize:\n        return data_time_resolver.get_current_data_time(asset_key, current_time)\n    elif asset_graph.has_non_source_parents(asset_key):\n        expected_data_time = None\n        for parent_key in asset_graph.get_parents(asset_key):\n            if isinstance(asset_graph, ExternalAssetGraph) and AssetKeyPartitionKey(parent_key) in will_materialize_mapping[parent_key]:\n                parent_repo = asset_graph.get_repository_handle(parent_key)\n                if parent_repo != asset_graph.get_repository_handle(asset_key):\n                    return data_time_resolver.get_current_data_time(asset_key, current_time)\n            parent_expected_data_time = expected_data_time_mapping.get(parent_key) or data_time_resolver.get_current_data_time(parent_key, current_time)\n            expected_data_time = min(filter(None, [expected_data_time, parent_expected_data_time]), default=None)\n        return expected_data_time\n    else:\n        return current_time",
            "def get_expected_data_time_for_asset_key(asset_graph: AssetGraph, asset_key: AssetKey, will_materialize_mapping: Mapping[AssetKey, AbstractSet[AssetKeyPartitionKey]], expected_data_time_mapping: Mapping[AssetKey, Optional[datetime.datetime]], data_time_resolver: 'CachingDataTimeResolver', current_time: datetime.datetime, will_materialize: bool) -> Optional[datetime.datetime]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns the data time that you would expect this asset to have if you were to execute it\\n    on this tick.\\n    '\n    from dagster._core.definitions.external_asset_graph import ExternalAssetGraph\n    if not asset_graph.get_downstream_freshness_policies(asset_key=asset_key):\n        return None\n    elif not will_materialize:\n        return data_time_resolver.get_current_data_time(asset_key, current_time)\n    elif asset_graph.has_non_source_parents(asset_key):\n        expected_data_time = None\n        for parent_key in asset_graph.get_parents(asset_key):\n            if isinstance(asset_graph, ExternalAssetGraph) and AssetKeyPartitionKey(parent_key) in will_materialize_mapping[parent_key]:\n                parent_repo = asset_graph.get_repository_handle(parent_key)\n                if parent_repo != asset_graph.get_repository_handle(asset_key):\n                    return data_time_resolver.get_current_data_time(asset_key, current_time)\n            parent_expected_data_time = expected_data_time_mapping.get(parent_key) or data_time_resolver.get_current_data_time(parent_key, current_time)\n            expected_data_time = min(filter(None, [expected_data_time, parent_expected_data_time]), default=None)\n        return expected_data_time\n    else:\n        return current_time",
            "def get_expected_data_time_for_asset_key(asset_graph: AssetGraph, asset_key: AssetKey, will_materialize_mapping: Mapping[AssetKey, AbstractSet[AssetKeyPartitionKey]], expected_data_time_mapping: Mapping[AssetKey, Optional[datetime.datetime]], data_time_resolver: 'CachingDataTimeResolver', current_time: datetime.datetime, will_materialize: bool) -> Optional[datetime.datetime]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns the data time that you would expect this asset to have if you were to execute it\\n    on this tick.\\n    '\n    from dagster._core.definitions.external_asset_graph import ExternalAssetGraph\n    if not asset_graph.get_downstream_freshness_policies(asset_key=asset_key):\n        return None\n    elif not will_materialize:\n        return data_time_resolver.get_current_data_time(asset_key, current_time)\n    elif asset_graph.has_non_source_parents(asset_key):\n        expected_data_time = None\n        for parent_key in asset_graph.get_parents(asset_key):\n            if isinstance(asset_graph, ExternalAssetGraph) and AssetKeyPartitionKey(parent_key) in will_materialize_mapping[parent_key]:\n                parent_repo = asset_graph.get_repository_handle(parent_key)\n                if parent_repo != asset_graph.get_repository_handle(asset_key):\n                    return data_time_resolver.get_current_data_time(asset_key, current_time)\n            parent_expected_data_time = expected_data_time_mapping.get(parent_key) or data_time_resolver.get_current_data_time(parent_key, current_time)\n            expected_data_time = min(filter(None, [expected_data_time, parent_expected_data_time]), default=None)\n        return expected_data_time\n    else:\n        return current_time"
        ]
    },
    {
        "func_name": "freshness_evaluation_results_for_asset_key",
        "original": "def freshness_evaluation_results_for_asset_key(asset_key: AssetKey, data_time_resolver: 'CachingDataTimeResolver', asset_graph: AssetGraph, current_time: datetime.datetime, will_materialize_mapping: Mapping[AssetKey, AbstractSet[AssetKeyPartitionKey]], expected_data_time_mapping: Mapping[AssetKey, Optional[datetime.datetime]]) -> 'RuleEvaluationResults':\n    \"\"\"Returns a set of AssetKeyPartitionKeys to materialize in order to abide by the given\n    FreshnessPolicies.\n\n    Attempts to minimize the total number of asset executions.\n    \"\"\"\n    if not asset_graph.get_downstream_freshness_policies(asset_key=asset_key) or asset_graph.is_partitioned(asset_key):\n        return []\n    current_data_time = data_time_resolver.get_current_data_time(asset_key, current_time)\n    expected_data_time = get_expected_data_time_for_asset_key(asset_graph=asset_graph, asset_key=asset_key, will_materialize_mapping=will_materialize_mapping, expected_data_time_mapping=expected_data_time_mapping, data_time_resolver=data_time_resolver, current_time=current_time, will_materialize=True)\n    if current_data_time == expected_data_time:\n        return []\n    in_progress_data_time = data_time_resolver.get_in_progress_data_time(asset_key, current_time)\n    failed_data_time = data_time_resolver.get_ignored_failure_data_time(asset_key, current_time)\n    effective_data_time = max(filter(None, (current_data_time, in_progress_data_time, failed_data_time)), default=None)\n    (execution_period, evaluation_data) = get_execution_period_and_evaluation_data_for_policies(local_policy=asset_graph.freshness_policies_by_key.get(asset_key), policies=asset_graph.get_downstream_freshness_policies(asset_key=asset_key), effective_data_time=effective_data_time, current_time=current_time)\n    asset_partition = AssetKeyPartitionKey(asset_key, None)\n    if execution_period is not None and execution_period.start <= current_time and (expected_data_time is not None) and (expected_data_time >= execution_period.start) and (evaluation_data is not None):\n        return [(evaluation_data, {asset_partition})]\n    else:\n        return []",
        "mutated": [
            "def freshness_evaluation_results_for_asset_key(asset_key: AssetKey, data_time_resolver: 'CachingDataTimeResolver', asset_graph: AssetGraph, current_time: datetime.datetime, will_materialize_mapping: Mapping[AssetKey, AbstractSet[AssetKeyPartitionKey]], expected_data_time_mapping: Mapping[AssetKey, Optional[datetime.datetime]]) -> 'RuleEvaluationResults':\n    if False:\n        i = 10\n    'Returns a set of AssetKeyPartitionKeys to materialize in order to abide by the given\\n    FreshnessPolicies.\\n\\n    Attempts to minimize the total number of asset executions.\\n    '\n    if not asset_graph.get_downstream_freshness_policies(asset_key=asset_key) or asset_graph.is_partitioned(asset_key):\n        return []\n    current_data_time = data_time_resolver.get_current_data_time(asset_key, current_time)\n    expected_data_time = get_expected_data_time_for_asset_key(asset_graph=asset_graph, asset_key=asset_key, will_materialize_mapping=will_materialize_mapping, expected_data_time_mapping=expected_data_time_mapping, data_time_resolver=data_time_resolver, current_time=current_time, will_materialize=True)\n    if current_data_time == expected_data_time:\n        return []\n    in_progress_data_time = data_time_resolver.get_in_progress_data_time(asset_key, current_time)\n    failed_data_time = data_time_resolver.get_ignored_failure_data_time(asset_key, current_time)\n    effective_data_time = max(filter(None, (current_data_time, in_progress_data_time, failed_data_time)), default=None)\n    (execution_period, evaluation_data) = get_execution_period_and_evaluation_data_for_policies(local_policy=asset_graph.freshness_policies_by_key.get(asset_key), policies=asset_graph.get_downstream_freshness_policies(asset_key=asset_key), effective_data_time=effective_data_time, current_time=current_time)\n    asset_partition = AssetKeyPartitionKey(asset_key, None)\n    if execution_period is not None and execution_period.start <= current_time and (expected_data_time is not None) and (expected_data_time >= execution_period.start) and (evaluation_data is not None):\n        return [(evaluation_data, {asset_partition})]\n    else:\n        return []",
            "def freshness_evaluation_results_for_asset_key(asset_key: AssetKey, data_time_resolver: 'CachingDataTimeResolver', asset_graph: AssetGraph, current_time: datetime.datetime, will_materialize_mapping: Mapping[AssetKey, AbstractSet[AssetKeyPartitionKey]], expected_data_time_mapping: Mapping[AssetKey, Optional[datetime.datetime]]) -> 'RuleEvaluationResults':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns a set of AssetKeyPartitionKeys to materialize in order to abide by the given\\n    FreshnessPolicies.\\n\\n    Attempts to minimize the total number of asset executions.\\n    '\n    if not asset_graph.get_downstream_freshness_policies(asset_key=asset_key) or asset_graph.is_partitioned(asset_key):\n        return []\n    current_data_time = data_time_resolver.get_current_data_time(asset_key, current_time)\n    expected_data_time = get_expected_data_time_for_asset_key(asset_graph=asset_graph, asset_key=asset_key, will_materialize_mapping=will_materialize_mapping, expected_data_time_mapping=expected_data_time_mapping, data_time_resolver=data_time_resolver, current_time=current_time, will_materialize=True)\n    if current_data_time == expected_data_time:\n        return []\n    in_progress_data_time = data_time_resolver.get_in_progress_data_time(asset_key, current_time)\n    failed_data_time = data_time_resolver.get_ignored_failure_data_time(asset_key, current_time)\n    effective_data_time = max(filter(None, (current_data_time, in_progress_data_time, failed_data_time)), default=None)\n    (execution_period, evaluation_data) = get_execution_period_and_evaluation_data_for_policies(local_policy=asset_graph.freshness_policies_by_key.get(asset_key), policies=asset_graph.get_downstream_freshness_policies(asset_key=asset_key), effective_data_time=effective_data_time, current_time=current_time)\n    asset_partition = AssetKeyPartitionKey(asset_key, None)\n    if execution_period is not None and execution_period.start <= current_time and (expected_data_time is not None) and (expected_data_time >= execution_period.start) and (evaluation_data is not None):\n        return [(evaluation_data, {asset_partition})]\n    else:\n        return []",
            "def freshness_evaluation_results_for_asset_key(asset_key: AssetKey, data_time_resolver: 'CachingDataTimeResolver', asset_graph: AssetGraph, current_time: datetime.datetime, will_materialize_mapping: Mapping[AssetKey, AbstractSet[AssetKeyPartitionKey]], expected_data_time_mapping: Mapping[AssetKey, Optional[datetime.datetime]]) -> 'RuleEvaluationResults':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns a set of AssetKeyPartitionKeys to materialize in order to abide by the given\\n    FreshnessPolicies.\\n\\n    Attempts to minimize the total number of asset executions.\\n    '\n    if not asset_graph.get_downstream_freshness_policies(asset_key=asset_key) or asset_graph.is_partitioned(asset_key):\n        return []\n    current_data_time = data_time_resolver.get_current_data_time(asset_key, current_time)\n    expected_data_time = get_expected_data_time_for_asset_key(asset_graph=asset_graph, asset_key=asset_key, will_materialize_mapping=will_materialize_mapping, expected_data_time_mapping=expected_data_time_mapping, data_time_resolver=data_time_resolver, current_time=current_time, will_materialize=True)\n    if current_data_time == expected_data_time:\n        return []\n    in_progress_data_time = data_time_resolver.get_in_progress_data_time(asset_key, current_time)\n    failed_data_time = data_time_resolver.get_ignored_failure_data_time(asset_key, current_time)\n    effective_data_time = max(filter(None, (current_data_time, in_progress_data_time, failed_data_time)), default=None)\n    (execution_period, evaluation_data) = get_execution_period_and_evaluation_data_for_policies(local_policy=asset_graph.freshness_policies_by_key.get(asset_key), policies=asset_graph.get_downstream_freshness_policies(asset_key=asset_key), effective_data_time=effective_data_time, current_time=current_time)\n    asset_partition = AssetKeyPartitionKey(asset_key, None)\n    if execution_period is not None and execution_period.start <= current_time and (expected_data_time is not None) and (expected_data_time >= execution_period.start) and (evaluation_data is not None):\n        return [(evaluation_data, {asset_partition})]\n    else:\n        return []",
            "def freshness_evaluation_results_for_asset_key(asset_key: AssetKey, data_time_resolver: 'CachingDataTimeResolver', asset_graph: AssetGraph, current_time: datetime.datetime, will_materialize_mapping: Mapping[AssetKey, AbstractSet[AssetKeyPartitionKey]], expected_data_time_mapping: Mapping[AssetKey, Optional[datetime.datetime]]) -> 'RuleEvaluationResults':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns a set of AssetKeyPartitionKeys to materialize in order to abide by the given\\n    FreshnessPolicies.\\n\\n    Attempts to minimize the total number of asset executions.\\n    '\n    if not asset_graph.get_downstream_freshness_policies(asset_key=asset_key) or asset_graph.is_partitioned(asset_key):\n        return []\n    current_data_time = data_time_resolver.get_current_data_time(asset_key, current_time)\n    expected_data_time = get_expected_data_time_for_asset_key(asset_graph=asset_graph, asset_key=asset_key, will_materialize_mapping=will_materialize_mapping, expected_data_time_mapping=expected_data_time_mapping, data_time_resolver=data_time_resolver, current_time=current_time, will_materialize=True)\n    if current_data_time == expected_data_time:\n        return []\n    in_progress_data_time = data_time_resolver.get_in_progress_data_time(asset_key, current_time)\n    failed_data_time = data_time_resolver.get_ignored_failure_data_time(asset_key, current_time)\n    effective_data_time = max(filter(None, (current_data_time, in_progress_data_time, failed_data_time)), default=None)\n    (execution_period, evaluation_data) = get_execution_period_and_evaluation_data_for_policies(local_policy=asset_graph.freshness_policies_by_key.get(asset_key), policies=asset_graph.get_downstream_freshness_policies(asset_key=asset_key), effective_data_time=effective_data_time, current_time=current_time)\n    asset_partition = AssetKeyPartitionKey(asset_key, None)\n    if execution_period is not None and execution_period.start <= current_time and (expected_data_time is not None) and (expected_data_time >= execution_period.start) and (evaluation_data is not None):\n        return [(evaluation_data, {asset_partition})]\n    else:\n        return []",
            "def freshness_evaluation_results_for_asset_key(asset_key: AssetKey, data_time_resolver: 'CachingDataTimeResolver', asset_graph: AssetGraph, current_time: datetime.datetime, will_materialize_mapping: Mapping[AssetKey, AbstractSet[AssetKeyPartitionKey]], expected_data_time_mapping: Mapping[AssetKey, Optional[datetime.datetime]]) -> 'RuleEvaluationResults':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns a set of AssetKeyPartitionKeys to materialize in order to abide by the given\\n    FreshnessPolicies.\\n\\n    Attempts to minimize the total number of asset executions.\\n    '\n    if not asset_graph.get_downstream_freshness_policies(asset_key=asset_key) or asset_graph.is_partitioned(asset_key):\n        return []\n    current_data_time = data_time_resolver.get_current_data_time(asset_key, current_time)\n    expected_data_time = get_expected_data_time_for_asset_key(asset_graph=asset_graph, asset_key=asset_key, will_materialize_mapping=will_materialize_mapping, expected_data_time_mapping=expected_data_time_mapping, data_time_resolver=data_time_resolver, current_time=current_time, will_materialize=True)\n    if current_data_time == expected_data_time:\n        return []\n    in_progress_data_time = data_time_resolver.get_in_progress_data_time(asset_key, current_time)\n    failed_data_time = data_time_resolver.get_ignored_failure_data_time(asset_key, current_time)\n    effective_data_time = max(filter(None, (current_data_time, in_progress_data_time, failed_data_time)), default=None)\n    (execution_period, evaluation_data) = get_execution_period_and_evaluation_data_for_policies(local_policy=asset_graph.freshness_policies_by_key.get(asset_key), policies=asset_graph.get_downstream_freshness_policies(asset_key=asset_key), effective_data_time=effective_data_time, current_time=current_time)\n    asset_partition = AssetKeyPartitionKey(asset_key, None)\n    if execution_period is not None and execution_period.start <= current_time and (expected_data_time is not None) and (expected_data_time >= execution_period.start) and (evaluation_data is not None):\n        return [(evaluation_data, {asset_partition})]\n    else:\n        return []"
        ]
    }
]