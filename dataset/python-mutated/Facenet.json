[
    {
        "func_name": "scaling",
        "original": "def scaling(x, scale):\n    return x * scale",
        "mutated": [
            "def scaling(x, scale):\n    if False:\n        i = 10\n    return x * scale",
            "def scaling(x, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return x * scale",
            "def scaling(x, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return x * scale",
            "def scaling(x, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return x * scale",
            "def scaling(x, scale):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return x * scale"
        ]
    },
    {
        "func_name": "InceptionResNetV2",
        "original": "def InceptionResNetV2(dimension=128):\n    inputs = Input(shape=(160, 160, 3))\n    x = Conv2D(32, 3, strides=2, padding='valid', use_bias=False, name='Conv2d_1a_3x3')(inputs)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_1a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_1a_3x3_Activation')(x)\n    x = Conv2D(32, 3, strides=1, padding='valid', use_bias=False, name='Conv2d_2a_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_2a_3x3_Activation')(x)\n    x = Conv2D(64, 3, strides=1, padding='same', use_bias=False, name='Conv2d_2b_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2b_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_2b_3x3_Activation')(x)\n    x = MaxPooling2D(3, strides=2, name='MaxPool_3a_3x3')(x)\n    x = Conv2D(80, 1, strides=1, padding='valid', use_bias=False, name='Conv2d_3b_1x1')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_3b_1x1_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_3b_1x1_Activation')(x)\n    x = Conv2D(192, 3, strides=1, padding='valid', use_bias=False, name='Conv2d_4a_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_4a_3x3_Activation')(x)\n    x = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Conv2d_4b_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4b_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_4b_3x3_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_1_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_1_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_2_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_2_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_3_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_3_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_4_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_4_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_5_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_5_Activation')(x)\n    branch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name='Mixed_6a_Branch_0_Conv2d_1a_3x3')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_6a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, 3, strides=1, padding='same', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_1a_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n    branch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_6a_Branch_2_MaxPool_1a_3x3')(x)\n    branches = [branch_0, branch_1, branch_pool]\n    x = Concatenate(axis=3, name='Mixed_6a')(branches)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_1_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_1_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_2_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_2_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_3_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_3_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_4_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_4_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_5_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_5_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_6_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_6_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_6_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_6_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_7_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_7_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_7_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_7_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_7_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_8_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_8_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_8_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_8_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_8_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_9_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_9_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_9_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_9_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_9_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_10_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_10_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_10_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_10_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_10_Activation')(x)\n    branch_0 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_0_Conv2d_0a_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_0a_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_0a_1x1_Activation')(branch_0)\n    branch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_0_Conv2d_1a_3x3')(branch_0)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n    branch_1 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_1_Conv2d_1a_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(256, 3, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_1a_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_1a_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_1a_3x3_Activation')(branch_2)\n    branch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_7a_Branch_3_MaxPool_1a_3x3')(x)\n    branches = [branch_0, branch_1, branch_2, branch_pool]\n    x = Concatenate(axis=3, name='Mixed_7a')(branches)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_1_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_1_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_2_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_2_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_3_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_3_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_4_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_4_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_5_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_5_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_6_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_6_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_6_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 1})(up)\n    x = add([x, up])\n    x = GlobalAveragePooling2D(name='AvgPool')(x)\n    x = Dropout(1.0 - 0.8, name='Dropout')(x)\n    x = Dense(dimension, use_bias=False, name='Bottleneck')(x)\n    x = BatchNormalization(momentum=0.995, epsilon=0.001, scale=False, name='Bottleneck_BatchNorm')(x)\n    model = Model(inputs, x, name='inception_resnet_v1')\n    return model",
        "mutated": [
            "def InceptionResNetV2(dimension=128):\n    if False:\n        i = 10\n    inputs = Input(shape=(160, 160, 3))\n    x = Conv2D(32, 3, strides=2, padding='valid', use_bias=False, name='Conv2d_1a_3x3')(inputs)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_1a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_1a_3x3_Activation')(x)\n    x = Conv2D(32, 3, strides=1, padding='valid', use_bias=False, name='Conv2d_2a_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_2a_3x3_Activation')(x)\n    x = Conv2D(64, 3, strides=1, padding='same', use_bias=False, name='Conv2d_2b_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2b_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_2b_3x3_Activation')(x)\n    x = MaxPooling2D(3, strides=2, name='MaxPool_3a_3x3')(x)\n    x = Conv2D(80, 1, strides=1, padding='valid', use_bias=False, name='Conv2d_3b_1x1')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_3b_1x1_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_3b_1x1_Activation')(x)\n    x = Conv2D(192, 3, strides=1, padding='valid', use_bias=False, name='Conv2d_4a_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_4a_3x3_Activation')(x)\n    x = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Conv2d_4b_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4b_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_4b_3x3_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_1_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_1_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_2_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_2_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_3_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_3_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_4_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_4_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_5_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_5_Activation')(x)\n    branch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name='Mixed_6a_Branch_0_Conv2d_1a_3x3')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_6a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, 3, strides=1, padding='same', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_1a_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n    branch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_6a_Branch_2_MaxPool_1a_3x3')(x)\n    branches = [branch_0, branch_1, branch_pool]\n    x = Concatenate(axis=3, name='Mixed_6a')(branches)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_1_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_1_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_2_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_2_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_3_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_3_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_4_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_4_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_5_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_5_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_6_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_6_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_6_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_6_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_7_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_7_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_7_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_7_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_7_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_8_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_8_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_8_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_8_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_8_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_9_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_9_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_9_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_9_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_9_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_10_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_10_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_10_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_10_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_10_Activation')(x)\n    branch_0 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_0_Conv2d_0a_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_0a_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_0a_1x1_Activation')(branch_0)\n    branch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_0_Conv2d_1a_3x3')(branch_0)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n    branch_1 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_1_Conv2d_1a_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(256, 3, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_1a_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_1a_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_1a_3x3_Activation')(branch_2)\n    branch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_7a_Branch_3_MaxPool_1a_3x3')(x)\n    branches = [branch_0, branch_1, branch_2, branch_pool]\n    x = Concatenate(axis=3, name='Mixed_7a')(branches)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_1_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_1_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_2_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_2_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_3_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_3_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_4_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_4_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_5_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_5_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_6_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_6_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_6_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 1})(up)\n    x = add([x, up])\n    x = GlobalAveragePooling2D(name='AvgPool')(x)\n    x = Dropout(1.0 - 0.8, name='Dropout')(x)\n    x = Dense(dimension, use_bias=False, name='Bottleneck')(x)\n    x = BatchNormalization(momentum=0.995, epsilon=0.001, scale=False, name='Bottleneck_BatchNorm')(x)\n    model = Model(inputs, x, name='inception_resnet_v1')\n    return model",
            "def InceptionResNetV2(dimension=128):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    inputs = Input(shape=(160, 160, 3))\n    x = Conv2D(32, 3, strides=2, padding='valid', use_bias=False, name='Conv2d_1a_3x3')(inputs)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_1a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_1a_3x3_Activation')(x)\n    x = Conv2D(32, 3, strides=1, padding='valid', use_bias=False, name='Conv2d_2a_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_2a_3x3_Activation')(x)\n    x = Conv2D(64, 3, strides=1, padding='same', use_bias=False, name='Conv2d_2b_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2b_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_2b_3x3_Activation')(x)\n    x = MaxPooling2D(3, strides=2, name='MaxPool_3a_3x3')(x)\n    x = Conv2D(80, 1, strides=1, padding='valid', use_bias=False, name='Conv2d_3b_1x1')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_3b_1x1_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_3b_1x1_Activation')(x)\n    x = Conv2D(192, 3, strides=1, padding='valid', use_bias=False, name='Conv2d_4a_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_4a_3x3_Activation')(x)\n    x = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Conv2d_4b_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4b_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_4b_3x3_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_1_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_1_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_2_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_2_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_3_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_3_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_4_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_4_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_5_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_5_Activation')(x)\n    branch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name='Mixed_6a_Branch_0_Conv2d_1a_3x3')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_6a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, 3, strides=1, padding='same', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_1a_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n    branch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_6a_Branch_2_MaxPool_1a_3x3')(x)\n    branches = [branch_0, branch_1, branch_pool]\n    x = Concatenate(axis=3, name='Mixed_6a')(branches)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_1_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_1_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_2_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_2_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_3_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_3_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_4_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_4_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_5_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_5_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_6_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_6_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_6_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_6_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_7_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_7_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_7_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_7_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_7_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_8_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_8_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_8_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_8_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_8_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_9_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_9_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_9_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_9_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_9_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_10_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_10_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_10_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_10_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_10_Activation')(x)\n    branch_0 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_0_Conv2d_0a_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_0a_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_0a_1x1_Activation')(branch_0)\n    branch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_0_Conv2d_1a_3x3')(branch_0)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n    branch_1 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_1_Conv2d_1a_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(256, 3, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_1a_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_1a_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_1a_3x3_Activation')(branch_2)\n    branch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_7a_Branch_3_MaxPool_1a_3x3')(x)\n    branches = [branch_0, branch_1, branch_2, branch_pool]\n    x = Concatenate(axis=3, name='Mixed_7a')(branches)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_1_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_1_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_2_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_2_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_3_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_3_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_4_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_4_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_5_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_5_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_6_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_6_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_6_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 1})(up)\n    x = add([x, up])\n    x = GlobalAveragePooling2D(name='AvgPool')(x)\n    x = Dropout(1.0 - 0.8, name='Dropout')(x)\n    x = Dense(dimension, use_bias=False, name='Bottleneck')(x)\n    x = BatchNormalization(momentum=0.995, epsilon=0.001, scale=False, name='Bottleneck_BatchNorm')(x)\n    model = Model(inputs, x, name='inception_resnet_v1')\n    return model",
            "def InceptionResNetV2(dimension=128):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    inputs = Input(shape=(160, 160, 3))\n    x = Conv2D(32, 3, strides=2, padding='valid', use_bias=False, name='Conv2d_1a_3x3')(inputs)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_1a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_1a_3x3_Activation')(x)\n    x = Conv2D(32, 3, strides=1, padding='valid', use_bias=False, name='Conv2d_2a_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_2a_3x3_Activation')(x)\n    x = Conv2D(64, 3, strides=1, padding='same', use_bias=False, name='Conv2d_2b_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2b_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_2b_3x3_Activation')(x)\n    x = MaxPooling2D(3, strides=2, name='MaxPool_3a_3x3')(x)\n    x = Conv2D(80, 1, strides=1, padding='valid', use_bias=False, name='Conv2d_3b_1x1')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_3b_1x1_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_3b_1x1_Activation')(x)\n    x = Conv2D(192, 3, strides=1, padding='valid', use_bias=False, name='Conv2d_4a_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_4a_3x3_Activation')(x)\n    x = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Conv2d_4b_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4b_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_4b_3x3_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_1_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_1_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_2_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_2_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_3_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_3_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_4_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_4_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_5_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_5_Activation')(x)\n    branch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name='Mixed_6a_Branch_0_Conv2d_1a_3x3')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_6a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, 3, strides=1, padding='same', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_1a_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n    branch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_6a_Branch_2_MaxPool_1a_3x3')(x)\n    branches = [branch_0, branch_1, branch_pool]\n    x = Concatenate(axis=3, name='Mixed_6a')(branches)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_1_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_1_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_2_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_2_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_3_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_3_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_4_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_4_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_5_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_5_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_6_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_6_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_6_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_6_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_7_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_7_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_7_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_7_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_7_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_8_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_8_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_8_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_8_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_8_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_9_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_9_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_9_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_9_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_9_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_10_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_10_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_10_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_10_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_10_Activation')(x)\n    branch_0 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_0_Conv2d_0a_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_0a_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_0a_1x1_Activation')(branch_0)\n    branch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_0_Conv2d_1a_3x3')(branch_0)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n    branch_1 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_1_Conv2d_1a_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(256, 3, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_1a_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_1a_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_1a_3x3_Activation')(branch_2)\n    branch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_7a_Branch_3_MaxPool_1a_3x3')(x)\n    branches = [branch_0, branch_1, branch_2, branch_pool]\n    x = Concatenate(axis=3, name='Mixed_7a')(branches)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_1_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_1_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_2_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_2_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_3_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_3_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_4_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_4_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_5_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_5_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_6_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_6_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_6_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 1})(up)\n    x = add([x, up])\n    x = GlobalAveragePooling2D(name='AvgPool')(x)\n    x = Dropout(1.0 - 0.8, name='Dropout')(x)\n    x = Dense(dimension, use_bias=False, name='Bottleneck')(x)\n    x = BatchNormalization(momentum=0.995, epsilon=0.001, scale=False, name='Bottleneck_BatchNorm')(x)\n    model = Model(inputs, x, name='inception_resnet_v1')\n    return model",
            "def InceptionResNetV2(dimension=128):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    inputs = Input(shape=(160, 160, 3))\n    x = Conv2D(32, 3, strides=2, padding='valid', use_bias=False, name='Conv2d_1a_3x3')(inputs)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_1a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_1a_3x3_Activation')(x)\n    x = Conv2D(32, 3, strides=1, padding='valid', use_bias=False, name='Conv2d_2a_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_2a_3x3_Activation')(x)\n    x = Conv2D(64, 3, strides=1, padding='same', use_bias=False, name='Conv2d_2b_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2b_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_2b_3x3_Activation')(x)\n    x = MaxPooling2D(3, strides=2, name='MaxPool_3a_3x3')(x)\n    x = Conv2D(80, 1, strides=1, padding='valid', use_bias=False, name='Conv2d_3b_1x1')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_3b_1x1_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_3b_1x1_Activation')(x)\n    x = Conv2D(192, 3, strides=1, padding='valid', use_bias=False, name='Conv2d_4a_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_4a_3x3_Activation')(x)\n    x = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Conv2d_4b_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4b_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_4b_3x3_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_1_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_1_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_2_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_2_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_3_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_3_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_4_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_4_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_5_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_5_Activation')(x)\n    branch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name='Mixed_6a_Branch_0_Conv2d_1a_3x3')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_6a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, 3, strides=1, padding='same', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_1a_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n    branch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_6a_Branch_2_MaxPool_1a_3x3')(x)\n    branches = [branch_0, branch_1, branch_pool]\n    x = Concatenate(axis=3, name='Mixed_6a')(branches)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_1_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_1_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_2_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_2_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_3_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_3_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_4_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_4_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_5_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_5_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_6_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_6_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_6_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_6_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_7_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_7_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_7_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_7_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_7_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_8_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_8_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_8_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_8_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_8_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_9_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_9_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_9_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_9_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_9_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_10_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_10_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_10_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_10_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_10_Activation')(x)\n    branch_0 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_0_Conv2d_0a_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_0a_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_0a_1x1_Activation')(branch_0)\n    branch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_0_Conv2d_1a_3x3')(branch_0)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n    branch_1 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_1_Conv2d_1a_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(256, 3, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_1a_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_1a_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_1a_3x3_Activation')(branch_2)\n    branch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_7a_Branch_3_MaxPool_1a_3x3')(x)\n    branches = [branch_0, branch_1, branch_2, branch_pool]\n    x = Concatenate(axis=3, name='Mixed_7a')(branches)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_1_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_1_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_2_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_2_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_3_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_3_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_4_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_4_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_5_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_5_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_6_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_6_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_6_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 1})(up)\n    x = add([x, up])\n    x = GlobalAveragePooling2D(name='AvgPool')(x)\n    x = Dropout(1.0 - 0.8, name='Dropout')(x)\n    x = Dense(dimension, use_bias=False, name='Bottleneck')(x)\n    x = BatchNormalization(momentum=0.995, epsilon=0.001, scale=False, name='Bottleneck_BatchNorm')(x)\n    model = Model(inputs, x, name='inception_resnet_v1')\n    return model",
            "def InceptionResNetV2(dimension=128):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    inputs = Input(shape=(160, 160, 3))\n    x = Conv2D(32, 3, strides=2, padding='valid', use_bias=False, name='Conv2d_1a_3x3')(inputs)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_1a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_1a_3x3_Activation')(x)\n    x = Conv2D(32, 3, strides=1, padding='valid', use_bias=False, name='Conv2d_2a_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_2a_3x3_Activation')(x)\n    x = Conv2D(64, 3, strides=1, padding='same', use_bias=False, name='Conv2d_2b_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2b_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_2b_3x3_Activation')(x)\n    x = MaxPooling2D(3, strides=2, name='MaxPool_3a_3x3')(x)\n    x = Conv2D(80, 1, strides=1, padding='valid', use_bias=False, name='Conv2d_3b_1x1')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_3b_1x1_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_3b_1x1_Activation')(x)\n    x = Conv2D(192, 3, strides=1, padding='valid', use_bias=False, name='Conv2d_4a_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4a_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_4a_3x3_Activation')(x)\n    x = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Conv2d_4b_3x3')(x)\n    x = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4b_3x3_BatchNorm')(x)\n    x = Activation('relu', name='Conv2d_4b_3x3_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_1_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_1_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_1_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_2_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_2_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_2_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_3_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_3_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_3_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_4_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_4_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_4_Activation')(x)\n    branch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block35_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name='Block35_5_Branch_2_Conv2d_0c_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n    branches = [branch_0, branch_1, branch_2]\n    mixed = Concatenate(axis=3, name='Block35_5_Concatenate')(branches)\n    up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name='Block35_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block35_5_Activation')(x)\n    branch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name='Mixed_6a_Branch_0_Conv2d_1a_3x3')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_6a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, 3, strides=1, padding='same', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_0b_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n    branch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_6a_Branch_1_Conv2d_1a_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n    branch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_6a_Branch_2_MaxPool_1a_3x3')(x)\n    branches = [branch_0, branch_1, branch_pool]\n    x = Concatenate(axis=3, name='Mixed_6a')(branches)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_1_Branch_1_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_1_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_1_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_2_Branch_2_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_2_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_2_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_3_Branch_3_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_3_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_3_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_4_Branch_4_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_4_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_4_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_5_Branch_5_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_5_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_5_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_6_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_6_Branch_6_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_6_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_6_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_6_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_7_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_7_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_7_Branch_7_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_7_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_7_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_7_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_8_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_8_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_8_Branch_8_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_8_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_8_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_8_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_9_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_9_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_9_Branch_9_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_9_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_9_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_9_Activation')(x)\n    branch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_10_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block17_10_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0b_1x7')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0b_1x7_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0b_1x7_Activation')(branch_1)\n    branch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name='Block17_10_Branch_10_Conv2d_0c_7x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0c_7x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0c_7x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block17_10_Concatenate')(branches)\n    up = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name='Block17_10_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block17_10_Activation')(x)\n    branch_0 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_0_Conv2d_0a_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_0a_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_0a_1x1_Activation')(branch_0)\n    branch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_0_Conv2d_1a_3x3')(branch_0)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n    branch_1 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_1_Conv2d_1a_3x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n    branch_2 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_0a_1x1')(x)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n    branch_2 = Conv2D(256, 3, strides=1, padding='same', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_0b_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n    branch_2 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name='Mixed_7a_Branch_2_Conv2d_1a_3x3')(branch_2)\n    branch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_1a_3x3_BatchNorm')(branch_2)\n    branch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_1a_3x3_Activation')(branch_2)\n    branch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_7a_Branch_3_MaxPool_1a_3x3')(x)\n    branches = [branch_0, branch_1, branch_2, branch_pool]\n    x = Concatenate(axis=3, name='Mixed_7a')(branches)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_1_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_1_Branch_1_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_1_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_1_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_1_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_2_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_2_Branch_2_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_2_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_2_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_2_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_3_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_3_Branch_3_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_3_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_3_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_3_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_4_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_4_Branch_4_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_4_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_4_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_4_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_5_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_5_Branch_5_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_5_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_5_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n    x = add([x, up])\n    x = Activation('relu', name='Block8_5_Activation')(x)\n    branch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_6_Branch_0_Conv2d_1x1')(x)\n    branch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n    branch_0 = Activation('relu', name='Block8_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n    branch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0a_1x1')(x)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n    branch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0b_1x3')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n    branch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name='Block8_6_Branch_1_Conv2d_0c_3x1')(branch_1)\n    branch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n    branch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n    branches = [branch_0, branch_1]\n    mixed = Concatenate(axis=3, name='Block8_6_Concatenate')(branches)\n    up = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name='Block8_6_Conv2d_1x1')(mixed)\n    up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 1})(up)\n    x = add([x, up])\n    x = GlobalAveragePooling2D(name='AvgPool')(x)\n    x = Dropout(1.0 - 0.8, name='Dropout')(x)\n    x = Dense(dimension, use_bias=False, name='Bottleneck')(x)\n    x = BatchNormalization(momentum=0.995, epsilon=0.001, scale=False, name='Bottleneck_BatchNorm')(x)\n    model = Model(inputs, x, name='inception_resnet_v1')\n    return model"
        ]
    },
    {
        "func_name": "loadModel",
        "original": "def loadModel(url='https://github.com/serengil/deepface_models/releases/download/v1.0/facenet_weights.h5'):\n    model = InceptionResNetV2()\n    home = functions.get_deepface_home()\n    if os.path.isfile(home + '/.deepface/weights/facenet_weights.h5') != True:\n        print('facenet_weights.h5 will be downloaded...')\n        output = home + '/.deepface/weights/facenet_weights.h5'\n        gdown.download(url, output, quiet=False)\n    model.load_weights(home + '/.deepface/weights/facenet_weights.h5')\n    return model",
        "mutated": [
            "def loadModel(url='https://github.com/serengil/deepface_models/releases/download/v1.0/facenet_weights.h5'):\n    if False:\n        i = 10\n    model = InceptionResNetV2()\n    home = functions.get_deepface_home()\n    if os.path.isfile(home + '/.deepface/weights/facenet_weights.h5') != True:\n        print('facenet_weights.h5 will be downloaded...')\n        output = home + '/.deepface/weights/facenet_weights.h5'\n        gdown.download(url, output, quiet=False)\n    model.load_weights(home + '/.deepface/weights/facenet_weights.h5')\n    return model",
            "def loadModel(url='https://github.com/serengil/deepface_models/releases/download/v1.0/facenet_weights.h5'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    model = InceptionResNetV2()\n    home = functions.get_deepface_home()\n    if os.path.isfile(home + '/.deepface/weights/facenet_weights.h5') != True:\n        print('facenet_weights.h5 will be downloaded...')\n        output = home + '/.deepface/weights/facenet_weights.h5'\n        gdown.download(url, output, quiet=False)\n    model.load_weights(home + '/.deepface/weights/facenet_weights.h5')\n    return model",
            "def loadModel(url='https://github.com/serengil/deepface_models/releases/download/v1.0/facenet_weights.h5'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    model = InceptionResNetV2()\n    home = functions.get_deepface_home()\n    if os.path.isfile(home + '/.deepface/weights/facenet_weights.h5') != True:\n        print('facenet_weights.h5 will be downloaded...')\n        output = home + '/.deepface/weights/facenet_weights.h5'\n        gdown.download(url, output, quiet=False)\n    model.load_weights(home + '/.deepface/weights/facenet_weights.h5')\n    return model",
            "def loadModel(url='https://github.com/serengil/deepface_models/releases/download/v1.0/facenet_weights.h5'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    model = InceptionResNetV2()\n    home = functions.get_deepface_home()\n    if os.path.isfile(home + '/.deepface/weights/facenet_weights.h5') != True:\n        print('facenet_weights.h5 will be downloaded...')\n        output = home + '/.deepface/weights/facenet_weights.h5'\n        gdown.download(url, output, quiet=False)\n    model.load_weights(home + '/.deepface/weights/facenet_weights.h5')\n    return model",
            "def loadModel(url='https://github.com/serengil/deepface_models/releases/download/v1.0/facenet_weights.h5'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    model = InceptionResNetV2()\n    home = functions.get_deepface_home()\n    if os.path.isfile(home + '/.deepface/weights/facenet_weights.h5') != True:\n        print('facenet_weights.h5 will be downloaded...')\n        output = home + '/.deepface/weights/facenet_weights.h5'\n        gdown.download(url, output, quiet=False)\n    model.load_weights(home + '/.deepface/weights/facenet_weights.h5')\n    return model"
        ]
    }
]