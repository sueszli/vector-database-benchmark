[
    {
        "func_name": "graph_send_recv",
        "original": "@deprecated(since='2.4.0', update_to='paddle.geometric.send_u_recv', level=1, reason='graph_send_recv in paddle.incubate will be removed in future')\ndef graph_send_recv(x, src_index, dst_index, pool_type='sum', out_size=None, name=None):\n    \"\"\"\n\n    Graph Learning Send_Recv combine operator.\n\n    This operator is mainly used in Graph Learning domain, and the main purpose is to reduce intermediate memory\n    consumption in the process of message passing. Take `x` as the input tensor, we first use `src_index`\n    to gather the corresponding data, and then use `dst_index` to update the corresponding position of output tensor\n    in different pooling types, like sum, mean, max, or min. Besides, we can set `out_size` to get necessary output shape.\n\n    .. code-block:: text\n\n           Given:\n\n           X = [[0, 2, 3],\n                [1, 4, 5],\n                [2, 6, 7]]\n\n           src_index = [0, 1, 2, 0]\n\n           dst_index = [1, 2, 1, 0]\n\n           pool_type = \"sum\"\n\n           out_size = None\n\n           Then:\n\n           Out = [[0, 2, 3],\n                  [2, 8, 10],\n                  [1, 4, 5]]\n\n    Args:\n        x (Tensor): The input tensor, and the available data type is float32, float64, int32, int64.\n        src_index (Tensor): An 1-D tensor, and the available data type is int32, int64.\n        dst_index (Tensor): An 1-D tensor, and should have the same shape as `src_index`.\n                            The available data type is int32, int64.\n        pool_type (str): The pooling types of graph_send_recv, including `sum`, `mean`, `max`, `min`.\n                         Default value is `sum`.\n        out_size (int|Tensor|None): We can set `out_size` to get necessary output shape. If not set or\n                                    out_size is smaller or equal to 0, then this input will not be used.\n                                    Otherwise, `out_size` should be equal with or larger than\n                                    max(dst_index) + 1.\n        name (str, optional): Name for the operation (optional, default is None).\n                              For more information, please refer to :ref:`api_guide_Name`.\n\n    Returns:\n        out (Tensor): The output tensor, should have the same shape and same dtype as input tensor `x`.\n                      If `out_size` is set correctly, then it should have the same shape as `x` except\n                      the 0th dimension.\n\n    Examples:\n\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\n            >>> indexes = paddle.to_tensor([[0, 1], [1, 2], [2, 1], [0, 0]], dtype=\"int32\")\n            >>> src_index = indexes[:, 0]\n            >>> dst_index = indexes[:, 1]\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\")\n            >>> print(out)\n            Tensor(shape=[3, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\n            [[0. , 2. , 3. ],\n             [2. , 8. , 10.],\n             [1. , 4. , 5. ]])\n\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\n            >>> indexes = paddle.to_tensor([[0, 1], [2, 1], [0, 0]], dtype=\"int32\")\n            >>> src_index = indexes[:, 0]\n            >>> dst_index = indexes[:, 1]\n            >>> out_size = paddle.max(dst_index) + 1\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\", out_size=out_size)\n            >>> print(out)\n            Tensor(shape=[2, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\n            [[0. , 2. , 3. ],\n             [2. , 8. , 10.]])\n\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\n            >>> indexes = paddle.to_tensor([[0, 1], [2, 1], [0, 0]], dtype=\"int32\")\n            >>> src_index = indexes[:, 0]\n            >>> dst_index = indexes[:, 1]\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\")\n            >>> print(out)\n            Tensor(shape=[3, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\n            [[0. , 2. , 3. ],\n             [2. , 8. , 10.],\n             [0. , 0. , 0. ]])\n    \"\"\"\n    if pool_type not in ['sum', 'mean', 'max', 'min']:\n        raise ValueError('pool_type should be `sum`, `mean`, `max` or `min`, but received %s' % pool_type)\n    if in_dynamic_or_pir_mode():\n        out_size = convert_out_size_to_list(out_size, 'graph_send_recv')\n        return _C_ops.send_u_recv(x, src_index, dst_index, pool_type.upper(), out_size)\n    else:\n        check_variable_and_dtype(x, 'X', ('float32', 'float64', 'int32', 'int64'), 'graph_send_recv')\n        check_variable_and_dtype(src_index, 'Src_index', ('int32', 'int64'), 'graph_send_recv')\n        check_variable_and_dtype(dst_index, 'Dst_index', ('int32', 'int64'), 'graph_send_recv')\n        if out_size:\n            check_type(out_size, 'out_size', (int, np.int32, np.int64, Variable), 'graph_send_recv')\n        if isinstance(out_size, Variable):\n            check_dtype(out_size.dtype, 'out_size', ['int32', 'int64'], 'graph_send_recv')\n        helper = LayerHelper('graph_send_recv', **locals())\n        out = helper.create_variable_for_type_inference(dtype=x.dtype)\n        dst_count = helper.create_variable_for_type_inference(dtype='int32', stop_gradient=True)\n        inputs = {'X': x, 'Src_index': src_index, 'Dst_index': dst_index}\n        attrs = {'reduce_op': pool_type.upper()}\n        get_out_size_tensor_inputs(inputs=inputs, attrs=attrs, out_size=out_size, op_type='graph_send_recv')\n        helper.append_op(type='graph_send_recv', inputs=inputs, outputs={'Out': out, 'Dst_count': dst_count}, attrs=attrs)\n    return out",
        "mutated": [
            "@deprecated(since='2.4.0', update_to='paddle.geometric.send_u_recv', level=1, reason='graph_send_recv in paddle.incubate will be removed in future')\ndef graph_send_recv(x, src_index, dst_index, pool_type='sum', out_size=None, name=None):\n    if False:\n        i = 10\n    '\\n\\n    Graph Learning Send_Recv combine operator.\\n\\n    This operator is mainly used in Graph Learning domain, and the main purpose is to reduce intermediate memory\\n    consumption in the process of message passing. Take `x` as the input tensor, we first use `src_index`\\n    to gather the corresponding data, and then use `dst_index` to update the corresponding position of output tensor\\n    in different pooling types, like sum, mean, max, or min. Besides, we can set `out_size` to get necessary output shape.\\n\\n    .. code-block:: text\\n\\n           Given:\\n\\n           X = [[0, 2, 3],\\n                [1, 4, 5],\\n                [2, 6, 7]]\\n\\n           src_index = [0, 1, 2, 0]\\n\\n           dst_index = [1, 2, 1, 0]\\n\\n           pool_type = \"sum\"\\n\\n           out_size = None\\n\\n           Then:\\n\\n           Out = [[0, 2, 3],\\n                  [2, 8, 10],\\n                  [1, 4, 5]]\\n\\n    Args:\\n        x (Tensor): The input tensor, and the available data type is float32, float64, int32, int64.\\n        src_index (Tensor): An 1-D tensor, and the available data type is int32, int64.\\n        dst_index (Tensor): An 1-D tensor, and should have the same shape as `src_index`.\\n                            The available data type is int32, int64.\\n        pool_type (str): The pooling types of graph_send_recv, including `sum`, `mean`, `max`, `min`.\\n                         Default value is `sum`.\\n        out_size (int|Tensor|None): We can set `out_size` to get necessary output shape. If not set or\\n                                    out_size is smaller or equal to 0, then this input will not be used.\\n                                    Otherwise, `out_size` should be equal with or larger than\\n                                    max(dst_index) + 1.\\n        name (str, optional): Name for the operation (optional, default is None).\\n                              For more information, please refer to :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out (Tensor): The output tensor, should have the same shape and same dtype as input tensor `x`.\\n                      If `out_size` is set correctly, then it should have the same shape as `x` except\\n                      the 0th dimension.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [1, 2], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\")\\n            >>> print(out)\\n            Tensor(shape=[3, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.],\\n             [1. , 4. , 5. ]])\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out_size = paddle.max(dst_index) + 1\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\", out_size=out_size)\\n            >>> print(out)\\n            Tensor(shape=[2, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.]])\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\")\\n            >>> print(out)\\n            Tensor(shape=[3, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.],\\n             [0. , 0. , 0. ]])\\n    '\n    if pool_type not in ['sum', 'mean', 'max', 'min']:\n        raise ValueError('pool_type should be `sum`, `mean`, `max` or `min`, but received %s' % pool_type)\n    if in_dynamic_or_pir_mode():\n        out_size = convert_out_size_to_list(out_size, 'graph_send_recv')\n        return _C_ops.send_u_recv(x, src_index, dst_index, pool_type.upper(), out_size)\n    else:\n        check_variable_and_dtype(x, 'X', ('float32', 'float64', 'int32', 'int64'), 'graph_send_recv')\n        check_variable_and_dtype(src_index, 'Src_index', ('int32', 'int64'), 'graph_send_recv')\n        check_variable_and_dtype(dst_index, 'Dst_index', ('int32', 'int64'), 'graph_send_recv')\n        if out_size:\n            check_type(out_size, 'out_size', (int, np.int32, np.int64, Variable), 'graph_send_recv')\n        if isinstance(out_size, Variable):\n            check_dtype(out_size.dtype, 'out_size', ['int32', 'int64'], 'graph_send_recv')\n        helper = LayerHelper('graph_send_recv', **locals())\n        out = helper.create_variable_for_type_inference(dtype=x.dtype)\n        dst_count = helper.create_variable_for_type_inference(dtype='int32', stop_gradient=True)\n        inputs = {'X': x, 'Src_index': src_index, 'Dst_index': dst_index}\n        attrs = {'reduce_op': pool_type.upper()}\n        get_out_size_tensor_inputs(inputs=inputs, attrs=attrs, out_size=out_size, op_type='graph_send_recv')\n        helper.append_op(type='graph_send_recv', inputs=inputs, outputs={'Out': out, 'Dst_count': dst_count}, attrs=attrs)\n    return out",
            "@deprecated(since='2.4.0', update_to='paddle.geometric.send_u_recv', level=1, reason='graph_send_recv in paddle.incubate will be removed in future')\ndef graph_send_recv(x, src_index, dst_index, pool_type='sum', out_size=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n\\n    Graph Learning Send_Recv combine operator.\\n\\n    This operator is mainly used in Graph Learning domain, and the main purpose is to reduce intermediate memory\\n    consumption in the process of message passing. Take `x` as the input tensor, we first use `src_index`\\n    to gather the corresponding data, and then use `dst_index` to update the corresponding position of output tensor\\n    in different pooling types, like sum, mean, max, or min. Besides, we can set `out_size` to get necessary output shape.\\n\\n    .. code-block:: text\\n\\n           Given:\\n\\n           X = [[0, 2, 3],\\n                [1, 4, 5],\\n                [2, 6, 7]]\\n\\n           src_index = [0, 1, 2, 0]\\n\\n           dst_index = [1, 2, 1, 0]\\n\\n           pool_type = \"sum\"\\n\\n           out_size = None\\n\\n           Then:\\n\\n           Out = [[0, 2, 3],\\n                  [2, 8, 10],\\n                  [1, 4, 5]]\\n\\n    Args:\\n        x (Tensor): The input tensor, and the available data type is float32, float64, int32, int64.\\n        src_index (Tensor): An 1-D tensor, and the available data type is int32, int64.\\n        dst_index (Tensor): An 1-D tensor, and should have the same shape as `src_index`.\\n                            The available data type is int32, int64.\\n        pool_type (str): The pooling types of graph_send_recv, including `sum`, `mean`, `max`, `min`.\\n                         Default value is `sum`.\\n        out_size (int|Tensor|None): We can set `out_size` to get necessary output shape. If not set or\\n                                    out_size is smaller or equal to 0, then this input will not be used.\\n                                    Otherwise, `out_size` should be equal with or larger than\\n                                    max(dst_index) + 1.\\n        name (str, optional): Name for the operation (optional, default is None).\\n                              For more information, please refer to :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out (Tensor): The output tensor, should have the same shape and same dtype as input tensor `x`.\\n                      If `out_size` is set correctly, then it should have the same shape as `x` except\\n                      the 0th dimension.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [1, 2], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\")\\n            >>> print(out)\\n            Tensor(shape=[3, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.],\\n             [1. , 4. , 5. ]])\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out_size = paddle.max(dst_index) + 1\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\", out_size=out_size)\\n            >>> print(out)\\n            Tensor(shape=[2, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.]])\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\")\\n            >>> print(out)\\n            Tensor(shape=[3, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.],\\n             [0. , 0. , 0. ]])\\n    '\n    if pool_type not in ['sum', 'mean', 'max', 'min']:\n        raise ValueError('pool_type should be `sum`, `mean`, `max` or `min`, but received %s' % pool_type)\n    if in_dynamic_or_pir_mode():\n        out_size = convert_out_size_to_list(out_size, 'graph_send_recv')\n        return _C_ops.send_u_recv(x, src_index, dst_index, pool_type.upper(), out_size)\n    else:\n        check_variable_and_dtype(x, 'X', ('float32', 'float64', 'int32', 'int64'), 'graph_send_recv')\n        check_variable_and_dtype(src_index, 'Src_index', ('int32', 'int64'), 'graph_send_recv')\n        check_variable_and_dtype(dst_index, 'Dst_index', ('int32', 'int64'), 'graph_send_recv')\n        if out_size:\n            check_type(out_size, 'out_size', (int, np.int32, np.int64, Variable), 'graph_send_recv')\n        if isinstance(out_size, Variable):\n            check_dtype(out_size.dtype, 'out_size', ['int32', 'int64'], 'graph_send_recv')\n        helper = LayerHelper('graph_send_recv', **locals())\n        out = helper.create_variable_for_type_inference(dtype=x.dtype)\n        dst_count = helper.create_variable_for_type_inference(dtype='int32', stop_gradient=True)\n        inputs = {'X': x, 'Src_index': src_index, 'Dst_index': dst_index}\n        attrs = {'reduce_op': pool_type.upper()}\n        get_out_size_tensor_inputs(inputs=inputs, attrs=attrs, out_size=out_size, op_type='graph_send_recv')\n        helper.append_op(type='graph_send_recv', inputs=inputs, outputs={'Out': out, 'Dst_count': dst_count}, attrs=attrs)\n    return out",
            "@deprecated(since='2.4.0', update_to='paddle.geometric.send_u_recv', level=1, reason='graph_send_recv in paddle.incubate will be removed in future')\ndef graph_send_recv(x, src_index, dst_index, pool_type='sum', out_size=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n\\n    Graph Learning Send_Recv combine operator.\\n\\n    This operator is mainly used in Graph Learning domain, and the main purpose is to reduce intermediate memory\\n    consumption in the process of message passing. Take `x` as the input tensor, we first use `src_index`\\n    to gather the corresponding data, and then use `dst_index` to update the corresponding position of output tensor\\n    in different pooling types, like sum, mean, max, or min. Besides, we can set `out_size` to get necessary output shape.\\n\\n    .. code-block:: text\\n\\n           Given:\\n\\n           X = [[0, 2, 3],\\n                [1, 4, 5],\\n                [2, 6, 7]]\\n\\n           src_index = [0, 1, 2, 0]\\n\\n           dst_index = [1, 2, 1, 0]\\n\\n           pool_type = \"sum\"\\n\\n           out_size = None\\n\\n           Then:\\n\\n           Out = [[0, 2, 3],\\n                  [2, 8, 10],\\n                  [1, 4, 5]]\\n\\n    Args:\\n        x (Tensor): The input tensor, and the available data type is float32, float64, int32, int64.\\n        src_index (Tensor): An 1-D tensor, and the available data type is int32, int64.\\n        dst_index (Tensor): An 1-D tensor, and should have the same shape as `src_index`.\\n                            The available data type is int32, int64.\\n        pool_type (str): The pooling types of graph_send_recv, including `sum`, `mean`, `max`, `min`.\\n                         Default value is `sum`.\\n        out_size (int|Tensor|None): We can set `out_size` to get necessary output shape. If not set or\\n                                    out_size is smaller or equal to 0, then this input will not be used.\\n                                    Otherwise, `out_size` should be equal with or larger than\\n                                    max(dst_index) + 1.\\n        name (str, optional): Name for the operation (optional, default is None).\\n                              For more information, please refer to :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out (Tensor): The output tensor, should have the same shape and same dtype as input tensor `x`.\\n                      If `out_size` is set correctly, then it should have the same shape as `x` except\\n                      the 0th dimension.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [1, 2], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\")\\n            >>> print(out)\\n            Tensor(shape=[3, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.],\\n             [1. , 4. , 5. ]])\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out_size = paddle.max(dst_index) + 1\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\", out_size=out_size)\\n            >>> print(out)\\n            Tensor(shape=[2, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.]])\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\")\\n            >>> print(out)\\n            Tensor(shape=[3, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.],\\n             [0. , 0. , 0. ]])\\n    '\n    if pool_type not in ['sum', 'mean', 'max', 'min']:\n        raise ValueError('pool_type should be `sum`, `mean`, `max` or `min`, but received %s' % pool_type)\n    if in_dynamic_or_pir_mode():\n        out_size = convert_out_size_to_list(out_size, 'graph_send_recv')\n        return _C_ops.send_u_recv(x, src_index, dst_index, pool_type.upper(), out_size)\n    else:\n        check_variable_and_dtype(x, 'X', ('float32', 'float64', 'int32', 'int64'), 'graph_send_recv')\n        check_variable_and_dtype(src_index, 'Src_index', ('int32', 'int64'), 'graph_send_recv')\n        check_variable_and_dtype(dst_index, 'Dst_index', ('int32', 'int64'), 'graph_send_recv')\n        if out_size:\n            check_type(out_size, 'out_size', (int, np.int32, np.int64, Variable), 'graph_send_recv')\n        if isinstance(out_size, Variable):\n            check_dtype(out_size.dtype, 'out_size', ['int32', 'int64'], 'graph_send_recv')\n        helper = LayerHelper('graph_send_recv', **locals())\n        out = helper.create_variable_for_type_inference(dtype=x.dtype)\n        dst_count = helper.create_variable_for_type_inference(dtype='int32', stop_gradient=True)\n        inputs = {'X': x, 'Src_index': src_index, 'Dst_index': dst_index}\n        attrs = {'reduce_op': pool_type.upper()}\n        get_out_size_tensor_inputs(inputs=inputs, attrs=attrs, out_size=out_size, op_type='graph_send_recv')\n        helper.append_op(type='graph_send_recv', inputs=inputs, outputs={'Out': out, 'Dst_count': dst_count}, attrs=attrs)\n    return out",
            "@deprecated(since='2.4.0', update_to='paddle.geometric.send_u_recv', level=1, reason='graph_send_recv in paddle.incubate will be removed in future')\ndef graph_send_recv(x, src_index, dst_index, pool_type='sum', out_size=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n\\n    Graph Learning Send_Recv combine operator.\\n\\n    This operator is mainly used in Graph Learning domain, and the main purpose is to reduce intermediate memory\\n    consumption in the process of message passing. Take `x` as the input tensor, we first use `src_index`\\n    to gather the corresponding data, and then use `dst_index` to update the corresponding position of output tensor\\n    in different pooling types, like sum, mean, max, or min. Besides, we can set `out_size` to get necessary output shape.\\n\\n    .. code-block:: text\\n\\n           Given:\\n\\n           X = [[0, 2, 3],\\n                [1, 4, 5],\\n                [2, 6, 7]]\\n\\n           src_index = [0, 1, 2, 0]\\n\\n           dst_index = [1, 2, 1, 0]\\n\\n           pool_type = \"sum\"\\n\\n           out_size = None\\n\\n           Then:\\n\\n           Out = [[0, 2, 3],\\n                  [2, 8, 10],\\n                  [1, 4, 5]]\\n\\n    Args:\\n        x (Tensor): The input tensor, and the available data type is float32, float64, int32, int64.\\n        src_index (Tensor): An 1-D tensor, and the available data type is int32, int64.\\n        dst_index (Tensor): An 1-D tensor, and should have the same shape as `src_index`.\\n                            The available data type is int32, int64.\\n        pool_type (str): The pooling types of graph_send_recv, including `sum`, `mean`, `max`, `min`.\\n                         Default value is `sum`.\\n        out_size (int|Tensor|None): We can set `out_size` to get necessary output shape. If not set or\\n                                    out_size is smaller or equal to 0, then this input will not be used.\\n                                    Otherwise, `out_size` should be equal with or larger than\\n                                    max(dst_index) + 1.\\n        name (str, optional): Name for the operation (optional, default is None).\\n                              For more information, please refer to :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out (Tensor): The output tensor, should have the same shape and same dtype as input tensor `x`.\\n                      If `out_size` is set correctly, then it should have the same shape as `x` except\\n                      the 0th dimension.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [1, 2], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\")\\n            >>> print(out)\\n            Tensor(shape=[3, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.],\\n             [1. , 4. , 5. ]])\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out_size = paddle.max(dst_index) + 1\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\", out_size=out_size)\\n            >>> print(out)\\n            Tensor(shape=[2, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.]])\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\")\\n            >>> print(out)\\n            Tensor(shape=[3, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.],\\n             [0. , 0. , 0. ]])\\n    '\n    if pool_type not in ['sum', 'mean', 'max', 'min']:\n        raise ValueError('pool_type should be `sum`, `mean`, `max` or `min`, but received %s' % pool_type)\n    if in_dynamic_or_pir_mode():\n        out_size = convert_out_size_to_list(out_size, 'graph_send_recv')\n        return _C_ops.send_u_recv(x, src_index, dst_index, pool_type.upper(), out_size)\n    else:\n        check_variable_and_dtype(x, 'X', ('float32', 'float64', 'int32', 'int64'), 'graph_send_recv')\n        check_variable_and_dtype(src_index, 'Src_index', ('int32', 'int64'), 'graph_send_recv')\n        check_variable_and_dtype(dst_index, 'Dst_index', ('int32', 'int64'), 'graph_send_recv')\n        if out_size:\n            check_type(out_size, 'out_size', (int, np.int32, np.int64, Variable), 'graph_send_recv')\n        if isinstance(out_size, Variable):\n            check_dtype(out_size.dtype, 'out_size', ['int32', 'int64'], 'graph_send_recv')\n        helper = LayerHelper('graph_send_recv', **locals())\n        out = helper.create_variable_for_type_inference(dtype=x.dtype)\n        dst_count = helper.create_variable_for_type_inference(dtype='int32', stop_gradient=True)\n        inputs = {'X': x, 'Src_index': src_index, 'Dst_index': dst_index}\n        attrs = {'reduce_op': pool_type.upper()}\n        get_out_size_tensor_inputs(inputs=inputs, attrs=attrs, out_size=out_size, op_type='graph_send_recv')\n        helper.append_op(type='graph_send_recv', inputs=inputs, outputs={'Out': out, 'Dst_count': dst_count}, attrs=attrs)\n    return out",
            "@deprecated(since='2.4.0', update_to='paddle.geometric.send_u_recv', level=1, reason='graph_send_recv in paddle.incubate will be removed in future')\ndef graph_send_recv(x, src_index, dst_index, pool_type='sum', out_size=None, name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n\\n    Graph Learning Send_Recv combine operator.\\n\\n    This operator is mainly used in Graph Learning domain, and the main purpose is to reduce intermediate memory\\n    consumption in the process of message passing. Take `x` as the input tensor, we first use `src_index`\\n    to gather the corresponding data, and then use `dst_index` to update the corresponding position of output tensor\\n    in different pooling types, like sum, mean, max, or min. Besides, we can set `out_size` to get necessary output shape.\\n\\n    .. code-block:: text\\n\\n           Given:\\n\\n           X = [[0, 2, 3],\\n                [1, 4, 5],\\n                [2, 6, 7]]\\n\\n           src_index = [0, 1, 2, 0]\\n\\n           dst_index = [1, 2, 1, 0]\\n\\n           pool_type = \"sum\"\\n\\n           out_size = None\\n\\n           Then:\\n\\n           Out = [[0, 2, 3],\\n                  [2, 8, 10],\\n                  [1, 4, 5]]\\n\\n    Args:\\n        x (Tensor): The input tensor, and the available data type is float32, float64, int32, int64.\\n        src_index (Tensor): An 1-D tensor, and the available data type is int32, int64.\\n        dst_index (Tensor): An 1-D tensor, and should have the same shape as `src_index`.\\n                            The available data type is int32, int64.\\n        pool_type (str): The pooling types of graph_send_recv, including `sum`, `mean`, `max`, `min`.\\n                         Default value is `sum`.\\n        out_size (int|Tensor|None): We can set `out_size` to get necessary output shape. If not set or\\n                                    out_size is smaller or equal to 0, then this input will not be used.\\n                                    Otherwise, `out_size` should be equal with or larger than\\n                                    max(dst_index) + 1.\\n        name (str, optional): Name for the operation (optional, default is None).\\n                              For more information, please refer to :ref:`api_guide_Name`.\\n\\n    Returns:\\n        out (Tensor): The output tensor, should have the same shape and same dtype as input tensor `x`.\\n                      If `out_size` is set correctly, then it should have the same shape as `x` except\\n                      the 0th dimension.\\n\\n    Examples:\\n\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [1, 2], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\")\\n            >>> print(out)\\n            Tensor(shape=[3, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.],\\n             [1. , 4. , 5. ]])\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out_size = paddle.max(dst_index) + 1\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\", out_size=out_size)\\n            >>> print(out)\\n            Tensor(shape=[2, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.]])\\n\\n            >>> x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype=\"float32\")\\n            >>> indexes = paddle.to_tensor([[0, 1], [2, 1], [0, 0]], dtype=\"int32\")\\n            >>> src_index = indexes[:, 0]\\n            >>> dst_index = indexes[:, 1]\\n            >>> out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type=\"sum\")\\n            >>> print(out)\\n            Tensor(shape=[3, 3], dtype=float32, place=Place(cpu), stop_gradient=True,\\n            [[0. , 2. , 3. ],\\n             [2. , 8. , 10.],\\n             [0. , 0. , 0. ]])\\n    '\n    if pool_type not in ['sum', 'mean', 'max', 'min']:\n        raise ValueError('pool_type should be `sum`, `mean`, `max` or `min`, but received %s' % pool_type)\n    if in_dynamic_or_pir_mode():\n        out_size = convert_out_size_to_list(out_size, 'graph_send_recv')\n        return _C_ops.send_u_recv(x, src_index, dst_index, pool_type.upper(), out_size)\n    else:\n        check_variable_and_dtype(x, 'X', ('float32', 'float64', 'int32', 'int64'), 'graph_send_recv')\n        check_variable_and_dtype(src_index, 'Src_index', ('int32', 'int64'), 'graph_send_recv')\n        check_variable_and_dtype(dst_index, 'Dst_index', ('int32', 'int64'), 'graph_send_recv')\n        if out_size:\n            check_type(out_size, 'out_size', (int, np.int32, np.int64, Variable), 'graph_send_recv')\n        if isinstance(out_size, Variable):\n            check_dtype(out_size.dtype, 'out_size', ['int32', 'int64'], 'graph_send_recv')\n        helper = LayerHelper('graph_send_recv', **locals())\n        out = helper.create_variable_for_type_inference(dtype=x.dtype)\n        dst_count = helper.create_variable_for_type_inference(dtype='int32', stop_gradient=True)\n        inputs = {'X': x, 'Src_index': src_index, 'Dst_index': dst_index}\n        attrs = {'reduce_op': pool_type.upper()}\n        get_out_size_tensor_inputs(inputs=inputs, attrs=attrs, out_size=out_size, op_type='graph_send_recv')\n        helper.append_op(type='graph_send_recv', inputs=inputs, outputs={'Out': out, 'Dst_count': dst_count}, attrs=attrs)\n    return out"
        ]
    }
]