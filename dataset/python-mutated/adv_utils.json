[
    {
        "func_name": "_symmetric_kl_div",
        "original": "def _symmetric_kl_div(logits1, logits2, attention_mask=None):\n    \"\"\"\n    Calclate two logits' the KL div value symmetrically.\n    :param logits1: The first logit.\n    :param logits2: The second logit.\n    :param attention_mask: An optional attention_mask which is used to mask some element out.\n    This is usually useful in token_classification tasks.\n    If the shape of logits is [N1, N2, ... Nn, D], the shape of attention_mask should be [N1, N2, ... Nn]\n    :return: The mean loss.\n    \"\"\"\n    labels_num = logits1.shape[-1]\n    KLDiv = nn.KLDivLoss(reduction='none')\n    loss = torch.sum(KLDiv(nn.LogSoftmax(dim=-1)(logits1), nn.Softmax(dim=-1)(logits2)), dim=-1) + torch.sum(KLDiv(nn.LogSoftmax(dim=-1)(logits2), nn.Softmax(dim=-1)(logits1)), dim=-1)\n    if attention_mask is not None:\n        loss = torch.sum(loss * attention_mask) / torch.sum(attention_mask) / labels_num\n    else:\n        loss = torch.mean(loss) / labels_num\n    return loss",
        "mutated": [
            "def _symmetric_kl_div(logits1, logits2, attention_mask=None):\n    if False:\n        i = 10\n    \"\\n    Calclate two logits' the KL div value symmetrically.\\n    :param logits1: The first logit.\\n    :param logits2: The second logit.\\n    :param attention_mask: An optional attention_mask which is used to mask some element out.\\n    This is usually useful in token_classification tasks.\\n    If the shape of logits is [N1, N2, ... Nn, D], the shape of attention_mask should be [N1, N2, ... Nn]\\n    :return: The mean loss.\\n    \"\n    labels_num = logits1.shape[-1]\n    KLDiv = nn.KLDivLoss(reduction='none')\n    loss = torch.sum(KLDiv(nn.LogSoftmax(dim=-1)(logits1), nn.Softmax(dim=-1)(logits2)), dim=-1) + torch.sum(KLDiv(nn.LogSoftmax(dim=-1)(logits2), nn.Softmax(dim=-1)(logits1)), dim=-1)\n    if attention_mask is not None:\n        loss = torch.sum(loss * attention_mask) / torch.sum(attention_mask) / labels_num\n    else:\n        loss = torch.mean(loss) / labels_num\n    return loss",
            "def _symmetric_kl_div(logits1, logits2, attention_mask=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Calclate two logits' the KL div value symmetrically.\\n    :param logits1: The first logit.\\n    :param logits2: The second logit.\\n    :param attention_mask: An optional attention_mask which is used to mask some element out.\\n    This is usually useful in token_classification tasks.\\n    If the shape of logits is [N1, N2, ... Nn, D], the shape of attention_mask should be [N1, N2, ... Nn]\\n    :return: The mean loss.\\n    \"\n    labels_num = logits1.shape[-1]\n    KLDiv = nn.KLDivLoss(reduction='none')\n    loss = torch.sum(KLDiv(nn.LogSoftmax(dim=-1)(logits1), nn.Softmax(dim=-1)(logits2)), dim=-1) + torch.sum(KLDiv(nn.LogSoftmax(dim=-1)(logits2), nn.Softmax(dim=-1)(logits1)), dim=-1)\n    if attention_mask is not None:\n        loss = torch.sum(loss * attention_mask) / torch.sum(attention_mask) / labels_num\n    else:\n        loss = torch.mean(loss) / labels_num\n    return loss",
            "def _symmetric_kl_div(logits1, logits2, attention_mask=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Calclate two logits' the KL div value symmetrically.\\n    :param logits1: The first logit.\\n    :param logits2: The second logit.\\n    :param attention_mask: An optional attention_mask which is used to mask some element out.\\n    This is usually useful in token_classification tasks.\\n    If the shape of logits is [N1, N2, ... Nn, D], the shape of attention_mask should be [N1, N2, ... Nn]\\n    :return: The mean loss.\\n    \"\n    labels_num = logits1.shape[-1]\n    KLDiv = nn.KLDivLoss(reduction='none')\n    loss = torch.sum(KLDiv(nn.LogSoftmax(dim=-1)(logits1), nn.Softmax(dim=-1)(logits2)), dim=-1) + torch.sum(KLDiv(nn.LogSoftmax(dim=-1)(logits2), nn.Softmax(dim=-1)(logits1)), dim=-1)\n    if attention_mask is not None:\n        loss = torch.sum(loss * attention_mask) / torch.sum(attention_mask) / labels_num\n    else:\n        loss = torch.mean(loss) / labels_num\n    return loss",
            "def _symmetric_kl_div(logits1, logits2, attention_mask=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Calclate two logits' the KL div value symmetrically.\\n    :param logits1: The first logit.\\n    :param logits2: The second logit.\\n    :param attention_mask: An optional attention_mask which is used to mask some element out.\\n    This is usually useful in token_classification tasks.\\n    If the shape of logits is [N1, N2, ... Nn, D], the shape of attention_mask should be [N1, N2, ... Nn]\\n    :return: The mean loss.\\n    \"\n    labels_num = logits1.shape[-1]\n    KLDiv = nn.KLDivLoss(reduction='none')\n    loss = torch.sum(KLDiv(nn.LogSoftmax(dim=-1)(logits1), nn.Softmax(dim=-1)(logits2)), dim=-1) + torch.sum(KLDiv(nn.LogSoftmax(dim=-1)(logits2), nn.Softmax(dim=-1)(logits1)), dim=-1)\n    if attention_mask is not None:\n        loss = torch.sum(loss * attention_mask) / torch.sum(attention_mask) / labels_num\n    else:\n        loss = torch.mean(loss) / labels_num\n    return loss",
            "def _symmetric_kl_div(logits1, logits2, attention_mask=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Calclate two logits' the KL div value symmetrically.\\n    :param logits1: The first logit.\\n    :param logits2: The second logit.\\n    :param attention_mask: An optional attention_mask which is used to mask some element out.\\n    This is usually useful in token_classification tasks.\\n    If the shape of logits is [N1, N2, ... Nn, D], the shape of attention_mask should be [N1, N2, ... Nn]\\n    :return: The mean loss.\\n    \"\n    labels_num = logits1.shape[-1]\n    KLDiv = nn.KLDivLoss(reduction='none')\n    loss = torch.sum(KLDiv(nn.LogSoftmax(dim=-1)(logits1), nn.Softmax(dim=-1)(logits2)), dim=-1) + torch.sum(KLDiv(nn.LogSoftmax(dim=-1)(logits2), nn.Softmax(dim=-1)(logits1)), dim=-1)\n    if attention_mask is not None:\n        loss = torch.sum(loss * attention_mask) / torch.sum(attention_mask) / labels_num\n    else:\n        loss = torch.mean(loss) / labels_num\n    return loss"
        ]
    },
    {
        "func_name": "compute_adv_loss",
        "original": "def compute_adv_loss(embedding, model, ori_logits, ori_loss, adv_grad_factor, adv_bound=None, sigma=5e-06, **kwargs):\n    \"\"\"\n    Calculate the adv loss of the model.\n    :param embedding: Original sentense embedding\n    :param model: The model, or the forward function(including decoder/classifier),\n            accept kwargs as input, output logits\n    :param ori_logits: The original logits outputed from the model function\n    :param ori_loss: The original loss\n    :param adv_grad_factor: This factor will be multipled by the KL loss grad and then the result will be added to\n            the original embedding.\n            More details please check:https://arxiv.org/abs/1908.04577\n            The range of this value always be 1e-3~1e-7\n    :param adv_bound: adv_bound is used to cut the top and the bottom bound of the produced embedding.\n            If not proveded, 2 * sigma will be used as the adv_bound factor\n    :param sigma: The std factor used to produce a 0 mean normal distribution.\n            If adv_bound not proveded, 2 * sigma will be used as the adv_bound factor\n    :param kwargs: the input param used in model function\n    :return: The original loss adds the adv loss\n    \"\"\"\n    adv_bound = adv_bound if adv_bound is not None else 2 * sigma\n    embedding_1 = embedding + embedding.data.new(embedding.size()).normal_(0, sigma)\n    kwargs.pop('input_ids')\n    if 'inputs_embeds' in kwargs:\n        kwargs.pop('inputs_embeds')\n    with_attention_mask = False if 'with_attention_mask' not in kwargs else kwargs['with_attention_mask']\n    attention_mask = kwargs['attention_mask']\n    if not with_attention_mask:\n        attention_mask = None\n    if 'with_attention_mask' in kwargs:\n        kwargs.pop('with_attention_mask')\n    outputs = model(**kwargs, inputs_embeds=embedding_1)\n    v1_logits = outputs.logits\n    loss = _symmetric_kl_div(ori_logits, v1_logits, attention_mask)\n    emb_grad = torch.autograd.grad(loss, embedding_1)[0].data\n    emb_grad_norm = emb_grad.norm(dim=2, keepdim=True, p=float('inf')).max(1, keepdim=True)[0]\n    is_nan = torch.any(torch.isnan(emb_grad_norm))\n    if is_nan:\n        logger.warning('Nan occurred when calculating adv loss.')\n        return ori_loss\n    emb_grad = emb_grad / (emb_grad_norm + 1e-06)\n    embedding_2 = embedding_1 + adv_grad_factor * emb_grad\n    embedding_2 = torch.max(embedding_1 - adv_bound, embedding_2)\n    embedding_2 = torch.min(embedding_1 + adv_bound, embedding_2)\n    outputs = model(**kwargs, inputs_embeds=embedding_2)\n    adv_logits = outputs.logits\n    adv_loss = _symmetric_kl_div(ori_logits, adv_logits, attention_mask)\n    return ori_loss + adv_loss",
        "mutated": [
            "def compute_adv_loss(embedding, model, ori_logits, ori_loss, adv_grad_factor, adv_bound=None, sigma=5e-06, **kwargs):\n    if False:\n        i = 10\n    '\\n    Calculate the adv loss of the model.\\n    :param embedding: Original sentense embedding\\n    :param model: The model, or the forward function(including decoder/classifier),\\n            accept kwargs as input, output logits\\n    :param ori_logits: The original logits outputed from the model function\\n    :param ori_loss: The original loss\\n    :param adv_grad_factor: This factor will be multipled by the KL loss grad and then the result will be added to\\n            the original embedding.\\n            More details please check:https://arxiv.org/abs/1908.04577\\n            The range of this value always be 1e-3~1e-7\\n    :param adv_bound: adv_bound is used to cut the top and the bottom bound of the produced embedding.\\n            If not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param sigma: The std factor used to produce a 0 mean normal distribution.\\n            If adv_bound not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param kwargs: the input param used in model function\\n    :return: The original loss adds the adv loss\\n    '\n    adv_bound = adv_bound if adv_bound is not None else 2 * sigma\n    embedding_1 = embedding + embedding.data.new(embedding.size()).normal_(0, sigma)\n    kwargs.pop('input_ids')\n    if 'inputs_embeds' in kwargs:\n        kwargs.pop('inputs_embeds')\n    with_attention_mask = False if 'with_attention_mask' not in kwargs else kwargs['with_attention_mask']\n    attention_mask = kwargs['attention_mask']\n    if not with_attention_mask:\n        attention_mask = None\n    if 'with_attention_mask' in kwargs:\n        kwargs.pop('with_attention_mask')\n    outputs = model(**kwargs, inputs_embeds=embedding_1)\n    v1_logits = outputs.logits\n    loss = _symmetric_kl_div(ori_logits, v1_logits, attention_mask)\n    emb_grad = torch.autograd.grad(loss, embedding_1)[0].data\n    emb_grad_norm = emb_grad.norm(dim=2, keepdim=True, p=float('inf')).max(1, keepdim=True)[0]\n    is_nan = torch.any(torch.isnan(emb_grad_norm))\n    if is_nan:\n        logger.warning('Nan occurred when calculating adv loss.')\n        return ori_loss\n    emb_grad = emb_grad / (emb_grad_norm + 1e-06)\n    embedding_2 = embedding_1 + adv_grad_factor * emb_grad\n    embedding_2 = torch.max(embedding_1 - adv_bound, embedding_2)\n    embedding_2 = torch.min(embedding_1 + adv_bound, embedding_2)\n    outputs = model(**kwargs, inputs_embeds=embedding_2)\n    adv_logits = outputs.logits\n    adv_loss = _symmetric_kl_div(ori_logits, adv_logits, attention_mask)\n    return ori_loss + adv_loss",
            "def compute_adv_loss(embedding, model, ori_logits, ori_loss, adv_grad_factor, adv_bound=None, sigma=5e-06, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculate the adv loss of the model.\\n    :param embedding: Original sentense embedding\\n    :param model: The model, or the forward function(including decoder/classifier),\\n            accept kwargs as input, output logits\\n    :param ori_logits: The original logits outputed from the model function\\n    :param ori_loss: The original loss\\n    :param adv_grad_factor: This factor will be multipled by the KL loss grad and then the result will be added to\\n            the original embedding.\\n            More details please check:https://arxiv.org/abs/1908.04577\\n            The range of this value always be 1e-3~1e-7\\n    :param adv_bound: adv_bound is used to cut the top and the bottom bound of the produced embedding.\\n            If not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param sigma: The std factor used to produce a 0 mean normal distribution.\\n            If adv_bound not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param kwargs: the input param used in model function\\n    :return: The original loss adds the adv loss\\n    '\n    adv_bound = adv_bound if adv_bound is not None else 2 * sigma\n    embedding_1 = embedding + embedding.data.new(embedding.size()).normal_(0, sigma)\n    kwargs.pop('input_ids')\n    if 'inputs_embeds' in kwargs:\n        kwargs.pop('inputs_embeds')\n    with_attention_mask = False if 'with_attention_mask' not in kwargs else kwargs['with_attention_mask']\n    attention_mask = kwargs['attention_mask']\n    if not with_attention_mask:\n        attention_mask = None\n    if 'with_attention_mask' in kwargs:\n        kwargs.pop('with_attention_mask')\n    outputs = model(**kwargs, inputs_embeds=embedding_1)\n    v1_logits = outputs.logits\n    loss = _symmetric_kl_div(ori_logits, v1_logits, attention_mask)\n    emb_grad = torch.autograd.grad(loss, embedding_1)[0].data\n    emb_grad_norm = emb_grad.norm(dim=2, keepdim=True, p=float('inf')).max(1, keepdim=True)[0]\n    is_nan = torch.any(torch.isnan(emb_grad_norm))\n    if is_nan:\n        logger.warning('Nan occurred when calculating adv loss.')\n        return ori_loss\n    emb_grad = emb_grad / (emb_grad_norm + 1e-06)\n    embedding_2 = embedding_1 + adv_grad_factor * emb_grad\n    embedding_2 = torch.max(embedding_1 - adv_bound, embedding_2)\n    embedding_2 = torch.min(embedding_1 + adv_bound, embedding_2)\n    outputs = model(**kwargs, inputs_embeds=embedding_2)\n    adv_logits = outputs.logits\n    adv_loss = _symmetric_kl_div(ori_logits, adv_logits, attention_mask)\n    return ori_loss + adv_loss",
            "def compute_adv_loss(embedding, model, ori_logits, ori_loss, adv_grad_factor, adv_bound=None, sigma=5e-06, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculate the adv loss of the model.\\n    :param embedding: Original sentense embedding\\n    :param model: The model, or the forward function(including decoder/classifier),\\n            accept kwargs as input, output logits\\n    :param ori_logits: The original logits outputed from the model function\\n    :param ori_loss: The original loss\\n    :param adv_grad_factor: This factor will be multipled by the KL loss grad and then the result will be added to\\n            the original embedding.\\n            More details please check:https://arxiv.org/abs/1908.04577\\n            The range of this value always be 1e-3~1e-7\\n    :param adv_bound: adv_bound is used to cut the top and the bottom bound of the produced embedding.\\n            If not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param sigma: The std factor used to produce a 0 mean normal distribution.\\n            If adv_bound not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param kwargs: the input param used in model function\\n    :return: The original loss adds the adv loss\\n    '\n    adv_bound = adv_bound if adv_bound is not None else 2 * sigma\n    embedding_1 = embedding + embedding.data.new(embedding.size()).normal_(0, sigma)\n    kwargs.pop('input_ids')\n    if 'inputs_embeds' in kwargs:\n        kwargs.pop('inputs_embeds')\n    with_attention_mask = False if 'with_attention_mask' not in kwargs else kwargs['with_attention_mask']\n    attention_mask = kwargs['attention_mask']\n    if not with_attention_mask:\n        attention_mask = None\n    if 'with_attention_mask' in kwargs:\n        kwargs.pop('with_attention_mask')\n    outputs = model(**kwargs, inputs_embeds=embedding_1)\n    v1_logits = outputs.logits\n    loss = _symmetric_kl_div(ori_logits, v1_logits, attention_mask)\n    emb_grad = torch.autograd.grad(loss, embedding_1)[0].data\n    emb_grad_norm = emb_grad.norm(dim=2, keepdim=True, p=float('inf')).max(1, keepdim=True)[0]\n    is_nan = torch.any(torch.isnan(emb_grad_norm))\n    if is_nan:\n        logger.warning('Nan occurred when calculating adv loss.')\n        return ori_loss\n    emb_grad = emb_grad / (emb_grad_norm + 1e-06)\n    embedding_2 = embedding_1 + adv_grad_factor * emb_grad\n    embedding_2 = torch.max(embedding_1 - adv_bound, embedding_2)\n    embedding_2 = torch.min(embedding_1 + adv_bound, embedding_2)\n    outputs = model(**kwargs, inputs_embeds=embedding_2)\n    adv_logits = outputs.logits\n    adv_loss = _symmetric_kl_div(ori_logits, adv_logits, attention_mask)\n    return ori_loss + adv_loss",
            "def compute_adv_loss(embedding, model, ori_logits, ori_loss, adv_grad_factor, adv_bound=None, sigma=5e-06, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculate the adv loss of the model.\\n    :param embedding: Original sentense embedding\\n    :param model: The model, or the forward function(including decoder/classifier),\\n            accept kwargs as input, output logits\\n    :param ori_logits: The original logits outputed from the model function\\n    :param ori_loss: The original loss\\n    :param adv_grad_factor: This factor will be multipled by the KL loss grad and then the result will be added to\\n            the original embedding.\\n            More details please check:https://arxiv.org/abs/1908.04577\\n            The range of this value always be 1e-3~1e-7\\n    :param adv_bound: adv_bound is used to cut the top and the bottom bound of the produced embedding.\\n            If not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param sigma: The std factor used to produce a 0 mean normal distribution.\\n            If adv_bound not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param kwargs: the input param used in model function\\n    :return: The original loss adds the adv loss\\n    '\n    adv_bound = adv_bound if adv_bound is not None else 2 * sigma\n    embedding_1 = embedding + embedding.data.new(embedding.size()).normal_(0, sigma)\n    kwargs.pop('input_ids')\n    if 'inputs_embeds' in kwargs:\n        kwargs.pop('inputs_embeds')\n    with_attention_mask = False if 'with_attention_mask' not in kwargs else kwargs['with_attention_mask']\n    attention_mask = kwargs['attention_mask']\n    if not with_attention_mask:\n        attention_mask = None\n    if 'with_attention_mask' in kwargs:\n        kwargs.pop('with_attention_mask')\n    outputs = model(**kwargs, inputs_embeds=embedding_1)\n    v1_logits = outputs.logits\n    loss = _symmetric_kl_div(ori_logits, v1_logits, attention_mask)\n    emb_grad = torch.autograd.grad(loss, embedding_1)[0].data\n    emb_grad_norm = emb_grad.norm(dim=2, keepdim=True, p=float('inf')).max(1, keepdim=True)[0]\n    is_nan = torch.any(torch.isnan(emb_grad_norm))\n    if is_nan:\n        logger.warning('Nan occurred when calculating adv loss.')\n        return ori_loss\n    emb_grad = emb_grad / (emb_grad_norm + 1e-06)\n    embedding_2 = embedding_1 + adv_grad_factor * emb_grad\n    embedding_2 = torch.max(embedding_1 - adv_bound, embedding_2)\n    embedding_2 = torch.min(embedding_1 + adv_bound, embedding_2)\n    outputs = model(**kwargs, inputs_embeds=embedding_2)\n    adv_logits = outputs.logits\n    adv_loss = _symmetric_kl_div(ori_logits, adv_logits, attention_mask)\n    return ori_loss + adv_loss",
            "def compute_adv_loss(embedding, model, ori_logits, ori_loss, adv_grad_factor, adv_bound=None, sigma=5e-06, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculate the adv loss of the model.\\n    :param embedding: Original sentense embedding\\n    :param model: The model, or the forward function(including decoder/classifier),\\n            accept kwargs as input, output logits\\n    :param ori_logits: The original logits outputed from the model function\\n    :param ori_loss: The original loss\\n    :param adv_grad_factor: This factor will be multipled by the KL loss grad and then the result will be added to\\n            the original embedding.\\n            More details please check:https://arxiv.org/abs/1908.04577\\n            The range of this value always be 1e-3~1e-7\\n    :param adv_bound: adv_bound is used to cut the top and the bottom bound of the produced embedding.\\n            If not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param sigma: The std factor used to produce a 0 mean normal distribution.\\n            If adv_bound not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param kwargs: the input param used in model function\\n    :return: The original loss adds the adv loss\\n    '\n    adv_bound = adv_bound if adv_bound is not None else 2 * sigma\n    embedding_1 = embedding + embedding.data.new(embedding.size()).normal_(0, sigma)\n    kwargs.pop('input_ids')\n    if 'inputs_embeds' in kwargs:\n        kwargs.pop('inputs_embeds')\n    with_attention_mask = False if 'with_attention_mask' not in kwargs else kwargs['with_attention_mask']\n    attention_mask = kwargs['attention_mask']\n    if not with_attention_mask:\n        attention_mask = None\n    if 'with_attention_mask' in kwargs:\n        kwargs.pop('with_attention_mask')\n    outputs = model(**kwargs, inputs_embeds=embedding_1)\n    v1_logits = outputs.logits\n    loss = _symmetric_kl_div(ori_logits, v1_logits, attention_mask)\n    emb_grad = torch.autograd.grad(loss, embedding_1)[0].data\n    emb_grad_norm = emb_grad.norm(dim=2, keepdim=True, p=float('inf')).max(1, keepdim=True)[0]\n    is_nan = torch.any(torch.isnan(emb_grad_norm))\n    if is_nan:\n        logger.warning('Nan occurred when calculating adv loss.')\n        return ori_loss\n    emb_grad = emb_grad / (emb_grad_norm + 1e-06)\n    embedding_2 = embedding_1 + adv_grad_factor * emb_grad\n    embedding_2 = torch.max(embedding_1 - adv_bound, embedding_2)\n    embedding_2 = torch.min(embedding_1 + adv_bound, embedding_2)\n    outputs = model(**kwargs, inputs_embeds=embedding_2)\n    adv_logits = outputs.logits\n    adv_loss = _symmetric_kl_div(ori_logits, adv_logits, attention_mask)\n    return ori_loss + adv_loss"
        ]
    },
    {
        "func_name": "compute_adv_loss_pair",
        "original": "def compute_adv_loss_pair(embedding, model, start_logits, end_logits, ori_loss, adv_grad_factor, adv_bound=None, sigma=5e-06, **kwargs):\n    \"\"\"\n    Calculate the adv loss of the model. This function is used in the pair logits scenerio.\n    :param embedding: Original sentense embedding\n    :param model: The model, or the forward function(including decoder/classifier),\n            accept kwargs as input, output logits\n    :param start_logits: The original start logits outputed from the model function\n    :param end_logits: The original end logits outputed from the model function\n    :param ori_loss: The original loss\n    :param adv_grad_factor: This factor will be multipled by the KL loss grad and then the result will be added to\n            the original embedding.\n            More details please check:https://arxiv.org/abs/1908.04577\n            The range of this value always be 1e-3~1e-7\n    :param adv_bound: adv_bound is used to cut the top and the bottom bound of the produced embedding.\n            If not proveded, 2 * sigma will be used as the adv_bound factor\n    :param sigma: The std factor used to produce a 0 mean normal distribution.\n            If adv_bound not proveded, 2 * sigma will be used as the adv_bound factor\n    :param kwargs: the input param used in model function\n    :return: The original loss adds the adv loss\n    \"\"\"\n    adv_bound = adv_bound if adv_bound is not None else 2 * sigma\n    embedding_1 = embedding + embedding.data.new(embedding.size()).normal_(0, sigma)\n    kwargs.pop('input_ids')\n    if 'inputs_embeds' in kwargs:\n        kwargs.pop('inputs_embeds')\n    outputs = model(**kwargs, inputs_embeds=embedding_1)\n    (v1_logits_start, v1_logits_end) = outputs.logits\n    loss = _symmetric_kl_div(start_logits, v1_logits_start) + _symmetric_kl_div(end_logits, v1_logits_end)\n    loss = loss / 2\n    emb_grad = torch.autograd.grad(loss, embedding_1)[0].data\n    emb_grad_norm = emb_grad.norm(dim=2, keepdim=True, p=float('inf')).max(1, keepdim=True)[0]\n    is_nan = torch.any(torch.isnan(emb_grad_norm))\n    if is_nan:\n        logger.warning('Nan occurred when calculating pair adv loss.')\n        return ori_loss\n    emb_grad = emb_grad / emb_grad_norm\n    embedding_2 = embedding_1 + adv_grad_factor * emb_grad\n    embedding_2 = torch.max(embedding_1 - adv_bound, embedding_2)\n    embedding_2 = torch.min(embedding_1 + adv_bound, embedding_2)\n    outputs = model(**kwargs, inputs_embeds=embedding_2)\n    (adv_logits_start, adv_logits_end) = outputs.logits\n    adv_loss = _symmetric_kl_div(start_logits, adv_logits_start) + _symmetric_kl_div(end_logits, adv_logits_end)\n    return ori_loss + adv_loss",
        "mutated": [
            "def compute_adv_loss_pair(embedding, model, start_logits, end_logits, ori_loss, adv_grad_factor, adv_bound=None, sigma=5e-06, **kwargs):\n    if False:\n        i = 10\n    '\\n    Calculate the adv loss of the model. This function is used in the pair logits scenerio.\\n    :param embedding: Original sentense embedding\\n    :param model: The model, or the forward function(including decoder/classifier),\\n            accept kwargs as input, output logits\\n    :param start_logits: The original start logits outputed from the model function\\n    :param end_logits: The original end logits outputed from the model function\\n    :param ori_loss: The original loss\\n    :param adv_grad_factor: This factor will be multipled by the KL loss grad and then the result will be added to\\n            the original embedding.\\n            More details please check:https://arxiv.org/abs/1908.04577\\n            The range of this value always be 1e-3~1e-7\\n    :param adv_bound: adv_bound is used to cut the top and the bottom bound of the produced embedding.\\n            If not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param sigma: The std factor used to produce a 0 mean normal distribution.\\n            If adv_bound not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param kwargs: the input param used in model function\\n    :return: The original loss adds the adv loss\\n    '\n    adv_bound = adv_bound if adv_bound is not None else 2 * sigma\n    embedding_1 = embedding + embedding.data.new(embedding.size()).normal_(0, sigma)\n    kwargs.pop('input_ids')\n    if 'inputs_embeds' in kwargs:\n        kwargs.pop('inputs_embeds')\n    outputs = model(**kwargs, inputs_embeds=embedding_1)\n    (v1_logits_start, v1_logits_end) = outputs.logits\n    loss = _symmetric_kl_div(start_logits, v1_logits_start) + _symmetric_kl_div(end_logits, v1_logits_end)\n    loss = loss / 2\n    emb_grad = torch.autograd.grad(loss, embedding_1)[0].data\n    emb_grad_norm = emb_grad.norm(dim=2, keepdim=True, p=float('inf')).max(1, keepdim=True)[0]\n    is_nan = torch.any(torch.isnan(emb_grad_norm))\n    if is_nan:\n        logger.warning('Nan occurred when calculating pair adv loss.')\n        return ori_loss\n    emb_grad = emb_grad / emb_grad_norm\n    embedding_2 = embedding_1 + adv_grad_factor * emb_grad\n    embedding_2 = torch.max(embedding_1 - adv_bound, embedding_2)\n    embedding_2 = torch.min(embedding_1 + adv_bound, embedding_2)\n    outputs = model(**kwargs, inputs_embeds=embedding_2)\n    (adv_logits_start, adv_logits_end) = outputs.logits\n    adv_loss = _symmetric_kl_div(start_logits, adv_logits_start) + _symmetric_kl_div(end_logits, adv_logits_end)\n    return ori_loss + adv_loss",
            "def compute_adv_loss_pair(embedding, model, start_logits, end_logits, ori_loss, adv_grad_factor, adv_bound=None, sigma=5e-06, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculate the adv loss of the model. This function is used in the pair logits scenerio.\\n    :param embedding: Original sentense embedding\\n    :param model: The model, or the forward function(including decoder/classifier),\\n            accept kwargs as input, output logits\\n    :param start_logits: The original start logits outputed from the model function\\n    :param end_logits: The original end logits outputed from the model function\\n    :param ori_loss: The original loss\\n    :param adv_grad_factor: This factor will be multipled by the KL loss grad and then the result will be added to\\n            the original embedding.\\n            More details please check:https://arxiv.org/abs/1908.04577\\n            The range of this value always be 1e-3~1e-7\\n    :param adv_bound: adv_bound is used to cut the top and the bottom bound of the produced embedding.\\n            If not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param sigma: The std factor used to produce a 0 mean normal distribution.\\n            If adv_bound not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param kwargs: the input param used in model function\\n    :return: The original loss adds the adv loss\\n    '\n    adv_bound = adv_bound if adv_bound is not None else 2 * sigma\n    embedding_1 = embedding + embedding.data.new(embedding.size()).normal_(0, sigma)\n    kwargs.pop('input_ids')\n    if 'inputs_embeds' in kwargs:\n        kwargs.pop('inputs_embeds')\n    outputs = model(**kwargs, inputs_embeds=embedding_1)\n    (v1_logits_start, v1_logits_end) = outputs.logits\n    loss = _symmetric_kl_div(start_logits, v1_logits_start) + _symmetric_kl_div(end_logits, v1_logits_end)\n    loss = loss / 2\n    emb_grad = torch.autograd.grad(loss, embedding_1)[0].data\n    emb_grad_norm = emb_grad.norm(dim=2, keepdim=True, p=float('inf')).max(1, keepdim=True)[0]\n    is_nan = torch.any(torch.isnan(emb_grad_norm))\n    if is_nan:\n        logger.warning('Nan occurred when calculating pair adv loss.')\n        return ori_loss\n    emb_grad = emb_grad / emb_grad_norm\n    embedding_2 = embedding_1 + adv_grad_factor * emb_grad\n    embedding_2 = torch.max(embedding_1 - adv_bound, embedding_2)\n    embedding_2 = torch.min(embedding_1 + adv_bound, embedding_2)\n    outputs = model(**kwargs, inputs_embeds=embedding_2)\n    (adv_logits_start, adv_logits_end) = outputs.logits\n    adv_loss = _symmetric_kl_div(start_logits, adv_logits_start) + _symmetric_kl_div(end_logits, adv_logits_end)\n    return ori_loss + adv_loss",
            "def compute_adv_loss_pair(embedding, model, start_logits, end_logits, ori_loss, adv_grad_factor, adv_bound=None, sigma=5e-06, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculate the adv loss of the model. This function is used in the pair logits scenerio.\\n    :param embedding: Original sentense embedding\\n    :param model: The model, or the forward function(including decoder/classifier),\\n            accept kwargs as input, output logits\\n    :param start_logits: The original start logits outputed from the model function\\n    :param end_logits: The original end logits outputed from the model function\\n    :param ori_loss: The original loss\\n    :param adv_grad_factor: This factor will be multipled by the KL loss grad and then the result will be added to\\n            the original embedding.\\n            More details please check:https://arxiv.org/abs/1908.04577\\n            The range of this value always be 1e-3~1e-7\\n    :param adv_bound: adv_bound is used to cut the top and the bottom bound of the produced embedding.\\n            If not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param sigma: The std factor used to produce a 0 mean normal distribution.\\n            If adv_bound not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param kwargs: the input param used in model function\\n    :return: The original loss adds the adv loss\\n    '\n    adv_bound = adv_bound if adv_bound is not None else 2 * sigma\n    embedding_1 = embedding + embedding.data.new(embedding.size()).normal_(0, sigma)\n    kwargs.pop('input_ids')\n    if 'inputs_embeds' in kwargs:\n        kwargs.pop('inputs_embeds')\n    outputs = model(**kwargs, inputs_embeds=embedding_1)\n    (v1_logits_start, v1_logits_end) = outputs.logits\n    loss = _symmetric_kl_div(start_logits, v1_logits_start) + _symmetric_kl_div(end_logits, v1_logits_end)\n    loss = loss / 2\n    emb_grad = torch.autograd.grad(loss, embedding_1)[0].data\n    emb_grad_norm = emb_grad.norm(dim=2, keepdim=True, p=float('inf')).max(1, keepdim=True)[0]\n    is_nan = torch.any(torch.isnan(emb_grad_norm))\n    if is_nan:\n        logger.warning('Nan occurred when calculating pair adv loss.')\n        return ori_loss\n    emb_grad = emb_grad / emb_grad_norm\n    embedding_2 = embedding_1 + adv_grad_factor * emb_grad\n    embedding_2 = torch.max(embedding_1 - adv_bound, embedding_2)\n    embedding_2 = torch.min(embedding_1 + adv_bound, embedding_2)\n    outputs = model(**kwargs, inputs_embeds=embedding_2)\n    (adv_logits_start, adv_logits_end) = outputs.logits\n    adv_loss = _symmetric_kl_div(start_logits, adv_logits_start) + _symmetric_kl_div(end_logits, adv_logits_end)\n    return ori_loss + adv_loss",
            "def compute_adv_loss_pair(embedding, model, start_logits, end_logits, ori_loss, adv_grad_factor, adv_bound=None, sigma=5e-06, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculate the adv loss of the model. This function is used in the pair logits scenerio.\\n    :param embedding: Original sentense embedding\\n    :param model: The model, or the forward function(including decoder/classifier),\\n            accept kwargs as input, output logits\\n    :param start_logits: The original start logits outputed from the model function\\n    :param end_logits: The original end logits outputed from the model function\\n    :param ori_loss: The original loss\\n    :param adv_grad_factor: This factor will be multipled by the KL loss grad and then the result will be added to\\n            the original embedding.\\n            More details please check:https://arxiv.org/abs/1908.04577\\n            The range of this value always be 1e-3~1e-7\\n    :param adv_bound: adv_bound is used to cut the top and the bottom bound of the produced embedding.\\n            If not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param sigma: The std factor used to produce a 0 mean normal distribution.\\n            If adv_bound not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param kwargs: the input param used in model function\\n    :return: The original loss adds the adv loss\\n    '\n    adv_bound = adv_bound if adv_bound is not None else 2 * sigma\n    embedding_1 = embedding + embedding.data.new(embedding.size()).normal_(0, sigma)\n    kwargs.pop('input_ids')\n    if 'inputs_embeds' in kwargs:\n        kwargs.pop('inputs_embeds')\n    outputs = model(**kwargs, inputs_embeds=embedding_1)\n    (v1_logits_start, v1_logits_end) = outputs.logits\n    loss = _symmetric_kl_div(start_logits, v1_logits_start) + _symmetric_kl_div(end_logits, v1_logits_end)\n    loss = loss / 2\n    emb_grad = torch.autograd.grad(loss, embedding_1)[0].data\n    emb_grad_norm = emb_grad.norm(dim=2, keepdim=True, p=float('inf')).max(1, keepdim=True)[0]\n    is_nan = torch.any(torch.isnan(emb_grad_norm))\n    if is_nan:\n        logger.warning('Nan occurred when calculating pair adv loss.')\n        return ori_loss\n    emb_grad = emb_grad / emb_grad_norm\n    embedding_2 = embedding_1 + adv_grad_factor * emb_grad\n    embedding_2 = torch.max(embedding_1 - adv_bound, embedding_2)\n    embedding_2 = torch.min(embedding_1 + adv_bound, embedding_2)\n    outputs = model(**kwargs, inputs_embeds=embedding_2)\n    (adv_logits_start, adv_logits_end) = outputs.logits\n    adv_loss = _symmetric_kl_div(start_logits, adv_logits_start) + _symmetric_kl_div(end_logits, adv_logits_end)\n    return ori_loss + adv_loss",
            "def compute_adv_loss_pair(embedding, model, start_logits, end_logits, ori_loss, adv_grad_factor, adv_bound=None, sigma=5e-06, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculate the adv loss of the model. This function is used in the pair logits scenerio.\\n    :param embedding: Original sentense embedding\\n    :param model: The model, or the forward function(including decoder/classifier),\\n            accept kwargs as input, output logits\\n    :param start_logits: The original start logits outputed from the model function\\n    :param end_logits: The original end logits outputed from the model function\\n    :param ori_loss: The original loss\\n    :param adv_grad_factor: This factor will be multipled by the KL loss grad and then the result will be added to\\n            the original embedding.\\n            More details please check:https://arxiv.org/abs/1908.04577\\n            The range of this value always be 1e-3~1e-7\\n    :param adv_bound: adv_bound is used to cut the top and the bottom bound of the produced embedding.\\n            If not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param sigma: The std factor used to produce a 0 mean normal distribution.\\n            If adv_bound not proveded, 2 * sigma will be used as the adv_bound factor\\n    :param kwargs: the input param used in model function\\n    :return: The original loss adds the adv loss\\n    '\n    adv_bound = adv_bound if adv_bound is not None else 2 * sigma\n    embedding_1 = embedding + embedding.data.new(embedding.size()).normal_(0, sigma)\n    kwargs.pop('input_ids')\n    if 'inputs_embeds' in kwargs:\n        kwargs.pop('inputs_embeds')\n    outputs = model(**kwargs, inputs_embeds=embedding_1)\n    (v1_logits_start, v1_logits_end) = outputs.logits\n    loss = _symmetric_kl_div(start_logits, v1_logits_start) + _symmetric_kl_div(end_logits, v1_logits_end)\n    loss = loss / 2\n    emb_grad = torch.autograd.grad(loss, embedding_1)[0].data\n    emb_grad_norm = emb_grad.norm(dim=2, keepdim=True, p=float('inf')).max(1, keepdim=True)[0]\n    is_nan = torch.any(torch.isnan(emb_grad_norm))\n    if is_nan:\n        logger.warning('Nan occurred when calculating pair adv loss.')\n        return ori_loss\n    emb_grad = emb_grad / emb_grad_norm\n    embedding_2 = embedding_1 + adv_grad_factor * emb_grad\n    embedding_2 = torch.max(embedding_1 - adv_bound, embedding_2)\n    embedding_2 = torch.min(embedding_1 + adv_bound, embedding_2)\n    outputs = model(**kwargs, inputs_embeds=embedding_2)\n    (adv_logits_start, adv_logits_end) = outputs.logits\n    adv_loss = _symmetric_kl_div(start_logits, adv_logits_start) + _symmetric_kl_div(end_logits, adv_logits_end)\n    return ori_loss + adv_loss"
        ]
    }
]