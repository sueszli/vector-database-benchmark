[
    {
        "func_name": "__init__",
        "original": "def __init__(self, data, var_type, bw=None, defaults=None):\n    self.var_type = var_type\n    self.k_vars = len(self.var_type)\n    self.data = _adjust_shape(data, self.k_vars)\n    self.data_type = var_type\n    (self.nobs, self.k_vars) = np.shape(self.data)\n    if self.nobs <= self.k_vars:\n        raise ValueError('The number of observations must be larger than the number of variables.')\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not self.efficient:\n        self.bw = self._compute_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
        "mutated": [
            "def __init__(self, data, var_type, bw=None, defaults=None):\n    if False:\n        i = 10\n    self.var_type = var_type\n    self.k_vars = len(self.var_type)\n    self.data = _adjust_shape(data, self.k_vars)\n    self.data_type = var_type\n    (self.nobs, self.k_vars) = np.shape(self.data)\n    if self.nobs <= self.k_vars:\n        raise ValueError('The number of observations must be larger than the number of variables.')\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not self.efficient:\n        self.bw = self._compute_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, data, var_type, bw=None, defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.var_type = var_type\n    self.k_vars = len(self.var_type)\n    self.data = _adjust_shape(data, self.k_vars)\n    self.data_type = var_type\n    (self.nobs, self.k_vars) = np.shape(self.data)\n    if self.nobs <= self.k_vars:\n        raise ValueError('The number of observations must be larger than the number of variables.')\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not self.efficient:\n        self.bw = self._compute_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, data, var_type, bw=None, defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.var_type = var_type\n    self.k_vars = len(self.var_type)\n    self.data = _adjust_shape(data, self.k_vars)\n    self.data_type = var_type\n    (self.nobs, self.k_vars) = np.shape(self.data)\n    if self.nobs <= self.k_vars:\n        raise ValueError('The number of observations must be larger than the number of variables.')\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not self.efficient:\n        self.bw = self._compute_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, data, var_type, bw=None, defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.var_type = var_type\n    self.k_vars = len(self.var_type)\n    self.data = _adjust_shape(data, self.k_vars)\n    self.data_type = var_type\n    (self.nobs, self.k_vars) = np.shape(self.data)\n    if self.nobs <= self.k_vars:\n        raise ValueError('The number of observations must be larger than the number of variables.')\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not self.efficient:\n        self.bw = self._compute_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, data, var_type, bw=None, defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.var_type = var_type\n    self.k_vars = len(self.var_type)\n    self.data = _adjust_shape(data, self.k_vars)\n    self.data_type = var_type\n    (self.nobs, self.k_vars) = np.shape(self.data)\n    if self.nobs <= self.k_vars:\n        raise ValueError('The number of observations must be larger than the number of variables.')\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not self.efficient:\n        self.bw = self._compute_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)"
        ]
    },
    {
        "func_name": "__repr__",
        "original": "def __repr__(self):\n    \"\"\"Provide something sane to print.\"\"\"\n    rpr = 'KDE instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    return rpr",
        "mutated": [
            "def __repr__(self):\n    if False:\n        i = 10\n    'Provide something sane to print.'\n    rpr = 'KDE instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Provide something sane to print.'\n    rpr = 'KDE instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Provide something sane to print.'\n    rpr = 'KDE instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Provide something sane to print.'\n    rpr = 'KDE instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Provide something sane to print.'\n    rpr = 'KDE instance\\n'\n    rpr += 'Number of variables: k_vars = ' + str(self.k_vars) + '\\n'\n    rpr += 'Number of samples:   nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Variable types:      ' + self.var_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    return rpr"
        ]
    },
    {
        "func_name": "loo_likelihood",
        "original": "def loo_likelihood(self, bw, func=lambda x: x):\n    \"\"\"\n        Returns the leave-one-out likelihood function.\n\n        The leave-one-out likelihood function for the unconditional KDE.\n\n        Parameters\n        ----------\n        bw : array_like\n            The value for the bandwidth parameter(s).\n        func : callable, optional\n            Function to transform the likelihood values (before summing); for\n            the log likelihood, use ``func=np.log``.  Default is ``f(x) = x``.\n\n        Notes\n        -----\n        The leave-one-out kernel estimator of :math:`f_{-i}` is:\n\n        .. math:: f_{-i}(X_{i})=\\\\frac{1}{(n-1)h}\n                    \\\\sum_{j=1,j\\\\neq i}K_{h}(X_{i},X_{j})\n\n        where :math:`K_{h}` represents the generalized product kernel\n        estimator:\n\n        .. math:: K_{h}(X_{i},X_{j}) =\n            \\\\prod_{s=1}^{q}h_{s}^{-1}k\\\\left(\\\\frac{X_{is}-X_{js}}{h_{s}}\\\\right)\n        \"\"\"\n    LOO = LeaveOneOut(self.data)\n    L = 0\n    for (i, X_not_i) in enumerate(LOO):\n        f_i = gpke(bw, data=-X_not_i, data_predict=-self.data[i, :], var_type=self.var_type)\n        L += func(f_i)\n    return -L",
        "mutated": [
            "def loo_likelihood(self, bw, func=lambda x: x):\n    if False:\n        i = 10\n    '\\n        Returns the leave-one-out likelihood function.\\n\\n        The leave-one-out likelihood function for the unconditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The value for the bandwidth parameter(s).\\n        func : callable, optional\\n            Function to transform the likelihood values (before summing); for\\n            the log likelihood, use ``func=np.log``.  Default is ``f(x) = x``.\\n\\n        Notes\\n        -----\\n        The leave-one-out kernel estimator of :math:`f_{-i}` is:\\n\\n        .. math:: f_{-i}(X_{i})=\\\\frac{1}{(n-1)h}\\n                    \\\\sum_{j=1,j\\\\neq i}K_{h}(X_{i},X_{j})\\n\\n        where :math:`K_{h}` represents the generalized product kernel\\n        estimator:\\n\\n        .. math:: K_{h}(X_{i},X_{j}) =\\n            \\\\prod_{s=1}^{q}h_{s}^{-1}k\\\\left(\\\\frac{X_{is}-X_{js}}{h_{s}}\\\\right)\\n        '\n    LOO = LeaveOneOut(self.data)\n    L = 0\n    for (i, X_not_i) in enumerate(LOO):\n        f_i = gpke(bw, data=-X_not_i, data_predict=-self.data[i, :], var_type=self.var_type)\n        L += func(f_i)\n    return -L",
            "def loo_likelihood(self, bw, func=lambda x: x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the leave-one-out likelihood function.\\n\\n        The leave-one-out likelihood function for the unconditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The value for the bandwidth parameter(s).\\n        func : callable, optional\\n            Function to transform the likelihood values (before summing); for\\n            the log likelihood, use ``func=np.log``.  Default is ``f(x) = x``.\\n\\n        Notes\\n        -----\\n        The leave-one-out kernel estimator of :math:`f_{-i}` is:\\n\\n        .. math:: f_{-i}(X_{i})=\\\\frac{1}{(n-1)h}\\n                    \\\\sum_{j=1,j\\\\neq i}K_{h}(X_{i},X_{j})\\n\\n        where :math:`K_{h}` represents the generalized product kernel\\n        estimator:\\n\\n        .. math:: K_{h}(X_{i},X_{j}) =\\n            \\\\prod_{s=1}^{q}h_{s}^{-1}k\\\\left(\\\\frac{X_{is}-X_{js}}{h_{s}}\\\\right)\\n        '\n    LOO = LeaveOneOut(self.data)\n    L = 0\n    for (i, X_not_i) in enumerate(LOO):\n        f_i = gpke(bw, data=-X_not_i, data_predict=-self.data[i, :], var_type=self.var_type)\n        L += func(f_i)\n    return -L",
            "def loo_likelihood(self, bw, func=lambda x: x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the leave-one-out likelihood function.\\n\\n        The leave-one-out likelihood function for the unconditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The value for the bandwidth parameter(s).\\n        func : callable, optional\\n            Function to transform the likelihood values (before summing); for\\n            the log likelihood, use ``func=np.log``.  Default is ``f(x) = x``.\\n\\n        Notes\\n        -----\\n        The leave-one-out kernel estimator of :math:`f_{-i}` is:\\n\\n        .. math:: f_{-i}(X_{i})=\\\\frac{1}{(n-1)h}\\n                    \\\\sum_{j=1,j\\\\neq i}K_{h}(X_{i},X_{j})\\n\\n        where :math:`K_{h}` represents the generalized product kernel\\n        estimator:\\n\\n        .. math:: K_{h}(X_{i},X_{j}) =\\n            \\\\prod_{s=1}^{q}h_{s}^{-1}k\\\\left(\\\\frac{X_{is}-X_{js}}{h_{s}}\\\\right)\\n        '\n    LOO = LeaveOneOut(self.data)\n    L = 0\n    for (i, X_not_i) in enumerate(LOO):\n        f_i = gpke(bw, data=-X_not_i, data_predict=-self.data[i, :], var_type=self.var_type)\n        L += func(f_i)\n    return -L",
            "def loo_likelihood(self, bw, func=lambda x: x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the leave-one-out likelihood function.\\n\\n        The leave-one-out likelihood function for the unconditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The value for the bandwidth parameter(s).\\n        func : callable, optional\\n            Function to transform the likelihood values (before summing); for\\n            the log likelihood, use ``func=np.log``.  Default is ``f(x) = x``.\\n\\n        Notes\\n        -----\\n        The leave-one-out kernel estimator of :math:`f_{-i}` is:\\n\\n        .. math:: f_{-i}(X_{i})=\\\\frac{1}{(n-1)h}\\n                    \\\\sum_{j=1,j\\\\neq i}K_{h}(X_{i},X_{j})\\n\\n        where :math:`K_{h}` represents the generalized product kernel\\n        estimator:\\n\\n        .. math:: K_{h}(X_{i},X_{j}) =\\n            \\\\prod_{s=1}^{q}h_{s}^{-1}k\\\\left(\\\\frac{X_{is}-X_{js}}{h_{s}}\\\\right)\\n        '\n    LOO = LeaveOneOut(self.data)\n    L = 0\n    for (i, X_not_i) in enumerate(LOO):\n        f_i = gpke(bw, data=-X_not_i, data_predict=-self.data[i, :], var_type=self.var_type)\n        L += func(f_i)\n    return -L",
            "def loo_likelihood(self, bw, func=lambda x: x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the leave-one-out likelihood function.\\n\\n        The leave-one-out likelihood function for the unconditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The value for the bandwidth parameter(s).\\n        func : callable, optional\\n            Function to transform the likelihood values (before summing); for\\n            the log likelihood, use ``func=np.log``.  Default is ``f(x) = x``.\\n\\n        Notes\\n        -----\\n        The leave-one-out kernel estimator of :math:`f_{-i}` is:\\n\\n        .. math:: f_{-i}(X_{i})=\\\\frac{1}{(n-1)h}\\n                    \\\\sum_{j=1,j\\\\neq i}K_{h}(X_{i},X_{j})\\n\\n        where :math:`K_{h}` represents the generalized product kernel\\n        estimator:\\n\\n        .. math:: K_{h}(X_{i},X_{j}) =\\n            \\\\prod_{s=1}^{q}h_{s}^{-1}k\\\\left(\\\\frac{X_{is}-X_{js}}{h_{s}}\\\\right)\\n        '\n    LOO = LeaveOneOut(self.data)\n    L = 0\n    for (i, X_not_i) in enumerate(LOO):\n        f_i = gpke(bw, data=-X_not_i, data_predict=-self.data[i, :], var_type=self.var_type)\n        L += func(f_i)\n    return -L"
        ]
    },
    {
        "func_name": "pdf",
        "original": "def pdf(self, data_predict=None):\n    \"\"\"\n        Evaluate the probability density function.\n\n        Parameters\n        ----------\n        data_predict : array_like, optional\n            Points to evaluate at.  If unspecified, the training data is used.\n\n        Returns\n        -------\n        pdf_est : array_like\n            Probability density function evaluated at `data_predict`.\n\n        Notes\n        -----\n        The probability density is given by the generalized product kernel\n        estimator:\n\n        .. math:: K_{h}(X_{i},X_{j}) =\n            \\\\prod_{s=1}^{q}h_{s}^{-1}k\\\\left(\\\\frac{X_{is}-X_{js}}{h_{s}}\\\\right)\n        \"\"\"\n    if data_predict is None:\n        data_predict = self.data\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    pdf_est = []\n    for i in range(np.shape(data_predict)[0]):\n        pdf_est.append(gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.var_type) / self.nobs)\n    pdf_est = np.squeeze(pdf_est)\n    return pdf_est",
        "mutated": [
            "def pdf(self, data_predict=None):\n    if False:\n        i = 10\n    '\\n        Evaluate the probability density function.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points to evaluate at.  If unspecified, the training data is used.\\n\\n        Returns\\n        -------\\n        pdf_est : array_like\\n            Probability density function evaluated at `data_predict`.\\n\\n        Notes\\n        -----\\n        The probability density is given by the generalized product kernel\\n        estimator:\\n\\n        .. math:: K_{h}(X_{i},X_{j}) =\\n            \\\\prod_{s=1}^{q}h_{s}^{-1}k\\\\left(\\\\frac{X_{is}-X_{js}}{h_{s}}\\\\right)\\n        '\n    if data_predict is None:\n        data_predict = self.data\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    pdf_est = []\n    for i in range(np.shape(data_predict)[0]):\n        pdf_est.append(gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.var_type) / self.nobs)\n    pdf_est = np.squeeze(pdf_est)\n    return pdf_est",
            "def pdf(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Evaluate the probability density function.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points to evaluate at.  If unspecified, the training data is used.\\n\\n        Returns\\n        -------\\n        pdf_est : array_like\\n            Probability density function evaluated at `data_predict`.\\n\\n        Notes\\n        -----\\n        The probability density is given by the generalized product kernel\\n        estimator:\\n\\n        .. math:: K_{h}(X_{i},X_{j}) =\\n            \\\\prod_{s=1}^{q}h_{s}^{-1}k\\\\left(\\\\frac{X_{is}-X_{js}}{h_{s}}\\\\right)\\n        '\n    if data_predict is None:\n        data_predict = self.data\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    pdf_est = []\n    for i in range(np.shape(data_predict)[0]):\n        pdf_est.append(gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.var_type) / self.nobs)\n    pdf_est = np.squeeze(pdf_est)\n    return pdf_est",
            "def pdf(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Evaluate the probability density function.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points to evaluate at.  If unspecified, the training data is used.\\n\\n        Returns\\n        -------\\n        pdf_est : array_like\\n            Probability density function evaluated at `data_predict`.\\n\\n        Notes\\n        -----\\n        The probability density is given by the generalized product kernel\\n        estimator:\\n\\n        .. math:: K_{h}(X_{i},X_{j}) =\\n            \\\\prod_{s=1}^{q}h_{s}^{-1}k\\\\left(\\\\frac{X_{is}-X_{js}}{h_{s}}\\\\right)\\n        '\n    if data_predict is None:\n        data_predict = self.data\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    pdf_est = []\n    for i in range(np.shape(data_predict)[0]):\n        pdf_est.append(gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.var_type) / self.nobs)\n    pdf_est = np.squeeze(pdf_est)\n    return pdf_est",
            "def pdf(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Evaluate the probability density function.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points to evaluate at.  If unspecified, the training data is used.\\n\\n        Returns\\n        -------\\n        pdf_est : array_like\\n            Probability density function evaluated at `data_predict`.\\n\\n        Notes\\n        -----\\n        The probability density is given by the generalized product kernel\\n        estimator:\\n\\n        .. math:: K_{h}(X_{i},X_{j}) =\\n            \\\\prod_{s=1}^{q}h_{s}^{-1}k\\\\left(\\\\frac{X_{is}-X_{js}}{h_{s}}\\\\right)\\n        '\n    if data_predict is None:\n        data_predict = self.data\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    pdf_est = []\n    for i in range(np.shape(data_predict)[0]):\n        pdf_est.append(gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.var_type) / self.nobs)\n    pdf_est = np.squeeze(pdf_est)\n    return pdf_est",
            "def pdf(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Evaluate the probability density function.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points to evaluate at.  If unspecified, the training data is used.\\n\\n        Returns\\n        -------\\n        pdf_est : array_like\\n            Probability density function evaluated at `data_predict`.\\n\\n        Notes\\n        -----\\n        The probability density is given by the generalized product kernel\\n        estimator:\\n\\n        .. math:: K_{h}(X_{i},X_{j}) =\\n            \\\\prod_{s=1}^{q}h_{s}^{-1}k\\\\left(\\\\frac{X_{is}-X_{js}}{h_{s}}\\\\right)\\n        '\n    if data_predict is None:\n        data_predict = self.data\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    pdf_est = []\n    for i in range(np.shape(data_predict)[0]):\n        pdf_est.append(gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.var_type) / self.nobs)\n    pdf_est = np.squeeze(pdf_est)\n    return pdf_est"
        ]
    },
    {
        "func_name": "cdf",
        "original": "def cdf(self, data_predict=None):\n    \"\"\"\n        Evaluate the cumulative distribution function.\n\n        Parameters\n        ----------\n        data_predict : array_like, optional\n            Points to evaluate at.  If unspecified, the training data is used.\n\n        Returns\n        -------\n        cdf_est : array_like\n            The estimate of the cdf.\n\n        Notes\n        -----\n        See https://en.wikipedia.org/wiki/Cumulative_distribution_function\n        For more details on the estimation see Ref. [5] in module docstring.\n\n        The multivariate CDF for mixed data (continuous and ordered/unordered\n        discrete) is estimated by:\n\n        .. math::\n\n            F(x^{c},x^{d})=n^{-1}\\\\sum_{i=1}^{n}\\\\left[G(\\\\frac{x^{c}-X_{i}}{h})\\\\sum_{u\\\\leq x^{d}}L(X_{i}^{d},x_{i}^{d}, \\\\lambda)\\\\right]\n\n        where G() is the product kernel CDF estimator for the continuous\n        and L() for the discrete variables.\n\n        Used bandwidth is ``self.bw``.\n        \"\"\"\n    if data_predict is None:\n        data_predict = self.data\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    cdf_est = []\n    for i in range(np.shape(data_predict)[0]):\n        cdf_est.append(gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.var_type, ckertype='gaussian_cdf', ukertype='aitchisonaitken_cdf', okertype='wangryzin_cdf') / self.nobs)\n    cdf_est = np.squeeze(cdf_est)\n    return cdf_est",
        "mutated": [
            "def cdf(self, data_predict=None):\n    if False:\n        i = 10\n    '\\n        Evaluate the cumulative distribution function.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points to evaluate at.  If unspecified, the training data is used.\\n\\n        Returns\\n        -------\\n        cdf_est : array_like\\n            The estimate of the cdf.\\n\\n        Notes\\n        -----\\n        See https://en.wikipedia.org/wiki/Cumulative_distribution_function\\n        For more details on the estimation see Ref. [5] in module docstring.\\n\\n        The multivariate CDF for mixed data (continuous and ordered/unordered\\n        discrete) is estimated by:\\n\\n        .. math::\\n\\n            F(x^{c},x^{d})=n^{-1}\\\\sum_{i=1}^{n}\\\\left[G(\\\\frac{x^{c}-X_{i}}{h})\\\\sum_{u\\\\leq x^{d}}L(X_{i}^{d},x_{i}^{d}, \\\\lambda)\\\\right]\\n\\n        where G() is the product kernel CDF estimator for the continuous\\n        and L() for the discrete variables.\\n\\n        Used bandwidth is ``self.bw``.\\n        '\n    if data_predict is None:\n        data_predict = self.data\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    cdf_est = []\n    for i in range(np.shape(data_predict)[0]):\n        cdf_est.append(gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.var_type, ckertype='gaussian_cdf', ukertype='aitchisonaitken_cdf', okertype='wangryzin_cdf') / self.nobs)\n    cdf_est = np.squeeze(cdf_est)\n    return cdf_est",
            "def cdf(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Evaluate the cumulative distribution function.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points to evaluate at.  If unspecified, the training data is used.\\n\\n        Returns\\n        -------\\n        cdf_est : array_like\\n            The estimate of the cdf.\\n\\n        Notes\\n        -----\\n        See https://en.wikipedia.org/wiki/Cumulative_distribution_function\\n        For more details on the estimation see Ref. [5] in module docstring.\\n\\n        The multivariate CDF for mixed data (continuous and ordered/unordered\\n        discrete) is estimated by:\\n\\n        .. math::\\n\\n            F(x^{c},x^{d})=n^{-1}\\\\sum_{i=1}^{n}\\\\left[G(\\\\frac{x^{c}-X_{i}}{h})\\\\sum_{u\\\\leq x^{d}}L(X_{i}^{d},x_{i}^{d}, \\\\lambda)\\\\right]\\n\\n        where G() is the product kernel CDF estimator for the continuous\\n        and L() for the discrete variables.\\n\\n        Used bandwidth is ``self.bw``.\\n        '\n    if data_predict is None:\n        data_predict = self.data\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    cdf_est = []\n    for i in range(np.shape(data_predict)[0]):\n        cdf_est.append(gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.var_type, ckertype='gaussian_cdf', ukertype='aitchisonaitken_cdf', okertype='wangryzin_cdf') / self.nobs)\n    cdf_est = np.squeeze(cdf_est)\n    return cdf_est",
            "def cdf(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Evaluate the cumulative distribution function.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points to evaluate at.  If unspecified, the training data is used.\\n\\n        Returns\\n        -------\\n        cdf_est : array_like\\n            The estimate of the cdf.\\n\\n        Notes\\n        -----\\n        See https://en.wikipedia.org/wiki/Cumulative_distribution_function\\n        For more details on the estimation see Ref. [5] in module docstring.\\n\\n        The multivariate CDF for mixed data (continuous and ordered/unordered\\n        discrete) is estimated by:\\n\\n        .. math::\\n\\n            F(x^{c},x^{d})=n^{-1}\\\\sum_{i=1}^{n}\\\\left[G(\\\\frac{x^{c}-X_{i}}{h})\\\\sum_{u\\\\leq x^{d}}L(X_{i}^{d},x_{i}^{d}, \\\\lambda)\\\\right]\\n\\n        where G() is the product kernel CDF estimator for the continuous\\n        and L() for the discrete variables.\\n\\n        Used bandwidth is ``self.bw``.\\n        '\n    if data_predict is None:\n        data_predict = self.data\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    cdf_est = []\n    for i in range(np.shape(data_predict)[0]):\n        cdf_est.append(gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.var_type, ckertype='gaussian_cdf', ukertype='aitchisonaitken_cdf', okertype='wangryzin_cdf') / self.nobs)\n    cdf_est = np.squeeze(cdf_est)\n    return cdf_est",
            "def cdf(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Evaluate the cumulative distribution function.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points to evaluate at.  If unspecified, the training data is used.\\n\\n        Returns\\n        -------\\n        cdf_est : array_like\\n            The estimate of the cdf.\\n\\n        Notes\\n        -----\\n        See https://en.wikipedia.org/wiki/Cumulative_distribution_function\\n        For more details on the estimation see Ref. [5] in module docstring.\\n\\n        The multivariate CDF for mixed data (continuous and ordered/unordered\\n        discrete) is estimated by:\\n\\n        .. math::\\n\\n            F(x^{c},x^{d})=n^{-1}\\\\sum_{i=1}^{n}\\\\left[G(\\\\frac{x^{c}-X_{i}}{h})\\\\sum_{u\\\\leq x^{d}}L(X_{i}^{d},x_{i}^{d}, \\\\lambda)\\\\right]\\n\\n        where G() is the product kernel CDF estimator for the continuous\\n        and L() for the discrete variables.\\n\\n        Used bandwidth is ``self.bw``.\\n        '\n    if data_predict is None:\n        data_predict = self.data\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    cdf_est = []\n    for i in range(np.shape(data_predict)[0]):\n        cdf_est.append(gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.var_type, ckertype='gaussian_cdf', ukertype='aitchisonaitken_cdf', okertype='wangryzin_cdf') / self.nobs)\n    cdf_est = np.squeeze(cdf_est)\n    return cdf_est",
            "def cdf(self, data_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Evaluate the cumulative distribution function.\\n\\n        Parameters\\n        ----------\\n        data_predict : array_like, optional\\n            Points to evaluate at.  If unspecified, the training data is used.\\n\\n        Returns\\n        -------\\n        cdf_est : array_like\\n            The estimate of the cdf.\\n\\n        Notes\\n        -----\\n        See https://en.wikipedia.org/wiki/Cumulative_distribution_function\\n        For more details on the estimation see Ref. [5] in module docstring.\\n\\n        The multivariate CDF for mixed data (continuous and ordered/unordered\\n        discrete) is estimated by:\\n\\n        .. math::\\n\\n            F(x^{c},x^{d})=n^{-1}\\\\sum_{i=1}^{n}\\\\left[G(\\\\frac{x^{c}-X_{i}}{h})\\\\sum_{u\\\\leq x^{d}}L(X_{i}^{d},x_{i}^{d}, \\\\lambda)\\\\right]\\n\\n        where G() is the product kernel CDF estimator for the continuous\\n        and L() for the discrete variables.\\n\\n        Used bandwidth is ``self.bw``.\\n        '\n    if data_predict is None:\n        data_predict = self.data\n    else:\n        data_predict = _adjust_shape(data_predict, self.k_vars)\n    cdf_est = []\n    for i in range(np.shape(data_predict)[0]):\n        cdf_est.append(gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.var_type, ckertype='gaussian_cdf', ukertype='aitchisonaitken_cdf', okertype='wangryzin_cdf') / self.nobs)\n    cdf_est = np.squeeze(cdf_est)\n    return cdf_est"
        ]
    },
    {
        "func_name": "imse",
        "original": "def imse(self, bw):\n    \"\"\"\n        Returns the Integrated Mean Square Error for the unconditional KDE.\n\n        Parameters\n        ----------\n        bw : array_like\n            The bandwidth parameter(s).\n\n        Returns\n        -------\n        CV : float\n            The cross-validation objective function.\n\n        Notes\n        -----\n        See p. 27 in [1]_ for details on how to handle the multivariate\n        estimation with mixed data types see p.6 in [2]_.\n\n        The formula for the cross-validation objective function is:\n\n        .. math:: CV=\\\\frac{1}{n^{2}}\\\\sum_{i=1}^{n}\\\\sum_{j=1}^{N}\n            \\\\bar{K}_{h}(X_{i},X_{j})-\\\\frac{2}{n(n-1)}\\\\sum_{i=1}^{n}\n            \\\\sum_{j=1,j\\\\neq i}^{N}K_{h}(X_{i},X_{j})\n\n        Where :math:`\\\\bar{K}_{h}` is the multivariate product convolution\n        kernel (consult [2]_ for mixed data types).\n\n        References\n        ----------\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\n                practice. Princeton University Press. (2007)\n        .. [2] Racine, J., Li, Q. \"Nonparametric Estimation of Distributions\n                with Categorical and Continuous Data.\" Working Paper. (2000)\n        \"\"\"\n    F = 0\n    kertypes = dict(c=kernels.gaussian_convolution, o=kernels.wang_ryzin_convolution, u=kernels.aitchison_aitken_convolution)\n    nobs = self.nobs\n    data = -self.data\n    var_type = self.var_type\n    ix_cont = np.array([c == 'c' for c in var_type])\n    _bw_cont_product = bw[ix_cont].prod()\n    Kval = np.empty(data.shape)\n    for i in range(nobs):\n        for (ii, vtype) in enumerate(var_type):\n            Kval[:, ii] = kertypes[vtype](bw[ii], data[:, ii], data[i, ii])\n        dens = Kval.prod(axis=1) / _bw_cont_product\n        k_bar_sum = dens.sum(axis=0)\n        F += k_bar_sum\n    kertypes = dict(c=kernels.gaussian, o=kernels.wang_ryzin, u=kernels.aitchison_aitken)\n    LOO = LeaveOneOut(self.data)\n    L = 0\n    Kval = np.empty((data.shape[0] - 1, data.shape[1]))\n    for (i, X_not_i) in enumerate(LOO):\n        for (ii, vtype) in enumerate(var_type):\n            Kval[:, ii] = kertypes[vtype](bw[ii], -X_not_i[:, ii], data[i, ii])\n        dens = Kval.prod(axis=1) / _bw_cont_product\n        L += dens.sum(axis=0)\n    return F / nobs ** 2 - 2 * L / (nobs * (nobs - 1))",
        "mutated": [
            "def imse(self, bw):\n    if False:\n        i = 10\n    '\\n        Returns the Integrated Mean Square Error for the unconditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n\\n        Returns\\n        -------\\n        CV : float\\n            The cross-validation objective function.\\n\\n        Notes\\n        -----\\n        See p. 27 in [1]_ for details on how to handle the multivariate\\n        estimation with mixed data types see p.6 in [2]_.\\n\\n        The formula for the cross-validation objective function is:\\n\\n        .. math:: CV=\\\\frac{1}{n^{2}}\\\\sum_{i=1}^{n}\\\\sum_{j=1}^{N}\\n            \\\\bar{K}_{h}(X_{i},X_{j})-\\\\frac{2}{n(n-1)}\\\\sum_{i=1}^{n}\\n            \\\\sum_{j=1,j\\\\neq i}^{N}K_{h}(X_{i},X_{j})\\n\\n        Where :math:`\\\\bar{K}_{h}` is the multivariate product convolution\\n        kernel (consult [2]_ for mixed data types).\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Racine, J., Li, Q. \"Nonparametric Estimation of Distributions\\n                with Categorical and Continuous Data.\" Working Paper. (2000)\\n        '\n    F = 0\n    kertypes = dict(c=kernels.gaussian_convolution, o=kernels.wang_ryzin_convolution, u=kernels.aitchison_aitken_convolution)\n    nobs = self.nobs\n    data = -self.data\n    var_type = self.var_type\n    ix_cont = np.array([c == 'c' for c in var_type])\n    _bw_cont_product = bw[ix_cont].prod()\n    Kval = np.empty(data.shape)\n    for i in range(nobs):\n        for (ii, vtype) in enumerate(var_type):\n            Kval[:, ii] = kertypes[vtype](bw[ii], data[:, ii], data[i, ii])\n        dens = Kval.prod(axis=1) / _bw_cont_product\n        k_bar_sum = dens.sum(axis=0)\n        F += k_bar_sum\n    kertypes = dict(c=kernels.gaussian, o=kernels.wang_ryzin, u=kernels.aitchison_aitken)\n    LOO = LeaveOneOut(self.data)\n    L = 0\n    Kval = np.empty((data.shape[0] - 1, data.shape[1]))\n    for (i, X_not_i) in enumerate(LOO):\n        for (ii, vtype) in enumerate(var_type):\n            Kval[:, ii] = kertypes[vtype](bw[ii], -X_not_i[:, ii], data[i, ii])\n        dens = Kval.prod(axis=1) / _bw_cont_product\n        L += dens.sum(axis=0)\n    return F / nobs ** 2 - 2 * L / (nobs * (nobs - 1))",
            "def imse(self, bw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Returns the Integrated Mean Square Error for the unconditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n\\n        Returns\\n        -------\\n        CV : float\\n            The cross-validation objective function.\\n\\n        Notes\\n        -----\\n        See p. 27 in [1]_ for details on how to handle the multivariate\\n        estimation with mixed data types see p.6 in [2]_.\\n\\n        The formula for the cross-validation objective function is:\\n\\n        .. math:: CV=\\\\frac{1}{n^{2}}\\\\sum_{i=1}^{n}\\\\sum_{j=1}^{N}\\n            \\\\bar{K}_{h}(X_{i},X_{j})-\\\\frac{2}{n(n-1)}\\\\sum_{i=1}^{n}\\n            \\\\sum_{j=1,j\\\\neq i}^{N}K_{h}(X_{i},X_{j})\\n\\n        Where :math:`\\\\bar{K}_{h}` is the multivariate product convolution\\n        kernel (consult [2]_ for mixed data types).\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Racine, J., Li, Q. \"Nonparametric Estimation of Distributions\\n                with Categorical and Continuous Data.\" Working Paper. (2000)\\n        '\n    F = 0\n    kertypes = dict(c=kernels.gaussian_convolution, o=kernels.wang_ryzin_convolution, u=kernels.aitchison_aitken_convolution)\n    nobs = self.nobs\n    data = -self.data\n    var_type = self.var_type\n    ix_cont = np.array([c == 'c' for c in var_type])\n    _bw_cont_product = bw[ix_cont].prod()\n    Kval = np.empty(data.shape)\n    for i in range(nobs):\n        for (ii, vtype) in enumerate(var_type):\n            Kval[:, ii] = kertypes[vtype](bw[ii], data[:, ii], data[i, ii])\n        dens = Kval.prod(axis=1) / _bw_cont_product\n        k_bar_sum = dens.sum(axis=0)\n        F += k_bar_sum\n    kertypes = dict(c=kernels.gaussian, o=kernels.wang_ryzin, u=kernels.aitchison_aitken)\n    LOO = LeaveOneOut(self.data)\n    L = 0\n    Kval = np.empty((data.shape[0] - 1, data.shape[1]))\n    for (i, X_not_i) in enumerate(LOO):\n        for (ii, vtype) in enumerate(var_type):\n            Kval[:, ii] = kertypes[vtype](bw[ii], -X_not_i[:, ii], data[i, ii])\n        dens = Kval.prod(axis=1) / _bw_cont_product\n        L += dens.sum(axis=0)\n    return F / nobs ** 2 - 2 * L / (nobs * (nobs - 1))",
            "def imse(self, bw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Returns the Integrated Mean Square Error for the unconditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n\\n        Returns\\n        -------\\n        CV : float\\n            The cross-validation objective function.\\n\\n        Notes\\n        -----\\n        See p. 27 in [1]_ for details on how to handle the multivariate\\n        estimation with mixed data types see p.6 in [2]_.\\n\\n        The formula for the cross-validation objective function is:\\n\\n        .. math:: CV=\\\\frac{1}{n^{2}}\\\\sum_{i=1}^{n}\\\\sum_{j=1}^{N}\\n            \\\\bar{K}_{h}(X_{i},X_{j})-\\\\frac{2}{n(n-1)}\\\\sum_{i=1}^{n}\\n            \\\\sum_{j=1,j\\\\neq i}^{N}K_{h}(X_{i},X_{j})\\n\\n        Where :math:`\\\\bar{K}_{h}` is the multivariate product convolution\\n        kernel (consult [2]_ for mixed data types).\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Racine, J., Li, Q. \"Nonparametric Estimation of Distributions\\n                with Categorical and Continuous Data.\" Working Paper. (2000)\\n        '\n    F = 0\n    kertypes = dict(c=kernels.gaussian_convolution, o=kernels.wang_ryzin_convolution, u=kernels.aitchison_aitken_convolution)\n    nobs = self.nobs\n    data = -self.data\n    var_type = self.var_type\n    ix_cont = np.array([c == 'c' for c in var_type])\n    _bw_cont_product = bw[ix_cont].prod()\n    Kval = np.empty(data.shape)\n    for i in range(nobs):\n        for (ii, vtype) in enumerate(var_type):\n            Kval[:, ii] = kertypes[vtype](bw[ii], data[:, ii], data[i, ii])\n        dens = Kval.prod(axis=1) / _bw_cont_product\n        k_bar_sum = dens.sum(axis=0)\n        F += k_bar_sum\n    kertypes = dict(c=kernels.gaussian, o=kernels.wang_ryzin, u=kernels.aitchison_aitken)\n    LOO = LeaveOneOut(self.data)\n    L = 0\n    Kval = np.empty((data.shape[0] - 1, data.shape[1]))\n    for (i, X_not_i) in enumerate(LOO):\n        for (ii, vtype) in enumerate(var_type):\n            Kval[:, ii] = kertypes[vtype](bw[ii], -X_not_i[:, ii], data[i, ii])\n        dens = Kval.prod(axis=1) / _bw_cont_product\n        L += dens.sum(axis=0)\n    return F / nobs ** 2 - 2 * L / (nobs * (nobs - 1))",
            "def imse(self, bw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Returns the Integrated Mean Square Error for the unconditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n\\n        Returns\\n        -------\\n        CV : float\\n            The cross-validation objective function.\\n\\n        Notes\\n        -----\\n        See p. 27 in [1]_ for details on how to handle the multivariate\\n        estimation with mixed data types see p.6 in [2]_.\\n\\n        The formula for the cross-validation objective function is:\\n\\n        .. math:: CV=\\\\frac{1}{n^{2}}\\\\sum_{i=1}^{n}\\\\sum_{j=1}^{N}\\n            \\\\bar{K}_{h}(X_{i},X_{j})-\\\\frac{2}{n(n-1)}\\\\sum_{i=1}^{n}\\n            \\\\sum_{j=1,j\\\\neq i}^{N}K_{h}(X_{i},X_{j})\\n\\n        Where :math:`\\\\bar{K}_{h}` is the multivariate product convolution\\n        kernel (consult [2]_ for mixed data types).\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Racine, J., Li, Q. \"Nonparametric Estimation of Distributions\\n                with Categorical and Continuous Data.\" Working Paper. (2000)\\n        '\n    F = 0\n    kertypes = dict(c=kernels.gaussian_convolution, o=kernels.wang_ryzin_convolution, u=kernels.aitchison_aitken_convolution)\n    nobs = self.nobs\n    data = -self.data\n    var_type = self.var_type\n    ix_cont = np.array([c == 'c' for c in var_type])\n    _bw_cont_product = bw[ix_cont].prod()\n    Kval = np.empty(data.shape)\n    for i in range(nobs):\n        for (ii, vtype) in enumerate(var_type):\n            Kval[:, ii] = kertypes[vtype](bw[ii], data[:, ii], data[i, ii])\n        dens = Kval.prod(axis=1) / _bw_cont_product\n        k_bar_sum = dens.sum(axis=0)\n        F += k_bar_sum\n    kertypes = dict(c=kernels.gaussian, o=kernels.wang_ryzin, u=kernels.aitchison_aitken)\n    LOO = LeaveOneOut(self.data)\n    L = 0\n    Kval = np.empty((data.shape[0] - 1, data.shape[1]))\n    for (i, X_not_i) in enumerate(LOO):\n        for (ii, vtype) in enumerate(var_type):\n            Kval[:, ii] = kertypes[vtype](bw[ii], -X_not_i[:, ii], data[i, ii])\n        dens = Kval.prod(axis=1) / _bw_cont_product\n        L += dens.sum(axis=0)\n    return F / nobs ** 2 - 2 * L / (nobs * (nobs - 1))",
            "def imse(self, bw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Returns the Integrated Mean Square Error for the unconditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n\\n        Returns\\n        -------\\n        CV : float\\n            The cross-validation objective function.\\n\\n        Notes\\n        -----\\n        See p. 27 in [1]_ for details on how to handle the multivariate\\n        estimation with mixed data types see p.6 in [2]_.\\n\\n        The formula for the cross-validation objective function is:\\n\\n        .. math:: CV=\\\\frac{1}{n^{2}}\\\\sum_{i=1}^{n}\\\\sum_{j=1}^{N}\\n            \\\\bar{K}_{h}(X_{i},X_{j})-\\\\frac{2}{n(n-1)}\\\\sum_{i=1}^{n}\\n            \\\\sum_{j=1,j\\\\neq i}^{N}K_{h}(X_{i},X_{j})\\n\\n        Where :math:`\\\\bar{K}_{h}` is the multivariate product convolution\\n        kernel (consult [2]_ for mixed data types).\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Racine, J., Li, Q. \"Nonparametric Estimation of Distributions\\n                with Categorical and Continuous Data.\" Working Paper. (2000)\\n        '\n    F = 0\n    kertypes = dict(c=kernels.gaussian_convolution, o=kernels.wang_ryzin_convolution, u=kernels.aitchison_aitken_convolution)\n    nobs = self.nobs\n    data = -self.data\n    var_type = self.var_type\n    ix_cont = np.array([c == 'c' for c in var_type])\n    _bw_cont_product = bw[ix_cont].prod()\n    Kval = np.empty(data.shape)\n    for i in range(nobs):\n        for (ii, vtype) in enumerate(var_type):\n            Kval[:, ii] = kertypes[vtype](bw[ii], data[:, ii], data[i, ii])\n        dens = Kval.prod(axis=1) / _bw_cont_product\n        k_bar_sum = dens.sum(axis=0)\n        F += k_bar_sum\n    kertypes = dict(c=kernels.gaussian, o=kernels.wang_ryzin, u=kernels.aitchison_aitken)\n    LOO = LeaveOneOut(self.data)\n    L = 0\n    Kval = np.empty((data.shape[0] - 1, data.shape[1]))\n    for (i, X_not_i) in enumerate(LOO):\n        for (ii, vtype) in enumerate(var_type):\n            Kval[:, ii] = kertypes[vtype](bw[ii], -X_not_i[:, ii], data[i, ii])\n        dens = Kval.prod(axis=1) / _bw_cont_product\n        L += dens.sum(axis=0)\n    return F / nobs ** 2 - 2 * L / (nobs * (nobs - 1))"
        ]
    },
    {
        "func_name": "_get_class_vars_type",
        "original": "def _get_class_vars_type(self):\n    \"\"\"Helper method to be able to pass needed vars to _compute_subset.\"\"\"\n    class_type = 'KDEMultivariate'\n    class_vars = (self.var_type,)\n    return (class_type, class_vars)",
        "mutated": [
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KDEMultivariate'\n    class_vars = (self.var_type,)\n    return (class_type, class_vars)",
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KDEMultivariate'\n    class_vars = (self.var_type,)\n    return (class_type, class_vars)",
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KDEMultivariate'\n    class_vars = (self.var_type,)\n    return (class_type, class_vars)",
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KDEMultivariate'\n    class_vars = (self.var_type,)\n    return (class_type, class_vars)",
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KDEMultivariate'\n    class_vars = (self.var_type,)\n    return (class_type, class_vars)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, endog, exog, dep_type, indep_type, bw, defaults=None):\n    self.dep_type = dep_type\n    self.indep_type = indep_type\n    self.data_type = dep_type + indep_type\n    self.k_dep = len(self.dep_type)\n    self.k_indep = len(self.indep_type)\n    self.endog = _adjust_shape(endog, self.k_dep)\n    self.exog = _adjust_shape(exog, self.k_indep)\n    (self.nobs, self.k_dep) = np.shape(self.endog)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.k_vars = np.shape(self.data)[1]\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not self.efficient:\n        self.bw = self._compute_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
        "mutated": [
            "def __init__(self, endog, exog, dep_type, indep_type, bw, defaults=None):\n    if False:\n        i = 10\n    self.dep_type = dep_type\n    self.indep_type = indep_type\n    self.data_type = dep_type + indep_type\n    self.k_dep = len(self.dep_type)\n    self.k_indep = len(self.indep_type)\n    self.endog = _adjust_shape(endog, self.k_dep)\n    self.exog = _adjust_shape(exog, self.k_indep)\n    (self.nobs, self.k_dep) = np.shape(self.endog)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.k_vars = np.shape(self.data)[1]\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not self.efficient:\n        self.bw = self._compute_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, endog, exog, dep_type, indep_type, bw, defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.dep_type = dep_type\n    self.indep_type = indep_type\n    self.data_type = dep_type + indep_type\n    self.k_dep = len(self.dep_type)\n    self.k_indep = len(self.indep_type)\n    self.endog = _adjust_shape(endog, self.k_dep)\n    self.exog = _adjust_shape(exog, self.k_indep)\n    (self.nobs, self.k_dep) = np.shape(self.endog)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.k_vars = np.shape(self.data)[1]\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not self.efficient:\n        self.bw = self._compute_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, endog, exog, dep_type, indep_type, bw, defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.dep_type = dep_type\n    self.indep_type = indep_type\n    self.data_type = dep_type + indep_type\n    self.k_dep = len(self.dep_type)\n    self.k_indep = len(self.indep_type)\n    self.endog = _adjust_shape(endog, self.k_dep)\n    self.exog = _adjust_shape(exog, self.k_indep)\n    (self.nobs, self.k_dep) = np.shape(self.endog)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.k_vars = np.shape(self.data)[1]\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not self.efficient:\n        self.bw = self._compute_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, endog, exog, dep_type, indep_type, bw, defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.dep_type = dep_type\n    self.indep_type = indep_type\n    self.data_type = dep_type + indep_type\n    self.k_dep = len(self.dep_type)\n    self.k_indep = len(self.indep_type)\n    self.endog = _adjust_shape(endog, self.k_dep)\n    self.exog = _adjust_shape(exog, self.k_indep)\n    (self.nobs, self.k_dep) = np.shape(self.endog)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.k_vars = np.shape(self.data)[1]\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not self.efficient:\n        self.bw = self._compute_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)",
            "def __init__(self, endog, exog, dep_type, indep_type, bw, defaults=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.dep_type = dep_type\n    self.indep_type = indep_type\n    self.data_type = dep_type + indep_type\n    self.k_dep = len(self.dep_type)\n    self.k_indep = len(self.indep_type)\n    self.endog = _adjust_shape(endog, self.k_dep)\n    self.exog = _adjust_shape(exog, self.k_indep)\n    (self.nobs, self.k_dep) = np.shape(self.endog)\n    self.data = np.column_stack((self.endog, self.exog))\n    self.k_vars = np.shape(self.data)[1]\n    defaults = EstimatorSettings() if defaults is None else defaults\n    self._set_defaults(defaults)\n    if not self.efficient:\n        self.bw = self._compute_bw(bw)\n    else:\n        self.bw = self._compute_efficient(bw)"
        ]
    },
    {
        "func_name": "__repr__",
        "original": "def __repr__(self):\n    \"\"\"Provide something sane to print.\"\"\"\n    rpr = 'KDEMultivariateConditional instance\\n'\n    rpr += 'Number of independent variables: k_indep = ' + str(self.k_indep) + '\\n'\n    rpr += 'Number of dependent variables: k_dep = ' + str(self.k_dep) + '\\n'\n    rpr += 'Number of observations: nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Independent variable types:      ' + self.indep_type + '\\n'\n    rpr += 'Dependent variable types:      ' + self.dep_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    return rpr",
        "mutated": [
            "def __repr__(self):\n    if False:\n        i = 10\n    'Provide something sane to print.'\n    rpr = 'KDEMultivariateConditional instance\\n'\n    rpr += 'Number of independent variables: k_indep = ' + str(self.k_indep) + '\\n'\n    rpr += 'Number of dependent variables: k_dep = ' + str(self.k_dep) + '\\n'\n    rpr += 'Number of observations: nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Independent variable types:      ' + self.indep_type + '\\n'\n    rpr += 'Dependent variable types:      ' + self.dep_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Provide something sane to print.'\n    rpr = 'KDEMultivariateConditional instance\\n'\n    rpr += 'Number of independent variables: k_indep = ' + str(self.k_indep) + '\\n'\n    rpr += 'Number of dependent variables: k_dep = ' + str(self.k_dep) + '\\n'\n    rpr += 'Number of observations: nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Independent variable types:      ' + self.indep_type + '\\n'\n    rpr += 'Dependent variable types:      ' + self.dep_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Provide something sane to print.'\n    rpr = 'KDEMultivariateConditional instance\\n'\n    rpr += 'Number of independent variables: k_indep = ' + str(self.k_indep) + '\\n'\n    rpr += 'Number of dependent variables: k_dep = ' + str(self.k_dep) + '\\n'\n    rpr += 'Number of observations: nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Independent variable types:      ' + self.indep_type + '\\n'\n    rpr += 'Dependent variable types:      ' + self.dep_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Provide something sane to print.'\n    rpr = 'KDEMultivariateConditional instance\\n'\n    rpr += 'Number of independent variables: k_indep = ' + str(self.k_indep) + '\\n'\n    rpr += 'Number of dependent variables: k_dep = ' + str(self.k_dep) + '\\n'\n    rpr += 'Number of observations: nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Independent variable types:      ' + self.indep_type + '\\n'\n    rpr += 'Dependent variable types:      ' + self.dep_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    return rpr",
            "def __repr__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Provide something sane to print.'\n    rpr = 'KDEMultivariateConditional instance\\n'\n    rpr += 'Number of independent variables: k_indep = ' + str(self.k_indep) + '\\n'\n    rpr += 'Number of dependent variables: k_dep = ' + str(self.k_dep) + '\\n'\n    rpr += 'Number of observations: nobs = ' + str(self.nobs) + '\\n'\n    rpr += 'Independent variable types:      ' + self.indep_type + '\\n'\n    rpr += 'Dependent variable types:      ' + self.dep_type + '\\n'\n    rpr += 'BW selection method: ' + self._bw_method + '\\n'\n    return rpr"
        ]
    },
    {
        "func_name": "loo_likelihood",
        "original": "def loo_likelihood(self, bw, func=lambda x: x):\n    \"\"\"\n        Returns the leave-one-out conditional likelihood of the data.\n\n        If `func` is not equal to the default, what's calculated is a function\n        of the leave-one-out conditional likelihood.\n\n        Parameters\n        ----------\n        bw : array_like\n            The bandwidth parameter(s).\n        func : callable, optional\n            Function to transform the likelihood values (before summing); for\n            the log likelihood, use ``func=np.log``.  Default is ``f(x) = x``.\n\n        Returns\n        -------\n        L : float\n            The value of the leave-one-out function for the data.\n\n        Notes\n        -----\n        Similar to ``KDE.loo_likelihood`, but substitute ``f(y|x)=f(x,y)/f(x)``\n        for ``f(x)``.\n        \"\"\"\n    yLOO = LeaveOneOut(self.data)\n    xLOO = LeaveOneOut(self.exog).__iter__()\n    L = 0\n    for (i, Y_j) in enumerate(yLOO):\n        X_not_i = next(xLOO)\n        f_yx = gpke(bw, data=-Y_j, data_predict=-self.data[i, :], var_type=self.dep_type + self.indep_type)\n        f_x = gpke(bw[self.k_dep:], data=-X_not_i, data_predict=-self.exog[i, :], var_type=self.indep_type)\n        f_i = f_yx / f_x\n        L += func(f_i)\n    return -L",
        "mutated": [
            "def loo_likelihood(self, bw, func=lambda x: x):\n    if False:\n        i = 10\n    \"\\n        Returns the leave-one-out conditional likelihood of the data.\\n\\n        If `func` is not equal to the default, what's calculated is a function\\n        of the leave-one-out conditional likelihood.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n        func : callable, optional\\n            Function to transform the likelihood values (before summing); for\\n            the log likelihood, use ``func=np.log``.  Default is ``f(x) = x``.\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the leave-one-out function for the data.\\n\\n        Notes\\n        -----\\n        Similar to ``KDE.loo_likelihood`, but substitute ``f(y|x)=f(x,y)/f(x)``\\n        for ``f(x)``.\\n        \"\n    yLOO = LeaveOneOut(self.data)\n    xLOO = LeaveOneOut(self.exog).__iter__()\n    L = 0\n    for (i, Y_j) in enumerate(yLOO):\n        X_not_i = next(xLOO)\n        f_yx = gpke(bw, data=-Y_j, data_predict=-self.data[i, :], var_type=self.dep_type + self.indep_type)\n        f_x = gpke(bw[self.k_dep:], data=-X_not_i, data_predict=-self.exog[i, :], var_type=self.indep_type)\n        f_i = f_yx / f_x\n        L += func(f_i)\n    return -L",
            "def loo_likelihood(self, bw, func=lambda x: x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Returns the leave-one-out conditional likelihood of the data.\\n\\n        If `func` is not equal to the default, what's calculated is a function\\n        of the leave-one-out conditional likelihood.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n        func : callable, optional\\n            Function to transform the likelihood values (before summing); for\\n            the log likelihood, use ``func=np.log``.  Default is ``f(x) = x``.\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the leave-one-out function for the data.\\n\\n        Notes\\n        -----\\n        Similar to ``KDE.loo_likelihood`, but substitute ``f(y|x)=f(x,y)/f(x)``\\n        for ``f(x)``.\\n        \"\n    yLOO = LeaveOneOut(self.data)\n    xLOO = LeaveOneOut(self.exog).__iter__()\n    L = 0\n    for (i, Y_j) in enumerate(yLOO):\n        X_not_i = next(xLOO)\n        f_yx = gpke(bw, data=-Y_j, data_predict=-self.data[i, :], var_type=self.dep_type + self.indep_type)\n        f_x = gpke(bw[self.k_dep:], data=-X_not_i, data_predict=-self.exog[i, :], var_type=self.indep_type)\n        f_i = f_yx / f_x\n        L += func(f_i)\n    return -L",
            "def loo_likelihood(self, bw, func=lambda x: x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Returns the leave-one-out conditional likelihood of the data.\\n\\n        If `func` is not equal to the default, what's calculated is a function\\n        of the leave-one-out conditional likelihood.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n        func : callable, optional\\n            Function to transform the likelihood values (before summing); for\\n            the log likelihood, use ``func=np.log``.  Default is ``f(x) = x``.\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the leave-one-out function for the data.\\n\\n        Notes\\n        -----\\n        Similar to ``KDE.loo_likelihood`, but substitute ``f(y|x)=f(x,y)/f(x)``\\n        for ``f(x)``.\\n        \"\n    yLOO = LeaveOneOut(self.data)\n    xLOO = LeaveOneOut(self.exog).__iter__()\n    L = 0\n    for (i, Y_j) in enumerate(yLOO):\n        X_not_i = next(xLOO)\n        f_yx = gpke(bw, data=-Y_j, data_predict=-self.data[i, :], var_type=self.dep_type + self.indep_type)\n        f_x = gpke(bw[self.k_dep:], data=-X_not_i, data_predict=-self.exog[i, :], var_type=self.indep_type)\n        f_i = f_yx / f_x\n        L += func(f_i)\n    return -L",
            "def loo_likelihood(self, bw, func=lambda x: x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Returns the leave-one-out conditional likelihood of the data.\\n\\n        If `func` is not equal to the default, what's calculated is a function\\n        of the leave-one-out conditional likelihood.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n        func : callable, optional\\n            Function to transform the likelihood values (before summing); for\\n            the log likelihood, use ``func=np.log``.  Default is ``f(x) = x``.\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the leave-one-out function for the data.\\n\\n        Notes\\n        -----\\n        Similar to ``KDE.loo_likelihood`, but substitute ``f(y|x)=f(x,y)/f(x)``\\n        for ``f(x)``.\\n        \"\n    yLOO = LeaveOneOut(self.data)\n    xLOO = LeaveOneOut(self.exog).__iter__()\n    L = 0\n    for (i, Y_j) in enumerate(yLOO):\n        X_not_i = next(xLOO)\n        f_yx = gpke(bw, data=-Y_j, data_predict=-self.data[i, :], var_type=self.dep_type + self.indep_type)\n        f_x = gpke(bw[self.k_dep:], data=-X_not_i, data_predict=-self.exog[i, :], var_type=self.indep_type)\n        f_i = f_yx / f_x\n        L += func(f_i)\n    return -L",
            "def loo_likelihood(self, bw, func=lambda x: x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Returns the leave-one-out conditional likelihood of the data.\\n\\n        If `func` is not equal to the default, what's calculated is a function\\n        of the leave-one-out conditional likelihood.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n        func : callable, optional\\n            Function to transform the likelihood values (before summing); for\\n            the log likelihood, use ``func=np.log``.  Default is ``f(x) = x``.\\n\\n        Returns\\n        -------\\n        L : float\\n            The value of the leave-one-out function for the data.\\n\\n        Notes\\n        -----\\n        Similar to ``KDE.loo_likelihood`, but substitute ``f(y|x)=f(x,y)/f(x)``\\n        for ``f(x)``.\\n        \"\n    yLOO = LeaveOneOut(self.data)\n    xLOO = LeaveOneOut(self.exog).__iter__()\n    L = 0\n    for (i, Y_j) in enumerate(yLOO):\n        X_not_i = next(xLOO)\n        f_yx = gpke(bw, data=-Y_j, data_predict=-self.data[i, :], var_type=self.dep_type + self.indep_type)\n        f_x = gpke(bw[self.k_dep:], data=-X_not_i, data_predict=-self.exog[i, :], var_type=self.indep_type)\n        f_i = f_yx / f_x\n        L += func(f_i)\n    return -L"
        ]
    },
    {
        "func_name": "pdf",
        "original": "def pdf(self, endog_predict=None, exog_predict=None):\n    \"\"\"\n        Evaluate the probability density function.\n\n        Parameters\n        ----------\n        endog_predict : array_like, optional\n            Evaluation data for the dependent variables.  If unspecified, the\n            training data is used.\n        exog_predict : array_like, optional\n            Evaluation data for the independent variables.\n\n        Returns\n        -------\n        pdf : array_like\n            The value of the probability density at `endog_predict` and `exog_predict`.\n\n        Notes\n        -----\n        The formula for the conditional probability density is:\n\n        .. math:: f(y|x)=\\\\frac{f(x,y)}{f(x)}\n\n        with\n\n        .. math:: f(x)=\\\\prod_{s=1}^{q}h_{s}^{-1}k\n                            \\\\left(\\\\frac{x_{is}-x_{js}}{h_{s}}\\\\right)\n\n        where :math:`k` is the appropriate kernel for each variable.\n        \"\"\"\n    if endog_predict is None:\n        endog_predict = self.endog\n    else:\n        endog_predict = _adjust_shape(endog_predict, self.k_dep)\n    if exog_predict is None:\n        exog_predict = self.exog\n    else:\n        exog_predict = _adjust_shape(exog_predict, self.k_indep)\n    pdf_est = []\n    data_predict = np.column_stack((endog_predict, exog_predict))\n    for i in range(np.shape(data_predict)[0]):\n        f_yx = gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.dep_type + self.indep_type)\n        f_x = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type)\n        pdf_est.append(f_yx / f_x)\n    return np.squeeze(pdf_est)",
        "mutated": [
            "def pdf(self, endog_predict=None, exog_predict=None):\n    if False:\n        i = 10\n    '\\n        Evaluate the probability density function.\\n\\n        Parameters\\n        ----------\\n        endog_predict : array_like, optional\\n            Evaluation data for the dependent variables.  If unspecified, the\\n            training data is used.\\n        exog_predict : array_like, optional\\n            Evaluation data for the independent variables.\\n\\n        Returns\\n        -------\\n        pdf : array_like\\n            The value of the probability density at `endog_predict` and `exog_predict`.\\n\\n        Notes\\n        -----\\n        The formula for the conditional probability density is:\\n\\n        .. math:: f(y|x)=\\\\frac{f(x,y)}{f(x)}\\n\\n        with\\n\\n        .. math:: f(x)=\\\\prod_{s=1}^{q}h_{s}^{-1}k\\n                            \\\\left(\\\\frac{x_{is}-x_{js}}{h_{s}}\\\\right)\\n\\n        where :math:`k` is the appropriate kernel for each variable.\\n        '\n    if endog_predict is None:\n        endog_predict = self.endog\n    else:\n        endog_predict = _adjust_shape(endog_predict, self.k_dep)\n    if exog_predict is None:\n        exog_predict = self.exog\n    else:\n        exog_predict = _adjust_shape(exog_predict, self.k_indep)\n    pdf_est = []\n    data_predict = np.column_stack((endog_predict, exog_predict))\n    for i in range(np.shape(data_predict)[0]):\n        f_yx = gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.dep_type + self.indep_type)\n        f_x = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type)\n        pdf_est.append(f_yx / f_x)\n    return np.squeeze(pdf_est)",
            "def pdf(self, endog_predict=None, exog_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Evaluate the probability density function.\\n\\n        Parameters\\n        ----------\\n        endog_predict : array_like, optional\\n            Evaluation data for the dependent variables.  If unspecified, the\\n            training data is used.\\n        exog_predict : array_like, optional\\n            Evaluation data for the independent variables.\\n\\n        Returns\\n        -------\\n        pdf : array_like\\n            The value of the probability density at `endog_predict` and `exog_predict`.\\n\\n        Notes\\n        -----\\n        The formula for the conditional probability density is:\\n\\n        .. math:: f(y|x)=\\\\frac{f(x,y)}{f(x)}\\n\\n        with\\n\\n        .. math:: f(x)=\\\\prod_{s=1}^{q}h_{s}^{-1}k\\n                            \\\\left(\\\\frac{x_{is}-x_{js}}{h_{s}}\\\\right)\\n\\n        where :math:`k` is the appropriate kernel for each variable.\\n        '\n    if endog_predict is None:\n        endog_predict = self.endog\n    else:\n        endog_predict = _adjust_shape(endog_predict, self.k_dep)\n    if exog_predict is None:\n        exog_predict = self.exog\n    else:\n        exog_predict = _adjust_shape(exog_predict, self.k_indep)\n    pdf_est = []\n    data_predict = np.column_stack((endog_predict, exog_predict))\n    for i in range(np.shape(data_predict)[0]):\n        f_yx = gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.dep_type + self.indep_type)\n        f_x = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type)\n        pdf_est.append(f_yx / f_x)\n    return np.squeeze(pdf_est)",
            "def pdf(self, endog_predict=None, exog_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Evaluate the probability density function.\\n\\n        Parameters\\n        ----------\\n        endog_predict : array_like, optional\\n            Evaluation data for the dependent variables.  If unspecified, the\\n            training data is used.\\n        exog_predict : array_like, optional\\n            Evaluation data for the independent variables.\\n\\n        Returns\\n        -------\\n        pdf : array_like\\n            The value of the probability density at `endog_predict` and `exog_predict`.\\n\\n        Notes\\n        -----\\n        The formula for the conditional probability density is:\\n\\n        .. math:: f(y|x)=\\\\frac{f(x,y)}{f(x)}\\n\\n        with\\n\\n        .. math:: f(x)=\\\\prod_{s=1}^{q}h_{s}^{-1}k\\n                            \\\\left(\\\\frac{x_{is}-x_{js}}{h_{s}}\\\\right)\\n\\n        where :math:`k` is the appropriate kernel for each variable.\\n        '\n    if endog_predict is None:\n        endog_predict = self.endog\n    else:\n        endog_predict = _adjust_shape(endog_predict, self.k_dep)\n    if exog_predict is None:\n        exog_predict = self.exog\n    else:\n        exog_predict = _adjust_shape(exog_predict, self.k_indep)\n    pdf_est = []\n    data_predict = np.column_stack((endog_predict, exog_predict))\n    for i in range(np.shape(data_predict)[0]):\n        f_yx = gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.dep_type + self.indep_type)\n        f_x = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type)\n        pdf_est.append(f_yx / f_x)\n    return np.squeeze(pdf_est)",
            "def pdf(self, endog_predict=None, exog_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Evaluate the probability density function.\\n\\n        Parameters\\n        ----------\\n        endog_predict : array_like, optional\\n            Evaluation data for the dependent variables.  If unspecified, the\\n            training data is used.\\n        exog_predict : array_like, optional\\n            Evaluation data for the independent variables.\\n\\n        Returns\\n        -------\\n        pdf : array_like\\n            The value of the probability density at `endog_predict` and `exog_predict`.\\n\\n        Notes\\n        -----\\n        The formula for the conditional probability density is:\\n\\n        .. math:: f(y|x)=\\\\frac{f(x,y)}{f(x)}\\n\\n        with\\n\\n        .. math:: f(x)=\\\\prod_{s=1}^{q}h_{s}^{-1}k\\n                            \\\\left(\\\\frac{x_{is}-x_{js}}{h_{s}}\\\\right)\\n\\n        where :math:`k` is the appropriate kernel for each variable.\\n        '\n    if endog_predict is None:\n        endog_predict = self.endog\n    else:\n        endog_predict = _adjust_shape(endog_predict, self.k_dep)\n    if exog_predict is None:\n        exog_predict = self.exog\n    else:\n        exog_predict = _adjust_shape(exog_predict, self.k_indep)\n    pdf_est = []\n    data_predict = np.column_stack((endog_predict, exog_predict))\n    for i in range(np.shape(data_predict)[0]):\n        f_yx = gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.dep_type + self.indep_type)\n        f_x = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type)\n        pdf_est.append(f_yx / f_x)\n    return np.squeeze(pdf_est)",
            "def pdf(self, endog_predict=None, exog_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Evaluate the probability density function.\\n\\n        Parameters\\n        ----------\\n        endog_predict : array_like, optional\\n            Evaluation data for the dependent variables.  If unspecified, the\\n            training data is used.\\n        exog_predict : array_like, optional\\n            Evaluation data for the independent variables.\\n\\n        Returns\\n        -------\\n        pdf : array_like\\n            The value of the probability density at `endog_predict` and `exog_predict`.\\n\\n        Notes\\n        -----\\n        The formula for the conditional probability density is:\\n\\n        .. math:: f(y|x)=\\\\frac{f(x,y)}{f(x)}\\n\\n        with\\n\\n        .. math:: f(x)=\\\\prod_{s=1}^{q}h_{s}^{-1}k\\n                            \\\\left(\\\\frac{x_{is}-x_{js}}{h_{s}}\\\\right)\\n\\n        where :math:`k` is the appropriate kernel for each variable.\\n        '\n    if endog_predict is None:\n        endog_predict = self.endog\n    else:\n        endog_predict = _adjust_shape(endog_predict, self.k_dep)\n    if exog_predict is None:\n        exog_predict = self.exog\n    else:\n        exog_predict = _adjust_shape(exog_predict, self.k_indep)\n    pdf_est = []\n    data_predict = np.column_stack((endog_predict, exog_predict))\n    for i in range(np.shape(data_predict)[0]):\n        f_yx = gpke(self.bw, data=self.data, data_predict=data_predict[i, :], var_type=self.dep_type + self.indep_type)\n        f_x = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type)\n        pdf_est.append(f_yx / f_x)\n    return np.squeeze(pdf_est)"
        ]
    },
    {
        "func_name": "cdf",
        "original": "def cdf(self, endog_predict=None, exog_predict=None):\n    \"\"\"\n        Cumulative distribution function for the conditional density.\n\n        Parameters\n        ----------\n        endog_predict : array_like, optional\n            The evaluation dependent variables at which the cdf is estimated.\n            If not specified the training dependent variables are used.\n        exog_predict : array_like, optional\n            The evaluation independent variables at which the cdf is estimated.\n            If not specified the training independent variables are used.\n\n        Returns\n        -------\n        cdf_est : array_like\n            The estimate of the cdf.\n\n        Notes\n        -----\n        For more details on the estimation see [2]_, and p.181 in [1]_.\n\n        The multivariate conditional CDF for mixed data (continuous and\n        ordered/unordered discrete) is estimated by:\n\n        .. math::\n\n            F(y|x)=\\\\frac{n^{-1}\\\\sum_{i=1}^{n}G(\\\\frac{y-Y_{i}}{h_{0}}) W_{h}(X_{i},x)}{\\\\widehat{\\\\mu}(x)}\n\n        where G() is the product kernel CDF estimator for the dependent (y)\n        variable(s) and W() is the product kernel CDF estimator for the\n        independent variable(s).\n\n        References\n        ----------\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\n                practice. Princeton University Press. (2007)\n        .. [2] Liu, R., Yang, L. \"Kernel estimation of multivariate cumulative\n                    distribution function.\" Journal of Nonparametric\n                    Statistics (2008)\n        \"\"\"\n    if endog_predict is None:\n        endog_predict = self.endog\n    else:\n        endog_predict = _adjust_shape(endog_predict, self.k_dep)\n    if exog_predict is None:\n        exog_predict = self.exog\n    else:\n        exog_predict = _adjust_shape(exog_predict, self.k_indep)\n    N_data_predict = np.shape(exog_predict)[0]\n    cdf_est = np.empty(N_data_predict)\n    for i in range(N_data_predict):\n        mu_x = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type) / self.nobs\n        mu_x = np.squeeze(mu_x)\n        cdf_endog = gpke(self.bw[0:self.k_dep], data=self.endog, data_predict=endog_predict[i, :], var_type=self.dep_type, ckertype='gaussian_cdf', ukertype='aitchisonaitken_cdf', okertype='wangryzin_cdf', tosum=False)\n        cdf_exog = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type, tosum=False)\n        S = (cdf_endog * cdf_exog).sum(axis=0)\n        cdf_est[i] = S / (self.nobs * mu_x)\n    return cdf_est",
        "mutated": [
            "def cdf(self, endog_predict=None, exog_predict=None):\n    if False:\n        i = 10\n    '\\n        Cumulative distribution function for the conditional density.\\n\\n        Parameters\\n        ----------\\n        endog_predict : array_like, optional\\n            The evaluation dependent variables at which the cdf is estimated.\\n            If not specified the training dependent variables are used.\\n        exog_predict : array_like, optional\\n            The evaluation independent variables at which the cdf is estimated.\\n            If not specified the training independent variables are used.\\n\\n        Returns\\n        -------\\n        cdf_est : array_like\\n            The estimate of the cdf.\\n\\n        Notes\\n        -----\\n        For more details on the estimation see [2]_, and p.181 in [1]_.\\n\\n        The multivariate conditional CDF for mixed data (continuous and\\n        ordered/unordered discrete) is estimated by:\\n\\n        .. math::\\n\\n            F(y|x)=\\\\frac{n^{-1}\\\\sum_{i=1}^{n}G(\\\\frac{y-Y_{i}}{h_{0}}) W_{h}(X_{i},x)}{\\\\widehat{\\\\mu}(x)}\\n\\n        where G() is the product kernel CDF estimator for the dependent (y)\\n        variable(s) and W() is the product kernel CDF estimator for the\\n        independent variable(s).\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Liu, R., Yang, L. \"Kernel estimation of multivariate cumulative\\n                    distribution function.\" Journal of Nonparametric\\n                    Statistics (2008)\\n        '\n    if endog_predict is None:\n        endog_predict = self.endog\n    else:\n        endog_predict = _adjust_shape(endog_predict, self.k_dep)\n    if exog_predict is None:\n        exog_predict = self.exog\n    else:\n        exog_predict = _adjust_shape(exog_predict, self.k_indep)\n    N_data_predict = np.shape(exog_predict)[0]\n    cdf_est = np.empty(N_data_predict)\n    for i in range(N_data_predict):\n        mu_x = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type) / self.nobs\n        mu_x = np.squeeze(mu_x)\n        cdf_endog = gpke(self.bw[0:self.k_dep], data=self.endog, data_predict=endog_predict[i, :], var_type=self.dep_type, ckertype='gaussian_cdf', ukertype='aitchisonaitken_cdf', okertype='wangryzin_cdf', tosum=False)\n        cdf_exog = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type, tosum=False)\n        S = (cdf_endog * cdf_exog).sum(axis=0)\n        cdf_est[i] = S / (self.nobs * mu_x)\n    return cdf_est",
            "def cdf(self, endog_predict=None, exog_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Cumulative distribution function for the conditional density.\\n\\n        Parameters\\n        ----------\\n        endog_predict : array_like, optional\\n            The evaluation dependent variables at which the cdf is estimated.\\n            If not specified the training dependent variables are used.\\n        exog_predict : array_like, optional\\n            The evaluation independent variables at which the cdf is estimated.\\n            If not specified the training independent variables are used.\\n\\n        Returns\\n        -------\\n        cdf_est : array_like\\n            The estimate of the cdf.\\n\\n        Notes\\n        -----\\n        For more details on the estimation see [2]_, and p.181 in [1]_.\\n\\n        The multivariate conditional CDF for mixed data (continuous and\\n        ordered/unordered discrete) is estimated by:\\n\\n        .. math::\\n\\n            F(y|x)=\\\\frac{n^{-1}\\\\sum_{i=1}^{n}G(\\\\frac{y-Y_{i}}{h_{0}}) W_{h}(X_{i},x)}{\\\\widehat{\\\\mu}(x)}\\n\\n        where G() is the product kernel CDF estimator for the dependent (y)\\n        variable(s) and W() is the product kernel CDF estimator for the\\n        independent variable(s).\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Liu, R., Yang, L. \"Kernel estimation of multivariate cumulative\\n                    distribution function.\" Journal of Nonparametric\\n                    Statistics (2008)\\n        '\n    if endog_predict is None:\n        endog_predict = self.endog\n    else:\n        endog_predict = _adjust_shape(endog_predict, self.k_dep)\n    if exog_predict is None:\n        exog_predict = self.exog\n    else:\n        exog_predict = _adjust_shape(exog_predict, self.k_indep)\n    N_data_predict = np.shape(exog_predict)[0]\n    cdf_est = np.empty(N_data_predict)\n    for i in range(N_data_predict):\n        mu_x = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type) / self.nobs\n        mu_x = np.squeeze(mu_x)\n        cdf_endog = gpke(self.bw[0:self.k_dep], data=self.endog, data_predict=endog_predict[i, :], var_type=self.dep_type, ckertype='gaussian_cdf', ukertype='aitchisonaitken_cdf', okertype='wangryzin_cdf', tosum=False)\n        cdf_exog = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type, tosum=False)\n        S = (cdf_endog * cdf_exog).sum(axis=0)\n        cdf_est[i] = S / (self.nobs * mu_x)\n    return cdf_est",
            "def cdf(self, endog_predict=None, exog_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Cumulative distribution function for the conditional density.\\n\\n        Parameters\\n        ----------\\n        endog_predict : array_like, optional\\n            The evaluation dependent variables at which the cdf is estimated.\\n            If not specified the training dependent variables are used.\\n        exog_predict : array_like, optional\\n            The evaluation independent variables at which the cdf is estimated.\\n            If not specified the training independent variables are used.\\n\\n        Returns\\n        -------\\n        cdf_est : array_like\\n            The estimate of the cdf.\\n\\n        Notes\\n        -----\\n        For more details on the estimation see [2]_, and p.181 in [1]_.\\n\\n        The multivariate conditional CDF for mixed data (continuous and\\n        ordered/unordered discrete) is estimated by:\\n\\n        .. math::\\n\\n            F(y|x)=\\\\frac{n^{-1}\\\\sum_{i=1}^{n}G(\\\\frac{y-Y_{i}}{h_{0}}) W_{h}(X_{i},x)}{\\\\widehat{\\\\mu}(x)}\\n\\n        where G() is the product kernel CDF estimator for the dependent (y)\\n        variable(s) and W() is the product kernel CDF estimator for the\\n        independent variable(s).\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Liu, R., Yang, L. \"Kernel estimation of multivariate cumulative\\n                    distribution function.\" Journal of Nonparametric\\n                    Statistics (2008)\\n        '\n    if endog_predict is None:\n        endog_predict = self.endog\n    else:\n        endog_predict = _adjust_shape(endog_predict, self.k_dep)\n    if exog_predict is None:\n        exog_predict = self.exog\n    else:\n        exog_predict = _adjust_shape(exog_predict, self.k_indep)\n    N_data_predict = np.shape(exog_predict)[0]\n    cdf_est = np.empty(N_data_predict)\n    for i in range(N_data_predict):\n        mu_x = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type) / self.nobs\n        mu_x = np.squeeze(mu_x)\n        cdf_endog = gpke(self.bw[0:self.k_dep], data=self.endog, data_predict=endog_predict[i, :], var_type=self.dep_type, ckertype='gaussian_cdf', ukertype='aitchisonaitken_cdf', okertype='wangryzin_cdf', tosum=False)\n        cdf_exog = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type, tosum=False)\n        S = (cdf_endog * cdf_exog).sum(axis=0)\n        cdf_est[i] = S / (self.nobs * mu_x)\n    return cdf_est",
            "def cdf(self, endog_predict=None, exog_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Cumulative distribution function for the conditional density.\\n\\n        Parameters\\n        ----------\\n        endog_predict : array_like, optional\\n            The evaluation dependent variables at which the cdf is estimated.\\n            If not specified the training dependent variables are used.\\n        exog_predict : array_like, optional\\n            The evaluation independent variables at which the cdf is estimated.\\n            If not specified the training independent variables are used.\\n\\n        Returns\\n        -------\\n        cdf_est : array_like\\n            The estimate of the cdf.\\n\\n        Notes\\n        -----\\n        For more details on the estimation see [2]_, and p.181 in [1]_.\\n\\n        The multivariate conditional CDF for mixed data (continuous and\\n        ordered/unordered discrete) is estimated by:\\n\\n        .. math::\\n\\n            F(y|x)=\\\\frac{n^{-1}\\\\sum_{i=1}^{n}G(\\\\frac{y-Y_{i}}{h_{0}}) W_{h}(X_{i},x)}{\\\\widehat{\\\\mu}(x)}\\n\\n        where G() is the product kernel CDF estimator for the dependent (y)\\n        variable(s) and W() is the product kernel CDF estimator for the\\n        independent variable(s).\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Liu, R., Yang, L. \"Kernel estimation of multivariate cumulative\\n                    distribution function.\" Journal of Nonparametric\\n                    Statistics (2008)\\n        '\n    if endog_predict is None:\n        endog_predict = self.endog\n    else:\n        endog_predict = _adjust_shape(endog_predict, self.k_dep)\n    if exog_predict is None:\n        exog_predict = self.exog\n    else:\n        exog_predict = _adjust_shape(exog_predict, self.k_indep)\n    N_data_predict = np.shape(exog_predict)[0]\n    cdf_est = np.empty(N_data_predict)\n    for i in range(N_data_predict):\n        mu_x = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type) / self.nobs\n        mu_x = np.squeeze(mu_x)\n        cdf_endog = gpke(self.bw[0:self.k_dep], data=self.endog, data_predict=endog_predict[i, :], var_type=self.dep_type, ckertype='gaussian_cdf', ukertype='aitchisonaitken_cdf', okertype='wangryzin_cdf', tosum=False)\n        cdf_exog = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type, tosum=False)\n        S = (cdf_endog * cdf_exog).sum(axis=0)\n        cdf_est[i] = S / (self.nobs * mu_x)\n    return cdf_est",
            "def cdf(self, endog_predict=None, exog_predict=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Cumulative distribution function for the conditional density.\\n\\n        Parameters\\n        ----------\\n        endog_predict : array_like, optional\\n            The evaluation dependent variables at which the cdf is estimated.\\n            If not specified the training dependent variables are used.\\n        exog_predict : array_like, optional\\n            The evaluation independent variables at which the cdf is estimated.\\n            If not specified the training independent variables are used.\\n\\n        Returns\\n        -------\\n        cdf_est : array_like\\n            The estimate of the cdf.\\n\\n        Notes\\n        -----\\n        For more details on the estimation see [2]_, and p.181 in [1]_.\\n\\n        The multivariate conditional CDF for mixed data (continuous and\\n        ordered/unordered discrete) is estimated by:\\n\\n        .. math::\\n\\n            F(y|x)=\\\\frac{n^{-1}\\\\sum_{i=1}^{n}G(\\\\frac{y-Y_{i}}{h_{0}}) W_{h}(X_{i},x)}{\\\\widehat{\\\\mu}(x)}\\n\\n        where G() is the product kernel CDF estimator for the dependent (y)\\n        variable(s) and W() is the product kernel CDF estimator for the\\n        independent variable(s).\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Liu, R., Yang, L. \"Kernel estimation of multivariate cumulative\\n                    distribution function.\" Journal of Nonparametric\\n                    Statistics (2008)\\n        '\n    if endog_predict is None:\n        endog_predict = self.endog\n    else:\n        endog_predict = _adjust_shape(endog_predict, self.k_dep)\n    if exog_predict is None:\n        exog_predict = self.exog\n    else:\n        exog_predict = _adjust_shape(exog_predict, self.k_indep)\n    N_data_predict = np.shape(exog_predict)[0]\n    cdf_est = np.empty(N_data_predict)\n    for i in range(N_data_predict):\n        mu_x = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type) / self.nobs\n        mu_x = np.squeeze(mu_x)\n        cdf_endog = gpke(self.bw[0:self.k_dep], data=self.endog, data_predict=endog_predict[i, :], var_type=self.dep_type, ckertype='gaussian_cdf', ukertype='aitchisonaitken_cdf', okertype='wangryzin_cdf', tosum=False)\n        cdf_exog = gpke(self.bw[self.k_dep:], data=self.exog, data_predict=exog_predict[i, :], var_type=self.indep_type, tosum=False)\n        S = (cdf_endog * cdf_exog).sum(axis=0)\n        cdf_est[i] = S / (self.nobs * mu_x)\n    return cdf_est"
        ]
    },
    {
        "func_name": "imse",
        "original": "def imse(self, bw):\n    \"\"\"\n        The integrated mean square error for the conditional KDE.\n\n        Parameters\n        ----------\n        bw : array_like\n            The bandwidth parameter(s).\n\n        Returns\n        -------\n        CV : float\n            The cross-validation objective function.\n\n        Notes\n        -----\n        For more details see pp. 156-166 in [1]_. For details on how to\n        handle the mixed variable types see [2]_.\n\n        The formula for the cross-validation objective function for mixed\n        variable types is:\n\n        .. math:: CV(h,\\\\lambda)=\\\\frac{1}{n}\\\\sum_{l=1}^{n}\n            \\\\frac{G_{-l}(X_{l})}{\\\\left[\\\\mu_{-l}(X_{l})\\\\right]^{2}}-\n            \\\\frac{2}{n}\\\\sum_{l=1}^{n}\\\\frac{f_{-l}(X_{l},Y_{l})}{\\\\mu_{-l}(X_{l})}\n\n        where\n\n        .. math:: G_{-l}(X_{l}) = n^{-2}\\\\sum_{i\\\\neq l}\\\\sum_{j\\\\neq l}\n                        K_{X_{i},X_{l}} K_{X_{j},X_{l}}K_{Y_{i},Y_{j}}^{(2)}\n\n        where :math:`K_{X_{i},X_{l}}` is the multivariate product kernel and\n        :math:`\\\\mu_{-l}(X_{l})` is the leave-one-out estimator of the pdf.\n\n        :math:`K_{Y_{i},Y_{j}}^{(2)}` is the convolution kernel.\n\n        The value of the function is minimized by the ``_cv_ls`` method of the\n        `GenericKDE` class to return the bw estimates that minimize the\n        distance between the estimated and \"true\" probability density.\n\n        References\n        ----------\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\n                practice. Princeton University Press. (2007)\n        .. [2] Racine, J., Li, Q. \"Nonparametric Estimation of Distributions\n                with Categorical and Continuous Data.\" Working Paper. (2000)\n        \"\"\"\n    zLOO = LeaveOneOut(self.data)\n    CV = 0\n    nobs = float(self.nobs)\n    expander = np.ones((self.nobs - 1, 1))\n    for (ii, Z) in enumerate(zLOO):\n        X = Z[:, self.k_dep:]\n        Y = Z[:, :self.k_dep]\n        Ye_L = np.kron(Y, expander)\n        Ye_R = np.kron(expander, Y)\n        Xe_L = np.kron(X, expander)\n        Xe_R = np.kron(expander, X)\n        K_Xi_Xl = gpke(bw[self.k_dep:], data=Xe_L, data_predict=self.exog[ii, :], var_type=self.indep_type, tosum=False)\n        K_Xj_Xl = gpke(bw[self.k_dep:], data=Xe_R, data_predict=self.exog[ii, :], var_type=self.indep_type, tosum=False)\n        K2_Yi_Yj = gpke(bw[0:self.k_dep], data=Ye_L, data_predict=Ye_R, var_type=self.dep_type, ckertype='gauss_convolution', okertype='wangryzin_convolution', ukertype='aitchisonaitken_convolution', tosum=False)\n        G = (K_Xi_Xl * K_Xj_Xl * K2_Yi_Yj).sum() / nobs ** 2\n        f_X_Y = gpke(bw, data=-Z, data_predict=-self.data[ii, :], var_type=self.dep_type + self.indep_type) / nobs\n        m_x = gpke(bw[self.k_dep:], data=-X, data_predict=-self.exog[ii, :], var_type=self.indep_type) / nobs\n        CV += G / m_x ** 2 - 2 * (f_X_Y / m_x)\n    return CV / nobs",
        "mutated": [
            "def imse(self, bw):\n    if False:\n        i = 10\n    '\\n        The integrated mean square error for the conditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n\\n        Returns\\n        -------\\n        CV : float\\n            The cross-validation objective function.\\n\\n        Notes\\n        -----\\n        For more details see pp. 156-166 in [1]_. For details on how to\\n        handle the mixed variable types see [2]_.\\n\\n        The formula for the cross-validation objective function for mixed\\n        variable types is:\\n\\n        .. math:: CV(h,\\\\lambda)=\\\\frac{1}{n}\\\\sum_{l=1}^{n}\\n            \\\\frac{G_{-l}(X_{l})}{\\\\left[\\\\mu_{-l}(X_{l})\\\\right]^{2}}-\\n            \\\\frac{2}{n}\\\\sum_{l=1}^{n}\\\\frac{f_{-l}(X_{l},Y_{l})}{\\\\mu_{-l}(X_{l})}\\n\\n        where\\n\\n        .. math:: G_{-l}(X_{l}) = n^{-2}\\\\sum_{i\\\\neq l}\\\\sum_{j\\\\neq l}\\n                        K_{X_{i},X_{l}} K_{X_{j},X_{l}}K_{Y_{i},Y_{j}}^{(2)}\\n\\n        where :math:`K_{X_{i},X_{l}}` is the multivariate product kernel and\\n        :math:`\\\\mu_{-l}(X_{l})` is the leave-one-out estimator of the pdf.\\n\\n        :math:`K_{Y_{i},Y_{j}}^{(2)}` is the convolution kernel.\\n\\n        The value of the function is minimized by the ``_cv_ls`` method of the\\n        `GenericKDE` class to return the bw estimates that minimize the\\n        distance between the estimated and \"true\" probability density.\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Racine, J., Li, Q. \"Nonparametric Estimation of Distributions\\n                with Categorical and Continuous Data.\" Working Paper. (2000)\\n        '\n    zLOO = LeaveOneOut(self.data)\n    CV = 0\n    nobs = float(self.nobs)\n    expander = np.ones((self.nobs - 1, 1))\n    for (ii, Z) in enumerate(zLOO):\n        X = Z[:, self.k_dep:]\n        Y = Z[:, :self.k_dep]\n        Ye_L = np.kron(Y, expander)\n        Ye_R = np.kron(expander, Y)\n        Xe_L = np.kron(X, expander)\n        Xe_R = np.kron(expander, X)\n        K_Xi_Xl = gpke(bw[self.k_dep:], data=Xe_L, data_predict=self.exog[ii, :], var_type=self.indep_type, tosum=False)\n        K_Xj_Xl = gpke(bw[self.k_dep:], data=Xe_R, data_predict=self.exog[ii, :], var_type=self.indep_type, tosum=False)\n        K2_Yi_Yj = gpke(bw[0:self.k_dep], data=Ye_L, data_predict=Ye_R, var_type=self.dep_type, ckertype='gauss_convolution', okertype='wangryzin_convolution', ukertype='aitchisonaitken_convolution', tosum=False)\n        G = (K_Xi_Xl * K_Xj_Xl * K2_Yi_Yj).sum() / nobs ** 2\n        f_X_Y = gpke(bw, data=-Z, data_predict=-self.data[ii, :], var_type=self.dep_type + self.indep_type) / nobs\n        m_x = gpke(bw[self.k_dep:], data=-X, data_predict=-self.exog[ii, :], var_type=self.indep_type) / nobs\n        CV += G / m_x ** 2 - 2 * (f_X_Y / m_x)\n    return CV / nobs",
            "def imse(self, bw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        The integrated mean square error for the conditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n\\n        Returns\\n        -------\\n        CV : float\\n            The cross-validation objective function.\\n\\n        Notes\\n        -----\\n        For more details see pp. 156-166 in [1]_. For details on how to\\n        handle the mixed variable types see [2]_.\\n\\n        The formula for the cross-validation objective function for mixed\\n        variable types is:\\n\\n        .. math:: CV(h,\\\\lambda)=\\\\frac{1}{n}\\\\sum_{l=1}^{n}\\n            \\\\frac{G_{-l}(X_{l})}{\\\\left[\\\\mu_{-l}(X_{l})\\\\right]^{2}}-\\n            \\\\frac{2}{n}\\\\sum_{l=1}^{n}\\\\frac{f_{-l}(X_{l},Y_{l})}{\\\\mu_{-l}(X_{l})}\\n\\n        where\\n\\n        .. math:: G_{-l}(X_{l}) = n^{-2}\\\\sum_{i\\\\neq l}\\\\sum_{j\\\\neq l}\\n                        K_{X_{i},X_{l}} K_{X_{j},X_{l}}K_{Y_{i},Y_{j}}^{(2)}\\n\\n        where :math:`K_{X_{i},X_{l}}` is the multivariate product kernel and\\n        :math:`\\\\mu_{-l}(X_{l})` is the leave-one-out estimator of the pdf.\\n\\n        :math:`K_{Y_{i},Y_{j}}^{(2)}` is the convolution kernel.\\n\\n        The value of the function is minimized by the ``_cv_ls`` method of the\\n        `GenericKDE` class to return the bw estimates that minimize the\\n        distance between the estimated and \"true\" probability density.\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Racine, J., Li, Q. \"Nonparametric Estimation of Distributions\\n                with Categorical and Continuous Data.\" Working Paper. (2000)\\n        '\n    zLOO = LeaveOneOut(self.data)\n    CV = 0\n    nobs = float(self.nobs)\n    expander = np.ones((self.nobs - 1, 1))\n    for (ii, Z) in enumerate(zLOO):\n        X = Z[:, self.k_dep:]\n        Y = Z[:, :self.k_dep]\n        Ye_L = np.kron(Y, expander)\n        Ye_R = np.kron(expander, Y)\n        Xe_L = np.kron(X, expander)\n        Xe_R = np.kron(expander, X)\n        K_Xi_Xl = gpke(bw[self.k_dep:], data=Xe_L, data_predict=self.exog[ii, :], var_type=self.indep_type, tosum=False)\n        K_Xj_Xl = gpke(bw[self.k_dep:], data=Xe_R, data_predict=self.exog[ii, :], var_type=self.indep_type, tosum=False)\n        K2_Yi_Yj = gpke(bw[0:self.k_dep], data=Ye_L, data_predict=Ye_R, var_type=self.dep_type, ckertype='gauss_convolution', okertype='wangryzin_convolution', ukertype='aitchisonaitken_convolution', tosum=False)\n        G = (K_Xi_Xl * K_Xj_Xl * K2_Yi_Yj).sum() / nobs ** 2\n        f_X_Y = gpke(bw, data=-Z, data_predict=-self.data[ii, :], var_type=self.dep_type + self.indep_type) / nobs\n        m_x = gpke(bw[self.k_dep:], data=-X, data_predict=-self.exog[ii, :], var_type=self.indep_type) / nobs\n        CV += G / m_x ** 2 - 2 * (f_X_Y / m_x)\n    return CV / nobs",
            "def imse(self, bw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        The integrated mean square error for the conditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n\\n        Returns\\n        -------\\n        CV : float\\n            The cross-validation objective function.\\n\\n        Notes\\n        -----\\n        For more details see pp. 156-166 in [1]_. For details on how to\\n        handle the mixed variable types see [2]_.\\n\\n        The formula for the cross-validation objective function for mixed\\n        variable types is:\\n\\n        .. math:: CV(h,\\\\lambda)=\\\\frac{1}{n}\\\\sum_{l=1}^{n}\\n            \\\\frac{G_{-l}(X_{l})}{\\\\left[\\\\mu_{-l}(X_{l})\\\\right]^{2}}-\\n            \\\\frac{2}{n}\\\\sum_{l=1}^{n}\\\\frac{f_{-l}(X_{l},Y_{l})}{\\\\mu_{-l}(X_{l})}\\n\\n        where\\n\\n        .. math:: G_{-l}(X_{l}) = n^{-2}\\\\sum_{i\\\\neq l}\\\\sum_{j\\\\neq l}\\n                        K_{X_{i},X_{l}} K_{X_{j},X_{l}}K_{Y_{i},Y_{j}}^{(2)}\\n\\n        where :math:`K_{X_{i},X_{l}}` is the multivariate product kernel and\\n        :math:`\\\\mu_{-l}(X_{l})` is the leave-one-out estimator of the pdf.\\n\\n        :math:`K_{Y_{i},Y_{j}}^{(2)}` is the convolution kernel.\\n\\n        The value of the function is minimized by the ``_cv_ls`` method of the\\n        `GenericKDE` class to return the bw estimates that minimize the\\n        distance between the estimated and \"true\" probability density.\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Racine, J., Li, Q. \"Nonparametric Estimation of Distributions\\n                with Categorical and Continuous Data.\" Working Paper. (2000)\\n        '\n    zLOO = LeaveOneOut(self.data)\n    CV = 0\n    nobs = float(self.nobs)\n    expander = np.ones((self.nobs - 1, 1))\n    for (ii, Z) in enumerate(zLOO):\n        X = Z[:, self.k_dep:]\n        Y = Z[:, :self.k_dep]\n        Ye_L = np.kron(Y, expander)\n        Ye_R = np.kron(expander, Y)\n        Xe_L = np.kron(X, expander)\n        Xe_R = np.kron(expander, X)\n        K_Xi_Xl = gpke(bw[self.k_dep:], data=Xe_L, data_predict=self.exog[ii, :], var_type=self.indep_type, tosum=False)\n        K_Xj_Xl = gpke(bw[self.k_dep:], data=Xe_R, data_predict=self.exog[ii, :], var_type=self.indep_type, tosum=False)\n        K2_Yi_Yj = gpke(bw[0:self.k_dep], data=Ye_L, data_predict=Ye_R, var_type=self.dep_type, ckertype='gauss_convolution', okertype='wangryzin_convolution', ukertype='aitchisonaitken_convolution', tosum=False)\n        G = (K_Xi_Xl * K_Xj_Xl * K2_Yi_Yj).sum() / nobs ** 2\n        f_X_Y = gpke(bw, data=-Z, data_predict=-self.data[ii, :], var_type=self.dep_type + self.indep_type) / nobs\n        m_x = gpke(bw[self.k_dep:], data=-X, data_predict=-self.exog[ii, :], var_type=self.indep_type) / nobs\n        CV += G / m_x ** 2 - 2 * (f_X_Y / m_x)\n    return CV / nobs",
            "def imse(self, bw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        The integrated mean square error for the conditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n\\n        Returns\\n        -------\\n        CV : float\\n            The cross-validation objective function.\\n\\n        Notes\\n        -----\\n        For more details see pp. 156-166 in [1]_. For details on how to\\n        handle the mixed variable types see [2]_.\\n\\n        The formula for the cross-validation objective function for mixed\\n        variable types is:\\n\\n        .. math:: CV(h,\\\\lambda)=\\\\frac{1}{n}\\\\sum_{l=1}^{n}\\n            \\\\frac{G_{-l}(X_{l})}{\\\\left[\\\\mu_{-l}(X_{l})\\\\right]^{2}}-\\n            \\\\frac{2}{n}\\\\sum_{l=1}^{n}\\\\frac{f_{-l}(X_{l},Y_{l})}{\\\\mu_{-l}(X_{l})}\\n\\n        where\\n\\n        .. math:: G_{-l}(X_{l}) = n^{-2}\\\\sum_{i\\\\neq l}\\\\sum_{j\\\\neq l}\\n                        K_{X_{i},X_{l}} K_{X_{j},X_{l}}K_{Y_{i},Y_{j}}^{(2)}\\n\\n        where :math:`K_{X_{i},X_{l}}` is the multivariate product kernel and\\n        :math:`\\\\mu_{-l}(X_{l})` is the leave-one-out estimator of the pdf.\\n\\n        :math:`K_{Y_{i},Y_{j}}^{(2)}` is the convolution kernel.\\n\\n        The value of the function is minimized by the ``_cv_ls`` method of the\\n        `GenericKDE` class to return the bw estimates that minimize the\\n        distance between the estimated and \"true\" probability density.\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Racine, J., Li, Q. \"Nonparametric Estimation of Distributions\\n                with Categorical and Continuous Data.\" Working Paper. (2000)\\n        '\n    zLOO = LeaveOneOut(self.data)\n    CV = 0\n    nobs = float(self.nobs)\n    expander = np.ones((self.nobs - 1, 1))\n    for (ii, Z) in enumerate(zLOO):\n        X = Z[:, self.k_dep:]\n        Y = Z[:, :self.k_dep]\n        Ye_L = np.kron(Y, expander)\n        Ye_R = np.kron(expander, Y)\n        Xe_L = np.kron(X, expander)\n        Xe_R = np.kron(expander, X)\n        K_Xi_Xl = gpke(bw[self.k_dep:], data=Xe_L, data_predict=self.exog[ii, :], var_type=self.indep_type, tosum=False)\n        K_Xj_Xl = gpke(bw[self.k_dep:], data=Xe_R, data_predict=self.exog[ii, :], var_type=self.indep_type, tosum=False)\n        K2_Yi_Yj = gpke(bw[0:self.k_dep], data=Ye_L, data_predict=Ye_R, var_type=self.dep_type, ckertype='gauss_convolution', okertype='wangryzin_convolution', ukertype='aitchisonaitken_convolution', tosum=False)\n        G = (K_Xi_Xl * K_Xj_Xl * K2_Yi_Yj).sum() / nobs ** 2\n        f_X_Y = gpke(bw, data=-Z, data_predict=-self.data[ii, :], var_type=self.dep_type + self.indep_type) / nobs\n        m_x = gpke(bw[self.k_dep:], data=-X, data_predict=-self.exog[ii, :], var_type=self.indep_type) / nobs\n        CV += G / m_x ** 2 - 2 * (f_X_Y / m_x)\n    return CV / nobs",
            "def imse(self, bw):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        The integrated mean square error for the conditional KDE.\\n\\n        Parameters\\n        ----------\\n        bw : array_like\\n            The bandwidth parameter(s).\\n\\n        Returns\\n        -------\\n        CV : float\\n            The cross-validation objective function.\\n\\n        Notes\\n        -----\\n        For more details see pp. 156-166 in [1]_. For details on how to\\n        handle the mixed variable types see [2]_.\\n\\n        The formula for the cross-validation objective function for mixed\\n        variable types is:\\n\\n        .. math:: CV(h,\\\\lambda)=\\\\frac{1}{n}\\\\sum_{l=1}^{n}\\n            \\\\frac{G_{-l}(X_{l})}{\\\\left[\\\\mu_{-l}(X_{l})\\\\right]^{2}}-\\n            \\\\frac{2}{n}\\\\sum_{l=1}^{n}\\\\frac{f_{-l}(X_{l},Y_{l})}{\\\\mu_{-l}(X_{l})}\\n\\n        where\\n\\n        .. math:: G_{-l}(X_{l}) = n^{-2}\\\\sum_{i\\\\neq l}\\\\sum_{j\\\\neq l}\\n                        K_{X_{i},X_{l}} K_{X_{j},X_{l}}K_{Y_{i},Y_{j}}^{(2)}\\n\\n        where :math:`K_{X_{i},X_{l}}` is the multivariate product kernel and\\n        :math:`\\\\mu_{-l}(X_{l})` is the leave-one-out estimator of the pdf.\\n\\n        :math:`K_{Y_{i},Y_{j}}^{(2)}` is the convolution kernel.\\n\\n        The value of the function is minimized by the ``_cv_ls`` method of the\\n        `GenericKDE` class to return the bw estimates that minimize the\\n        distance between the estimated and \"true\" probability density.\\n\\n        References\\n        ----------\\n        .. [1] Racine, J., Li, Q. Nonparametric econometrics: theory and\\n                practice. Princeton University Press. (2007)\\n        .. [2] Racine, J., Li, Q. \"Nonparametric Estimation of Distributions\\n                with Categorical and Continuous Data.\" Working Paper. (2000)\\n        '\n    zLOO = LeaveOneOut(self.data)\n    CV = 0\n    nobs = float(self.nobs)\n    expander = np.ones((self.nobs - 1, 1))\n    for (ii, Z) in enumerate(zLOO):\n        X = Z[:, self.k_dep:]\n        Y = Z[:, :self.k_dep]\n        Ye_L = np.kron(Y, expander)\n        Ye_R = np.kron(expander, Y)\n        Xe_L = np.kron(X, expander)\n        Xe_R = np.kron(expander, X)\n        K_Xi_Xl = gpke(bw[self.k_dep:], data=Xe_L, data_predict=self.exog[ii, :], var_type=self.indep_type, tosum=False)\n        K_Xj_Xl = gpke(bw[self.k_dep:], data=Xe_R, data_predict=self.exog[ii, :], var_type=self.indep_type, tosum=False)\n        K2_Yi_Yj = gpke(bw[0:self.k_dep], data=Ye_L, data_predict=Ye_R, var_type=self.dep_type, ckertype='gauss_convolution', okertype='wangryzin_convolution', ukertype='aitchisonaitken_convolution', tosum=False)\n        G = (K_Xi_Xl * K_Xj_Xl * K2_Yi_Yj).sum() / nobs ** 2\n        f_X_Y = gpke(bw, data=-Z, data_predict=-self.data[ii, :], var_type=self.dep_type + self.indep_type) / nobs\n        m_x = gpke(bw[self.k_dep:], data=-X, data_predict=-self.exog[ii, :], var_type=self.indep_type) / nobs\n        CV += G / m_x ** 2 - 2 * (f_X_Y / m_x)\n    return CV / nobs"
        ]
    },
    {
        "func_name": "_get_class_vars_type",
        "original": "def _get_class_vars_type(self):\n    \"\"\"Helper method to be able to pass needed vars to _compute_subset.\"\"\"\n    class_type = 'KDEMultivariateConditional'\n    class_vars = (self.k_dep, self.dep_type, self.indep_type)\n    return (class_type, class_vars)",
        "mutated": [
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KDEMultivariateConditional'\n    class_vars = (self.k_dep, self.dep_type, self.indep_type)\n    return (class_type, class_vars)",
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KDEMultivariateConditional'\n    class_vars = (self.k_dep, self.dep_type, self.indep_type)\n    return (class_type, class_vars)",
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KDEMultivariateConditional'\n    class_vars = (self.k_dep, self.dep_type, self.indep_type)\n    return (class_type, class_vars)",
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KDEMultivariateConditional'\n    class_vars = (self.k_dep, self.dep_type, self.indep_type)\n    return (class_type, class_vars)",
            "def _get_class_vars_type(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Helper method to be able to pass needed vars to _compute_subset.'\n    class_type = 'KDEMultivariateConditional'\n    class_vars = (self.k_dep, self.dep_type, self.indep_type)\n    return (class_type, class_vars)"
        ]
    }
]