[
    {
        "func_name": "_fetch_all_embeddings",
        "original": "def _fetch_all_embeddings(model):\n    \"\"\"Fetches Embedding and EmbeddingBag modules from the model\n    \"\"\"\n    embedding_modules = []\n    stack = [model]\n    while stack:\n        module = stack.pop()\n        for (_, child) in module.named_children():\n            fqn_name = module_to_fqn(model, child)\n            if type(child) in SUPPORTED_MODULES:\n                embedding_modules.append((fqn_name, child))\n            else:\n                stack.append(child)\n    return embedding_modules",
        "mutated": [
            "def _fetch_all_embeddings(model):\n    if False:\n        i = 10\n    'Fetches Embedding and EmbeddingBag modules from the model\\n    '\n    embedding_modules = []\n    stack = [model]\n    while stack:\n        module = stack.pop()\n        for (_, child) in module.named_children():\n            fqn_name = module_to_fqn(model, child)\n            if type(child) in SUPPORTED_MODULES:\n                embedding_modules.append((fqn_name, child))\n            else:\n                stack.append(child)\n    return embedding_modules",
            "def _fetch_all_embeddings(model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Fetches Embedding and EmbeddingBag modules from the model\\n    '\n    embedding_modules = []\n    stack = [model]\n    while stack:\n        module = stack.pop()\n        for (_, child) in module.named_children():\n            fqn_name = module_to_fqn(model, child)\n            if type(child) in SUPPORTED_MODULES:\n                embedding_modules.append((fqn_name, child))\n            else:\n                stack.append(child)\n    return embedding_modules",
            "def _fetch_all_embeddings(model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Fetches Embedding and EmbeddingBag modules from the model\\n    '\n    embedding_modules = []\n    stack = [model]\n    while stack:\n        module = stack.pop()\n        for (_, child) in module.named_children():\n            fqn_name = module_to_fqn(model, child)\n            if type(child) in SUPPORTED_MODULES:\n                embedding_modules.append((fqn_name, child))\n            else:\n                stack.append(child)\n    return embedding_modules",
            "def _fetch_all_embeddings(model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Fetches Embedding and EmbeddingBag modules from the model\\n    '\n    embedding_modules = []\n    stack = [model]\n    while stack:\n        module = stack.pop()\n        for (_, child) in module.named_children():\n            fqn_name = module_to_fqn(model, child)\n            if type(child) in SUPPORTED_MODULES:\n                embedding_modules.append((fqn_name, child))\n            else:\n                stack.append(child)\n    return embedding_modules",
            "def _fetch_all_embeddings(model):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Fetches Embedding and EmbeddingBag modules from the model\\n    '\n    embedding_modules = []\n    stack = [model]\n    while stack:\n        module = stack.pop()\n        for (_, child) in module.named_children():\n            fqn_name = module_to_fqn(model, child)\n            if type(child) in SUPPORTED_MODULES:\n                embedding_modules.append((fqn_name, child))\n            else:\n                stack.append(child)\n    return embedding_modules"
        ]
    },
    {
        "func_name": "post_training_sparse_quantize",
        "original": "def post_training_sparse_quantize(model, data_sparsifier_class, sparsify_first=True, select_embeddings: Optional[List[nn.Module]]=None, **sparse_config):\n    \"\"\"Takes in a model and applies sparsification and quantization to only embeddings & embeddingbags.\n    The quantization step can happen before or after sparsification depending on the `sparsify_first` argument.\n\n    Args:\n        - model (nn.Module)\n            model whose embeddings needs to be sparsified\n        - data_sparsifier_class (type of data sparsifier)\n            Type of sparsification that needs to be applied to model\n        - sparsify_first (bool)\n            if true, sparsifies first and then quantizes\n            otherwise, quantizes first and then sparsifies.\n        - select_embeddings (List of Embedding modules)\n            List of embedding modules to in the model to be sparsified & quantized.\n            If None, all embedding modules with be sparsified\n        - sparse_config (Dict)\n            config that will be passed to the constructor of data sparsifier object.\n\n    Note:\n        1. When `sparsify_first=False`, quantization occurs first followed by sparsification.\n            - before sparsifying, the embedding layers are dequantized.\n            - scales and zero-points are saved\n            - embedding layers are sparsified and `squash_mask` is applied\n            - embedding weights are requantized using the saved scales and zero-points\n        2. When `sparsify_first=True`, sparsification occurs first followed by quantization.\n            - embeddings are sparsified first\n            - quantization is applied on the sparsified embeddings\n    \"\"\"\n    data_sparsifier = data_sparsifier_class(**sparse_config)\n    if select_embeddings is None:\n        embedding_modules = _fetch_all_embeddings(model)\n    else:\n        embedding_modules = []\n        assert isinstance(select_embeddings, List), 'the embedding_modules must be a list of embedding modules'\n        for emb in select_embeddings:\n            assert type(emb) in SUPPORTED_MODULES, 'the embedding_modules list must be an embedding or embedding bags'\n            fqn_name = module_to_fqn(model, emb)\n            assert fqn_name is not None, 'the embedding modules must be part of input model'\n            embedding_modules.append((fqn_name, emb))\n    if sparsify_first:\n        for (name, emb_module) in embedding_modules:\n            valid_name = name.replace('.', '_')\n            data_sparsifier.add_data(name=valid_name, data=emb_module)\n        data_sparsifier.step()\n        data_sparsifier.squash_mask()\n        for (_, emb_module) in embedding_modules:\n            emb_module.qconfig = torch.ao.quantization.float_qparams_weight_only_qconfig\n        torch.ao.quantization.prepare(model, inplace=True)\n        torch.ao.quantization.convert(model, inplace=True)\n    else:\n        for (_, emb_module) in embedding_modules:\n            emb_module.qconfig = torch.ao.quantization.float_qparams_weight_only_qconfig\n        torch.ao.quantization.prepare(model, inplace=True)\n        torch.ao.quantization.convert(model, inplace=True)\n        quantize_params: Dict[str, Dict] = {'scales': {}, 'zero_points': {}, 'dequant_weights': {}, 'axis': {}, 'dtype': {}}\n        for (name, _) in embedding_modules:\n            quantized_emb = fqn_to_module(model, name)\n            assert quantized_emb is not None\n            quantized_weight = quantized_emb.weight()\n            quantize_params['scales'][name] = quantized_weight.q_per_channel_scales()\n            quantize_params['zero_points'][name] = quantized_weight.q_per_channel_zero_points()\n            quantize_params['dequant_weights'][name] = torch.dequantize(quantized_weight)\n            quantize_params['axis'][name] = quantized_weight.q_per_channel_axis()\n            quantize_params['dtype'][name] = quantized_weight.dtype\n            data_sparsifier.add_data(name=name.replace('.', '_'), data=quantize_params['dequant_weights'][name])\n        data_sparsifier.step()\n        data_sparsifier.squash_mask()\n        for (name, _) in embedding_modules:\n            quantized_emb = fqn_to_module(model, name)\n            assert quantized_emb is not None\n            requantized_vector = torch.quantize_per_channel(quantize_params['dequant_weights'][name], scales=quantize_params['scales'][name], zero_points=quantize_params['zero_points'][name], dtype=quantize_params['dtype'][name], axis=quantize_params['axis'][name])\n            quantized_emb.set_weight(requantized_vector)",
        "mutated": [
            "def post_training_sparse_quantize(model, data_sparsifier_class, sparsify_first=True, select_embeddings: Optional[List[nn.Module]]=None, **sparse_config):\n    if False:\n        i = 10\n    'Takes in a model and applies sparsification and quantization to only embeddings & embeddingbags.\\n    The quantization step can happen before or after sparsification depending on the `sparsify_first` argument.\\n\\n    Args:\\n        - model (nn.Module)\\n            model whose embeddings needs to be sparsified\\n        - data_sparsifier_class (type of data sparsifier)\\n            Type of sparsification that needs to be applied to model\\n        - sparsify_first (bool)\\n            if true, sparsifies first and then quantizes\\n            otherwise, quantizes first and then sparsifies.\\n        - select_embeddings (List of Embedding modules)\\n            List of embedding modules to in the model to be sparsified & quantized.\\n            If None, all embedding modules with be sparsified\\n        - sparse_config (Dict)\\n            config that will be passed to the constructor of data sparsifier object.\\n\\n    Note:\\n        1. When `sparsify_first=False`, quantization occurs first followed by sparsification.\\n            - before sparsifying, the embedding layers are dequantized.\\n            - scales and zero-points are saved\\n            - embedding layers are sparsified and `squash_mask` is applied\\n            - embedding weights are requantized using the saved scales and zero-points\\n        2. When `sparsify_first=True`, sparsification occurs first followed by quantization.\\n            - embeddings are sparsified first\\n            - quantization is applied on the sparsified embeddings\\n    '\n    data_sparsifier = data_sparsifier_class(**sparse_config)\n    if select_embeddings is None:\n        embedding_modules = _fetch_all_embeddings(model)\n    else:\n        embedding_modules = []\n        assert isinstance(select_embeddings, List), 'the embedding_modules must be a list of embedding modules'\n        for emb in select_embeddings:\n            assert type(emb) in SUPPORTED_MODULES, 'the embedding_modules list must be an embedding or embedding bags'\n            fqn_name = module_to_fqn(model, emb)\n            assert fqn_name is not None, 'the embedding modules must be part of input model'\n            embedding_modules.append((fqn_name, emb))\n    if sparsify_first:\n        for (name, emb_module) in embedding_modules:\n            valid_name = name.replace('.', '_')\n            data_sparsifier.add_data(name=valid_name, data=emb_module)\n        data_sparsifier.step()\n        data_sparsifier.squash_mask()\n        for (_, emb_module) in embedding_modules:\n            emb_module.qconfig = torch.ao.quantization.float_qparams_weight_only_qconfig\n        torch.ao.quantization.prepare(model, inplace=True)\n        torch.ao.quantization.convert(model, inplace=True)\n    else:\n        for (_, emb_module) in embedding_modules:\n            emb_module.qconfig = torch.ao.quantization.float_qparams_weight_only_qconfig\n        torch.ao.quantization.prepare(model, inplace=True)\n        torch.ao.quantization.convert(model, inplace=True)\n        quantize_params: Dict[str, Dict] = {'scales': {}, 'zero_points': {}, 'dequant_weights': {}, 'axis': {}, 'dtype': {}}\n        for (name, _) in embedding_modules:\n            quantized_emb = fqn_to_module(model, name)\n            assert quantized_emb is not None\n            quantized_weight = quantized_emb.weight()\n            quantize_params['scales'][name] = quantized_weight.q_per_channel_scales()\n            quantize_params['zero_points'][name] = quantized_weight.q_per_channel_zero_points()\n            quantize_params['dequant_weights'][name] = torch.dequantize(quantized_weight)\n            quantize_params['axis'][name] = quantized_weight.q_per_channel_axis()\n            quantize_params['dtype'][name] = quantized_weight.dtype\n            data_sparsifier.add_data(name=name.replace('.', '_'), data=quantize_params['dequant_weights'][name])\n        data_sparsifier.step()\n        data_sparsifier.squash_mask()\n        for (name, _) in embedding_modules:\n            quantized_emb = fqn_to_module(model, name)\n            assert quantized_emb is not None\n            requantized_vector = torch.quantize_per_channel(quantize_params['dequant_weights'][name], scales=quantize_params['scales'][name], zero_points=quantize_params['zero_points'][name], dtype=quantize_params['dtype'][name], axis=quantize_params['axis'][name])\n            quantized_emb.set_weight(requantized_vector)",
            "def post_training_sparse_quantize(model, data_sparsifier_class, sparsify_first=True, select_embeddings: Optional[List[nn.Module]]=None, **sparse_config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Takes in a model and applies sparsification and quantization to only embeddings & embeddingbags.\\n    The quantization step can happen before or after sparsification depending on the `sparsify_first` argument.\\n\\n    Args:\\n        - model (nn.Module)\\n            model whose embeddings needs to be sparsified\\n        - data_sparsifier_class (type of data sparsifier)\\n            Type of sparsification that needs to be applied to model\\n        - sparsify_first (bool)\\n            if true, sparsifies first and then quantizes\\n            otherwise, quantizes first and then sparsifies.\\n        - select_embeddings (List of Embedding modules)\\n            List of embedding modules to in the model to be sparsified & quantized.\\n            If None, all embedding modules with be sparsified\\n        - sparse_config (Dict)\\n            config that will be passed to the constructor of data sparsifier object.\\n\\n    Note:\\n        1. When `sparsify_first=False`, quantization occurs first followed by sparsification.\\n            - before sparsifying, the embedding layers are dequantized.\\n            - scales and zero-points are saved\\n            - embedding layers are sparsified and `squash_mask` is applied\\n            - embedding weights are requantized using the saved scales and zero-points\\n        2. When `sparsify_first=True`, sparsification occurs first followed by quantization.\\n            - embeddings are sparsified first\\n            - quantization is applied on the sparsified embeddings\\n    '\n    data_sparsifier = data_sparsifier_class(**sparse_config)\n    if select_embeddings is None:\n        embedding_modules = _fetch_all_embeddings(model)\n    else:\n        embedding_modules = []\n        assert isinstance(select_embeddings, List), 'the embedding_modules must be a list of embedding modules'\n        for emb in select_embeddings:\n            assert type(emb) in SUPPORTED_MODULES, 'the embedding_modules list must be an embedding or embedding bags'\n            fqn_name = module_to_fqn(model, emb)\n            assert fqn_name is not None, 'the embedding modules must be part of input model'\n            embedding_modules.append((fqn_name, emb))\n    if sparsify_first:\n        for (name, emb_module) in embedding_modules:\n            valid_name = name.replace('.', '_')\n            data_sparsifier.add_data(name=valid_name, data=emb_module)\n        data_sparsifier.step()\n        data_sparsifier.squash_mask()\n        for (_, emb_module) in embedding_modules:\n            emb_module.qconfig = torch.ao.quantization.float_qparams_weight_only_qconfig\n        torch.ao.quantization.prepare(model, inplace=True)\n        torch.ao.quantization.convert(model, inplace=True)\n    else:\n        for (_, emb_module) in embedding_modules:\n            emb_module.qconfig = torch.ao.quantization.float_qparams_weight_only_qconfig\n        torch.ao.quantization.prepare(model, inplace=True)\n        torch.ao.quantization.convert(model, inplace=True)\n        quantize_params: Dict[str, Dict] = {'scales': {}, 'zero_points': {}, 'dequant_weights': {}, 'axis': {}, 'dtype': {}}\n        for (name, _) in embedding_modules:\n            quantized_emb = fqn_to_module(model, name)\n            assert quantized_emb is not None\n            quantized_weight = quantized_emb.weight()\n            quantize_params['scales'][name] = quantized_weight.q_per_channel_scales()\n            quantize_params['zero_points'][name] = quantized_weight.q_per_channel_zero_points()\n            quantize_params['dequant_weights'][name] = torch.dequantize(quantized_weight)\n            quantize_params['axis'][name] = quantized_weight.q_per_channel_axis()\n            quantize_params['dtype'][name] = quantized_weight.dtype\n            data_sparsifier.add_data(name=name.replace('.', '_'), data=quantize_params['dequant_weights'][name])\n        data_sparsifier.step()\n        data_sparsifier.squash_mask()\n        for (name, _) in embedding_modules:\n            quantized_emb = fqn_to_module(model, name)\n            assert quantized_emb is not None\n            requantized_vector = torch.quantize_per_channel(quantize_params['dequant_weights'][name], scales=quantize_params['scales'][name], zero_points=quantize_params['zero_points'][name], dtype=quantize_params['dtype'][name], axis=quantize_params['axis'][name])\n            quantized_emb.set_weight(requantized_vector)",
            "def post_training_sparse_quantize(model, data_sparsifier_class, sparsify_first=True, select_embeddings: Optional[List[nn.Module]]=None, **sparse_config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Takes in a model and applies sparsification and quantization to only embeddings & embeddingbags.\\n    The quantization step can happen before or after sparsification depending on the `sparsify_first` argument.\\n\\n    Args:\\n        - model (nn.Module)\\n            model whose embeddings needs to be sparsified\\n        - data_sparsifier_class (type of data sparsifier)\\n            Type of sparsification that needs to be applied to model\\n        - sparsify_first (bool)\\n            if true, sparsifies first and then quantizes\\n            otherwise, quantizes first and then sparsifies.\\n        - select_embeddings (List of Embedding modules)\\n            List of embedding modules to in the model to be sparsified & quantized.\\n            If None, all embedding modules with be sparsified\\n        - sparse_config (Dict)\\n            config that will be passed to the constructor of data sparsifier object.\\n\\n    Note:\\n        1. When `sparsify_first=False`, quantization occurs first followed by sparsification.\\n            - before sparsifying, the embedding layers are dequantized.\\n            - scales and zero-points are saved\\n            - embedding layers are sparsified and `squash_mask` is applied\\n            - embedding weights are requantized using the saved scales and zero-points\\n        2. When `sparsify_first=True`, sparsification occurs first followed by quantization.\\n            - embeddings are sparsified first\\n            - quantization is applied on the sparsified embeddings\\n    '\n    data_sparsifier = data_sparsifier_class(**sparse_config)\n    if select_embeddings is None:\n        embedding_modules = _fetch_all_embeddings(model)\n    else:\n        embedding_modules = []\n        assert isinstance(select_embeddings, List), 'the embedding_modules must be a list of embedding modules'\n        for emb in select_embeddings:\n            assert type(emb) in SUPPORTED_MODULES, 'the embedding_modules list must be an embedding or embedding bags'\n            fqn_name = module_to_fqn(model, emb)\n            assert fqn_name is not None, 'the embedding modules must be part of input model'\n            embedding_modules.append((fqn_name, emb))\n    if sparsify_first:\n        for (name, emb_module) in embedding_modules:\n            valid_name = name.replace('.', '_')\n            data_sparsifier.add_data(name=valid_name, data=emb_module)\n        data_sparsifier.step()\n        data_sparsifier.squash_mask()\n        for (_, emb_module) in embedding_modules:\n            emb_module.qconfig = torch.ao.quantization.float_qparams_weight_only_qconfig\n        torch.ao.quantization.prepare(model, inplace=True)\n        torch.ao.quantization.convert(model, inplace=True)\n    else:\n        for (_, emb_module) in embedding_modules:\n            emb_module.qconfig = torch.ao.quantization.float_qparams_weight_only_qconfig\n        torch.ao.quantization.prepare(model, inplace=True)\n        torch.ao.quantization.convert(model, inplace=True)\n        quantize_params: Dict[str, Dict] = {'scales': {}, 'zero_points': {}, 'dequant_weights': {}, 'axis': {}, 'dtype': {}}\n        for (name, _) in embedding_modules:\n            quantized_emb = fqn_to_module(model, name)\n            assert quantized_emb is not None\n            quantized_weight = quantized_emb.weight()\n            quantize_params['scales'][name] = quantized_weight.q_per_channel_scales()\n            quantize_params['zero_points'][name] = quantized_weight.q_per_channel_zero_points()\n            quantize_params['dequant_weights'][name] = torch.dequantize(quantized_weight)\n            quantize_params['axis'][name] = quantized_weight.q_per_channel_axis()\n            quantize_params['dtype'][name] = quantized_weight.dtype\n            data_sparsifier.add_data(name=name.replace('.', '_'), data=quantize_params['dequant_weights'][name])\n        data_sparsifier.step()\n        data_sparsifier.squash_mask()\n        for (name, _) in embedding_modules:\n            quantized_emb = fqn_to_module(model, name)\n            assert quantized_emb is not None\n            requantized_vector = torch.quantize_per_channel(quantize_params['dequant_weights'][name], scales=quantize_params['scales'][name], zero_points=quantize_params['zero_points'][name], dtype=quantize_params['dtype'][name], axis=quantize_params['axis'][name])\n            quantized_emb.set_weight(requantized_vector)",
            "def post_training_sparse_quantize(model, data_sparsifier_class, sparsify_first=True, select_embeddings: Optional[List[nn.Module]]=None, **sparse_config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Takes in a model and applies sparsification and quantization to only embeddings & embeddingbags.\\n    The quantization step can happen before or after sparsification depending on the `sparsify_first` argument.\\n\\n    Args:\\n        - model (nn.Module)\\n            model whose embeddings needs to be sparsified\\n        - data_sparsifier_class (type of data sparsifier)\\n            Type of sparsification that needs to be applied to model\\n        - sparsify_first (bool)\\n            if true, sparsifies first and then quantizes\\n            otherwise, quantizes first and then sparsifies.\\n        - select_embeddings (List of Embedding modules)\\n            List of embedding modules to in the model to be sparsified & quantized.\\n            If None, all embedding modules with be sparsified\\n        - sparse_config (Dict)\\n            config that will be passed to the constructor of data sparsifier object.\\n\\n    Note:\\n        1. When `sparsify_first=False`, quantization occurs first followed by sparsification.\\n            - before sparsifying, the embedding layers are dequantized.\\n            - scales and zero-points are saved\\n            - embedding layers are sparsified and `squash_mask` is applied\\n            - embedding weights are requantized using the saved scales and zero-points\\n        2. When `sparsify_first=True`, sparsification occurs first followed by quantization.\\n            - embeddings are sparsified first\\n            - quantization is applied on the sparsified embeddings\\n    '\n    data_sparsifier = data_sparsifier_class(**sparse_config)\n    if select_embeddings is None:\n        embedding_modules = _fetch_all_embeddings(model)\n    else:\n        embedding_modules = []\n        assert isinstance(select_embeddings, List), 'the embedding_modules must be a list of embedding modules'\n        for emb in select_embeddings:\n            assert type(emb) in SUPPORTED_MODULES, 'the embedding_modules list must be an embedding or embedding bags'\n            fqn_name = module_to_fqn(model, emb)\n            assert fqn_name is not None, 'the embedding modules must be part of input model'\n            embedding_modules.append((fqn_name, emb))\n    if sparsify_first:\n        for (name, emb_module) in embedding_modules:\n            valid_name = name.replace('.', '_')\n            data_sparsifier.add_data(name=valid_name, data=emb_module)\n        data_sparsifier.step()\n        data_sparsifier.squash_mask()\n        for (_, emb_module) in embedding_modules:\n            emb_module.qconfig = torch.ao.quantization.float_qparams_weight_only_qconfig\n        torch.ao.quantization.prepare(model, inplace=True)\n        torch.ao.quantization.convert(model, inplace=True)\n    else:\n        for (_, emb_module) in embedding_modules:\n            emb_module.qconfig = torch.ao.quantization.float_qparams_weight_only_qconfig\n        torch.ao.quantization.prepare(model, inplace=True)\n        torch.ao.quantization.convert(model, inplace=True)\n        quantize_params: Dict[str, Dict] = {'scales': {}, 'zero_points': {}, 'dequant_weights': {}, 'axis': {}, 'dtype': {}}\n        for (name, _) in embedding_modules:\n            quantized_emb = fqn_to_module(model, name)\n            assert quantized_emb is not None\n            quantized_weight = quantized_emb.weight()\n            quantize_params['scales'][name] = quantized_weight.q_per_channel_scales()\n            quantize_params['zero_points'][name] = quantized_weight.q_per_channel_zero_points()\n            quantize_params['dequant_weights'][name] = torch.dequantize(quantized_weight)\n            quantize_params['axis'][name] = quantized_weight.q_per_channel_axis()\n            quantize_params['dtype'][name] = quantized_weight.dtype\n            data_sparsifier.add_data(name=name.replace('.', '_'), data=quantize_params['dequant_weights'][name])\n        data_sparsifier.step()\n        data_sparsifier.squash_mask()\n        for (name, _) in embedding_modules:\n            quantized_emb = fqn_to_module(model, name)\n            assert quantized_emb is not None\n            requantized_vector = torch.quantize_per_channel(quantize_params['dequant_weights'][name], scales=quantize_params['scales'][name], zero_points=quantize_params['zero_points'][name], dtype=quantize_params['dtype'][name], axis=quantize_params['axis'][name])\n            quantized_emb.set_weight(requantized_vector)",
            "def post_training_sparse_quantize(model, data_sparsifier_class, sparsify_first=True, select_embeddings: Optional[List[nn.Module]]=None, **sparse_config):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Takes in a model and applies sparsification and quantization to only embeddings & embeddingbags.\\n    The quantization step can happen before or after sparsification depending on the `sparsify_first` argument.\\n\\n    Args:\\n        - model (nn.Module)\\n            model whose embeddings needs to be sparsified\\n        - data_sparsifier_class (type of data sparsifier)\\n            Type of sparsification that needs to be applied to model\\n        - sparsify_first (bool)\\n            if true, sparsifies first and then quantizes\\n            otherwise, quantizes first and then sparsifies.\\n        - select_embeddings (List of Embedding modules)\\n            List of embedding modules to in the model to be sparsified & quantized.\\n            If None, all embedding modules with be sparsified\\n        - sparse_config (Dict)\\n            config that will be passed to the constructor of data sparsifier object.\\n\\n    Note:\\n        1. When `sparsify_first=False`, quantization occurs first followed by sparsification.\\n            - before sparsifying, the embedding layers are dequantized.\\n            - scales and zero-points are saved\\n            - embedding layers are sparsified and `squash_mask` is applied\\n            - embedding weights are requantized using the saved scales and zero-points\\n        2. When `sparsify_first=True`, sparsification occurs first followed by quantization.\\n            - embeddings are sparsified first\\n            - quantization is applied on the sparsified embeddings\\n    '\n    data_sparsifier = data_sparsifier_class(**sparse_config)\n    if select_embeddings is None:\n        embedding_modules = _fetch_all_embeddings(model)\n    else:\n        embedding_modules = []\n        assert isinstance(select_embeddings, List), 'the embedding_modules must be a list of embedding modules'\n        for emb in select_embeddings:\n            assert type(emb) in SUPPORTED_MODULES, 'the embedding_modules list must be an embedding or embedding bags'\n            fqn_name = module_to_fqn(model, emb)\n            assert fqn_name is not None, 'the embedding modules must be part of input model'\n            embedding_modules.append((fqn_name, emb))\n    if sparsify_first:\n        for (name, emb_module) in embedding_modules:\n            valid_name = name.replace('.', '_')\n            data_sparsifier.add_data(name=valid_name, data=emb_module)\n        data_sparsifier.step()\n        data_sparsifier.squash_mask()\n        for (_, emb_module) in embedding_modules:\n            emb_module.qconfig = torch.ao.quantization.float_qparams_weight_only_qconfig\n        torch.ao.quantization.prepare(model, inplace=True)\n        torch.ao.quantization.convert(model, inplace=True)\n    else:\n        for (_, emb_module) in embedding_modules:\n            emb_module.qconfig = torch.ao.quantization.float_qparams_weight_only_qconfig\n        torch.ao.quantization.prepare(model, inplace=True)\n        torch.ao.quantization.convert(model, inplace=True)\n        quantize_params: Dict[str, Dict] = {'scales': {}, 'zero_points': {}, 'dequant_weights': {}, 'axis': {}, 'dtype': {}}\n        for (name, _) in embedding_modules:\n            quantized_emb = fqn_to_module(model, name)\n            assert quantized_emb is not None\n            quantized_weight = quantized_emb.weight()\n            quantize_params['scales'][name] = quantized_weight.q_per_channel_scales()\n            quantize_params['zero_points'][name] = quantized_weight.q_per_channel_zero_points()\n            quantize_params['dequant_weights'][name] = torch.dequantize(quantized_weight)\n            quantize_params['axis'][name] = quantized_weight.q_per_channel_axis()\n            quantize_params['dtype'][name] = quantized_weight.dtype\n            data_sparsifier.add_data(name=name.replace('.', '_'), data=quantize_params['dequant_weights'][name])\n        data_sparsifier.step()\n        data_sparsifier.squash_mask()\n        for (name, _) in embedding_modules:\n            quantized_emb = fqn_to_module(model, name)\n            assert quantized_emb is not None\n            requantized_vector = torch.quantize_per_channel(quantize_params['dequant_weights'][name], scales=quantize_params['scales'][name], zero_points=quantize_params['zero_points'][name], dtype=quantize_params['dtype'][name], axis=quantize_params['axis'][name])\n            quantized_emb.set_weight(requantized_vector)"
        ]
    }
]