[
    {
        "func_name": "test_positional_embeddings",
        "original": "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_positional_embeddings(positional_encoding: Optional[str]):\n    batch_size = 7\n    max_seq_len = 101\n    n_head = 5\n    dims = 11 * n_head\n    transformer = PytorchTransformer(dims, 3, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        assert not torch.isnan(inputs).any()\n        assert torch.isfinite(inputs).all()\n        outputs = transformer(inputs, mask)\n        assert outputs.size() == inputs.size()\n        assert not torch.isnan(outputs).any()\n        assert torch.isfinite(outputs).all()",
        "mutated": [
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_positional_embeddings(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n    batch_size = 7\n    max_seq_len = 101\n    n_head = 5\n    dims = 11 * n_head\n    transformer = PytorchTransformer(dims, 3, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        assert not torch.isnan(inputs).any()\n        assert torch.isfinite(inputs).all()\n        outputs = transformer(inputs, mask)\n        assert outputs.size() == inputs.size()\n        assert not torch.isnan(outputs).any()\n        assert torch.isfinite(outputs).all()",
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_positional_embeddings(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    batch_size = 7\n    max_seq_len = 101\n    n_head = 5\n    dims = 11 * n_head\n    transformer = PytorchTransformer(dims, 3, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        assert not torch.isnan(inputs).any()\n        assert torch.isfinite(inputs).all()\n        outputs = transformer(inputs, mask)\n        assert outputs.size() == inputs.size()\n        assert not torch.isnan(outputs).any()\n        assert torch.isfinite(outputs).all()",
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_positional_embeddings(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    batch_size = 7\n    max_seq_len = 101\n    n_head = 5\n    dims = 11 * n_head\n    transformer = PytorchTransformer(dims, 3, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        assert not torch.isnan(inputs).any()\n        assert torch.isfinite(inputs).all()\n        outputs = transformer(inputs, mask)\n        assert outputs.size() == inputs.size()\n        assert not torch.isnan(outputs).any()\n        assert torch.isfinite(outputs).all()",
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_positional_embeddings(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    batch_size = 7\n    max_seq_len = 101\n    n_head = 5\n    dims = 11 * n_head\n    transformer = PytorchTransformer(dims, 3, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        assert not torch.isnan(inputs).any()\n        assert torch.isfinite(inputs).all()\n        outputs = transformer(inputs, mask)\n        assert outputs.size() == inputs.size()\n        assert not torch.isnan(outputs).any()\n        assert torch.isfinite(outputs).all()",
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_positional_embeddings(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    batch_size = 7\n    max_seq_len = 101\n    n_head = 5\n    dims = 11 * n_head\n    transformer = PytorchTransformer(dims, 3, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        assert not torch.isnan(inputs).any()\n        assert torch.isfinite(inputs).all()\n        outputs = transformer(inputs, mask)\n        assert outputs.size() == inputs.size()\n        assert not torch.isnan(outputs).any()\n        assert torch.isfinite(outputs).all()"
        ]
    },
    {
        "func_name": "test_positional_encodings",
        "original": "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_positional_encodings(positional_encoding: Optional[str]):\n    batch_size = 3\n    max_seq_len = 11\n    n_head = 2\n    dims = 7 * n_head\n    transformer = PytorchTransformer(dims, 2, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        unshuffled_output = transformer(inputs, mask)\n        shuffle = torch.arange(0, max_seq_len).unsqueeze(0).expand_as(mask).clone()\n        for b in range(batch_size):\n            perm = torch.randperm(max_seq_len - b)\n            shuffle[b, :max_seq_len - b] = shuffle[b, perm]\n        shuffle = shuffle.unsqueeze(2).expand_as(inputs)\n        shuffled_input = torch.gather(inputs, 1, shuffle)\n        shuffled_output = transformer(shuffled_input, mask)\n        if positional_encoding is None:\n            assert torch.allclose(torch.gather(unshuffled_output, 1, shuffle), shuffled_output, atol=2e-05)\n        else:\n            assert not torch.allclose(torch.gather(unshuffled_output, 1, shuffle), shuffled_output, atol=2e-05)",
        "mutated": [
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_positional_encodings(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n    batch_size = 3\n    max_seq_len = 11\n    n_head = 2\n    dims = 7 * n_head\n    transformer = PytorchTransformer(dims, 2, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        unshuffled_output = transformer(inputs, mask)\n        shuffle = torch.arange(0, max_seq_len).unsqueeze(0).expand_as(mask).clone()\n        for b in range(batch_size):\n            perm = torch.randperm(max_seq_len - b)\n            shuffle[b, :max_seq_len - b] = shuffle[b, perm]\n        shuffle = shuffle.unsqueeze(2).expand_as(inputs)\n        shuffled_input = torch.gather(inputs, 1, shuffle)\n        shuffled_output = transformer(shuffled_input, mask)\n        if positional_encoding is None:\n            assert torch.allclose(torch.gather(unshuffled_output, 1, shuffle), shuffled_output, atol=2e-05)\n        else:\n            assert not torch.allclose(torch.gather(unshuffled_output, 1, shuffle), shuffled_output, atol=2e-05)",
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_positional_encodings(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    batch_size = 3\n    max_seq_len = 11\n    n_head = 2\n    dims = 7 * n_head\n    transformer = PytorchTransformer(dims, 2, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        unshuffled_output = transformer(inputs, mask)\n        shuffle = torch.arange(0, max_seq_len).unsqueeze(0).expand_as(mask).clone()\n        for b in range(batch_size):\n            perm = torch.randperm(max_seq_len - b)\n            shuffle[b, :max_seq_len - b] = shuffle[b, perm]\n        shuffle = shuffle.unsqueeze(2).expand_as(inputs)\n        shuffled_input = torch.gather(inputs, 1, shuffle)\n        shuffled_output = transformer(shuffled_input, mask)\n        if positional_encoding is None:\n            assert torch.allclose(torch.gather(unshuffled_output, 1, shuffle), shuffled_output, atol=2e-05)\n        else:\n            assert not torch.allclose(torch.gather(unshuffled_output, 1, shuffle), shuffled_output, atol=2e-05)",
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_positional_encodings(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    batch_size = 3\n    max_seq_len = 11\n    n_head = 2\n    dims = 7 * n_head\n    transformer = PytorchTransformer(dims, 2, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        unshuffled_output = transformer(inputs, mask)\n        shuffle = torch.arange(0, max_seq_len).unsqueeze(0).expand_as(mask).clone()\n        for b in range(batch_size):\n            perm = torch.randperm(max_seq_len - b)\n            shuffle[b, :max_seq_len - b] = shuffle[b, perm]\n        shuffle = shuffle.unsqueeze(2).expand_as(inputs)\n        shuffled_input = torch.gather(inputs, 1, shuffle)\n        shuffled_output = transformer(shuffled_input, mask)\n        if positional_encoding is None:\n            assert torch.allclose(torch.gather(unshuffled_output, 1, shuffle), shuffled_output, atol=2e-05)\n        else:\n            assert not torch.allclose(torch.gather(unshuffled_output, 1, shuffle), shuffled_output, atol=2e-05)",
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_positional_encodings(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    batch_size = 3\n    max_seq_len = 11\n    n_head = 2\n    dims = 7 * n_head\n    transformer = PytorchTransformer(dims, 2, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        unshuffled_output = transformer(inputs, mask)\n        shuffle = torch.arange(0, max_seq_len).unsqueeze(0).expand_as(mask).clone()\n        for b in range(batch_size):\n            perm = torch.randperm(max_seq_len - b)\n            shuffle[b, :max_seq_len - b] = shuffle[b, perm]\n        shuffle = shuffle.unsqueeze(2).expand_as(inputs)\n        shuffled_input = torch.gather(inputs, 1, shuffle)\n        shuffled_output = transformer(shuffled_input, mask)\n        if positional_encoding is None:\n            assert torch.allclose(torch.gather(unshuffled_output, 1, shuffle), shuffled_output, atol=2e-05)\n        else:\n            assert not torch.allclose(torch.gather(unshuffled_output, 1, shuffle), shuffled_output, atol=2e-05)",
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_positional_encodings(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    batch_size = 3\n    max_seq_len = 11\n    n_head = 2\n    dims = 7 * n_head\n    transformer = PytorchTransformer(dims, 2, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        unshuffled_output = transformer(inputs, mask)\n        shuffle = torch.arange(0, max_seq_len).unsqueeze(0).expand_as(mask).clone()\n        for b in range(batch_size):\n            perm = torch.randperm(max_seq_len - b)\n            shuffle[b, :max_seq_len - b] = shuffle[b, perm]\n        shuffle = shuffle.unsqueeze(2).expand_as(inputs)\n        shuffled_input = torch.gather(inputs, 1, shuffle)\n        shuffled_output = transformer(shuffled_input, mask)\n        if positional_encoding is None:\n            assert torch.allclose(torch.gather(unshuffled_output, 1, shuffle), shuffled_output, atol=2e-05)\n        else:\n            assert not torch.allclose(torch.gather(unshuffled_output, 1, shuffle), shuffled_output, atol=2e-05)"
        ]
    },
    {
        "func_name": "test_mask_works",
        "original": "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_mask_works(positional_encoding: Optional[str]):\n    batch_size = 3\n    max_seq_len = 11\n    n_head = 2\n    dims = 7 * n_head\n    transformer = PytorchTransformer(dims, 2, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        all_ones_mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        mask = all_ones_mask.clone()\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        altered_inputs = inputs + (~mask).unsqueeze(2) * 10.0\n        assert not torch.allclose(transformer(inputs, all_ones_mask), transformer(altered_inputs, all_ones_mask))\n        assert torch.allclose(torch.masked_select(transformer(inputs, mask), mask.unsqueeze(2)), torch.masked_select(transformer(altered_inputs, mask), mask.unsqueeze(2)))",
        "mutated": [
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_mask_works(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n    batch_size = 3\n    max_seq_len = 11\n    n_head = 2\n    dims = 7 * n_head\n    transformer = PytorchTransformer(dims, 2, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        all_ones_mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        mask = all_ones_mask.clone()\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        altered_inputs = inputs + (~mask).unsqueeze(2) * 10.0\n        assert not torch.allclose(transformer(inputs, all_ones_mask), transformer(altered_inputs, all_ones_mask))\n        assert torch.allclose(torch.masked_select(transformer(inputs, mask), mask.unsqueeze(2)), torch.masked_select(transformer(altered_inputs, mask), mask.unsqueeze(2)))",
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_mask_works(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    batch_size = 3\n    max_seq_len = 11\n    n_head = 2\n    dims = 7 * n_head\n    transformer = PytorchTransformer(dims, 2, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        all_ones_mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        mask = all_ones_mask.clone()\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        altered_inputs = inputs + (~mask).unsqueeze(2) * 10.0\n        assert not torch.allclose(transformer(inputs, all_ones_mask), transformer(altered_inputs, all_ones_mask))\n        assert torch.allclose(torch.masked_select(transformer(inputs, mask), mask.unsqueeze(2)), torch.masked_select(transformer(altered_inputs, mask), mask.unsqueeze(2)))",
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_mask_works(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    batch_size = 3\n    max_seq_len = 11\n    n_head = 2\n    dims = 7 * n_head\n    transformer = PytorchTransformer(dims, 2, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        all_ones_mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        mask = all_ones_mask.clone()\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        altered_inputs = inputs + (~mask).unsqueeze(2) * 10.0\n        assert not torch.allclose(transformer(inputs, all_ones_mask), transformer(altered_inputs, all_ones_mask))\n        assert torch.allclose(torch.masked_select(transformer(inputs, mask), mask.unsqueeze(2)), torch.masked_select(transformer(altered_inputs, mask), mask.unsqueeze(2)))",
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_mask_works(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    batch_size = 3\n    max_seq_len = 11\n    n_head = 2\n    dims = 7 * n_head\n    transformer = PytorchTransformer(dims, 2, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        all_ones_mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        mask = all_ones_mask.clone()\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        altered_inputs = inputs + (~mask).unsqueeze(2) * 10.0\n        assert not torch.allclose(transformer(inputs, all_ones_mask), transformer(altered_inputs, all_ones_mask))\n        assert torch.allclose(torch.masked_select(transformer(inputs, mask), mask.unsqueeze(2)), torch.masked_select(transformer(altered_inputs, mask), mask.unsqueeze(2)))",
            "@pytest.mark.parametrize('positional_encoding', [None, 'sinusoidal', 'embedding'])\ndef test_mask_works(positional_encoding: Optional[str]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    batch_size = 3\n    max_seq_len = 11\n    n_head = 2\n    dims = 7 * n_head\n    transformer = PytorchTransformer(dims, 2, positional_encoding=positional_encoding, num_attention_heads=n_head)\n    transformer.eval()\n    with torch.no_grad():\n        inputs = torch.randn(batch_size, max_seq_len, dims)\n        all_ones_mask = torch.ones(batch_size, max_seq_len, dtype=torch.bool)\n        mask = all_ones_mask.clone()\n        for b in range(batch_size):\n            mask[b, max_seq_len - b:] = False\n        altered_inputs = inputs + (~mask).unsqueeze(2) * 10.0\n        assert not torch.allclose(transformer(inputs, all_ones_mask), transformer(altered_inputs, all_ones_mask))\n        assert torch.allclose(torch.masked_select(transformer(inputs, mask), mask.unsqueeze(2)), torch.masked_select(transformer(altered_inputs, mask), mask.unsqueeze(2)))"
        ]
    }
]