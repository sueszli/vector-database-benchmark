[
    {
        "func_name": "_convert_word_to_ids_tensor",
        "original": "def _convert_word_to_ids_tensor(word, tokenizer, vocab, namespace, all_cases):\n    if all_cases:\n        words_list = [word.lower(), word.title(), word.upper()]\n    else:\n        words_list = [word]\n    ids = []\n    for w in words_list:\n        if vocab:\n            tokens = tokenizer.tokenize(w)\n            ids.append(torch.tensor([vocab.get_token_index(t.text, namespace) for t in tokens]))\n        else:\n            ids.append(torch.tensor(tokenizer.tokenizer(w)['input_ids']))\n    return ids",
        "mutated": [
            "def _convert_word_to_ids_tensor(word, tokenizer, vocab, namespace, all_cases):\n    if False:\n        i = 10\n    if all_cases:\n        words_list = [word.lower(), word.title(), word.upper()]\n    else:\n        words_list = [word]\n    ids = []\n    for w in words_list:\n        if vocab:\n            tokens = tokenizer.tokenize(w)\n            ids.append(torch.tensor([vocab.get_token_index(t.text, namespace) for t in tokens]))\n        else:\n            ids.append(torch.tensor(tokenizer.tokenizer(w)['input_ids']))\n    return ids",
            "def _convert_word_to_ids_tensor(word, tokenizer, vocab, namespace, all_cases):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if all_cases:\n        words_list = [word.lower(), word.title(), word.upper()]\n    else:\n        words_list = [word]\n    ids = []\n    for w in words_list:\n        if vocab:\n            tokens = tokenizer.tokenize(w)\n            ids.append(torch.tensor([vocab.get_token_index(t.text, namespace) for t in tokens]))\n        else:\n            ids.append(torch.tensor(tokenizer.tokenizer(w)['input_ids']))\n    return ids",
            "def _convert_word_to_ids_tensor(word, tokenizer, vocab, namespace, all_cases):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if all_cases:\n        words_list = [word.lower(), word.title(), word.upper()]\n    else:\n        words_list = [word]\n    ids = []\n    for w in words_list:\n        if vocab:\n            tokens = tokenizer.tokenize(w)\n            ids.append(torch.tensor([vocab.get_token_index(t.text, namespace) for t in tokens]))\n        else:\n            ids.append(torch.tensor(tokenizer.tokenizer(w)['input_ids']))\n    return ids",
            "def _convert_word_to_ids_tensor(word, tokenizer, vocab, namespace, all_cases):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if all_cases:\n        words_list = [word.lower(), word.title(), word.upper()]\n    else:\n        words_list = [word]\n    ids = []\n    for w in words_list:\n        if vocab:\n            tokens = tokenizer.tokenize(w)\n            ids.append(torch.tensor([vocab.get_token_index(t.text, namespace) for t in tokens]))\n        else:\n            ids.append(torch.tensor(tokenizer.tokenizer(w)['input_ids']))\n    return ids",
            "def _convert_word_to_ids_tensor(word, tokenizer, vocab, namespace, all_cases):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if all_cases:\n        words_list = [word.lower(), word.title(), word.upper()]\n    else:\n        words_list = [word]\n    ids = []\n    for w in words_list:\n        if vocab:\n            tokens = tokenizer.tokenize(w)\n            ids.append(torch.tensor([vocab.get_token_index(t.text, namespace) for t in tokens]))\n        else:\n            ids.append(torch.tensor(tokenizer.tokenizer(w)['input_ids']))\n    return ids"
        ]
    },
    {
        "func_name": "load_words",
        "original": "def load_words(fname: Union[str, PathLike], tokenizer: Tokenizer, vocab: Optional[Vocabulary]=None, namespace: str='tokens', all_cases: bool=True) -> List[torch.Tensor]:\n    \"\"\"\n    This function loads a list of words from a file,\n    tokenizes each word into subword tokens, and converts the\n    tokens into IDs.\n\n    # Parameters\n\n    fname : `Union[str, PathLike]`\n        Name of file containing list of words to load.\n    tokenizer : `Tokenizer`\n        Tokenizer to tokenize words in file.\n    vocab : `Vocabulary`, optional (default=`None`)\n        Vocabulary of tokenizer. If `None`, assumes tokenizer is of\n        type `PreTrainedTokenizer` and uses tokenizer's `vocab` attribute.\n    namespace : `str`\n        Namespace of vocab to use when tokenizing.\n    all_cases : `bool`, optional (default=`True`)\n        Whether to tokenize lower, title, and upper cases of each word.\n\n    # Returns\n\n    word_ids : `List[torch.Tensor]`\n        List of tensors containing the IDs of subword tokens for\n        each word in the file.\n    \"\"\"\n    word_ids = []\n    with open(cached_path(fname)) as f:\n        words = json.load(f)\n        for w in words:\n            word_ids.extend(_convert_word_to_ids_tensor(w, tokenizer, vocab, namespace, all_cases))\n    return word_ids",
        "mutated": [
            "def load_words(fname: Union[str, PathLike], tokenizer: Tokenizer, vocab: Optional[Vocabulary]=None, namespace: str='tokens', all_cases: bool=True) -> List[torch.Tensor]:\n    if False:\n        i = 10\n    \"\\n    This function loads a list of words from a file,\\n    tokenizes each word into subword tokens, and converts the\\n    tokens into IDs.\\n\\n    # Parameters\\n\\n    fname : `Union[str, PathLike]`\\n        Name of file containing list of words to load.\\n    tokenizer : `Tokenizer`\\n        Tokenizer to tokenize words in file.\\n    vocab : `Vocabulary`, optional (default=`None`)\\n        Vocabulary of tokenizer. If `None`, assumes tokenizer is of\\n        type `PreTrainedTokenizer` and uses tokenizer's `vocab` attribute.\\n    namespace : `str`\\n        Namespace of vocab to use when tokenizing.\\n    all_cases : `bool`, optional (default=`True`)\\n        Whether to tokenize lower, title, and upper cases of each word.\\n\\n    # Returns\\n\\n    word_ids : `List[torch.Tensor]`\\n        List of tensors containing the IDs of subword tokens for\\n        each word in the file.\\n    \"\n    word_ids = []\n    with open(cached_path(fname)) as f:\n        words = json.load(f)\n        for w in words:\n            word_ids.extend(_convert_word_to_ids_tensor(w, tokenizer, vocab, namespace, all_cases))\n    return word_ids",
            "def load_words(fname: Union[str, PathLike], tokenizer: Tokenizer, vocab: Optional[Vocabulary]=None, namespace: str='tokens', all_cases: bool=True) -> List[torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    This function loads a list of words from a file,\\n    tokenizes each word into subword tokens, and converts the\\n    tokens into IDs.\\n\\n    # Parameters\\n\\n    fname : `Union[str, PathLike]`\\n        Name of file containing list of words to load.\\n    tokenizer : `Tokenizer`\\n        Tokenizer to tokenize words in file.\\n    vocab : `Vocabulary`, optional (default=`None`)\\n        Vocabulary of tokenizer. If `None`, assumes tokenizer is of\\n        type `PreTrainedTokenizer` and uses tokenizer's `vocab` attribute.\\n    namespace : `str`\\n        Namespace of vocab to use when tokenizing.\\n    all_cases : `bool`, optional (default=`True`)\\n        Whether to tokenize lower, title, and upper cases of each word.\\n\\n    # Returns\\n\\n    word_ids : `List[torch.Tensor]`\\n        List of tensors containing the IDs of subword tokens for\\n        each word in the file.\\n    \"\n    word_ids = []\n    with open(cached_path(fname)) as f:\n        words = json.load(f)\n        for w in words:\n            word_ids.extend(_convert_word_to_ids_tensor(w, tokenizer, vocab, namespace, all_cases))\n    return word_ids",
            "def load_words(fname: Union[str, PathLike], tokenizer: Tokenizer, vocab: Optional[Vocabulary]=None, namespace: str='tokens', all_cases: bool=True) -> List[torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    This function loads a list of words from a file,\\n    tokenizes each word into subword tokens, and converts the\\n    tokens into IDs.\\n\\n    # Parameters\\n\\n    fname : `Union[str, PathLike]`\\n        Name of file containing list of words to load.\\n    tokenizer : `Tokenizer`\\n        Tokenizer to tokenize words in file.\\n    vocab : `Vocabulary`, optional (default=`None`)\\n        Vocabulary of tokenizer. If `None`, assumes tokenizer is of\\n        type `PreTrainedTokenizer` and uses tokenizer's `vocab` attribute.\\n    namespace : `str`\\n        Namespace of vocab to use when tokenizing.\\n    all_cases : `bool`, optional (default=`True`)\\n        Whether to tokenize lower, title, and upper cases of each word.\\n\\n    # Returns\\n\\n    word_ids : `List[torch.Tensor]`\\n        List of tensors containing the IDs of subword tokens for\\n        each word in the file.\\n    \"\n    word_ids = []\n    with open(cached_path(fname)) as f:\n        words = json.load(f)\n        for w in words:\n            word_ids.extend(_convert_word_to_ids_tensor(w, tokenizer, vocab, namespace, all_cases))\n    return word_ids",
            "def load_words(fname: Union[str, PathLike], tokenizer: Tokenizer, vocab: Optional[Vocabulary]=None, namespace: str='tokens', all_cases: bool=True) -> List[torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    This function loads a list of words from a file,\\n    tokenizes each word into subword tokens, and converts the\\n    tokens into IDs.\\n\\n    # Parameters\\n\\n    fname : `Union[str, PathLike]`\\n        Name of file containing list of words to load.\\n    tokenizer : `Tokenizer`\\n        Tokenizer to tokenize words in file.\\n    vocab : `Vocabulary`, optional (default=`None`)\\n        Vocabulary of tokenizer. If `None`, assumes tokenizer is of\\n        type `PreTrainedTokenizer` and uses tokenizer's `vocab` attribute.\\n    namespace : `str`\\n        Namespace of vocab to use when tokenizing.\\n    all_cases : `bool`, optional (default=`True`)\\n        Whether to tokenize lower, title, and upper cases of each word.\\n\\n    # Returns\\n\\n    word_ids : `List[torch.Tensor]`\\n        List of tensors containing the IDs of subword tokens for\\n        each word in the file.\\n    \"\n    word_ids = []\n    with open(cached_path(fname)) as f:\n        words = json.load(f)\n        for w in words:\n            word_ids.extend(_convert_word_to_ids_tensor(w, tokenizer, vocab, namespace, all_cases))\n    return word_ids",
            "def load_words(fname: Union[str, PathLike], tokenizer: Tokenizer, vocab: Optional[Vocabulary]=None, namespace: str='tokens', all_cases: bool=True) -> List[torch.Tensor]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    This function loads a list of words from a file,\\n    tokenizes each word into subword tokens, and converts the\\n    tokens into IDs.\\n\\n    # Parameters\\n\\n    fname : `Union[str, PathLike]`\\n        Name of file containing list of words to load.\\n    tokenizer : `Tokenizer`\\n        Tokenizer to tokenize words in file.\\n    vocab : `Vocabulary`, optional (default=`None`)\\n        Vocabulary of tokenizer. If `None`, assumes tokenizer is of\\n        type `PreTrainedTokenizer` and uses tokenizer's `vocab` attribute.\\n    namespace : `str`\\n        Namespace of vocab to use when tokenizing.\\n    all_cases : `bool`, optional (default=`True`)\\n        Whether to tokenize lower, title, and upper cases of each word.\\n\\n    # Returns\\n\\n    word_ids : `List[torch.Tensor]`\\n        List of tensors containing the IDs of subword tokens for\\n        each word in the file.\\n    \"\n    word_ids = []\n    with open(cached_path(fname)) as f:\n        words = json.load(f)\n        for w in words:\n            word_ids.extend(_convert_word_to_ids_tensor(w, tokenizer, vocab, namespace, all_cases))\n    return word_ids"
        ]
    },
    {
        "func_name": "load_word_pairs",
        "original": "def load_word_pairs(fname: Union[str, PathLike], tokenizer: Tokenizer, vocab: Optional[Vocabulary]=None, namespace: str='token', all_cases: bool=True) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    \"\"\"\n    This function loads a list of pairs of words from a file,\n    tokenizes each word into subword tokens, and converts the\n    tokens into IDs.\n\n    # Parameters\n\n    fname : `Union[str, PathLike]`\n        Name of file containing list of pairs of words to load.\n    tokenizer : `Tokenizer`\n        Tokenizer to tokenize words in file.\n    vocab : `Vocabulary`, optional (default=`None`)\n        Vocabulary of tokenizer. If `None`, assumes tokenizer is of\n        type `PreTrainedTokenizer` and uses tokenizer's `vocab` attribute.\n    namespace : `str`\n        Namespace of vocab to use when tokenizing.\n    all_cases : `bool`, optional (default=`True`)\n        Whether to tokenize lower, title, and upper cases of each word.\n\n    # Returns\n\n    word_ids : `Tuple[List[torch.Tensor], List[torch.Tensor]]`\n        Pair of lists of tensors containing the IDs of subword tokens for\n        words in the file.\n    \"\"\"\n    word_ids1 = []\n    word_ids2 = []\n    with open(cached_path(fname)) as f:\n        words = json.load(f)\n        for (w1, w2) in words:\n            word_ids1.extend(_convert_word_to_ids_tensor(w1, tokenizer, vocab, namespace, all_cases))\n            word_ids2.extend(_convert_word_to_ids_tensor(w2, tokenizer, vocab, namespace, all_cases))\n    return (word_ids1, word_ids2)",
        "mutated": [
            "def load_word_pairs(fname: Union[str, PathLike], tokenizer: Tokenizer, vocab: Optional[Vocabulary]=None, namespace: str='token', all_cases: bool=True) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    if False:\n        i = 10\n    \"\\n    This function loads a list of pairs of words from a file,\\n    tokenizes each word into subword tokens, and converts the\\n    tokens into IDs.\\n\\n    # Parameters\\n\\n    fname : `Union[str, PathLike]`\\n        Name of file containing list of pairs of words to load.\\n    tokenizer : `Tokenizer`\\n        Tokenizer to tokenize words in file.\\n    vocab : `Vocabulary`, optional (default=`None`)\\n        Vocabulary of tokenizer. If `None`, assumes tokenizer is of\\n        type `PreTrainedTokenizer` and uses tokenizer's `vocab` attribute.\\n    namespace : `str`\\n        Namespace of vocab to use when tokenizing.\\n    all_cases : `bool`, optional (default=`True`)\\n        Whether to tokenize lower, title, and upper cases of each word.\\n\\n    # Returns\\n\\n    word_ids : `Tuple[List[torch.Tensor], List[torch.Tensor]]`\\n        Pair of lists of tensors containing the IDs of subword tokens for\\n        words in the file.\\n    \"\n    word_ids1 = []\n    word_ids2 = []\n    with open(cached_path(fname)) as f:\n        words = json.load(f)\n        for (w1, w2) in words:\n            word_ids1.extend(_convert_word_to_ids_tensor(w1, tokenizer, vocab, namespace, all_cases))\n            word_ids2.extend(_convert_word_to_ids_tensor(w2, tokenizer, vocab, namespace, all_cases))\n    return (word_ids1, word_ids2)",
            "def load_word_pairs(fname: Union[str, PathLike], tokenizer: Tokenizer, vocab: Optional[Vocabulary]=None, namespace: str='token', all_cases: bool=True) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    This function loads a list of pairs of words from a file,\\n    tokenizes each word into subword tokens, and converts the\\n    tokens into IDs.\\n\\n    # Parameters\\n\\n    fname : `Union[str, PathLike]`\\n        Name of file containing list of pairs of words to load.\\n    tokenizer : `Tokenizer`\\n        Tokenizer to tokenize words in file.\\n    vocab : `Vocabulary`, optional (default=`None`)\\n        Vocabulary of tokenizer. If `None`, assumes tokenizer is of\\n        type `PreTrainedTokenizer` and uses tokenizer's `vocab` attribute.\\n    namespace : `str`\\n        Namespace of vocab to use when tokenizing.\\n    all_cases : `bool`, optional (default=`True`)\\n        Whether to tokenize lower, title, and upper cases of each word.\\n\\n    # Returns\\n\\n    word_ids : `Tuple[List[torch.Tensor], List[torch.Tensor]]`\\n        Pair of lists of tensors containing the IDs of subword tokens for\\n        words in the file.\\n    \"\n    word_ids1 = []\n    word_ids2 = []\n    with open(cached_path(fname)) as f:\n        words = json.load(f)\n        for (w1, w2) in words:\n            word_ids1.extend(_convert_word_to_ids_tensor(w1, tokenizer, vocab, namespace, all_cases))\n            word_ids2.extend(_convert_word_to_ids_tensor(w2, tokenizer, vocab, namespace, all_cases))\n    return (word_ids1, word_ids2)",
            "def load_word_pairs(fname: Union[str, PathLike], tokenizer: Tokenizer, vocab: Optional[Vocabulary]=None, namespace: str='token', all_cases: bool=True) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    This function loads a list of pairs of words from a file,\\n    tokenizes each word into subword tokens, and converts the\\n    tokens into IDs.\\n\\n    # Parameters\\n\\n    fname : `Union[str, PathLike]`\\n        Name of file containing list of pairs of words to load.\\n    tokenizer : `Tokenizer`\\n        Tokenizer to tokenize words in file.\\n    vocab : `Vocabulary`, optional (default=`None`)\\n        Vocabulary of tokenizer. If `None`, assumes tokenizer is of\\n        type `PreTrainedTokenizer` and uses tokenizer's `vocab` attribute.\\n    namespace : `str`\\n        Namespace of vocab to use when tokenizing.\\n    all_cases : `bool`, optional (default=`True`)\\n        Whether to tokenize lower, title, and upper cases of each word.\\n\\n    # Returns\\n\\n    word_ids : `Tuple[List[torch.Tensor], List[torch.Tensor]]`\\n        Pair of lists of tensors containing the IDs of subword tokens for\\n        words in the file.\\n    \"\n    word_ids1 = []\n    word_ids2 = []\n    with open(cached_path(fname)) as f:\n        words = json.load(f)\n        for (w1, w2) in words:\n            word_ids1.extend(_convert_word_to_ids_tensor(w1, tokenizer, vocab, namespace, all_cases))\n            word_ids2.extend(_convert_word_to_ids_tensor(w2, tokenizer, vocab, namespace, all_cases))\n    return (word_ids1, word_ids2)",
            "def load_word_pairs(fname: Union[str, PathLike], tokenizer: Tokenizer, vocab: Optional[Vocabulary]=None, namespace: str='token', all_cases: bool=True) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    This function loads a list of pairs of words from a file,\\n    tokenizes each word into subword tokens, and converts the\\n    tokens into IDs.\\n\\n    # Parameters\\n\\n    fname : `Union[str, PathLike]`\\n        Name of file containing list of pairs of words to load.\\n    tokenizer : `Tokenizer`\\n        Tokenizer to tokenize words in file.\\n    vocab : `Vocabulary`, optional (default=`None`)\\n        Vocabulary of tokenizer. If `None`, assumes tokenizer is of\\n        type `PreTrainedTokenizer` and uses tokenizer's `vocab` attribute.\\n    namespace : `str`\\n        Namespace of vocab to use when tokenizing.\\n    all_cases : `bool`, optional (default=`True`)\\n        Whether to tokenize lower, title, and upper cases of each word.\\n\\n    # Returns\\n\\n    word_ids : `Tuple[List[torch.Tensor], List[torch.Tensor]]`\\n        Pair of lists of tensors containing the IDs of subword tokens for\\n        words in the file.\\n    \"\n    word_ids1 = []\n    word_ids2 = []\n    with open(cached_path(fname)) as f:\n        words = json.load(f)\n        for (w1, w2) in words:\n            word_ids1.extend(_convert_word_to_ids_tensor(w1, tokenizer, vocab, namespace, all_cases))\n            word_ids2.extend(_convert_word_to_ids_tensor(w2, tokenizer, vocab, namespace, all_cases))\n    return (word_ids1, word_ids2)",
            "def load_word_pairs(fname: Union[str, PathLike], tokenizer: Tokenizer, vocab: Optional[Vocabulary]=None, namespace: str='token', all_cases: bool=True) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    This function loads a list of pairs of words from a file,\\n    tokenizes each word into subword tokens, and converts the\\n    tokens into IDs.\\n\\n    # Parameters\\n\\n    fname : `Union[str, PathLike]`\\n        Name of file containing list of pairs of words to load.\\n    tokenizer : `Tokenizer`\\n        Tokenizer to tokenize words in file.\\n    vocab : `Vocabulary`, optional (default=`None`)\\n        Vocabulary of tokenizer. If `None`, assumes tokenizer is of\\n        type `PreTrainedTokenizer` and uses tokenizer's `vocab` attribute.\\n    namespace : `str`\\n        Namespace of vocab to use when tokenizing.\\n    all_cases : `bool`, optional (default=`True`)\\n        Whether to tokenize lower, title, and upper cases of each word.\\n\\n    # Returns\\n\\n    word_ids : `Tuple[List[torch.Tensor], List[torch.Tensor]]`\\n        Pair of lists of tensors containing the IDs of subword tokens for\\n        words in the file.\\n    \"\n    word_ids1 = []\n    word_ids2 = []\n    with open(cached_path(fname)) as f:\n        words = json.load(f)\n        for (w1, w2) in words:\n            word_ids1.extend(_convert_word_to_ids_tensor(w1, tokenizer, vocab, namespace, all_cases))\n            word_ids2.extend(_convert_word_to_ids_tensor(w2, tokenizer, vocab, namespace, all_cases))\n    return (word_ids1, word_ids2)"
        ]
    }
]