[
    {
        "func_name": "__init__",
        "original": "def __init__(self):\n    pass",
        "mutated": [
            "def __init__(self):\n    if False:\n        i = 10\n    pass",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pass",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pass",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pass",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pass"
        ]
    },
    {
        "func_name": "is_fp16_cast_op",
        "original": "@staticmethod\ndef is_fp16_cast_op(block, op, params):\n    if op.type != 'cast':\n        return False\n    if is_optimizer_op(op):\n        return False\n    assert len(op.desc.input_arg_names()) == 1\n    assert len(op.desc.output_arg_names()) == 1\n    (input_name, output_name) = (op.desc.input_arg_names()[0], op.desc.output_arg_names()[0])\n    if input_name not in params:\n        return False\n    input_var = block.var(input_name)\n    output_var = block.var(output_name)\n    if input_var.dtype != core.VarDesc.VarType.FP32 or output_var.dtype != core.VarDesc.VarType.FP16:\n        return False\n    return True",
        "mutated": [
            "@staticmethod\ndef is_fp16_cast_op(block, op, params):\n    if False:\n        i = 10\n    if op.type != 'cast':\n        return False\n    if is_optimizer_op(op):\n        return False\n    assert len(op.desc.input_arg_names()) == 1\n    assert len(op.desc.output_arg_names()) == 1\n    (input_name, output_name) = (op.desc.input_arg_names()[0], op.desc.output_arg_names()[0])\n    if input_name not in params:\n        return False\n    input_var = block.var(input_name)\n    output_var = block.var(output_name)\n    if input_var.dtype != core.VarDesc.VarType.FP32 or output_var.dtype != core.VarDesc.VarType.FP16:\n        return False\n    return True",
            "@staticmethod\ndef is_fp16_cast_op(block, op, params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if op.type != 'cast':\n        return False\n    if is_optimizer_op(op):\n        return False\n    assert len(op.desc.input_arg_names()) == 1\n    assert len(op.desc.output_arg_names()) == 1\n    (input_name, output_name) = (op.desc.input_arg_names()[0], op.desc.output_arg_names()[0])\n    if input_name not in params:\n        return False\n    input_var = block.var(input_name)\n    output_var = block.var(output_name)\n    if input_var.dtype != core.VarDesc.VarType.FP32 or output_var.dtype != core.VarDesc.VarType.FP16:\n        return False\n    return True",
            "@staticmethod\ndef is_fp16_cast_op(block, op, params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if op.type != 'cast':\n        return False\n    if is_optimizer_op(op):\n        return False\n    assert len(op.desc.input_arg_names()) == 1\n    assert len(op.desc.output_arg_names()) == 1\n    (input_name, output_name) = (op.desc.input_arg_names()[0], op.desc.output_arg_names()[0])\n    if input_name not in params:\n        return False\n    input_var = block.var(input_name)\n    output_var = block.var(output_name)\n    if input_var.dtype != core.VarDesc.VarType.FP32 or output_var.dtype != core.VarDesc.VarType.FP16:\n        return False\n    return True",
            "@staticmethod\ndef is_fp16_cast_op(block, op, params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if op.type != 'cast':\n        return False\n    if is_optimizer_op(op):\n        return False\n    assert len(op.desc.input_arg_names()) == 1\n    assert len(op.desc.output_arg_names()) == 1\n    (input_name, output_name) = (op.desc.input_arg_names()[0], op.desc.output_arg_names()[0])\n    if input_name not in params:\n        return False\n    input_var = block.var(input_name)\n    output_var = block.var(output_name)\n    if input_var.dtype != core.VarDesc.VarType.FP32 or output_var.dtype != core.VarDesc.VarType.FP16:\n        return False\n    return True",
            "@staticmethod\ndef is_fp16_cast_op(block, op, params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if op.type != 'cast':\n        return False\n    if is_optimizer_op(op):\n        return False\n    assert len(op.desc.input_arg_names()) == 1\n    assert len(op.desc.output_arg_names()) == 1\n    (input_name, output_name) = (op.desc.input_arg_names()[0], op.desc.output_arg_names()[0])\n    if input_name not in params:\n        return False\n    input_var = block.var(input_name)\n    output_var = block.var(output_name)\n    if input_var.dtype != core.VarDesc.VarType.FP32 or output_var.dtype != core.VarDesc.VarType.FP16:\n        return False\n    return True"
        ]
    },
    {
        "func_name": "is_fp32_cast_op",
        "original": "@staticmethod\ndef is_fp32_cast_op(block, op):\n    if op.type != 'cast':\n        return False\n    if not is_optimizer_op(op):\n        return False\n    assert len(op.desc.input_arg_names()) == 1\n    assert len(op.desc.output_arg_names()) == 1\n    (input_name, output_name) = (op.desc.input_arg_names()[0], op.desc.output_arg_names()[0])\n    input_var = block.var(input_name)\n    output_var = block.var(output_name)\n    if input_var.dtype != core.VarDesc.VarType.FP16 or output_var.dtype != core.VarDesc.VarType.FP32:\n        return False\n    return True",
        "mutated": [
            "@staticmethod\ndef is_fp32_cast_op(block, op):\n    if False:\n        i = 10\n    if op.type != 'cast':\n        return False\n    if not is_optimizer_op(op):\n        return False\n    assert len(op.desc.input_arg_names()) == 1\n    assert len(op.desc.output_arg_names()) == 1\n    (input_name, output_name) = (op.desc.input_arg_names()[0], op.desc.output_arg_names()[0])\n    input_var = block.var(input_name)\n    output_var = block.var(output_name)\n    if input_var.dtype != core.VarDesc.VarType.FP16 or output_var.dtype != core.VarDesc.VarType.FP32:\n        return False\n    return True",
            "@staticmethod\ndef is_fp32_cast_op(block, op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if op.type != 'cast':\n        return False\n    if not is_optimizer_op(op):\n        return False\n    assert len(op.desc.input_arg_names()) == 1\n    assert len(op.desc.output_arg_names()) == 1\n    (input_name, output_name) = (op.desc.input_arg_names()[0], op.desc.output_arg_names()[0])\n    input_var = block.var(input_name)\n    output_var = block.var(output_name)\n    if input_var.dtype != core.VarDesc.VarType.FP16 or output_var.dtype != core.VarDesc.VarType.FP32:\n        return False\n    return True",
            "@staticmethod\ndef is_fp32_cast_op(block, op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if op.type != 'cast':\n        return False\n    if not is_optimizer_op(op):\n        return False\n    assert len(op.desc.input_arg_names()) == 1\n    assert len(op.desc.output_arg_names()) == 1\n    (input_name, output_name) = (op.desc.input_arg_names()[0], op.desc.output_arg_names()[0])\n    input_var = block.var(input_name)\n    output_var = block.var(output_name)\n    if input_var.dtype != core.VarDesc.VarType.FP16 or output_var.dtype != core.VarDesc.VarType.FP32:\n        return False\n    return True",
            "@staticmethod\ndef is_fp32_cast_op(block, op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if op.type != 'cast':\n        return False\n    if not is_optimizer_op(op):\n        return False\n    assert len(op.desc.input_arg_names()) == 1\n    assert len(op.desc.output_arg_names()) == 1\n    (input_name, output_name) = (op.desc.input_arg_names()[0], op.desc.output_arg_names()[0])\n    input_var = block.var(input_name)\n    output_var = block.var(output_name)\n    if input_var.dtype != core.VarDesc.VarType.FP16 or output_var.dtype != core.VarDesc.VarType.FP32:\n        return False\n    return True",
            "@staticmethod\ndef is_fp32_cast_op(block, op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if op.type != 'cast':\n        return False\n    if not is_optimizer_op(op):\n        return False\n    assert len(op.desc.input_arg_names()) == 1\n    assert len(op.desc.output_arg_names()) == 1\n    (input_name, output_name) = (op.desc.input_arg_names()[0], op.desc.output_arg_names()[0])\n    input_var = block.var(input_name)\n    output_var = block.var(output_name)\n    if input_var.dtype != core.VarDesc.VarType.FP16 or output_var.dtype != core.VarDesc.VarType.FP32:\n        return False\n    return True"
        ]
    },
    {
        "func_name": "remove_cast_op",
        "original": "@staticmethod\ndef remove_cast_op(block, params, segment, offset):\n    inserted_op_num = 0\n    for op_idx in reversed(range(offset + segment._start_idx, offset + segment._end_idx)):\n        op = block.ops[op_idx]\n        if FP16Utils.is_fp16_cast_op(block, op, params):\n            block._remove_op(op_idx, sync=False)\n            inserted_op_num -= 1\n    block._sync_with_cpp()\n    return inserted_op_num",
        "mutated": [
            "@staticmethod\ndef remove_cast_op(block, params, segment, offset):\n    if False:\n        i = 10\n    inserted_op_num = 0\n    for op_idx in reversed(range(offset + segment._start_idx, offset + segment._end_idx)):\n        op = block.ops[op_idx]\n        if FP16Utils.is_fp16_cast_op(block, op, params):\n            block._remove_op(op_idx, sync=False)\n            inserted_op_num -= 1\n    block._sync_with_cpp()\n    return inserted_op_num",
            "@staticmethod\ndef remove_cast_op(block, params, segment, offset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    inserted_op_num = 0\n    for op_idx in reversed(range(offset + segment._start_idx, offset + segment._end_idx)):\n        op = block.ops[op_idx]\n        if FP16Utils.is_fp16_cast_op(block, op, params):\n            block._remove_op(op_idx, sync=False)\n            inserted_op_num -= 1\n    block._sync_with_cpp()\n    return inserted_op_num",
            "@staticmethod\ndef remove_cast_op(block, params, segment, offset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    inserted_op_num = 0\n    for op_idx in reversed(range(offset + segment._start_idx, offset + segment._end_idx)):\n        op = block.ops[op_idx]\n        if FP16Utils.is_fp16_cast_op(block, op, params):\n            block._remove_op(op_idx, sync=False)\n            inserted_op_num -= 1\n    block._sync_with_cpp()\n    return inserted_op_num",
            "@staticmethod\ndef remove_cast_op(block, params, segment, offset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    inserted_op_num = 0\n    for op_idx in reversed(range(offset + segment._start_idx, offset + segment._end_idx)):\n        op = block.ops[op_idx]\n        if FP16Utils.is_fp16_cast_op(block, op, params):\n            block._remove_op(op_idx, sync=False)\n            inserted_op_num -= 1\n    block._sync_with_cpp()\n    return inserted_op_num",
            "@staticmethod\ndef remove_cast_op(block, params, segment, offset):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    inserted_op_num = 0\n    for op_idx in reversed(range(offset + segment._start_idx, offset + segment._end_idx)):\n        op = block.ops[op_idx]\n        if FP16Utils.is_fp16_cast_op(block, op, params):\n            block._remove_op(op_idx, sync=False)\n            inserted_op_num -= 1\n    block._sync_with_cpp()\n    return inserted_op_num"
        ]
    },
    {
        "func_name": "prune_fp16",
        "original": "@staticmethod\ndef prune_fp16(block, shard, reduced_grads_to_param, ring_ids):\n    \"\"\"\n        1. prune all cast_fp16_to_fp32 ops if the param not belongs to this shard\n        2. revise amp inifine grad checking for sharding\n        \"\"\"\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if not FP16Utils.is_fp32_cast_op(block, op):\n            continue\n        output_name = op.desc.output_arg_names()[0]\n        param_name = output_name.strip('@GRAD@MERGED') if '@MERGED' in output_name else output_name.strip('@GRAD')\n        if param_name not in shard.global_params:\n            raise ValueError(f\"Output 'X' of cast_op must be a grad ofmodel param, but {output_name} is not a grad\")\n        if output_name in reduced_grads_to_param:\n            continue\n        if shard.has_param(param_name):\n            continue\n        block._remove_op(idx, sync=False)\n        block._remove_var(output_name, sync=False)\n    block._sync_with_cpp()\n    update_loss_scaling_op_idx = -1\n    inf_var_name = ''\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type == 'update_loss_scaling':\n            update_loss_scaling_op_idx = idx\n            inf_var_name = op.desc.input('FoundInfinite')[0]\n        if op.type in ['check_finite_and_unscale', 'update_loss_scaling']:\n            reversed_x = []\n            reversed_x_paramname = []\n            for input_name in op.desc.input('X'):\n                if '@MERGED' in input_name:\n                    param_name = input_name.strip('@GRAD@MERGED')\n                else:\n                    param_name = input_name.strip('@GRAD')\n                if param_name not in shard.global_params:\n                    raise ValueError(f\"Input 'X' of check_finite_and_unscale mustbe grads, but {input_name} is not a grad\")\n                if shard.has_param(param_name):\n                    reversed_x.append(input_name)\n                    reversed_x_paramname.append(param_name)\n            op.desc.set_input('X', reversed_x)\n            op.desc.set_output('Out', reversed_x)\n            to_check_param = set(reversed_x_paramname)\n            should_check_param = set(shard.global_params).intersection({param for (param, worker_idx) in shard.global_param2device.items() if worker_idx == shard.worker_idx})\n            assert to_check_param == should_check_param, 'amp                     check_finite_and_unscale checking miss [{}] and got unexpected [{}]'.format(should_check_param - to_check_param, to_check_param - should_check_param)\n    if update_loss_scaling_op_idx == -1:\n        return\n    inf_var = block.var(inf_var_name)\n    inf_var_int32 = block.create_var(name=inf_var_name + '@cast_int32', shape=inf_var.shape, dtype=core.VarDesc.VarType.INT32)\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var}, outputs={'Out': inf_var_int32}, attrs={'in_dtype': inf_var.dtype, 'out_dtype': inf_var_int32.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    for ring_id in ring_ids:\n        if ring_id == -1:\n            continue\n        block._insert_op_without_sync(update_loss_scaling_op_idx, type='c_allreduce_max', inputs={'X': inf_var_int32}, outputs={'Out': inf_var_int32}, attrs={'ring_id': ring_id, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Optimize})\n        update_loss_scaling_op_idx += 1\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var_int32}, outputs={'Out': inf_var}, attrs={'in_dtype': inf_var_int32.dtype, 'out_dtype': inf_var.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    block._sync_with_cpp()",
        "mutated": [
            "@staticmethod\ndef prune_fp16(block, shard, reduced_grads_to_param, ring_ids):\n    if False:\n        i = 10\n    '\\n        1. prune all cast_fp16_to_fp32 ops if the param not belongs to this shard\\n        2. revise amp inifine grad checking for sharding\\n        '\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if not FP16Utils.is_fp32_cast_op(block, op):\n            continue\n        output_name = op.desc.output_arg_names()[0]\n        param_name = output_name.strip('@GRAD@MERGED') if '@MERGED' in output_name else output_name.strip('@GRAD')\n        if param_name not in shard.global_params:\n            raise ValueError(f\"Output 'X' of cast_op must be a grad ofmodel param, but {output_name} is not a grad\")\n        if output_name in reduced_grads_to_param:\n            continue\n        if shard.has_param(param_name):\n            continue\n        block._remove_op(idx, sync=False)\n        block._remove_var(output_name, sync=False)\n    block._sync_with_cpp()\n    update_loss_scaling_op_idx = -1\n    inf_var_name = ''\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type == 'update_loss_scaling':\n            update_loss_scaling_op_idx = idx\n            inf_var_name = op.desc.input('FoundInfinite')[0]\n        if op.type in ['check_finite_and_unscale', 'update_loss_scaling']:\n            reversed_x = []\n            reversed_x_paramname = []\n            for input_name in op.desc.input('X'):\n                if '@MERGED' in input_name:\n                    param_name = input_name.strip('@GRAD@MERGED')\n                else:\n                    param_name = input_name.strip('@GRAD')\n                if param_name not in shard.global_params:\n                    raise ValueError(f\"Input 'X' of check_finite_and_unscale mustbe grads, but {input_name} is not a grad\")\n                if shard.has_param(param_name):\n                    reversed_x.append(input_name)\n                    reversed_x_paramname.append(param_name)\n            op.desc.set_input('X', reversed_x)\n            op.desc.set_output('Out', reversed_x)\n            to_check_param = set(reversed_x_paramname)\n            should_check_param = set(shard.global_params).intersection({param for (param, worker_idx) in shard.global_param2device.items() if worker_idx == shard.worker_idx})\n            assert to_check_param == should_check_param, 'amp                     check_finite_and_unscale checking miss [{}] and got unexpected [{}]'.format(should_check_param - to_check_param, to_check_param - should_check_param)\n    if update_loss_scaling_op_idx == -1:\n        return\n    inf_var = block.var(inf_var_name)\n    inf_var_int32 = block.create_var(name=inf_var_name + '@cast_int32', shape=inf_var.shape, dtype=core.VarDesc.VarType.INT32)\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var}, outputs={'Out': inf_var_int32}, attrs={'in_dtype': inf_var.dtype, 'out_dtype': inf_var_int32.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    for ring_id in ring_ids:\n        if ring_id == -1:\n            continue\n        block._insert_op_without_sync(update_loss_scaling_op_idx, type='c_allreduce_max', inputs={'X': inf_var_int32}, outputs={'Out': inf_var_int32}, attrs={'ring_id': ring_id, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Optimize})\n        update_loss_scaling_op_idx += 1\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var_int32}, outputs={'Out': inf_var}, attrs={'in_dtype': inf_var_int32.dtype, 'out_dtype': inf_var.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    block._sync_with_cpp()",
            "@staticmethod\ndef prune_fp16(block, shard, reduced_grads_to_param, ring_ids):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        1. prune all cast_fp16_to_fp32 ops if the param not belongs to this shard\\n        2. revise amp inifine grad checking for sharding\\n        '\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if not FP16Utils.is_fp32_cast_op(block, op):\n            continue\n        output_name = op.desc.output_arg_names()[0]\n        param_name = output_name.strip('@GRAD@MERGED') if '@MERGED' in output_name else output_name.strip('@GRAD')\n        if param_name not in shard.global_params:\n            raise ValueError(f\"Output 'X' of cast_op must be a grad ofmodel param, but {output_name} is not a grad\")\n        if output_name in reduced_grads_to_param:\n            continue\n        if shard.has_param(param_name):\n            continue\n        block._remove_op(idx, sync=False)\n        block._remove_var(output_name, sync=False)\n    block._sync_with_cpp()\n    update_loss_scaling_op_idx = -1\n    inf_var_name = ''\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type == 'update_loss_scaling':\n            update_loss_scaling_op_idx = idx\n            inf_var_name = op.desc.input('FoundInfinite')[0]\n        if op.type in ['check_finite_and_unscale', 'update_loss_scaling']:\n            reversed_x = []\n            reversed_x_paramname = []\n            for input_name in op.desc.input('X'):\n                if '@MERGED' in input_name:\n                    param_name = input_name.strip('@GRAD@MERGED')\n                else:\n                    param_name = input_name.strip('@GRAD')\n                if param_name not in shard.global_params:\n                    raise ValueError(f\"Input 'X' of check_finite_and_unscale mustbe grads, but {input_name} is not a grad\")\n                if shard.has_param(param_name):\n                    reversed_x.append(input_name)\n                    reversed_x_paramname.append(param_name)\n            op.desc.set_input('X', reversed_x)\n            op.desc.set_output('Out', reversed_x)\n            to_check_param = set(reversed_x_paramname)\n            should_check_param = set(shard.global_params).intersection({param for (param, worker_idx) in shard.global_param2device.items() if worker_idx == shard.worker_idx})\n            assert to_check_param == should_check_param, 'amp                     check_finite_and_unscale checking miss [{}] and got unexpected [{}]'.format(should_check_param - to_check_param, to_check_param - should_check_param)\n    if update_loss_scaling_op_idx == -1:\n        return\n    inf_var = block.var(inf_var_name)\n    inf_var_int32 = block.create_var(name=inf_var_name + '@cast_int32', shape=inf_var.shape, dtype=core.VarDesc.VarType.INT32)\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var}, outputs={'Out': inf_var_int32}, attrs={'in_dtype': inf_var.dtype, 'out_dtype': inf_var_int32.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    for ring_id in ring_ids:\n        if ring_id == -1:\n            continue\n        block._insert_op_without_sync(update_loss_scaling_op_idx, type='c_allreduce_max', inputs={'X': inf_var_int32}, outputs={'Out': inf_var_int32}, attrs={'ring_id': ring_id, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Optimize})\n        update_loss_scaling_op_idx += 1\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var_int32}, outputs={'Out': inf_var}, attrs={'in_dtype': inf_var_int32.dtype, 'out_dtype': inf_var.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    block._sync_with_cpp()",
            "@staticmethod\ndef prune_fp16(block, shard, reduced_grads_to_param, ring_ids):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        1. prune all cast_fp16_to_fp32 ops if the param not belongs to this shard\\n        2. revise amp inifine grad checking for sharding\\n        '\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if not FP16Utils.is_fp32_cast_op(block, op):\n            continue\n        output_name = op.desc.output_arg_names()[0]\n        param_name = output_name.strip('@GRAD@MERGED') if '@MERGED' in output_name else output_name.strip('@GRAD')\n        if param_name not in shard.global_params:\n            raise ValueError(f\"Output 'X' of cast_op must be a grad ofmodel param, but {output_name} is not a grad\")\n        if output_name in reduced_grads_to_param:\n            continue\n        if shard.has_param(param_name):\n            continue\n        block._remove_op(idx, sync=False)\n        block._remove_var(output_name, sync=False)\n    block._sync_with_cpp()\n    update_loss_scaling_op_idx = -1\n    inf_var_name = ''\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type == 'update_loss_scaling':\n            update_loss_scaling_op_idx = idx\n            inf_var_name = op.desc.input('FoundInfinite')[0]\n        if op.type in ['check_finite_and_unscale', 'update_loss_scaling']:\n            reversed_x = []\n            reversed_x_paramname = []\n            for input_name in op.desc.input('X'):\n                if '@MERGED' in input_name:\n                    param_name = input_name.strip('@GRAD@MERGED')\n                else:\n                    param_name = input_name.strip('@GRAD')\n                if param_name not in shard.global_params:\n                    raise ValueError(f\"Input 'X' of check_finite_and_unscale mustbe grads, but {input_name} is not a grad\")\n                if shard.has_param(param_name):\n                    reversed_x.append(input_name)\n                    reversed_x_paramname.append(param_name)\n            op.desc.set_input('X', reversed_x)\n            op.desc.set_output('Out', reversed_x)\n            to_check_param = set(reversed_x_paramname)\n            should_check_param = set(shard.global_params).intersection({param for (param, worker_idx) in shard.global_param2device.items() if worker_idx == shard.worker_idx})\n            assert to_check_param == should_check_param, 'amp                     check_finite_and_unscale checking miss [{}] and got unexpected [{}]'.format(should_check_param - to_check_param, to_check_param - should_check_param)\n    if update_loss_scaling_op_idx == -1:\n        return\n    inf_var = block.var(inf_var_name)\n    inf_var_int32 = block.create_var(name=inf_var_name + '@cast_int32', shape=inf_var.shape, dtype=core.VarDesc.VarType.INT32)\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var}, outputs={'Out': inf_var_int32}, attrs={'in_dtype': inf_var.dtype, 'out_dtype': inf_var_int32.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    for ring_id in ring_ids:\n        if ring_id == -1:\n            continue\n        block._insert_op_without_sync(update_loss_scaling_op_idx, type='c_allreduce_max', inputs={'X': inf_var_int32}, outputs={'Out': inf_var_int32}, attrs={'ring_id': ring_id, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Optimize})\n        update_loss_scaling_op_idx += 1\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var_int32}, outputs={'Out': inf_var}, attrs={'in_dtype': inf_var_int32.dtype, 'out_dtype': inf_var.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    block._sync_with_cpp()",
            "@staticmethod\ndef prune_fp16(block, shard, reduced_grads_to_param, ring_ids):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        1. prune all cast_fp16_to_fp32 ops if the param not belongs to this shard\\n        2. revise amp inifine grad checking for sharding\\n        '\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if not FP16Utils.is_fp32_cast_op(block, op):\n            continue\n        output_name = op.desc.output_arg_names()[0]\n        param_name = output_name.strip('@GRAD@MERGED') if '@MERGED' in output_name else output_name.strip('@GRAD')\n        if param_name not in shard.global_params:\n            raise ValueError(f\"Output 'X' of cast_op must be a grad ofmodel param, but {output_name} is not a grad\")\n        if output_name in reduced_grads_to_param:\n            continue\n        if shard.has_param(param_name):\n            continue\n        block._remove_op(idx, sync=False)\n        block._remove_var(output_name, sync=False)\n    block._sync_with_cpp()\n    update_loss_scaling_op_idx = -1\n    inf_var_name = ''\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type == 'update_loss_scaling':\n            update_loss_scaling_op_idx = idx\n            inf_var_name = op.desc.input('FoundInfinite')[0]\n        if op.type in ['check_finite_and_unscale', 'update_loss_scaling']:\n            reversed_x = []\n            reversed_x_paramname = []\n            for input_name in op.desc.input('X'):\n                if '@MERGED' in input_name:\n                    param_name = input_name.strip('@GRAD@MERGED')\n                else:\n                    param_name = input_name.strip('@GRAD')\n                if param_name not in shard.global_params:\n                    raise ValueError(f\"Input 'X' of check_finite_and_unscale mustbe grads, but {input_name} is not a grad\")\n                if shard.has_param(param_name):\n                    reversed_x.append(input_name)\n                    reversed_x_paramname.append(param_name)\n            op.desc.set_input('X', reversed_x)\n            op.desc.set_output('Out', reversed_x)\n            to_check_param = set(reversed_x_paramname)\n            should_check_param = set(shard.global_params).intersection({param for (param, worker_idx) in shard.global_param2device.items() if worker_idx == shard.worker_idx})\n            assert to_check_param == should_check_param, 'amp                     check_finite_and_unscale checking miss [{}] and got unexpected [{}]'.format(should_check_param - to_check_param, to_check_param - should_check_param)\n    if update_loss_scaling_op_idx == -1:\n        return\n    inf_var = block.var(inf_var_name)\n    inf_var_int32 = block.create_var(name=inf_var_name + '@cast_int32', shape=inf_var.shape, dtype=core.VarDesc.VarType.INT32)\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var}, outputs={'Out': inf_var_int32}, attrs={'in_dtype': inf_var.dtype, 'out_dtype': inf_var_int32.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    for ring_id in ring_ids:\n        if ring_id == -1:\n            continue\n        block._insert_op_without_sync(update_loss_scaling_op_idx, type='c_allreduce_max', inputs={'X': inf_var_int32}, outputs={'Out': inf_var_int32}, attrs={'ring_id': ring_id, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Optimize})\n        update_loss_scaling_op_idx += 1\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var_int32}, outputs={'Out': inf_var}, attrs={'in_dtype': inf_var_int32.dtype, 'out_dtype': inf_var.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    block._sync_with_cpp()",
            "@staticmethod\ndef prune_fp16(block, shard, reduced_grads_to_param, ring_ids):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        1. prune all cast_fp16_to_fp32 ops if the param not belongs to this shard\\n        2. revise amp inifine grad checking for sharding\\n        '\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if not FP16Utils.is_fp32_cast_op(block, op):\n            continue\n        output_name = op.desc.output_arg_names()[0]\n        param_name = output_name.strip('@GRAD@MERGED') if '@MERGED' in output_name else output_name.strip('@GRAD')\n        if param_name not in shard.global_params:\n            raise ValueError(f\"Output 'X' of cast_op must be a grad ofmodel param, but {output_name} is not a grad\")\n        if output_name in reduced_grads_to_param:\n            continue\n        if shard.has_param(param_name):\n            continue\n        block._remove_op(idx, sync=False)\n        block._remove_var(output_name, sync=False)\n    block._sync_with_cpp()\n    update_loss_scaling_op_idx = -1\n    inf_var_name = ''\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type == 'update_loss_scaling':\n            update_loss_scaling_op_idx = idx\n            inf_var_name = op.desc.input('FoundInfinite')[0]\n        if op.type in ['check_finite_and_unscale', 'update_loss_scaling']:\n            reversed_x = []\n            reversed_x_paramname = []\n            for input_name in op.desc.input('X'):\n                if '@MERGED' in input_name:\n                    param_name = input_name.strip('@GRAD@MERGED')\n                else:\n                    param_name = input_name.strip('@GRAD')\n                if param_name not in shard.global_params:\n                    raise ValueError(f\"Input 'X' of check_finite_and_unscale mustbe grads, but {input_name} is not a grad\")\n                if shard.has_param(param_name):\n                    reversed_x.append(input_name)\n                    reversed_x_paramname.append(param_name)\n            op.desc.set_input('X', reversed_x)\n            op.desc.set_output('Out', reversed_x)\n            to_check_param = set(reversed_x_paramname)\n            should_check_param = set(shard.global_params).intersection({param for (param, worker_idx) in shard.global_param2device.items() if worker_idx == shard.worker_idx})\n            assert to_check_param == should_check_param, 'amp                     check_finite_and_unscale checking miss [{}] and got unexpected [{}]'.format(should_check_param - to_check_param, to_check_param - should_check_param)\n    if update_loss_scaling_op_idx == -1:\n        return\n    inf_var = block.var(inf_var_name)\n    inf_var_int32 = block.create_var(name=inf_var_name + '@cast_int32', shape=inf_var.shape, dtype=core.VarDesc.VarType.INT32)\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var}, outputs={'Out': inf_var_int32}, attrs={'in_dtype': inf_var.dtype, 'out_dtype': inf_var_int32.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    for ring_id in ring_ids:\n        if ring_id == -1:\n            continue\n        block._insert_op_without_sync(update_loss_scaling_op_idx, type='c_allreduce_max', inputs={'X': inf_var_int32}, outputs={'Out': inf_var_int32}, attrs={'ring_id': ring_id, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Optimize})\n        update_loss_scaling_op_idx += 1\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var_int32}, outputs={'Out': inf_var}, attrs={'in_dtype': inf_var_int32.dtype, 'out_dtype': inf_var.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    block._sync_with_cpp()"
        ]
    },
    {
        "func_name": "sync_amp_check_nan_inf",
        "original": "@staticmethod\ndef sync_amp_check_nan_inf(block, ring_ids):\n    update_loss_scaling_op_idx = -1\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type == 'update_loss_scaling':\n            update_loss_scaling_op_idx = idx\n            inf_var_name = op.desc.input('FoundInfinite')[0]\n            break\n    if update_loss_scaling_op_idx == -1:\n        return\n    inf_var = block.var(inf_var_name)\n    inf_var_int32 = block.create_var(name=inf_var_name + '@cast_int32', shape=inf_var.shape, dtype=core.VarDesc.VarType.INT32)\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var}, outputs={'Out': inf_var_int32}, attrs={'in_dtype': inf_var.dtype, 'out_dtype': inf_var_int32.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    for ring_id in ring_ids:\n        if ring_id == -1:\n            continue\n        block._insert_op_without_sync(update_loss_scaling_op_idx, type='c_allreduce_max', inputs={'X': inf_var_int32}, outputs={'Out': inf_var_int32}, attrs={'ring_id': ring_id, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Optimize})\n        update_loss_scaling_op_idx += 1\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var_int32}, outputs={'Out': inf_var}, attrs={'in_dtype': inf_var_int32.dtype, 'out_dtype': inf_var.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    block._sync_with_cpp()",
        "mutated": [
            "@staticmethod\ndef sync_amp_check_nan_inf(block, ring_ids):\n    if False:\n        i = 10\n    update_loss_scaling_op_idx = -1\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type == 'update_loss_scaling':\n            update_loss_scaling_op_idx = idx\n            inf_var_name = op.desc.input('FoundInfinite')[0]\n            break\n    if update_loss_scaling_op_idx == -1:\n        return\n    inf_var = block.var(inf_var_name)\n    inf_var_int32 = block.create_var(name=inf_var_name + '@cast_int32', shape=inf_var.shape, dtype=core.VarDesc.VarType.INT32)\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var}, outputs={'Out': inf_var_int32}, attrs={'in_dtype': inf_var.dtype, 'out_dtype': inf_var_int32.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    for ring_id in ring_ids:\n        if ring_id == -1:\n            continue\n        block._insert_op_without_sync(update_loss_scaling_op_idx, type='c_allreduce_max', inputs={'X': inf_var_int32}, outputs={'Out': inf_var_int32}, attrs={'ring_id': ring_id, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Optimize})\n        update_loss_scaling_op_idx += 1\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var_int32}, outputs={'Out': inf_var}, attrs={'in_dtype': inf_var_int32.dtype, 'out_dtype': inf_var.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    block._sync_with_cpp()",
            "@staticmethod\ndef sync_amp_check_nan_inf(block, ring_ids):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    update_loss_scaling_op_idx = -1\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type == 'update_loss_scaling':\n            update_loss_scaling_op_idx = idx\n            inf_var_name = op.desc.input('FoundInfinite')[0]\n            break\n    if update_loss_scaling_op_idx == -1:\n        return\n    inf_var = block.var(inf_var_name)\n    inf_var_int32 = block.create_var(name=inf_var_name + '@cast_int32', shape=inf_var.shape, dtype=core.VarDesc.VarType.INT32)\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var}, outputs={'Out': inf_var_int32}, attrs={'in_dtype': inf_var.dtype, 'out_dtype': inf_var_int32.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    for ring_id in ring_ids:\n        if ring_id == -1:\n            continue\n        block._insert_op_without_sync(update_loss_scaling_op_idx, type='c_allreduce_max', inputs={'X': inf_var_int32}, outputs={'Out': inf_var_int32}, attrs={'ring_id': ring_id, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Optimize})\n        update_loss_scaling_op_idx += 1\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var_int32}, outputs={'Out': inf_var}, attrs={'in_dtype': inf_var_int32.dtype, 'out_dtype': inf_var.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    block._sync_with_cpp()",
            "@staticmethod\ndef sync_amp_check_nan_inf(block, ring_ids):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    update_loss_scaling_op_idx = -1\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type == 'update_loss_scaling':\n            update_loss_scaling_op_idx = idx\n            inf_var_name = op.desc.input('FoundInfinite')[0]\n            break\n    if update_loss_scaling_op_idx == -1:\n        return\n    inf_var = block.var(inf_var_name)\n    inf_var_int32 = block.create_var(name=inf_var_name + '@cast_int32', shape=inf_var.shape, dtype=core.VarDesc.VarType.INT32)\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var}, outputs={'Out': inf_var_int32}, attrs={'in_dtype': inf_var.dtype, 'out_dtype': inf_var_int32.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    for ring_id in ring_ids:\n        if ring_id == -1:\n            continue\n        block._insert_op_without_sync(update_loss_scaling_op_idx, type='c_allreduce_max', inputs={'X': inf_var_int32}, outputs={'Out': inf_var_int32}, attrs={'ring_id': ring_id, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Optimize})\n        update_loss_scaling_op_idx += 1\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var_int32}, outputs={'Out': inf_var}, attrs={'in_dtype': inf_var_int32.dtype, 'out_dtype': inf_var.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    block._sync_with_cpp()",
            "@staticmethod\ndef sync_amp_check_nan_inf(block, ring_ids):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    update_loss_scaling_op_idx = -1\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type == 'update_loss_scaling':\n            update_loss_scaling_op_idx = idx\n            inf_var_name = op.desc.input('FoundInfinite')[0]\n            break\n    if update_loss_scaling_op_idx == -1:\n        return\n    inf_var = block.var(inf_var_name)\n    inf_var_int32 = block.create_var(name=inf_var_name + '@cast_int32', shape=inf_var.shape, dtype=core.VarDesc.VarType.INT32)\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var}, outputs={'Out': inf_var_int32}, attrs={'in_dtype': inf_var.dtype, 'out_dtype': inf_var_int32.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    for ring_id in ring_ids:\n        if ring_id == -1:\n            continue\n        block._insert_op_without_sync(update_loss_scaling_op_idx, type='c_allreduce_max', inputs={'X': inf_var_int32}, outputs={'Out': inf_var_int32}, attrs={'ring_id': ring_id, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Optimize})\n        update_loss_scaling_op_idx += 1\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var_int32}, outputs={'Out': inf_var}, attrs={'in_dtype': inf_var_int32.dtype, 'out_dtype': inf_var.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    block._sync_with_cpp()",
            "@staticmethod\ndef sync_amp_check_nan_inf(block, ring_ids):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    update_loss_scaling_op_idx = -1\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type == 'update_loss_scaling':\n            update_loss_scaling_op_idx = idx\n            inf_var_name = op.desc.input('FoundInfinite')[0]\n            break\n    if update_loss_scaling_op_idx == -1:\n        return\n    inf_var = block.var(inf_var_name)\n    inf_var_int32 = block.create_var(name=inf_var_name + '@cast_int32', shape=inf_var.shape, dtype=core.VarDesc.VarType.INT32)\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var}, outputs={'Out': inf_var_int32}, attrs={'in_dtype': inf_var.dtype, 'out_dtype': inf_var_int32.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    for ring_id in ring_ids:\n        if ring_id == -1:\n            continue\n        block._insert_op_without_sync(update_loss_scaling_op_idx, type='c_allreduce_max', inputs={'X': inf_var_int32}, outputs={'Out': inf_var_int32}, attrs={'ring_id': ring_id, 'use_calc_stream': True, OP_ROLE_KEY: OpRole.Optimize})\n        update_loss_scaling_op_idx += 1\n    block._insert_op_without_sync(update_loss_scaling_op_idx, type='cast', inputs={'X': inf_var_int32}, outputs={'Out': inf_var}, attrs={'in_dtype': inf_var_int32.dtype, 'out_dtype': inf_var.dtype, OP_ROLE_KEY: OpRole.Optimize})\n    update_loss_scaling_op_idx += 1\n    block._sync_with_cpp()"
        ]
    }
]