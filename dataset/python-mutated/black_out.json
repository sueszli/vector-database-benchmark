[
    {
        "func_name": "black_out",
        "original": "def black_out(x, t, W, samples, reduce='mean'):\n    \"\"\"BlackOut loss function.\n\n    BlackOut loss function is defined as\n\n    .. math::\n\n      -\\\\log(p(t)) - \\\\sum_{s \\\\in S} \\\\log(1 - p(s)),\n\n    where :math:`t` is the correct label, :math:`S` is a set of negative\n    examples and :math:`p(\\\\cdot)` is likelihood of a given label.\n    And, :math:`p` is defined as\n\n    .. math::\n\n       p(y) = \\\\frac{\\\\exp(W_y^\\\\top x)}{\n       \\\\sum_{s \\\\in samples} \\\\exp(W_s^\\\\top x)}.\n\n    The output is a variable whose value depends on the value of\n    the option ``reduce``. If it is ``'no'``, it holds the\n    no loss values. If it is ``'mean'``, this function takes\n    a mean of loss values.\n\n    Args:\n        x (:class:`~chainer.Variable` or :ref:`ndarray`):\n            Batch of input vectors.\n            Its shape should be :math:`(N, D)`.\n        t (:class:`~chainer.Variable` or :ref:`ndarray`):\n            Vector of ground truth labels.\n            Its shape should be :math:`(N,)`. Each elements :math:`v`\n            should satisfy :math:`0 \\\\geq v \\\\geq V` or :math:`-1`\n            where :math:`V` is the number of label types.\n        W (:class:`~chainer.Variable` or :ref:`ndarray`):\n            Weight matrix.\n            Its shape should be :math:`(V, D)`\n        samples (~chainer.Variable): Negative samples.\n            Its shape should be :math:`(N, S)` where :math:`S` is\n            the number of negative samples.\n        reduce (str): Reduction option. Its value must be either\n            ``'no'`` or ``'mean'``. Otherwise,\n            :class:`ValueError` is raised.\n\n    Returns:\n        ~chainer.Variable:\n            A variable object holding loss value(s).\n            If ``reduce`` is ``'no'``, the output variable holds an\n            array whose shape is :math:`(N,)` .\n            If it is ``'mean'``, it holds a scalar.\n\n    See: `BlackOut: Speeding up Recurrent Neural Network Language Models With\n    Very Large Vocabularies <https://arxiv.org/abs/1511.06909>`_\n\n    .. seealso::\n\n        :class:`~chainer.links.BlackOut` to manage the model parameter ``W``.\n\n    \"\"\"\n    batch_size = x.shape[0]\n    neg_emb = embed_id.embed_id(samples, W)\n    neg_y = matmul.matmul(neg_emb, x[:, :, None])\n    neg_y = reshape.reshape(neg_y, neg_y.shape[:-1])\n    pos_emb = expand_dims.expand_dims(embed_id.embed_id(t, W), 1)\n    pos_y = matmul.matmul(pos_emb, x[:, :, None])\n    pos_y = reshape.reshape(pos_y, pos_y.shape[:-1])\n    logz = logsumexp.logsumexp(concat.concat([pos_y, neg_y]), axis=1)\n    (blogz, bneg_y) = broadcast.broadcast(reshape.reshape(logz, (batch_size, 1)), neg_y)\n    ny = exponential.log(1 - exponential.exp(bneg_y - blogz))\n    py = reshape.reshape(pos_y, (batch_size,))\n    loss = -(py - logz + _sum.sum(ny, axis=1))\n    if reduce == 'mean':\n        loss = average.average(loss)\n    return loss",
        "mutated": [
            "def black_out(x, t, W, samples, reduce='mean'):\n    if False:\n        i = 10\n    \"BlackOut loss function.\\n\\n    BlackOut loss function is defined as\\n\\n    .. math::\\n\\n      -\\\\log(p(t)) - \\\\sum_{s \\\\in S} \\\\log(1 - p(s)),\\n\\n    where :math:`t` is the correct label, :math:`S` is a set of negative\\n    examples and :math:`p(\\\\cdot)` is likelihood of a given label.\\n    And, :math:`p` is defined as\\n\\n    .. math::\\n\\n       p(y) = \\\\frac{\\\\exp(W_y^\\\\top x)}{\\n       \\\\sum_{s \\\\in samples} \\\\exp(W_s^\\\\top x)}.\\n\\n    The output is a variable whose value depends on the value of\\n    the option ``reduce``. If it is ``'no'``, it holds the\\n    no loss values. If it is ``'mean'``, this function takes\\n    a mean of loss values.\\n\\n    Args:\\n        x (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Batch of input vectors.\\n            Its shape should be :math:`(N, D)`.\\n        t (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Vector of ground truth labels.\\n            Its shape should be :math:`(N,)`. Each elements :math:`v`\\n            should satisfy :math:`0 \\\\geq v \\\\geq V` or :math:`-1`\\n            where :math:`V` is the number of label types.\\n        W (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Weight matrix.\\n            Its shape should be :math:`(V, D)`\\n        samples (~chainer.Variable): Negative samples.\\n            Its shape should be :math:`(N, S)` where :math:`S` is\\n            the number of negative samples.\\n        reduce (str): Reduction option. Its value must be either\\n            ``'no'`` or ``'mean'``. Otherwise,\\n            :class:`ValueError` is raised.\\n\\n    Returns:\\n        ~chainer.Variable:\\n            A variable object holding loss value(s).\\n            If ``reduce`` is ``'no'``, the output variable holds an\\n            array whose shape is :math:`(N,)` .\\n            If it is ``'mean'``, it holds a scalar.\\n\\n    See: `BlackOut: Speeding up Recurrent Neural Network Language Models With\\n    Very Large Vocabularies <https://arxiv.org/abs/1511.06909>`_\\n\\n    .. seealso::\\n\\n        :class:`~chainer.links.BlackOut` to manage the model parameter ``W``.\\n\\n    \"\n    batch_size = x.shape[0]\n    neg_emb = embed_id.embed_id(samples, W)\n    neg_y = matmul.matmul(neg_emb, x[:, :, None])\n    neg_y = reshape.reshape(neg_y, neg_y.shape[:-1])\n    pos_emb = expand_dims.expand_dims(embed_id.embed_id(t, W), 1)\n    pos_y = matmul.matmul(pos_emb, x[:, :, None])\n    pos_y = reshape.reshape(pos_y, pos_y.shape[:-1])\n    logz = logsumexp.logsumexp(concat.concat([pos_y, neg_y]), axis=1)\n    (blogz, bneg_y) = broadcast.broadcast(reshape.reshape(logz, (batch_size, 1)), neg_y)\n    ny = exponential.log(1 - exponential.exp(bneg_y - blogz))\n    py = reshape.reshape(pos_y, (batch_size,))\n    loss = -(py - logz + _sum.sum(ny, axis=1))\n    if reduce == 'mean':\n        loss = average.average(loss)\n    return loss",
            "def black_out(x, t, W, samples, reduce='mean'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"BlackOut loss function.\\n\\n    BlackOut loss function is defined as\\n\\n    .. math::\\n\\n      -\\\\log(p(t)) - \\\\sum_{s \\\\in S} \\\\log(1 - p(s)),\\n\\n    where :math:`t` is the correct label, :math:`S` is a set of negative\\n    examples and :math:`p(\\\\cdot)` is likelihood of a given label.\\n    And, :math:`p` is defined as\\n\\n    .. math::\\n\\n       p(y) = \\\\frac{\\\\exp(W_y^\\\\top x)}{\\n       \\\\sum_{s \\\\in samples} \\\\exp(W_s^\\\\top x)}.\\n\\n    The output is a variable whose value depends on the value of\\n    the option ``reduce``. If it is ``'no'``, it holds the\\n    no loss values. If it is ``'mean'``, this function takes\\n    a mean of loss values.\\n\\n    Args:\\n        x (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Batch of input vectors.\\n            Its shape should be :math:`(N, D)`.\\n        t (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Vector of ground truth labels.\\n            Its shape should be :math:`(N,)`. Each elements :math:`v`\\n            should satisfy :math:`0 \\\\geq v \\\\geq V` or :math:`-1`\\n            where :math:`V` is the number of label types.\\n        W (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Weight matrix.\\n            Its shape should be :math:`(V, D)`\\n        samples (~chainer.Variable): Negative samples.\\n            Its shape should be :math:`(N, S)` where :math:`S` is\\n            the number of negative samples.\\n        reduce (str): Reduction option. Its value must be either\\n            ``'no'`` or ``'mean'``. Otherwise,\\n            :class:`ValueError` is raised.\\n\\n    Returns:\\n        ~chainer.Variable:\\n            A variable object holding loss value(s).\\n            If ``reduce`` is ``'no'``, the output variable holds an\\n            array whose shape is :math:`(N,)` .\\n            If it is ``'mean'``, it holds a scalar.\\n\\n    See: `BlackOut: Speeding up Recurrent Neural Network Language Models With\\n    Very Large Vocabularies <https://arxiv.org/abs/1511.06909>`_\\n\\n    .. seealso::\\n\\n        :class:`~chainer.links.BlackOut` to manage the model parameter ``W``.\\n\\n    \"\n    batch_size = x.shape[0]\n    neg_emb = embed_id.embed_id(samples, W)\n    neg_y = matmul.matmul(neg_emb, x[:, :, None])\n    neg_y = reshape.reshape(neg_y, neg_y.shape[:-1])\n    pos_emb = expand_dims.expand_dims(embed_id.embed_id(t, W), 1)\n    pos_y = matmul.matmul(pos_emb, x[:, :, None])\n    pos_y = reshape.reshape(pos_y, pos_y.shape[:-1])\n    logz = logsumexp.logsumexp(concat.concat([pos_y, neg_y]), axis=1)\n    (blogz, bneg_y) = broadcast.broadcast(reshape.reshape(logz, (batch_size, 1)), neg_y)\n    ny = exponential.log(1 - exponential.exp(bneg_y - blogz))\n    py = reshape.reshape(pos_y, (batch_size,))\n    loss = -(py - logz + _sum.sum(ny, axis=1))\n    if reduce == 'mean':\n        loss = average.average(loss)\n    return loss",
            "def black_out(x, t, W, samples, reduce='mean'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"BlackOut loss function.\\n\\n    BlackOut loss function is defined as\\n\\n    .. math::\\n\\n      -\\\\log(p(t)) - \\\\sum_{s \\\\in S} \\\\log(1 - p(s)),\\n\\n    where :math:`t` is the correct label, :math:`S` is a set of negative\\n    examples and :math:`p(\\\\cdot)` is likelihood of a given label.\\n    And, :math:`p` is defined as\\n\\n    .. math::\\n\\n       p(y) = \\\\frac{\\\\exp(W_y^\\\\top x)}{\\n       \\\\sum_{s \\\\in samples} \\\\exp(W_s^\\\\top x)}.\\n\\n    The output is a variable whose value depends on the value of\\n    the option ``reduce``. If it is ``'no'``, it holds the\\n    no loss values. If it is ``'mean'``, this function takes\\n    a mean of loss values.\\n\\n    Args:\\n        x (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Batch of input vectors.\\n            Its shape should be :math:`(N, D)`.\\n        t (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Vector of ground truth labels.\\n            Its shape should be :math:`(N,)`. Each elements :math:`v`\\n            should satisfy :math:`0 \\\\geq v \\\\geq V` or :math:`-1`\\n            where :math:`V` is the number of label types.\\n        W (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Weight matrix.\\n            Its shape should be :math:`(V, D)`\\n        samples (~chainer.Variable): Negative samples.\\n            Its shape should be :math:`(N, S)` where :math:`S` is\\n            the number of negative samples.\\n        reduce (str): Reduction option. Its value must be either\\n            ``'no'`` or ``'mean'``. Otherwise,\\n            :class:`ValueError` is raised.\\n\\n    Returns:\\n        ~chainer.Variable:\\n            A variable object holding loss value(s).\\n            If ``reduce`` is ``'no'``, the output variable holds an\\n            array whose shape is :math:`(N,)` .\\n            If it is ``'mean'``, it holds a scalar.\\n\\n    See: `BlackOut: Speeding up Recurrent Neural Network Language Models With\\n    Very Large Vocabularies <https://arxiv.org/abs/1511.06909>`_\\n\\n    .. seealso::\\n\\n        :class:`~chainer.links.BlackOut` to manage the model parameter ``W``.\\n\\n    \"\n    batch_size = x.shape[0]\n    neg_emb = embed_id.embed_id(samples, W)\n    neg_y = matmul.matmul(neg_emb, x[:, :, None])\n    neg_y = reshape.reshape(neg_y, neg_y.shape[:-1])\n    pos_emb = expand_dims.expand_dims(embed_id.embed_id(t, W), 1)\n    pos_y = matmul.matmul(pos_emb, x[:, :, None])\n    pos_y = reshape.reshape(pos_y, pos_y.shape[:-1])\n    logz = logsumexp.logsumexp(concat.concat([pos_y, neg_y]), axis=1)\n    (blogz, bneg_y) = broadcast.broadcast(reshape.reshape(logz, (batch_size, 1)), neg_y)\n    ny = exponential.log(1 - exponential.exp(bneg_y - blogz))\n    py = reshape.reshape(pos_y, (batch_size,))\n    loss = -(py - logz + _sum.sum(ny, axis=1))\n    if reduce == 'mean':\n        loss = average.average(loss)\n    return loss",
            "def black_out(x, t, W, samples, reduce='mean'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"BlackOut loss function.\\n\\n    BlackOut loss function is defined as\\n\\n    .. math::\\n\\n      -\\\\log(p(t)) - \\\\sum_{s \\\\in S} \\\\log(1 - p(s)),\\n\\n    where :math:`t` is the correct label, :math:`S` is a set of negative\\n    examples and :math:`p(\\\\cdot)` is likelihood of a given label.\\n    And, :math:`p` is defined as\\n\\n    .. math::\\n\\n       p(y) = \\\\frac{\\\\exp(W_y^\\\\top x)}{\\n       \\\\sum_{s \\\\in samples} \\\\exp(W_s^\\\\top x)}.\\n\\n    The output is a variable whose value depends on the value of\\n    the option ``reduce``. If it is ``'no'``, it holds the\\n    no loss values. If it is ``'mean'``, this function takes\\n    a mean of loss values.\\n\\n    Args:\\n        x (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Batch of input vectors.\\n            Its shape should be :math:`(N, D)`.\\n        t (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Vector of ground truth labels.\\n            Its shape should be :math:`(N,)`. Each elements :math:`v`\\n            should satisfy :math:`0 \\\\geq v \\\\geq V` or :math:`-1`\\n            where :math:`V` is the number of label types.\\n        W (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Weight matrix.\\n            Its shape should be :math:`(V, D)`\\n        samples (~chainer.Variable): Negative samples.\\n            Its shape should be :math:`(N, S)` where :math:`S` is\\n            the number of negative samples.\\n        reduce (str): Reduction option. Its value must be either\\n            ``'no'`` or ``'mean'``. Otherwise,\\n            :class:`ValueError` is raised.\\n\\n    Returns:\\n        ~chainer.Variable:\\n            A variable object holding loss value(s).\\n            If ``reduce`` is ``'no'``, the output variable holds an\\n            array whose shape is :math:`(N,)` .\\n            If it is ``'mean'``, it holds a scalar.\\n\\n    See: `BlackOut: Speeding up Recurrent Neural Network Language Models With\\n    Very Large Vocabularies <https://arxiv.org/abs/1511.06909>`_\\n\\n    .. seealso::\\n\\n        :class:`~chainer.links.BlackOut` to manage the model parameter ``W``.\\n\\n    \"\n    batch_size = x.shape[0]\n    neg_emb = embed_id.embed_id(samples, W)\n    neg_y = matmul.matmul(neg_emb, x[:, :, None])\n    neg_y = reshape.reshape(neg_y, neg_y.shape[:-1])\n    pos_emb = expand_dims.expand_dims(embed_id.embed_id(t, W), 1)\n    pos_y = matmul.matmul(pos_emb, x[:, :, None])\n    pos_y = reshape.reshape(pos_y, pos_y.shape[:-1])\n    logz = logsumexp.logsumexp(concat.concat([pos_y, neg_y]), axis=1)\n    (blogz, bneg_y) = broadcast.broadcast(reshape.reshape(logz, (batch_size, 1)), neg_y)\n    ny = exponential.log(1 - exponential.exp(bneg_y - blogz))\n    py = reshape.reshape(pos_y, (batch_size,))\n    loss = -(py - logz + _sum.sum(ny, axis=1))\n    if reduce == 'mean':\n        loss = average.average(loss)\n    return loss",
            "def black_out(x, t, W, samples, reduce='mean'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"BlackOut loss function.\\n\\n    BlackOut loss function is defined as\\n\\n    .. math::\\n\\n      -\\\\log(p(t)) - \\\\sum_{s \\\\in S} \\\\log(1 - p(s)),\\n\\n    where :math:`t` is the correct label, :math:`S` is a set of negative\\n    examples and :math:`p(\\\\cdot)` is likelihood of a given label.\\n    And, :math:`p` is defined as\\n\\n    .. math::\\n\\n       p(y) = \\\\frac{\\\\exp(W_y^\\\\top x)}{\\n       \\\\sum_{s \\\\in samples} \\\\exp(W_s^\\\\top x)}.\\n\\n    The output is a variable whose value depends on the value of\\n    the option ``reduce``. If it is ``'no'``, it holds the\\n    no loss values. If it is ``'mean'``, this function takes\\n    a mean of loss values.\\n\\n    Args:\\n        x (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Batch of input vectors.\\n            Its shape should be :math:`(N, D)`.\\n        t (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Vector of ground truth labels.\\n            Its shape should be :math:`(N,)`. Each elements :math:`v`\\n            should satisfy :math:`0 \\\\geq v \\\\geq V` or :math:`-1`\\n            where :math:`V` is the number of label types.\\n        W (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            Weight matrix.\\n            Its shape should be :math:`(V, D)`\\n        samples (~chainer.Variable): Negative samples.\\n            Its shape should be :math:`(N, S)` where :math:`S` is\\n            the number of negative samples.\\n        reduce (str): Reduction option. Its value must be either\\n            ``'no'`` or ``'mean'``. Otherwise,\\n            :class:`ValueError` is raised.\\n\\n    Returns:\\n        ~chainer.Variable:\\n            A variable object holding loss value(s).\\n            If ``reduce`` is ``'no'``, the output variable holds an\\n            array whose shape is :math:`(N,)` .\\n            If it is ``'mean'``, it holds a scalar.\\n\\n    See: `BlackOut: Speeding up Recurrent Neural Network Language Models With\\n    Very Large Vocabularies <https://arxiv.org/abs/1511.06909>`_\\n\\n    .. seealso::\\n\\n        :class:`~chainer.links.BlackOut` to manage the model parameter ``W``.\\n\\n    \"\n    batch_size = x.shape[0]\n    neg_emb = embed_id.embed_id(samples, W)\n    neg_y = matmul.matmul(neg_emb, x[:, :, None])\n    neg_y = reshape.reshape(neg_y, neg_y.shape[:-1])\n    pos_emb = expand_dims.expand_dims(embed_id.embed_id(t, W), 1)\n    pos_y = matmul.matmul(pos_emb, x[:, :, None])\n    pos_y = reshape.reshape(pos_y, pos_y.shape[:-1])\n    logz = logsumexp.logsumexp(concat.concat([pos_y, neg_y]), axis=1)\n    (blogz, bneg_y) = broadcast.broadcast(reshape.reshape(logz, (batch_size, 1)), neg_y)\n    ny = exponential.log(1 - exponential.exp(bneg_y - blogz))\n    py = reshape.reshape(pos_y, (batch_size,))\n    loss = -(py - logz + _sum.sum(ny, axis=1))\n    if reduce == 'mean':\n        loss = average.average(loss)\n    return loss"
        ]
    }
]