[
    {
        "func_name": "check_dropout",
        "original": "def check_dropout(mge_val, xla_val, drop_prob):\n    nr_zero = np.sum(np.array(xla_val == 0, np.uint32))\n    nr_el = np.prod(xla_val.shape)\n    xla_drop_rate = nr_zero * 1.0 / nr_el\n    np.testing.assert_allclose(drop_prob, xla_drop_rate, atol=0.001)\n    mge_mask = mge_val == 0\n    xla_mask = xla_val == 0\n    both_mask = np.bitwise_or(xla_mask, mge_mask)\n    both_left = np.bitwise_not(both_mask)\n    mge_left = mge_val * both_left\n    xla_left = xla_val * both_left\n    np.testing.assert_allclose(mge_left, xla_left, atol=1e-06)",
        "mutated": [
            "def check_dropout(mge_val, xla_val, drop_prob):\n    if False:\n        i = 10\n    nr_zero = np.sum(np.array(xla_val == 0, np.uint32))\n    nr_el = np.prod(xla_val.shape)\n    xla_drop_rate = nr_zero * 1.0 / nr_el\n    np.testing.assert_allclose(drop_prob, xla_drop_rate, atol=0.001)\n    mge_mask = mge_val == 0\n    xla_mask = xla_val == 0\n    both_mask = np.bitwise_or(xla_mask, mge_mask)\n    both_left = np.bitwise_not(both_mask)\n    mge_left = mge_val * both_left\n    xla_left = xla_val * both_left\n    np.testing.assert_allclose(mge_left, xla_left, atol=1e-06)",
            "def check_dropout(mge_val, xla_val, drop_prob):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    nr_zero = np.sum(np.array(xla_val == 0, np.uint32))\n    nr_el = np.prod(xla_val.shape)\n    xla_drop_rate = nr_zero * 1.0 / nr_el\n    np.testing.assert_allclose(drop_prob, xla_drop_rate, atol=0.001)\n    mge_mask = mge_val == 0\n    xla_mask = xla_val == 0\n    both_mask = np.bitwise_or(xla_mask, mge_mask)\n    both_left = np.bitwise_not(both_mask)\n    mge_left = mge_val * both_left\n    xla_left = xla_val * both_left\n    np.testing.assert_allclose(mge_left, xla_left, atol=1e-06)",
            "def check_dropout(mge_val, xla_val, drop_prob):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    nr_zero = np.sum(np.array(xla_val == 0, np.uint32))\n    nr_el = np.prod(xla_val.shape)\n    xla_drop_rate = nr_zero * 1.0 / nr_el\n    np.testing.assert_allclose(drop_prob, xla_drop_rate, atol=0.001)\n    mge_mask = mge_val == 0\n    xla_mask = xla_val == 0\n    both_mask = np.bitwise_or(xla_mask, mge_mask)\n    both_left = np.bitwise_not(both_mask)\n    mge_left = mge_val * both_left\n    xla_left = xla_val * both_left\n    np.testing.assert_allclose(mge_left, xla_left, atol=1e-06)",
            "def check_dropout(mge_val, xla_val, drop_prob):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    nr_zero = np.sum(np.array(xla_val == 0, np.uint32))\n    nr_el = np.prod(xla_val.shape)\n    xla_drop_rate = nr_zero * 1.0 / nr_el\n    np.testing.assert_allclose(drop_prob, xla_drop_rate, atol=0.001)\n    mge_mask = mge_val == 0\n    xla_mask = xla_val == 0\n    both_mask = np.bitwise_or(xla_mask, mge_mask)\n    both_left = np.bitwise_not(both_mask)\n    mge_left = mge_val * both_left\n    xla_left = xla_val * both_left\n    np.testing.assert_allclose(mge_left, xla_left, atol=1e-06)",
            "def check_dropout(mge_val, xla_val, drop_prob):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    nr_zero = np.sum(np.array(xla_val == 0, np.uint32))\n    nr_el = np.prod(xla_val.shape)\n    xla_drop_rate = nr_zero * 1.0 / nr_el\n    np.testing.assert_allclose(drop_prob, xla_drop_rate, atol=0.001)\n    mge_mask = mge_val == 0\n    xla_mask = xla_val == 0\n    both_mask = np.bitwise_or(xla_mask, mge_mask)\n    both_left = np.bitwise_not(both_mask)\n    mge_left = mge_val * both_left\n    xla_left = xla_val * both_left\n    np.testing.assert_allclose(mge_left, xla_left, atol=1e-06)"
        ]
    },
    {
        "func_name": "func",
        "original": "@jit.xla_trace(without_host=True)\ndef func(x, dy):\n    gm.attach([x])\n    with gm:\n        y = F.dropout(x, drop_prob, True)\n        gm.backward(y, dy)\n    return (y, x.grad)",
        "mutated": [
            "@jit.xla_trace(without_host=True)\ndef func(x, dy):\n    if False:\n        i = 10\n    gm.attach([x])\n    with gm:\n        y = F.dropout(x, drop_prob, True)\n        gm.backward(y, dy)\n    return (y, x.grad)",
            "@jit.xla_trace(without_host=True)\ndef func(x, dy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    gm.attach([x])\n    with gm:\n        y = F.dropout(x, drop_prob, True)\n        gm.backward(y, dy)\n    return (y, x.grad)",
            "@jit.xla_trace(without_host=True)\ndef func(x, dy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    gm.attach([x])\n    with gm:\n        y = F.dropout(x, drop_prob, True)\n        gm.backward(y, dy)\n    return (y, x.grad)",
            "@jit.xla_trace(without_host=True)\ndef func(x, dy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    gm.attach([x])\n    with gm:\n        y = F.dropout(x, drop_prob, True)\n        gm.backward(y, dy)\n    return (y, x.grad)",
            "@jit.xla_trace(without_host=True)\ndef func(x, dy):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    gm.attach([x])\n    with gm:\n        y = F.dropout(x, drop_prob, True)\n        gm.backward(y, dy)\n    return (y, x.grad)"
        ]
    },
    {
        "func_name": "tester",
        "original": "def tester(shape, drop_prob, dtype=None):\n    dtype = dtype or np.float32\n    x = tensor(np.random.randn(*shape), dtype=dtype)\n    dy = tensor(np.random.randn(*shape), dtype=dtype)\n    gm = GradManager()\n\n    @jit.xla_trace(without_host=True)\n    def func(x, dy):\n        gm.attach([x])\n        with gm:\n            y = F.dropout(x, drop_prob, True)\n            gm.backward(y, dy)\n        return (y, x.grad)\n    mge_rsts = func(x, dy)\n    xla_rsts = func(x, dy)\n    for (mge_rst, xla_rst) in zip(mge_rsts, xla_rsts):\n        check_dropout(mge_rst.numpy(), xla_rst.numpy(), drop_prob)",
        "mutated": [
            "def tester(shape, drop_prob, dtype=None):\n    if False:\n        i = 10\n    dtype = dtype or np.float32\n    x = tensor(np.random.randn(*shape), dtype=dtype)\n    dy = tensor(np.random.randn(*shape), dtype=dtype)\n    gm = GradManager()\n\n    @jit.xla_trace(without_host=True)\n    def func(x, dy):\n        gm.attach([x])\n        with gm:\n            y = F.dropout(x, drop_prob, True)\n            gm.backward(y, dy)\n        return (y, x.grad)\n    mge_rsts = func(x, dy)\n    xla_rsts = func(x, dy)\n    for (mge_rst, xla_rst) in zip(mge_rsts, xla_rsts):\n        check_dropout(mge_rst.numpy(), xla_rst.numpy(), drop_prob)",
            "def tester(shape, drop_prob, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dtype = dtype or np.float32\n    x = tensor(np.random.randn(*shape), dtype=dtype)\n    dy = tensor(np.random.randn(*shape), dtype=dtype)\n    gm = GradManager()\n\n    @jit.xla_trace(without_host=True)\n    def func(x, dy):\n        gm.attach([x])\n        with gm:\n            y = F.dropout(x, drop_prob, True)\n            gm.backward(y, dy)\n        return (y, x.grad)\n    mge_rsts = func(x, dy)\n    xla_rsts = func(x, dy)\n    for (mge_rst, xla_rst) in zip(mge_rsts, xla_rsts):\n        check_dropout(mge_rst.numpy(), xla_rst.numpy(), drop_prob)",
            "def tester(shape, drop_prob, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dtype = dtype or np.float32\n    x = tensor(np.random.randn(*shape), dtype=dtype)\n    dy = tensor(np.random.randn(*shape), dtype=dtype)\n    gm = GradManager()\n\n    @jit.xla_trace(without_host=True)\n    def func(x, dy):\n        gm.attach([x])\n        with gm:\n            y = F.dropout(x, drop_prob, True)\n            gm.backward(y, dy)\n        return (y, x.grad)\n    mge_rsts = func(x, dy)\n    xla_rsts = func(x, dy)\n    for (mge_rst, xla_rst) in zip(mge_rsts, xla_rsts):\n        check_dropout(mge_rst.numpy(), xla_rst.numpy(), drop_prob)",
            "def tester(shape, drop_prob, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dtype = dtype or np.float32\n    x = tensor(np.random.randn(*shape), dtype=dtype)\n    dy = tensor(np.random.randn(*shape), dtype=dtype)\n    gm = GradManager()\n\n    @jit.xla_trace(without_host=True)\n    def func(x, dy):\n        gm.attach([x])\n        with gm:\n            y = F.dropout(x, drop_prob, True)\n            gm.backward(y, dy)\n        return (y, x.grad)\n    mge_rsts = func(x, dy)\n    xla_rsts = func(x, dy)\n    for (mge_rst, xla_rst) in zip(mge_rsts, xla_rsts):\n        check_dropout(mge_rst.numpy(), xla_rst.numpy(), drop_prob)",
            "def tester(shape, drop_prob, dtype=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dtype = dtype or np.float32\n    x = tensor(np.random.randn(*shape), dtype=dtype)\n    dy = tensor(np.random.randn(*shape), dtype=dtype)\n    gm = GradManager()\n\n    @jit.xla_trace(without_host=True)\n    def func(x, dy):\n        gm.attach([x])\n        with gm:\n            y = F.dropout(x, drop_prob, True)\n            gm.backward(y, dy)\n        return (y, x.grad)\n    mge_rsts = func(x, dy)\n    xla_rsts = func(x, dy)\n    for (mge_rst, xla_rst) in zip(mge_rsts, xla_rsts):\n        check_dropout(mge_rst.numpy(), xla_rst.numpy(), drop_prob)"
        ]
    },
    {
        "func_name": "test_dropout",
        "original": "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_dropout():\n\n    def check_dropout(mge_val, xla_val, drop_prob):\n        nr_zero = np.sum(np.array(xla_val == 0, np.uint32))\n        nr_el = np.prod(xla_val.shape)\n        xla_drop_rate = nr_zero * 1.0 / nr_el\n        np.testing.assert_allclose(drop_prob, xla_drop_rate, atol=0.001)\n        mge_mask = mge_val == 0\n        xla_mask = xla_val == 0\n        both_mask = np.bitwise_or(xla_mask, mge_mask)\n        both_left = np.bitwise_not(both_mask)\n        mge_left = mge_val * both_left\n        xla_left = xla_val * both_left\n        np.testing.assert_allclose(mge_left, xla_left, atol=1e-06)\n\n    def tester(shape, drop_prob, dtype=None):\n        dtype = dtype or np.float32\n        x = tensor(np.random.randn(*shape), dtype=dtype)\n        dy = tensor(np.random.randn(*shape), dtype=dtype)\n        gm = GradManager()\n\n        @jit.xla_trace(without_host=True)\n        def func(x, dy):\n            gm.attach([x])\n            with gm:\n                y = F.dropout(x, drop_prob, True)\n                gm.backward(y, dy)\n            return (y, x.grad)\n        mge_rsts = func(x, dy)\n        xla_rsts = func(x, dy)\n        for (mge_rst, xla_rst) in zip(mge_rsts, xla_rsts):\n            check_dropout(mge_rst.numpy(), xla_rst.numpy(), drop_prob)\n    tester((32, 128, 128, 1, 16), 0.1)\n    tester((32, 128, 128, 1, 16), 0.3)\n    tester((32, 128, 128, 1, 16), 0.5)\n    tester((32, 128, 128, 1, 16), 0.9)",
        "mutated": [
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_dropout():\n    if False:\n        i = 10\n\n    def check_dropout(mge_val, xla_val, drop_prob):\n        nr_zero = np.sum(np.array(xla_val == 0, np.uint32))\n        nr_el = np.prod(xla_val.shape)\n        xla_drop_rate = nr_zero * 1.0 / nr_el\n        np.testing.assert_allclose(drop_prob, xla_drop_rate, atol=0.001)\n        mge_mask = mge_val == 0\n        xla_mask = xla_val == 0\n        both_mask = np.bitwise_or(xla_mask, mge_mask)\n        both_left = np.bitwise_not(both_mask)\n        mge_left = mge_val * both_left\n        xla_left = xla_val * both_left\n        np.testing.assert_allclose(mge_left, xla_left, atol=1e-06)\n\n    def tester(shape, drop_prob, dtype=None):\n        dtype = dtype or np.float32\n        x = tensor(np.random.randn(*shape), dtype=dtype)\n        dy = tensor(np.random.randn(*shape), dtype=dtype)\n        gm = GradManager()\n\n        @jit.xla_trace(without_host=True)\n        def func(x, dy):\n            gm.attach([x])\n            with gm:\n                y = F.dropout(x, drop_prob, True)\n                gm.backward(y, dy)\n            return (y, x.grad)\n        mge_rsts = func(x, dy)\n        xla_rsts = func(x, dy)\n        for (mge_rst, xla_rst) in zip(mge_rsts, xla_rsts):\n            check_dropout(mge_rst.numpy(), xla_rst.numpy(), drop_prob)\n    tester((32, 128, 128, 1, 16), 0.1)\n    tester((32, 128, 128, 1, 16), 0.3)\n    tester((32, 128, 128, 1, 16), 0.5)\n    tester((32, 128, 128, 1, 16), 0.9)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_dropout():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n\n    def check_dropout(mge_val, xla_val, drop_prob):\n        nr_zero = np.sum(np.array(xla_val == 0, np.uint32))\n        nr_el = np.prod(xla_val.shape)\n        xla_drop_rate = nr_zero * 1.0 / nr_el\n        np.testing.assert_allclose(drop_prob, xla_drop_rate, atol=0.001)\n        mge_mask = mge_val == 0\n        xla_mask = xla_val == 0\n        both_mask = np.bitwise_or(xla_mask, mge_mask)\n        both_left = np.bitwise_not(both_mask)\n        mge_left = mge_val * both_left\n        xla_left = xla_val * both_left\n        np.testing.assert_allclose(mge_left, xla_left, atol=1e-06)\n\n    def tester(shape, drop_prob, dtype=None):\n        dtype = dtype or np.float32\n        x = tensor(np.random.randn(*shape), dtype=dtype)\n        dy = tensor(np.random.randn(*shape), dtype=dtype)\n        gm = GradManager()\n\n        @jit.xla_trace(without_host=True)\n        def func(x, dy):\n            gm.attach([x])\n            with gm:\n                y = F.dropout(x, drop_prob, True)\n                gm.backward(y, dy)\n            return (y, x.grad)\n        mge_rsts = func(x, dy)\n        xla_rsts = func(x, dy)\n        for (mge_rst, xla_rst) in zip(mge_rsts, xla_rsts):\n            check_dropout(mge_rst.numpy(), xla_rst.numpy(), drop_prob)\n    tester((32, 128, 128, 1, 16), 0.1)\n    tester((32, 128, 128, 1, 16), 0.3)\n    tester((32, 128, 128, 1, 16), 0.5)\n    tester((32, 128, 128, 1, 16), 0.9)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_dropout():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n    def check_dropout(mge_val, xla_val, drop_prob):\n        nr_zero = np.sum(np.array(xla_val == 0, np.uint32))\n        nr_el = np.prod(xla_val.shape)\n        xla_drop_rate = nr_zero * 1.0 / nr_el\n        np.testing.assert_allclose(drop_prob, xla_drop_rate, atol=0.001)\n        mge_mask = mge_val == 0\n        xla_mask = xla_val == 0\n        both_mask = np.bitwise_or(xla_mask, mge_mask)\n        both_left = np.bitwise_not(both_mask)\n        mge_left = mge_val * both_left\n        xla_left = xla_val * both_left\n        np.testing.assert_allclose(mge_left, xla_left, atol=1e-06)\n\n    def tester(shape, drop_prob, dtype=None):\n        dtype = dtype or np.float32\n        x = tensor(np.random.randn(*shape), dtype=dtype)\n        dy = tensor(np.random.randn(*shape), dtype=dtype)\n        gm = GradManager()\n\n        @jit.xla_trace(without_host=True)\n        def func(x, dy):\n            gm.attach([x])\n            with gm:\n                y = F.dropout(x, drop_prob, True)\n                gm.backward(y, dy)\n            return (y, x.grad)\n        mge_rsts = func(x, dy)\n        xla_rsts = func(x, dy)\n        for (mge_rst, xla_rst) in zip(mge_rsts, xla_rsts):\n            check_dropout(mge_rst.numpy(), xla_rst.numpy(), drop_prob)\n    tester((32, 128, 128, 1, 16), 0.1)\n    tester((32, 128, 128, 1, 16), 0.3)\n    tester((32, 128, 128, 1, 16), 0.5)\n    tester((32, 128, 128, 1, 16), 0.9)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_dropout():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n\n    def check_dropout(mge_val, xla_val, drop_prob):\n        nr_zero = np.sum(np.array(xla_val == 0, np.uint32))\n        nr_el = np.prod(xla_val.shape)\n        xla_drop_rate = nr_zero * 1.0 / nr_el\n        np.testing.assert_allclose(drop_prob, xla_drop_rate, atol=0.001)\n        mge_mask = mge_val == 0\n        xla_mask = xla_val == 0\n        both_mask = np.bitwise_or(xla_mask, mge_mask)\n        both_left = np.bitwise_not(both_mask)\n        mge_left = mge_val * both_left\n        xla_left = xla_val * both_left\n        np.testing.assert_allclose(mge_left, xla_left, atol=1e-06)\n\n    def tester(shape, drop_prob, dtype=None):\n        dtype = dtype or np.float32\n        x = tensor(np.random.randn(*shape), dtype=dtype)\n        dy = tensor(np.random.randn(*shape), dtype=dtype)\n        gm = GradManager()\n\n        @jit.xla_trace(without_host=True)\n        def func(x, dy):\n            gm.attach([x])\n            with gm:\n                y = F.dropout(x, drop_prob, True)\n                gm.backward(y, dy)\n            return (y, x.grad)\n        mge_rsts = func(x, dy)\n        xla_rsts = func(x, dy)\n        for (mge_rst, xla_rst) in zip(mge_rsts, xla_rsts):\n            check_dropout(mge_rst.numpy(), xla_rst.numpy(), drop_prob)\n    tester((32, 128, 128, 1, 16), 0.1)\n    tester((32, 128, 128, 1, 16), 0.3)\n    tester((32, 128, 128, 1, 16), 0.5)\n    tester((32, 128, 128, 1, 16), 0.9)",
            "@pytest.mark.skipif(int(platform.python_version_tuple()[1]) < 8, reason='need py38')\n@pytest.mark.skipif(platform.system() != 'Linux', reason='only support linux now')\n@pytest.mark.skipif(not is_cuda_available(), reason='only support cuda now')\ndef test_dropout():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n\n    def check_dropout(mge_val, xla_val, drop_prob):\n        nr_zero = np.sum(np.array(xla_val == 0, np.uint32))\n        nr_el = np.prod(xla_val.shape)\n        xla_drop_rate = nr_zero * 1.0 / nr_el\n        np.testing.assert_allclose(drop_prob, xla_drop_rate, atol=0.001)\n        mge_mask = mge_val == 0\n        xla_mask = xla_val == 0\n        both_mask = np.bitwise_or(xla_mask, mge_mask)\n        both_left = np.bitwise_not(both_mask)\n        mge_left = mge_val * both_left\n        xla_left = xla_val * both_left\n        np.testing.assert_allclose(mge_left, xla_left, atol=1e-06)\n\n    def tester(shape, drop_prob, dtype=None):\n        dtype = dtype or np.float32\n        x = tensor(np.random.randn(*shape), dtype=dtype)\n        dy = tensor(np.random.randn(*shape), dtype=dtype)\n        gm = GradManager()\n\n        @jit.xla_trace(without_host=True)\n        def func(x, dy):\n            gm.attach([x])\n            with gm:\n                y = F.dropout(x, drop_prob, True)\n                gm.backward(y, dy)\n            return (y, x.grad)\n        mge_rsts = func(x, dy)\n        xla_rsts = func(x, dy)\n        for (mge_rst, xla_rst) in zip(mge_rsts, xla_rsts):\n            check_dropout(mge_rst.numpy(), xla_rst.numpy(), drop_prob)\n    tester((32, 128, 128, 1, 16), 0.1)\n    tester((32, 128, 128, 1, 16), 0.3)\n    tester((32, 128, 128, 1, 16), 0.5)\n    tester((32, 128, 128, 1, 16), 0.9)"
        ]
    }
]