from __future__ import annotations
import inspect
from typing import TYPE_CHECKING
import paddle
from paddle.amp.auto_cast import amp_state
from paddle.base.data_feeder import convert_dtype
from paddle.framework import _dygraph_tracer
from ..profiler import EventGuard
from ..utils import Cache, GraphLogger, Singleton, StepInfoManager, log, log_do, map_if
from .interpreter import compile_sir
if TYPE_CHECKING:
    from .symbolic_context import SymbolicTraceContext

def trace_back_frames():
    if False:
        while True:
            i = 10
    frame = inspect.currentframe()
    while frame.f_back is not None:
        frame = frame.f_back
        code = frame.f_code
        paddle.framework.core.sot_set_with_graph(code)

def clear_eager_tensor_name(output_tensors):
    if False:
        for i in range(10):
            print('nop')
    for output_tensor in output_tensors:
        output_tensor.name = ''

class FallbackWrapper:
    """
    Used to store and call static graph methods generated by paddle.jit.to_static
    """

    def __init__(self, compiled_fn, SIR):
        if False:
            i = 10
            return i + 15
        self.compiled_fn = compiled_fn
        self.partial_program = None
        self.concrete_program = None
        self.SIR = SIR

    def amp_cast_inputs(self, args, kwargs):
        if False:
            print('Hello World!')
        'Prepare inputs for amp, cast float16 into float32 if needed.'
        current_amp_state = amp_state()
        if current_amp_state is None:
            return (args, kwargs)
        tracer = _dygraph_tracer()
        if not (tracer._expected_place.is_gpu_place() or tracer._expected_place.is_xpu_place() or tracer._expected_place.is_custom_place()):
            return (args, kwargs)
        amp_dtype = convert_dtype(current_amp_state['dtype'])
        log(3, f'[AMP] Cast {amp_dtype} into float32\n')
        return map_if((args, kwargs), pred=lambda x: isinstance(x, paddle.Tensor) and convert_dtype(x.dtype) == amp_dtype, true_fn=lambda x: x.cast(paddle.float32), false_fn=lambda x: x)

    def __call__(self, *args, **kwargs):
        if False:
            return 10
        with EventGuard(f'FallbackWrapper: {self.SIR.name}'):
            if StepInfoManager().need_back_trace:
                trace_back_frames()
            log_do(2, lambda : print('[FallbackWrapper] start run SIR: \n', self.SIR))
            (args, kwargs) = self.amp_cast_inputs(args, kwargs)
            log_do(4, lambda : print(self.compiled_fn.get_concrete_program(*args, **kwargs)[1].train_program))
            if self.partial_program is None:
                with EventGuard('FallbackWrapper: call compiled_fn'):
                    outputs = self.compiled_fn(*args, **kwargs)
                    (self.concrete_program, self.partial_program) = self.compiled_fn.get_concrete_program(*args, **kwargs)
            else:
                with EventGuard('FallbackWrapper: call partial_program'):
                    outputs = self.partial_program(*args, **kwargs)
            clear_eager_tensor_name(outputs)
            log_do(1, lambda : GraphLogger().add_subgraph(self.concrete_program.main_program))
            log_do(4, lambda : print('[CompileCache] run sir forward success.'))
            return outputs

@Singleton
class CompileSIRCache(Cache):
    """
    Cache the compiled function of SIR
    """

    def __init__(self):
        if False:
            i = 10
            return i + 15
        super().__init__(weak=False)

    def key_fn(self, context: SymbolicTraceContext, sir_name: str, **kwargs):
        if False:
            while True:
                i = 10
        '\n        generate a hash key for a SIR\n\n        Args:\n            context: The context to compile\n            sir_name: The name of the sir to compile\n            build_strategy: The build strategy to compile\n\n        Returns:\n            The hash key of the SIR\n        '
        sir = context.get_sir(sir_name)
        hash_key = hash(str(sir))
        return hash_key

    def value_fn(self, context: SymbolicTraceContext, sir_name: str, **kwargs):
        if False:
            i = 10
            return i + 15
        '\n        Generate static graph function\n\n        Args:\n            context: The context to compile\n            sir_name: The name of the sir to compile\n            build_strategy: The build strategy to compile\n\n        Returns:\n            The static graph function\n        '
        build_strategy = kwargs.get('build_strategy', None)
        backend = kwargs.get('backend', None)
        return FallbackWrapper(paddle.jit.to_static(compile_sir(context, sir_name), build_strategy=build_strategy, backend=backend, full_graph=True), context.get_sir(sir_name))