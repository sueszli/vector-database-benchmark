[
    {
        "func_name": "can_softmax_v1",
        "original": "def can_softmax_v1(a, dim):\n    if not jt.flags.use_cuda:\n        return False\n    if dim != -1 and dim != len(a.shape) - 1:\n        return False\n    if a.shape[len(a.shape) - 1] > 10000:\n        return False\n    return True",
        "mutated": [
            "def can_softmax_v1(a, dim):\n    if False:\n        i = 10\n    if not jt.flags.use_cuda:\n        return False\n    if dim != -1 and dim != len(a.shape) - 1:\n        return False\n    if a.shape[len(a.shape) - 1] > 10000:\n        return False\n    return True",
            "def can_softmax_v1(a, dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not jt.flags.use_cuda:\n        return False\n    if dim != -1 and dim != len(a.shape) - 1:\n        return False\n    if a.shape[len(a.shape) - 1] > 10000:\n        return False\n    return True",
            "def can_softmax_v1(a, dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not jt.flags.use_cuda:\n        return False\n    if dim != -1 and dim != len(a.shape) - 1:\n        return False\n    if a.shape[len(a.shape) - 1] > 10000:\n        return False\n    return True",
            "def can_softmax_v1(a, dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not jt.flags.use_cuda:\n        return False\n    if dim != -1 and dim != len(a.shape) - 1:\n        return False\n    if a.shape[len(a.shape) - 1] > 10000:\n        return False\n    return True",
            "def can_softmax_v1(a, dim):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not jt.flags.use_cuda:\n        return False\n    if dim != -1 and dim != len(a.shape) - 1:\n        return False\n    if a.shape[len(a.shape) - 1] > 10000:\n        return False\n    return True"
        ]
    },
    {
        "func_name": "execute",
        "original": "def execute(self, x):\n    self.save_vars = jt.code(x.shape, x.dtype, [x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f'\\n__global__ void kernel(in0_type* x, out0_type* y, int len) {{\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    constexpr int need_log = {int(log)};\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n\\n    int id = blockIdx.x * len;\\n    in0_type v[{per_thread}][{ILP}];\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(v[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    // v[i] = x[id+i*{tnum}+threadIdx.x];\\n    float v1 = -1e30;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            v1 = ::max(v1, float(v[i][j]));\\n        }}\\n    __shared__ float vmax;\\n    auto tmp = BlockReduce(temp_storage).Reduce(v1, cub::Max());\\n    if (threadIdx.x == 0)\\n        vmax = tmp;\\n    __syncthreads();\\n\\n    v1 = 0;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log) {{\\n                v[i][j] = float(v[i][j]) - vmax;\\n                v1 += expf(float(v[i][j]));\\n            }} else {{\\n                v[i][j] = expf(float(v[i][j]) - vmax);\\n                v1 += float(v[i][j]);\\n            }}\\n        }}\\n\\n    tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float vsum;\\n    if (threadIdx.x == 0)\\n        vsum = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log)\\n                v[i][j] = v[i][j] - @expand_op(log,@in0_type,vsum);\\n            else\\n                v[i][j] = float(v[i][j])/vsum;\\n        }}\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&y[id+(i*{tnum}+threadIdx.x)*{ILP}], v[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n')\n    return self.save_vars",
        "mutated": [
            "def execute(self, x):\n    if False:\n        i = 10\n    self.save_vars = jt.code(x.shape, x.dtype, [x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f'\\n__global__ void kernel(in0_type* x, out0_type* y, int len) {{\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    constexpr int need_log = {int(log)};\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n\\n    int id = blockIdx.x * len;\\n    in0_type v[{per_thread}][{ILP}];\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(v[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    // v[i] = x[id+i*{tnum}+threadIdx.x];\\n    float v1 = -1e30;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            v1 = ::max(v1, float(v[i][j]));\\n        }}\\n    __shared__ float vmax;\\n    auto tmp = BlockReduce(temp_storage).Reduce(v1, cub::Max());\\n    if (threadIdx.x == 0)\\n        vmax = tmp;\\n    __syncthreads();\\n\\n    v1 = 0;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log) {{\\n                v[i][j] = float(v[i][j]) - vmax;\\n                v1 += expf(float(v[i][j]));\\n            }} else {{\\n                v[i][j] = expf(float(v[i][j]) - vmax);\\n                v1 += float(v[i][j]);\\n            }}\\n        }}\\n\\n    tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float vsum;\\n    if (threadIdx.x == 0)\\n        vsum = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log)\\n                v[i][j] = v[i][j] - @expand_op(log,@in0_type,vsum);\\n            else\\n                v[i][j] = float(v[i][j])/vsum;\\n        }}\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&y[id+(i*{tnum}+threadIdx.x)*{ILP}], v[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n')\n    return self.save_vars",
            "def execute(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.save_vars = jt.code(x.shape, x.dtype, [x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f'\\n__global__ void kernel(in0_type* x, out0_type* y, int len) {{\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    constexpr int need_log = {int(log)};\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n\\n    int id = blockIdx.x * len;\\n    in0_type v[{per_thread}][{ILP}];\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(v[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    // v[i] = x[id+i*{tnum}+threadIdx.x];\\n    float v1 = -1e30;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            v1 = ::max(v1, float(v[i][j]));\\n        }}\\n    __shared__ float vmax;\\n    auto tmp = BlockReduce(temp_storage).Reduce(v1, cub::Max());\\n    if (threadIdx.x == 0)\\n        vmax = tmp;\\n    __syncthreads();\\n\\n    v1 = 0;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log) {{\\n                v[i][j] = float(v[i][j]) - vmax;\\n                v1 += expf(float(v[i][j]));\\n            }} else {{\\n                v[i][j] = expf(float(v[i][j]) - vmax);\\n                v1 += float(v[i][j]);\\n            }}\\n        }}\\n\\n    tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float vsum;\\n    if (threadIdx.x == 0)\\n        vsum = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log)\\n                v[i][j] = v[i][j] - @expand_op(log,@in0_type,vsum);\\n            else\\n                v[i][j] = float(v[i][j])/vsum;\\n        }}\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&y[id+(i*{tnum}+threadIdx.x)*{ILP}], v[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n')\n    return self.save_vars",
            "def execute(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.save_vars = jt.code(x.shape, x.dtype, [x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f'\\n__global__ void kernel(in0_type* x, out0_type* y, int len) {{\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    constexpr int need_log = {int(log)};\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n\\n    int id = blockIdx.x * len;\\n    in0_type v[{per_thread}][{ILP}];\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(v[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    // v[i] = x[id+i*{tnum}+threadIdx.x];\\n    float v1 = -1e30;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            v1 = ::max(v1, float(v[i][j]));\\n        }}\\n    __shared__ float vmax;\\n    auto tmp = BlockReduce(temp_storage).Reduce(v1, cub::Max());\\n    if (threadIdx.x == 0)\\n        vmax = tmp;\\n    __syncthreads();\\n\\n    v1 = 0;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log) {{\\n                v[i][j] = float(v[i][j]) - vmax;\\n                v1 += expf(float(v[i][j]));\\n            }} else {{\\n                v[i][j] = expf(float(v[i][j]) - vmax);\\n                v1 += float(v[i][j]);\\n            }}\\n        }}\\n\\n    tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float vsum;\\n    if (threadIdx.x == 0)\\n        vsum = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log)\\n                v[i][j] = v[i][j] - @expand_op(log,@in0_type,vsum);\\n            else\\n                v[i][j] = float(v[i][j])/vsum;\\n        }}\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&y[id+(i*{tnum}+threadIdx.x)*{ILP}], v[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n')\n    return self.save_vars",
            "def execute(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.save_vars = jt.code(x.shape, x.dtype, [x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f'\\n__global__ void kernel(in0_type* x, out0_type* y, int len) {{\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    constexpr int need_log = {int(log)};\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n\\n    int id = blockIdx.x * len;\\n    in0_type v[{per_thread}][{ILP}];\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(v[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    // v[i] = x[id+i*{tnum}+threadIdx.x];\\n    float v1 = -1e30;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            v1 = ::max(v1, float(v[i][j]));\\n        }}\\n    __shared__ float vmax;\\n    auto tmp = BlockReduce(temp_storage).Reduce(v1, cub::Max());\\n    if (threadIdx.x == 0)\\n        vmax = tmp;\\n    __syncthreads();\\n\\n    v1 = 0;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log) {{\\n                v[i][j] = float(v[i][j]) - vmax;\\n                v1 += expf(float(v[i][j]));\\n            }} else {{\\n                v[i][j] = expf(float(v[i][j]) - vmax);\\n                v1 += float(v[i][j]);\\n            }}\\n        }}\\n\\n    tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float vsum;\\n    if (threadIdx.x == 0)\\n        vsum = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log)\\n                v[i][j] = v[i][j] - @expand_op(log,@in0_type,vsum);\\n            else\\n                v[i][j] = float(v[i][j])/vsum;\\n        }}\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&y[id+(i*{tnum}+threadIdx.x)*{ILP}], v[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n')\n    return self.save_vars",
            "def execute(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.save_vars = jt.code(x.shape, x.dtype, [x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f'\\n__global__ void kernel(in0_type* x, out0_type* y, int len) {{\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    constexpr int need_log = {int(log)};\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n\\n    int id = blockIdx.x * len;\\n    in0_type v[{per_thread}][{ILP}];\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(v[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    // v[i] = x[id+i*{tnum}+threadIdx.x];\\n    float v1 = -1e30;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            v1 = ::max(v1, float(v[i][j]));\\n        }}\\n    __shared__ float vmax;\\n    auto tmp = BlockReduce(temp_storage).Reduce(v1, cub::Max());\\n    if (threadIdx.x == 0)\\n        vmax = tmp;\\n    __syncthreads();\\n\\n    v1 = 0;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log) {{\\n                v[i][j] = float(v[i][j]) - vmax;\\n                v1 += expf(float(v[i][j]));\\n            }} else {{\\n                v[i][j] = expf(float(v[i][j]) - vmax);\\n                v1 += float(v[i][j]);\\n            }}\\n        }}\\n\\n    tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float vsum;\\n    if (threadIdx.x == 0)\\n        vsum = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log)\\n                v[i][j] = v[i][j] - @expand_op(log,@in0_type,vsum);\\n            else\\n                v[i][j] = float(v[i][j])/vsum;\\n        }}\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&y[id+(i*{tnum}+threadIdx.x)*{ILP}], v[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n')\n    return self.save_vars"
        ]
    },
    {
        "func_name": "grad",
        "original": "def grad(self, grad_x):\n    x = self.save_vars\n    return jt.code(x.shape, x.dtype, [x, grad_x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f\"\\n__global__ void kernel(in0_type* x, in1_type* y, out0_type* z, int len) {{\\n    int id = blockIdx.x * len;\\n    in0_type vx[{per_thread}][{ILP}];\\n    in0_type vy[{per_thread}][{ILP}];\\n    {for_loop} {{\\n        vload<sizeof(in0_type)*{ILP}>(vx[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n        vload<sizeof(in0_type)*{ILP}>(vy[i], &y[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    }}\\n    float v1 = 0;\\n    {for_loop} \\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            v1 += {('float(vy[i][j]);' if log else 'float(vx[i][j]*vy[i][j]);')}\\n\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n    auto tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float reduce_var;\\n    if (threadIdx.x == 0)\\n        reduce_var = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            vx[i][j] = {('vy[i][j] - in0_type(expf(vx[i][j]) * reduce_var);' if log else 'vx[i][j] * (vy[i][j] - in0_type(reduce_var));')}\\n\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&z[id+(i*{tnum}+threadIdx.x)*{ILP}],\\n            vx[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, in1_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n\")",
        "mutated": [
            "def grad(self, grad_x):\n    if False:\n        i = 10\n    x = self.save_vars\n    return jt.code(x.shape, x.dtype, [x, grad_x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f\"\\n__global__ void kernel(in0_type* x, in1_type* y, out0_type* z, int len) {{\\n    int id = blockIdx.x * len;\\n    in0_type vx[{per_thread}][{ILP}];\\n    in0_type vy[{per_thread}][{ILP}];\\n    {for_loop} {{\\n        vload<sizeof(in0_type)*{ILP}>(vx[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n        vload<sizeof(in0_type)*{ILP}>(vy[i], &y[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    }}\\n    float v1 = 0;\\n    {for_loop} \\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            v1 += {('float(vy[i][j]);' if log else 'float(vx[i][j]*vy[i][j]);')}\\n\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n    auto tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float reduce_var;\\n    if (threadIdx.x == 0)\\n        reduce_var = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            vx[i][j] = {('vy[i][j] - in0_type(expf(vx[i][j]) * reduce_var);' if log else 'vx[i][j] * (vy[i][j] - in0_type(reduce_var));')}\\n\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&z[id+(i*{tnum}+threadIdx.x)*{ILP}],\\n            vx[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, in1_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n\")",
            "def grad(self, grad_x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    x = self.save_vars\n    return jt.code(x.shape, x.dtype, [x, grad_x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f\"\\n__global__ void kernel(in0_type* x, in1_type* y, out0_type* z, int len) {{\\n    int id = blockIdx.x * len;\\n    in0_type vx[{per_thread}][{ILP}];\\n    in0_type vy[{per_thread}][{ILP}];\\n    {for_loop} {{\\n        vload<sizeof(in0_type)*{ILP}>(vx[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n        vload<sizeof(in0_type)*{ILP}>(vy[i], &y[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    }}\\n    float v1 = 0;\\n    {for_loop} \\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            v1 += {('float(vy[i][j]);' if log else 'float(vx[i][j]*vy[i][j]);')}\\n\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n    auto tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float reduce_var;\\n    if (threadIdx.x == 0)\\n        reduce_var = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            vx[i][j] = {('vy[i][j] - in0_type(expf(vx[i][j]) * reduce_var);' if log else 'vx[i][j] * (vy[i][j] - in0_type(reduce_var));')}\\n\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&z[id+(i*{tnum}+threadIdx.x)*{ILP}],\\n            vx[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, in1_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n\")",
            "def grad(self, grad_x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    x = self.save_vars\n    return jt.code(x.shape, x.dtype, [x, grad_x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f\"\\n__global__ void kernel(in0_type* x, in1_type* y, out0_type* z, int len) {{\\n    int id = blockIdx.x * len;\\n    in0_type vx[{per_thread}][{ILP}];\\n    in0_type vy[{per_thread}][{ILP}];\\n    {for_loop} {{\\n        vload<sizeof(in0_type)*{ILP}>(vx[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n        vload<sizeof(in0_type)*{ILP}>(vy[i], &y[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    }}\\n    float v1 = 0;\\n    {for_loop} \\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            v1 += {('float(vy[i][j]);' if log else 'float(vx[i][j]*vy[i][j]);')}\\n\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n    auto tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float reduce_var;\\n    if (threadIdx.x == 0)\\n        reduce_var = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            vx[i][j] = {('vy[i][j] - in0_type(expf(vx[i][j]) * reduce_var);' if log else 'vx[i][j] * (vy[i][j] - in0_type(reduce_var));')}\\n\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&z[id+(i*{tnum}+threadIdx.x)*{ILP}],\\n            vx[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, in1_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n\")",
            "def grad(self, grad_x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    x = self.save_vars\n    return jt.code(x.shape, x.dtype, [x, grad_x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f\"\\n__global__ void kernel(in0_type* x, in1_type* y, out0_type* z, int len) {{\\n    int id = blockIdx.x * len;\\n    in0_type vx[{per_thread}][{ILP}];\\n    in0_type vy[{per_thread}][{ILP}];\\n    {for_loop} {{\\n        vload<sizeof(in0_type)*{ILP}>(vx[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n        vload<sizeof(in0_type)*{ILP}>(vy[i], &y[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    }}\\n    float v1 = 0;\\n    {for_loop} \\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            v1 += {('float(vy[i][j]);' if log else 'float(vx[i][j]*vy[i][j]);')}\\n\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n    auto tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float reduce_var;\\n    if (threadIdx.x == 0)\\n        reduce_var = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            vx[i][j] = {('vy[i][j] - in0_type(expf(vx[i][j]) * reduce_var);' if log else 'vx[i][j] * (vy[i][j] - in0_type(reduce_var));')}\\n\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&z[id+(i*{tnum}+threadIdx.x)*{ILP}],\\n            vx[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, in1_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n\")",
            "def grad(self, grad_x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    x = self.save_vars\n    return jt.code(x.shape, x.dtype, [x, grad_x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f\"\\n__global__ void kernel(in0_type* x, in1_type* y, out0_type* z, int len) {{\\n    int id = blockIdx.x * len;\\n    in0_type vx[{per_thread}][{ILP}];\\n    in0_type vy[{per_thread}][{ILP}];\\n    {for_loop} {{\\n        vload<sizeof(in0_type)*{ILP}>(vx[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n        vload<sizeof(in0_type)*{ILP}>(vy[i], &y[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    }}\\n    float v1 = 0;\\n    {for_loop} \\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            v1 += {('float(vy[i][j]);' if log else 'float(vx[i][j]*vy[i][j]);')}\\n\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n    auto tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float reduce_var;\\n    if (threadIdx.x == 0)\\n        reduce_var = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            vx[i][j] = {('vy[i][j] - in0_type(expf(vx[i][j]) * reduce_var);' if log else 'vx[i][j] * (vy[i][j] - in0_type(reduce_var));')}\\n\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&z[id+(i*{tnum}+threadIdx.x)*{ILP}],\\n            vx[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, in1_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n\")"
        ]
    },
    {
        "func_name": "softmax_v1",
        "original": "def softmax_v1(a, log=False):\n    assert can_softmax_v1(a, -1)\n    length = a.shape[-1]\n    tnum = 500 if length % 500 == 0 else 512\n    tnum = 125 if length % 125 == 0 else 128\n    per_thread = (length - 1) // tnum + 1\n    ILP = 1\n    for ilp in [8, 4, 2]:\n        if length % tnum == 0 and per_thread % ilp == 0:\n            ILP = ilp\n            per_thread //= ILP\n            break\n    for_loop = f'\\n    #pragma unroll\\n    for (int i=0; i<{per_thread}; i++)\\n    '\n    if length % tnum != 0:\n        for_loop += f'if ((i*{tnum}+threadIdx.x)*{ILP} < len)\\n'\n\n    class CodeSoftmax(jt.Function):\n\n        def execute(self, x):\n            self.save_vars = jt.code(x.shape, x.dtype, [x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f'\\n__global__ void kernel(in0_type* x, out0_type* y, int len) {{\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    constexpr int need_log = {int(log)};\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n\\n    int id = blockIdx.x * len;\\n    in0_type v[{per_thread}][{ILP}];\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(v[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    // v[i] = x[id+i*{tnum}+threadIdx.x];\\n    float v1 = -1e30;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            v1 = ::max(v1, float(v[i][j]));\\n        }}\\n    __shared__ float vmax;\\n    auto tmp = BlockReduce(temp_storage).Reduce(v1, cub::Max());\\n    if (threadIdx.x == 0)\\n        vmax = tmp;\\n    __syncthreads();\\n\\n    v1 = 0;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log) {{\\n                v[i][j] = float(v[i][j]) - vmax;\\n                v1 += expf(float(v[i][j]));\\n            }} else {{\\n                v[i][j] = expf(float(v[i][j]) - vmax);\\n                v1 += float(v[i][j]);\\n            }}\\n        }}\\n\\n    tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float vsum;\\n    if (threadIdx.x == 0)\\n        vsum = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log)\\n                v[i][j] = v[i][j] - @expand_op(log,@in0_type,vsum);\\n            else\\n                v[i][j] = float(v[i][j])/vsum;\\n        }}\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&y[id+(i*{tnum}+threadIdx.x)*{ILP}], v[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n')\n            return self.save_vars\n\n        def grad(self, grad_x):\n            x = self.save_vars\n            return jt.code(x.shape, x.dtype, [x, grad_x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f\"\\n__global__ void kernel(in0_type* x, in1_type* y, out0_type* z, int len) {{\\n    int id = blockIdx.x * len;\\n    in0_type vx[{per_thread}][{ILP}];\\n    in0_type vy[{per_thread}][{ILP}];\\n    {for_loop} {{\\n        vload<sizeof(in0_type)*{ILP}>(vx[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n        vload<sizeof(in0_type)*{ILP}>(vy[i], &y[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    }}\\n    float v1 = 0;\\n    {for_loop} \\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            v1 += {('float(vy[i][j]);' if log else 'float(vx[i][j]*vy[i][j]);')}\\n\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n    auto tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float reduce_var;\\n    if (threadIdx.x == 0)\\n        reduce_var = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            vx[i][j] = {('vy[i][j] - in0_type(expf(vx[i][j]) * reduce_var);' if log else 'vx[i][j] * (vy[i][j] - in0_type(reduce_var));')}\\n\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&z[id+(i*{tnum}+threadIdx.x)*{ILP}],\\n            vx[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, in1_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n\")\n    return CodeSoftmax()(a)",
        "mutated": [
            "def softmax_v1(a, log=False):\n    if False:\n        i = 10\n    assert can_softmax_v1(a, -1)\n    length = a.shape[-1]\n    tnum = 500 if length % 500 == 0 else 512\n    tnum = 125 if length % 125 == 0 else 128\n    per_thread = (length - 1) // tnum + 1\n    ILP = 1\n    for ilp in [8, 4, 2]:\n        if length % tnum == 0 and per_thread % ilp == 0:\n            ILP = ilp\n            per_thread //= ILP\n            break\n    for_loop = f'\\n    #pragma unroll\\n    for (int i=0; i<{per_thread}; i++)\\n    '\n    if length % tnum != 0:\n        for_loop += f'if ((i*{tnum}+threadIdx.x)*{ILP} < len)\\n'\n\n    class CodeSoftmax(jt.Function):\n\n        def execute(self, x):\n            self.save_vars = jt.code(x.shape, x.dtype, [x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f'\\n__global__ void kernel(in0_type* x, out0_type* y, int len) {{\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    constexpr int need_log = {int(log)};\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n\\n    int id = blockIdx.x * len;\\n    in0_type v[{per_thread}][{ILP}];\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(v[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    // v[i] = x[id+i*{tnum}+threadIdx.x];\\n    float v1 = -1e30;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            v1 = ::max(v1, float(v[i][j]));\\n        }}\\n    __shared__ float vmax;\\n    auto tmp = BlockReduce(temp_storage).Reduce(v1, cub::Max());\\n    if (threadIdx.x == 0)\\n        vmax = tmp;\\n    __syncthreads();\\n\\n    v1 = 0;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log) {{\\n                v[i][j] = float(v[i][j]) - vmax;\\n                v1 += expf(float(v[i][j]));\\n            }} else {{\\n                v[i][j] = expf(float(v[i][j]) - vmax);\\n                v1 += float(v[i][j]);\\n            }}\\n        }}\\n\\n    tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float vsum;\\n    if (threadIdx.x == 0)\\n        vsum = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log)\\n                v[i][j] = v[i][j] - @expand_op(log,@in0_type,vsum);\\n            else\\n                v[i][j] = float(v[i][j])/vsum;\\n        }}\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&y[id+(i*{tnum}+threadIdx.x)*{ILP}], v[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n')\n            return self.save_vars\n\n        def grad(self, grad_x):\n            x = self.save_vars\n            return jt.code(x.shape, x.dtype, [x, grad_x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f\"\\n__global__ void kernel(in0_type* x, in1_type* y, out0_type* z, int len) {{\\n    int id = blockIdx.x * len;\\n    in0_type vx[{per_thread}][{ILP}];\\n    in0_type vy[{per_thread}][{ILP}];\\n    {for_loop} {{\\n        vload<sizeof(in0_type)*{ILP}>(vx[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n        vload<sizeof(in0_type)*{ILP}>(vy[i], &y[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    }}\\n    float v1 = 0;\\n    {for_loop} \\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            v1 += {('float(vy[i][j]);' if log else 'float(vx[i][j]*vy[i][j]);')}\\n\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n    auto tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float reduce_var;\\n    if (threadIdx.x == 0)\\n        reduce_var = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            vx[i][j] = {('vy[i][j] - in0_type(expf(vx[i][j]) * reduce_var);' if log else 'vx[i][j] * (vy[i][j] - in0_type(reduce_var));')}\\n\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&z[id+(i*{tnum}+threadIdx.x)*{ILP}],\\n            vx[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, in1_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n\")\n    return CodeSoftmax()(a)",
            "def softmax_v1(a, log=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert can_softmax_v1(a, -1)\n    length = a.shape[-1]\n    tnum = 500 if length % 500 == 0 else 512\n    tnum = 125 if length % 125 == 0 else 128\n    per_thread = (length - 1) // tnum + 1\n    ILP = 1\n    for ilp in [8, 4, 2]:\n        if length % tnum == 0 and per_thread % ilp == 0:\n            ILP = ilp\n            per_thread //= ILP\n            break\n    for_loop = f'\\n    #pragma unroll\\n    for (int i=0; i<{per_thread}; i++)\\n    '\n    if length % tnum != 0:\n        for_loop += f'if ((i*{tnum}+threadIdx.x)*{ILP} < len)\\n'\n\n    class CodeSoftmax(jt.Function):\n\n        def execute(self, x):\n            self.save_vars = jt.code(x.shape, x.dtype, [x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f'\\n__global__ void kernel(in0_type* x, out0_type* y, int len) {{\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    constexpr int need_log = {int(log)};\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n\\n    int id = blockIdx.x * len;\\n    in0_type v[{per_thread}][{ILP}];\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(v[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    // v[i] = x[id+i*{tnum}+threadIdx.x];\\n    float v1 = -1e30;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            v1 = ::max(v1, float(v[i][j]));\\n        }}\\n    __shared__ float vmax;\\n    auto tmp = BlockReduce(temp_storage).Reduce(v1, cub::Max());\\n    if (threadIdx.x == 0)\\n        vmax = tmp;\\n    __syncthreads();\\n\\n    v1 = 0;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log) {{\\n                v[i][j] = float(v[i][j]) - vmax;\\n                v1 += expf(float(v[i][j]));\\n            }} else {{\\n                v[i][j] = expf(float(v[i][j]) - vmax);\\n                v1 += float(v[i][j]);\\n            }}\\n        }}\\n\\n    tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float vsum;\\n    if (threadIdx.x == 0)\\n        vsum = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log)\\n                v[i][j] = v[i][j] - @expand_op(log,@in0_type,vsum);\\n            else\\n                v[i][j] = float(v[i][j])/vsum;\\n        }}\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&y[id+(i*{tnum}+threadIdx.x)*{ILP}], v[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n')\n            return self.save_vars\n\n        def grad(self, grad_x):\n            x = self.save_vars\n            return jt.code(x.shape, x.dtype, [x, grad_x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f\"\\n__global__ void kernel(in0_type* x, in1_type* y, out0_type* z, int len) {{\\n    int id = blockIdx.x * len;\\n    in0_type vx[{per_thread}][{ILP}];\\n    in0_type vy[{per_thread}][{ILP}];\\n    {for_loop} {{\\n        vload<sizeof(in0_type)*{ILP}>(vx[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n        vload<sizeof(in0_type)*{ILP}>(vy[i], &y[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    }}\\n    float v1 = 0;\\n    {for_loop} \\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            v1 += {('float(vy[i][j]);' if log else 'float(vx[i][j]*vy[i][j]);')}\\n\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n    auto tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float reduce_var;\\n    if (threadIdx.x == 0)\\n        reduce_var = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            vx[i][j] = {('vy[i][j] - in0_type(expf(vx[i][j]) * reduce_var);' if log else 'vx[i][j] * (vy[i][j] - in0_type(reduce_var));')}\\n\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&z[id+(i*{tnum}+threadIdx.x)*{ILP}],\\n            vx[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, in1_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n\")\n    return CodeSoftmax()(a)",
            "def softmax_v1(a, log=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert can_softmax_v1(a, -1)\n    length = a.shape[-1]\n    tnum = 500 if length % 500 == 0 else 512\n    tnum = 125 if length % 125 == 0 else 128\n    per_thread = (length - 1) // tnum + 1\n    ILP = 1\n    for ilp in [8, 4, 2]:\n        if length % tnum == 0 and per_thread % ilp == 0:\n            ILP = ilp\n            per_thread //= ILP\n            break\n    for_loop = f'\\n    #pragma unroll\\n    for (int i=0; i<{per_thread}; i++)\\n    '\n    if length % tnum != 0:\n        for_loop += f'if ((i*{tnum}+threadIdx.x)*{ILP} < len)\\n'\n\n    class CodeSoftmax(jt.Function):\n\n        def execute(self, x):\n            self.save_vars = jt.code(x.shape, x.dtype, [x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f'\\n__global__ void kernel(in0_type* x, out0_type* y, int len) {{\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    constexpr int need_log = {int(log)};\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n\\n    int id = blockIdx.x * len;\\n    in0_type v[{per_thread}][{ILP}];\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(v[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    // v[i] = x[id+i*{tnum}+threadIdx.x];\\n    float v1 = -1e30;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            v1 = ::max(v1, float(v[i][j]));\\n        }}\\n    __shared__ float vmax;\\n    auto tmp = BlockReduce(temp_storage).Reduce(v1, cub::Max());\\n    if (threadIdx.x == 0)\\n        vmax = tmp;\\n    __syncthreads();\\n\\n    v1 = 0;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log) {{\\n                v[i][j] = float(v[i][j]) - vmax;\\n                v1 += expf(float(v[i][j]));\\n            }} else {{\\n                v[i][j] = expf(float(v[i][j]) - vmax);\\n                v1 += float(v[i][j]);\\n            }}\\n        }}\\n\\n    tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float vsum;\\n    if (threadIdx.x == 0)\\n        vsum = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log)\\n                v[i][j] = v[i][j] - @expand_op(log,@in0_type,vsum);\\n            else\\n                v[i][j] = float(v[i][j])/vsum;\\n        }}\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&y[id+(i*{tnum}+threadIdx.x)*{ILP}], v[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n')\n            return self.save_vars\n\n        def grad(self, grad_x):\n            x = self.save_vars\n            return jt.code(x.shape, x.dtype, [x, grad_x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f\"\\n__global__ void kernel(in0_type* x, in1_type* y, out0_type* z, int len) {{\\n    int id = blockIdx.x * len;\\n    in0_type vx[{per_thread}][{ILP}];\\n    in0_type vy[{per_thread}][{ILP}];\\n    {for_loop} {{\\n        vload<sizeof(in0_type)*{ILP}>(vx[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n        vload<sizeof(in0_type)*{ILP}>(vy[i], &y[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    }}\\n    float v1 = 0;\\n    {for_loop} \\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            v1 += {('float(vy[i][j]);' if log else 'float(vx[i][j]*vy[i][j]);')}\\n\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n    auto tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float reduce_var;\\n    if (threadIdx.x == 0)\\n        reduce_var = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            vx[i][j] = {('vy[i][j] - in0_type(expf(vx[i][j]) * reduce_var);' if log else 'vx[i][j] * (vy[i][j] - in0_type(reduce_var));')}\\n\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&z[id+(i*{tnum}+threadIdx.x)*{ILP}],\\n            vx[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, in1_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n\")\n    return CodeSoftmax()(a)",
            "def softmax_v1(a, log=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert can_softmax_v1(a, -1)\n    length = a.shape[-1]\n    tnum = 500 if length % 500 == 0 else 512\n    tnum = 125 if length % 125 == 0 else 128\n    per_thread = (length - 1) // tnum + 1\n    ILP = 1\n    for ilp in [8, 4, 2]:\n        if length % tnum == 0 and per_thread % ilp == 0:\n            ILP = ilp\n            per_thread //= ILP\n            break\n    for_loop = f'\\n    #pragma unroll\\n    for (int i=0; i<{per_thread}; i++)\\n    '\n    if length % tnum != 0:\n        for_loop += f'if ((i*{tnum}+threadIdx.x)*{ILP} < len)\\n'\n\n    class CodeSoftmax(jt.Function):\n\n        def execute(self, x):\n            self.save_vars = jt.code(x.shape, x.dtype, [x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f'\\n__global__ void kernel(in0_type* x, out0_type* y, int len) {{\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    constexpr int need_log = {int(log)};\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n\\n    int id = blockIdx.x * len;\\n    in0_type v[{per_thread}][{ILP}];\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(v[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    // v[i] = x[id+i*{tnum}+threadIdx.x];\\n    float v1 = -1e30;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            v1 = ::max(v1, float(v[i][j]));\\n        }}\\n    __shared__ float vmax;\\n    auto tmp = BlockReduce(temp_storage).Reduce(v1, cub::Max());\\n    if (threadIdx.x == 0)\\n        vmax = tmp;\\n    __syncthreads();\\n\\n    v1 = 0;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log) {{\\n                v[i][j] = float(v[i][j]) - vmax;\\n                v1 += expf(float(v[i][j]));\\n            }} else {{\\n                v[i][j] = expf(float(v[i][j]) - vmax);\\n                v1 += float(v[i][j]);\\n            }}\\n        }}\\n\\n    tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float vsum;\\n    if (threadIdx.x == 0)\\n        vsum = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log)\\n                v[i][j] = v[i][j] - @expand_op(log,@in0_type,vsum);\\n            else\\n                v[i][j] = float(v[i][j])/vsum;\\n        }}\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&y[id+(i*{tnum}+threadIdx.x)*{ILP}], v[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n')\n            return self.save_vars\n\n        def grad(self, grad_x):\n            x = self.save_vars\n            return jt.code(x.shape, x.dtype, [x, grad_x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f\"\\n__global__ void kernel(in0_type* x, in1_type* y, out0_type* z, int len) {{\\n    int id = blockIdx.x * len;\\n    in0_type vx[{per_thread}][{ILP}];\\n    in0_type vy[{per_thread}][{ILP}];\\n    {for_loop} {{\\n        vload<sizeof(in0_type)*{ILP}>(vx[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n        vload<sizeof(in0_type)*{ILP}>(vy[i], &y[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    }}\\n    float v1 = 0;\\n    {for_loop} \\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            v1 += {('float(vy[i][j]);' if log else 'float(vx[i][j]*vy[i][j]);')}\\n\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n    auto tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float reduce_var;\\n    if (threadIdx.x == 0)\\n        reduce_var = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            vx[i][j] = {('vy[i][j] - in0_type(expf(vx[i][j]) * reduce_var);' if log else 'vx[i][j] * (vy[i][j] - in0_type(reduce_var));')}\\n\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&z[id+(i*{tnum}+threadIdx.x)*{ILP}],\\n            vx[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, in1_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n\")\n    return CodeSoftmax()(a)",
            "def softmax_v1(a, log=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert can_softmax_v1(a, -1)\n    length = a.shape[-1]\n    tnum = 500 if length % 500 == 0 else 512\n    tnum = 125 if length % 125 == 0 else 128\n    per_thread = (length - 1) // tnum + 1\n    ILP = 1\n    for ilp in [8, 4, 2]:\n        if length % tnum == 0 and per_thread % ilp == 0:\n            ILP = ilp\n            per_thread //= ILP\n            break\n    for_loop = f'\\n    #pragma unroll\\n    for (int i=0; i<{per_thread}; i++)\\n    '\n    if length % tnum != 0:\n        for_loop += f'if ((i*{tnum}+threadIdx.x)*{ILP} < len)\\n'\n\n    class CodeSoftmax(jt.Function):\n\n        def execute(self, x):\n            self.save_vars = jt.code(x.shape, x.dtype, [x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f'\\n__global__ void kernel(in0_type* x, out0_type* y, int len) {{\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    constexpr int need_log = {int(log)};\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n\\n    int id = blockIdx.x * len;\\n    in0_type v[{per_thread}][{ILP}];\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(v[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    // v[i] = x[id+i*{tnum}+threadIdx.x];\\n    float v1 = -1e30;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            v1 = ::max(v1, float(v[i][j]));\\n        }}\\n    __shared__ float vmax;\\n    auto tmp = BlockReduce(temp_storage).Reduce(v1, cub::Max());\\n    if (threadIdx.x == 0)\\n        vmax = tmp;\\n    __syncthreads();\\n\\n    v1 = 0;\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log) {{\\n                v[i][j] = float(v[i][j]) - vmax;\\n                v1 += expf(float(v[i][j]));\\n            }} else {{\\n                v[i][j] = expf(float(v[i][j]) - vmax);\\n                v1 += float(v[i][j]);\\n            }}\\n        }}\\n\\n    tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float vsum;\\n    if (threadIdx.x == 0)\\n        vsum = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++) {{\\n            if (need_log)\\n                v[i][j] = v[i][j] - @expand_op(log,@in0_type,vsum);\\n            else\\n                v[i][j] = float(v[i][j])/vsum;\\n        }}\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&y[id+(i*{tnum}+threadIdx.x)*{ILP}], v[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n')\n            return self.save_vars\n\n        def grad(self, grad_x):\n            x = self.save_vars\n            return jt.code(x.shape, x.dtype, [x, grad_x], cuda_header=f'\\n#include <{jt.compile_extern.cub_home}cub/cub.cuh>\\n#include <type/fp16_compute.h>\\n', cuda_src=f\"\\n__global__ void kernel(in0_type* x, in1_type* y, out0_type* z, int len) {{\\n    int id = blockIdx.x * len;\\n    in0_type vx[{per_thread}][{ILP}];\\n    in0_type vy[{per_thread}][{ILP}];\\n    {for_loop} {{\\n        vload<sizeof(in0_type)*{ILP}>(vx[i], &x[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n        vload<sizeof(in0_type)*{ILP}>(vy[i], &y[id+(i*{tnum}+threadIdx.x)*{ILP}]);\\n    }}\\n    float v1 = 0;\\n    {for_loop} \\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            v1 += {('float(vy[i][j]);' if log else 'float(vx[i][j]*vy[i][j]);')}\\n\\n    typedef cub::BlockReduce<float, {tnum}> BlockReduce;\\n    __shared__ typename BlockReduce::TempStorage temp_storage;\\n    auto tmp = BlockReduce(temp_storage).Sum(v1);\\n    __shared__ float reduce_var;\\n    if (threadIdx.x == 0)\\n        reduce_var = tmp;\\n    __syncthreads();\\n\\n    {for_loop}\\n        #pragma unroll\\n        for (int j=0; j<{ILP}; j++)\\n            vx[i][j] = {('vy[i][j] - in0_type(expf(vx[i][j]) * reduce_var);' if log else 'vx[i][j] * (vy[i][j] - in0_type(reduce_var));')}\\n\\n    {for_loop}\\n        vload<sizeof(in0_type)*{ILP}>(&z[id+(i*{tnum}+threadIdx.x)*{ILP}],\\n            vx[i]);\\n}}\\nint len = in0->shape[in0->shape.size()-1];\\nint bnum = in0->numel() / len;\\ncudaGetLastError();\\nkernel<<<bnum, {tnum}>>>(in0_p, in1_p, out0_p, len);\\nCHECK(0 == cudaGetLastError());\\n\")\n    return CodeSoftmax()(a)"
        ]
    }
]