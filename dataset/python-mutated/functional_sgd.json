[
    {
        "func_name": "__init__",
        "original": "def __init__(self, params: List[Tensor], lr: float=0.01, momentum: float=0.0, dampening: float=0.0, weight_decay: float=0.0, nesterov: bool=False, maximize: bool=False, foreach: bool=False, _allow_empty_param_list: bool=False):\n    self.defaults = {'lr': lr, 'momentum': momentum, 'dampening': dampening, 'weight_decay': weight_decay}\n    self.nesterov = nesterov\n    self.maximize = maximize\n    self.foreach = foreach\n    self.state = torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})\n    if len(params) == 0 and (not _allow_empty_param_list):\n        raise ValueError('optimizer got an empty parameter list')\n    self.param_group = {'params': params}",
        "mutated": [
            "def __init__(self, params: List[Tensor], lr: float=0.01, momentum: float=0.0, dampening: float=0.0, weight_decay: float=0.0, nesterov: bool=False, maximize: bool=False, foreach: bool=False, _allow_empty_param_list: bool=False):\n    if False:\n        i = 10\n    self.defaults = {'lr': lr, 'momentum': momentum, 'dampening': dampening, 'weight_decay': weight_decay}\n    self.nesterov = nesterov\n    self.maximize = maximize\n    self.foreach = foreach\n    self.state = torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})\n    if len(params) == 0 and (not _allow_empty_param_list):\n        raise ValueError('optimizer got an empty parameter list')\n    self.param_group = {'params': params}",
            "def __init__(self, params: List[Tensor], lr: float=0.01, momentum: float=0.0, dampening: float=0.0, weight_decay: float=0.0, nesterov: bool=False, maximize: bool=False, foreach: bool=False, _allow_empty_param_list: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.defaults = {'lr': lr, 'momentum': momentum, 'dampening': dampening, 'weight_decay': weight_decay}\n    self.nesterov = nesterov\n    self.maximize = maximize\n    self.foreach = foreach\n    self.state = torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})\n    if len(params) == 0 and (not _allow_empty_param_list):\n        raise ValueError('optimizer got an empty parameter list')\n    self.param_group = {'params': params}",
            "def __init__(self, params: List[Tensor], lr: float=0.01, momentum: float=0.0, dampening: float=0.0, weight_decay: float=0.0, nesterov: bool=False, maximize: bool=False, foreach: bool=False, _allow_empty_param_list: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.defaults = {'lr': lr, 'momentum': momentum, 'dampening': dampening, 'weight_decay': weight_decay}\n    self.nesterov = nesterov\n    self.maximize = maximize\n    self.foreach = foreach\n    self.state = torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})\n    if len(params) == 0 and (not _allow_empty_param_list):\n        raise ValueError('optimizer got an empty parameter list')\n    self.param_group = {'params': params}",
            "def __init__(self, params: List[Tensor], lr: float=0.01, momentum: float=0.0, dampening: float=0.0, weight_decay: float=0.0, nesterov: bool=False, maximize: bool=False, foreach: bool=False, _allow_empty_param_list: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.defaults = {'lr': lr, 'momentum': momentum, 'dampening': dampening, 'weight_decay': weight_decay}\n    self.nesterov = nesterov\n    self.maximize = maximize\n    self.foreach = foreach\n    self.state = torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})\n    if len(params) == 0 and (not _allow_empty_param_list):\n        raise ValueError('optimizer got an empty parameter list')\n    self.param_group = {'params': params}",
            "def __init__(self, params: List[Tensor], lr: float=0.01, momentum: float=0.0, dampening: float=0.0, weight_decay: float=0.0, nesterov: bool=False, maximize: bool=False, foreach: bool=False, _allow_empty_param_list: bool=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.defaults = {'lr': lr, 'momentum': momentum, 'dampening': dampening, 'weight_decay': weight_decay}\n    self.nesterov = nesterov\n    self.maximize = maximize\n    self.foreach = foreach\n    self.state = torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})\n    if len(params) == 0 and (not _allow_empty_param_list):\n        raise ValueError('optimizer got an empty parameter list')\n    self.param_group = {'params': params}"
        ]
    },
    {
        "func_name": "step_param",
        "original": "def step_param(self, param: Tensor, grad: Optional[Tensor]):\n    \"\"\"Similar to self.step, but operates on a single parameter and\n        its gradient.\n        \"\"\"\n    weight_decay = self.defaults['weight_decay']\n    momentum = self.defaults['momentum']\n    dampening = self.defaults['dampening']\n    lr = self.defaults['lr']\n    params = [param]\n    momentum_buffer_list: List[Optional[Tensor]] = []\n    grads = []\n    has_sparse_grad = False\n    if grad is not None:\n        grads.append(grad)\n        if grad.is_sparse:\n            has_sparse_grad = True\n        if param not in self.state:\n            self.state[param] = {}\n        state = self.state[param]\n        if 'momentum_buffer' not in state:\n            momentum_buffer_list.append(None)\n        else:\n            momentum_buffer_list.append(state['momentum_buffer'])\n    with torch.no_grad():\n        F.sgd(params, grads, momentum_buffer_list, weight_decay=weight_decay, momentum=momentum, lr=lr, dampening=dampening, nesterov=self.nesterov, maximize=self.maximize, has_sparse_grad=has_sparse_grad, foreach=self.foreach)\n    state = self.state[param]\n    momentum_buffer = momentum_buffer_list[0]\n    if momentum_buffer is not None:\n        state['momentum_buffer'] = momentum_buffer",
        "mutated": [
            "def step_param(self, param: Tensor, grad: Optional[Tensor]):\n    if False:\n        i = 10\n    'Similar to self.step, but operates on a single parameter and\\n        its gradient.\\n        '\n    weight_decay = self.defaults['weight_decay']\n    momentum = self.defaults['momentum']\n    dampening = self.defaults['dampening']\n    lr = self.defaults['lr']\n    params = [param]\n    momentum_buffer_list: List[Optional[Tensor]] = []\n    grads = []\n    has_sparse_grad = False\n    if grad is not None:\n        grads.append(grad)\n        if grad.is_sparse:\n            has_sparse_grad = True\n        if param not in self.state:\n            self.state[param] = {}\n        state = self.state[param]\n        if 'momentum_buffer' not in state:\n            momentum_buffer_list.append(None)\n        else:\n            momentum_buffer_list.append(state['momentum_buffer'])\n    with torch.no_grad():\n        F.sgd(params, grads, momentum_buffer_list, weight_decay=weight_decay, momentum=momentum, lr=lr, dampening=dampening, nesterov=self.nesterov, maximize=self.maximize, has_sparse_grad=has_sparse_grad, foreach=self.foreach)\n    state = self.state[param]\n    momentum_buffer = momentum_buffer_list[0]\n    if momentum_buffer is not None:\n        state['momentum_buffer'] = momentum_buffer",
            "def step_param(self, param: Tensor, grad: Optional[Tensor]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Similar to self.step, but operates on a single parameter and\\n        its gradient.\\n        '\n    weight_decay = self.defaults['weight_decay']\n    momentum = self.defaults['momentum']\n    dampening = self.defaults['dampening']\n    lr = self.defaults['lr']\n    params = [param]\n    momentum_buffer_list: List[Optional[Tensor]] = []\n    grads = []\n    has_sparse_grad = False\n    if grad is not None:\n        grads.append(grad)\n        if grad.is_sparse:\n            has_sparse_grad = True\n        if param not in self.state:\n            self.state[param] = {}\n        state = self.state[param]\n        if 'momentum_buffer' not in state:\n            momentum_buffer_list.append(None)\n        else:\n            momentum_buffer_list.append(state['momentum_buffer'])\n    with torch.no_grad():\n        F.sgd(params, grads, momentum_buffer_list, weight_decay=weight_decay, momentum=momentum, lr=lr, dampening=dampening, nesterov=self.nesterov, maximize=self.maximize, has_sparse_grad=has_sparse_grad, foreach=self.foreach)\n    state = self.state[param]\n    momentum_buffer = momentum_buffer_list[0]\n    if momentum_buffer is not None:\n        state['momentum_buffer'] = momentum_buffer",
            "def step_param(self, param: Tensor, grad: Optional[Tensor]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Similar to self.step, but operates on a single parameter and\\n        its gradient.\\n        '\n    weight_decay = self.defaults['weight_decay']\n    momentum = self.defaults['momentum']\n    dampening = self.defaults['dampening']\n    lr = self.defaults['lr']\n    params = [param]\n    momentum_buffer_list: List[Optional[Tensor]] = []\n    grads = []\n    has_sparse_grad = False\n    if grad is not None:\n        grads.append(grad)\n        if grad.is_sparse:\n            has_sparse_grad = True\n        if param not in self.state:\n            self.state[param] = {}\n        state = self.state[param]\n        if 'momentum_buffer' not in state:\n            momentum_buffer_list.append(None)\n        else:\n            momentum_buffer_list.append(state['momentum_buffer'])\n    with torch.no_grad():\n        F.sgd(params, grads, momentum_buffer_list, weight_decay=weight_decay, momentum=momentum, lr=lr, dampening=dampening, nesterov=self.nesterov, maximize=self.maximize, has_sparse_grad=has_sparse_grad, foreach=self.foreach)\n    state = self.state[param]\n    momentum_buffer = momentum_buffer_list[0]\n    if momentum_buffer is not None:\n        state['momentum_buffer'] = momentum_buffer",
            "def step_param(self, param: Tensor, grad: Optional[Tensor]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Similar to self.step, but operates on a single parameter and\\n        its gradient.\\n        '\n    weight_decay = self.defaults['weight_decay']\n    momentum = self.defaults['momentum']\n    dampening = self.defaults['dampening']\n    lr = self.defaults['lr']\n    params = [param]\n    momentum_buffer_list: List[Optional[Tensor]] = []\n    grads = []\n    has_sparse_grad = False\n    if grad is not None:\n        grads.append(grad)\n        if grad.is_sparse:\n            has_sparse_grad = True\n        if param not in self.state:\n            self.state[param] = {}\n        state = self.state[param]\n        if 'momentum_buffer' not in state:\n            momentum_buffer_list.append(None)\n        else:\n            momentum_buffer_list.append(state['momentum_buffer'])\n    with torch.no_grad():\n        F.sgd(params, grads, momentum_buffer_list, weight_decay=weight_decay, momentum=momentum, lr=lr, dampening=dampening, nesterov=self.nesterov, maximize=self.maximize, has_sparse_grad=has_sparse_grad, foreach=self.foreach)\n    state = self.state[param]\n    momentum_buffer = momentum_buffer_list[0]\n    if momentum_buffer is not None:\n        state['momentum_buffer'] = momentum_buffer",
            "def step_param(self, param: Tensor, grad: Optional[Tensor]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Similar to self.step, but operates on a single parameter and\\n        its gradient.\\n        '\n    weight_decay = self.defaults['weight_decay']\n    momentum = self.defaults['momentum']\n    dampening = self.defaults['dampening']\n    lr = self.defaults['lr']\n    params = [param]\n    momentum_buffer_list: List[Optional[Tensor]] = []\n    grads = []\n    has_sparse_grad = False\n    if grad is not None:\n        grads.append(grad)\n        if grad.is_sparse:\n            has_sparse_grad = True\n        if param not in self.state:\n            self.state[param] = {}\n        state = self.state[param]\n        if 'momentum_buffer' not in state:\n            momentum_buffer_list.append(None)\n        else:\n            momentum_buffer_list.append(state['momentum_buffer'])\n    with torch.no_grad():\n        F.sgd(params, grads, momentum_buffer_list, weight_decay=weight_decay, momentum=momentum, lr=lr, dampening=dampening, nesterov=self.nesterov, maximize=self.maximize, has_sparse_grad=has_sparse_grad, foreach=self.foreach)\n    state = self.state[param]\n    momentum_buffer = momentum_buffer_list[0]\n    if momentum_buffer is not None:\n        state['momentum_buffer'] = momentum_buffer"
        ]
    },
    {
        "func_name": "step",
        "original": "def step(self, gradients: List[Optional[Tensor]]):\n    params = self.param_group['params']\n    params_with_grad = []\n    grads = []\n    momentum_buffer_list: List[Optional[Tensor]] = []\n    lr = self.defaults['lr']\n    weight_decay = self.defaults['weight_decay']\n    momentum = self.defaults['momentum']\n    dampening = self.defaults['dampening']\n    if len(params) != len(gradients):\n        raise ValueError('the gradients passed in does not equal to the size of the parameters!' + f'Params length: {len(params)}. ' + f'Gradients length: {len(gradients)}')\n    has_sparse_grad = False\n    for (param, gradient) in zip(params, gradients):\n        if gradient is not None:\n            params_with_grad.append(param)\n            grads.append(gradient)\n            if gradient.is_sparse:\n                has_sparse_grad = True\n            if param not in self.state:\n                self.state[param] = {}\n            state = self.state[param]\n            if 'momentum_buffer' not in state:\n                momentum_buffer_list.append(None)\n            else:\n                momentum_buffer_list.append(state['momentum_buffer'])\n    with torch.no_grad():\n        F.sgd(params_with_grad, grads, momentum_buffer_list, weight_decay=weight_decay, momentum=momentum, lr=lr, dampening=dampening, nesterov=self.nesterov, maximize=self.maximize, has_sparse_grad=has_sparse_grad, foreach=self.foreach)\n    for (i, p) in enumerate(params_with_grad):\n        state = self.state[p]\n        momentum_buffer = momentum_buffer_list[i]\n        if momentum_buffer is not None:\n            state['momentum_buffer'] = momentum_buffer",
        "mutated": [
            "def step(self, gradients: List[Optional[Tensor]]):\n    if False:\n        i = 10\n    params = self.param_group['params']\n    params_with_grad = []\n    grads = []\n    momentum_buffer_list: List[Optional[Tensor]] = []\n    lr = self.defaults['lr']\n    weight_decay = self.defaults['weight_decay']\n    momentum = self.defaults['momentum']\n    dampening = self.defaults['dampening']\n    if len(params) != len(gradients):\n        raise ValueError('the gradients passed in does not equal to the size of the parameters!' + f'Params length: {len(params)}. ' + f'Gradients length: {len(gradients)}')\n    has_sparse_grad = False\n    for (param, gradient) in zip(params, gradients):\n        if gradient is not None:\n            params_with_grad.append(param)\n            grads.append(gradient)\n            if gradient.is_sparse:\n                has_sparse_grad = True\n            if param not in self.state:\n                self.state[param] = {}\n            state = self.state[param]\n            if 'momentum_buffer' not in state:\n                momentum_buffer_list.append(None)\n            else:\n                momentum_buffer_list.append(state['momentum_buffer'])\n    with torch.no_grad():\n        F.sgd(params_with_grad, grads, momentum_buffer_list, weight_decay=weight_decay, momentum=momentum, lr=lr, dampening=dampening, nesterov=self.nesterov, maximize=self.maximize, has_sparse_grad=has_sparse_grad, foreach=self.foreach)\n    for (i, p) in enumerate(params_with_grad):\n        state = self.state[p]\n        momentum_buffer = momentum_buffer_list[i]\n        if momentum_buffer is not None:\n            state['momentum_buffer'] = momentum_buffer",
            "def step(self, gradients: List[Optional[Tensor]]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    params = self.param_group['params']\n    params_with_grad = []\n    grads = []\n    momentum_buffer_list: List[Optional[Tensor]] = []\n    lr = self.defaults['lr']\n    weight_decay = self.defaults['weight_decay']\n    momentum = self.defaults['momentum']\n    dampening = self.defaults['dampening']\n    if len(params) != len(gradients):\n        raise ValueError('the gradients passed in does not equal to the size of the parameters!' + f'Params length: {len(params)}. ' + f'Gradients length: {len(gradients)}')\n    has_sparse_grad = False\n    for (param, gradient) in zip(params, gradients):\n        if gradient is not None:\n            params_with_grad.append(param)\n            grads.append(gradient)\n            if gradient.is_sparse:\n                has_sparse_grad = True\n            if param not in self.state:\n                self.state[param] = {}\n            state = self.state[param]\n            if 'momentum_buffer' not in state:\n                momentum_buffer_list.append(None)\n            else:\n                momentum_buffer_list.append(state['momentum_buffer'])\n    with torch.no_grad():\n        F.sgd(params_with_grad, grads, momentum_buffer_list, weight_decay=weight_decay, momentum=momentum, lr=lr, dampening=dampening, nesterov=self.nesterov, maximize=self.maximize, has_sparse_grad=has_sparse_grad, foreach=self.foreach)\n    for (i, p) in enumerate(params_with_grad):\n        state = self.state[p]\n        momentum_buffer = momentum_buffer_list[i]\n        if momentum_buffer is not None:\n            state['momentum_buffer'] = momentum_buffer",
            "def step(self, gradients: List[Optional[Tensor]]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    params = self.param_group['params']\n    params_with_grad = []\n    grads = []\n    momentum_buffer_list: List[Optional[Tensor]] = []\n    lr = self.defaults['lr']\n    weight_decay = self.defaults['weight_decay']\n    momentum = self.defaults['momentum']\n    dampening = self.defaults['dampening']\n    if len(params) != len(gradients):\n        raise ValueError('the gradients passed in does not equal to the size of the parameters!' + f'Params length: {len(params)}. ' + f'Gradients length: {len(gradients)}')\n    has_sparse_grad = False\n    for (param, gradient) in zip(params, gradients):\n        if gradient is not None:\n            params_with_grad.append(param)\n            grads.append(gradient)\n            if gradient.is_sparse:\n                has_sparse_grad = True\n            if param not in self.state:\n                self.state[param] = {}\n            state = self.state[param]\n            if 'momentum_buffer' not in state:\n                momentum_buffer_list.append(None)\n            else:\n                momentum_buffer_list.append(state['momentum_buffer'])\n    with torch.no_grad():\n        F.sgd(params_with_grad, grads, momentum_buffer_list, weight_decay=weight_decay, momentum=momentum, lr=lr, dampening=dampening, nesterov=self.nesterov, maximize=self.maximize, has_sparse_grad=has_sparse_grad, foreach=self.foreach)\n    for (i, p) in enumerate(params_with_grad):\n        state = self.state[p]\n        momentum_buffer = momentum_buffer_list[i]\n        if momentum_buffer is not None:\n            state['momentum_buffer'] = momentum_buffer",
            "def step(self, gradients: List[Optional[Tensor]]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    params = self.param_group['params']\n    params_with_grad = []\n    grads = []\n    momentum_buffer_list: List[Optional[Tensor]] = []\n    lr = self.defaults['lr']\n    weight_decay = self.defaults['weight_decay']\n    momentum = self.defaults['momentum']\n    dampening = self.defaults['dampening']\n    if len(params) != len(gradients):\n        raise ValueError('the gradients passed in does not equal to the size of the parameters!' + f'Params length: {len(params)}. ' + f'Gradients length: {len(gradients)}')\n    has_sparse_grad = False\n    for (param, gradient) in zip(params, gradients):\n        if gradient is not None:\n            params_with_grad.append(param)\n            grads.append(gradient)\n            if gradient.is_sparse:\n                has_sparse_grad = True\n            if param not in self.state:\n                self.state[param] = {}\n            state = self.state[param]\n            if 'momentum_buffer' not in state:\n                momentum_buffer_list.append(None)\n            else:\n                momentum_buffer_list.append(state['momentum_buffer'])\n    with torch.no_grad():\n        F.sgd(params_with_grad, grads, momentum_buffer_list, weight_decay=weight_decay, momentum=momentum, lr=lr, dampening=dampening, nesterov=self.nesterov, maximize=self.maximize, has_sparse_grad=has_sparse_grad, foreach=self.foreach)\n    for (i, p) in enumerate(params_with_grad):\n        state = self.state[p]\n        momentum_buffer = momentum_buffer_list[i]\n        if momentum_buffer is not None:\n            state['momentum_buffer'] = momentum_buffer",
            "def step(self, gradients: List[Optional[Tensor]]):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    params = self.param_group['params']\n    params_with_grad = []\n    grads = []\n    momentum_buffer_list: List[Optional[Tensor]] = []\n    lr = self.defaults['lr']\n    weight_decay = self.defaults['weight_decay']\n    momentum = self.defaults['momentum']\n    dampening = self.defaults['dampening']\n    if len(params) != len(gradients):\n        raise ValueError('the gradients passed in does not equal to the size of the parameters!' + f'Params length: {len(params)}. ' + f'Gradients length: {len(gradients)}')\n    has_sparse_grad = False\n    for (param, gradient) in zip(params, gradients):\n        if gradient is not None:\n            params_with_grad.append(param)\n            grads.append(gradient)\n            if gradient.is_sparse:\n                has_sparse_grad = True\n            if param not in self.state:\n                self.state[param] = {}\n            state = self.state[param]\n            if 'momentum_buffer' not in state:\n                momentum_buffer_list.append(None)\n            else:\n                momentum_buffer_list.append(state['momentum_buffer'])\n    with torch.no_grad():\n        F.sgd(params_with_grad, grads, momentum_buffer_list, weight_decay=weight_decay, momentum=momentum, lr=lr, dampening=dampening, nesterov=self.nesterov, maximize=self.maximize, has_sparse_grad=has_sparse_grad, foreach=self.foreach)\n    for (i, p) in enumerate(params_with_grad):\n        state = self.state[p]\n        momentum_buffer = momentum_buffer_list[i]\n        if momentum_buffer is not None:\n            state['momentum_buffer'] = momentum_buffer"
        ]
    }
]