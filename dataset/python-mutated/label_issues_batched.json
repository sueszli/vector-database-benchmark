[
    {
        "func_name": "find_label_issues_batched",
        "original": "def find_label_issues_batched(labels: Optional[LabelLike]=None, pred_probs: Optional[np.ndarray]=None, *, labels_file: Optional[str]=None, pred_probs_file: Optional[str]=None, batch_size: int=10000, n_jobs: Optional[int]=1, verbose: bool=True, quality_score_kwargs: Optional[dict]=None, num_issue_kwargs: Optional[dict]=None, return_mask: bool=False) -> np.ndarray:\n    \"\"\"\n    Variant of :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\n    that requires less memory by reading from `pred_probs`, `labels` in mini-batches.\n    To avoid loading big `pred_probs`, `labels` arrays into memory,\n    provide these as memory-mapped objects like Zarr arrays or memmap arrays instead of regular numpy arrays.\n    See: https://pythonspeed.com/articles/mmap-vs-zarr-hdf5/\n\n    With default settings, the results returned from this method closely approximate those returned from:\n    ``cleanlab.filter.find_label_issues(..., filter_by=\"low_self_confidence\", return_indices_ranked_by=\"self_confidence\")``\n\n    This function internally implements the example usage script of the ``LabelInspector`` class,\n    but you can further customize that script by running it yourself instead of this function.\n    See the documentation of ``LabelInspector`` to learn more about how this method works internally.\n\n    Parameters\n    ----------\n    labels: np.ndarray-like object, optional\n      1D array of given class labels for each example in the dataset, (int) values in ``0,1,2,...,K-1``.\n      To avoid loading big objects into memory, you should pass this as a memory-mapped object like:\n      Zarr array loaded with ``zarr.convenience.open(YOURFILE.zarr, mode=\"r\")``,\n      or memmap array loaded with ``np.load(YOURFILE.npy, mmap_mode=\"r\")``.\n\n      Tip: You can save an existing numpy array to Zarr via: ``zarr.convenience.save_array(YOURFILE.zarr, your_array)``,\n      or to .npy file that can be loaded with mmap via: ``np.save(YOURFILE.npy, your_array)``.\n\n    pred_probs: np.ndarray-like object, optional\n      2D array of model-predicted class probabilities (floats) for each example in the dataset.\n      To avoid loading big objects into memory, you should pass this as a memory-mapped object like:\n      Zarr array loaded with ``zarr.convenience.open(YOURFILE.zarr, mode=\"r\")``\n      or memmap array loaded with ``np.load(YOURFILE.npy, mmap_mode=\"r\")``.\n\n    labels_file: str, optional\n      Specify this instead of `labels` if you want this method to load from file for you into a memmap array.\n      Path to .npy file where the entire 1D `labels` numpy array is stored on disk (list format is not supported).\n      This is loaded using: ``np.load(labels_file, mmap_mode=\"r\")``\n      so make sure this file was created via: ``np.save()`` or other compatible methods (.npz not supported).\n\n    pred_probs_file: str, optional\n      Specify this instead of `pred_probs` if you want this method to load from file for you into a memmap array.\n      Path to .npy file where the entire `pred_probs` numpy array is stored on disk.\n      This is loaded using: ``np.load(pred_probs_file, mmap_mode=\"r\")``\n      so make sure this file was created via: ``np.save()`` or other compatible methods (.npz not supported).\n\n    batch_size : int, optional\n      Size of mini-batches to use for estimating the label issues.\n      To maximize efficiency, try to use the largest `batch_size` your memory allows.\n\n    n_jobs: int, optional\n      Number of processes for multiprocessing (default value = 1). Only used on Linux.\n      If `n_jobs=None`, will use either the number of: physical cores if psutil is installed, or logical cores otherwise.\n\n    verbose : bool, optional\n      Whether to suppress print statements or not.\n\n    quality_score_kwargs : dict, optional\n      Keyword arguments to pass into :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\n\n    num_issue_kwargs : dict, optional\n      Keyword arguments to :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`\n      to control estimation of the number of label issues.\n      The only supported kwarg here for now is: `estimation_method`.\n    return_mask : bool, optional\n       Determines what is returned by this method: If `return_mask=True`, return a boolean mask.\n       If `False`, return a list of indices specifying examples with label issues, sorted by label quality score.\n\n    Returns\n    -------\n    label_issues : np.ndarray\n      If `return_mask` is `True`, returns a boolean **mask** for the entire dataset\n      where ``True`` represents a label issue and ``False`` represents an example that is\n      accurately labeled with high confidence.\n      If `return_mask` is `False`, returns an array containing **indices** of examples identified to have\n      label issues (i.e. those indices where the mask would be ``True``), sorted by likelihood that the corresponding label is correct.\n    --------\n    >>> batch_size = 10000  # for efficiency, set this to as large of a value as your memory can handle\n    >>> # Just demonstrating how to save your existing numpy labels, pred_probs arrays to compatible .npy files:\n    >>> np.save(\"LABELS.npy\", labels_array)\n    >>> np.save(\"PREDPROBS.npy\", pred_probs_array)\n    >>> # You can load these back into memmap arrays via: labels = np.load(\"LABELS.npy\", mmap_mode=\"r\")\n    >>> # and then run this method on the memmap arrays, or just run it directly on the .npy files like this:\n    >>> issues = find_label_issues_batched(labels_file=\"LABELS.npy\", pred_probs_file=\"PREDPROBS.npy\", batch_size=batch_size)\n    >>> # This method also works with Zarr arrays:\n    >>> import zarr\n    >>> # Just demonstrating how to save your existing numpy labels, pred_probs arrays to compatible .zarr files:\n    >>> zarr.convenience.save_array(\"LABELS.zarr\", labels_array)\n    >>> zarr.convenience.save_array(\"PREDPROBS.zarr\", pred_probs_array)\n    >>> # You can load from such files into Zarr arrays:\n    >>> labels = zarr.convenience.open(\"LABELS.zarr\", mode=\"r\")\n    >>> pred_probs = zarr.convenience.open(\"PREDPROBS.zarr\", mode=\"r\")\n    >>> # This method can be directly run on Zarr arrays, memmap arrays, or regular numpy arrays:\n    >>> issues = find_label_issues_batched(labels=labels, pred_probs=pred_probs, batch_size=batch_size)\n    \"\"\"\n    if labels_file is not None:\n        if labels is not None:\n            raise ValueError('only specify one of: `labels` or `labels_file`')\n        if not isinstance(labels_file, str):\n            raise ValueError('labels_file must be str specifying path to .npy file containing the array of labels')\n        labels = np.load(labels_file, mmap_mode='r')\n        assert isinstance(labels, np.ndarray)\n    if pred_probs_file is not None:\n        if pred_probs is not None:\n            raise ValueError('only specify one of: `pred_probs` or `pred_probs_file`')\n        if not isinstance(pred_probs_file, str):\n            raise ValueError('pred_probs_file must be str specifying path to .npy file containing 2D array of pred_probs')\n        pred_probs = np.load(pred_probs_file, mmap_mode='r')\n        assert isinstance(pred_probs, np.ndarray)\n        if verbose:\n            print(f'mmap-loaded numpy arrays have: {len(pred_probs)} examples, {pred_probs.shape[1]} classes')\n    if labels is None:\n        raise ValueError('must provide one of: `labels` or `labels_file`')\n    if pred_probs is None:\n        raise ValueError('must provide one of: `pred_probs` or `pred_probs_file`')\n    assert pred_probs is not None\n    if len(labels) != len(pred_probs):\n        raise ValueError(f'len(labels)={len(labels)} does not match len(pred_probs)={len(pred_probs)}. Perhaps an issue loading mmap numpy arrays from file.')\n    lab = LabelInspector(num_class=pred_probs.shape[1], verbose=verbose, n_jobs=n_jobs, quality_score_kwargs=quality_score_kwargs, num_issue_kwargs=num_issue_kwargs)\n    n = len(labels)\n    if verbose:\n        from tqdm.auto import tqdm\n        pbar = tqdm(desc='number of examples processed for estimating thresholds', total=n)\n    i = 0\n    while i < n:\n        end_index = i + batch_size\n        labels_batch = labels[i:end_index]\n        pred_probs_batch = pred_probs[i:end_index, :]\n        i = end_index\n        lab.update_confident_thresholds(labels_batch, pred_probs_batch)\n        if verbose:\n            pbar.update(batch_size)\n    if verbose:\n        pbar.close()\n        pbar = tqdm(desc='number of examples processed for checking labels', total=n)\n    i = 0\n    while i < n:\n        end_index = i + batch_size\n        labels_batch = labels[i:end_index]\n        pred_probs_batch = pred_probs[i:end_index, :]\n        i = end_index\n        _ = lab.score_label_quality(labels_batch, pred_probs_batch)\n        if verbose:\n            pbar.update(batch_size)\n    if verbose:\n        pbar.close()\n    label_issues_indices = lab.get_label_issues()\n    if return_mask:\n        label_issues_mask = np.zeros(len(labels), dtype=bool)\n        label_issues_mask[label_issues_indices] = True\n        return label_issues_mask\n    return label_issues_indices",
        "mutated": [
            "def find_label_issues_batched(labels: Optional[LabelLike]=None, pred_probs: Optional[np.ndarray]=None, *, labels_file: Optional[str]=None, pred_probs_file: Optional[str]=None, batch_size: int=10000, n_jobs: Optional[int]=1, verbose: bool=True, quality_score_kwargs: Optional[dict]=None, num_issue_kwargs: Optional[dict]=None, return_mask: bool=False) -> np.ndarray:\n    if False:\n        i = 10\n    '\\n    Variant of :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n    that requires less memory by reading from `pred_probs`, `labels` in mini-batches.\\n    To avoid loading big `pred_probs`, `labels` arrays into memory,\\n    provide these as memory-mapped objects like Zarr arrays or memmap arrays instead of regular numpy arrays.\\n    See: https://pythonspeed.com/articles/mmap-vs-zarr-hdf5/\\n\\n    With default settings, the results returned from this method closely approximate those returned from:\\n    ``cleanlab.filter.find_label_issues(..., filter_by=\"low_self_confidence\", return_indices_ranked_by=\"self_confidence\")``\\n\\n    This function internally implements the example usage script of the ``LabelInspector`` class,\\n    but you can further customize that script by running it yourself instead of this function.\\n    See the documentation of ``LabelInspector`` to learn more about how this method works internally.\\n\\n    Parameters\\n    ----------\\n    labels: np.ndarray-like object, optional\\n      1D array of given class labels for each example in the dataset, (int) values in ``0,1,2,...,K-1``.\\n      To avoid loading big objects into memory, you should pass this as a memory-mapped object like:\\n      Zarr array loaded with ``zarr.convenience.open(YOURFILE.zarr, mode=\"r\")``,\\n      or memmap array loaded with ``np.load(YOURFILE.npy, mmap_mode=\"r\")``.\\n\\n      Tip: You can save an existing numpy array to Zarr via: ``zarr.convenience.save_array(YOURFILE.zarr, your_array)``,\\n      or to .npy file that can be loaded with mmap via: ``np.save(YOURFILE.npy, your_array)``.\\n\\n    pred_probs: np.ndarray-like object, optional\\n      2D array of model-predicted class probabilities (floats) for each example in the dataset.\\n      To avoid loading big objects into memory, you should pass this as a memory-mapped object like:\\n      Zarr array loaded with ``zarr.convenience.open(YOURFILE.zarr, mode=\"r\")``\\n      or memmap array loaded with ``np.load(YOURFILE.npy, mmap_mode=\"r\")``.\\n\\n    labels_file: str, optional\\n      Specify this instead of `labels` if you want this method to load from file for you into a memmap array.\\n      Path to .npy file where the entire 1D `labels` numpy array is stored on disk (list format is not supported).\\n      This is loaded using: ``np.load(labels_file, mmap_mode=\"r\")``\\n      so make sure this file was created via: ``np.save()`` or other compatible methods (.npz not supported).\\n\\n    pred_probs_file: str, optional\\n      Specify this instead of `pred_probs` if you want this method to load from file for you into a memmap array.\\n      Path to .npy file where the entire `pred_probs` numpy array is stored on disk.\\n      This is loaded using: ``np.load(pred_probs_file, mmap_mode=\"r\")``\\n      so make sure this file was created via: ``np.save()`` or other compatible methods (.npz not supported).\\n\\n    batch_size : int, optional\\n      Size of mini-batches to use for estimating the label issues.\\n      To maximize efficiency, try to use the largest `batch_size` your memory allows.\\n\\n    n_jobs: int, optional\\n      Number of processes for multiprocessing (default value = 1). Only used on Linux.\\n      If `n_jobs=None`, will use either the number of: physical cores if psutil is installed, or logical cores otherwise.\\n\\n    verbose : bool, optional\\n      Whether to suppress print statements or not.\\n\\n    quality_score_kwargs : dict, optional\\n      Keyword arguments to pass into :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n    num_issue_kwargs : dict, optional\\n      Keyword arguments to :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`\\n      to control estimation of the number of label issues.\\n      The only supported kwarg here for now is: `estimation_method`.\\n    return_mask : bool, optional\\n       Determines what is returned by this method: If `return_mask=True`, return a boolean mask.\\n       If `False`, return a list of indices specifying examples with label issues, sorted by label quality score.\\n\\n    Returns\\n    -------\\n    label_issues : np.ndarray\\n      If `return_mask` is `True`, returns a boolean **mask** for the entire dataset\\n      where ``True`` represents a label issue and ``False`` represents an example that is\\n      accurately labeled with high confidence.\\n      If `return_mask` is `False`, returns an array containing **indices** of examples identified to have\\n      label issues (i.e. those indices where the mask would be ``True``), sorted by likelihood that the corresponding label is correct.\\n    --------\\n    >>> batch_size = 10000  # for efficiency, set this to as large of a value as your memory can handle\\n    >>> # Just demonstrating how to save your existing numpy labels, pred_probs arrays to compatible .npy files:\\n    >>> np.save(\"LABELS.npy\", labels_array)\\n    >>> np.save(\"PREDPROBS.npy\", pred_probs_array)\\n    >>> # You can load these back into memmap arrays via: labels = np.load(\"LABELS.npy\", mmap_mode=\"r\")\\n    >>> # and then run this method on the memmap arrays, or just run it directly on the .npy files like this:\\n    >>> issues = find_label_issues_batched(labels_file=\"LABELS.npy\", pred_probs_file=\"PREDPROBS.npy\", batch_size=batch_size)\\n    >>> # This method also works with Zarr arrays:\\n    >>> import zarr\\n    >>> # Just demonstrating how to save your existing numpy labels, pred_probs arrays to compatible .zarr files:\\n    >>> zarr.convenience.save_array(\"LABELS.zarr\", labels_array)\\n    >>> zarr.convenience.save_array(\"PREDPROBS.zarr\", pred_probs_array)\\n    >>> # You can load from such files into Zarr arrays:\\n    >>> labels = zarr.convenience.open(\"LABELS.zarr\", mode=\"r\")\\n    >>> pred_probs = zarr.convenience.open(\"PREDPROBS.zarr\", mode=\"r\")\\n    >>> # This method can be directly run on Zarr arrays, memmap arrays, or regular numpy arrays:\\n    >>> issues = find_label_issues_batched(labels=labels, pred_probs=pred_probs, batch_size=batch_size)\\n    '\n    if labels_file is not None:\n        if labels is not None:\n            raise ValueError('only specify one of: `labels` or `labels_file`')\n        if not isinstance(labels_file, str):\n            raise ValueError('labels_file must be str specifying path to .npy file containing the array of labels')\n        labels = np.load(labels_file, mmap_mode='r')\n        assert isinstance(labels, np.ndarray)\n    if pred_probs_file is not None:\n        if pred_probs is not None:\n            raise ValueError('only specify one of: `pred_probs` or `pred_probs_file`')\n        if not isinstance(pred_probs_file, str):\n            raise ValueError('pred_probs_file must be str specifying path to .npy file containing 2D array of pred_probs')\n        pred_probs = np.load(pred_probs_file, mmap_mode='r')\n        assert isinstance(pred_probs, np.ndarray)\n        if verbose:\n            print(f'mmap-loaded numpy arrays have: {len(pred_probs)} examples, {pred_probs.shape[1]} classes')\n    if labels is None:\n        raise ValueError('must provide one of: `labels` or `labels_file`')\n    if pred_probs is None:\n        raise ValueError('must provide one of: `pred_probs` or `pred_probs_file`')\n    assert pred_probs is not None\n    if len(labels) != len(pred_probs):\n        raise ValueError(f'len(labels)={len(labels)} does not match len(pred_probs)={len(pred_probs)}. Perhaps an issue loading mmap numpy arrays from file.')\n    lab = LabelInspector(num_class=pred_probs.shape[1], verbose=verbose, n_jobs=n_jobs, quality_score_kwargs=quality_score_kwargs, num_issue_kwargs=num_issue_kwargs)\n    n = len(labels)\n    if verbose:\n        from tqdm.auto import tqdm\n        pbar = tqdm(desc='number of examples processed for estimating thresholds', total=n)\n    i = 0\n    while i < n:\n        end_index = i + batch_size\n        labels_batch = labels[i:end_index]\n        pred_probs_batch = pred_probs[i:end_index, :]\n        i = end_index\n        lab.update_confident_thresholds(labels_batch, pred_probs_batch)\n        if verbose:\n            pbar.update(batch_size)\n    if verbose:\n        pbar.close()\n        pbar = tqdm(desc='number of examples processed for checking labels', total=n)\n    i = 0\n    while i < n:\n        end_index = i + batch_size\n        labels_batch = labels[i:end_index]\n        pred_probs_batch = pred_probs[i:end_index, :]\n        i = end_index\n        _ = lab.score_label_quality(labels_batch, pred_probs_batch)\n        if verbose:\n            pbar.update(batch_size)\n    if verbose:\n        pbar.close()\n    label_issues_indices = lab.get_label_issues()\n    if return_mask:\n        label_issues_mask = np.zeros(len(labels), dtype=bool)\n        label_issues_mask[label_issues_indices] = True\n        return label_issues_mask\n    return label_issues_indices",
            "def find_label_issues_batched(labels: Optional[LabelLike]=None, pred_probs: Optional[np.ndarray]=None, *, labels_file: Optional[str]=None, pred_probs_file: Optional[str]=None, batch_size: int=10000, n_jobs: Optional[int]=1, verbose: bool=True, quality_score_kwargs: Optional[dict]=None, num_issue_kwargs: Optional[dict]=None, return_mask: bool=False) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Variant of :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n    that requires less memory by reading from `pred_probs`, `labels` in mini-batches.\\n    To avoid loading big `pred_probs`, `labels` arrays into memory,\\n    provide these as memory-mapped objects like Zarr arrays or memmap arrays instead of regular numpy arrays.\\n    See: https://pythonspeed.com/articles/mmap-vs-zarr-hdf5/\\n\\n    With default settings, the results returned from this method closely approximate those returned from:\\n    ``cleanlab.filter.find_label_issues(..., filter_by=\"low_self_confidence\", return_indices_ranked_by=\"self_confidence\")``\\n\\n    This function internally implements the example usage script of the ``LabelInspector`` class,\\n    but you can further customize that script by running it yourself instead of this function.\\n    See the documentation of ``LabelInspector`` to learn more about how this method works internally.\\n\\n    Parameters\\n    ----------\\n    labels: np.ndarray-like object, optional\\n      1D array of given class labels for each example in the dataset, (int) values in ``0,1,2,...,K-1``.\\n      To avoid loading big objects into memory, you should pass this as a memory-mapped object like:\\n      Zarr array loaded with ``zarr.convenience.open(YOURFILE.zarr, mode=\"r\")``,\\n      or memmap array loaded with ``np.load(YOURFILE.npy, mmap_mode=\"r\")``.\\n\\n      Tip: You can save an existing numpy array to Zarr via: ``zarr.convenience.save_array(YOURFILE.zarr, your_array)``,\\n      or to .npy file that can be loaded with mmap via: ``np.save(YOURFILE.npy, your_array)``.\\n\\n    pred_probs: np.ndarray-like object, optional\\n      2D array of model-predicted class probabilities (floats) for each example in the dataset.\\n      To avoid loading big objects into memory, you should pass this as a memory-mapped object like:\\n      Zarr array loaded with ``zarr.convenience.open(YOURFILE.zarr, mode=\"r\")``\\n      or memmap array loaded with ``np.load(YOURFILE.npy, mmap_mode=\"r\")``.\\n\\n    labels_file: str, optional\\n      Specify this instead of `labels` if you want this method to load from file for you into a memmap array.\\n      Path to .npy file where the entire 1D `labels` numpy array is stored on disk (list format is not supported).\\n      This is loaded using: ``np.load(labels_file, mmap_mode=\"r\")``\\n      so make sure this file was created via: ``np.save()`` or other compatible methods (.npz not supported).\\n\\n    pred_probs_file: str, optional\\n      Specify this instead of `pred_probs` if you want this method to load from file for you into a memmap array.\\n      Path to .npy file where the entire `pred_probs` numpy array is stored on disk.\\n      This is loaded using: ``np.load(pred_probs_file, mmap_mode=\"r\")``\\n      so make sure this file was created via: ``np.save()`` or other compatible methods (.npz not supported).\\n\\n    batch_size : int, optional\\n      Size of mini-batches to use for estimating the label issues.\\n      To maximize efficiency, try to use the largest `batch_size` your memory allows.\\n\\n    n_jobs: int, optional\\n      Number of processes for multiprocessing (default value = 1). Only used on Linux.\\n      If `n_jobs=None`, will use either the number of: physical cores if psutil is installed, or logical cores otherwise.\\n\\n    verbose : bool, optional\\n      Whether to suppress print statements or not.\\n\\n    quality_score_kwargs : dict, optional\\n      Keyword arguments to pass into :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n    num_issue_kwargs : dict, optional\\n      Keyword arguments to :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`\\n      to control estimation of the number of label issues.\\n      The only supported kwarg here for now is: `estimation_method`.\\n    return_mask : bool, optional\\n       Determines what is returned by this method: If `return_mask=True`, return a boolean mask.\\n       If `False`, return a list of indices specifying examples with label issues, sorted by label quality score.\\n\\n    Returns\\n    -------\\n    label_issues : np.ndarray\\n      If `return_mask` is `True`, returns a boolean **mask** for the entire dataset\\n      where ``True`` represents a label issue and ``False`` represents an example that is\\n      accurately labeled with high confidence.\\n      If `return_mask` is `False`, returns an array containing **indices** of examples identified to have\\n      label issues (i.e. those indices where the mask would be ``True``), sorted by likelihood that the corresponding label is correct.\\n    --------\\n    >>> batch_size = 10000  # for efficiency, set this to as large of a value as your memory can handle\\n    >>> # Just demonstrating how to save your existing numpy labels, pred_probs arrays to compatible .npy files:\\n    >>> np.save(\"LABELS.npy\", labels_array)\\n    >>> np.save(\"PREDPROBS.npy\", pred_probs_array)\\n    >>> # You can load these back into memmap arrays via: labels = np.load(\"LABELS.npy\", mmap_mode=\"r\")\\n    >>> # and then run this method on the memmap arrays, or just run it directly on the .npy files like this:\\n    >>> issues = find_label_issues_batched(labels_file=\"LABELS.npy\", pred_probs_file=\"PREDPROBS.npy\", batch_size=batch_size)\\n    >>> # This method also works with Zarr arrays:\\n    >>> import zarr\\n    >>> # Just demonstrating how to save your existing numpy labels, pred_probs arrays to compatible .zarr files:\\n    >>> zarr.convenience.save_array(\"LABELS.zarr\", labels_array)\\n    >>> zarr.convenience.save_array(\"PREDPROBS.zarr\", pred_probs_array)\\n    >>> # You can load from such files into Zarr arrays:\\n    >>> labels = zarr.convenience.open(\"LABELS.zarr\", mode=\"r\")\\n    >>> pred_probs = zarr.convenience.open(\"PREDPROBS.zarr\", mode=\"r\")\\n    >>> # This method can be directly run on Zarr arrays, memmap arrays, or regular numpy arrays:\\n    >>> issues = find_label_issues_batched(labels=labels, pred_probs=pred_probs, batch_size=batch_size)\\n    '\n    if labels_file is not None:\n        if labels is not None:\n            raise ValueError('only specify one of: `labels` or `labels_file`')\n        if not isinstance(labels_file, str):\n            raise ValueError('labels_file must be str specifying path to .npy file containing the array of labels')\n        labels = np.load(labels_file, mmap_mode='r')\n        assert isinstance(labels, np.ndarray)\n    if pred_probs_file is not None:\n        if pred_probs is not None:\n            raise ValueError('only specify one of: `pred_probs` or `pred_probs_file`')\n        if not isinstance(pred_probs_file, str):\n            raise ValueError('pred_probs_file must be str specifying path to .npy file containing 2D array of pred_probs')\n        pred_probs = np.load(pred_probs_file, mmap_mode='r')\n        assert isinstance(pred_probs, np.ndarray)\n        if verbose:\n            print(f'mmap-loaded numpy arrays have: {len(pred_probs)} examples, {pred_probs.shape[1]} classes')\n    if labels is None:\n        raise ValueError('must provide one of: `labels` or `labels_file`')\n    if pred_probs is None:\n        raise ValueError('must provide one of: `pred_probs` or `pred_probs_file`')\n    assert pred_probs is not None\n    if len(labels) != len(pred_probs):\n        raise ValueError(f'len(labels)={len(labels)} does not match len(pred_probs)={len(pred_probs)}. Perhaps an issue loading mmap numpy arrays from file.')\n    lab = LabelInspector(num_class=pred_probs.shape[1], verbose=verbose, n_jobs=n_jobs, quality_score_kwargs=quality_score_kwargs, num_issue_kwargs=num_issue_kwargs)\n    n = len(labels)\n    if verbose:\n        from tqdm.auto import tqdm\n        pbar = tqdm(desc='number of examples processed for estimating thresholds', total=n)\n    i = 0\n    while i < n:\n        end_index = i + batch_size\n        labels_batch = labels[i:end_index]\n        pred_probs_batch = pred_probs[i:end_index, :]\n        i = end_index\n        lab.update_confident_thresholds(labels_batch, pred_probs_batch)\n        if verbose:\n            pbar.update(batch_size)\n    if verbose:\n        pbar.close()\n        pbar = tqdm(desc='number of examples processed for checking labels', total=n)\n    i = 0\n    while i < n:\n        end_index = i + batch_size\n        labels_batch = labels[i:end_index]\n        pred_probs_batch = pred_probs[i:end_index, :]\n        i = end_index\n        _ = lab.score_label_quality(labels_batch, pred_probs_batch)\n        if verbose:\n            pbar.update(batch_size)\n    if verbose:\n        pbar.close()\n    label_issues_indices = lab.get_label_issues()\n    if return_mask:\n        label_issues_mask = np.zeros(len(labels), dtype=bool)\n        label_issues_mask[label_issues_indices] = True\n        return label_issues_mask\n    return label_issues_indices",
            "def find_label_issues_batched(labels: Optional[LabelLike]=None, pred_probs: Optional[np.ndarray]=None, *, labels_file: Optional[str]=None, pred_probs_file: Optional[str]=None, batch_size: int=10000, n_jobs: Optional[int]=1, verbose: bool=True, quality_score_kwargs: Optional[dict]=None, num_issue_kwargs: Optional[dict]=None, return_mask: bool=False) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Variant of :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n    that requires less memory by reading from `pred_probs`, `labels` in mini-batches.\\n    To avoid loading big `pred_probs`, `labels` arrays into memory,\\n    provide these as memory-mapped objects like Zarr arrays or memmap arrays instead of regular numpy arrays.\\n    See: https://pythonspeed.com/articles/mmap-vs-zarr-hdf5/\\n\\n    With default settings, the results returned from this method closely approximate those returned from:\\n    ``cleanlab.filter.find_label_issues(..., filter_by=\"low_self_confidence\", return_indices_ranked_by=\"self_confidence\")``\\n\\n    This function internally implements the example usage script of the ``LabelInspector`` class,\\n    but you can further customize that script by running it yourself instead of this function.\\n    See the documentation of ``LabelInspector`` to learn more about how this method works internally.\\n\\n    Parameters\\n    ----------\\n    labels: np.ndarray-like object, optional\\n      1D array of given class labels for each example in the dataset, (int) values in ``0,1,2,...,K-1``.\\n      To avoid loading big objects into memory, you should pass this as a memory-mapped object like:\\n      Zarr array loaded with ``zarr.convenience.open(YOURFILE.zarr, mode=\"r\")``,\\n      or memmap array loaded with ``np.load(YOURFILE.npy, mmap_mode=\"r\")``.\\n\\n      Tip: You can save an existing numpy array to Zarr via: ``zarr.convenience.save_array(YOURFILE.zarr, your_array)``,\\n      or to .npy file that can be loaded with mmap via: ``np.save(YOURFILE.npy, your_array)``.\\n\\n    pred_probs: np.ndarray-like object, optional\\n      2D array of model-predicted class probabilities (floats) for each example in the dataset.\\n      To avoid loading big objects into memory, you should pass this as a memory-mapped object like:\\n      Zarr array loaded with ``zarr.convenience.open(YOURFILE.zarr, mode=\"r\")``\\n      or memmap array loaded with ``np.load(YOURFILE.npy, mmap_mode=\"r\")``.\\n\\n    labels_file: str, optional\\n      Specify this instead of `labels` if you want this method to load from file for you into a memmap array.\\n      Path to .npy file where the entire 1D `labels` numpy array is stored on disk (list format is not supported).\\n      This is loaded using: ``np.load(labels_file, mmap_mode=\"r\")``\\n      so make sure this file was created via: ``np.save()`` or other compatible methods (.npz not supported).\\n\\n    pred_probs_file: str, optional\\n      Specify this instead of `pred_probs` if you want this method to load from file for you into a memmap array.\\n      Path to .npy file where the entire `pred_probs` numpy array is stored on disk.\\n      This is loaded using: ``np.load(pred_probs_file, mmap_mode=\"r\")``\\n      so make sure this file was created via: ``np.save()`` or other compatible methods (.npz not supported).\\n\\n    batch_size : int, optional\\n      Size of mini-batches to use for estimating the label issues.\\n      To maximize efficiency, try to use the largest `batch_size` your memory allows.\\n\\n    n_jobs: int, optional\\n      Number of processes for multiprocessing (default value = 1). Only used on Linux.\\n      If `n_jobs=None`, will use either the number of: physical cores if psutil is installed, or logical cores otherwise.\\n\\n    verbose : bool, optional\\n      Whether to suppress print statements or not.\\n\\n    quality_score_kwargs : dict, optional\\n      Keyword arguments to pass into :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n    num_issue_kwargs : dict, optional\\n      Keyword arguments to :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`\\n      to control estimation of the number of label issues.\\n      The only supported kwarg here for now is: `estimation_method`.\\n    return_mask : bool, optional\\n       Determines what is returned by this method: If `return_mask=True`, return a boolean mask.\\n       If `False`, return a list of indices specifying examples with label issues, sorted by label quality score.\\n\\n    Returns\\n    -------\\n    label_issues : np.ndarray\\n      If `return_mask` is `True`, returns a boolean **mask** for the entire dataset\\n      where ``True`` represents a label issue and ``False`` represents an example that is\\n      accurately labeled with high confidence.\\n      If `return_mask` is `False`, returns an array containing **indices** of examples identified to have\\n      label issues (i.e. those indices where the mask would be ``True``), sorted by likelihood that the corresponding label is correct.\\n    --------\\n    >>> batch_size = 10000  # for efficiency, set this to as large of a value as your memory can handle\\n    >>> # Just demonstrating how to save your existing numpy labels, pred_probs arrays to compatible .npy files:\\n    >>> np.save(\"LABELS.npy\", labels_array)\\n    >>> np.save(\"PREDPROBS.npy\", pred_probs_array)\\n    >>> # You can load these back into memmap arrays via: labels = np.load(\"LABELS.npy\", mmap_mode=\"r\")\\n    >>> # and then run this method on the memmap arrays, or just run it directly on the .npy files like this:\\n    >>> issues = find_label_issues_batched(labels_file=\"LABELS.npy\", pred_probs_file=\"PREDPROBS.npy\", batch_size=batch_size)\\n    >>> # This method also works with Zarr arrays:\\n    >>> import zarr\\n    >>> # Just demonstrating how to save your existing numpy labels, pred_probs arrays to compatible .zarr files:\\n    >>> zarr.convenience.save_array(\"LABELS.zarr\", labels_array)\\n    >>> zarr.convenience.save_array(\"PREDPROBS.zarr\", pred_probs_array)\\n    >>> # You can load from such files into Zarr arrays:\\n    >>> labels = zarr.convenience.open(\"LABELS.zarr\", mode=\"r\")\\n    >>> pred_probs = zarr.convenience.open(\"PREDPROBS.zarr\", mode=\"r\")\\n    >>> # This method can be directly run on Zarr arrays, memmap arrays, or regular numpy arrays:\\n    >>> issues = find_label_issues_batched(labels=labels, pred_probs=pred_probs, batch_size=batch_size)\\n    '\n    if labels_file is not None:\n        if labels is not None:\n            raise ValueError('only specify one of: `labels` or `labels_file`')\n        if not isinstance(labels_file, str):\n            raise ValueError('labels_file must be str specifying path to .npy file containing the array of labels')\n        labels = np.load(labels_file, mmap_mode='r')\n        assert isinstance(labels, np.ndarray)\n    if pred_probs_file is not None:\n        if pred_probs is not None:\n            raise ValueError('only specify one of: `pred_probs` or `pred_probs_file`')\n        if not isinstance(pred_probs_file, str):\n            raise ValueError('pred_probs_file must be str specifying path to .npy file containing 2D array of pred_probs')\n        pred_probs = np.load(pred_probs_file, mmap_mode='r')\n        assert isinstance(pred_probs, np.ndarray)\n        if verbose:\n            print(f'mmap-loaded numpy arrays have: {len(pred_probs)} examples, {pred_probs.shape[1]} classes')\n    if labels is None:\n        raise ValueError('must provide one of: `labels` or `labels_file`')\n    if pred_probs is None:\n        raise ValueError('must provide one of: `pred_probs` or `pred_probs_file`')\n    assert pred_probs is not None\n    if len(labels) != len(pred_probs):\n        raise ValueError(f'len(labels)={len(labels)} does not match len(pred_probs)={len(pred_probs)}. Perhaps an issue loading mmap numpy arrays from file.')\n    lab = LabelInspector(num_class=pred_probs.shape[1], verbose=verbose, n_jobs=n_jobs, quality_score_kwargs=quality_score_kwargs, num_issue_kwargs=num_issue_kwargs)\n    n = len(labels)\n    if verbose:\n        from tqdm.auto import tqdm\n        pbar = tqdm(desc='number of examples processed for estimating thresholds', total=n)\n    i = 0\n    while i < n:\n        end_index = i + batch_size\n        labels_batch = labels[i:end_index]\n        pred_probs_batch = pred_probs[i:end_index, :]\n        i = end_index\n        lab.update_confident_thresholds(labels_batch, pred_probs_batch)\n        if verbose:\n            pbar.update(batch_size)\n    if verbose:\n        pbar.close()\n        pbar = tqdm(desc='number of examples processed for checking labels', total=n)\n    i = 0\n    while i < n:\n        end_index = i + batch_size\n        labels_batch = labels[i:end_index]\n        pred_probs_batch = pred_probs[i:end_index, :]\n        i = end_index\n        _ = lab.score_label_quality(labels_batch, pred_probs_batch)\n        if verbose:\n            pbar.update(batch_size)\n    if verbose:\n        pbar.close()\n    label_issues_indices = lab.get_label_issues()\n    if return_mask:\n        label_issues_mask = np.zeros(len(labels), dtype=bool)\n        label_issues_mask[label_issues_indices] = True\n        return label_issues_mask\n    return label_issues_indices",
            "def find_label_issues_batched(labels: Optional[LabelLike]=None, pred_probs: Optional[np.ndarray]=None, *, labels_file: Optional[str]=None, pred_probs_file: Optional[str]=None, batch_size: int=10000, n_jobs: Optional[int]=1, verbose: bool=True, quality_score_kwargs: Optional[dict]=None, num_issue_kwargs: Optional[dict]=None, return_mask: bool=False) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Variant of :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n    that requires less memory by reading from `pred_probs`, `labels` in mini-batches.\\n    To avoid loading big `pred_probs`, `labels` arrays into memory,\\n    provide these as memory-mapped objects like Zarr arrays or memmap arrays instead of regular numpy arrays.\\n    See: https://pythonspeed.com/articles/mmap-vs-zarr-hdf5/\\n\\n    With default settings, the results returned from this method closely approximate those returned from:\\n    ``cleanlab.filter.find_label_issues(..., filter_by=\"low_self_confidence\", return_indices_ranked_by=\"self_confidence\")``\\n\\n    This function internally implements the example usage script of the ``LabelInspector`` class,\\n    but you can further customize that script by running it yourself instead of this function.\\n    See the documentation of ``LabelInspector`` to learn more about how this method works internally.\\n\\n    Parameters\\n    ----------\\n    labels: np.ndarray-like object, optional\\n      1D array of given class labels for each example in the dataset, (int) values in ``0,1,2,...,K-1``.\\n      To avoid loading big objects into memory, you should pass this as a memory-mapped object like:\\n      Zarr array loaded with ``zarr.convenience.open(YOURFILE.zarr, mode=\"r\")``,\\n      or memmap array loaded with ``np.load(YOURFILE.npy, mmap_mode=\"r\")``.\\n\\n      Tip: You can save an existing numpy array to Zarr via: ``zarr.convenience.save_array(YOURFILE.zarr, your_array)``,\\n      or to .npy file that can be loaded with mmap via: ``np.save(YOURFILE.npy, your_array)``.\\n\\n    pred_probs: np.ndarray-like object, optional\\n      2D array of model-predicted class probabilities (floats) for each example in the dataset.\\n      To avoid loading big objects into memory, you should pass this as a memory-mapped object like:\\n      Zarr array loaded with ``zarr.convenience.open(YOURFILE.zarr, mode=\"r\")``\\n      or memmap array loaded with ``np.load(YOURFILE.npy, mmap_mode=\"r\")``.\\n\\n    labels_file: str, optional\\n      Specify this instead of `labels` if you want this method to load from file for you into a memmap array.\\n      Path to .npy file where the entire 1D `labels` numpy array is stored on disk (list format is not supported).\\n      This is loaded using: ``np.load(labels_file, mmap_mode=\"r\")``\\n      so make sure this file was created via: ``np.save()`` or other compatible methods (.npz not supported).\\n\\n    pred_probs_file: str, optional\\n      Specify this instead of `pred_probs` if you want this method to load from file for you into a memmap array.\\n      Path to .npy file where the entire `pred_probs` numpy array is stored on disk.\\n      This is loaded using: ``np.load(pred_probs_file, mmap_mode=\"r\")``\\n      so make sure this file was created via: ``np.save()`` or other compatible methods (.npz not supported).\\n\\n    batch_size : int, optional\\n      Size of mini-batches to use for estimating the label issues.\\n      To maximize efficiency, try to use the largest `batch_size` your memory allows.\\n\\n    n_jobs: int, optional\\n      Number of processes for multiprocessing (default value = 1). Only used on Linux.\\n      If `n_jobs=None`, will use either the number of: physical cores if psutil is installed, or logical cores otherwise.\\n\\n    verbose : bool, optional\\n      Whether to suppress print statements or not.\\n\\n    quality_score_kwargs : dict, optional\\n      Keyword arguments to pass into :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n    num_issue_kwargs : dict, optional\\n      Keyword arguments to :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`\\n      to control estimation of the number of label issues.\\n      The only supported kwarg here for now is: `estimation_method`.\\n    return_mask : bool, optional\\n       Determines what is returned by this method: If `return_mask=True`, return a boolean mask.\\n       If `False`, return a list of indices specifying examples with label issues, sorted by label quality score.\\n\\n    Returns\\n    -------\\n    label_issues : np.ndarray\\n      If `return_mask` is `True`, returns a boolean **mask** for the entire dataset\\n      where ``True`` represents a label issue and ``False`` represents an example that is\\n      accurately labeled with high confidence.\\n      If `return_mask` is `False`, returns an array containing **indices** of examples identified to have\\n      label issues (i.e. those indices where the mask would be ``True``), sorted by likelihood that the corresponding label is correct.\\n    --------\\n    >>> batch_size = 10000  # for efficiency, set this to as large of a value as your memory can handle\\n    >>> # Just demonstrating how to save your existing numpy labels, pred_probs arrays to compatible .npy files:\\n    >>> np.save(\"LABELS.npy\", labels_array)\\n    >>> np.save(\"PREDPROBS.npy\", pred_probs_array)\\n    >>> # You can load these back into memmap arrays via: labels = np.load(\"LABELS.npy\", mmap_mode=\"r\")\\n    >>> # and then run this method on the memmap arrays, or just run it directly on the .npy files like this:\\n    >>> issues = find_label_issues_batched(labels_file=\"LABELS.npy\", pred_probs_file=\"PREDPROBS.npy\", batch_size=batch_size)\\n    >>> # This method also works with Zarr arrays:\\n    >>> import zarr\\n    >>> # Just demonstrating how to save your existing numpy labels, pred_probs arrays to compatible .zarr files:\\n    >>> zarr.convenience.save_array(\"LABELS.zarr\", labels_array)\\n    >>> zarr.convenience.save_array(\"PREDPROBS.zarr\", pred_probs_array)\\n    >>> # You can load from such files into Zarr arrays:\\n    >>> labels = zarr.convenience.open(\"LABELS.zarr\", mode=\"r\")\\n    >>> pred_probs = zarr.convenience.open(\"PREDPROBS.zarr\", mode=\"r\")\\n    >>> # This method can be directly run on Zarr arrays, memmap arrays, or regular numpy arrays:\\n    >>> issues = find_label_issues_batched(labels=labels, pred_probs=pred_probs, batch_size=batch_size)\\n    '\n    if labels_file is not None:\n        if labels is not None:\n            raise ValueError('only specify one of: `labels` or `labels_file`')\n        if not isinstance(labels_file, str):\n            raise ValueError('labels_file must be str specifying path to .npy file containing the array of labels')\n        labels = np.load(labels_file, mmap_mode='r')\n        assert isinstance(labels, np.ndarray)\n    if pred_probs_file is not None:\n        if pred_probs is not None:\n            raise ValueError('only specify one of: `pred_probs` or `pred_probs_file`')\n        if not isinstance(pred_probs_file, str):\n            raise ValueError('pred_probs_file must be str specifying path to .npy file containing 2D array of pred_probs')\n        pred_probs = np.load(pred_probs_file, mmap_mode='r')\n        assert isinstance(pred_probs, np.ndarray)\n        if verbose:\n            print(f'mmap-loaded numpy arrays have: {len(pred_probs)} examples, {pred_probs.shape[1]} classes')\n    if labels is None:\n        raise ValueError('must provide one of: `labels` or `labels_file`')\n    if pred_probs is None:\n        raise ValueError('must provide one of: `pred_probs` or `pred_probs_file`')\n    assert pred_probs is not None\n    if len(labels) != len(pred_probs):\n        raise ValueError(f'len(labels)={len(labels)} does not match len(pred_probs)={len(pred_probs)}. Perhaps an issue loading mmap numpy arrays from file.')\n    lab = LabelInspector(num_class=pred_probs.shape[1], verbose=verbose, n_jobs=n_jobs, quality_score_kwargs=quality_score_kwargs, num_issue_kwargs=num_issue_kwargs)\n    n = len(labels)\n    if verbose:\n        from tqdm.auto import tqdm\n        pbar = tqdm(desc='number of examples processed for estimating thresholds', total=n)\n    i = 0\n    while i < n:\n        end_index = i + batch_size\n        labels_batch = labels[i:end_index]\n        pred_probs_batch = pred_probs[i:end_index, :]\n        i = end_index\n        lab.update_confident_thresholds(labels_batch, pred_probs_batch)\n        if verbose:\n            pbar.update(batch_size)\n    if verbose:\n        pbar.close()\n        pbar = tqdm(desc='number of examples processed for checking labels', total=n)\n    i = 0\n    while i < n:\n        end_index = i + batch_size\n        labels_batch = labels[i:end_index]\n        pred_probs_batch = pred_probs[i:end_index, :]\n        i = end_index\n        _ = lab.score_label_quality(labels_batch, pred_probs_batch)\n        if verbose:\n            pbar.update(batch_size)\n    if verbose:\n        pbar.close()\n    label_issues_indices = lab.get_label_issues()\n    if return_mask:\n        label_issues_mask = np.zeros(len(labels), dtype=bool)\n        label_issues_mask[label_issues_indices] = True\n        return label_issues_mask\n    return label_issues_indices",
            "def find_label_issues_batched(labels: Optional[LabelLike]=None, pred_probs: Optional[np.ndarray]=None, *, labels_file: Optional[str]=None, pred_probs_file: Optional[str]=None, batch_size: int=10000, n_jobs: Optional[int]=1, verbose: bool=True, quality_score_kwargs: Optional[dict]=None, num_issue_kwargs: Optional[dict]=None, return_mask: bool=False) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Variant of :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n    that requires less memory by reading from `pred_probs`, `labels` in mini-batches.\\n    To avoid loading big `pred_probs`, `labels` arrays into memory,\\n    provide these as memory-mapped objects like Zarr arrays or memmap arrays instead of regular numpy arrays.\\n    See: https://pythonspeed.com/articles/mmap-vs-zarr-hdf5/\\n\\n    With default settings, the results returned from this method closely approximate those returned from:\\n    ``cleanlab.filter.find_label_issues(..., filter_by=\"low_self_confidence\", return_indices_ranked_by=\"self_confidence\")``\\n\\n    This function internally implements the example usage script of the ``LabelInspector`` class,\\n    but you can further customize that script by running it yourself instead of this function.\\n    See the documentation of ``LabelInspector`` to learn more about how this method works internally.\\n\\n    Parameters\\n    ----------\\n    labels: np.ndarray-like object, optional\\n      1D array of given class labels for each example in the dataset, (int) values in ``0,1,2,...,K-1``.\\n      To avoid loading big objects into memory, you should pass this as a memory-mapped object like:\\n      Zarr array loaded with ``zarr.convenience.open(YOURFILE.zarr, mode=\"r\")``,\\n      or memmap array loaded with ``np.load(YOURFILE.npy, mmap_mode=\"r\")``.\\n\\n      Tip: You can save an existing numpy array to Zarr via: ``zarr.convenience.save_array(YOURFILE.zarr, your_array)``,\\n      or to .npy file that can be loaded with mmap via: ``np.save(YOURFILE.npy, your_array)``.\\n\\n    pred_probs: np.ndarray-like object, optional\\n      2D array of model-predicted class probabilities (floats) for each example in the dataset.\\n      To avoid loading big objects into memory, you should pass this as a memory-mapped object like:\\n      Zarr array loaded with ``zarr.convenience.open(YOURFILE.zarr, mode=\"r\")``\\n      or memmap array loaded with ``np.load(YOURFILE.npy, mmap_mode=\"r\")``.\\n\\n    labels_file: str, optional\\n      Specify this instead of `labels` if you want this method to load from file for you into a memmap array.\\n      Path to .npy file where the entire 1D `labels` numpy array is stored on disk (list format is not supported).\\n      This is loaded using: ``np.load(labels_file, mmap_mode=\"r\")``\\n      so make sure this file was created via: ``np.save()`` or other compatible methods (.npz not supported).\\n\\n    pred_probs_file: str, optional\\n      Specify this instead of `pred_probs` if you want this method to load from file for you into a memmap array.\\n      Path to .npy file where the entire `pred_probs` numpy array is stored on disk.\\n      This is loaded using: ``np.load(pred_probs_file, mmap_mode=\"r\")``\\n      so make sure this file was created via: ``np.save()`` or other compatible methods (.npz not supported).\\n\\n    batch_size : int, optional\\n      Size of mini-batches to use for estimating the label issues.\\n      To maximize efficiency, try to use the largest `batch_size` your memory allows.\\n\\n    n_jobs: int, optional\\n      Number of processes for multiprocessing (default value = 1). Only used on Linux.\\n      If `n_jobs=None`, will use either the number of: physical cores if psutil is installed, or logical cores otherwise.\\n\\n    verbose : bool, optional\\n      Whether to suppress print statements or not.\\n\\n    quality_score_kwargs : dict, optional\\n      Keyword arguments to pass into :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n    num_issue_kwargs : dict, optional\\n      Keyword arguments to :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`\\n      to control estimation of the number of label issues.\\n      The only supported kwarg here for now is: `estimation_method`.\\n    return_mask : bool, optional\\n       Determines what is returned by this method: If `return_mask=True`, return a boolean mask.\\n       If `False`, return a list of indices specifying examples with label issues, sorted by label quality score.\\n\\n    Returns\\n    -------\\n    label_issues : np.ndarray\\n      If `return_mask` is `True`, returns a boolean **mask** for the entire dataset\\n      where ``True`` represents a label issue and ``False`` represents an example that is\\n      accurately labeled with high confidence.\\n      If `return_mask` is `False`, returns an array containing **indices** of examples identified to have\\n      label issues (i.e. those indices where the mask would be ``True``), sorted by likelihood that the corresponding label is correct.\\n    --------\\n    >>> batch_size = 10000  # for efficiency, set this to as large of a value as your memory can handle\\n    >>> # Just demonstrating how to save your existing numpy labels, pred_probs arrays to compatible .npy files:\\n    >>> np.save(\"LABELS.npy\", labels_array)\\n    >>> np.save(\"PREDPROBS.npy\", pred_probs_array)\\n    >>> # You can load these back into memmap arrays via: labels = np.load(\"LABELS.npy\", mmap_mode=\"r\")\\n    >>> # and then run this method on the memmap arrays, or just run it directly on the .npy files like this:\\n    >>> issues = find_label_issues_batched(labels_file=\"LABELS.npy\", pred_probs_file=\"PREDPROBS.npy\", batch_size=batch_size)\\n    >>> # This method also works with Zarr arrays:\\n    >>> import zarr\\n    >>> # Just demonstrating how to save your existing numpy labels, pred_probs arrays to compatible .zarr files:\\n    >>> zarr.convenience.save_array(\"LABELS.zarr\", labels_array)\\n    >>> zarr.convenience.save_array(\"PREDPROBS.zarr\", pred_probs_array)\\n    >>> # You can load from such files into Zarr arrays:\\n    >>> labels = zarr.convenience.open(\"LABELS.zarr\", mode=\"r\")\\n    >>> pred_probs = zarr.convenience.open(\"PREDPROBS.zarr\", mode=\"r\")\\n    >>> # This method can be directly run on Zarr arrays, memmap arrays, or regular numpy arrays:\\n    >>> issues = find_label_issues_batched(labels=labels, pred_probs=pred_probs, batch_size=batch_size)\\n    '\n    if labels_file is not None:\n        if labels is not None:\n            raise ValueError('only specify one of: `labels` or `labels_file`')\n        if not isinstance(labels_file, str):\n            raise ValueError('labels_file must be str specifying path to .npy file containing the array of labels')\n        labels = np.load(labels_file, mmap_mode='r')\n        assert isinstance(labels, np.ndarray)\n    if pred_probs_file is not None:\n        if pred_probs is not None:\n            raise ValueError('only specify one of: `pred_probs` or `pred_probs_file`')\n        if not isinstance(pred_probs_file, str):\n            raise ValueError('pred_probs_file must be str specifying path to .npy file containing 2D array of pred_probs')\n        pred_probs = np.load(pred_probs_file, mmap_mode='r')\n        assert isinstance(pred_probs, np.ndarray)\n        if verbose:\n            print(f'mmap-loaded numpy arrays have: {len(pred_probs)} examples, {pred_probs.shape[1]} classes')\n    if labels is None:\n        raise ValueError('must provide one of: `labels` or `labels_file`')\n    if pred_probs is None:\n        raise ValueError('must provide one of: `pred_probs` or `pred_probs_file`')\n    assert pred_probs is not None\n    if len(labels) != len(pred_probs):\n        raise ValueError(f'len(labels)={len(labels)} does not match len(pred_probs)={len(pred_probs)}. Perhaps an issue loading mmap numpy arrays from file.')\n    lab = LabelInspector(num_class=pred_probs.shape[1], verbose=verbose, n_jobs=n_jobs, quality_score_kwargs=quality_score_kwargs, num_issue_kwargs=num_issue_kwargs)\n    n = len(labels)\n    if verbose:\n        from tqdm.auto import tqdm\n        pbar = tqdm(desc='number of examples processed for estimating thresholds', total=n)\n    i = 0\n    while i < n:\n        end_index = i + batch_size\n        labels_batch = labels[i:end_index]\n        pred_probs_batch = pred_probs[i:end_index, :]\n        i = end_index\n        lab.update_confident_thresholds(labels_batch, pred_probs_batch)\n        if verbose:\n            pbar.update(batch_size)\n    if verbose:\n        pbar.close()\n        pbar = tqdm(desc='number of examples processed for checking labels', total=n)\n    i = 0\n    while i < n:\n        end_index = i + batch_size\n        labels_batch = labels[i:end_index]\n        pred_probs_batch = pred_probs[i:end_index, :]\n        i = end_index\n        _ = lab.score_label_quality(labels_batch, pred_probs_batch)\n        if verbose:\n            pbar.update(batch_size)\n    if verbose:\n        pbar.close()\n    label_issues_indices = lab.get_label_issues()\n    if return_mask:\n        label_issues_mask = np.zeros(len(labels), dtype=bool)\n        label_issues_mask[label_issues_indices] = True\n        return label_issues_mask\n    return label_issues_indices"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, *, num_class: int, store_results: bool=True, verbose: bool=True, quality_score_kwargs: Optional[dict]=None, num_issue_kwargs: Optional[dict]=None, n_jobs: Optional[int]=1):\n    if quality_score_kwargs is None:\n        quality_score_kwargs = {}\n    if num_issue_kwargs is None:\n        num_issue_kwargs = {}\n    self.num_class = num_class\n    self.store_results = store_results\n    self.verbose = verbose\n    self.quality_score_kwargs = quality_score_kwargs\n    self.num_issue_kwargs = num_issue_kwargs\n    self.off_diagonal_calibrated = False\n    if num_issue_kwargs.get('estimation_method') == 'off_diagonal_calibrated':\n        self.off_diagonal_calibrated = True\n        self.prune_counts = np.zeros(self.num_class)\n        self.class_counts = np.zeros(self.num_class)\n        self.normalization = np.zeros(self.num_class)\n    else:\n        self.prune_count = 0\n    if self.store_results:\n        self.label_quality_scores: List[float] = []\n    self.confident_thresholds = np.zeros((num_class,))\n    self.examples_per_class = np.zeros((num_class,))\n    self.examples_processed_thresh = 0\n    self.examples_processed_quality = 0\n    self.n_jobs: Optional[int] = None\n    os_name = platform.system()\n    if os_name != 'Linux':\n        self.n_jobs = 1\n        if n_jobs is not None and n_jobs != 1 and self.verbose:\n            print('n_jobs is overridden to 1 because multiprocessing is only supported for Linux.')\n    elif n_jobs is not None:\n        self.n_jobs = n_jobs\n    else:\n        if PSUTIL_EXISTS:\n            self.n_jobs = psutil.cpu_count(logical=False)\n        if not self.n_jobs:\n            self.n_jobs = mp.cpu_count()\n            if self.verbose:\n                print(f'Multiprocessing will default to using the number of logical cores ({self.n_jobs}). To default to number of physical cores: pip install psutil')",
        "mutated": [
            "def __init__(self, *, num_class: int, store_results: bool=True, verbose: bool=True, quality_score_kwargs: Optional[dict]=None, num_issue_kwargs: Optional[dict]=None, n_jobs: Optional[int]=1):\n    if False:\n        i = 10\n    if quality_score_kwargs is None:\n        quality_score_kwargs = {}\n    if num_issue_kwargs is None:\n        num_issue_kwargs = {}\n    self.num_class = num_class\n    self.store_results = store_results\n    self.verbose = verbose\n    self.quality_score_kwargs = quality_score_kwargs\n    self.num_issue_kwargs = num_issue_kwargs\n    self.off_diagonal_calibrated = False\n    if num_issue_kwargs.get('estimation_method') == 'off_diagonal_calibrated':\n        self.off_diagonal_calibrated = True\n        self.prune_counts = np.zeros(self.num_class)\n        self.class_counts = np.zeros(self.num_class)\n        self.normalization = np.zeros(self.num_class)\n    else:\n        self.prune_count = 0\n    if self.store_results:\n        self.label_quality_scores: List[float] = []\n    self.confident_thresholds = np.zeros((num_class,))\n    self.examples_per_class = np.zeros((num_class,))\n    self.examples_processed_thresh = 0\n    self.examples_processed_quality = 0\n    self.n_jobs: Optional[int] = None\n    os_name = platform.system()\n    if os_name != 'Linux':\n        self.n_jobs = 1\n        if n_jobs is not None and n_jobs != 1 and self.verbose:\n            print('n_jobs is overridden to 1 because multiprocessing is only supported for Linux.')\n    elif n_jobs is not None:\n        self.n_jobs = n_jobs\n    else:\n        if PSUTIL_EXISTS:\n            self.n_jobs = psutil.cpu_count(logical=False)\n        if not self.n_jobs:\n            self.n_jobs = mp.cpu_count()\n            if self.verbose:\n                print(f'Multiprocessing will default to using the number of logical cores ({self.n_jobs}). To default to number of physical cores: pip install psutil')",
            "def __init__(self, *, num_class: int, store_results: bool=True, verbose: bool=True, quality_score_kwargs: Optional[dict]=None, num_issue_kwargs: Optional[dict]=None, n_jobs: Optional[int]=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if quality_score_kwargs is None:\n        quality_score_kwargs = {}\n    if num_issue_kwargs is None:\n        num_issue_kwargs = {}\n    self.num_class = num_class\n    self.store_results = store_results\n    self.verbose = verbose\n    self.quality_score_kwargs = quality_score_kwargs\n    self.num_issue_kwargs = num_issue_kwargs\n    self.off_diagonal_calibrated = False\n    if num_issue_kwargs.get('estimation_method') == 'off_diagonal_calibrated':\n        self.off_diagonal_calibrated = True\n        self.prune_counts = np.zeros(self.num_class)\n        self.class_counts = np.zeros(self.num_class)\n        self.normalization = np.zeros(self.num_class)\n    else:\n        self.prune_count = 0\n    if self.store_results:\n        self.label_quality_scores: List[float] = []\n    self.confident_thresholds = np.zeros((num_class,))\n    self.examples_per_class = np.zeros((num_class,))\n    self.examples_processed_thresh = 0\n    self.examples_processed_quality = 0\n    self.n_jobs: Optional[int] = None\n    os_name = platform.system()\n    if os_name != 'Linux':\n        self.n_jobs = 1\n        if n_jobs is not None and n_jobs != 1 and self.verbose:\n            print('n_jobs is overridden to 1 because multiprocessing is only supported for Linux.')\n    elif n_jobs is not None:\n        self.n_jobs = n_jobs\n    else:\n        if PSUTIL_EXISTS:\n            self.n_jobs = psutil.cpu_count(logical=False)\n        if not self.n_jobs:\n            self.n_jobs = mp.cpu_count()\n            if self.verbose:\n                print(f'Multiprocessing will default to using the number of logical cores ({self.n_jobs}). To default to number of physical cores: pip install psutil')",
            "def __init__(self, *, num_class: int, store_results: bool=True, verbose: bool=True, quality_score_kwargs: Optional[dict]=None, num_issue_kwargs: Optional[dict]=None, n_jobs: Optional[int]=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if quality_score_kwargs is None:\n        quality_score_kwargs = {}\n    if num_issue_kwargs is None:\n        num_issue_kwargs = {}\n    self.num_class = num_class\n    self.store_results = store_results\n    self.verbose = verbose\n    self.quality_score_kwargs = quality_score_kwargs\n    self.num_issue_kwargs = num_issue_kwargs\n    self.off_diagonal_calibrated = False\n    if num_issue_kwargs.get('estimation_method') == 'off_diagonal_calibrated':\n        self.off_diagonal_calibrated = True\n        self.prune_counts = np.zeros(self.num_class)\n        self.class_counts = np.zeros(self.num_class)\n        self.normalization = np.zeros(self.num_class)\n    else:\n        self.prune_count = 0\n    if self.store_results:\n        self.label_quality_scores: List[float] = []\n    self.confident_thresholds = np.zeros((num_class,))\n    self.examples_per_class = np.zeros((num_class,))\n    self.examples_processed_thresh = 0\n    self.examples_processed_quality = 0\n    self.n_jobs: Optional[int] = None\n    os_name = platform.system()\n    if os_name != 'Linux':\n        self.n_jobs = 1\n        if n_jobs is not None and n_jobs != 1 and self.verbose:\n            print('n_jobs is overridden to 1 because multiprocessing is only supported for Linux.')\n    elif n_jobs is not None:\n        self.n_jobs = n_jobs\n    else:\n        if PSUTIL_EXISTS:\n            self.n_jobs = psutil.cpu_count(logical=False)\n        if not self.n_jobs:\n            self.n_jobs = mp.cpu_count()\n            if self.verbose:\n                print(f'Multiprocessing will default to using the number of logical cores ({self.n_jobs}). To default to number of physical cores: pip install psutil')",
            "def __init__(self, *, num_class: int, store_results: bool=True, verbose: bool=True, quality_score_kwargs: Optional[dict]=None, num_issue_kwargs: Optional[dict]=None, n_jobs: Optional[int]=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if quality_score_kwargs is None:\n        quality_score_kwargs = {}\n    if num_issue_kwargs is None:\n        num_issue_kwargs = {}\n    self.num_class = num_class\n    self.store_results = store_results\n    self.verbose = verbose\n    self.quality_score_kwargs = quality_score_kwargs\n    self.num_issue_kwargs = num_issue_kwargs\n    self.off_diagonal_calibrated = False\n    if num_issue_kwargs.get('estimation_method') == 'off_diagonal_calibrated':\n        self.off_diagonal_calibrated = True\n        self.prune_counts = np.zeros(self.num_class)\n        self.class_counts = np.zeros(self.num_class)\n        self.normalization = np.zeros(self.num_class)\n    else:\n        self.prune_count = 0\n    if self.store_results:\n        self.label_quality_scores: List[float] = []\n    self.confident_thresholds = np.zeros((num_class,))\n    self.examples_per_class = np.zeros((num_class,))\n    self.examples_processed_thresh = 0\n    self.examples_processed_quality = 0\n    self.n_jobs: Optional[int] = None\n    os_name = platform.system()\n    if os_name != 'Linux':\n        self.n_jobs = 1\n        if n_jobs is not None and n_jobs != 1 and self.verbose:\n            print('n_jobs is overridden to 1 because multiprocessing is only supported for Linux.')\n    elif n_jobs is not None:\n        self.n_jobs = n_jobs\n    else:\n        if PSUTIL_EXISTS:\n            self.n_jobs = psutil.cpu_count(logical=False)\n        if not self.n_jobs:\n            self.n_jobs = mp.cpu_count()\n            if self.verbose:\n                print(f'Multiprocessing will default to using the number of logical cores ({self.n_jobs}). To default to number of physical cores: pip install psutil')",
            "def __init__(self, *, num_class: int, store_results: bool=True, verbose: bool=True, quality_score_kwargs: Optional[dict]=None, num_issue_kwargs: Optional[dict]=None, n_jobs: Optional[int]=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if quality_score_kwargs is None:\n        quality_score_kwargs = {}\n    if num_issue_kwargs is None:\n        num_issue_kwargs = {}\n    self.num_class = num_class\n    self.store_results = store_results\n    self.verbose = verbose\n    self.quality_score_kwargs = quality_score_kwargs\n    self.num_issue_kwargs = num_issue_kwargs\n    self.off_diagonal_calibrated = False\n    if num_issue_kwargs.get('estimation_method') == 'off_diagonal_calibrated':\n        self.off_diagonal_calibrated = True\n        self.prune_counts = np.zeros(self.num_class)\n        self.class_counts = np.zeros(self.num_class)\n        self.normalization = np.zeros(self.num_class)\n    else:\n        self.prune_count = 0\n    if self.store_results:\n        self.label_quality_scores: List[float] = []\n    self.confident_thresholds = np.zeros((num_class,))\n    self.examples_per_class = np.zeros((num_class,))\n    self.examples_processed_thresh = 0\n    self.examples_processed_quality = 0\n    self.n_jobs: Optional[int] = None\n    os_name = platform.system()\n    if os_name != 'Linux':\n        self.n_jobs = 1\n        if n_jobs is not None and n_jobs != 1 and self.verbose:\n            print('n_jobs is overridden to 1 because multiprocessing is only supported for Linux.')\n    elif n_jobs is not None:\n        self.n_jobs = n_jobs\n    else:\n        if PSUTIL_EXISTS:\n            self.n_jobs = psutil.cpu_count(logical=False)\n        if not self.n_jobs:\n            self.n_jobs = mp.cpu_count()\n            if self.verbose:\n                print(f'Multiprocessing will default to using the number of logical cores ({self.n_jobs}). To default to number of physical cores: pip install psutil')"
        ]
    },
    {
        "func_name": "get_confident_thresholds",
        "original": "def get_confident_thresholds(self, silent: bool=False) -> np.ndarray:\n    \"\"\"\n        Fetches already-computed confident thresholds from the data seen so far\n        in same format as: :py:func:`count.get_confident_thresholds <cleanlab.count.get_confident_thresholds>`.\n\n\n        Returns\n        -------\n        confident_thresholds : np.ndarray\n          An array of shape ``(K, )`` where ``K`` is the number of classes.\n        \"\"\"\n    if self.examples_processed_thresh < 1:\n        raise ValueError('Have not computed any confident_thresholds yet. Call `update_confident_thresholds()` first.')\n    else:\n        if self.verbose and (not silent):\n            print(f'Total number of examples used to estimate confident thresholds: {self.examples_processed_thresh}')\n        return self.confident_thresholds",
        "mutated": [
            "def get_confident_thresholds(self, silent: bool=False) -> np.ndarray:\n    if False:\n        i = 10\n    '\\n        Fetches already-computed confident thresholds from the data seen so far\\n        in same format as: :py:func:`count.get_confident_thresholds <cleanlab.count.get_confident_thresholds>`.\\n\\n\\n        Returns\\n        -------\\n        confident_thresholds : np.ndarray\\n          An array of shape ``(K, )`` where ``K`` is the number of classes.\\n        '\n    if self.examples_processed_thresh < 1:\n        raise ValueError('Have not computed any confident_thresholds yet. Call `update_confident_thresholds()` first.')\n    else:\n        if self.verbose and (not silent):\n            print(f'Total number of examples used to estimate confident thresholds: {self.examples_processed_thresh}')\n        return self.confident_thresholds",
            "def get_confident_thresholds(self, silent: bool=False) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Fetches already-computed confident thresholds from the data seen so far\\n        in same format as: :py:func:`count.get_confident_thresholds <cleanlab.count.get_confident_thresholds>`.\\n\\n\\n        Returns\\n        -------\\n        confident_thresholds : np.ndarray\\n          An array of shape ``(K, )`` where ``K`` is the number of classes.\\n        '\n    if self.examples_processed_thresh < 1:\n        raise ValueError('Have not computed any confident_thresholds yet. Call `update_confident_thresholds()` first.')\n    else:\n        if self.verbose and (not silent):\n            print(f'Total number of examples used to estimate confident thresholds: {self.examples_processed_thresh}')\n        return self.confident_thresholds",
            "def get_confident_thresholds(self, silent: bool=False) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Fetches already-computed confident thresholds from the data seen so far\\n        in same format as: :py:func:`count.get_confident_thresholds <cleanlab.count.get_confident_thresholds>`.\\n\\n\\n        Returns\\n        -------\\n        confident_thresholds : np.ndarray\\n          An array of shape ``(K, )`` where ``K`` is the number of classes.\\n        '\n    if self.examples_processed_thresh < 1:\n        raise ValueError('Have not computed any confident_thresholds yet. Call `update_confident_thresholds()` first.')\n    else:\n        if self.verbose and (not silent):\n            print(f'Total number of examples used to estimate confident thresholds: {self.examples_processed_thresh}')\n        return self.confident_thresholds",
            "def get_confident_thresholds(self, silent: bool=False) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Fetches already-computed confident thresholds from the data seen so far\\n        in same format as: :py:func:`count.get_confident_thresholds <cleanlab.count.get_confident_thresholds>`.\\n\\n\\n        Returns\\n        -------\\n        confident_thresholds : np.ndarray\\n          An array of shape ``(K, )`` where ``K`` is the number of classes.\\n        '\n    if self.examples_processed_thresh < 1:\n        raise ValueError('Have not computed any confident_thresholds yet. Call `update_confident_thresholds()` first.')\n    else:\n        if self.verbose and (not silent):\n            print(f'Total number of examples used to estimate confident thresholds: {self.examples_processed_thresh}')\n        return self.confident_thresholds",
            "def get_confident_thresholds(self, silent: bool=False) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Fetches already-computed confident thresholds from the data seen so far\\n        in same format as: :py:func:`count.get_confident_thresholds <cleanlab.count.get_confident_thresholds>`.\\n\\n\\n        Returns\\n        -------\\n        confident_thresholds : np.ndarray\\n          An array of shape ``(K, )`` where ``K`` is the number of classes.\\n        '\n    if self.examples_processed_thresh < 1:\n        raise ValueError('Have not computed any confident_thresholds yet. Call `update_confident_thresholds()` first.')\n    else:\n        if self.verbose and (not silent):\n            print(f'Total number of examples used to estimate confident thresholds: {self.examples_processed_thresh}')\n        return self.confident_thresholds"
        ]
    },
    {
        "func_name": "get_num_issues",
        "original": "def get_num_issues(self, silent: bool=False) -> int:\n    \"\"\"\n        Fetches already-computed estimate of the number of label issues in the data seen so far\n        in the same format as: :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`.\n\n        Note: The estimated number of issues may differ from :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`\n        by 1 due to rounding differences.\n\n        Returns\n        -------\n        num_issues : int\n          The estimated number of examples with label issues in the data seen so far.\n        \"\"\"\n    if self.examples_processed_quality < 1:\n        raise ValueError('Have not evaluated any labels yet. Call `score_label_quality()` first.')\n    else:\n        if self.verbose and (not silent):\n            print(f'Total number of examples whose labels have been evaluated: {self.examples_processed_quality}')\n        if self.off_diagonal_calibrated:\n            calibrated_prune_counts = self.prune_counts * self.class_counts / np.clip(self.normalization, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n            return np.rint(np.sum(calibrated_prune_counts)).astype('int')\n        else:\n            return self.prune_count",
        "mutated": [
            "def get_num_issues(self, silent: bool=False) -> int:\n    if False:\n        i = 10\n    '\\n        Fetches already-computed estimate of the number of label issues in the data seen so far\\n        in the same format as: :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`.\\n\\n        Note: The estimated number of issues may differ from :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`\\n        by 1 due to rounding differences.\\n\\n        Returns\\n        -------\\n        num_issues : int\\n          The estimated number of examples with label issues in the data seen so far.\\n        '\n    if self.examples_processed_quality < 1:\n        raise ValueError('Have not evaluated any labels yet. Call `score_label_quality()` first.')\n    else:\n        if self.verbose and (not silent):\n            print(f'Total number of examples whose labels have been evaluated: {self.examples_processed_quality}')\n        if self.off_diagonal_calibrated:\n            calibrated_prune_counts = self.prune_counts * self.class_counts / np.clip(self.normalization, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n            return np.rint(np.sum(calibrated_prune_counts)).astype('int')\n        else:\n            return self.prune_count",
            "def get_num_issues(self, silent: bool=False) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Fetches already-computed estimate of the number of label issues in the data seen so far\\n        in the same format as: :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`.\\n\\n        Note: The estimated number of issues may differ from :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`\\n        by 1 due to rounding differences.\\n\\n        Returns\\n        -------\\n        num_issues : int\\n          The estimated number of examples with label issues in the data seen so far.\\n        '\n    if self.examples_processed_quality < 1:\n        raise ValueError('Have not evaluated any labels yet. Call `score_label_quality()` first.')\n    else:\n        if self.verbose and (not silent):\n            print(f'Total number of examples whose labels have been evaluated: {self.examples_processed_quality}')\n        if self.off_diagonal_calibrated:\n            calibrated_prune_counts = self.prune_counts * self.class_counts / np.clip(self.normalization, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n            return np.rint(np.sum(calibrated_prune_counts)).astype('int')\n        else:\n            return self.prune_count",
            "def get_num_issues(self, silent: bool=False) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Fetches already-computed estimate of the number of label issues in the data seen so far\\n        in the same format as: :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`.\\n\\n        Note: The estimated number of issues may differ from :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`\\n        by 1 due to rounding differences.\\n\\n        Returns\\n        -------\\n        num_issues : int\\n          The estimated number of examples with label issues in the data seen so far.\\n        '\n    if self.examples_processed_quality < 1:\n        raise ValueError('Have not evaluated any labels yet. Call `score_label_quality()` first.')\n    else:\n        if self.verbose and (not silent):\n            print(f'Total number of examples whose labels have been evaluated: {self.examples_processed_quality}')\n        if self.off_diagonal_calibrated:\n            calibrated_prune_counts = self.prune_counts * self.class_counts / np.clip(self.normalization, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n            return np.rint(np.sum(calibrated_prune_counts)).astype('int')\n        else:\n            return self.prune_count",
            "def get_num_issues(self, silent: bool=False) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Fetches already-computed estimate of the number of label issues in the data seen so far\\n        in the same format as: :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`.\\n\\n        Note: The estimated number of issues may differ from :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`\\n        by 1 due to rounding differences.\\n\\n        Returns\\n        -------\\n        num_issues : int\\n          The estimated number of examples with label issues in the data seen so far.\\n        '\n    if self.examples_processed_quality < 1:\n        raise ValueError('Have not evaluated any labels yet. Call `score_label_quality()` first.')\n    else:\n        if self.verbose and (not silent):\n            print(f'Total number of examples whose labels have been evaluated: {self.examples_processed_quality}')\n        if self.off_diagonal_calibrated:\n            calibrated_prune_counts = self.prune_counts * self.class_counts / np.clip(self.normalization, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n            return np.rint(np.sum(calibrated_prune_counts)).astype('int')\n        else:\n            return self.prune_count",
            "def get_num_issues(self, silent: bool=False) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Fetches already-computed estimate of the number of label issues in the data seen so far\\n        in the same format as: :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`.\\n\\n        Note: The estimated number of issues may differ from :py:func:`count.num_label_issues <cleanlab.count.num_label_issues>`\\n        by 1 due to rounding differences.\\n\\n        Returns\\n        -------\\n        num_issues : int\\n          The estimated number of examples with label issues in the data seen so far.\\n        '\n    if self.examples_processed_quality < 1:\n        raise ValueError('Have not evaluated any labels yet. Call `score_label_quality()` first.')\n    else:\n        if self.verbose and (not silent):\n            print(f'Total number of examples whose labels have been evaluated: {self.examples_processed_quality}')\n        if self.off_diagonal_calibrated:\n            calibrated_prune_counts = self.prune_counts * self.class_counts / np.clip(self.normalization, a_min=CLIPPING_LOWER_BOUND, a_max=None)\n            return np.rint(np.sum(calibrated_prune_counts)).astype('int')\n        else:\n            return self.prune_count"
        ]
    },
    {
        "func_name": "get_quality_scores",
        "original": "def get_quality_scores(self) -> np.ndarray:\n    \"\"\"\n        Fetches already-computed estimate of the label quality of each example seen so far\n        in the same format as: :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\n\n        Returns\n        -------\n        label_quality_scores : np.ndarray\n          Contains one score (between 0 and 1) per example seen so far.\n          Lower scores indicate more likely mislabeled examples.\n        \"\"\"\n    if not self.store_results:\n        raise ValueError('Must initialize the LabelInspector with `store_results` == True. Otherwise you can assemble the label quality scores yourself based on the scores returned for each batch of data from `score_label_quality()`')\n    else:\n        return np.asarray(self.label_quality_scores)",
        "mutated": [
            "def get_quality_scores(self) -> np.ndarray:\n    if False:\n        i = 10\n    '\\n        Fetches already-computed estimate of the label quality of each example seen so far\\n        in the same format as: :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n        Returns\\n        -------\\n        label_quality_scores : np.ndarray\\n          Contains one score (between 0 and 1) per example seen so far.\\n          Lower scores indicate more likely mislabeled examples.\\n        '\n    if not self.store_results:\n        raise ValueError('Must initialize the LabelInspector with `store_results` == True. Otherwise you can assemble the label quality scores yourself based on the scores returned for each batch of data from `score_label_quality()`')\n    else:\n        return np.asarray(self.label_quality_scores)",
            "def get_quality_scores(self) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Fetches already-computed estimate of the label quality of each example seen so far\\n        in the same format as: :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n        Returns\\n        -------\\n        label_quality_scores : np.ndarray\\n          Contains one score (between 0 and 1) per example seen so far.\\n          Lower scores indicate more likely mislabeled examples.\\n        '\n    if not self.store_results:\n        raise ValueError('Must initialize the LabelInspector with `store_results` == True. Otherwise you can assemble the label quality scores yourself based on the scores returned for each batch of data from `score_label_quality()`')\n    else:\n        return np.asarray(self.label_quality_scores)",
            "def get_quality_scores(self) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Fetches already-computed estimate of the label quality of each example seen so far\\n        in the same format as: :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n        Returns\\n        -------\\n        label_quality_scores : np.ndarray\\n          Contains one score (between 0 and 1) per example seen so far.\\n          Lower scores indicate more likely mislabeled examples.\\n        '\n    if not self.store_results:\n        raise ValueError('Must initialize the LabelInspector with `store_results` == True. Otherwise you can assemble the label quality scores yourself based on the scores returned for each batch of data from `score_label_quality()`')\n    else:\n        return np.asarray(self.label_quality_scores)",
            "def get_quality_scores(self) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Fetches already-computed estimate of the label quality of each example seen so far\\n        in the same format as: :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n        Returns\\n        -------\\n        label_quality_scores : np.ndarray\\n          Contains one score (between 0 and 1) per example seen so far.\\n          Lower scores indicate more likely mislabeled examples.\\n        '\n    if not self.store_results:\n        raise ValueError('Must initialize the LabelInspector with `store_results` == True. Otherwise you can assemble the label quality scores yourself based on the scores returned for each batch of data from `score_label_quality()`')\n    else:\n        return np.asarray(self.label_quality_scores)",
            "def get_quality_scores(self) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Fetches already-computed estimate of the label quality of each example seen so far\\n        in the same format as: :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n        Returns\\n        -------\\n        label_quality_scores : np.ndarray\\n          Contains one score (between 0 and 1) per example seen so far.\\n          Lower scores indicate more likely mislabeled examples.\\n        '\n    if not self.store_results:\n        raise ValueError('Must initialize the LabelInspector with `store_results` == True. Otherwise you can assemble the label quality scores yourself based on the scores returned for each batch of data from `score_label_quality()`')\n    else:\n        return np.asarray(self.label_quality_scores)"
        ]
    },
    {
        "func_name": "get_label_issues",
        "original": "def get_label_issues(self) -> np.ndarray:\n    \"\"\"\n        Fetches already-computed estimate of indices of examples with label issues in the data seen so far,\n        in the same format as: :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\n        with its `return_indices_ranked_by` argument specified.\n\n        Note: this method corresponds to ``filter.find_label_issues(..., filter_by=METHOD1, return_indices_ranked_by=METHOD2)``\n        where by default: ``METHOD1=\"low_self_confidence\"``, ``METHOD2=\"self_confidence\"``\n        or if this object was instantiated with ``quality_score_kwargs = {\"method\": \"normalized_margin\"}`` then we instead have:\n        ``METHOD1=\"low_normalized_margin\"``, ``METHOD2=\"normalized_margin\"``.\n\n        Note: The estimated number of issues may differ from :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\n        by 1 due to rounding differences.\n\n        Returns\n        -------\n        issue_indices : np.ndarray\n          Indices of examples with label issues, sorted by label quality score.\n        \"\"\"\n    if not self.store_results:\n        raise ValueError('Must initialize the LabelInspector with `store_results` == True. Otherwise you can identify label issues yourself based on the scores from all the batches of data and the total number of issues returned by `get_num_issues()`')\n    if self.examples_processed_quality < 1:\n        raise ValueError('Have not evaluated any labels yet. Call `score_label_quality()` first.')\n    if self.verbose:\n        print(f'Total number of examples whose labels have been evaluated: {self.examples_processed_quality}')\n    return find_top_issues(self.get_quality_scores(), top=self.get_num_issues(silent=True))",
        "mutated": [
            "def get_label_issues(self) -> np.ndarray:\n    if False:\n        i = 10\n    '\\n        Fetches already-computed estimate of indices of examples with label issues in the data seen so far,\\n        in the same format as: :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n        with its `return_indices_ranked_by` argument specified.\\n\\n        Note: this method corresponds to ``filter.find_label_issues(..., filter_by=METHOD1, return_indices_ranked_by=METHOD2)``\\n        where by default: ``METHOD1=\"low_self_confidence\"``, ``METHOD2=\"self_confidence\"``\\n        or if this object was instantiated with ``quality_score_kwargs = {\"method\": \"normalized_margin\"}`` then we instead have:\\n        ``METHOD1=\"low_normalized_margin\"``, ``METHOD2=\"normalized_margin\"``.\\n\\n        Note: The estimated number of issues may differ from :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n        by 1 due to rounding differences.\\n\\n        Returns\\n        -------\\n        issue_indices : np.ndarray\\n          Indices of examples with label issues, sorted by label quality score.\\n        '\n    if not self.store_results:\n        raise ValueError('Must initialize the LabelInspector with `store_results` == True. Otherwise you can identify label issues yourself based on the scores from all the batches of data and the total number of issues returned by `get_num_issues()`')\n    if self.examples_processed_quality < 1:\n        raise ValueError('Have not evaluated any labels yet. Call `score_label_quality()` first.')\n    if self.verbose:\n        print(f'Total number of examples whose labels have been evaluated: {self.examples_processed_quality}')\n    return find_top_issues(self.get_quality_scores(), top=self.get_num_issues(silent=True))",
            "def get_label_issues(self) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Fetches already-computed estimate of indices of examples with label issues in the data seen so far,\\n        in the same format as: :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n        with its `return_indices_ranked_by` argument specified.\\n\\n        Note: this method corresponds to ``filter.find_label_issues(..., filter_by=METHOD1, return_indices_ranked_by=METHOD2)``\\n        where by default: ``METHOD1=\"low_self_confidence\"``, ``METHOD2=\"self_confidence\"``\\n        or if this object was instantiated with ``quality_score_kwargs = {\"method\": \"normalized_margin\"}`` then we instead have:\\n        ``METHOD1=\"low_normalized_margin\"``, ``METHOD2=\"normalized_margin\"``.\\n\\n        Note: The estimated number of issues may differ from :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n        by 1 due to rounding differences.\\n\\n        Returns\\n        -------\\n        issue_indices : np.ndarray\\n          Indices of examples with label issues, sorted by label quality score.\\n        '\n    if not self.store_results:\n        raise ValueError('Must initialize the LabelInspector with `store_results` == True. Otherwise you can identify label issues yourself based on the scores from all the batches of data and the total number of issues returned by `get_num_issues()`')\n    if self.examples_processed_quality < 1:\n        raise ValueError('Have not evaluated any labels yet. Call `score_label_quality()` first.')\n    if self.verbose:\n        print(f'Total number of examples whose labels have been evaluated: {self.examples_processed_quality}')\n    return find_top_issues(self.get_quality_scores(), top=self.get_num_issues(silent=True))",
            "def get_label_issues(self) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Fetches already-computed estimate of indices of examples with label issues in the data seen so far,\\n        in the same format as: :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n        with its `return_indices_ranked_by` argument specified.\\n\\n        Note: this method corresponds to ``filter.find_label_issues(..., filter_by=METHOD1, return_indices_ranked_by=METHOD2)``\\n        where by default: ``METHOD1=\"low_self_confidence\"``, ``METHOD2=\"self_confidence\"``\\n        or if this object was instantiated with ``quality_score_kwargs = {\"method\": \"normalized_margin\"}`` then we instead have:\\n        ``METHOD1=\"low_normalized_margin\"``, ``METHOD2=\"normalized_margin\"``.\\n\\n        Note: The estimated number of issues may differ from :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n        by 1 due to rounding differences.\\n\\n        Returns\\n        -------\\n        issue_indices : np.ndarray\\n          Indices of examples with label issues, sorted by label quality score.\\n        '\n    if not self.store_results:\n        raise ValueError('Must initialize the LabelInspector with `store_results` == True. Otherwise you can identify label issues yourself based on the scores from all the batches of data and the total number of issues returned by `get_num_issues()`')\n    if self.examples_processed_quality < 1:\n        raise ValueError('Have not evaluated any labels yet. Call `score_label_quality()` first.')\n    if self.verbose:\n        print(f'Total number of examples whose labels have been evaluated: {self.examples_processed_quality}')\n    return find_top_issues(self.get_quality_scores(), top=self.get_num_issues(silent=True))",
            "def get_label_issues(self) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Fetches already-computed estimate of indices of examples with label issues in the data seen so far,\\n        in the same format as: :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n        with its `return_indices_ranked_by` argument specified.\\n\\n        Note: this method corresponds to ``filter.find_label_issues(..., filter_by=METHOD1, return_indices_ranked_by=METHOD2)``\\n        where by default: ``METHOD1=\"low_self_confidence\"``, ``METHOD2=\"self_confidence\"``\\n        or if this object was instantiated with ``quality_score_kwargs = {\"method\": \"normalized_margin\"}`` then we instead have:\\n        ``METHOD1=\"low_normalized_margin\"``, ``METHOD2=\"normalized_margin\"``.\\n\\n        Note: The estimated number of issues may differ from :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n        by 1 due to rounding differences.\\n\\n        Returns\\n        -------\\n        issue_indices : np.ndarray\\n          Indices of examples with label issues, sorted by label quality score.\\n        '\n    if not self.store_results:\n        raise ValueError('Must initialize the LabelInspector with `store_results` == True. Otherwise you can identify label issues yourself based on the scores from all the batches of data and the total number of issues returned by `get_num_issues()`')\n    if self.examples_processed_quality < 1:\n        raise ValueError('Have not evaluated any labels yet. Call `score_label_quality()` first.')\n    if self.verbose:\n        print(f'Total number of examples whose labels have been evaluated: {self.examples_processed_quality}')\n    return find_top_issues(self.get_quality_scores(), top=self.get_num_issues(silent=True))",
            "def get_label_issues(self) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Fetches already-computed estimate of indices of examples with label issues in the data seen so far,\\n        in the same format as: :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n        with its `return_indices_ranked_by` argument specified.\\n\\n        Note: this method corresponds to ``filter.find_label_issues(..., filter_by=METHOD1, return_indices_ranked_by=METHOD2)``\\n        where by default: ``METHOD1=\"low_self_confidence\"``, ``METHOD2=\"self_confidence\"``\\n        or if this object was instantiated with ``quality_score_kwargs = {\"method\": \"normalized_margin\"}`` then we instead have:\\n        ``METHOD1=\"low_normalized_margin\"``, ``METHOD2=\"normalized_margin\"``.\\n\\n        Note: The estimated number of issues may differ from :py:func:`filter.find_label_issues <cleanlab.filter.find_label_issues>`\\n        by 1 due to rounding differences.\\n\\n        Returns\\n        -------\\n        issue_indices : np.ndarray\\n          Indices of examples with label issues, sorted by label quality score.\\n        '\n    if not self.store_results:\n        raise ValueError('Must initialize the LabelInspector with `store_results` == True. Otherwise you can identify label issues yourself based on the scores from all the batches of data and the total number of issues returned by `get_num_issues()`')\n    if self.examples_processed_quality < 1:\n        raise ValueError('Have not evaluated any labels yet. Call `score_label_quality()` first.')\n    if self.verbose:\n        print(f'Total number of examples whose labels have been evaluated: {self.examples_processed_quality}')\n    return find_top_issues(self.get_quality_scores(), top=self.get_num_issues(silent=True))"
        ]
    },
    {
        "func_name": "update_confident_thresholds",
        "original": "def update_confident_thresholds(self, labels: LabelLike, pred_probs: np.ndarray):\n    \"\"\"\n        Updates the estimate of confident_thresholds stored in this class using a new batch of data.\n        Inputs should be in same format as for: :py:func:`count.get_confident_thresholds <cleanlab.count.get_confident_thresholds>`.\n\n        Parameters\n        ----------\n        labels: np.ndarray or list\n          Given class labels for each example in the batch, values in ``0,1,2,...,K-1``.\n\n        pred_probs: np.ndarray\n          2D array of model-predicted class probabilities for each example in the batch.\n        \"\"\"\n    labels = _batch_check(labels, pred_probs, self.num_class)\n    batch_size = len(labels)\n    batch_thresholds = get_confident_thresholds(labels, pred_probs)\n    batch_class_counts = value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n    self.confident_thresholds = (self.examples_per_class * self.confident_thresholds + batch_class_counts * batch_thresholds) / np.clip(self.examples_per_class + batch_class_counts, a_min=1, a_max=None)\n    self.confident_thresholds = np.clip(self.confident_thresholds, a_min=CONFIDENT_THRESHOLDS_LOWER_BOUND, a_max=None)\n    self.examples_per_class += batch_class_counts\n    self.examples_processed_thresh += batch_size",
        "mutated": [
            "def update_confident_thresholds(self, labels: LabelLike, pred_probs: np.ndarray):\n    if False:\n        i = 10\n    '\\n        Updates the estimate of confident_thresholds stored in this class using a new batch of data.\\n        Inputs should be in same format as for: :py:func:`count.get_confident_thresholds <cleanlab.count.get_confident_thresholds>`.\\n\\n        Parameters\\n        ----------\\n        labels: np.ndarray or list\\n          Given class labels for each example in the batch, values in ``0,1,2,...,K-1``.\\n\\n        pred_probs: np.ndarray\\n          2D array of model-predicted class probabilities for each example in the batch.\\n        '\n    labels = _batch_check(labels, pred_probs, self.num_class)\n    batch_size = len(labels)\n    batch_thresholds = get_confident_thresholds(labels, pred_probs)\n    batch_class_counts = value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n    self.confident_thresholds = (self.examples_per_class * self.confident_thresholds + batch_class_counts * batch_thresholds) / np.clip(self.examples_per_class + batch_class_counts, a_min=1, a_max=None)\n    self.confident_thresholds = np.clip(self.confident_thresholds, a_min=CONFIDENT_THRESHOLDS_LOWER_BOUND, a_max=None)\n    self.examples_per_class += batch_class_counts\n    self.examples_processed_thresh += batch_size",
            "def update_confident_thresholds(self, labels: LabelLike, pred_probs: np.ndarray):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Updates the estimate of confident_thresholds stored in this class using a new batch of data.\\n        Inputs should be in same format as for: :py:func:`count.get_confident_thresholds <cleanlab.count.get_confident_thresholds>`.\\n\\n        Parameters\\n        ----------\\n        labels: np.ndarray or list\\n          Given class labels for each example in the batch, values in ``0,1,2,...,K-1``.\\n\\n        pred_probs: np.ndarray\\n          2D array of model-predicted class probabilities for each example in the batch.\\n        '\n    labels = _batch_check(labels, pred_probs, self.num_class)\n    batch_size = len(labels)\n    batch_thresholds = get_confident_thresholds(labels, pred_probs)\n    batch_class_counts = value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n    self.confident_thresholds = (self.examples_per_class * self.confident_thresholds + batch_class_counts * batch_thresholds) / np.clip(self.examples_per_class + batch_class_counts, a_min=1, a_max=None)\n    self.confident_thresholds = np.clip(self.confident_thresholds, a_min=CONFIDENT_THRESHOLDS_LOWER_BOUND, a_max=None)\n    self.examples_per_class += batch_class_counts\n    self.examples_processed_thresh += batch_size",
            "def update_confident_thresholds(self, labels: LabelLike, pred_probs: np.ndarray):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Updates the estimate of confident_thresholds stored in this class using a new batch of data.\\n        Inputs should be in same format as for: :py:func:`count.get_confident_thresholds <cleanlab.count.get_confident_thresholds>`.\\n\\n        Parameters\\n        ----------\\n        labels: np.ndarray or list\\n          Given class labels for each example in the batch, values in ``0,1,2,...,K-1``.\\n\\n        pred_probs: np.ndarray\\n          2D array of model-predicted class probabilities for each example in the batch.\\n        '\n    labels = _batch_check(labels, pred_probs, self.num_class)\n    batch_size = len(labels)\n    batch_thresholds = get_confident_thresholds(labels, pred_probs)\n    batch_class_counts = value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n    self.confident_thresholds = (self.examples_per_class * self.confident_thresholds + batch_class_counts * batch_thresholds) / np.clip(self.examples_per_class + batch_class_counts, a_min=1, a_max=None)\n    self.confident_thresholds = np.clip(self.confident_thresholds, a_min=CONFIDENT_THRESHOLDS_LOWER_BOUND, a_max=None)\n    self.examples_per_class += batch_class_counts\n    self.examples_processed_thresh += batch_size",
            "def update_confident_thresholds(self, labels: LabelLike, pred_probs: np.ndarray):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Updates the estimate of confident_thresholds stored in this class using a new batch of data.\\n        Inputs should be in same format as for: :py:func:`count.get_confident_thresholds <cleanlab.count.get_confident_thresholds>`.\\n\\n        Parameters\\n        ----------\\n        labels: np.ndarray or list\\n          Given class labels for each example in the batch, values in ``0,1,2,...,K-1``.\\n\\n        pred_probs: np.ndarray\\n          2D array of model-predicted class probabilities for each example in the batch.\\n        '\n    labels = _batch_check(labels, pred_probs, self.num_class)\n    batch_size = len(labels)\n    batch_thresholds = get_confident_thresholds(labels, pred_probs)\n    batch_class_counts = value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n    self.confident_thresholds = (self.examples_per_class * self.confident_thresholds + batch_class_counts * batch_thresholds) / np.clip(self.examples_per_class + batch_class_counts, a_min=1, a_max=None)\n    self.confident_thresholds = np.clip(self.confident_thresholds, a_min=CONFIDENT_THRESHOLDS_LOWER_BOUND, a_max=None)\n    self.examples_per_class += batch_class_counts\n    self.examples_processed_thresh += batch_size",
            "def update_confident_thresholds(self, labels: LabelLike, pred_probs: np.ndarray):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Updates the estimate of confident_thresholds stored in this class using a new batch of data.\\n        Inputs should be in same format as for: :py:func:`count.get_confident_thresholds <cleanlab.count.get_confident_thresholds>`.\\n\\n        Parameters\\n        ----------\\n        labels: np.ndarray or list\\n          Given class labels for each example in the batch, values in ``0,1,2,...,K-1``.\\n\\n        pred_probs: np.ndarray\\n          2D array of model-predicted class probabilities for each example in the batch.\\n        '\n    labels = _batch_check(labels, pred_probs, self.num_class)\n    batch_size = len(labels)\n    batch_thresholds = get_confident_thresholds(labels, pred_probs)\n    batch_class_counts = value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n    self.confident_thresholds = (self.examples_per_class * self.confident_thresholds + batch_class_counts * batch_thresholds) / np.clip(self.examples_per_class + batch_class_counts, a_min=1, a_max=None)\n    self.confident_thresholds = np.clip(self.confident_thresholds, a_min=CONFIDENT_THRESHOLDS_LOWER_BOUND, a_max=None)\n    self.examples_per_class += batch_class_counts\n    self.examples_processed_thresh += batch_size"
        ]
    },
    {
        "func_name": "score_label_quality",
        "original": "def score_label_quality(self, labels: LabelLike, pred_probs: np.ndarray, *, update_num_issues: bool=True) -> np.ndarray:\n    \"\"\"\n        Scores the label quality of each example in the provided batch of data,\n        and also updates the number of label issues stored in this class.\n        Inputs should be in same format as for: :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\n\n        Parameters\n        ----------\n        labels: np.ndarray\n          Given class labels for each example in the batch, values in ``0,1,2,...,K-1``.\n\n        pred_probs: np.ndarray\n          2D array of model-predicted class probabilities for each example in the batch of data.\n\n        update_num_issues: bool, optional\n          Whether or not to update the number of label issues or only compute label quality scores.\n          For lower runtimes, set this to ``False`` if you only want to score label quality and not find label issues.\n\n        Returns\n        -------\n        label_quality_scores : np.ndarray\n          Contains one score (between 0 and 1) for each example in the batch of data.\n        \"\"\"\n    labels = _batch_check(labels, pred_probs, self.num_class)\n    batch_size = len(labels)\n    scores = _compute_label_quality_scores(labels, pred_probs, confident_thresholds=self.get_confident_thresholds(silent=True), **self.quality_score_kwargs)\n    class_counts = value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n    if update_num_issues:\n        self._update_num_label_issues(labels, pred_probs, **self.num_issue_kwargs)\n    self.examples_processed_quality += batch_size\n    if self.store_results:\n        self.label_quality_scores += list(scores)\n    return scores",
        "mutated": [
            "def score_label_quality(self, labels: LabelLike, pred_probs: np.ndarray, *, update_num_issues: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n    '\\n        Scores the label quality of each example in the provided batch of data,\\n        and also updates the number of label issues stored in this class.\\n        Inputs should be in same format as for: :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n        Parameters\\n        ----------\\n        labels: np.ndarray\\n          Given class labels for each example in the batch, values in ``0,1,2,...,K-1``.\\n\\n        pred_probs: np.ndarray\\n          2D array of model-predicted class probabilities for each example in the batch of data.\\n\\n        update_num_issues: bool, optional\\n          Whether or not to update the number of label issues or only compute label quality scores.\\n          For lower runtimes, set this to ``False`` if you only want to score label quality and not find label issues.\\n\\n        Returns\\n        -------\\n        label_quality_scores : np.ndarray\\n          Contains one score (between 0 and 1) for each example in the batch of data.\\n        '\n    labels = _batch_check(labels, pred_probs, self.num_class)\n    batch_size = len(labels)\n    scores = _compute_label_quality_scores(labels, pred_probs, confident_thresholds=self.get_confident_thresholds(silent=True), **self.quality_score_kwargs)\n    class_counts = value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n    if update_num_issues:\n        self._update_num_label_issues(labels, pred_probs, **self.num_issue_kwargs)\n    self.examples_processed_quality += batch_size\n    if self.store_results:\n        self.label_quality_scores += list(scores)\n    return scores",
            "def score_label_quality(self, labels: LabelLike, pred_probs: np.ndarray, *, update_num_issues: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Scores the label quality of each example in the provided batch of data,\\n        and also updates the number of label issues stored in this class.\\n        Inputs should be in same format as for: :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n        Parameters\\n        ----------\\n        labels: np.ndarray\\n          Given class labels for each example in the batch, values in ``0,1,2,...,K-1``.\\n\\n        pred_probs: np.ndarray\\n          2D array of model-predicted class probabilities for each example in the batch of data.\\n\\n        update_num_issues: bool, optional\\n          Whether or not to update the number of label issues or only compute label quality scores.\\n          For lower runtimes, set this to ``False`` if you only want to score label quality and not find label issues.\\n\\n        Returns\\n        -------\\n        label_quality_scores : np.ndarray\\n          Contains one score (between 0 and 1) for each example in the batch of data.\\n        '\n    labels = _batch_check(labels, pred_probs, self.num_class)\n    batch_size = len(labels)\n    scores = _compute_label_quality_scores(labels, pred_probs, confident_thresholds=self.get_confident_thresholds(silent=True), **self.quality_score_kwargs)\n    class_counts = value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n    if update_num_issues:\n        self._update_num_label_issues(labels, pred_probs, **self.num_issue_kwargs)\n    self.examples_processed_quality += batch_size\n    if self.store_results:\n        self.label_quality_scores += list(scores)\n    return scores",
            "def score_label_quality(self, labels: LabelLike, pred_probs: np.ndarray, *, update_num_issues: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Scores the label quality of each example in the provided batch of data,\\n        and also updates the number of label issues stored in this class.\\n        Inputs should be in same format as for: :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n        Parameters\\n        ----------\\n        labels: np.ndarray\\n          Given class labels for each example in the batch, values in ``0,1,2,...,K-1``.\\n\\n        pred_probs: np.ndarray\\n          2D array of model-predicted class probabilities for each example in the batch of data.\\n\\n        update_num_issues: bool, optional\\n          Whether or not to update the number of label issues or only compute label quality scores.\\n          For lower runtimes, set this to ``False`` if you only want to score label quality and not find label issues.\\n\\n        Returns\\n        -------\\n        label_quality_scores : np.ndarray\\n          Contains one score (between 0 and 1) for each example in the batch of data.\\n        '\n    labels = _batch_check(labels, pred_probs, self.num_class)\n    batch_size = len(labels)\n    scores = _compute_label_quality_scores(labels, pred_probs, confident_thresholds=self.get_confident_thresholds(silent=True), **self.quality_score_kwargs)\n    class_counts = value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n    if update_num_issues:\n        self._update_num_label_issues(labels, pred_probs, **self.num_issue_kwargs)\n    self.examples_processed_quality += batch_size\n    if self.store_results:\n        self.label_quality_scores += list(scores)\n    return scores",
            "def score_label_quality(self, labels: LabelLike, pred_probs: np.ndarray, *, update_num_issues: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Scores the label quality of each example in the provided batch of data,\\n        and also updates the number of label issues stored in this class.\\n        Inputs should be in same format as for: :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n        Parameters\\n        ----------\\n        labels: np.ndarray\\n          Given class labels for each example in the batch, values in ``0,1,2,...,K-1``.\\n\\n        pred_probs: np.ndarray\\n          2D array of model-predicted class probabilities for each example in the batch of data.\\n\\n        update_num_issues: bool, optional\\n          Whether or not to update the number of label issues or only compute label quality scores.\\n          For lower runtimes, set this to ``False`` if you only want to score label quality and not find label issues.\\n\\n        Returns\\n        -------\\n        label_quality_scores : np.ndarray\\n          Contains one score (between 0 and 1) for each example in the batch of data.\\n        '\n    labels = _batch_check(labels, pred_probs, self.num_class)\n    batch_size = len(labels)\n    scores = _compute_label_quality_scores(labels, pred_probs, confident_thresholds=self.get_confident_thresholds(silent=True), **self.quality_score_kwargs)\n    class_counts = value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n    if update_num_issues:\n        self._update_num_label_issues(labels, pred_probs, **self.num_issue_kwargs)\n    self.examples_processed_quality += batch_size\n    if self.store_results:\n        self.label_quality_scores += list(scores)\n    return scores",
            "def score_label_quality(self, labels: LabelLike, pred_probs: np.ndarray, *, update_num_issues: bool=True) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Scores the label quality of each example in the provided batch of data,\\n        and also updates the number of label issues stored in this class.\\n        Inputs should be in same format as for: :py:func:`rank.get_label_quality_scores <cleanlab.rank.get_label_quality_scores>`.\\n\\n        Parameters\\n        ----------\\n        labels: np.ndarray\\n          Given class labels for each example in the batch, values in ``0,1,2,...,K-1``.\\n\\n        pred_probs: np.ndarray\\n          2D array of model-predicted class probabilities for each example in the batch of data.\\n\\n        update_num_issues: bool, optional\\n          Whether or not to update the number of label issues or only compute label quality scores.\\n          For lower runtimes, set this to ``False`` if you only want to score label quality and not find label issues.\\n\\n        Returns\\n        -------\\n        label_quality_scores : np.ndarray\\n          Contains one score (between 0 and 1) for each example in the batch of data.\\n        '\n    labels = _batch_check(labels, pred_probs, self.num_class)\n    batch_size = len(labels)\n    scores = _compute_label_quality_scores(labels, pred_probs, confident_thresholds=self.get_confident_thresholds(silent=True), **self.quality_score_kwargs)\n    class_counts = value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n    if update_num_issues:\n        self._update_num_label_issues(labels, pred_probs, **self.num_issue_kwargs)\n    self.examples_processed_quality += batch_size\n    if self.store_results:\n        self.label_quality_scores += list(scores)\n    return scores"
        ]
    },
    {
        "func_name": "_update_num_label_issues",
        "original": "def _update_num_label_issues(self, labels: LabelLike, pred_probs: np.ndarray, **kwargs):\n    \"\"\"\n        Update the estimate of num_label_issues stored in this class using a new batch of data.\n        Kwargs are ignored here for now (included for forwards compatibility).\n        Instead of being specified here, `estimation_method` should be declared when this class is initialized.\n        \"\"\"\n    thorough = False\n    if self.examples_processed_thresh < 1:\n        raise ValueError('Have not computed any confident_thresholds yet. Call `update_confident_thresholds()` first.')\n    if self.n_jobs == 1:\n        adj_confident_thresholds = self.confident_thresholds - FLOATING_POINT_COMPARISON\n        pred_class = np.argmax(pred_probs, axis=1)\n        batch_size = len(labels)\n        if thorough:\n            pred_gt_thresholds = pred_probs >= adj_confident_thresholds\n            max_ind = np.argmax(pred_probs * pred_gt_thresholds, axis=1)\n            if not self.off_diagonal_calibrated:\n                mask = (max_ind != labels) & (pred_class != labels)\n            else:\n                mask = pred_class != labels\n        else:\n            max_ind = pred_class\n            mask = pred_class != labels\n        if not self.off_diagonal_calibrated:\n            prune_count_batch = np.sum((pred_probs[np.arange(batch_size), max_ind] >= adj_confident_thresholds[max_ind]) & mask)\n            self.prune_count += prune_count_batch\n        else:\n            self.class_counts += value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n            to_increment = pred_probs[np.arange(batch_size), max_ind] >= adj_confident_thresholds[max_ind]\n            for class_label in range(self.num_class):\n                labels_equal_to_class = labels == class_label\n                self.normalization[class_label] += np.sum(labels_equal_to_class & to_increment)\n                self.prune_counts[class_label] += np.sum(labels_equal_to_class & to_increment & (max_ind != labels))\n    else:\n        global adj_confident_thresholds_shared\n        adj_confident_thresholds_shared = self.confident_thresholds - FLOATING_POINT_COMPARISON\n        global labels_shared, pred_probs_shared\n        labels_shared = labels\n        pred_probs_shared = pred_probs\n        processes = 5000\n        if len(labels) <= processes:\n            chunksize = 1\n        else:\n            chunksize = len(labels) // processes\n        inds = split_arr(np.arange(len(labels)), chunksize)\n        if thorough:\n            use_thorough = np.ones(len(inds), dtype=bool)\n        else:\n            use_thorough = np.zeros(len(inds), dtype=bool)\n        args = zip(inds, use_thorough)\n        with mp.Pool(self.n_jobs) as pool:\n            if not self.off_diagonal_calibrated:\n                prune_count_batch = np.sum(np.asarray(list(pool.imap_unordered(_compute_num_issues, args))))\n                self.prune_count += prune_count_batch\n            else:\n                results = list(pool.imap_unordered(_compute_num_issues_calibrated, args))\n                for result in results:\n                    class_label = result[0]\n                    self.class_counts[class_label] += 1\n                    self.normalization[class_label] += result[1]\n                    self.prune_counts[class_label] += result[2]",
        "mutated": [
            "def _update_num_label_issues(self, labels: LabelLike, pred_probs: np.ndarray, **kwargs):\n    if False:\n        i = 10\n    '\\n        Update the estimate of num_label_issues stored in this class using a new batch of data.\\n        Kwargs are ignored here for now (included for forwards compatibility).\\n        Instead of being specified here, `estimation_method` should be declared when this class is initialized.\\n        '\n    thorough = False\n    if self.examples_processed_thresh < 1:\n        raise ValueError('Have not computed any confident_thresholds yet. Call `update_confident_thresholds()` first.')\n    if self.n_jobs == 1:\n        adj_confident_thresholds = self.confident_thresholds - FLOATING_POINT_COMPARISON\n        pred_class = np.argmax(pred_probs, axis=1)\n        batch_size = len(labels)\n        if thorough:\n            pred_gt_thresholds = pred_probs >= adj_confident_thresholds\n            max_ind = np.argmax(pred_probs * pred_gt_thresholds, axis=1)\n            if not self.off_diagonal_calibrated:\n                mask = (max_ind != labels) & (pred_class != labels)\n            else:\n                mask = pred_class != labels\n        else:\n            max_ind = pred_class\n            mask = pred_class != labels\n        if not self.off_diagonal_calibrated:\n            prune_count_batch = np.sum((pred_probs[np.arange(batch_size), max_ind] >= adj_confident_thresholds[max_ind]) & mask)\n            self.prune_count += prune_count_batch\n        else:\n            self.class_counts += value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n            to_increment = pred_probs[np.arange(batch_size), max_ind] >= adj_confident_thresholds[max_ind]\n            for class_label in range(self.num_class):\n                labels_equal_to_class = labels == class_label\n                self.normalization[class_label] += np.sum(labels_equal_to_class & to_increment)\n                self.prune_counts[class_label] += np.sum(labels_equal_to_class & to_increment & (max_ind != labels))\n    else:\n        global adj_confident_thresholds_shared\n        adj_confident_thresholds_shared = self.confident_thresholds - FLOATING_POINT_COMPARISON\n        global labels_shared, pred_probs_shared\n        labels_shared = labels\n        pred_probs_shared = pred_probs\n        processes = 5000\n        if len(labels) <= processes:\n            chunksize = 1\n        else:\n            chunksize = len(labels) // processes\n        inds = split_arr(np.arange(len(labels)), chunksize)\n        if thorough:\n            use_thorough = np.ones(len(inds), dtype=bool)\n        else:\n            use_thorough = np.zeros(len(inds), dtype=bool)\n        args = zip(inds, use_thorough)\n        with mp.Pool(self.n_jobs) as pool:\n            if not self.off_diagonal_calibrated:\n                prune_count_batch = np.sum(np.asarray(list(pool.imap_unordered(_compute_num_issues, args))))\n                self.prune_count += prune_count_batch\n            else:\n                results = list(pool.imap_unordered(_compute_num_issues_calibrated, args))\n                for result in results:\n                    class_label = result[0]\n                    self.class_counts[class_label] += 1\n                    self.normalization[class_label] += result[1]\n                    self.prune_counts[class_label] += result[2]",
            "def _update_num_label_issues(self, labels: LabelLike, pred_probs: np.ndarray, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Update the estimate of num_label_issues stored in this class using a new batch of data.\\n        Kwargs are ignored here for now (included for forwards compatibility).\\n        Instead of being specified here, `estimation_method` should be declared when this class is initialized.\\n        '\n    thorough = False\n    if self.examples_processed_thresh < 1:\n        raise ValueError('Have not computed any confident_thresholds yet. Call `update_confident_thresholds()` first.')\n    if self.n_jobs == 1:\n        adj_confident_thresholds = self.confident_thresholds - FLOATING_POINT_COMPARISON\n        pred_class = np.argmax(pred_probs, axis=1)\n        batch_size = len(labels)\n        if thorough:\n            pred_gt_thresholds = pred_probs >= adj_confident_thresholds\n            max_ind = np.argmax(pred_probs * pred_gt_thresholds, axis=1)\n            if not self.off_diagonal_calibrated:\n                mask = (max_ind != labels) & (pred_class != labels)\n            else:\n                mask = pred_class != labels\n        else:\n            max_ind = pred_class\n            mask = pred_class != labels\n        if not self.off_diagonal_calibrated:\n            prune_count_batch = np.sum((pred_probs[np.arange(batch_size), max_ind] >= adj_confident_thresholds[max_ind]) & mask)\n            self.prune_count += prune_count_batch\n        else:\n            self.class_counts += value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n            to_increment = pred_probs[np.arange(batch_size), max_ind] >= adj_confident_thresholds[max_ind]\n            for class_label in range(self.num_class):\n                labels_equal_to_class = labels == class_label\n                self.normalization[class_label] += np.sum(labels_equal_to_class & to_increment)\n                self.prune_counts[class_label] += np.sum(labels_equal_to_class & to_increment & (max_ind != labels))\n    else:\n        global adj_confident_thresholds_shared\n        adj_confident_thresholds_shared = self.confident_thresholds - FLOATING_POINT_COMPARISON\n        global labels_shared, pred_probs_shared\n        labels_shared = labels\n        pred_probs_shared = pred_probs\n        processes = 5000\n        if len(labels) <= processes:\n            chunksize = 1\n        else:\n            chunksize = len(labels) // processes\n        inds = split_arr(np.arange(len(labels)), chunksize)\n        if thorough:\n            use_thorough = np.ones(len(inds), dtype=bool)\n        else:\n            use_thorough = np.zeros(len(inds), dtype=bool)\n        args = zip(inds, use_thorough)\n        with mp.Pool(self.n_jobs) as pool:\n            if not self.off_diagonal_calibrated:\n                prune_count_batch = np.sum(np.asarray(list(pool.imap_unordered(_compute_num_issues, args))))\n                self.prune_count += prune_count_batch\n            else:\n                results = list(pool.imap_unordered(_compute_num_issues_calibrated, args))\n                for result in results:\n                    class_label = result[0]\n                    self.class_counts[class_label] += 1\n                    self.normalization[class_label] += result[1]\n                    self.prune_counts[class_label] += result[2]",
            "def _update_num_label_issues(self, labels: LabelLike, pred_probs: np.ndarray, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Update the estimate of num_label_issues stored in this class using a new batch of data.\\n        Kwargs are ignored here for now (included for forwards compatibility).\\n        Instead of being specified here, `estimation_method` should be declared when this class is initialized.\\n        '\n    thorough = False\n    if self.examples_processed_thresh < 1:\n        raise ValueError('Have not computed any confident_thresholds yet. Call `update_confident_thresholds()` first.')\n    if self.n_jobs == 1:\n        adj_confident_thresholds = self.confident_thresholds - FLOATING_POINT_COMPARISON\n        pred_class = np.argmax(pred_probs, axis=1)\n        batch_size = len(labels)\n        if thorough:\n            pred_gt_thresholds = pred_probs >= adj_confident_thresholds\n            max_ind = np.argmax(pred_probs * pred_gt_thresholds, axis=1)\n            if not self.off_diagonal_calibrated:\n                mask = (max_ind != labels) & (pred_class != labels)\n            else:\n                mask = pred_class != labels\n        else:\n            max_ind = pred_class\n            mask = pred_class != labels\n        if not self.off_diagonal_calibrated:\n            prune_count_batch = np.sum((pred_probs[np.arange(batch_size), max_ind] >= adj_confident_thresholds[max_ind]) & mask)\n            self.prune_count += prune_count_batch\n        else:\n            self.class_counts += value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n            to_increment = pred_probs[np.arange(batch_size), max_ind] >= adj_confident_thresholds[max_ind]\n            for class_label in range(self.num_class):\n                labels_equal_to_class = labels == class_label\n                self.normalization[class_label] += np.sum(labels_equal_to_class & to_increment)\n                self.prune_counts[class_label] += np.sum(labels_equal_to_class & to_increment & (max_ind != labels))\n    else:\n        global adj_confident_thresholds_shared\n        adj_confident_thresholds_shared = self.confident_thresholds - FLOATING_POINT_COMPARISON\n        global labels_shared, pred_probs_shared\n        labels_shared = labels\n        pred_probs_shared = pred_probs\n        processes = 5000\n        if len(labels) <= processes:\n            chunksize = 1\n        else:\n            chunksize = len(labels) // processes\n        inds = split_arr(np.arange(len(labels)), chunksize)\n        if thorough:\n            use_thorough = np.ones(len(inds), dtype=bool)\n        else:\n            use_thorough = np.zeros(len(inds), dtype=bool)\n        args = zip(inds, use_thorough)\n        with mp.Pool(self.n_jobs) as pool:\n            if not self.off_diagonal_calibrated:\n                prune_count_batch = np.sum(np.asarray(list(pool.imap_unordered(_compute_num_issues, args))))\n                self.prune_count += prune_count_batch\n            else:\n                results = list(pool.imap_unordered(_compute_num_issues_calibrated, args))\n                for result in results:\n                    class_label = result[0]\n                    self.class_counts[class_label] += 1\n                    self.normalization[class_label] += result[1]\n                    self.prune_counts[class_label] += result[2]",
            "def _update_num_label_issues(self, labels: LabelLike, pred_probs: np.ndarray, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Update the estimate of num_label_issues stored in this class using a new batch of data.\\n        Kwargs are ignored here for now (included for forwards compatibility).\\n        Instead of being specified here, `estimation_method` should be declared when this class is initialized.\\n        '\n    thorough = False\n    if self.examples_processed_thresh < 1:\n        raise ValueError('Have not computed any confident_thresholds yet. Call `update_confident_thresholds()` first.')\n    if self.n_jobs == 1:\n        adj_confident_thresholds = self.confident_thresholds - FLOATING_POINT_COMPARISON\n        pred_class = np.argmax(pred_probs, axis=1)\n        batch_size = len(labels)\n        if thorough:\n            pred_gt_thresholds = pred_probs >= adj_confident_thresholds\n            max_ind = np.argmax(pred_probs * pred_gt_thresholds, axis=1)\n            if not self.off_diagonal_calibrated:\n                mask = (max_ind != labels) & (pred_class != labels)\n            else:\n                mask = pred_class != labels\n        else:\n            max_ind = pred_class\n            mask = pred_class != labels\n        if not self.off_diagonal_calibrated:\n            prune_count_batch = np.sum((pred_probs[np.arange(batch_size), max_ind] >= adj_confident_thresholds[max_ind]) & mask)\n            self.prune_count += prune_count_batch\n        else:\n            self.class_counts += value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n            to_increment = pred_probs[np.arange(batch_size), max_ind] >= adj_confident_thresholds[max_ind]\n            for class_label in range(self.num_class):\n                labels_equal_to_class = labels == class_label\n                self.normalization[class_label] += np.sum(labels_equal_to_class & to_increment)\n                self.prune_counts[class_label] += np.sum(labels_equal_to_class & to_increment & (max_ind != labels))\n    else:\n        global adj_confident_thresholds_shared\n        adj_confident_thresholds_shared = self.confident_thresholds - FLOATING_POINT_COMPARISON\n        global labels_shared, pred_probs_shared\n        labels_shared = labels\n        pred_probs_shared = pred_probs\n        processes = 5000\n        if len(labels) <= processes:\n            chunksize = 1\n        else:\n            chunksize = len(labels) // processes\n        inds = split_arr(np.arange(len(labels)), chunksize)\n        if thorough:\n            use_thorough = np.ones(len(inds), dtype=bool)\n        else:\n            use_thorough = np.zeros(len(inds), dtype=bool)\n        args = zip(inds, use_thorough)\n        with mp.Pool(self.n_jobs) as pool:\n            if not self.off_diagonal_calibrated:\n                prune_count_batch = np.sum(np.asarray(list(pool.imap_unordered(_compute_num_issues, args))))\n                self.prune_count += prune_count_batch\n            else:\n                results = list(pool.imap_unordered(_compute_num_issues_calibrated, args))\n                for result in results:\n                    class_label = result[0]\n                    self.class_counts[class_label] += 1\n                    self.normalization[class_label] += result[1]\n                    self.prune_counts[class_label] += result[2]",
            "def _update_num_label_issues(self, labels: LabelLike, pred_probs: np.ndarray, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Update the estimate of num_label_issues stored in this class using a new batch of data.\\n        Kwargs are ignored here for now (included for forwards compatibility).\\n        Instead of being specified here, `estimation_method` should be declared when this class is initialized.\\n        '\n    thorough = False\n    if self.examples_processed_thresh < 1:\n        raise ValueError('Have not computed any confident_thresholds yet. Call `update_confident_thresholds()` first.')\n    if self.n_jobs == 1:\n        adj_confident_thresholds = self.confident_thresholds - FLOATING_POINT_COMPARISON\n        pred_class = np.argmax(pred_probs, axis=1)\n        batch_size = len(labels)\n        if thorough:\n            pred_gt_thresholds = pred_probs >= adj_confident_thresholds\n            max_ind = np.argmax(pred_probs * pred_gt_thresholds, axis=1)\n            if not self.off_diagonal_calibrated:\n                mask = (max_ind != labels) & (pred_class != labels)\n            else:\n                mask = pred_class != labels\n        else:\n            max_ind = pred_class\n            mask = pred_class != labels\n        if not self.off_diagonal_calibrated:\n            prune_count_batch = np.sum((pred_probs[np.arange(batch_size), max_ind] >= adj_confident_thresholds[max_ind]) & mask)\n            self.prune_count += prune_count_batch\n        else:\n            self.class_counts += value_counts_fill_missing_classes(labels, num_classes=self.num_class)\n            to_increment = pred_probs[np.arange(batch_size), max_ind] >= adj_confident_thresholds[max_ind]\n            for class_label in range(self.num_class):\n                labels_equal_to_class = labels == class_label\n                self.normalization[class_label] += np.sum(labels_equal_to_class & to_increment)\n                self.prune_counts[class_label] += np.sum(labels_equal_to_class & to_increment & (max_ind != labels))\n    else:\n        global adj_confident_thresholds_shared\n        adj_confident_thresholds_shared = self.confident_thresholds - FLOATING_POINT_COMPARISON\n        global labels_shared, pred_probs_shared\n        labels_shared = labels\n        pred_probs_shared = pred_probs\n        processes = 5000\n        if len(labels) <= processes:\n            chunksize = 1\n        else:\n            chunksize = len(labels) // processes\n        inds = split_arr(np.arange(len(labels)), chunksize)\n        if thorough:\n            use_thorough = np.ones(len(inds), dtype=bool)\n        else:\n            use_thorough = np.zeros(len(inds), dtype=bool)\n        args = zip(inds, use_thorough)\n        with mp.Pool(self.n_jobs) as pool:\n            if not self.off_diagonal_calibrated:\n                prune_count_batch = np.sum(np.asarray(list(pool.imap_unordered(_compute_num_issues, args))))\n                self.prune_count += prune_count_batch\n            else:\n                results = list(pool.imap_unordered(_compute_num_issues_calibrated, args))\n                for result in results:\n                    class_label = result[0]\n                    self.class_counts[class_label] += 1\n                    self.normalization[class_label] += result[1]\n                    self.prune_counts[class_label] += result[2]"
        ]
    },
    {
        "func_name": "split_arr",
        "original": "def split_arr(arr: np.ndarray, chunksize: int) -> List[np.ndarray]:\n    \"\"\"\n    Helper function to split array into chunks for multiprocessing.\n    \"\"\"\n    return np.split(arr, np.arange(chunksize, arr.shape[0], chunksize), axis=0)",
        "mutated": [
            "def split_arr(arr: np.ndarray, chunksize: int) -> List[np.ndarray]:\n    if False:\n        i = 10\n    '\\n    Helper function to split array into chunks for multiprocessing.\\n    '\n    return np.split(arr, np.arange(chunksize, arr.shape[0], chunksize), axis=0)",
            "def split_arr(arr: np.ndarray, chunksize: int) -> List[np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Helper function to split array into chunks for multiprocessing.\\n    '\n    return np.split(arr, np.arange(chunksize, arr.shape[0], chunksize), axis=0)",
            "def split_arr(arr: np.ndarray, chunksize: int) -> List[np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Helper function to split array into chunks for multiprocessing.\\n    '\n    return np.split(arr, np.arange(chunksize, arr.shape[0], chunksize), axis=0)",
            "def split_arr(arr: np.ndarray, chunksize: int) -> List[np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Helper function to split array into chunks for multiprocessing.\\n    '\n    return np.split(arr, np.arange(chunksize, arr.shape[0], chunksize), axis=0)",
            "def split_arr(arr: np.ndarray, chunksize: int) -> List[np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Helper function to split array into chunks for multiprocessing.\\n    '\n    return np.split(arr, np.arange(chunksize, arr.shape[0], chunksize), axis=0)"
        ]
    },
    {
        "func_name": "_compute_num_issues",
        "original": "def _compute_num_issues(arg: Tuple[np.ndarray, bool]) -> int:\n    \"\"\"\n    Helper function for `_update_num_label_issues` multiprocessing without calibration.\n    \"\"\"\n    ind = arg[0]\n    thorough = arg[1]\n    label = labels_shared[ind]\n    pred_prob = pred_probs_shared[ind, :]\n    pred_class = np.argmax(pred_prob, axis=-1)\n    batch_size = len(label)\n    if thorough:\n        pred_gt_thresholds = pred_prob >= adj_confident_thresholds_shared\n        max_ind = np.argmax(pred_prob * pred_gt_thresholds, axis=-1)\n        prune_count_batch = np.sum((pred_prob[np.arange(batch_size), max_ind] >= adj_confident_thresholds_shared[max_ind]) & (max_ind != label) & (pred_class != label))\n    else:\n        prune_count_batch = np.sum((pred_prob[np.arange(batch_size), pred_class] >= adj_confident_thresholds_shared[pred_class]) & (pred_class != label))\n    return prune_count_batch",
        "mutated": [
            "def _compute_num_issues(arg: Tuple[np.ndarray, bool]) -> int:\n    if False:\n        i = 10\n    '\\n    Helper function for `_update_num_label_issues` multiprocessing without calibration.\\n    '\n    ind = arg[0]\n    thorough = arg[1]\n    label = labels_shared[ind]\n    pred_prob = pred_probs_shared[ind, :]\n    pred_class = np.argmax(pred_prob, axis=-1)\n    batch_size = len(label)\n    if thorough:\n        pred_gt_thresholds = pred_prob >= adj_confident_thresholds_shared\n        max_ind = np.argmax(pred_prob * pred_gt_thresholds, axis=-1)\n        prune_count_batch = np.sum((pred_prob[np.arange(batch_size), max_ind] >= adj_confident_thresholds_shared[max_ind]) & (max_ind != label) & (pred_class != label))\n    else:\n        prune_count_batch = np.sum((pred_prob[np.arange(batch_size), pred_class] >= adj_confident_thresholds_shared[pred_class]) & (pred_class != label))\n    return prune_count_batch",
            "def _compute_num_issues(arg: Tuple[np.ndarray, bool]) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Helper function for `_update_num_label_issues` multiprocessing without calibration.\\n    '\n    ind = arg[0]\n    thorough = arg[1]\n    label = labels_shared[ind]\n    pred_prob = pred_probs_shared[ind, :]\n    pred_class = np.argmax(pred_prob, axis=-1)\n    batch_size = len(label)\n    if thorough:\n        pred_gt_thresholds = pred_prob >= adj_confident_thresholds_shared\n        max_ind = np.argmax(pred_prob * pred_gt_thresholds, axis=-1)\n        prune_count_batch = np.sum((pred_prob[np.arange(batch_size), max_ind] >= adj_confident_thresholds_shared[max_ind]) & (max_ind != label) & (pred_class != label))\n    else:\n        prune_count_batch = np.sum((pred_prob[np.arange(batch_size), pred_class] >= adj_confident_thresholds_shared[pred_class]) & (pred_class != label))\n    return prune_count_batch",
            "def _compute_num_issues(arg: Tuple[np.ndarray, bool]) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Helper function for `_update_num_label_issues` multiprocessing without calibration.\\n    '\n    ind = arg[0]\n    thorough = arg[1]\n    label = labels_shared[ind]\n    pred_prob = pred_probs_shared[ind, :]\n    pred_class = np.argmax(pred_prob, axis=-1)\n    batch_size = len(label)\n    if thorough:\n        pred_gt_thresholds = pred_prob >= adj_confident_thresholds_shared\n        max_ind = np.argmax(pred_prob * pred_gt_thresholds, axis=-1)\n        prune_count_batch = np.sum((pred_prob[np.arange(batch_size), max_ind] >= adj_confident_thresholds_shared[max_ind]) & (max_ind != label) & (pred_class != label))\n    else:\n        prune_count_batch = np.sum((pred_prob[np.arange(batch_size), pred_class] >= adj_confident_thresholds_shared[pred_class]) & (pred_class != label))\n    return prune_count_batch",
            "def _compute_num_issues(arg: Tuple[np.ndarray, bool]) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Helper function for `_update_num_label_issues` multiprocessing without calibration.\\n    '\n    ind = arg[0]\n    thorough = arg[1]\n    label = labels_shared[ind]\n    pred_prob = pred_probs_shared[ind, :]\n    pred_class = np.argmax(pred_prob, axis=-1)\n    batch_size = len(label)\n    if thorough:\n        pred_gt_thresholds = pred_prob >= adj_confident_thresholds_shared\n        max_ind = np.argmax(pred_prob * pred_gt_thresholds, axis=-1)\n        prune_count_batch = np.sum((pred_prob[np.arange(batch_size), max_ind] >= adj_confident_thresholds_shared[max_ind]) & (max_ind != label) & (pred_class != label))\n    else:\n        prune_count_batch = np.sum((pred_prob[np.arange(batch_size), pred_class] >= adj_confident_thresholds_shared[pred_class]) & (pred_class != label))\n    return prune_count_batch",
            "def _compute_num_issues(arg: Tuple[np.ndarray, bool]) -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Helper function for `_update_num_label_issues` multiprocessing without calibration.\\n    '\n    ind = arg[0]\n    thorough = arg[1]\n    label = labels_shared[ind]\n    pred_prob = pred_probs_shared[ind, :]\n    pred_class = np.argmax(pred_prob, axis=-1)\n    batch_size = len(label)\n    if thorough:\n        pred_gt_thresholds = pred_prob >= adj_confident_thresholds_shared\n        max_ind = np.argmax(pred_prob * pred_gt_thresholds, axis=-1)\n        prune_count_batch = np.sum((pred_prob[np.arange(batch_size), max_ind] >= adj_confident_thresholds_shared[max_ind]) & (max_ind != label) & (pred_class != label))\n    else:\n        prune_count_batch = np.sum((pred_prob[np.arange(batch_size), pred_class] >= adj_confident_thresholds_shared[pred_class]) & (pred_class != label))\n    return prune_count_batch"
        ]
    },
    {
        "func_name": "_compute_num_issues_calibrated",
        "original": "def _compute_num_issues_calibrated(arg: Tuple[np.ndarray, bool]) -> Tuple[Any, int, int]:\n    \"\"\"\n    Helper function for `_update_num_label_issues` multiprocessing with calibration.\n    \"\"\"\n    ind = arg[0]\n    thorough = arg[1]\n    label = labels_shared[ind]\n    pred_prob = pred_probs_shared[ind, :]\n    batch_size = len(label)\n    pred_class = np.argmax(pred_prob, axis=-1)\n    if thorough:\n        pred_gt_thresholds = pred_prob >= adj_confident_thresholds_shared\n        max_ind = np.argmax(pred_prob * pred_gt_thresholds, axis=-1)\n        to_inc = pred_prob[np.arange(batch_size), max_ind] >= adj_confident_thresholds_shared[max_ind]\n        prune_count_batch = to_inc & (max_ind != label)\n        normalization_batch = to_inc\n    else:\n        to_inc = pred_prob[np.arange(batch_size), pred_class] >= adj_confident_thresholds_shared[pred_class]\n        normalization_batch = to_inc\n        prune_count_batch = to_inc & (pred_class != label)\n    return (label, normalization_batch, prune_count_batch)",
        "mutated": [
            "def _compute_num_issues_calibrated(arg: Tuple[np.ndarray, bool]) -> Tuple[Any, int, int]:\n    if False:\n        i = 10\n    '\\n    Helper function for `_update_num_label_issues` multiprocessing with calibration.\\n    '\n    ind = arg[0]\n    thorough = arg[1]\n    label = labels_shared[ind]\n    pred_prob = pred_probs_shared[ind, :]\n    batch_size = len(label)\n    pred_class = np.argmax(pred_prob, axis=-1)\n    if thorough:\n        pred_gt_thresholds = pred_prob >= adj_confident_thresholds_shared\n        max_ind = np.argmax(pred_prob * pred_gt_thresholds, axis=-1)\n        to_inc = pred_prob[np.arange(batch_size), max_ind] >= adj_confident_thresholds_shared[max_ind]\n        prune_count_batch = to_inc & (max_ind != label)\n        normalization_batch = to_inc\n    else:\n        to_inc = pred_prob[np.arange(batch_size), pred_class] >= adj_confident_thresholds_shared[pred_class]\n        normalization_batch = to_inc\n        prune_count_batch = to_inc & (pred_class != label)\n    return (label, normalization_batch, prune_count_batch)",
            "def _compute_num_issues_calibrated(arg: Tuple[np.ndarray, bool]) -> Tuple[Any, int, int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Helper function for `_update_num_label_issues` multiprocessing with calibration.\\n    '\n    ind = arg[0]\n    thorough = arg[1]\n    label = labels_shared[ind]\n    pred_prob = pred_probs_shared[ind, :]\n    batch_size = len(label)\n    pred_class = np.argmax(pred_prob, axis=-1)\n    if thorough:\n        pred_gt_thresholds = pred_prob >= adj_confident_thresholds_shared\n        max_ind = np.argmax(pred_prob * pred_gt_thresholds, axis=-1)\n        to_inc = pred_prob[np.arange(batch_size), max_ind] >= adj_confident_thresholds_shared[max_ind]\n        prune_count_batch = to_inc & (max_ind != label)\n        normalization_batch = to_inc\n    else:\n        to_inc = pred_prob[np.arange(batch_size), pred_class] >= adj_confident_thresholds_shared[pred_class]\n        normalization_batch = to_inc\n        prune_count_batch = to_inc & (pred_class != label)\n    return (label, normalization_batch, prune_count_batch)",
            "def _compute_num_issues_calibrated(arg: Tuple[np.ndarray, bool]) -> Tuple[Any, int, int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Helper function for `_update_num_label_issues` multiprocessing with calibration.\\n    '\n    ind = arg[0]\n    thorough = arg[1]\n    label = labels_shared[ind]\n    pred_prob = pred_probs_shared[ind, :]\n    batch_size = len(label)\n    pred_class = np.argmax(pred_prob, axis=-1)\n    if thorough:\n        pred_gt_thresholds = pred_prob >= adj_confident_thresholds_shared\n        max_ind = np.argmax(pred_prob * pred_gt_thresholds, axis=-1)\n        to_inc = pred_prob[np.arange(batch_size), max_ind] >= adj_confident_thresholds_shared[max_ind]\n        prune_count_batch = to_inc & (max_ind != label)\n        normalization_batch = to_inc\n    else:\n        to_inc = pred_prob[np.arange(batch_size), pred_class] >= adj_confident_thresholds_shared[pred_class]\n        normalization_batch = to_inc\n        prune_count_batch = to_inc & (pred_class != label)\n    return (label, normalization_batch, prune_count_batch)",
            "def _compute_num_issues_calibrated(arg: Tuple[np.ndarray, bool]) -> Tuple[Any, int, int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Helper function for `_update_num_label_issues` multiprocessing with calibration.\\n    '\n    ind = arg[0]\n    thorough = arg[1]\n    label = labels_shared[ind]\n    pred_prob = pred_probs_shared[ind, :]\n    batch_size = len(label)\n    pred_class = np.argmax(pred_prob, axis=-1)\n    if thorough:\n        pred_gt_thresholds = pred_prob >= adj_confident_thresholds_shared\n        max_ind = np.argmax(pred_prob * pred_gt_thresholds, axis=-1)\n        to_inc = pred_prob[np.arange(batch_size), max_ind] >= adj_confident_thresholds_shared[max_ind]\n        prune_count_batch = to_inc & (max_ind != label)\n        normalization_batch = to_inc\n    else:\n        to_inc = pred_prob[np.arange(batch_size), pred_class] >= adj_confident_thresholds_shared[pred_class]\n        normalization_batch = to_inc\n        prune_count_batch = to_inc & (pred_class != label)\n    return (label, normalization_batch, prune_count_batch)",
            "def _compute_num_issues_calibrated(arg: Tuple[np.ndarray, bool]) -> Tuple[Any, int, int]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Helper function for `_update_num_label_issues` multiprocessing with calibration.\\n    '\n    ind = arg[0]\n    thorough = arg[1]\n    label = labels_shared[ind]\n    pred_prob = pred_probs_shared[ind, :]\n    batch_size = len(label)\n    pred_class = np.argmax(pred_prob, axis=-1)\n    if thorough:\n        pred_gt_thresholds = pred_prob >= adj_confident_thresholds_shared\n        max_ind = np.argmax(pred_prob * pred_gt_thresholds, axis=-1)\n        to_inc = pred_prob[np.arange(batch_size), max_ind] >= adj_confident_thresholds_shared[max_ind]\n        prune_count_batch = to_inc & (max_ind != label)\n        normalization_batch = to_inc\n    else:\n        to_inc = pred_prob[np.arange(batch_size), pred_class] >= adj_confident_thresholds_shared[pred_class]\n        normalization_batch = to_inc\n        prune_count_batch = to_inc & (pred_class != label)\n    return (label, normalization_batch, prune_count_batch)"
        ]
    },
    {
        "func_name": "_batch_check",
        "original": "def _batch_check(labels: LabelLike, pred_probs: np.ndarray, num_class: int) -> np.ndarray:\n    \"\"\"\n    Basic checks to ensure batch of data looks ok. For efficiency, this check is quite minimal.\n\n    Returns\n    -------\n    labels : np.ndarray\n      `labels` formatted as a 1D array.\n    \"\"\"\n    batch_size = pred_probs.shape[0]\n    labels = np.asarray(labels)\n    if len(labels) != batch_size:\n        raise ValueError('labels and pred_probs must have same length')\n    if pred_probs.shape[1] != num_class:\n        raise ValueError('num_class must equal pred_probs.shape[1]')\n    return labels",
        "mutated": [
            "def _batch_check(labels: LabelLike, pred_probs: np.ndarray, num_class: int) -> np.ndarray:\n    if False:\n        i = 10\n    '\\n    Basic checks to ensure batch of data looks ok. For efficiency, this check is quite minimal.\\n\\n    Returns\\n    -------\\n    labels : np.ndarray\\n      `labels` formatted as a 1D array.\\n    '\n    batch_size = pred_probs.shape[0]\n    labels = np.asarray(labels)\n    if len(labels) != batch_size:\n        raise ValueError('labels and pred_probs must have same length')\n    if pred_probs.shape[1] != num_class:\n        raise ValueError('num_class must equal pred_probs.shape[1]')\n    return labels",
            "def _batch_check(labels: LabelLike, pred_probs: np.ndarray, num_class: int) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Basic checks to ensure batch of data looks ok. For efficiency, this check is quite minimal.\\n\\n    Returns\\n    -------\\n    labels : np.ndarray\\n      `labels` formatted as a 1D array.\\n    '\n    batch_size = pred_probs.shape[0]\n    labels = np.asarray(labels)\n    if len(labels) != batch_size:\n        raise ValueError('labels and pred_probs must have same length')\n    if pred_probs.shape[1] != num_class:\n        raise ValueError('num_class must equal pred_probs.shape[1]')\n    return labels",
            "def _batch_check(labels: LabelLike, pred_probs: np.ndarray, num_class: int) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Basic checks to ensure batch of data looks ok. For efficiency, this check is quite minimal.\\n\\n    Returns\\n    -------\\n    labels : np.ndarray\\n      `labels` formatted as a 1D array.\\n    '\n    batch_size = pred_probs.shape[0]\n    labels = np.asarray(labels)\n    if len(labels) != batch_size:\n        raise ValueError('labels and pred_probs must have same length')\n    if pred_probs.shape[1] != num_class:\n        raise ValueError('num_class must equal pred_probs.shape[1]')\n    return labels",
            "def _batch_check(labels: LabelLike, pred_probs: np.ndarray, num_class: int) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Basic checks to ensure batch of data looks ok. For efficiency, this check is quite minimal.\\n\\n    Returns\\n    -------\\n    labels : np.ndarray\\n      `labels` formatted as a 1D array.\\n    '\n    batch_size = pred_probs.shape[0]\n    labels = np.asarray(labels)\n    if len(labels) != batch_size:\n        raise ValueError('labels and pred_probs must have same length')\n    if pred_probs.shape[1] != num_class:\n        raise ValueError('num_class must equal pred_probs.shape[1]')\n    return labels",
            "def _batch_check(labels: LabelLike, pred_probs: np.ndarray, num_class: int) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Basic checks to ensure batch of data looks ok. For efficiency, this check is quite minimal.\\n\\n    Returns\\n    -------\\n    labels : np.ndarray\\n      `labels` formatted as a 1D array.\\n    '\n    batch_size = pred_probs.shape[0]\n    labels = np.asarray(labels)\n    if len(labels) != batch_size:\n        raise ValueError('labels and pred_probs must have same length')\n    if pred_probs.shape[1] != num_class:\n        raise ValueError('num_class must equal pred_probs.shape[1]')\n    return labels"
        ]
    }
]