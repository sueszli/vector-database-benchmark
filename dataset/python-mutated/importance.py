"""
Module which implements feature importance algorithms as described in Chapter 8 of Advances in Financial Machine
Learning and Clustered Feature Importance algorithms as described in Chapter 6 Section 6.5.2 of Machine Learning for
Asset Managers.

And feature importance algorithms multi-asset data set (stacked feature importance approach).
"""
from typing import Callable
import pandas as pd
import numpy as np
from sklearn.metrics import log_loss
from sklearn.model_selection import BaseCrossValidator
import matplotlib.pyplot as plt
from mlfinlab.cross_validation.cross_validation import ml_cross_val_score, stacked_dataset_from_dict

def mean_decrease_impurity(model, feature_names, clustered_subsets=None):
    if False:
        print('Hello World!')
    "\n    Advances in Financial Machine Learning, Snippet 8.2, page 115.\n\n    MDI Feature importance\n\n    Mean decrease impurity (MDI) is a fast, explanatory-importance (in-sample, IS) method specific to tree-based\n    classifiers, like RF. At each node of each decision tree, the selected feature splits the subset it received in\n    such a way that impurity is decreased. Therefore, we can derive for each decision tree how much of the overall\n    impurity decrease can be assigned to each feature. And given that we have a forest of trees, we can average those\n    values across all estimators and rank the features accordingly.\n\n    Tip:\n    Masking effects take place when some features are systematically ignored by tree-based classifiers in favor of\n    others. In order to avoid them, set max_features=int(1) when using sklearn’s RF class. In this way, only one random\n    feature is considered per level.\n\n    Notes:\n\n    * MDI cannot be generalized to other non-tree based classifiers\n    * The procedure is obviously in-sample.\n    * Every feature will have some importance, even if they have no predictive power whatsoever.\n    * MDI has the nice property that feature importances add up to 1, and every feature importance is bounded between 0 and 1.\n    * method does not address substitution effects in the presence of correlated features. MDI dilutes the importance of\n      substitute features, because of their interchangeability: The importance of two identical features will be halved,\n      as they are randomly chosen with equal probability.\n    * Sklearn’s RandomForest class implements MDI as the default feature importance score. This choice is likely\n      motivated by the ability to compute MDI on the fly, with minimum computational cost.\n\n    Clustered Feature Importance( Machine Learning for Asset Manager snippet 6.4 page 86) :\n    Clustered MDI  is the  modified version of MDI (Mean Decreased Impurity). It  is robust to substitution effect that\n    takes place when two or more explanatory variables share a substantial amount of information (predictive power).CFI\n    algorithm described by Dr Marcos Lopez de Prado  in Clustered Feature  Importance section of book Machine Learning\n    for Asset Manager. Here  instead of  taking the importance  of  every feature, we consider the importance of every\n    feature subsets, thus every feature receive the importance of subset it belongs to.\n\n    :param model: (object): Trained tree based classifier.\n    :param feature_names: (list): Array of feature names.\n    :param clustered_subsets: (list) Feature clusters for Clustered Feature Importance (CFI). Default None will not apply CFI.\n        Structure of the input must be a list of list/s i.e. a list containing the clusters/subsets of feature\n        name/s inside a list. E.g- [['I_0','I_1','R_0','R_1'],['N_1','N_2'],['R_3']]\n    :return: (pd.DataFrame): Mean and standard deviation feature importance.\n    "
    pass

def _mean_decrease_accuracy_round(model, X, y, cv_gen, clustered_subsets=None, sample_weight_train=None, sample_weight_score=None, scoring=log_loss, require_proba=True, random_state=42):
    if False:
        i = 10
        return i + 15
    "\n    Implements one round of MDA Feature importance.\n    Advances in Financial Machine Learning, Snippet 8.3, page 116-117.\n\n    MDA Feature Importance\n\n    Mean decrease accuracy (MDA) is a slow, predictive-importance (out-of-sample, OOS) method. First, it fits a\n    classifier; second, it derives its performance OOS according to some performance score (accuracy, negative log-loss,\n    etc.); third, it permutates each column of the features matrix (X), one column at a time, deriving the performance\n    OOS after each column’s permutation. The importance of a feature is a function of the loss in performance caused by\n    its column’s permutation. Some relevant considerations include:\n\n    * This method can be applied to any classifier, not only tree-based classifiers.\n    * MDA is not limited to accuracy as the sole performance score. For example, in the context of meta-labeling\n      applications, we may prefer to score a classifier with F1 rather than accuracy. That is one reason a better\n      descriptive name would have been “permutation importance.” When the scoring function does not correspond to a\n      metric space, MDA results should be used as a ranking.\n    * Like MDI, the procedure is also susceptible to substitution effects in the presence of correlated features.\n      Given two identical features, MDA always considers one to be redundant to the other. Unfortunately, MDA will make\n      both features appear to be outright irrelevant, even if they are critical.\n    * Unlike MDI, it is possible that MDA concludes that all features are unimportant. That is because MDA is based on\n      OOS performance.\n    * The CV must be purged and embargoed.\n\n    Clustered Feature Importance( Machine Learning for Asset Manager snippet 6.5 page 87) :\n    Clustered MDA is the modified version of MDA (Mean Decreased Accuracy). It is robust to substitution effect that takes\n    place when two or more explanatory variables share a substantial amount of information (predictive power).CFI algorithm\n    described by Dr Marcos Lopez de Prado  in Clustered Feature  Importance (Presentation Slides)\n    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3517595. Instead of shuffling (permutating) all variables\n    individually (like in MDA), we shuffle all variables in cluster together. Next, we follow all the  rest of the\n    steps as in MDA. It can used by simply specifying the clustered_subsets argument.\n\n    :param model: (sklearn.Classifier): Any sklearn classifier.\n    :param X: (pd.DataFrame): Train set features.\n    :param y: (pd.DataFrame, np.array): Train set labels.\n    :param cv_gen: (cross_validation.PurgedKFold): Cross-validation object.\n    :param clustered_subsets: (list) Feature clusters for Clustered Feature Importance (CFI). Default None will not apply CFI.\n        Structure of the input must be a list of list/s i.e. a list containing the clusters/subsets of feature\n        name/s inside a list. E.g- [['I_0','I_1','R_0','R_1'],['N_1','N_2'],['R_3']]\n    :param sample_weight_train: (np.array) Sample weights used to train the model for each record in the dataset.\n    :param sample_weight_score: (np.array) Sample weights used to evaluate the model quality.\n    :param scoring: (function): Scoring function used to determine importance.\n    :param require_proba: (bool): Boolean flag indicating that scoring function expects probabilities as input.\n    :param random_state: (int) Random seed for shuffling the features.\n    :return: (pd.DataFrame): Mean and standard deviation of feature importance.\n    "
    pass

def mean_decrease_accuracy(model, X, y, cv_gen, clustered_subsets=None, sample_weight_train=None, sample_weight_score=None, scoring=log_loss, require_proba=True, n_repeat=1):
    if False:
        print('Hello World!')
    "\n    Advances in Financial Machine Learning, Snippet 8.3, page 116-117.\n\n    MDA Feature Importance (averaged over different random seeds `n_repeat` times)\n\n    Mean decrease accuracy (MDA) is a slow, predictive-importance (out-of-sample, OOS) method. First, it fits a\n    classifier; second, it derives its performance OOS according to some performance score (accuracy, negative log-loss,\n    etc.); third, it permutates each column of the features matrix (X), one column at a time, deriving the performance\n    OOS after each column’s permutation. The importance of a feature is a function of the loss in performance caused by\n    its column’s permutation. Some relevant considerations include:\n\n    * This method can be applied to any classifier, not only tree-based classifiers.\n    * MDA is not limited to accuracy as the sole performance score. For example, in the context of meta-labeling\n      applications, we may prefer to score a classifier with F1 rather than accuracy. That is one reason a better\n      descriptive name would have been “permutation importance.” When the scoring function does not correspond to a\n      metric space, MDA results should be used as a ranking.\n    * Like MDI, the procedure is also susceptible to substitution effects in the presence of correlated features.\n      Given two identical features, MDA always considers one to be redundant to the other. Unfortunately, MDA will make\n      both features appear to be outright irrelevant, even if they are critical.\n    * Unlike MDI, it is possible that MDA concludes that all features are unimportant. That is because MDA is based on\n      OOS performance.\n    * The CV must be purged and embargoed.\n\n    Clustered Feature Importance( Machine Learning for Asset Manager snippet 6.5 page 87) :\n    Clustered MDA is the modified version of MDA (Mean Decreased Accuracy). It is robust to substitution effect that takes\n    place when two or more explanatory variables share a substantial amount of information (predictive power).CFI algorithm\n    described by Dr Marcos Lopez de Prado  in Clustered Feature  Importance (Presentation Slides)\n    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3517595. Instead of shuffling (permutating) all variables\n    individually (like in MDA), we shuffle all variables in cluster together. Next, we follow all the  rest of the\n    steps as in MDA. It can used by simply specifying the clustered_subsets argument.\n\n    :param model: (sklearn.Classifier): Any sklearn classifier.\n    :param X: (pd.DataFrame): Train set features.\n    :param y: (pd.DataFrame, np.array): Train set labels.\n    :param cv_gen: (cross_validation.PurgedKFold): Cross-validation object.\n    :param clustered_subsets: (list) Feature clusters for Clustered Feature Importance (CFI). Default None will not apply CFI.\n        Structure of the input must be a list of list/s i.e. a list containing the clusters/subsets of feature\n        name/s inside a list. E.g- [['I_0','I_1','R_0','R_1'],['N_1','N_2'],['R_3']]\n    :param sample_weight_train: (np.array) Sample weights used to train the model for each record in the dataset.\n    :param sample_weight_score: (np.array) Sample weights used to evaluate the model quality.\n    :param scoring: (function): Scoring function used to determine importance.\n    :param require_proba: (bool): Boolean flag indicating that scoring function expects probabilities as input.\n    :param n_repeat: (int) Number of times to repeat MDA feature importance with different random seeds.\n    :return: (pd.DataFrame): Mean and standard deviation of feature importance.\n    "
    pass

def single_feature_importance(clf, X, y, cv_gen, sample_weight_train=None, sample_weight_score=None, scoring=log_loss, require_proba=True):
    if False:
        print('Hello World!')
    '\n    Advances in Financial Machine Learning, Snippet 8.4, page 118.\n\n    Implementation of SFI\n\n    Substitution effects can lead us to discard important features that happen to be redundant. This is not generally a\n    problem in the context of prediction, but it could lead us to wrong conclusions when we are trying to understand,\n    improve, or simplify a model. For this reason, the following single feature importance method can be a good\n    complement to MDI and MDA.\n\n    Single feature importance (SFI) is a cross-section predictive-importance (out-of- sample) method. It computes the\n    OOS performance score of each feature in isolation. A few considerations:\n\n    * This method can be applied to any classifier, not only tree-based classifiers.\n    * SFI is not limited to accuracy as the sole performance score.\n    * Unlike MDI and MDA, no substitution effects take place, since only one feature is taken into consideration at a time.\n    * Like MDA, it can conclude that all features are unimportant, because performance is evaluated via OOS CV.\n\n    The main limitation of SFI is that a classifier with two features can perform better than the bagging of two\n    single-feature classifiers. For example, (1) feature B may be useful only in combination with feature A;\n    or (2) feature B may be useful in explaining the splits from feature A, even if feature B alone is inaccurate.\n    In other words, joint effects and hierarchical importance are lost in SFI. One alternative would be to compute the\n    OOS performance score from subsets of features, but that calculation will become intractable as more features are\n    considered.\n\n    :param clf: (sklearn.Classifier): Any sklearn classifier.\n    :param X: (pd.DataFrame): Train set features.\n    :param y: (pd.DataFrame, np.array): Train set labels.\n    :param cv_gen: (cross_validation.PurgedKFold): Cross-validation object.\n    :param sample_weight_train: (np.array) Sample weights used to train the model for each record in the dataset.\n    :param sample_weight_score: (np.array) Sample weights used to evaluate the model quality.\n    :param scoring: (function): Scoring function used to determine importance.\n    :param require_proba: (bool): Boolean flag indicating that scoring function expects probabilities as input.\n    :return: (pd.DataFrame): Mean and standard deviation of feature importance.\n    '
    pass

def plot_feature_importance(importance_df, oob_score, oos_score, save_fig=False, output_path=None):
    if False:
        i = 10
        return i + 15
    '\n    Advances in Financial Machine Learning, Snippet 8.10, page 124.\n\n    Feature importance plotting function.\n\n    Plot feature importance.\n\n    :param importance_df: (pd.DataFrame): Mean and standard deviation feature importance.\n    :param oob_score: (float): Out-of-bag score.\n    :param oos_score: (float): Out-of-sample (or cross-validation) score.\n    :param save_fig: (bool): Boolean flag to save figure to a file.\n    :param output_path: (str): If save_fig is True, path where figure should be saved.\n    '
    pass

def _stacked_mean_decrease_accuracy_round(model: object, X_dict: dict, y_dict: dict, cv_gen: BaseCrossValidator, clustered_subsets=None, sample_weight_train_dict: dict=None, sample_weight_score_dict=None, scoring: Callable[[np.array, np.array], float]=log_loss, require_proba: bool=True, random_state: int=42):
    if False:
        for i in range(10):
            print('nop')
    "\n    Implements one round of Stacked MDA Feature importance.\n    Advances in Financial Machine Learning, Snippet 8.3, page 116-117.\n\n    MDA Feature Importance\n\n    Mean decrease accuracy (MDA) is a slow, predictive-importance (out-of-sample, OOS) method. First, it fits a\n    classifier; second, it derives its performance OOS according to some performance score (accuracy, negative log-loss,\n    etc.); third, it permutates each column of the features matrix (X), one column at a time, deriving the performance\n    OOS after each column’s permutation. The importance of a feature is a function of the loss in performance caused by\n    its column’s permutation. Some relevant considerations include:\n\n    * This method can be applied to any classifier, not only tree-based classifiers.\n    * MDA is not limited to accuracy as the sole performance score. For example, in the context of meta-labeling\n      applications, we may prefer to score a classifier with F1 rather than accuracy. That is one reason a better\n      descriptive name would have been “permutation importance.” When the scoring function does not correspond to a\n      metric space, MDA results should be used as a ranking.\n    * Like MDI, the procedure is also susceptible to substitution effects in the presence of correlated features.\n      Given two identical features, MDA always considers one to be redundant to the other. Unfortunately, MDA will make\n      both features appear to be outright irrelevant, even if they are critical.\n    * Unlike MDI, it is possible that MDA concludes that all features are unimportant. That is because MDA is based on\n      OOS performance.\n    * The CV must be purged and embargoed.\n\n    Clustered Feature Importance( Machine Learning for Asset Manager snippet 6.5 page 87) :\n    Clustered MDA is the modified version of MDA (Mean Decreased Accuracy). It is robust to substitution effect that takes\n    place when two or more explanatory variables share a substantial amount of information (predictive power).CFI algorithm\n    described by Dr Marcos Lopez de Prado  in Clustered Feature  Importance (Presentation Slides)\n    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3517595. Instead of shuffling (permutating) all variables\n    individually (like in MDA), we shuffle all variables in cluster together. Next, we follow all the  rest of the\n    steps as in MDA. It can used by simply specifying the clustered_subsets argument.\n\n    :param model: (sklearn.Classifier): Any sklearn classifier.\n    :param X_dict: (dict) Dictionary of asset : X_{asset}.\n    :param y_dict: (dict) Dictionary of asset : y_{asset}\n    :param cv_gen: (cross_validation.PurgedKFold): Cross-validation object.\n    :param clustered_subsets: (list) Feature clusters for Clustered Feature Importance (CFI). Default None will not apply CFI.\n                              Structure of the input must be a list of list/s i.e. a list containing the clusters/subsets of feature\n                              name/s inside a list. E.g- [['I_0','I_1','R_0','R_1'],['N_1','N_2'],['R_3']]\n    :param sample_weight_train_dict: (dict) Dictionary of asset: sample_weights_train_{asset}\n    :param sample_weight_score_dict: (dict) Dictionary of asset: sample_weights_score_{asset}\n    :param scoring: (function): Scoring function used to determine importance.\n    :param require_proba: (bool): Boolean flag indicating that scoring function expects probabilities as input.\n    :param random_state: (int) Random seed for shuffling the features.\n    :return: (pd.DataFrame): Mean and standard deviation of feature importance.\n    "
    pass

def stacked_mean_decrease_accuracy(model: object, X_dict: dict, y_dict: dict, cv_gen: BaseCrossValidator, clustered_subsets=None, sample_weight_train_dict: dict=None, sample_weight_score_dict=None, scoring: Callable[[np.array, np.array], float]=log_loss, require_proba: bool=True, n_repeat: int=1):
    if False:
        while True:
            i = 10
    "\n    Advances in Financial Machine Learning, Snippet 8.3, page 116-117.\n\n    Stacked MDA Feature Importance for multi-asset dataset (averaged over different random seeds `n_repeat` times)\n\n    Mean decrease accuracy (MDA) is a slow, predictive-importance (out-of-sample, OOS) method. First, it fits a\n    classifier; second, it derives its performance OOS according to some performance score (accuracy, negative log-loss,\n    etc.); third, it permutates each column of the features matrix (X), one column at a time, deriving the performance\n    OOS after each column’s permutation. The importance of a feature is a function of the loss in performance caused by\n    its column’s permutation. Some relevant considerations include:\n\n    * This method can be applied to any classifier, not only tree-based classifiers.\n    * MDA is not limited to accuracy as the sole performance score. For example, in the context of meta-labeling\n      applications, we may prefer to score a classifier with F1 rather than accuracy. That is one reason a better\n      descriptive name would have been “permutation importance.” When the scoring function does not correspond to a\n      metric space, MDA results should be used as a ranking.\n    * Like MDI, the procedure is also susceptible to substitution effects in the presence of correlated features.\n      Given two identical features, MDA always considers one to be redundant to the other. Unfortunately, MDA will make\n      both features appear to be outright irrelevant, even if they are critical.\n    * Unlike MDI, it is possible that MDA concludes that all features are unimportant. That is because MDA is based on\n      OOS performance.\n    * The CV must be purged and embargoed.\n\n    Clustered Feature Importance( Machine Learning for Asset Manager snippet 6.5 page 87) :\n    Clustered MDA is the modified version of MDA (Mean Decreased Accuracy). It is robust to substitution effect that takes\n    place when two or more explanatory variables share a substantial amount of information (predictive power).CFI algorithm\n    described by Dr Marcos Lopez de Prado  in Clustered Feature  Importance (Presentation Slides)\n    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3517595. Instead of shuffling (permutating) all variables\n    individually (like in MDA), we shuffle all variables in cluster together. Next, we follow all the  rest of the\n    steps as in MDA. It can used by simply specifying the clustered_subsets argument.\n\n    :param model: (sklearn.Classifier): Any sklearn classifier.\n    :param X_dict: (dict) Dictionary of asset : X_{asset}.\n    :param y_dict: (dict) Dictionary of asset : y_{asset}\n    :param cv_gen: (cross_validation.StackedPurgedKFold): Cross-validation object.\n    :param clustered_subsets: (list) Feature clusters for Clustered Feature Importance (CFI). Default None will not apply CFI.\n                              Structure of the input must be a list of list/s i.e. a list containing the clusters/subsets of feature\n                              name/s inside a list. E.g- [['I_0','I_1','R_0','R_1'],['N_1','N_2'],['R_3']]\n    :param sample_weight_train_dict: (dict) Dictionary of asset: sample_weights_train_{asset}\n    :param sample_weight_score_dict: (dict) Dictionary of asset: sample_weights_score_{asset}\n    :param scoring: (function): Scoring function used to determine importance.\n    :param require_proba: (bool): Boolean flag indicating that scoring function expects probabilities as input.\n    :param n_repeat: (int) Number of times to repeat MDA feature importance with different random seeds.\n    :return: (pd.DataFrame): Mean and standard deviation of feature importance.\n    "
    pass