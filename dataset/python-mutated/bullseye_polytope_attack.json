[
    {
        "func_name": "__init__",
        "original": "def __init__(self, classifier: Union['CLASSIFIER_NEURALNETWORK_TYPE', List['CLASSIFIER_NEURALNETWORK_TYPE']], target: np.ndarray, feature_layer: Union[Union[str, int], List[Union[str, int]]], opt: str='adam', max_iter: int=4000, learning_rate: float=0.04, momentum: float=0.9, decay_iter: Union[int, List[int]]=10000, decay_coeff: float=0.5, epsilon: float=0.1, dropout: float=0.3, net_repeat: int=1, endtoend: bool=True, batch_size: int=128, verbose: bool=True):\n    \"\"\"\n        Initialize an Feature Collision Clean-Label poisoning attack\n\n        :param classifier: The proxy classifiers used for the attack. Can be a single classifier or list of classifiers\n                           with varying architectures.\n        :param target: The target input(s) of shape (N, W, H, C) to misclassify at test time. Multiple targets will be\n                       averaged.\n        :param feature_layer: The name(s) of the feature representation layer(s).\n        :param opt: The optimizer to use for the attack. Can be 'adam' or 'sgd'\n        :param max_iter: The maximum number of iterations for the attack.\n        :param learning_rate: The learning rate of clean-label attack optimization.\n        :param momentum: The momentum of clean-label attack optimization.\n        :param decay_iter: Which iterations to decay the learning rate.\n                           Can be a integer (every N iterations) or list of integers [0, 500, 1500]\n        :param decay_coeff: The decay coefficient of the learning rate.\n        :param epsilon: The perturbation budget\n        :param dropout: Dropout to apply while training\n        :param net_repeat: The number of times to repeat prediction on each network\n        :param endtoend: True for end-to-end training. False for transfer learning.\n        :param batch_size: Batch size.\n        :param verbose: Show progress bars.\n        \"\"\"\n    self.subsistute_networks: List['CLASSIFIER_NEURALNETWORK_TYPE'] = [classifier] if not isinstance(classifier, list) else classifier\n    super().__init__(classifier=self.subsistute_networks[0])\n    self.target = target\n    self.opt = opt\n    self.momentum = momentum\n    self.decay_iter = decay_iter\n    self.epsilon = epsilon\n    self.dropout = dropout\n    self.net_repeat = net_repeat\n    self.endtoend = endtoend\n    self.feature_layer = feature_layer\n    self.learning_rate = learning_rate\n    self.decay_coeff = decay_coeff\n    self.max_iter = max_iter\n    self.batch_size = batch_size\n    self.verbose = verbose\n    self._check_params()",
        "mutated": [
            "def __init__(self, classifier: Union['CLASSIFIER_NEURALNETWORK_TYPE', List['CLASSIFIER_NEURALNETWORK_TYPE']], target: np.ndarray, feature_layer: Union[Union[str, int], List[Union[str, int]]], opt: str='adam', max_iter: int=4000, learning_rate: float=0.04, momentum: float=0.9, decay_iter: Union[int, List[int]]=10000, decay_coeff: float=0.5, epsilon: float=0.1, dropout: float=0.3, net_repeat: int=1, endtoend: bool=True, batch_size: int=128, verbose: bool=True):\n    if False:\n        i = 10\n    \"\\n        Initialize an Feature Collision Clean-Label poisoning attack\\n\\n        :param classifier: The proxy classifiers used for the attack. Can be a single classifier or list of classifiers\\n                           with varying architectures.\\n        :param target: The target input(s) of shape (N, W, H, C) to misclassify at test time. Multiple targets will be\\n                       averaged.\\n        :param feature_layer: The name(s) of the feature representation layer(s).\\n        :param opt: The optimizer to use for the attack. Can be 'adam' or 'sgd'\\n        :param max_iter: The maximum number of iterations for the attack.\\n        :param learning_rate: The learning rate of clean-label attack optimization.\\n        :param momentum: The momentum of clean-label attack optimization.\\n        :param decay_iter: Which iterations to decay the learning rate.\\n                           Can be a integer (every N iterations) or list of integers [0, 500, 1500]\\n        :param decay_coeff: The decay coefficient of the learning rate.\\n        :param epsilon: The perturbation budget\\n        :param dropout: Dropout to apply while training\\n        :param net_repeat: The number of times to repeat prediction on each network\\n        :param endtoend: True for end-to-end training. False for transfer learning.\\n        :param batch_size: Batch size.\\n        :param verbose: Show progress bars.\\n        \"\n    self.subsistute_networks: List['CLASSIFIER_NEURALNETWORK_TYPE'] = [classifier] if not isinstance(classifier, list) else classifier\n    super().__init__(classifier=self.subsistute_networks[0])\n    self.target = target\n    self.opt = opt\n    self.momentum = momentum\n    self.decay_iter = decay_iter\n    self.epsilon = epsilon\n    self.dropout = dropout\n    self.net_repeat = net_repeat\n    self.endtoend = endtoend\n    self.feature_layer = feature_layer\n    self.learning_rate = learning_rate\n    self.decay_coeff = decay_coeff\n    self.max_iter = max_iter\n    self.batch_size = batch_size\n    self.verbose = verbose\n    self._check_params()",
            "def __init__(self, classifier: Union['CLASSIFIER_NEURALNETWORK_TYPE', List['CLASSIFIER_NEURALNETWORK_TYPE']], target: np.ndarray, feature_layer: Union[Union[str, int], List[Union[str, int]]], opt: str='adam', max_iter: int=4000, learning_rate: float=0.04, momentum: float=0.9, decay_iter: Union[int, List[int]]=10000, decay_coeff: float=0.5, epsilon: float=0.1, dropout: float=0.3, net_repeat: int=1, endtoend: bool=True, batch_size: int=128, verbose: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n        Initialize an Feature Collision Clean-Label poisoning attack\\n\\n        :param classifier: The proxy classifiers used for the attack. Can be a single classifier or list of classifiers\\n                           with varying architectures.\\n        :param target: The target input(s) of shape (N, W, H, C) to misclassify at test time. Multiple targets will be\\n                       averaged.\\n        :param feature_layer: The name(s) of the feature representation layer(s).\\n        :param opt: The optimizer to use for the attack. Can be 'adam' or 'sgd'\\n        :param max_iter: The maximum number of iterations for the attack.\\n        :param learning_rate: The learning rate of clean-label attack optimization.\\n        :param momentum: The momentum of clean-label attack optimization.\\n        :param decay_iter: Which iterations to decay the learning rate.\\n                           Can be a integer (every N iterations) or list of integers [0, 500, 1500]\\n        :param decay_coeff: The decay coefficient of the learning rate.\\n        :param epsilon: The perturbation budget\\n        :param dropout: Dropout to apply while training\\n        :param net_repeat: The number of times to repeat prediction on each network\\n        :param endtoend: True for end-to-end training. False for transfer learning.\\n        :param batch_size: Batch size.\\n        :param verbose: Show progress bars.\\n        \"\n    self.subsistute_networks: List['CLASSIFIER_NEURALNETWORK_TYPE'] = [classifier] if not isinstance(classifier, list) else classifier\n    super().__init__(classifier=self.subsistute_networks[0])\n    self.target = target\n    self.opt = opt\n    self.momentum = momentum\n    self.decay_iter = decay_iter\n    self.epsilon = epsilon\n    self.dropout = dropout\n    self.net_repeat = net_repeat\n    self.endtoend = endtoend\n    self.feature_layer = feature_layer\n    self.learning_rate = learning_rate\n    self.decay_coeff = decay_coeff\n    self.max_iter = max_iter\n    self.batch_size = batch_size\n    self.verbose = verbose\n    self._check_params()",
            "def __init__(self, classifier: Union['CLASSIFIER_NEURALNETWORK_TYPE', List['CLASSIFIER_NEURALNETWORK_TYPE']], target: np.ndarray, feature_layer: Union[Union[str, int], List[Union[str, int]]], opt: str='adam', max_iter: int=4000, learning_rate: float=0.04, momentum: float=0.9, decay_iter: Union[int, List[int]]=10000, decay_coeff: float=0.5, epsilon: float=0.1, dropout: float=0.3, net_repeat: int=1, endtoend: bool=True, batch_size: int=128, verbose: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n        Initialize an Feature Collision Clean-Label poisoning attack\\n\\n        :param classifier: The proxy classifiers used for the attack. Can be a single classifier or list of classifiers\\n                           with varying architectures.\\n        :param target: The target input(s) of shape (N, W, H, C) to misclassify at test time. Multiple targets will be\\n                       averaged.\\n        :param feature_layer: The name(s) of the feature representation layer(s).\\n        :param opt: The optimizer to use for the attack. Can be 'adam' or 'sgd'\\n        :param max_iter: The maximum number of iterations for the attack.\\n        :param learning_rate: The learning rate of clean-label attack optimization.\\n        :param momentum: The momentum of clean-label attack optimization.\\n        :param decay_iter: Which iterations to decay the learning rate.\\n                           Can be a integer (every N iterations) or list of integers [0, 500, 1500]\\n        :param decay_coeff: The decay coefficient of the learning rate.\\n        :param epsilon: The perturbation budget\\n        :param dropout: Dropout to apply while training\\n        :param net_repeat: The number of times to repeat prediction on each network\\n        :param endtoend: True for end-to-end training. False for transfer learning.\\n        :param batch_size: Batch size.\\n        :param verbose: Show progress bars.\\n        \"\n    self.subsistute_networks: List['CLASSIFIER_NEURALNETWORK_TYPE'] = [classifier] if not isinstance(classifier, list) else classifier\n    super().__init__(classifier=self.subsistute_networks[0])\n    self.target = target\n    self.opt = opt\n    self.momentum = momentum\n    self.decay_iter = decay_iter\n    self.epsilon = epsilon\n    self.dropout = dropout\n    self.net_repeat = net_repeat\n    self.endtoend = endtoend\n    self.feature_layer = feature_layer\n    self.learning_rate = learning_rate\n    self.decay_coeff = decay_coeff\n    self.max_iter = max_iter\n    self.batch_size = batch_size\n    self.verbose = verbose\n    self._check_params()",
            "def __init__(self, classifier: Union['CLASSIFIER_NEURALNETWORK_TYPE', List['CLASSIFIER_NEURALNETWORK_TYPE']], target: np.ndarray, feature_layer: Union[Union[str, int], List[Union[str, int]]], opt: str='adam', max_iter: int=4000, learning_rate: float=0.04, momentum: float=0.9, decay_iter: Union[int, List[int]]=10000, decay_coeff: float=0.5, epsilon: float=0.1, dropout: float=0.3, net_repeat: int=1, endtoend: bool=True, batch_size: int=128, verbose: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n        Initialize an Feature Collision Clean-Label poisoning attack\\n\\n        :param classifier: The proxy classifiers used for the attack. Can be a single classifier or list of classifiers\\n                           with varying architectures.\\n        :param target: The target input(s) of shape (N, W, H, C) to misclassify at test time. Multiple targets will be\\n                       averaged.\\n        :param feature_layer: The name(s) of the feature representation layer(s).\\n        :param opt: The optimizer to use for the attack. Can be 'adam' or 'sgd'\\n        :param max_iter: The maximum number of iterations for the attack.\\n        :param learning_rate: The learning rate of clean-label attack optimization.\\n        :param momentum: The momentum of clean-label attack optimization.\\n        :param decay_iter: Which iterations to decay the learning rate.\\n                           Can be a integer (every N iterations) or list of integers [0, 500, 1500]\\n        :param decay_coeff: The decay coefficient of the learning rate.\\n        :param epsilon: The perturbation budget\\n        :param dropout: Dropout to apply while training\\n        :param net_repeat: The number of times to repeat prediction on each network\\n        :param endtoend: True for end-to-end training. False for transfer learning.\\n        :param batch_size: Batch size.\\n        :param verbose: Show progress bars.\\n        \"\n    self.subsistute_networks: List['CLASSIFIER_NEURALNETWORK_TYPE'] = [classifier] if not isinstance(classifier, list) else classifier\n    super().__init__(classifier=self.subsistute_networks[0])\n    self.target = target\n    self.opt = opt\n    self.momentum = momentum\n    self.decay_iter = decay_iter\n    self.epsilon = epsilon\n    self.dropout = dropout\n    self.net_repeat = net_repeat\n    self.endtoend = endtoend\n    self.feature_layer = feature_layer\n    self.learning_rate = learning_rate\n    self.decay_coeff = decay_coeff\n    self.max_iter = max_iter\n    self.batch_size = batch_size\n    self.verbose = verbose\n    self._check_params()",
            "def __init__(self, classifier: Union['CLASSIFIER_NEURALNETWORK_TYPE', List['CLASSIFIER_NEURALNETWORK_TYPE']], target: np.ndarray, feature_layer: Union[Union[str, int], List[Union[str, int]]], opt: str='adam', max_iter: int=4000, learning_rate: float=0.04, momentum: float=0.9, decay_iter: Union[int, List[int]]=10000, decay_coeff: float=0.5, epsilon: float=0.1, dropout: float=0.3, net_repeat: int=1, endtoend: bool=True, batch_size: int=128, verbose: bool=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n        Initialize an Feature Collision Clean-Label poisoning attack\\n\\n        :param classifier: The proxy classifiers used for the attack. Can be a single classifier or list of classifiers\\n                           with varying architectures.\\n        :param target: The target input(s) of shape (N, W, H, C) to misclassify at test time. Multiple targets will be\\n                       averaged.\\n        :param feature_layer: The name(s) of the feature representation layer(s).\\n        :param opt: The optimizer to use for the attack. Can be 'adam' or 'sgd'\\n        :param max_iter: The maximum number of iterations for the attack.\\n        :param learning_rate: The learning rate of clean-label attack optimization.\\n        :param momentum: The momentum of clean-label attack optimization.\\n        :param decay_iter: Which iterations to decay the learning rate.\\n                           Can be a integer (every N iterations) or list of integers [0, 500, 1500]\\n        :param decay_coeff: The decay coefficient of the learning rate.\\n        :param epsilon: The perturbation budget\\n        :param dropout: Dropout to apply while training\\n        :param net_repeat: The number of times to repeat prediction on each network\\n        :param endtoend: True for end-to-end training. False for transfer learning.\\n        :param batch_size: Batch size.\\n        :param verbose: Show progress bars.\\n        \"\n    self.subsistute_networks: List['CLASSIFIER_NEURALNETWORK_TYPE'] = [classifier] if not isinstance(classifier, list) else classifier\n    super().__init__(classifier=self.subsistute_networks[0])\n    self.target = target\n    self.opt = opt\n    self.momentum = momentum\n    self.decay_iter = decay_iter\n    self.epsilon = epsilon\n    self.dropout = dropout\n    self.net_repeat = net_repeat\n    self.endtoend = endtoend\n    self.feature_layer = feature_layer\n    self.learning_rate = learning_rate\n    self.decay_coeff = decay_coeff\n    self.max_iter = max_iter\n    self.batch_size = batch_size\n    self.verbose = verbose\n    self._check_params()"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, base_list):\n    super().__init__()\n    base_batch = torch.stack(base_list, 0)\n    self.poison = torch.nn.Parameter(base_batch.clone())",
        "mutated": [
            "def __init__(self, base_list):\n    if False:\n        i = 10\n    super().__init__()\n    base_batch = torch.stack(base_list, 0)\n    self.poison = torch.nn.Parameter(base_batch.clone())",
            "def __init__(self, base_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    base_batch = torch.stack(base_list, 0)\n    self.poison = torch.nn.Parameter(base_batch.clone())",
            "def __init__(self, base_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    base_batch = torch.stack(base_list, 0)\n    self.poison = torch.nn.Parameter(base_batch.clone())",
            "def __init__(self, base_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    base_batch = torch.stack(base_list, 0)\n    self.poison = torch.nn.Parameter(base_batch.clone())",
            "def __init__(self, base_list):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    base_batch = torch.stack(base_list, 0)\n    self.poison = torch.nn.Parameter(base_batch.clone())"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self):\n    \"\"\"Forward method.\"\"\"\n    return self.poison",
        "mutated": [
            "def forward(self):\n    if False:\n        i = 10\n    'Forward method.'\n    return self.poison",
            "def forward(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Forward method.'\n    return self.poison",
            "def forward(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Forward method.'\n    return self.poison",
            "def forward(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Forward method.'\n    return self.poison",
            "def forward(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Forward method.'\n    return self.poison"
        ]
    },
    {
        "func_name": "poison",
        "original": "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n        Iteratively finds optimal attack points starting at values at x\n\n        :param x: The base images to begin the poison process.\n        :param y: Target label\n        :return: An tuple holding the (poisoning examples, poisoning labels).\n        \"\"\"\n    import torch\n\n    class PoisonBatch(torch.nn.Module):\n        \"\"\"\n            Implementing this to work with PyTorch optimizers.\n            \"\"\"\n\n        def __init__(self, base_list):\n            super().__init__()\n            base_batch = torch.stack(base_list, 0)\n            self.poison = torch.nn.Parameter(base_batch.clone())\n\n        def forward(self):\n            \"\"\"Forward method.\"\"\"\n            return self.poison\n    base_tensor_list = [torch.from_numpy(sample).to(self.estimator.device) for sample in x]\n    poison_batch = PoisonBatch([torch.from_numpy(np.copy(sample)).to(self.estimator.device) for sample in x])\n    opt_method = self.opt.lower()\n    if opt_method == 'sgd':\n        logger.info('Using SGD to craft poison samples')\n        optimizer = torch.optim.SGD(poison_batch.parameters(), lr=self.learning_rate, momentum=self.momentum)\n    elif opt_method == 'adam':\n        logger.info('Using Adam to craft poison samples')\n        optimizer = torch.optim.Adam(poison_batch.parameters(), lr=self.learning_rate, betas=(self.momentum, 0.999))\n    base_tensor_batch = torch.stack(base_tensor_list, 0)\n    base_range01_batch = base_tensor_batch\n    target_feat_list = []\n    s_init_coeff_list = []\n    n_poisons = len(x)\n    s_coeff: Union['torch.Tensor', List['torch.Tensor']]\n    for (_, net) in enumerate(self.subsistute_networks):\n        if self.endtoend:\n            if isinstance(self.feature_layer, list):\n                block_feats = []\n                for layer in self.feature_layer:\n                    activations = net.get_activations(x, layer=layer, batch_size=self.batch_size, framework=True)\n                    if activations is not None:\n                        block_feats += [torch.stack([feat.detach() for feat in activations], 0)]\n                    else:\n                        raise ValueError('Activations are None.')\n            else:\n                layer_2: Union[int, str] = self.feature_layer\n                activations = net.get_activations(x, layer=layer_2, batch_size=self.batch_size, framework=True)\n                if activations is not None:\n                    block_feats = [feat.detach() for feat in activations]\n                else:\n                    raise ValueError('Activations are None.')\n            target_feat_list.append(block_feats)\n            s_coeff = [torch.ones(n_poisons, 1).to(self.estimator.device) / n_poisons for _ in range(len(block_feats))]\n        else:\n            if isinstance(self.feature_layer, list):\n                raise NotImplementedError\n            layer_3: Union[int, str] = self.feature_layer\n            activations = net.get_activations(x, layer=layer_3, batch_size=self.batch_size, framework=True)\n            if activations is not None:\n                target_feat_list.append(activations.detach())\n            else:\n                raise ValueError('Activations are None.')\n            s_coeff = torch.ones(n_poisons, 1).to(self.estimator.device) / n_poisons\n        s_init_coeff_list.append(s_coeff)\n    for ite in trange(self.max_iter):\n        if ite % self.decay_iter == 0 and ite != 0:\n            for param_group in optimizer.param_groups:\n                param_group['lr'] *= self.decay_coeff\n            print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Iteration {ite}, Adjusted lr to {self.learning_rate:.2e}\")\n        poison_batch.zero_grad()\n        total_loss = loss_from_center(self.subsistute_networks, target_feat_list, poison_batch, self.net_repeat, self.endtoend, self.feature_layer)\n        total_loss.backward()\n        optimizer.step()\n        perturb_range01 = torch.clamp(poison_batch.poison.data - base_tensor_batch, -self.epsilon, self.epsilon)\n        perturbed_range01 = torch.clamp(base_range01_batch.data + perturb_range01.data, self.estimator.clip_values[0], self.estimator.clip_values[1])\n        poison_batch.poison.data = perturbed_range01\n    if y is None:\n        raise ValueError('You must pass in the target label as y')\n    return get_poison_tuples(poison_batch, y)",
        "mutated": [
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n    '\\n        Iteratively finds optimal attack points starting at values at x\\n\\n        :param x: The base images to begin the poison process.\\n        :param y: Target label\\n        :return: An tuple holding the (poisoning examples, poisoning labels).\\n        '\n    import torch\n\n    class PoisonBatch(torch.nn.Module):\n        \"\"\"\n            Implementing this to work with PyTorch optimizers.\n            \"\"\"\n\n        def __init__(self, base_list):\n            super().__init__()\n            base_batch = torch.stack(base_list, 0)\n            self.poison = torch.nn.Parameter(base_batch.clone())\n\n        def forward(self):\n            \"\"\"Forward method.\"\"\"\n            return self.poison\n    base_tensor_list = [torch.from_numpy(sample).to(self.estimator.device) for sample in x]\n    poison_batch = PoisonBatch([torch.from_numpy(np.copy(sample)).to(self.estimator.device) for sample in x])\n    opt_method = self.opt.lower()\n    if opt_method == 'sgd':\n        logger.info('Using SGD to craft poison samples')\n        optimizer = torch.optim.SGD(poison_batch.parameters(), lr=self.learning_rate, momentum=self.momentum)\n    elif opt_method == 'adam':\n        logger.info('Using Adam to craft poison samples')\n        optimizer = torch.optim.Adam(poison_batch.parameters(), lr=self.learning_rate, betas=(self.momentum, 0.999))\n    base_tensor_batch = torch.stack(base_tensor_list, 0)\n    base_range01_batch = base_tensor_batch\n    target_feat_list = []\n    s_init_coeff_list = []\n    n_poisons = len(x)\n    s_coeff: Union['torch.Tensor', List['torch.Tensor']]\n    for (_, net) in enumerate(self.subsistute_networks):\n        if self.endtoend:\n            if isinstance(self.feature_layer, list):\n                block_feats = []\n                for layer in self.feature_layer:\n                    activations = net.get_activations(x, layer=layer, batch_size=self.batch_size, framework=True)\n                    if activations is not None:\n                        block_feats += [torch.stack([feat.detach() for feat in activations], 0)]\n                    else:\n                        raise ValueError('Activations are None.')\n            else:\n                layer_2: Union[int, str] = self.feature_layer\n                activations = net.get_activations(x, layer=layer_2, batch_size=self.batch_size, framework=True)\n                if activations is not None:\n                    block_feats = [feat.detach() for feat in activations]\n                else:\n                    raise ValueError('Activations are None.')\n            target_feat_list.append(block_feats)\n            s_coeff = [torch.ones(n_poisons, 1).to(self.estimator.device) / n_poisons for _ in range(len(block_feats))]\n        else:\n            if isinstance(self.feature_layer, list):\n                raise NotImplementedError\n            layer_3: Union[int, str] = self.feature_layer\n            activations = net.get_activations(x, layer=layer_3, batch_size=self.batch_size, framework=True)\n            if activations is not None:\n                target_feat_list.append(activations.detach())\n            else:\n                raise ValueError('Activations are None.')\n            s_coeff = torch.ones(n_poisons, 1).to(self.estimator.device) / n_poisons\n        s_init_coeff_list.append(s_coeff)\n    for ite in trange(self.max_iter):\n        if ite % self.decay_iter == 0 and ite != 0:\n            for param_group in optimizer.param_groups:\n                param_group['lr'] *= self.decay_coeff\n            print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Iteration {ite}, Adjusted lr to {self.learning_rate:.2e}\")\n        poison_batch.zero_grad()\n        total_loss = loss_from_center(self.subsistute_networks, target_feat_list, poison_batch, self.net_repeat, self.endtoend, self.feature_layer)\n        total_loss.backward()\n        optimizer.step()\n        perturb_range01 = torch.clamp(poison_batch.poison.data - base_tensor_batch, -self.epsilon, self.epsilon)\n        perturbed_range01 = torch.clamp(base_range01_batch.data + perturb_range01.data, self.estimator.clip_values[0], self.estimator.clip_values[1])\n        poison_batch.poison.data = perturbed_range01\n    if y is None:\n        raise ValueError('You must pass in the target label as y')\n    return get_poison_tuples(poison_batch, y)",
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Iteratively finds optimal attack points starting at values at x\\n\\n        :param x: The base images to begin the poison process.\\n        :param y: Target label\\n        :return: An tuple holding the (poisoning examples, poisoning labels).\\n        '\n    import torch\n\n    class PoisonBatch(torch.nn.Module):\n        \"\"\"\n            Implementing this to work with PyTorch optimizers.\n            \"\"\"\n\n        def __init__(self, base_list):\n            super().__init__()\n            base_batch = torch.stack(base_list, 0)\n            self.poison = torch.nn.Parameter(base_batch.clone())\n\n        def forward(self):\n            \"\"\"Forward method.\"\"\"\n            return self.poison\n    base_tensor_list = [torch.from_numpy(sample).to(self.estimator.device) for sample in x]\n    poison_batch = PoisonBatch([torch.from_numpy(np.copy(sample)).to(self.estimator.device) for sample in x])\n    opt_method = self.opt.lower()\n    if opt_method == 'sgd':\n        logger.info('Using SGD to craft poison samples')\n        optimizer = torch.optim.SGD(poison_batch.parameters(), lr=self.learning_rate, momentum=self.momentum)\n    elif opt_method == 'adam':\n        logger.info('Using Adam to craft poison samples')\n        optimizer = torch.optim.Adam(poison_batch.parameters(), lr=self.learning_rate, betas=(self.momentum, 0.999))\n    base_tensor_batch = torch.stack(base_tensor_list, 0)\n    base_range01_batch = base_tensor_batch\n    target_feat_list = []\n    s_init_coeff_list = []\n    n_poisons = len(x)\n    s_coeff: Union['torch.Tensor', List['torch.Tensor']]\n    for (_, net) in enumerate(self.subsistute_networks):\n        if self.endtoend:\n            if isinstance(self.feature_layer, list):\n                block_feats = []\n                for layer in self.feature_layer:\n                    activations = net.get_activations(x, layer=layer, batch_size=self.batch_size, framework=True)\n                    if activations is not None:\n                        block_feats += [torch.stack([feat.detach() for feat in activations], 0)]\n                    else:\n                        raise ValueError('Activations are None.')\n            else:\n                layer_2: Union[int, str] = self.feature_layer\n                activations = net.get_activations(x, layer=layer_2, batch_size=self.batch_size, framework=True)\n                if activations is not None:\n                    block_feats = [feat.detach() for feat in activations]\n                else:\n                    raise ValueError('Activations are None.')\n            target_feat_list.append(block_feats)\n            s_coeff = [torch.ones(n_poisons, 1).to(self.estimator.device) / n_poisons for _ in range(len(block_feats))]\n        else:\n            if isinstance(self.feature_layer, list):\n                raise NotImplementedError\n            layer_3: Union[int, str] = self.feature_layer\n            activations = net.get_activations(x, layer=layer_3, batch_size=self.batch_size, framework=True)\n            if activations is not None:\n                target_feat_list.append(activations.detach())\n            else:\n                raise ValueError('Activations are None.')\n            s_coeff = torch.ones(n_poisons, 1).to(self.estimator.device) / n_poisons\n        s_init_coeff_list.append(s_coeff)\n    for ite in trange(self.max_iter):\n        if ite % self.decay_iter == 0 and ite != 0:\n            for param_group in optimizer.param_groups:\n                param_group['lr'] *= self.decay_coeff\n            print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Iteration {ite}, Adjusted lr to {self.learning_rate:.2e}\")\n        poison_batch.zero_grad()\n        total_loss = loss_from_center(self.subsistute_networks, target_feat_list, poison_batch, self.net_repeat, self.endtoend, self.feature_layer)\n        total_loss.backward()\n        optimizer.step()\n        perturb_range01 = torch.clamp(poison_batch.poison.data - base_tensor_batch, -self.epsilon, self.epsilon)\n        perturbed_range01 = torch.clamp(base_range01_batch.data + perturb_range01.data, self.estimator.clip_values[0], self.estimator.clip_values[1])\n        poison_batch.poison.data = perturbed_range01\n    if y is None:\n        raise ValueError('You must pass in the target label as y')\n    return get_poison_tuples(poison_batch, y)",
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Iteratively finds optimal attack points starting at values at x\\n\\n        :param x: The base images to begin the poison process.\\n        :param y: Target label\\n        :return: An tuple holding the (poisoning examples, poisoning labels).\\n        '\n    import torch\n\n    class PoisonBatch(torch.nn.Module):\n        \"\"\"\n            Implementing this to work with PyTorch optimizers.\n            \"\"\"\n\n        def __init__(self, base_list):\n            super().__init__()\n            base_batch = torch.stack(base_list, 0)\n            self.poison = torch.nn.Parameter(base_batch.clone())\n\n        def forward(self):\n            \"\"\"Forward method.\"\"\"\n            return self.poison\n    base_tensor_list = [torch.from_numpy(sample).to(self.estimator.device) for sample in x]\n    poison_batch = PoisonBatch([torch.from_numpy(np.copy(sample)).to(self.estimator.device) for sample in x])\n    opt_method = self.opt.lower()\n    if opt_method == 'sgd':\n        logger.info('Using SGD to craft poison samples')\n        optimizer = torch.optim.SGD(poison_batch.parameters(), lr=self.learning_rate, momentum=self.momentum)\n    elif opt_method == 'adam':\n        logger.info('Using Adam to craft poison samples')\n        optimizer = torch.optim.Adam(poison_batch.parameters(), lr=self.learning_rate, betas=(self.momentum, 0.999))\n    base_tensor_batch = torch.stack(base_tensor_list, 0)\n    base_range01_batch = base_tensor_batch\n    target_feat_list = []\n    s_init_coeff_list = []\n    n_poisons = len(x)\n    s_coeff: Union['torch.Tensor', List['torch.Tensor']]\n    for (_, net) in enumerate(self.subsistute_networks):\n        if self.endtoend:\n            if isinstance(self.feature_layer, list):\n                block_feats = []\n                for layer in self.feature_layer:\n                    activations = net.get_activations(x, layer=layer, batch_size=self.batch_size, framework=True)\n                    if activations is not None:\n                        block_feats += [torch.stack([feat.detach() for feat in activations], 0)]\n                    else:\n                        raise ValueError('Activations are None.')\n            else:\n                layer_2: Union[int, str] = self.feature_layer\n                activations = net.get_activations(x, layer=layer_2, batch_size=self.batch_size, framework=True)\n                if activations is not None:\n                    block_feats = [feat.detach() for feat in activations]\n                else:\n                    raise ValueError('Activations are None.')\n            target_feat_list.append(block_feats)\n            s_coeff = [torch.ones(n_poisons, 1).to(self.estimator.device) / n_poisons for _ in range(len(block_feats))]\n        else:\n            if isinstance(self.feature_layer, list):\n                raise NotImplementedError\n            layer_3: Union[int, str] = self.feature_layer\n            activations = net.get_activations(x, layer=layer_3, batch_size=self.batch_size, framework=True)\n            if activations is not None:\n                target_feat_list.append(activations.detach())\n            else:\n                raise ValueError('Activations are None.')\n            s_coeff = torch.ones(n_poisons, 1).to(self.estimator.device) / n_poisons\n        s_init_coeff_list.append(s_coeff)\n    for ite in trange(self.max_iter):\n        if ite % self.decay_iter == 0 and ite != 0:\n            for param_group in optimizer.param_groups:\n                param_group['lr'] *= self.decay_coeff\n            print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Iteration {ite}, Adjusted lr to {self.learning_rate:.2e}\")\n        poison_batch.zero_grad()\n        total_loss = loss_from_center(self.subsistute_networks, target_feat_list, poison_batch, self.net_repeat, self.endtoend, self.feature_layer)\n        total_loss.backward()\n        optimizer.step()\n        perturb_range01 = torch.clamp(poison_batch.poison.data - base_tensor_batch, -self.epsilon, self.epsilon)\n        perturbed_range01 = torch.clamp(base_range01_batch.data + perturb_range01.data, self.estimator.clip_values[0], self.estimator.clip_values[1])\n        poison_batch.poison.data = perturbed_range01\n    if y is None:\n        raise ValueError('You must pass in the target label as y')\n    return get_poison_tuples(poison_batch, y)",
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Iteratively finds optimal attack points starting at values at x\\n\\n        :param x: The base images to begin the poison process.\\n        :param y: Target label\\n        :return: An tuple holding the (poisoning examples, poisoning labels).\\n        '\n    import torch\n\n    class PoisonBatch(torch.nn.Module):\n        \"\"\"\n            Implementing this to work with PyTorch optimizers.\n            \"\"\"\n\n        def __init__(self, base_list):\n            super().__init__()\n            base_batch = torch.stack(base_list, 0)\n            self.poison = torch.nn.Parameter(base_batch.clone())\n\n        def forward(self):\n            \"\"\"Forward method.\"\"\"\n            return self.poison\n    base_tensor_list = [torch.from_numpy(sample).to(self.estimator.device) for sample in x]\n    poison_batch = PoisonBatch([torch.from_numpy(np.copy(sample)).to(self.estimator.device) for sample in x])\n    opt_method = self.opt.lower()\n    if opt_method == 'sgd':\n        logger.info('Using SGD to craft poison samples')\n        optimizer = torch.optim.SGD(poison_batch.parameters(), lr=self.learning_rate, momentum=self.momentum)\n    elif opt_method == 'adam':\n        logger.info('Using Adam to craft poison samples')\n        optimizer = torch.optim.Adam(poison_batch.parameters(), lr=self.learning_rate, betas=(self.momentum, 0.999))\n    base_tensor_batch = torch.stack(base_tensor_list, 0)\n    base_range01_batch = base_tensor_batch\n    target_feat_list = []\n    s_init_coeff_list = []\n    n_poisons = len(x)\n    s_coeff: Union['torch.Tensor', List['torch.Tensor']]\n    for (_, net) in enumerate(self.subsistute_networks):\n        if self.endtoend:\n            if isinstance(self.feature_layer, list):\n                block_feats = []\n                for layer in self.feature_layer:\n                    activations = net.get_activations(x, layer=layer, batch_size=self.batch_size, framework=True)\n                    if activations is not None:\n                        block_feats += [torch.stack([feat.detach() for feat in activations], 0)]\n                    else:\n                        raise ValueError('Activations are None.')\n            else:\n                layer_2: Union[int, str] = self.feature_layer\n                activations = net.get_activations(x, layer=layer_2, batch_size=self.batch_size, framework=True)\n                if activations is not None:\n                    block_feats = [feat.detach() for feat in activations]\n                else:\n                    raise ValueError('Activations are None.')\n            target_feat_list.append(block_feats)\n            s_coeff = [torch.ones(n_poisons, 1).to(self.estimator.device) / n_poisons for _ in range(len(block_feats))]\n        else:\n            if isinstance(self.feature_layer, list):\n                raise NotImplementedError\n            layer_3: Union[int, str] = self.feature_layer\n            activations = net.get_activations(x, layer=layer_3, batch_size=self.batch_size, framework=True)\n            if activations is not None:\n                target_feat_list.append(activations.detach())\n            else:\n                raise ValueError('Activations are None.')\n            s_coeff = torch.ones(n_poisons, 1).to(self.estimator.device) / n_poisons\n        s_init_coeff_list.append(s_coeff)\n    for ite in trange(self.max_iter):\n        if ite % self.decay_iter == 0 and ite != 0:\n            for param_group in optimizer.param_groups:\n                param_group['lr'] *= self.decay_coeff\n            print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Iteration {ite}, Adjusted lr to {self.learning_rate:.2e}\")\n        poison_batch.zero_grad()\n        total_loss = loss_from_center(self.subsistute_networks, target_feat_list, poison_batch, self.net_repeat, self.endtoend, self.feature_layer)\n        total_loss.backward()\n        optimizer.step()\n        perturb_range01 = torch.clamp(poison_batch.poison.data - base_tensor_batch, -self.epsilon, self.epsilon)\n        perturbed_range01 = torch.clamp(base_range01_batch.data + perturb_range01.data, self.estimator.clip_values[0], self.estimator.clip_values[1])\n        poison_batch.poison.data = perturbed_range01\n    if y is None:\n        raise ValueError('You must pass in the target label as y')\n    return get_poison_tuples(poison_batch, y)",
            "def poison(self, x: np.ndarray, y: Optional[np.ndarray]=None, **kwargs) -> Tuple[np.ndarray, np.ndarray]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Iteratively finds optimal attack points starting at values at x\\n\\n        :param x: The base images to begin the poison process.\\n        :param y: Target label\\n        :return: An tuple holding the (poisoning examples, poisoning labels).\\n        '\n    import torch\n\n    class PoisonBatch(torch.nn.Module):\n        \"\"\"\n            Implementing this to work with PyTorch optimizers.\n            \"\"\"\n\n        def __init__(self, base_list):\n            super().__init__()\n            base_batch = torch.stack(base_list, 0)\n            self.poison = torch.nn.Parameter(base_batch.clone())\n\n        def forward(self):\n            \"\"\"Forward method.\"\"\"\n            return self.poison\n    base_tensor_list = [torch.from_numpy(sample).to(self.estimator.device) for sample in x]\n    poison_batch = PoisonBatch([torch.from_numpy(np.copy(sample)).to(self.estimator.device) for sample in x])\n    opt_method = self.opt.lower()\n    if opt_method == 'sgd':\n        logger.info('Using SGD to craft poison samples')\n        optimizer = torch.optim.SGD(poison_batch.parameters(), lr=self.learning_rate, momentum=self.momentum)\n    elif opt_method == 'adam':\n        logger.info('Using Adam to craft poison samples')\n        optimizer = torch.optim.Adam(poison_batch.parameters(), lr=self.learning_rate, betas=(self.momentum, 0.999))\n    base_tensor_batch = torch.stack(base_tensor_list, 0)\n    base_range01_batch = base_tensor_batch\n    target_feat_list = []\n    s_init_coeff_list = []\n    n_poisons = len(x)\n    s_coeff: Union['torch.Tensor', List['torch.Tensor']]\n    for (_, net) in enumerate(self.subsistute_networks):\n        if self.endtoend:\n            if isinstance(self.feature_layer, list):\n                block_feats = []\n                for layer in self.feature_layer:\n                    activations = net.get_activations(x, layer=layer, batch_size=self.batch_size, framework=True)\n                    if activations is not None:\n                        block_feats += [torch.stack([feat.detach() for feat in activations], 0)]\n                    else:\n                        raise ValueError('Activations are None.')\n            else:\n                layer_2: Union[int, str] = self.feature_layer\n                activations = net.get_activations(x, layer=layer_2, batch_size=self.batch_size, framework=True)\n                if activations is not None:\n                    block_feats = [feat.detach() for feat in activations]\n                else:\n                    raise ValueError('Activations are None.')\n            target_feat_list.append(block_feats)\n            s_coeff = [torch.ones(n_poisons, 1).to(self.estimator.device) / n_poisons for _ in range(len(block_feats))]\n        else:\n            if isinstance(self.feature_layer, list):\n                raise NotImplementedError\n            layer_3: Union[int, str] = self.feature_layer\n            activations = net.get_activations(x, layer=layer_3, batch_size=self.batch_size, framework=True)\n            if activations is not None:\n                target_feat_list.append(activations.detach())\n            else:\n                raise ValueError('Activations are None.')\n            s_coeff = torch.ones(n_poisons, 1).to(self.estimator.device) / n_poisons\n        s_init_coeff_list.append(s_coeff)\n    for ite in trange(self.max_iter):\n        if ite % self.decay_iter == 0 and ite != 0:\n            for param_group in optimizer.param_groups:\n                param_group['lr'] *= self.decay_coeff\n            print(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} Iteration {ite}, Adjusted lr to {self.learning_rate:.2e}\")\n        poison_batch.zero_grad()\n        total_loss = loss_from_center(self.subsistute_networks, target_feat_list, poison_batch, self.net_repeat, self.endtoend, self.feature_layer)\n        total_loss.backward()\n        optimizer.step()\n        perturb_range01 = torch.clamp(poison_batch.poison.data - base_tensor_batch, -self.epsilon, self.epsilon)\n        perturbed_range01 = torch.clamp(base_range01_batch.data + perturb_range01.data, self.estimator.clip_values[0], self.estimator.clip_values[1])\n        poison_batch.poison.data = perturbed_range01\n    if y is None:\n        raise ValueError('You must pass in the target label as y')\n    return get_poison_tuples(poison_batch, y)"
        ]
    },
    {
        "func_name": "_check_params",
        "original": "def _check_params(self) -> None:\n    if self.learning_rate <= 0:\n        raise ValueError('Learning rate must be strictly positive')\n    if self.max_iter < 1:\n        raise ValueError('Value of max_iter at least 1')\n    if not isinstance(self.feature_layer, (str, int, list)):\n        raise TypeError('Feature layer should be a string or int or list of string or int')\n    if self.opt.lower() not in ['adam', 'sgd']:\n        raise ValueError(\"Optimizer must be 'adam' or 'sgd'\")\n    if not 0 <= self.momentum <= 1:\n        raise ValueError('Momentum must be between 0 and 1')\n    if isinstance(self.decay_iter, int) and self.decay_iter < 0:\n        raise ValueError('decay_iter must be at least 0')\n    if isinstance(self.decay_iter, list) and (not all((isinstance(decay_iter, int) and decay_iter > 0 for decay_iter in self.decay_iter))):\n        raise ValueError('decay_iter is not a list of positive integers')\n    if self.epsilon <= 0:\n        raise ValueError('epsilon must be at least 0')\n    if not 0 <= self.dropout <= 1:\n        raise ValueError('dropout must be between 0 and 1')\n    if self.net_repeat < 1:\n        raise ValueError('net_repeat must be at least 1')\n    if isinstance(self.feature_layer, list):\n        for layer in self.feature_layer:\n            if isinstance(layer, int):\n                if not 0 <= layer < len(self.estimator.layer_names):\n                    raise ValueError('feature_layer is not list of positive integers')\n            elif not isinstance(layer, str):\n                raise ValueError('feature_layer is not list of strings')\n    if isinstance(self.feature_layer, int):\n        if not 0 <= self.feature_layer < len(self.estimator.layer_names):\n            raise ValueError('feature_layer is not positive integer')\n    if not 0 <= self.decay_coeff <= 1:\n        raise ValueError('Decay coefficient must be between zero and one')\n    if not isinstance(self.batch_size, int) or self.batch_size <= 0:\n        raise ValueError('batch_size must be a positive integer')",
        "mutated": [
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n    if self.learning_rate <= 0:\n        raise ValueError('Learning rate must be strictly positive')\n    if self.max_iter < 1:\n        raise ValueError('Value of max_iter at least 1')\n    if not isinstance(self.feature_layer, (str, int, list)):\n        raise TypeError('Feature layer should be a string or int or list of string or int')\n    if self.opt.lower() not in ['adam', 'sgd']:\n        raise ValueError(\"Optimizer must be 'adam' or 'sgd'\")\n    if not 0 <= self.momentum <= 1:\n        raise ValueError('Momentum must be between 0 and 1')\n    if isinstance(self.decay_iter, int) and self.decay_iter < 0:\n        raise ValueError('decay_iter must be at least 0')\n    if isinstance(self.decay_iter, list) and (not all((isinstance(decay_iter, int) and decay_iter > 0 for decay_iter in self.decay_iter))):\n        raise ValueError('decay_iter is not a list of positive integers')\n    if self.epsilon <= 0:\n        raise ValueError('epsilon must be at least 0')\n    if not 0 <= self.dropout <= 1:\n        raise ValueError('dropout must be between 0 and 1')\n    if self.net_repeat < 1:\n        raise ValueError('net_repeat must be at least 1')\n    if isinstance(self.feature_layer, list):\n        for layer in self.feature_layer:\n            if isinstance(layer, int):\n                if not 0 <= layer < len(self.estimator.layer_names):\n                    raise ValueError('feature_layer is not list of positive integers')\n            elif not isinstance(layer, str):\n                raise ValueError('feature_layer is not list of strings')\n    if isinstance(self.feature_layer, int):\n        if not 0 <= self.feature_layer < len(self.estimator.layer_names):\n            raise ValueError('feature_layer is not positive integer')\n    if not 0 <= self.decay_coeff <= 1:\n        raise ValueError('Decay coefficient must be between zero and one')\n    if not isinstance(self.batch_size, int) or self.batch_size <= 0:\n        raise ValueError('batch_size must be a positive integer')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if self.learning_rate <= 0:\n        raise ValueError('Learning rate must be strictly positive')\n    if self.max_iter < 1:\n        raise ValueError('Value of max_iter at least 1')\n    if not isinstance(self.feature_layer, (str, int, list)):\n        raise TypeError('Feature layer should be a string or int or list of string or int')\n    if self.opt.lower() not in ['adam', 'sgd']:\n        raise ValueError(\"Optimizer must be 'adam' or 'sgd'\")\n    if not 0 <= self.momentum <= 1:\n        raise ValueError('Momentum must be between 0 and 1')\n    if isinstance(self.decay_iter, int) and self.decay_iter < 0:\n        raise ValueError('decay_iter must be at least 0')\n    if isinstance(self.decay_iter, list) and (not all((isinstance(decay_iter, int) and decay_iter > 0 for decay_iter in self.decay_iter))):\n        raise ValueError('decay_iter is not a list of positive integers')\n    if self.epsilon <= 0:\n        raise ValueError('epsilon must be at least 0')\n    if not 0 <= self.dropout <= 1:\n        raise ValueError('dropout must be between 0 and 1')\n    if self.net_repeat < 1:\n        raise ValueError('net_repeat must be at least 1')\n    if isinstance(self.feature_layer, list):\n        for layer in self.feature_layer:\n            if isinstance(layer, int):\n                if not 0 <= layer < len(self.estimator.layer_names):\n                    raise ValueError('feature_layer is not list of positive integers')\n            elif not isinstance(layer, str):\n                raise ValueError('feature_layer is not list of strings')\n    if isinstance(self.feature_layer, int):\n        if not 0 <= self.feature_layer < len(self.estimator.layer_names):\n            raise ValueError('feature_layer is not positive integer')\n    if not 0 <= self.decay_coeff <= 1:\n        raise ValueError('Decay coefficient must be between zero and one')\n    if not isinstance(self.batch_size, int) or self.batch_size <= 0:\n        raise ValueError('batch_size must be a positive integer')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if self.learning_rate <= 0:\n        raise ValueError('Learning rate must be strictly positive')\n    if self.max_iter < 1:\n        raise ValueError('Value of max_iter at least 1')\n    if not isinstance(self.feature_layer, (str, int, list)):\n        raise TypeError('Feature layer should be a string or int or list of string or int')\n    if self.opt.lower() not in ['adam', 'sgd']:\n        raise ValueError(\"Optimizer must be 'adam' or 'sgd'\")\n    if not 0 <= self.momentum <= 1:\n        raise ValueError('Momentum must be between 0 and 1')\n    if isinstance(self.decay_iter, int) and self.decay_iter < 0:\n        raise ValueError('decay_iter must be at least 0')\n    if isinstance(self.decay_iter, list) and (not all((isinstance(decay_iter, int) and decay_iter > 0 for decay_iter in self.decay_iter))):\n        raise ValueError('decay_iter is not a list of positive integers')\n    if self.epsilon <= 0:\n        raise ValueError('epsilon must be at least 0')\n    if not 0 <= self.dropout <= 1:\n        raise ValueError('dropout must be between 0 and 1')\n    if self.net_repeat < 1:\n        raise ValueError('net_repeat must be at least 1')\n    if isinstance(self.feature_layer, list):\n        for layer in self.feature_layer:\n            if isinstance(layer, int):\n                if not 0 <= layer < len(self.estimator.layer_names):\n                    raise ValueError('feature_layer is not list of positive integers')\n            elif not isinstance(layer, str):\n                raise ValueError('feature_layer is not list of strings')\n    if isinstance(self.feature_layer, int):\n        if not 0 <= self.feature_layer < len(self.estimator.layer_names):\n            raise ValueError('feature_layer is not positive integer')\n    if not 0 <= self.decay_coeff <= 1:\n        raise ValueError('Decay coefficient must be between zero and one')\n    if not isinstance(self.batch_size, int) or self.batch_size <= 0:\n        raise ValueError('batch_size must be a positive integer')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if self.learning_rate <= 0:\n        raise ValueError('Learning rate must be strictly positive')\n    if self.max_iter < 1:\n        raise ValueError('Value of max_iter at least 1')\n    if not isinstance(self.feature_layer, (str, int, list)):\n        raise TypeError('Feature layer should be a string or int or list of string or int')\n    if self.opt.lower() not in ['adam', 'sgd']:\n        raise ValueError(\"Optimizer must be 'adam' or 'sgd'\")\n    if not 0 <= self.momentum <= 1:\n        raise ValueError('Momentum must be between 0 and 1')\n    if isinstance(self.decay_iter, int) and self.decay_iter < 0:\n        raise ValueError('decay_iter must be at least 0')\n    if isinstance(self.decay_iter, list) and (not all((isinstance(decay_iter, int) and decay_iter > 0 for decay_iter in self.decay_iter))):\n        raise ValueError('decay_iter is not a list of positive integers')\n    if self.epsilon <= 0:\n        raise ValueError('epsilon must be at least 0')\n    if not 0 <= self.dropout <= 1:\n        raise ValueError('dropout must be between 0 and 1')\n    if self.net_repeat < 1:\n        raise ValueError('net_repeat must be at least 1')\n    if isinstance(self.feature_layer, list):\n        for layer in self.feature_layer:\n            if isinstance(layer, int):\n                if not 0 <= layer < len(self.estimator.layer_names):\n                    raise ValueError('feature_layer is not list of positive integers')\n            elif not isinstance(layer, str):\n                raise ValueError('feature_layer is not list of strings')\n    if isinstance(self.feature_layer, int):\n        if not 0 <= self.feature_layer < len(self.estimator.layer_names):\n            raise ValueError('feature_layer is not positive integer')\n    if not 0 <= self.decay_coeff <= 1:\n        raise ValueError('Decay coefficient must be between zero and one')\n    if not isinstance(self.batch_size, int) or self.batch_size <= 0:\n        raise ValueError('batch_size must be a positive integer')",
            "def _check_params(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if self.learning_rate <= 0:\n        raise ValueError('Learning rate must be strictly positive')\n    if self.max_iter < 1:\n        raise ValueError('Value of max_iter at least 1')\n    if not isinstance(self.feature_layer, (str, int, list)):\n        raise TypeError('Feature layer should be a string or int or list of string or int')\n    if self.opt.lower() not in ['adam', 'sgd']:\n        raise ValueError(\"Optimizer must be 'adam' or 'sgd'\")\n    if not 0 <= self.momentum <= 1:\n        raise ValueError('Momentum must be between 0 and 1')\n    if isinstance(self.decay_iter, int) and self.decay_iter < 0:\n        raise ValueError('decay_iter must be at least 0')\n    if isinstance(self.decay_iter, list) and (not all((isinstance(decay_iter, int) and decay_iter > 0 for decay_iter in self.decay_iter))):\n        raise ValueError('decay_iter is not a list of positive integers')\n    if self.epsilon <= 0:\n        raise ValueError('epsilon must be at least 0')\n    if not 0 <= self.dropout <= 1:\n        raise ValueError('dropout must be between 0 and 1')\n    if self.net_repeat < 1:\n        raise ValueError('net_repeat must be at least 1')\n    if isinstance(self.feature_layer, list):\n        for layer in self.feature_layer:\n            if isinstance(layer, int):\n                if not 0 <= layer < len(self.estimator.layer_names):\n                    raise ValueError('feature_layer is not list of positive integers')\n            elif not isinstance(layer, str):\n                raise ValueError('feature_layer is not list of strings')\n    if isinstance(self.feature_layer, int):\n        if not 0 <= self.feature_layer < len(self.estimator.layer_names):\n            raise ValueError('feature_layer is not positive integer')\n    if not 0 <= self.decay_coeff <= 1:\n        raise ValueError('Decay coefficient must be between zero and one')\n    if not isinstance(self.batch_size, int) or self.batch_size <= 0:\n        raise ValueError('batch_size must be a positive integer')"
        ]
    },
    {
        "func_name": "get_poison_tuples",
        "original": "def get_poison_tuples(poison_batch, poison_label):\n    \"\"\"\n    Includes the labels\n    \"\"\"\n    poison = [poison_batch.poison.data[num_p].unsqueeze(0).detach().cpu().numpy() for num_p in range(poison_batch.poison.size(0))]\n    return (np.vstack(poison), poison_label)",
        "mutated": [
            "def get_poison_tuples(poison_batch, poison_label):\n    if False:\n        i = 10\n    '\\n    Includes the labels\\n    '\n    poison = [poison_batch.poison.data[num_p].unsqueeze(0).detach().cpu().numpy() for num_p in range(poison_batch.poison.size(0))]\n    return (np.vstack(poison), poison_label)",
            "def get_poison_tuples(poison_batch, poison_label):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Includes the labels\\n    '\n    poison = [poison_batch.poison.data[num_p].unsqueeze(0).detach().cpu().numpy() for num_p in range(poison_batch.poison.size(0))]\n    return (np.vstack(poison), poison_label)",
            "def get_poison_tuples(poison_batch, poison_label):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Includes the labels\\n    '\n    poison = [poison_batch.poison.data[num_p].unsqueeze(0).detach().cpu().numpy() for num_p in range(poison_batch.poison.size(0))]\n    return (np.vstack(poison), poison_label)",
            "def get_poison_tuples(poison_batch, poison_label):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Includes the labels\\n    '\n    poison = [poison_batch.poison.data[num_p].unsqueeze(0).detach().cpu().numpy() for num_p in range(poison_batch.poison.size(0))]\n    return (np.vstack(poison), poison_label)",
            "def get_poison_tuples(poison_batch, poison_label):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Includes the labels\\n    '\n    poison = [poison_batch.poison.data[num_p].unsqueeze(0).detach().cpu().numpy() for num_p in range(poison_batch.poison.size(0))]\n    return (np.vstack(poison), poison_label)"
        ]
    },
    {
        "func_name": "loss_from_center",
        "original": "def loss_from_center(subs_net_list, target_feat_list, poison_batch, net_repeat, end2end, feature_layer) -> 'torch.Tensor':\n    \"\"\"\n    Calculate loss from center.\n    \"\"\"\n    import torch\n    if end2end:\n        loss = torch.tensor(0.0)\n        for (net, center_feats) in zip(subs_net_list, target_feat_list):\n            poisons_feats: Union[List[float], 'torch.Tensor', np.ndarray]\n            if net_repeat > 1:\n                poisons_feats_repeats = [net.get_activations(poison_batch(), layer=feature_layer, framework=True) for _ in range(net_repeat)]\n                block_num = len(poisons_feats_repeats[0])\n                poisons_feats = []\n                for block_idx in range(block_num):\n                    poisons_feats.append(sum([poisons_feat_r[block_idx] for poisons_feat_r in poisons_feats_repeats]) / net_repeat)\n            elif net_repeat == 1:\n                if isinstance(feature_layer, list):\n                    poisons_feats = [torch.flatten(net.get_activations(poison_batch(), layer=layer, framework=True), 0) for layer in feature_layer]\n                else:\n                    poisons_feats = net.get_activations(poison_batch(), layer=feature_layer, framework=True)\n            else:\n                assert False, f'net_repeat set to {net_repeat}'\n            net_loss = torch.tensor(0.0)\n            for (pfeat, cfeat) in zip(poisons_feats, center_feats):\n                diff = torch.mean(pfeat, dim=0) - cfeat\n                diff_norm = torch.norm(diff, dim=0)\n                cfeat_norm = torch.norm(cfeat, dim=0)\n                diff_norm = diff_norm / cfeat_norm\n                net_loss += torch.mean(diff_norm)\n            loss += net_loss / len(center_feats)\n        loss = loss / len(subs_net_list)\n    else:\n        loss = torch.tensor(0.0)\n        for (net, center) in zip(subs_net_list, target_feat_list):\n            poisons_list = [net.get_activations(poison_batch(), layer=feature_layer, framework=True) for _ in range(net_repeat)]\n            poisons = torch.tensor(sum(poisons_list) / len(poisons_list))\n            diff_2 = torch.mean(poisons, dim=0) - center\n            diff_norm = torch.norm(diff_2, dim=1) / torch.norm(center, dim=1)\n            loss += torch.mean(diff_norm)\n        loss = loss / len(subs_net_list)\n    return loss",
        "mutated": [
            "def loss_from_center(subs_net_list, target_feat_list, poison_batch, net_repeat, end2end, feature_layer) -> 'torch.Tensor':\n    if False:\n        i = 10\n    '\\n    Calculate loss from center.\\n    '\n    import torch\n    if end2end:\n        loss = torch.tensor(0.0)\n        for (net, center_feats) in zip(subs_net_list, target_feat_list):\n            poisons_feats: Union[List[float], 'torch.Tensor', np.ndarray]\n            if net_repeat > 1:\n                poisons_feats_repeats = [net.get_activations(poison_batch(), layer=feature_layer, framework=True) for _ in range(net_repeat)]\n                block_num = len(poisons_feats_repeats[0])\n                poisons_feats = []\n                for block_idx in range(block_num):\n                    poisons_feats.append(sum([poisons_feat_r[block_idx] for poisons_feat_r in poisons_feats_repeats]) / net_repeat)\n            elif net_repeat == 1:\n                if isinstance(feature_layer, list):\n                    poisons_feats = [torch.flatten(net.get_activations(poison_batch(), layer=layer, framework=True), 0) for layer in feature_layer]\n                else:\n                    poisons_feats = net.get_activations(poison_batch(), layer=feature_layer, framework=True)\n            else:\n                assert False, f'net_repeat set to {net_repeat}'\n            net_loss = torch.tensor(0.0)\n            for (pfeat, cfeat) in zip(poisons_feats, center_feats):\n                diff = torch.mean(pfeat, dim=0) - cfeat\n                diff_norm = torch.norm(diff, dim=0)\n                cfeat_norm = torch.norm(cfeat, dim=0)\n                diff_norm = diff_norm / cfeat_norm\n                net_loss += torch.mean(diff_norm)\n            loss += net_loss / len(center_feats)\n        loss = loss / len(subs_net_list)\n    else:\n        loss = torch.tensor(0.0)\n        for (net, center) in zip(subs_net_list, target_feat_list):\n            poisons_list = [net.get_activations(poison_batch(), layer=feature_layer, framework=True) for _ in range(net_repeat)]\n            poisons = torch.tensor(sum(poisons_list) / len(poisons_list))\n            diff_2 = torch.mean(poisons, dim=0) - center\n            diff_norm = torch.norm(diff_2, dim=1) / torch.norm(center, dim=1)\n            loss += torch.mean(diff_norm)\n        loss = loss / len(subs_net_list)\n    return loss",
            "def loss_from_center(subs_net_list, target_feat_list, poison_batch, net_repeat, end2end, feature_layer) -> 'torch.Tensor':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Calculate loss from center.\\n    '\n    import torch\n    if end2end:\n        loss = torch.tensor(0.0)\n        for (net, center_feats) in zip(subs_net_list, target_feat_list):\n            poisons_feats: Union[List[float], 'torch.Tensor', np.ndarray]\n            if net_repeat > 1:\n                poisons_feats_repeats = [net.get_activations(poison_batch(), layer=feature_layer, framework=True) for _ in range(net_repeat)]\n                block_num = len(poisons_feats_repeats[0])\n                poisons_feats = []\n                for block_idx in range(block_num):\n                    poisons_feats.append(sum([poisons_feat_r[block_idx] for poisons_feat_r in poisons_feats_repeats]) / net_repeat)\n            elif net_repeat == 1:\n                if isinstance(feature_layer, list):\n                    poisons_feats = [torch.flatten(net.get_activations(poison_batch(), layer=layer, framework=True), 0) for layer in feature_layer]\n                else:\n                    poisons_feats = net.get_activations(poison_batch(), layer=feature_layer, framework=True)\n            else:\n                assert False, f'net_repeat set to {net_repeat}'\n            net_loss = torch.tensor(0.0)\n            for (pfeat, cfeat) in zip(poisons_feats, center_feats):\n                diff = torch.mean(pfeat, dim=0) - cfeat\n                diff_norm = torch.norm(diff, dim=0)\n                cfeat_norm = torch.norm(cfeat, dim=0)\n                diff_norm = diff_norm / cfeat_norm\n                net_loss += torch.mean(diff_norm)\n            loss += net_loss / len(center_feats)\n        loss = loss / len(subs_net_list)\n    else:\n        loss = torch.tensor(0.0)\n        for (net, center) in zip(subs_net_list, target_feat_list):\n            poisons_list = [net.get_activations(poison_batch(), layer=feature_layer, framework=True) for _ in range(net_repeat)]\n            poisons = torch.tensor(sum(poisons_list) / len(poisons_list))\n            diff_2 = torch.mean(poisons, dim=0) - center\n            diff_norm = torch.norm(diff_2, dim=1) / torch.norm(center, dim=1)\n            loss += torch.mean(diff_norm)\n        loss = loss / len(subs_net_list)\n    return loss",
            "def loss_from_center(subs_net_list, target_feat_list, poison_batch, net_repeat, end2end, feature_layer) -> 'torch.Tensor':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Calculate loss from center.\\n    '\n    import torch\n    if end2end:\n        loss = torch.tensor(0.0)\n        for (net, center_feats) in zip(subs_net_list, target_feat_list):\n            poisons_feats: Union[List[float], 'torch.Tensor', np.ndarray]\n            if net_repeat > 1:\n                poisons_feats_repeats = [net.get_activations(poison_batch(), layer=feature_layer, framework=True) for _ in range(net_repeat)]\n                block_num = len(poisons_feats_repeats[0])\n                poisons_feats = []\n                for block_idx in range(block_num):\n                    poisons_feats.append(sum([poisons_feat_r[block_idx] for poisons_feat_r in poisons_feats_repeats]) / net_repeat)\n            elif net_repeat == 1:\n                if isinstance(feature_layer, list):\n                    poisons_feats = [torch.flatten(net.get_activations(poison_batch(), layer=layer, framework=True), 0) for layer in feature_layer]\n                else:\n                    poisons_feats = net.get_activations(poison_batch(), layer=feature_layer, framework=True)\n            else:\n                assert False, f'net_repeat set to {net_repeat}'\n            net_loss = torch.tensor(0.0)\n            for (pfeat, cfeat) in zip(poisons_feats, center_feats):\n                diff = torch.mean(pfeat, dim=0) - cfeat\n                diff_norm = torch.norm(diff, dim=0)\n                cfeat_norm = torch.norm(cfeat, dim=0)\n                diff_norm = diff_norm / cfeat_norm\n                net_loss += torch.mean(diff_norm)\n            loss += net_loss / len(center_feats)\n        loss = loss / len(subs_net_list)\n    else:\n        loss = torch.tensor(0.0)\n        for (net, center) in zip(subs_net_list, target_feat_list):\n            poisons_list = [net.get_activations(poison_batch(), layer=feature_layer, framework=True) for _ in range(net_repeat)]\n            poisons = torch.tensor(sum(poisons_list) / len(poisons_list))\n            diff_2 = torch.mean(poisons, dim=0) - center\n            diff_norm = torch.norm(diff_2, dim=1) / torch.norm(center, dim=1)\n            loss += torch.mean(diff_norm)\n        loss = loss / len(subs_net_list)\n    return loss",
            "def loss_from_center(subs_net_list, target_feat_list, poison_batch, net_repeat, end2end, feature_layer) -> 'torch.Tensor':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Calculate loss from center.\\n    '\n    import torch\n    if end2end:\n        loss = torch.tensor(0.0)\n        for (net, center_feats) in zip(subs_net_list, target_feat_list):\n            poisons_feats: Union[List[float], 'torch.Tensor', np.ndarray]\n            if net_repeat > 1:\n                poisons_feats_repeats = [net.get_activations(poison_batch(), layer=feature_layer, framework=True) for _ in range(net_repeat)]\n                block_num = len(poisons_feats_repeats[0])\n                poisons_feats = []\n                for block_idx in range(block_num):\n                    poisons_feats.append(sum([poisons_feat_r[block_idx] for poisons_feat_r in poisons_feats_repeats]) / net_repeat)\n            elif net_repeat == 1:\n                if isinstance(feature_layer, list):\n                    poisons_feats = [torch.flatten(net.get_activations(poison_batch(), layer=layer, framework=True), 0) for layer in feature_layer]\n                else:\n                    poisons_feats = net.get_activations(poison_batch(), layer=feature_layer, framework=True)\n            else:\n                assert False, f'net_repeat set to {net_repeat}'\n            net_loss = torch.tensor(0.0)\n            for (pfeat, cfeat) in zip(poisons_feats, center_feats):\n                diff = torch.mean(pfeat, dim=0) - cfeat\n                diff_norm = torch.norm(diff, dim=0)\n                cfeat_norm = torch.norm(cfeat, dim=0)\n                diff_norm = diff_norm / cfeat_norm\n                net_loss += torch.mean(diff_norm)\n            loss += net_loss / len(center_feats)\n        loss = loss / len(subs_net_list)\n    else:\n        loss = torch.tensor(0.0)\n        for (net, center) in zip(subs_net_list, target_feat_list):\n            poisons_list = [net.get_activations(poison_batch(), layer=feature_layer, framework=True) for _ in range(net_repeat)]\n            poisons = torch.tensor(sum(poisons_list) / len(poisons_list))\n            diff_2 = torch.mean(poisons, dim=0) - center\n            diff_norm = torch.norm(diff_2, dim=1) / torch.norm(center, dim=1)\n            loss += torch.mean(diff_norm)\n        loss = loss / len(subs_net_list)\n    return loss",
            "def loss_from_center(subs_net_list, target_feat_list, poison_batch, net_repeat, end2end, feature_layer) -> 'torch.Tensor':\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Calculate loss from center.\\n    '\n    import torch\n    if end2end:\n        loss = torch.tensor(0.0)\n        for (net, center_feats) in zip(subs_net_list, target_feat_list):\n            poisons_feats: Union[List[float], 'torch.Tensor', np.ndarray]\n            if net_repeat > 1:\n                poisons_feats_repeats = [net.get_activations(poison_batch(), layer=feature_layer, framework=True) for _ in range(net_repeat)]\n                block_num = len(poisons_feats_repeats[0])\n                poisons_feats = []\n                for block_idx in range(block_num):\n                    poisons_feats.append(sum([poisons_feat_r[block_idx] for poisons_feat_r in poisons_feats_repeats]) / net_repeat)\n            elif net_repeat == 1:\n                if isinstance(feature_layer, list):\n                    poisons_feats = [torch.flatten(net.get_activations(poison_batch(), layer=layer, framework=True), 0) for layer in feature_layer]\n                else:\n                    poisons_feats = net.get_activations(poison_batch(), layer=feature_layer, framework=True)\n            else:\n                assert False, f'net_repeat set to {net_repeat}'\n            net_loss = torch.tensor(0.0)\n            for (pfeat, cfeat) in zip(poisons_feats, center_feats):\n                diff = torch.mean(pfeat, dim=0) - cfeat\n                diff_norm = torch.norm(diff, dim=0)\n                cfeat_norm = torch.norm(cfeat, dim=0)\n                diff_norm = diff_norm / cfeat_norm\n                net_loss += torch.mean(diff_norm)\n            loss += net_loss / len(center_feats)\n        loss = loss / len(subs_net_list)\n    else:\n        loss = torch.tensor(0.0)\n        for (net, center) in zip(subs_net_list, target_feat_list):\n            poisons_list = [net.get_activations(poison_batch(), layer=feature_layer, framework=True) for _ in range(net_repeat)]\n            poisons = torch.tensor(sum(poisons_list) / len(poisons_list))\n            diff_2 = torch.mean(poisons, dim=0) - center\n            diff_norm = torch.norm(diff_2, dim=1) / torch.norm(center, dim=1)\n            loss += torch.mean(diff_norm)\n        loss = loss / len(subs_net_list)\n    return loss"
        ]
    }
]