[
    {
        "func_name": "test_stream_file_sink",
        "original": "def test_stream_file_sink(self):\n    self.env.set_parallelism(2)\n    ds = self.env.from_collection([('ab', 1), ('bdc', 2), ('cfgs', 3), ('deeefg', 4)], type_info=Types.ROW([Types.STRING(), Types.INT()]))\n    ds.map(lambda a: a[0], Types.STRING()).add_sink(StreamingFileSink.for_row_format(self.tempdir, Encoder.simple_string_encoder()).with_rolling_policy(RollingPolicy.default_rolling_policy(part_size=1024 * 1024 * 1024, rollover_interval=15 * 60 * 1000, inactivity_interval=5 * 60 * 1000)).with_output_file_config(OutputFileConfig.OutputFileConfigBuilder().with_part_prefix('prefix').with_part_suffix('suffix').build()).build())\n    self.env.execute('test_streaming_file_sink')\n    results = []\n    import os\n    for (root, dirs, files) in os.walk(self.tempdir, topdown=True):\n        for file in files:\n            self.assertTrue(file.startswith('.prefix'))\n            self.assertTrue('suffix' in file)\n            path = root + '/' + file\n            with open(path) as infile:\n                for line in infile:\n                    results.append(line)\n    expected = ['deeefg\\n', 'bdc\\n', 'ab\\n', 'cfgs\\n']\n    results.sort()\n    expected.sort()\n    self.assertEqual(expected, results)",
        "mutated": [
            "def test_stream_file_sink(self):\n    if False:\n        i = 10\n    self.env.set_parallelism(2)\n    ds = self.env.from_collection([('ab', 1), ('bdc', 2), ('cfgs', 3), ('deeefg', 4)], type_info=Types.ROW([Types.STRING(), Types.INT()]))\n    ds.map(lambda a: a[0], Types.STRING()).add_sink(StreamingFileSink.for_row_format(self.tempdir, Encoder.simple_string_encoder()).with_rolling_policy(RollingPolicy.default_rolling_policy(part_size=1024 * 1024 * 1024, rollover_interval=15 * 60 * 1000, inactivity_interval=5 * 60 * 1000)).with_output_file_config(OutputFileConfig.OutputFileConfigBuilder().with_part_prefix('prefix').with_part_suffix('suffix').build()).build())\n    self.env.execute('test_streaming_file_sink')\n    results = []\n    import os\n    for (root, dirs, files) in os.walk(self.tempdir, topdown=True):\n        for file in files:\n            self.assertTrue(file.startswith('.prefix'))\n            self.assertTrue('suffix' in file)\n            path = root + '/' + file\n            with open(path) as infile:\n                for line in infile:\n                    results.append(line)\n    expected = ['deeefg\\n', 'bdc\\n', 'ab\\n', 'cfgs\\n']\n    results.sort()\n    expected.sort()\n    self.assertEqual(expected, results)",
            "def test_stream_file_sink(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.env.set_parallelism(2)\n    ds = self.env.from_collection([('ab', 1), ('bdc', 2), ('cfgs', 3), ('deeefg', 4)], type_info=Types.ROW([Types.STRING(), Types.INT()]))\n    ds.map(lambda a: a[0], Types.STRING()).add_sink(StreamingFileSink.for_row_format(self.tempdir, Encoder.simple_string_encoder()).with_rolling_policy(RollingPolicy.default_rolling_policy(part_size=1024 * 1024 * 1024, rollover_interval=15 * 60 * 1000, inactivity_interval=5 * 60 * 1000)).with_output_file_config(OutputFileConfig.OutputFileConfigBuilder().with_part_prefix('prefix').with_part_suffix('suffix').build()).build())\n    self.env.execute('test_streaming_file_sink')\n    results = []\n    import os\n    for (root, dirs, files) in os.walk(self.tempdir, topdown=True):\n        for file in files:\n            self.assertTrue(file.startswith('.prefix'))\n            self.assertTrue('suffix' in file)\n            path = root + '/' + file\n            with open(path) as infile:\n                for line in infile:\n                    results.append(line)\n    expected = ['deeefg\\n', 'bdc\\n', 'ab\\n', 'cfgs\\n']\n    results.sort()\n    expected.sort()\n    self.assertEqual(expected, results)",
            "def test_stream_file_sink(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.env.set_parallelism(2)\n    ds = self.env.from_collection([('ab', 1), ('bdc', 2), ('cfgs', 3), ('deeefg', 4)], type_info=Types.ROW([Types.STRING(), Types.INT()]))\n    ds.map(lambda a: a[0], Types.STRING()).add_sink(StreamingFileSink.for_row_format(self.tempdir, Encoder.simple_string_encoder()).with_rolling_policy(RollingPolicy.default_rolling_policy(part_size=1024 * 1024 * 1024, rollover_interval=15 * 60 * 1000, inactivity_interval=5 * 60 * 1000)).with_output_file_config(OutputFileConfig.OutputFileConfigBuilder().with_part_prefix('prefix').with_part_suffix('suffix').build()).build())\n    self.env.execute('test_streaming_file_sink')\n    results = []\n    import os\n    for (root, dirs, files) in os.walk(self.tempdir, topdown=True):\n        for file in files:\n            self.assertTrue(file.startswith('.prefix'))\n            self.assertTrue('suffix' in file)\n            path = root + '/' + file\n            with open(path) as infile:\n                for line in infile:\n                    results.append(line)\n    expected = ['deeefg\\n', 'bdc\\n', 'ab\\n', 'cfgs\\n']\n    results.sort()\n    expected.sort()\n    self.assertEqual(expected, results)",
            "def test_stream_file_sink(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.env.set_parallelism(2)\n    ds = self.env.from_collection([('ab', 1), ('bdc', 2), ('cfgs', 3), ('deeefg', 4)], type_info=Types.ROW([Types.STRING(), Types.INT()]))\n    ds.map(lambda a: a[0], Types.STRING()).add_sink(StreamingFileSink.for_row_format(self.tempdir, Encoder.simple_string_encoder()).with_rolling_policy(RollingPolicy.default_rolling_policy(part_size=1024 * 1024 * 1024, rollover_interval=15 * 60 * 1000, inactivity_interval=5 * 60 * 1000)).with_output_file_config(OutputFileConfig.OutputFileConfigBuilder().with_part_prefix('prefix').with_part_suffix('suffix').build()).build())\n    self.env.execute('test_streaming_file_sink')\n    results = []\n    import os\n    for (root, dirs, files) in os.walk(self.tempdir, topdown=True):\n        for file in files:\n            self.assertTrue(file.startswith('.prefix'))\n            self.assertTrue('suffix' in file)\n            path = root + '/' + file\n            with open(path) as infile:\n                for line in infile:\n                    results.append(line)\n    expected = ['deeefg\\n', 'bdc\\n', 'ab\\n', 'cfgs\\n']\n    results.sort()\n    expected.sort()\n    self.assertEqual(expected, results)",
            "def test_stream_file_sink(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.env.set_parallelism(2)\n    ds = self.env.from_collection([('ab', 1), ('bdc', 2), ('cfgs', 3), ('deeefg', 4)], type_info=Types.ROW([Types.STRING(), Types.INT()]))\n    ds.map(lambda a: a[0], Types.STRING()).add_sink(StreamingFileSink.for_row_format(self.tempdir, Encoder.simple_string_encoder()).with_rolling_policy(RollingPolicy.default_rolling_policy(part_size=1024 * 1024 * 1024, rollover_interval=15 * 60 * 1000, inactivity_interval=5 * 60 * 1000)).with_output_file_config(OutputFileConfig.OutputFileConfigBuilder().with_part_prefix('prefix').with_part_suffix('suffix').build()).build())\n    self.env.execute('test_streaming_file_sink')\n    results = []\n    import os\n    for (root, dirs, files) in os.walk(self.tempdir, topdown=True):\n        for file in files:\n            self.assertTrue(file.startswith('.prefix'))\n            self.assertTrue('suffix' in file)\n            path = root + '/' + file\n            with open(path) as infile:\n                for line in infile:\n                    results.append(line)\n    expected = ['deeefg\\n', 'bdc\\n', 'ab\\n', 'cfgs\\n']\n    results.sort()\n    expected.sort()\n    self.assertEqual(expected, results)"
        ]
    },
    {
        "func_name": "test_file_source",
        "original": "def test_file_source(self):\n    stream_format = StreamFormat.text_line_format()\n    paths = ['/tmp/1.txt', '/tmp/2.txt']\n    file_source_builder = FileSource.for_record_stream_format(stream_format, *paths)\n    file_source = file_source_builder.monitor_continuously(Duration.of_days(1)).set_file_enumerator(FileEnumeratorProvider.default_splittable_file_enumerator()).set_split_assigner(FileSplitAssignerProvider.locality_aware_split_assigner()).build()\n    continuous_setting = file_source.get_java_function().getContinuousEnumerationSettings()\n    self.assertIsNotNone(continuous_setting)\n    self.assertEqual(Duration.of_days(1), Duration(continuous_setting.getDiscoveryInterval()))\n    input_paths_field = load_java_class('org.apache.flink.connector.file.src.AbstractFileSource').getDeclaredField('inputPaths')\n    input_paths_field.setAccessible(True)\n    input_paths = input_paths_field.get(file_source.get_java_function())\n    self.assertEqual(len(input_paths), len(paths))\n    self.assertEqual(str(input_paths[0]), paths[0])\n    self.assertEqual(str(input_paths[1]), paths[1])",
        "mutated": [
            "def test_file_source(self):\n    if False:\n        i = 10\n    stream_format = StreamFormat.text_line_format()\n    paths = ['/tmp/1.txt', '/tmp/2.txt']\n    file_source_builder = FileSource.for_record_stream_format(stream_format, *paths)\n    file_source = file_source_builder.monitor_continuously(Duration.of_days(1)).set_file_enumerator(FileEnumeratorProvider.default_splittable_file_enumerator()).set_split_assigner(FileSplitAssignerProvider.locality_aware_split_assigner()).build()\n    continuous_setting = file_source.get_java_function().getContinuousEnumerationSettings()\n    self.assertIsNotNone(continuous_setting)\n    self.assertEqual(Duration.of_days(1), Duration(continuous_setting.getDiscoveryInterval()))\n    input_paths_field = load_java_class('org.apache.flink.connector.file.src.AbstractFileSource').getDeclaredField('inputPaths')\n    input_paths_field.setAccessible(True)\n    input_paths = input_paths_field.get(file_source.get_java_function())\n    self.assertEqual(len(input_paths), len(paths))\n    self.assertEqual(str(input_paths[0]), paths[0])\n    self.assertEqual(str(input_paths[1]), paths[1])",
            "def test_file_source(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    stream_format = StreamFormat.text_line_format()\n    paths = ['/tmp/1.txt', '/tmp/2.txt']\n    file_source_builder = FileSource.for_record_stream_format(stream_format, *paths)\n    file_source = file_source_builder.monitor_continuously(Duration.of_days(1)).set_file_enumerator(FileEnumeratorProvider.default_splittable_file_enumerator()).set_split_assigner(FileSplitAssignerProvider.locality_aware_split_assigner()).build()\n    continuous_setting = file_source.get_java_function().getContinuousEnumerationSettings()\n    self.assertIsNotNone(continuous_setting)\n    self.assertEqual(Duration.of_days(1), Duration(continuous_setting.getDiscoveryInterval()))\n    input_paths_field = load_java_class('org.apache.flink.connector.file.src.AbstractFileSource').getDeclaredField('inputPaths')\n    input_paths_field.setAccessible(True)\n    input_paths = input_paths_field.get(file_source.get_java_function())\n    self.assertEqual(len(input_paths), len(paths))\n    self.assertEqual(str(input_paths[0]), paths[0])\n    self.assertEqual(str(input_paths[1]), paths[1])",
            "def test_file_source(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    stream_format = StreamFormat.text_line_format()\n    paths = ['/tmp/1.txt', '/tmp/2.txt']\n    file_source_builder = FileSource.for_record_stream_format(stream_format, *paths)\n    file_source = file_source_builder.monitor_continuously(Duration.of_days(1)).set_file_enumerator(FileEnumeratorProvider.default_splittable_file_enumerator()).set_split_assigner(FileSplitAssignerProvider.locality_aware_split_assigner()).build()\n    continuous_setting = file_source.get_java_function().getContinuousEnumerationSettings()\n    self.assertIsNotNone(continuous_setting)\n    self.assertEqual(Duration.of_days(1), Duration(continuous_setting.getDiscoveryInterval()))\n    input_paths_field = load_java_class('org.apache.flink.connector.file.src.AbstractFileSource').getDeclaredField('inputPaths')\n    input_paths_field.setAccessible(True)\n    input_paths = input_paths_field.get(file_source.get_java_function())\n    self.assertEqual(len(input_paths), len(paths))\n    self.assertEqual(str(input_paths[0]), paths[0])\n    self.assertEqual(str(input_paths[1]), paths[1])",
            "def test_file_source(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    stream_format = StreamFormat.text_line_format()\n    paths = ['/tmp/1.txt', '/tmp/2.txt']\n    file_source_builder = FileSource.for_record_stream_format(stream_format, *paths)\n    file_source = file_source_builder.monitor_continuously(Duration.of_days(1)).set_file_enumerator(FileEnumeratorProvider.default_splittable_file_enumerator()).set_split_assigner(FileSplitAssignerProvider.locality_aware_split_assigner()).build()\n    continuous_setting = file_source.get_java_function().getContinuousEnumerationSettings()\n    self.assertIsNotNone(continuous_setting)\n    self.assertEqual(Duration.of_days(1), Duration(continuous_setting.getDiscoveryInterval()))\n    input_paths_field = load_java_class('org.apache.flink.connector.file.src.AbstractFileSource').getDeclaredField('inputPaths')\n    input_paths_field.setAccessible(True)\n    input_paths = input_paths_field.get(file_source.get_java_function())\n    self.assertEqual(len(input_paths), len(paths))\n    self.assertEqual(str(input_paths[0]), paths[0])\n    self.assertEqual(str(input_paths[1]), paths[1])",
            "def test_file_source(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    stream_format = StreamFormat.text_line_format()\n    paths = ['/tmp/1.txt', '/tmp/2.txt']\n    file_source_builder = FileSource.for_record_stream_format(stream_format, *paths)\n    file_source = file_source_builder.monitor_continuously(Duration.of_days(1)).set_file_enumerator(FileEnumeratorProvider.default_splittable_file_enumerator()).set_split_assigner(FileSplitAssignerProvider.locality_aware_split_assigner()).build()\n    continuous_setting = file_source.get_java_function().getContinuousEnumerationSettings()\n    self.assertIsNotNone(continuous_setting)\n    self.assertEqual(Duration.of_days(1), Duration(continuous_setting.getDiscoveryInterval()))\n    input_paths_field = load_java_class('org.apache.flink.connector.file.src.AbstractFileSource').getDeclaredField('inputPaths')\n    input_paths_field.setAccessible(True)\n    input_paths = input_paths_field.get(file_source.get_java_function())\n    self.assertEqual(len(input_paths), len(paths))\n    self.assertEqual(str(input_paths[0]), paths[0])\n    self.assertEqual(str(input_paths[1]), paths[1])"
        ]
    },
    {
        "func_name": "test_file_sink",
        "original": "def test_file_sink(self):\n    base_path = '/tmp/1.txt'\n    encoder = Encoder.simple_string_encoder()\n    file_sink_builder = FileSink.for_row_format(base_path, encoder)\n    file_sink = file_sink_builder.with_bucket_check_interval(1000).with_bucket_assigner(BucketAssigner.base_path_bucket_assigner()).with_rolling_policy(RollingPolicy.on_checkpoint_rolling_policy()).with_output_file_config(OutputFileConfig.builder().with_part_prefix('pre').with_part_suffix('suf').build()).enable_compact(FileCompactStrategy.builder().enable_compaction_on_checkpoint(3).set_size_threshold(1024).set_num_compact_threads(2).build(), FileCompactor.concat_file_compactor(b'\\n')).build()\n    buckets_builder_field = load_java_class('org.apache.flink.connector.file.sink.FileSink').getDeclaredField('bucketsBuilder')\n    buckets_builder_field.setAccessible(True)\n    buckets_builder = buckets_builder_field.get(file_sink.get_java_function())\n    self.assertEqual('DefaultRowFormatBuilder', buckets_builder.getClass().getSimpleName())\n    row_format_builder_clz = load_java_class('org.apache.flink.connector.file.sink.FileSink$RowFormatBuilder')\n    encoder_field = row_format_builder_clz.getDeclaredField('encoder')\n    encoder_field.setAccessible(True)\n    self.assertEqual('SimpleStringEncoder', encoder_field.get(buckets_builder).getClass().getSimpleName())\n    interval_field = row_format_builder_clz.getDeclaredField('bucketCheckInterval')\n    interval_field.setAccessible(True)\n    self.assertEqual(1000, interval_field.get(buckets_builder))\n    bucket_assigner_field = row_format_builder_clz.getDeclaredField('bucketAssigner')\n    bucket_assigner_field.setAccessible(True)\n    self.assertEqual('BasePathBucketAssigner', bucket_assigner_field.get(buckets_builder).getClass().getSimpleName())\n    rolling_policy_field = row_format_builder_clz.getDeclaredField('rollingPolicy')\n    rolling_policy_field.setAccessible(True)\n    self.assertEqual('OnCheckpointRollingPolicy', rolling_policy_field.get(buckets_builder).getClass().getSimpleName())\n    output_file_config_field = row_format_builder_clz.getDeclaredField('outputFileConfig')\n    output_file_config_field.setAccessible(True)\n    output_file_config = output_file_config_field.get(buckets_builder)\n    self.assertEqual('pre', output_file_config.getPartPrefix())\n    self.assertEqual('suf', output_file_config.getPartSuffix())\n    compact_strategy_field = row_format_builder_clz.getDeclaredField('compactStrategy')\n    compact_strategy_field.setAccessible(True)\n    compact_strategy = compact_strategy_field.get(buckets_builder)\n    self.assertEqual(3, compact_strategy.getNumCheckpointsBeforeCompaction())\n    self.assertEqual(1024, compact_strategy.getSizeThreshold())\n    self.assertEqual(2, compact_strategy.getNumCompactThreads())\n    file_compactor_field = row_format_builder_clz.getDeclaredField('fileCompactor')\n    file_compactor_field.setAccessible(True)\n    file_compactor = file_compactor_field.get(buckets_builder)\n    self.assertEqual('ConcatFileCompactor', file_compactor.getClass().getSimpleName())\n    concat_file_compactor_clz = load_java_class('org.apache.flink.connector.file.sink.compactor.ConcatFileCompactor')\n    file_delimiter_field = concat_file_compactor_clz.getDeclaredField('fileDelimiter')\n    file_delimiter_field.setAccessible(True)\n    file_delimiter = file_delimiter_field.get(file_compactor)\n    self.assertEqual(b'\\n', file_delimiter)",
        "mutated": [
            "def test_file_sink(self):\n    if False:\n        i = 10\n    base_path = '/tmp/1.txt'\n    encoder = Encoder.simple_string_encoder()\n    file_sink_builder = FileSink.for_row_format(base_path, encoder)\n    file_sink = file_sink_builder.with_bucket_check_interval(1000).with_bucket_assigner(BucketAssigner.base_path_bucket_assigner()).with_rolling_policy(RollingPolicy.on_checkpoint_rolling_policy()).with_output_file_config(OutputFileConfig.builder().with_part_prefix('pre').with_part_suffix('suf').build()).enable_compact(FileCompactStrategy.builder().enable_compaction_on_checkpoint(3).set_size_threshold(1024).set_num_compact_threads(2).build(), FileCompactor.concat_file_compactor(b'\\n')).build()\n    buckets_builder_field = load_java_class('org.apache.flink.connector.file.sink.FileSink').getDeclaredField('bucketsBuilder')\n    buckets_builder_field.setAccessible(True)\n    buckets_builder = buckets_builder_field.get(file_sink.get_java_function())\n    self.assertEqual('DefaultRowFormatBuilder', buckets_builder.getClass().getSimpleName())\n    row_format_builder_clz = load_java_class('org.apache.flink.connector.file.sink.FileSink$RowFormatBuilder')\n    encoder_field = row_format_builder_clz.getDeclaredField('encoder')\n    encoder_field.setAccessible(True)\n    self.assertEqual('SimpleStringEncoder', encoder_field.get(buckets_builder).getClass().getSimpleName())\n    interval_field = row_format_builder_clz.getDeclaredField('bucketCheckInterval')\n    interval_field.setAccessible(True)\n    self.assertEqual(1000, interval_field.get(buckets_builder))\n    bucket_assigner_field = row_format_builder_clz.getDeclaredField('bucketAssigner')\n    bucket_assigner_field.setAccessible(True)\n    self.assertEqual('BasePathBucketAssigner', bucket_assigner_field.get(buckets_builder).getClass().getSimpleName())\n    rolling_policy_field = row_format_builder_clz.getDeclaredField('rollingPolicy')\n    rolling_policy_field.setAccessible(True)\n    self.assertEqual('OnCheckpointRollingPolicy', rolling_policy_field.get(buckets_builder).getClass().getSimpleName())\n    output_file_config_field = row_format_builder_clz.getDeclaredField('outputFileConfig')\n    output_file_config_field.setAccessible(True)\n    output_file_config = output_file_config_field.get(buckets_builder)\n    self.assertEqual('pre', output_file_config.getPartPrefix())\n    self.assertEqual('suf', output_file_config.getPartSuffix())\n    compact_strategy_field = row_format_builder_clz.getDeclaredField('compactStrategy')\n    compact_strategy_field.setAccessible(True)\n    compact_strategy = compact_strategy_field.get(buckets_builder)\n    self.assertEqual(3, compact_strategy.getNumCheckpointsBeforeCompaction())\n    self.assertEqual(1024, compact_strategy.getSizeThreshold())\n    self.assertEqual(2, compact_strategy.getNumCompactThreads())\n    file_compactor_field = row_format_builder_clz.getDeclaredField('fileCompactor')\n    file_compactor_field.setAccessible(True)\n    file_compactor = file_compactor_field.get(buckets_builder)\n    self.assertEqual('ConcatFileCompactor', file_compactor.getClass().getSimpleName())\n    concat_file_compactor_clz = load_java_class('org.apache.flink.connector.file.sink.compactor.ConcatFileCompactor')\n    file_delimiter_field = concat_file_compactor_clz.getDeclaredField('fileDelimiter')\n    file_delimiter_field.setAccessible(True)\n    file_delimiter = file_delimiter_field.get(file_compactor)\n    self.assertEqual(b'\\n', file_delimiter)",
            "def test_file_sink(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    base_path = '/tmp/1.txt'\n    encoder = Encoder.simple_string_encoder()\n    file_sink_builder = FileSink.for_row_format(base_path, encoder)\n    file_sink = file_sink_builder.with_bucket_check_interval(1000).with_bucket_assigner(BucketAssigner.base_path_bucket_assigner()).with_rolling_policy(RollingPolicy.on_checkpoint_rolling_policy()).with_output_file_config(OutputFileConfig.builder().with_part_prefix('pre').with_part_suffix('suf').build()).enable_compact(FileCompactStrategy.builder().enable_compaction_on_checkpoint(3).set_size_threshold(1024).set_num_compact_threads(2).build(), FileCompactor.concat_file_compactor(b'\\n')).build()\n    buckets_builder_field = load_java_class('org.apache.flink.connector.file.sink.FileSink').getDeclaredField('bucketsBuilder')\n    buckets_builder_field.setAccessible(True)\n    buckets_builder = buckets_builder_field.get(file_sink.get_java_function())\n    self.assertEqual('DefaultRowFormatBuilder', buckets_builder.getClass().getSimpleName())\n    row_format_builder_clz = load_java_class('org.apache.flink.connector.file.sink.FileSink$RowFormatBuilder')\n    encoder_field = row_format_builder_clz.getDeclaredField('encoder')\n    encoder_field.setAccessible(True)\n    self.assertEqual('SimpleStringEncoder', encoder_field.get(buckets_builder).getClass().getSimpleName())\n    interval_field = row_format_builder_clz.getDeclaredField('bucketCheckInterval')\n    interval_field.setAccessible(True)\n    self.assertEqual(1000, interval_field.get(buckets_builder))\n    bucket_assigner_field = row_format_builder_clz.getDeclaredField('bucketAssigner')\n    bucket_assigner_field.setAccessible(True)\n    self.assertEqual('BasePathBucketAssigner', bucket_assigner_field.get(buckets_builder).getClass().getSimpleName())\n    rolling_policy_field = row_format_builder_clz.getDeclaredField('rollingPolicy')\n    rolling_policy_field.setAccessible(True)\n    self.assertEqual('OnCheckpointRollingPolicy', rolling_policy_field.get(buckets_builder).getClass().getSimpleName())\n    output_file_config_field = row_format_builder_clz.getDeclaredField('outputFileConfig')\n    output_file_config_field.setAccessible(True)\n    output_file_config = output_file_config_field.get(buckets_builder)\n    self.assertEqual('pre', output_file_config.getPartPrefix())\n    self.assertEqual('suf', output_file_config.getPartSuffix())\n    compact_strategy_field = row_format_builder_clz.getDeclaredField('compactStrategy')\n    compact_strategy_field.setAccessible(True)\n    compact_strategy = compact_strategy_field.get(buckets_builder)\n    self.assertEqual(3, compact_strategy.getNumCheckpointsBeforeCompaction())\n    self.assertEqual(1024, compact_strategy.getSizeThreshold())\n    self.assertEqual(2, compact_strategy.getNumCompactThreads())\n    file_compactor_field = row_format_builder_clz.getDeclaredField('fileCompactor')\n    file_compactor_field.setAccessible(True)\n    file_compactor = file_compactor_field.get(buckets_builder)\n    self.assertEqual('ConcatFileCompactor', file_compactor.getClass().getSimpleName())\n    concat_file_compactor_clz = load_java_class('org.apache.flink.connector.file.sink.compactor.ConcatFileCompactor')\n    file_delimiter_field = concat_file_compactor_clz.getDeclaredField('fileDelimiter')\n    file_delimiter_field.setAccessible(True)\n    file_delimiter = file_delimiter_field.get(file_compactor)\n    self.assertEqual(b'\\n', file_delimiter)",
            "def test_file_sink(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    base_path = '/tmp/1.txt'\n    encoder = Encoder.simple_string_encoder()\n    file_sink_builder = FileSink.for_row_format(base_path, encoder)\n    file_sink = file_sink_builder.with_bucket_check_interval(1000).with_bucket_assigner(BucketAssigner.base_path_bucket_assigner()).with_rolling_policy(RollingPolicy.on_checkpoint_rolling_policy()).with_output_file_config(OutputFileConfig.builder().with_part_prefix('pre').with_part_suffix('suf').build()).enable_compact(FileCompactStrategy.builder().enable_compaction_on_checkpoint(3).set_size_threshold(1024).set_num_compact_threads(2).build(), FileCompactor.concat_file_compactor(b'\\n')).build()\n    buckets_builder_field = load_java_class('org.apache.flink.connector.file.sink.FileSink').getDeclaredField('bucketsBuilder')\n    buckets_builder_field.setAccessible(True)\n    buckets_builder = buckets_builder_field.get(file_sink.get_java_function())\n    self.assertEqual('DefaultRowFormatBuilder', buckets_builder.getClass().getSimpleName())\n    row_format_builder_clz = load_java_class('org.apache.flink.connector.file.sink.FileSink$RowFormatBuilder')\n    encoder_field = row_format_builder_clz.getDeclaredField('encoder')\n    encoder_field.setAccessible(True)\n    self.assertEqual('SimpleStringEncoder', encoder_field.get(buckets_builder).getClass().getSimpleName())\n    interval_field = row_format_builder_clz.getDeclaredField('bucketCheckInterval')\n    interval_field.setAccessible(True)\n    self.assertEqual(1000, interval_field.get(buckets_builder))\n    bucket_assigner_field = row_format_builder_clz.getDeclaredField('bucketAssigner')\n    bucket_assigner_field.setAccessible(True)\n    self.assertEqual('BasePathBucketAssigner', bucket_assigner_field.get(buckets_builder).getClass().getSimpleName())\n    rolling_policy_field = row_format_builder_clz.getDeclaredField('rollingPolicy')\n    rolling_policy_field.setAccessible(True)\n    self.assertEqual('OnCheckpointRollingPolicy', rolling_policy_field.get(buckets_builder).getClass().getSimpleName())\n    output_file_config_field = row_format_builder_clz.getDeclaredField('outputFileConfig')\n    output_file_config_field.setAccessible(True)\n    output_file_config = output_file_config_field.get(buckets_builder)\n    self.assertEqual('pre', output_file_config.getPartPrefix())\n    self.assertEqual('suf', output_file_config.getPartSuffix())\n    compact_strategy_field = row_format_builder_clz.getDeclaredField('compactStrategy')\n    compact_strategy_field.setAccessible(True)\n    compact_strategy = compact_strategy_field.get(buckets_builder)\n    self.assertEqual(3, compact_strategy.getNumCheckpointsBeforeCompaction())\n    self.assertEqual(1024, compact_strategy.getSizeThreshold())\n    self.assertEqual(2, compact_strategy.getNumCompactThreads())\n    file_compactor_field = row_format_builder_clz.getDeclaredField('fileCompactor')\n    file_compactor_field.setAccessible(True)\n    file_compactor = file_compactor_field.get(buckets_builder)\n    self.assertEqual('ConcatFileCompactor', file_compactor.getClass().getSimpleName())\n    concat_file_compactor_clz = load_java_class('org.apache.flink.connector.file.sink.compactor.ConcatFileCompactor')\n    file_delimiter_field = concat_file_compactor_clz.getDeclaredField('fileDelimiter')\n    file_delimiter_field.setAccessible(True)\n    file_delimiter = file_delimiter_field.get(file_compactor)\n    self.assertEqual(b'\\n', file_delimiter)",
            "def test_file_sink(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    base_path = '/tmp/1.txt'\n    encoder = Encoder.simple_string_encoder()\n    file_sink_builder = FileSink.for_row_format(base_path, encoder)\n    file_sink = file_sink_builder.with_bucket_check_interval(1000).with_bucket_assigner(BucketAssigner.base_path_bucket_assigner()).with_rolling_policy(RollingPolicy.on_checkpoint_rolling_policy()).with_output_file_config(OutputFileConfig.builder().with_part_prefix('pre').with_part_suffix('suf').build()).enable_compact(FileCompactStrategy.builder().enable_compaction_on_checkpoint(3).set_size_threshold(1024).set_num_compact_threads(2).build(), FileCompactor.concat_file_compactor(b'\\n')).build()\n    buckets_builder_field = load_java_class('org.apache.flink.connector.file.sink.FileSink').getDeclaredField('bucketsBuilder')\n    buckets_builder_field.setAccessible(True)\n    buckets_builder = buckets_builder_field.get(file_sink.get_java_function())\n    self.assertEqual('DefaultRowFormatBuilder', buckets_builder.getClass().getSimpleName())\n    row_format_builder_clz = load_java_class('org.apache.flink.connector.file.sink.FileSink$RowFormatBuilder')\n    encoder_field = row_format_builder_clz.getDeclaredField('encoder')\n    encoder_field.setAccessible(True)\n    self.assertEqual('SimpleStringEncoder', encoder_field.get(buckets_builder).getClass().getSimpleName())\n    interval_field = row_format_builder_clz.getDeclaredField('bucketCheckInterval')\n    interval_field.setAccessible(True)\n    self.assertEqual(1000, interval_field.get(buckets_builder))\n    bucket_assigner_field = row_format_builder_clz.getDeclaredField('bucketAssigner')\n    bucket_assigner_field.setAccessible(True)\n    self.assertEqual('BasePathBucketAssigner', bucket_assigner_field.get(buckets_builder).getClass().getSimpleName())\n    rolling_policy_field = row_format_builder_clz.getDeclaredField('rollingPolicy')\n    rolling_policy_field.setAccessible(True)\n    self.assertEqual('OnCheckpointRollingPolicy', rolling_policy_field.get(buckets_builder).getClass().getSimpleName())\n    output_file_config_field = row_format_builder_clz.getDeclaredField('outputFileConfig')\n    output_file_config_field.setAccessible(True)\n    output_file_config = output_file_config_field.get(buckets_builder)\n    self.assertEqual('pre', output_file_config.getPartPrefix())\n    self.assertEqual('suf', output_file_config.getPartSuffix())\n    compact_strategy_field = row_format_builder_clz.getDeclaredField('compactStrategy')\n    compact_strategy_field.setAccessible(True)\n    compact_strategy = compact_strategy_field.get(buckets_builder)\n    self.assertEqual(3, compact_strategy.getNumCheckpointsBeforeCompaction())\n    self.assertEqual(1024, compact_strategy.getSizeThreshold())\n    self.assertEqual(2, compact_strategy.getNumCompactThreads())\n    file_compactor_field = row_format_builder_clz.getDeclaredField('fileCompactor')\n    file_compactor_field.setAccessible(True)\n    file_compactor = file_compactor_field.get(buckets_builder)\n    self.assertEqual('ConcatFileCompactor', file_compactor.getClass().getSimpleName())\n    concat_file_compactor_clz = load_java_class('org.apache.flink.connector.file.sink.compactor.ConcatFileCompactor')\n    file_delimiter_field = concat_file_compactor_clz.getDeclaredField('fileDelimiter')\n    file_delimiter_field.setAccessible(True)\n    file_delimiter = file_delimiter_field.get(file_compactor)\n    self.assertEqual(b'\\n', file_delimiter)",
            "def test_file_sink(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    base_path = '/tmp/1.txt'\n    encoder = Encoder.simple_string_encoder()\n    file_sink_builder = FileSink.for_row_format(base_path, encoder)\n    file_sink = file_sink_builder.with_bucket_check_interval(1000).with_bucket_assigner(BucketAssigner.base_path_bucket_assigner()).with_rolling_policy(RollingPolicy.on_checkpoint_rolling_policy()).with_output_file_config(OutputFileConfig.builder().with_part_prefix('pre').with_part_suffix('suf').build()).enable_compact(FileCompactStrategy.builder().enable_compaction_on_checkpoint(3).set_size_threshold(1024).set_num_compact_threads(2).build(), FileCompactor.concat_file_compactor(b'\\n')).build()\n    buckets_builder_field = load_java_class('org.apache.flink.connector.file.sink.FileSink').getDeclaredField('bucketsBuilder')\n    buckets_builder_field.setAccessible(True)\n    buckets_builder = buckets_builder_field.get(file_sink.get_java_function())\n    self.assertEqual('DefaultRowFormatBuilder', buckets_builder.getClass().getSimpleName())\n    row_format_builder_clz = load_java_class('org.apache.flink.connector.file.sink.FileSink$RowFormatBuilder')\n    encoder_field = row_format_builder_clz.getDeclaredField('encoder')\n    encoder_field.setAccessible(True)\n    self.assertEqual('SimpleStringEncoder', encoder_field.get(buckets_builder).getClass().getSimpleName())\n    interval_field = row_format_builder_clz.getDeclaredField('bucketCheckInterval')\n    interval_field.setAccessible(True)\n    self.assertEqual(1000, interval_field.get(buckets_builder))\n    bucket_assigner_field = row_format_builder_clz.getDeclaredField('bucketAssigner')\n    bucket_assigner_field.setAccessible(True)\n    self.assertEqual('BasePathBucketAssigner', bucket_assigner_field.get(buckets_builder).getClass().getSimpleName())\n    rolling_policy_field = row_format_builder_clz.getDeclaredField('rollingPolicy')\n    rolling_policy_field.setAccessible(True)\n    self.assertEqual('OnCheckpointRollingPolicy', rolling_policy_field.get(buckets_builder).getClass().getSimpleName())\n    output_file_config_field = row_format_builder_clz.getDeclaredField('outputFileConfig')\n    output_file_config_field.setAccessible(True)\n    output_file_config = output_file_config_field.get(buckets_builder)\n    self.assertEqual('pre', output_file_config.getPartPrefix())\n    self.assertEqual('suf', output_file_config.getPartSuffix())\n    compact_strategy_field = row_format_builder_clz.getDeclaredField('compactStrategy')\n    compact_strategy_field.setAccessible(True)\n    compact_strategy = compact_strategy_field.get(buckets_builder)\n    self.assertEqual(3, compact_strategy.getNumCheckpointsBeforeCompaction())\n    self.assertEqual(1024, compact_strategy.getSizeThreshold())\n    self.assertEqual(2, compact_strategy.getNumCompactThreads())\n    file_compactor_field = row_format_builder_clz.getDeclaredField('fileCompactor')\n    file_compactor_field.setAccessible(True)\n    file_compactor = file_compactor_field.get(buckets_builder)\n    self.assertEqual('ConcatFileCompactor', file_compactor.getClass().getSimpleName())\n    concat_file_compactor_clz = load_java_class('org.apache.flink.connector.file.sink.compactor.ConcatFileCompactor')\n    file_delimiter_field = concat_file_compactor_clz.getDeclaredField('fileDelimiter')\n    file_delimiter_field.setAccessible(True)\n    file_delimiter = file_delimiter_field.get(file_compactor)\n    self.assertEqual(b'\\n', file_delimiter)"
        ]
    }
]