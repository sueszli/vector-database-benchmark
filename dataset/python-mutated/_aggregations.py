"""Mixin classes with reduction operations."""
from __future__ import annotations
from collections.abc import Sequence
from typing import TYPE_CHECKING, Any, Callable
from xarray.core import duck_array_ops
from xarray.core.options import OPTIONS
from xarray.core.types import Dims, Self
from xarray.core.utils import contains_only_chunked_or_numpy, module_available
if TYPE_CHECKING:
    from xarray.core.dataarray import DataArray
    from xarray.core.dataset import Dataset
flox_available = module_available('flox')

class DatasetAggregations:
    __slots__ = ()

    def reduce(self, func: Callable[..., Any], dim: Dims=None, *, axis: int | Sequence[int] | None=None, keep_attrs: bool | None=None, keepdims: bool=False, **kwargs: Any) -> Self:
        if False:
            while True:
                i = 10
        raise NotImplementedError()

    def count(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            while True:
                i = 10
        '\n        Reduce this Dataset\'s data by applying ``count`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``count``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``count`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``count`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        pandas.DataFrame.count\n        dask.dataframe.DataFrame.count\n        DataArray.count\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.count()\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       int64 5\n        '
        return self.reduce(duck_array_ops.count, dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def all(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            print('Hello World!')
        '\n        Reduce this Dataset\'s data by applying ``all`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``all``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``all`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``all`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.all\n        dask.array.all\n        DataArray.all\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([True, True, True, True, True, False], dtype=bool),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) bool True True True True True False\n\n        >>> ds.all()\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       bool False\n        '
        return self.reduce(duck_array_ops.array_all, dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def any(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            return 10
        '\n        Reduce this Dataset\'s data by applying ``any`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``any``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``any`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``any`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.any\n        dask.array.any\n        DataArray.any\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([True, True, True, True, True, False], dtype=bool),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) bool True True True True True False\n\n        >>> ds.any()\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       bool True\n        '
        return self.reduce(duck_array_ops.array_any, dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def max(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            return 10
        '\n        Reduce this Dataset\'s data by applying ``max`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``max``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``max`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``max`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.max\n        dask.array.max\n        DataArray.max\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.max()\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 3.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.max(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 nan\n        '
        return self.reduce(duck_array_ops.max, dim=dim, skipna=skipna, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def min(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            while True:
                i = 10
        '\n        Reduce this Dataset\'s data by applying ``min`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``min``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``min`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``min`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.min\n        dask.array.min\n        DataArray.min\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.min()\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 0.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.min(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 nan\n        '
        return self.reduce(duck_array_ops.min, dim=dim, skipna=skipna, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def mean(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            print('Hello World!')
        '\n        Reduce this Dataset\'s data by applying ``mean`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``mean``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``mean`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``mean`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.mean\n        dask.array.mean\n        DataArray.mean\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.mean()\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 1.6\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.mean(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 nan\n        '
        return self.reduce(duck_array_ops.mean, dim=dim, skipna=skipna, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def prod(self, dim: Dims=None, *, skipna: bool | None=None, min_count: int | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            for i in range(10):
                print('nop')
        '\n        Reduce this Dataset\'s data by applying ``prod`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``prod``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        min_count : int or None, optional\n            The required number of valid values to perform the operation. If\n            fewer than min_count non-NA values are present the result will be\n            NA. Only used if skipna is set to True or defaults to True for the\n            array\'s dtype. Changed in version 0.17.0: if specified on an integer\n            array and skipna=True, the result will be a float array.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``prod`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``prod`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.prod\n        dask.array.prod\n        DataArray.prod\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.prod()\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 0.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.prod(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 nan\n\n        Specify ``min_count`` for finer control over when NaNs are ignored.\n\n        >>> ds.prod(skipna=True, min_count=2)\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 0.0\n        '
        return self.reduce(duck_array_ops.prod, dim=dim, skipna=skipna, min_count=min_count, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def sum(self, dim: Dims=None, *, skipna: bool | None=None, min_count: int | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            print('Hello World!')
        '\n        Reduce this Dataset\'s data by applying ``sum`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``sum``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        min_count : int or None, optional\n            The required number of valid values to perform the operation. If\n            fewer than min_count non-NA values are present the result will be\n            NA. Only used if skipna is set to True or defaults to True for the\n            array\'s dtype. Changed in version 0.17.0: if specified on an integer\n            array and skipna=True, the result will be a float array.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``sum`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``sum`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.sum\n        dask.array.sum\n        DataArray.sum\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.sum()\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 8.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.sum(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 nan\n\n        Specify ``min_count`` for finer control over when NaNs are ignored.\n\n        >>> ds.sum(skipna=True, min_count=2)\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 8.0\n        '
        return self.reduce(duck_array_ops.sum, dim=dim, skipna=skipna, min_count=min_count, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def std(self, dim: Dims=None, *, skipna: bool | None=None, ddof: int=0, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            print('Hello World!')
        '\n        Reduce this Dataset\'s data by applying ``std`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``std``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        ddof : int, default: 0\n            “Delta Degrees of Freedom”: the divisor used in the calculation is ``N - ddof``,\n            where ``N`` represents the number of elements.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``std`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``std`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.std\n        dask.array.std\n        DataArray.std\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.std()\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 1.02\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.std(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 nan\n\n        Specify ``ddof=1`` for an unbiased estimate.\n\n        >>> ds.std(skipna=True, ddof=1)\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 1.14\n        '
        return self.reduce(duck_array_ops.std, dim=dim, skipna=skipna, ddof=ddof, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def var(self, dim: Dims=None, *, skipna: bool | None=None, ddof: int=0, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            while True:
                i = 10
        '\n        Reduce this Dataset\'s data by applying ``var`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``var``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        ddof : int, default: 0\n            “Delta Degrees of Freedom”: the divisor used in the calculation is ``N - ddof``,\n            where ``N`` represents the number of elements.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``var`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``var`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.var\n        dask.array.var\n        DataArray.var\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.var()\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 1.04\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.var(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 nan\n\n        Specify ``ddof=1`` for an unbiased estimate.\n\n        >>> ds.var(skipna=True, ddof=1)\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 1.3\n        '
        return self.reduce(duck_array_ops.var, dim=dim, skipna=skipna, ddof=ddof, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def median(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            print('Hello World!')
        '\n        Reduce this Dataset\'s data by applying ``median`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``median``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``median`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``median`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.median\n        dask.array.median\n        DataArray.median\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.median()\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 2.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.median(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  ()\n        Data variables:\n            da       float64 nan\n        '
        return self.reduce(duck_array_ops.median, dim=dim, skipna=skipna, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def cumsum(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            return 10
        '\n        Reduce this Dataset\'s data by applying ``cumsum`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``cumsum``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``cumsum`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``cumsum`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.cumsum\n        dask.array.cumsum\n        DataArray.cumsum\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.cumsum()\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Dimensions without coordinates: time\n        Data variables:\n            da       (time) float64 1.0 3.0 6.0 6.0 8.0 8.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.cumsum(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Dimensions without coordinates: time\n        Data variables:\n            da       (time) float64 1.0 3.0 6.0 6.0 8.0 nan\n        '
        return self.reduce(duck_array_ops.cumsum, dim=dim, skipna=skipna, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def cumprod(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            while True:
                i = 10
        '\n        Reduce this Dataset\'s data by applying ``cumprod`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``cumprod``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``cumprod`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``cumprod`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.cumprod\n        dask.array.cumprod\n        DataArray.cumprod\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.cumprod()\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Dimensions without coordinates: time\n        Data variables:\n            da       (time) float64 1.0 2.0 6.0 0.0 0.0 0.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.cumprod(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Dimensions without coordinates: time\n        Data variables:\n            da       (time) float64 1.0 2.0 6.0 0.0 0.0 nan\n        '
        return self.reduce(duck_array_ops.cumprod, dim=dim, skipna=skipna, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

class DataArrayAggregations:
    __slots__ = ()

    def reduce(self, func: Callable[..., Any], dim: Dims=None, *, axis: int | Sequence[int] | None=None, keep_attrs: bool | None=None, keepdims: bool=False, **kwargs: Any) -> Self:
        if False:
            for i in range(10):
                print('nop')
        raise NotImplementedError()

    def count(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            return 10
        '\n        Reduce this DataArray\'s data by applying ``count`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``count``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``count`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``count`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        pandas.DataFrame.count\n        dask.dataframe.DataFrame.count\n        Dataset.count\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.count()\n        <xarray.DataArray ()>\n        array(5)\n        '
        return self.reduce(duck_array_ops.count, dim=dim, keep_attrs=keep_attrs, **kwargs)

    def all(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            for i in range(10):
                print('nop')
        '\n        Reduce this DataArray\'s data by applying ``all`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``all``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``all`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``all`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.all\n        dask.array.all\n        Dataset.all\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([True, True, True, True, True, False], dtype=bool),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ True,  True,  True,  True,  True, False])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.all()\n        <xarray.DataArray ()>\n        array(False)\n        '
        return self.reduce(duck_array_ops.array_all, dim=dim, keep_attrs=keep_attrs, **kwargs)

    def any(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            while True:
                i = 10
        '\n        Reduce this DataArray\'s data by applying ``any`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``any``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``any`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``any`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.any\n        dask.array.any\n        Dataset.any\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([True, True, True, True, True, False], dtype=bool),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ True,  True,  True,  True,  True, False])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.any()\n        <xarray.DataArray ()>\n        array(True)\n        '
        return self.reduce(duck_array_ops.array_any, dim=dim, keep_attrs=keep_attrs, **kwargs)

    def max(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            return 10
        '\n        Reduce this DataArray\'s data by applying ``max`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``max``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``max`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``max`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.max\n        dask.array.max\n        Dataset.max\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.max()\n        <xarray.DataArray ()>\n        array(3.)\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.max(skipna=False)\n        <xarray.DataArray ()>\n        array(nan)\n        '
        return self.reduce(duck_array_ops.max, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def min(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            for i in range(10):
                print('nop')
        '\n        Reduce this DataArray\'s data by applying ``min`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``min``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``min`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``min`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.min\n        dask.array.min\n        Dataset.min\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.min()\n        <xarray.DataArray ()>\n        array(0.)\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.min(skipna=False)\n        <xarray.DataArray ()>\n        array(nan)\n        '
        return self.reduce(duck_array_ops.min, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def mean(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            while True:
                i = 10
        '\n        Reduce this DataArray\'s data by applying ``mean`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``mean``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``mean`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``mean`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.mean\n        dask.array.mean\n        Dataset.mean\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.mean()\n        <xarray.DataArray ()>\n        array(1.6)\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.mean(skipna=False)\n        <xarray.DataArray ()>\n        array(nan)\n        '
        return self.reduce(duck_array_ops.mean, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def prod(self, dim: Dims=None, *, skipna: bool | None=None, min_count: int | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            print('Hello World!')
        '\n        Reduce this DataArray\'s data by applying ``prod`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``prod``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        min_count : int or None, optional\n            The required number of valid values to perform the operation. If\n            fewer than min_count non-NA values are present the result will be\n            NA. Only used if skipna is set to True or defaults to True for the\n            array\'s dtype. Changed in version 0.17.0: if specified on an integer\n            array and skipna=True, the result will be a float array.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``prod`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``prod`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.prod\n        dask.array.prod\n        Dataset.prod\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.prod()\n        <xarray.DataArray ()>\n        array(0.)\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.prod(skipna=False)\n        <xarray.DataArray ()>\n        array(nan)\n\n        Specify ``min_count`` for finer control over when NaNs are ignored.\n\n        >>> da.prod(skipna=True, min_count=2)\n        <xarray.DataArray ()>\n        array(0.)\n        '
        return self.reduce(duck_array_ops.prod, dim=dim, skipna=skipna, min_count=min_count, keep_attrs=keep_attrs, **kwargs)

    def sum(self, dim: Dims=None, *, skipna: bool | None=None, min_count: int | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            print('Hello World!')
        '\n        Reduce this DataArray\'s data by applying ``sum`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``sum``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        min_count : int or None, optional\n            The required number of valid values to perform the operation. If\n            fewer than min_count non-NA values are present the result will be\n            NA. Only used if skipna is set to True or defaults to True for the\n            array\'s dtype. Changed in version 0.17.0: if specified on an integer\n            array and skipna=True, the result will be a float array.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``sum`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``sum`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.sum\n        dask.array.sum\n        Dataset.sum\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.sum()\n        <xarray.DataArray ()>\n        array(8.)\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.sum(skipna=False)\n        <xarray.DataArray ()>\n        array(nan)\n\n        Specify ``min_count`` for finer control over when NaNs are ignored.\n\n        >>> da.sum(skipna=True, min_count=2)\n        <xarray.DataArray ()>\n        array(8.)\n        '
        return self.reduce(duck_array_ops.sum, dim=dim, skipna=skipna, min_count=min_count, keep_attrs=keep_attrs, **kwargs)

    def std(self, dim: Dims=None, *, skipna: bool | None=None, ddof: int=0, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            i = 10
            return i + 15
        '\n        Reduce this DataArray\'s data by applying ``std`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``std``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        ddof : int, default: 0\n            “Delta Degrees of Freedom”: the divisor used in the calculation is ``N - ddof``,\n            where ``N`` represents the number of elements.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``std`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``std`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.std\n        dask.array.std\n        Dataset.std\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.std()\n        <xarray.DataArray ()>\n        array(1.0198039)\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.std(skipna=False)\n        <xarray.DataArray ()>\n        array(nan)\n\n        Specify ``ddof=1`` for an unbiased estimate.\n\n        >>> da.std(skipna=True, ddof=1)\n        <xarray.DataArray ()>\n        array(1.14017543)\n        '
        return self.reduce(duck_array_ops.std, dim=dim, skipna=skipna, ddof=ddof, keep_attrs=keep_attrs, **kwargs)

    def var(self, dim: Dims=None, *, skipna: bool | None=None, ddof: int=0, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            i = 10
            return i + 15
        '\n        Reduce this DataArray\'s data by applying ``var`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``var``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        ddof : int, default: 0\n            “Delta Degrees of Freedom”: the divisor used in the calculation is ``N - ddof``,\n            where ``N`` represents the number of elements.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``var`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``var`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.var\n        dask.array.var\n        Dataset.var\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.var()\n        <xarray.DataArray ()>\n        array(1.04)\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.var(skipna=False)\n        <xarray.DataArray ()>\n        array(nan)\n\n        Specify ``ddof=1`` for an unbiased estimate.\n\n        >>> da.var(skipna=True, ddof=1)\n        <xarray.DataArray ()>\n        array(1.3)\n        '
        return self.reduce(duck_array_ops.var, dim=dim, skipna=skipna, ddof=ddof, keep_attrs=keep_attrs, **kwargs)

    def median(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            return 10
        '\n        Reduce this DataArray\'s data by applying ``median`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``median``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``median`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``median`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.median\n        dask.array.median\n        Dataset.median\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.median()\n        <xarray.DataArray ()>\n        array(2.)\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.median(skipna=False)\n        <xarray.DataArray ()>\n        array(nan)\n        '
        return self.reduce(duck_array_ops.median, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def cumsum(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            print('Hello World!')
        '\n        Reduce this DataArray\'s data by applying ``cumsum`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``cumsum``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``cumsum`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``cumsum`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.cumsum\n        dask.array.cumsum\n        Dataset.cumsum\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.cumsum()\n        <xarray.DataArray (time: 6)>\n        array([1., 3., 6., 6., 8., 8.])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.cumsum(skipna=False)\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  3.,  6.,  6.,  8., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        '
        return self.reduce(duck_array_ops.cumsum, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def cumprod(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Self:
        if False:
            for i in range(10):
                print('nop')
        '\n        Reduce this DataArray\'s data by applying ``cumprod`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``cumprod``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If "..." or None, will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``cumprod`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``cumprod`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.cumprod\n        dask.array.cumprod\n        Dataset.cumprod\n        :ref:`agg`\n            User guide on reduction or aggregation operations.\n\n        Notes\n        -----\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.cumprod()\n        <xarray.DataArray (time: 6)>\n        array([1., 2., 6., 0., 0., 0.])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.cumprod(skipna=False)\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  6.,  0.,  0., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        '
        return self.reduce(duck_array_ops.cumprod, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

class DatasetGroupByAggregations:
    _obj: Dataset

    def reduce(self, func: Callable[..., Any], dim: Dims=None, *, axis: int | Sequence[int] | None=None, keep_attrs: bool | None=None, keepdims: bool=False, **kwargs: Any) -> Dataset:
        if False:
            print('Hello World!')
        raise NotImplementedError()

    def _flox_reduce(self, dim: Dims, **kwargs: Any) -> Dataset:
        if False:
            return 10
        raise NotImplementedError()

    def count(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            i = 10
            return i + 15
        '\n        Reduce this Dataset\'s data by applying ``count`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``count``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``count`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``count`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        pandas.DataFrame.count\n        dask.dataframe.DataFrame.count\n        Dataset.count\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.groupby("labels").count()\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) int64 1 2 2\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='count', dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.count, dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def all(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            i = 10
            return i + 15
        '\n        Reduce this Dataset\'s data by applying ``all`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``all``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``all`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``all`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.all\n        dask.array.all\n        Dataset.all\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([True, True, True, True, True, False], dtype=bool),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) bool True True True True True False\n\n        >>> ds.groupby("labels").all()\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) bool False True True\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='all', dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.array_all, dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def any(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            i = 10
            return i + 15
        '\n        Reduce this Dataset\'s data by applying ``any`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``any``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``any`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``any`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.any\n        dask.array.any\n        Dataset.any\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([True, True, True, True, True, False], dtype=bool),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) bool True True True True True False\n\n        >>> ds.groupby("labels").any()\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) bool True True True\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='any', dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.array_any, dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def max(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            print('Hello World!')
        '\n        Reduce this Dataset\'s data by applying ``max`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``max``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``max`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``max`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.max\n        dask.array.max\n        Dataset.max\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.groupby("labels").max()\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 1.0 2.0 3.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.groupby("labels").max(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 nan 2.0 3.0\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='max', dim=dim, skipna=skipna, numeric_only=False, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.max, dim=dim, skipna=skipna, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def min(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            return 10
        '\n        Reduce this Dataset\'s data by applying ``min`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``min``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``min`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``min`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.min\n        dask.array.min\n        Dataset.min\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.groupby("labels").min()\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 1.0 2.0 0.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.groupby("labels").min(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 nan 2.0 0.0\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='min', dim=dim, skipna=skipna, numeric_only=False, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.min, dim=dim, skipna=skipna, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def mean(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            while True:
                i = 10
        '\n        Reduce this Dataset\'s data by applying ``mean`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``mean``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``mean`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``mean`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.mean\n        dask.array.mean\n        Dataset.mean\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.groupby("labels").mean()\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 1.0 2.0 1.5\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.groupby("labels").mean(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 nan 2.0 1.5\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='mean', dim=dim, skipna=skipna, numeric_only=True, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.mean, dim=dim, skipna=skipna, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def prod(self, dim: Dims=None, *, skipna: bool | None=None, min_count: int | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            for i in range(10):
                print('nop')
        '\n        Reduce this Dataset\'s data by applying ``prod`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``prod``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        min_count : int or None, optional\n            The required number of valid values to perform the operation. If\n            fewer than min_count non-NA values are present the result will be\n            NA. Only used if skipna is set to True or defaults to True for the\n            array\'s dtype. Changed in version 0.17.0: if specified on an integer\n            array and skipna=True, the result will be a float array.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``prod`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``prod`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.prod\n        dask.array.prod\n        Dataset.prod\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.groupby("labels").prod()\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 1.0 4.0 0.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.groupby("labels").prod(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 nan 4.0 0.0\n\n        Specify ``min_count`` for finer control over when NaNs are ignored.\n\n        >>> ds.groupby("labels").prod(skipna=True, min_count=2)\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 nan 4.0 0.0\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='prod', dim=dim, skipna=skipna, min_count=min_count, numeric_only=True, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.prod, dim=dim, skipna=skipna, min_count=min_count, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def sum(self, dim: Dims=None, *, skipna: bool | None=None, min_count: int | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            return 10
        '\n        Reduce this Dataset\'s data by applying ``sum`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``sum``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        min_count : int or None, optional\n            The required number of valid values to perform the operation. If\n            fewer than min_count non-NA values are present the result will be\n            NA. Only used if skipna is set to True or defaults to True for the\n            array\'s dtype. Changed in version 0.17.0: if specified on an integer\n            array and skipna=True, the result will be a float array.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``sum`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``sum`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.sum\n        dask.array.sum\n        Dataset.sum\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.groupby("labels").sum()\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 1.0 4.0 3.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.groupby("labels").sum(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 nan 4.0 3.0\n\n        Specify ``min_count`` for finer control over when NaNs are ignored.\n\n        >>> ds.groupby("labels").sum(skipna=True, min_count=2)\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 nan 4.0 3.0\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='sum', dim=dim, skipna=skipna, min_count=min_count, numeric_only=True, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.sum, dim=dim, skipna=skipna, min_count=min_count, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def std(self, dim: Dims=None, *, skipna: bool | None=None, ddof: int=0, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            i = 10
            return i + 15
        '\n        Reduce this Dataset\'s data by applying ``std`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``std``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        ddof : int, default: 0\n            “Delta Degrees of Freedom”: the divisor used in the calculation is ``N - ddof``,\n            where ``N`` represents the number of elements.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``std`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``std`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.std\n        dask.array.std\n        Dataset.std\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.groupby("labels").std()\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 0.0 0.0 1.5\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.groupby("labels").std(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 nan 0.0 1.5\n\n        Specify ``ddof=1`` for an unbiased estimate.\n\n        >>> ds.groupby("labels").std(skipna=True, ddof=1)\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 nan 0.0 2.121\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='std', dim=dim, skipna=skipna, ddof=ddof, numeric_only=True, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.std, dim=dim, skipna=skipna, ddof=ddof, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def var(self, dim: Dims=None, *, skipna: bool | None=None, ddof: int=0, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            i = 10
            return i + 15
        '\n        Reduce this Dataset\'s data by applying ``var`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``var``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        ddof : int, default: 0\n            “Delta Degrees of Freedom”: the divisor used in the calculation is ``N - ddof``,\n            where ``N`` represents the number of elements.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``var`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``var`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.var\n        dask.array.var\n        Dataset.var\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.groupby("labels").var()\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 0.0 0.0 2.25\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.groupby("labels").var(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 nan 0.0 2.25\n\n        Specify ``ddof=1`` for an unbiased estimate.\n\n        >>> ds.groupby("labels").var(skipna=True, ddof=1)\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 nan 0.0 4.5\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='var', dim=dim, skipna=skipna, ddof=ddof, numeric_only=True, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.var, dim=dim, skipna=skipna, ddof=ddof, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def median(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            i = 10
            return i + 15
        '\n        Reduce this Dataset\'s data by applying ``median`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``median``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``median`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``median`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.median\n        dask.array.median\n        Dataset.median\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.groupby("labels").median()\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 1.0 2.0 1.5\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.groupby("labels").median(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (labels: 3)\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        Data variables:\n            da       (labels) float64 nan 2.0 1.5\n        '
        return self.reduce(duck_array_ops.median, dim=dim, skipna=skipna, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def cumsum(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            return 10
        '\n        Reduce this Dataset\'s data by applying ``cumsum`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``cumsum``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``cumsum`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``cumsum`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.cumsum\n        dask.array.cumsum\n        Dataset.cumsum\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.groupby("labels").cumsum()\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Dimensions without coordinates: time\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 3.0 4.0 1.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.groupby("labels").cumsum(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Dimensions without coordinates: time\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 3.0 4.0 nan\n        '
        return self.reduce(duck_array_ops.cumsum, dim=dim, skipna=skipna, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def cumprod(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            i = 10
            return i + 15
        '\n        Reduce this Dataset\'s data by applying ``cumprod`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``cumprod``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``cumprod`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``cumprod`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.cumprod\n        dask.array.cumprod\n        Dataset.cumprod\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.groupby("labels").cumprod()\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Dimensions without coordinates: time\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 4.0 1.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.groupby("labels").cumprod(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Dimensions without coordinates: time\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 4.0 nan\n        '
        return self.reduce(duck_array_ops.cumprod, dim=dim, skipna=skipna, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

class DatasetResampleAggregations:
    _obj: Dataset

    def reduce(self, func: Callable[..., Any], dim: Dims=None, *, axis: int | Sequence[int] | None=None, keep_attrs: bool | None=None, keepdims: bool=False, **kwargs: Any) -> Dataset:
        if False:
            print('Hello World!')
        raise NotImplementedError()

    def _flox_reduce(self, dim: Dims, **kwargs: Any) -> Dataset:
        if False:
            for i in range(10):
                print('nop')
        raise NotImplementedError()

    def count(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            return 10
        '\n        Reduce this Dataset\'s data by applying ``count`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``count``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``count`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``count`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        pandas.DataFrame.count\n        dask.dataframe.DataFrame.count\n        Dataset.count\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.resample(time="3M").count()\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) int64 1 3 1\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='count', dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.count, dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def all(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            print('Hello World!')
        '\n        Reduce this Dataset\'s data by applying ``all`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``all``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``all`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``all`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.all\n        dask.array.all\n        Dataset.all\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([True, True, True, True, True, False], dtype=bool),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) bool True True True True True False\n\n        >>> ds.resample(time="3M").all()\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) bool True True False\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='all', dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.array_all, dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def any(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            i = 10
            return i + 15
        '\n        Reduce this Dataset\'s data by applying ``any`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``any``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``any`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``any`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.any\n        dask.array.any\n        Dataset.any\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([True, True, True, True, True, False], dtype=bool),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) bool True True True True True False\n\n        >>> ds.resample(time="3M").any()\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) bool True True True\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='any', dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.array_any, dim=dim, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def max(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            for i in range(10):
                print('nop')
        '\n        Reduce this Dataset\'s data by applying ``max`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``max``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``max`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``max`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.max\n        dask.array.max\n        Dataset.max\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.resample(time="3M").max()\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 1.0 3.0 2.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.resample(time="3M").max(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 1.0 3.0 nan\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='max', dim=dim, skipna=skipna, numeric_only=False, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.max, dim=dim, skipna=skipna, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def min(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            while True:
                i = 10
        '\n        Reduce this Dataset\'s data by applying ``min`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``min``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``min`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``min`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.min\n        dask.array.min\n        Dataset.min\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.resample(time="3M").min()\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 1.0 0.0 2.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.resample(time="3M").min(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 1.0 0.0 nan\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='min', dim=dim, skipna=skipna, numeric_only=False, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.min, dim=dim, skipna=skipna, numeric_only=False, keep_attrs=keep_attrs, **kwargs)

    def mean(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            return 10
        '\n        Reduce this Dataset\'s data by applying ``mean`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``mean``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``mean`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``mean`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.mean\n        dask.array.mean\n        Dataset.mean\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.resample(time="3M").mean()\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 1.0 1.667 2.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.resample(time="3M").mean(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 1.0 1.667 nan\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='mean', dim=dim, skipna=skipna, numeric_only=True, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.mean, dim=dim, skipna=skipna, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def prod(self, dim: Dims=None, *, skipna: bool | None=None, min_count: int | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            return 10
        '\n        Reduce this Dataset\'s data by applying ``prod`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``prod``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        min_count : int or None, optional\n            The required number of valid values to perform the operation. If\n            fewer than min_count non-NA values are present the result will be\n            NA. Only used if skipna is set to True or defaults to True for the\n            array\'s dtype. Changed in version 0.17.0: if specified on an integer\n            array and skipna=True, the result will be a float array.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``prod`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``prod`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.prod\n        dask.array.prod\n        Dataset.prod\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.resample(time="3M").prod()\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 1.0 0.0 2.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.resample(time="3M").prod(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 1.0 0.0 nan\n\n        Specify ``min_count`` for finer control over when NaNs are ignored.\n\n        >>> ds.resample(time="3M").prod(skipna=True, min_count=2)\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 nan 0.0 nan\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='prod', dim=dim, skipna=skipna, min_count=min_count, numeric_only=True, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.prod, dim=dim, skipna=skipna, min_count=min_count, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def sum(self, dim: Dims=None, *, skipna: bool | None=None, min_count: int | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            return 10
        '\n        Reduce this Dataset\'s data by applying ``sum`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``sum``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        min_count : int or None, optional\n            The required number of valid values to perform the operation. If\n            fewer than min_count non-NA values are present the result will be\n            NA. Only used if skipna is set to True or defaults to True for the\n            array\'s dtype. Changed in version 0.17.0: if specified on an integer\n            array and skipna=True, the result will be a float array.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``sum`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``sum`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.sum\n        dask.array.sum\n        Dataset.sum\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.resample(time="3M").sum()\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 1.0 5.0 2.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.resample(time="3M").sum(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 1.0 5.0 nan\n\n        Specify ``min_count`` for finer control over when NaNs are ignored.\n\n        >>> ds.resample(time="3M").sum(skipna=True, min_count=2)\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 nan 5.0 nan\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='sum', dim=dim, skipna=skipna, min_count=min_count, numeric_only=True, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.sum, dim=dim, skipna=skipna, min_count=min_count, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def std(self, dim: Dims=None, *, skipna: bool | None=None, ddof: int=0, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            i = 10
            return i + 15
        '\n        Reduce this Dataset\'s data by applying ``std`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``std``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        ddof : int, default: 0\n            “Delta Degrees of Freedom”: the divisor used in the calculation is ``N - ddof``,\n            where ``N`` represents the number of elements.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``std`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``std`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.std\n        dask.array.std\n        Dataset.std\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.resample(time="3M").std()\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 0.0 1.247 0.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.resample(time="3M").std(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 0.0 1.247 nan\n\n        Specify ``ddof=1`` for an unbiased estimate.\n\n        >>> ds.resample(time="3M").std(skipna=True, ddof=1)\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 nan 1.528 nan\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='std', dim=dim, skipna=skipna, ddof=ddof, numeric_only=True, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.std, dim=dim, skipna=skipna, ddof=ddof, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def var(self, dim: Dims=None, *, skipna: bool | None=None, ddof: int=0, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            print('Hello World!')
        '\n        Reduce this Dataset\'s data by applying ``var`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``var``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        ddof : int, default: 0\n            “Delta Degrees of Freedom”: the divisor used in the calculation is ``N - ddof``,\n            where ``N`` represents the number of elements.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``var`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``var`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.var\n        dask.array.var\n        Dataset.var\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.resample(time="3M").var()\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 0.0 1.556 0.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.resample(time="3M").var(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 0.0 1.556 nan\n\n        Specify ``ddof=1`` for an unbiased estimate.\n\n        >>> ds.resample(time="3M").var(skipna=True, ddof=1)\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 nan 2.333 nan\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='var', dim=dim, skipna=skipna, ddof=ddof, numeric_only=True, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.var, dim=dim, skipna=skipna, ddof=ddof, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def median(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            return 10
        '\n        Reduce this Dataset\'s data by applying ``median`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``median``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``median`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``median`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.median\n        dask.array.median\n        Dataset.median\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.resample(time="3M").median()\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 1.0 2.0 2.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.resample(time="3M").median(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (time: 3)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        Data variables:\n            da       (time) float64 1.0 2.0 nan\n        '
        return self.reduce(duck_array_ops.median, dim=dim, skipna=skipna, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def cumsum(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            while True:
                i = 10
        '\n        Reduce this Dataset\'s data by applying ``cumsum`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``cumsum``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``cumsum`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``cumsum`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.cumsum\n        dask.array.cumsum\n        Dataset.cumsum\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.resample(time="3M").cumsum()\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Dimensions without coordinates: time\n        Data variables:\n            da       (time) float64 1.0 2.0 5.0 5.0 2.0 2.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.resample(time="3M").cumsum(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Dimensions without coordinates: time\n        Data variables:\n            da       (time) float64 1.0 2.0 5.0 5.0 2.0 nan\n        '
        return self.reduce(duck_array_ops.cumsum, dim=dim, skipna=skipna, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

    def cumprod(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> Dataset:
        if False:
            print('Hello World!')
        '\n        Reduce this Dataset\'s data by applying ``cumprod`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``cumprod``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``cumprod`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : Dataset\n            New Dataset with ``cumprod`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.cumprod\n        dask.array.cumprod\n        Dataset.cumprod\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> ds = xr.Dataset(dict(da=da))\n        >>> ds\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Data variables:\n            da       (time) float64 1.0 2.0 3.0 0.0 2.0 nan\n\n        >>> ds.resample(time="3M").cumprod()\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Dimensions without coordinates: time\n        Data variables:\n            da       (time) float64 1.0 2.0 6.0 0.0 2.0 2.0\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> ds.resample(time="3M").cumprod(skipna=False)\n        <xarray.Dataset>\n        Dimensions:  (time: 6)\n        Dimensions without coordinates: time\n        Data variables:\n            da       (time) float64 1.0 2.0 6.0 0.0 2.0 nan\n        '
        return self.reduce(duck_array_ops.cumprod, dim=dim, skipna=skipna, numeric_only=True, keep_attrs=keep_attrs, **kwargs)

class DataArrayGroupByAggregations:
    _obj: DataArray

    def reduce(self, func: Callable[..., Any], dim: Dims=None, *, axis: int | Sequence[int] | None=None, keep_attrs: bool | None=None, keepdims: bool=False, **kwargs: Any) -> DataArray:
        if False:
            for i in range(10):
                print('nop')
        raise NotImplementedError()

    def _flox_reduce(self, dim: Dims, **kwargs: Any) -> DataArray:
        if False:
            for i in range(10):
                print('nop')
        raise NotImplementedError()

    def count(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            while True:
                i = 10
        '\n        Reduce this DataArray\'s data by applying ``count`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``count``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``count`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``count`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        pandas.DataFrame.count\n        dask.dataframe.DataFrame.count\n        DataArray.count\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.groupby("labels").count()\n        <xarray.DataArray (labels: 3)>\n        array([1, 2, 2])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='count', dim=dim, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.count, dim=dim, keep_attrs=keep_attrs, **kwargs)

    def all(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            return 10
        '\n        Reduce this DataArray\'s data by applying ``all`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``all``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``all`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``all`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.all\n        dask.array.all\n        DataArray.all\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([True, True, True, True, True, False], dtype=bool),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ True,  True,  True,  True,  True, False])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.groupby("labels").all()\n        <xarray.DataArray (labels: 3)>\n        array([False,  True,  True])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='all', dim=dim, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.array_all, dim=dim, keep_attrs=keep_attrs, **kwargs)

    def any(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            while True:
                i = 10
        '\n        Reduce this DataArray\'s data by applying ``any`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``any``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``any`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``any`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.any\n        dask.array.any\n        DataArray.any\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([True, True, True, True, True, False], dtype=bool),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ True,  True,  True,  True,  True, False])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.groupby("labels").any()\n        <xarray.DataArray (labels: 3)>\n        array([ True,  True,  True])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='any', dim=dim, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.array_any, dim=dim, keep_attrs=keep_attrs, **kwargs)

    def max(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            print('Hello World!')
        '\n        Reduce this DataArray\'s data by applying ``max`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``max``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``max`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``max`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.max\n        dask.array.max\n        DataArray.max\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.groupby("labels").max()\n        <xarray.DataArray (labels: 3)>\n        array([1., 2., 3.])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.groupby("labels").max(skipna=False)\n        <xarray.DataArray (labels: 3)>\n        array([nan,  2.,  3.])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='max', dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.max, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def min(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            return 10
        '\n        Reduce this DataArray\'s data by applying ``min`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``min``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``min`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``min`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.min\n        dask.array.min\n        DataArray.min\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.groupby("labels").min()\n        <xarray.DataArray (labels: 3)>\n        array([1., 2., 0.])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.groupby("labels").min(skipna=False)\n        <xarray.DataArray (labels: 3)>\n        array([nan,  2.,  0.])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='min', dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.min, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def mean(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            for i in range(10):
                print('nop')
        '\n        Reduce this DataArray\'s data by applying ``mean`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``mean``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``mean`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``mean`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.mean\n        dask.array.mean\n        DataArray.mean\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.groupby("labels").mean()\n        <xarray.DataArray (labels: 3)>\n        array([1. , 2. , 1.5])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.groupby("labels").mean(skipna=False)\n        <xarray.DataArray (labels: 3)>\n        array([nan, 2. , 1.5])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='mean', dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.mean, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def prod(self, dim: Dims=None, *, skipna: bool | None=None, min_count: int | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            while True:
                i = 10
        '\n        Reduce this DataArray\'s data by applying ``prod`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``prod``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        min_count : int or None, optional\n            The required number of valid values to perform the operation. If\n            fewer than min_count non-NA values are present the result will be\n            NA. Only used if skipna is set to True or defaults to True for the\n            array\'s dtype. Changed in version 0.17.0: if specified on an integer\n            array and skipna=True, the result will be a float array.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``prod`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``prod`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.prod\n        dask.array.prod\n        DataArray.prod\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.groupby("labels").prod()\n        <xarray.DataArray (labels: 3)>\n        array([1., 4., 0.])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.groupby("labels").prod(skipna=False)\n        <xarray.DataArray (labels: 3)>\n        array([nan,  4.,  0.])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n\n        Specify ``min_count`` for finer control over when NaNs are ignored.\n\n        >>> da.groupby("labels").prod(skipna=True, min_count=2)\n        <xarray.DataArray (labels: 3)>\n        array([nan,  4.,  0.])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='prod', dim=dim, skipna=skipna, min_count=min_count, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.prod, dim=dim, skipna=skipna, min_count=min_count, keep_attrs=keep_attrs, **kwargs)

    def sum(self, dim: Dims=None, *, skipna: bool | None=None, min_count: int | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            while True:
                i = 10
        '\n        Reduce this DataArray\'s data by applying ``sum`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``sum``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        min_count : int or None, optional\n            The required number of valid values to perform the operation. If\n            fewer than min_count non-NA values are present the result will be\n            NA. Only used if skipna is set to True or defaults to True for the\n            array\'s dtype. Changed in version 0.17.0: if specified on an integer\n            array and skipna=True, the result will be a float array.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``sum`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``sum`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.sum\n        dask.array.sum\n        DataArray.sum\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.groupby("labels").sum()\n        <xarray.DataArray (labels: 3)>\n        array([1., 4., 3.])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.groupby("labels").sum(skipna=False)\n        <xarray.DataArray (labels: 3)>\n        array([nan,  4.,  3.])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n\n        Specify ``min_count`` for finer control over when NaNs are ignored.\n\n        >>> da.groupby("labels").sum(skipna=True, min_count=2)\n        <xarray.DataArray (labels: 3)>\n        array([nan,  4.,  3.])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='sum', dim=dim, skipna=skipna, min_count=min_count, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.sum, dim=dim, skipna=skipna, min_count=min_count, keep_attrs=keep_attrs, **kwargs)

    def std(self, dim: Dims=None, *, skipna: bool | None=None, ddof: int=0, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            return 10
        '\n        Reduce this DataArray\'s data by applying ``std`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``std``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        ddof : int, default: 0\n            “Delta Degrees of Freedom”: the divisor used in the calculation is ``N - ddof``,\n            where ``N`` represents the number of elements.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``std`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``std`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.std\n        dask.array.std\n        DataArray.std\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.groupby("labels").std()\n        <xarray.DataArray (labels: 3)>\n        array([0. , 0. , 1.5])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.groupby("labels").std(skipna=False)\n        <xarray.DataArray (labels: 3)>\n        array([nan, 0. , 1.5])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n\n        Specify ``ddof=1`` for an unbiased estimate.\n\n        >>> da.groupby("labels").std(skipna=True, ddof=1)\n        <xarray.DataArray (labels: 3)>\n        array([       nan, 0.        , 2.12132034])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='std', dim=dim, skipna=skipna, ddof=ddof, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.std, dim=dim, skipna=skipna, ddof=ddof, keep_attrs=keep_attrs, **kwargs)

    def var(self, dim: Dims=None, *, skipna: bool | None=None, ddof: int=0, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            i = 10
            return i + 15
        '\n        Reduce this DataArray\'s data by applying ``var`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``var``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        ddof : int, default: 0\n            “Delta Degrees of Freedom”: the divisor used in the calculation is ``N - ddof``,\n            where ``N`` represents the number of elements.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``var`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``var`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.var\n        dask.array.var\n        DataArray.var\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.groupby("labels").var()\n        <xarray.DataArray (labels: 3)>\n        array([0.  , 0.  , 2.25])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.groupby("labels").var(skipna=False)\n        <xarray.DataArray (labels: 3)>\n        array([ nan, 0.  , 2.25])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n\n        Specify ``ddof=1`` for an unbiased estimate.\n\n        >>> da.groupby("labels").var(skipna=True, ddof=1)\n        <xarray.DataArray (labels: 3)>\n        array([nan, 0. , 4.5])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='var', dim=dim, skipna=skipna, ddof=ddof, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.var, dim=dim, skipna=skipna, ddof=ddof, keep_attrs=keep_attrs, **kwargs)

    def median(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            for i in range(10):
                print('nop')
        '\n        Reduce this DataArray\'s data by applying ``median`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``median``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``median`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``median`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.median\n        dask.array.median\n        DataArray.median\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.groupby("labels").median()\n        <xarray.DataArray (labels: 3)>\n        array([1. , 2. , 1.5])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.groupby("labels").median(skipna=False)\n        <xarray.DataArray (labels: 3)>\n        array([nan, 2. , 1.5])\n        Coordinates:\n          * labels   (labels) object \'a\' \'b\' \'c\'\n        '
        return self.reduce(duck_array_ops.median, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def cumsum(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            print('Hello World!')
        '\n        Reduce this DataArray\'s data by applying ``cumsum`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``cumsum``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``cumsum`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``cumsum`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.cumsum\n        dask.array.cumsum\n        DataArray.cumsum\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.groupby("labels").cumsum()\n        <xarray.DataArray (time: 6)>\n        array([1., 2., 3., 3., 4., 1.])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.groupby("labels").cumsum(skipna=False)\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  3.,  4., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        '
        return self.reduce(duck_array_ops.cumsum, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def cumprod(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            print('Hello World!')
        '\n        Reduce this DataArray\'s data by applying ``cumprod`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``cumprod``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the GroupBy dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``cumprod`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``cumprod`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.cumprod\n        dask.array.cumprod\n        DataArray.cumprod\n        :ref:`groupby`\n            User guide on groupby operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up groupby computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        other methods might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.groupby("labels").cumprod()\n        <xarray.DataArray (time: 6)>\n        array([1., 2., 3., 0., 4., 1.])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.groupby("labels").cumprod(skipna=False)\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  4., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        '
        return self.reduce(duck_array_ops.cumprod, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

class DataArrayResampleAggregations:
    _obj: DataArray

    def reduce(self, func: Callable[..., Any], dim: Dims=None, *, axis: int | Sequence[int] | None=None, keep_attrs: bool | None=None, keepdims: bool=False, **kwargs: Any) -> DataArray:
        if False:
            while True:
                i = 10
        raise NotImplementedError()

    def _flox_reduce(self, dim: Dims, **kwargs: Any) -> DataArray:
        if False:
            print('Hello World!')
        raise NotImplementedError()

    def count(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            print('Hello World!')
        '\n        Reduce this DataArray\'s data by applying ``count`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``count``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``count`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``count`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        pandas.DataFrame.count\n        dask.dataframe.DataFrame.count\n        DataArray.count\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.resample(time="3M").count()\n        <xarray.DataArray (time: 3)>\n        array([1, 3, 1])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='count', dim=dim, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.count, dim=dim, keep_attrs=keep_attrs, **kwargs)

    def all(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            print('Hello World!')
        '\n        Reduce this DataArray\'s data by applying ``all`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``all``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``all`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``all`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.all\n        dask.array.all\n        DataArray.all\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([True, True, True, True, True, False], dtype=bool),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ True,  True,  True,  True,  True, False])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.resample(time="3M").all()\n        <xarray.DataArray (time: 3)>\n        array([ True,  True, False])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='all', dim=dim, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.array_all, dim=dim, keep_attrs=keep_attrs, **kwargs)

    def any(self, dim: Dims=None, *, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            print('Hello World!')
        '\n        Reduce this DataArray\'s data by applying ``any`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``any``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``any`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``any`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.any\n        dask.array.any\n        DataArray.any\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([True, True, True, True, True, False], dtype=bool),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ True,  True,  True,  True,  True, False])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.resample(time="3M").any()\n        <xarray.DataArray (time: 3)>\n        array([ True,  True,  True])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='any', dim=dim, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.array_any, dim=dim, keep_attrs=keep_attrs, **kwargs)

    def max(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            return 10
        '\n        Reduce this DataArray\'s data by applying ``max`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``max``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``max`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``max`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.max\n        dask.array.max\n        DataArray.max\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.resample(time="3M").max()\n        <xarray.DataArray (time: 3)>\n        array([1., 3., 2.])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.resample(time="3M").max(skipna=False)\n        <xarray.DataArray (time: 3)>\n        array([ 1.,  3., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='max', dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.max, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def min(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            print('Hello World!')
        '\n        Reduce this DataArray\'s data by applying ``min`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``min``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``min`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``min`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.min\n        dask.array.min\n        DataArray.min\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.resample(time="3M").min()\n        <xarray.DataArray (time: 3)>\n        array([1., 0., 2.])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.resample(time="3M").min(skipna=False)\n        <xarray.DataArray (time: 3)>\n        array([ 1.,  0., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='min', dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.min, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def mean(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            while True:
                i = 10
        '\n        Reduce this DataArray\'s data by applying ``mean`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``mean``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``mean`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``mean`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.mean\n        dask.array.mean\n        DataArray.mean\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.resample(time="3M").mean()\n        <xarray.DataArray (time: 3)>\n        array([1.        , 1.66666667, 2.        ])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.resample(time="3M").mean(skipna=False)\n        <xarray.DataArray (time: 3)>\n        array([1.        , 1.66666667,        nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='mean', dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.mean, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def prod(self, dim: Dims=None, *, skipna: bool | None=None, min_count: int | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            for i in range(10):
                print('nop')
        '\n        Reduce this DataArray\'s data by applying ``prod`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``prod``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        min_count : int or None, optional\n            The required number of valid values to perform the operation. If\n            fewer than min_count non-NA values are present the result will be\n            NA. Only used if skipna is set to True or defaults to True for the\n            array\'s dtype. Changed in version 0.17.0: if specified on an integer\n            array and skipna=True, the result will be a float array.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``prod`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``prod`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.prod\n        dask.array.prod\n        DataArray.prod\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.resample(time="3M").prod()\n        <xarray.DataArray (time: 3)>\n        array([1., 0., 2.])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.resample(time="3M").prod(skipna=False)\n        <xarray.DataArray (time: 3)>\n        array([ 1.,  0., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n\n        Specify ``min_count`` for finer control over when NaNs are ignored.\n\n        >>> da.resample(time="3M").prod(skipna=True, min_count=2)\n        <xarray.DataArray (time: 3)>\n        array([nan,  0., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='prod', dim=dim, skipna=skipna, min_count=min_count, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.prod, dim=dim, skipna=skipna, min_count=min_count, keep_attrs=keep_attrs, **kwargs)

    def sum(self, dim: Dims=None, *, skipna: bool | None=None, min_count: int | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            print('Hello World!')
        '\n        Reduce this DataArray\'s data by applying ``sum`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``sum``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        min_count : int or None, optional\n            The required number of valid values to perform the operation. If\n            fewer than min_count non-NA values are present the result will be\n            NA. Only used if skipna is set to True or defaults to True for the\n            array\'s dtype. Changed in version 0.17.0: if specified on an integer\n            array and skipna=True, the result will be a float array.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``sum`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``sum`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.sum\n        dask.array.sum\n        DataArray.sum\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.resample(time="3M").sum()\n        <xarray.DataArray (time: 3)>\n        array([1., 5., 2.])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.resample(time="3M").sum(skipna=False)\n        <xarray.DataArray (time: 3)>\n        array([ 1.,  5., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n\n        Specify ``min_count`` for finer control over when NaNs are ignored.\n\n        >>> da.resample(time="3M").sum(skipna=True, min_count=2)\n        <xarray.DataArray (time: 3)>\n        array([nan,  5., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='sum', dim=dim, skipna=skipna, min_count=min_count, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.sum, dim=dim, skipna=skipna, min_count=min_count, keep_attrs=keep_attrs, **kwargs)

    def std(self, dim: Dims=None, *, skipna: bool | None=None, ddof: int=0, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            return 10
        '\n        Reduce this DataArray\'s data by applying ``std`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``std``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        ddof : int, default: 0\n            “Delta Degrees of Freedom”: the divisor used in the calculation is ``N - ddof``,\n            where ``N`` represents the number of elements.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``std`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``std`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.std\n        dask.array.std\n        DataArray.std\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.resample(time="3M").std()\n        <xarray.DataArray (time: 3)>\n        array([0.        , 1.24721913, 0.        ])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.resample(time="3M").std(skipna=False)\n        <xarray.DataArray (time: 3)>\n        array([0.        , 1.24721913,        nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n\n        Specify ``ddof=1`` for an unbiased estimate.\n\n        >>> da.resample(time="3M").std(skipna=True, ddof=1)\n        <xarray.DataArray (time: 3)>\n        array([       nan, 1.52752523,        nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='std', dim=dim, skipna=skipna, ddof=ddof, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.std, dim=dim, skipna=skipna, ddof=ddof, keep_attrs=keep_attrs, **kwargs)

    def var(self, dim: Dims=None, *, skipna: bool | None=None, ddof: int=0, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            i = 10
            return i + 15
        '\n        Reduce this DataArray\'s data by applying ``var`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``var``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        ddof : int, default: 0\n            “Delta Degrees of Freedom”: the divisor used in the calculation is ``N - ddof``,\n            where ``N`` represents the number of elements.\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``var`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``var`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.var\n        dask.array.var\n        DataArray.var\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.resample(time="3M").var()\n        <xarray.DataArray (time: 3)>\n        array([0.        , 1.55555556, 0.        ])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.resample(time="3M").var(skipna=False)\n        <xarray.DataArray (time: 3)>\n        array([0.        , 1.55555556,        nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n\n        Specify ``ddof=1`` for an unbiased estimate.\n\n        >>> da.resample(time="3M").var(skipna=True, ddof=1)\n        <xarray.DataArray (time: 3)>\n        array([       nan, 2.33333333,        nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        '
        if flox_available and OPTIONS['use_flox'] and contains_only_chunked_or_numpy(self._obj):
            return self._flox_reduce(func='var', dim=dim, skipna=skipna, ddof=ddof, keep_attrs=keep_attrs, **kwargs)
        else:
            return self.reduce(duck_array_ops.var, dim=dim, skipna=skipna, ddof=ddof, keep_attrs=keep_attrs, **kwargs)

    def median(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            while True:
                i = 10
        '\n        Reduce this DataArray\'s data by applying ``median`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``median``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``median`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``median`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.median\n        dask.array.median\n        DataArray.median\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.resample(time="3M").median()\n        <xarray.DataArray (time: 3)>\n        array([1., 2., 2.])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.resample(time="3M").median(skipna=False)\n        <xarray.DataArray (time: 3)>\n        array([ 1.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-04-30 2001-07-31\n        '
        return self.reduce(duck_array_ops.median, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def cumsum(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            i = 10
            return i + 15
        '\n        Reduce this DataArray\'s data by applying ``cumsum`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``cumsum``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``cumsum`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``cumsum`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.cumsum\n        dask.array.cumsum\n        DataArray.cumsum\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.resample(time="3M").cumsum()\n        <xarray.DataArray (time: 6)>\n        array([1., 2., 5., 5., 2., 2.])\n        Coordinates:\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Dimensions without coordinates: time\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.resample(time="3M").cumsum(skipna=False)\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  5.,  5.,  2., nan])\n        Coordinates:\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Dimensions without coordinates: time\n        '
        return self.reduce(duck_array_ops.cumsum, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)

    def cumprod(self, dim: Dims=None, *, skipna: bool | None=None, keep_attrs: bool | None=None, **kwargs: Any) -> DataArray:
        if False:
            return 10
        '\n        Reduce this DataArray\'s data by applying ``cumprod`` along some dimension(s).\n\n        Parameters\n        ----------\n        dim : str, Iterable of Hashable, "..." or None, default: None\n            Name of dimension[s] along which to apply ``cumprod``. For e.g. ``dim="x"``\n            or ``dim=["x", "y"]``. If None, will reduce over the Resample dimensions.\n            If "...", will reduce over all dimensions.\n        skipna : bool or None, optional\n            If True, skip missing values (as marked by NaN). By default, only\n            skips missing values for float dtypes; other dtypes either do not\n            have a sentinel missing value (int) or ``skipna=True`` has not been\n            implemented (object, datetime64 or timedelta64).\n        keep_attrs : bool or None, optional\n            If True, ``attrs`` will be copied from the original\n            object to the new one.  If False, the new object will be\n            returned without attributes.\n        **kwargs : Any\n            Additional keyword arguments passed on to the appropriate array\n            function for calculating ``cumprod`` on this object\'s data.\n            These could include dask-specific kwargs like ``split_every``.\n\n        Returns\n        -------\n        reduced : DataArray\n            New DataArray with ``cumprod`` applied to its data and the\n            indicated dimension(s) removed\n\n        See Also\n        --------\n        numpy.cumprod\n        dask.array.cumprod\n        DataArray.cumprod\n        :ref:`resampling`\n            User guide on resampling operations.\n\n        Notes\n        -----\n        Use the ``flox`` package to significantly speed up resampling computations,\n        especially with dask arrays. Xarray will use flox by default if installed.\n        Pass flox-specific keyword arguments in ``**kwargs``.\n        The default choice is ``method="cohorts"`` which generalizes the best,\n        ``method="blockwise"`` might work better for your problem.\n        See the `flox documentation <https://flox.readthedocs.io>`_ for more.\n\n        Non-numeric variables will be removed prior to reducing.\n\n        Examples\n        --------\n        >>> da = xr.DataArray(\n        ...     np.array([1, 2, 3, 0, 2, np.nan]),\n        ...     dims="time",\n        ...     coords=dict(\n        ...         time=("time", pd.date_range("2001-01-01", freq="M", periods=6)),\n        ...         labels=("time", np.array(["a", "b", "c", "c", "b", "a"])),\n        ...     ),\n        ... )\n        >>> da\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  3.,  0.,  2., nan])\n        Coordinates:\n          * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n\n        >>> da.resample(time="3M").cumprod()\n        <xarray.DataArray (time: 6)>\n        array([1., 2., 6., 0., 2., 2.])\n        Coordinates:\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Dimensions without coordinates: time\n\n        Use ``skipna`` to control whether NaNs are ignored.\n\n        >>> da.resample(time="3M").cumprod(skipna=False)\n        <xarray.DataArray (time: 6)>\n        array([ 1.,  2.,  6.,  0.,  2., nan])\n        Coordinates:\n            labels   (time) <U1 \'a\' \'b\' \'c\' \'c\' \'b\' \'a\'\n        Dimensions without coordinates: time\n        '
        return self.reduce(duck_array_ops.cumprod, dim=dim, skipna=skipna, keep_attrs=keep_attrs, **kwargs)