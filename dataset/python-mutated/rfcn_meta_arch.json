[
    {
        "func_name": "__init__",
        "original": "def __init__(self, is_training, num_classes, image_resizer_fn, feature_extractor, number_of_stages, first_stage_anchor_generator, first_stage_target_assigner, first_stage_atrous_rate, first_stage_box_predictor_arg_scope_fn, first_stage_box_predictor_kernel_size, first_stage_box_predictor_depth, first_stage_minibatch_size, first_stage_sampler, first_stage_non_max_suppression_fn, first_stage_max_proposals, first_stage_localization_loss_weight, first_stage_objectness_loss_weight, crop_and_resize_fn, second_stage_target_assigner, second_stage_rfcn_box_predictor, second_stage_batch_size, second_stage_sampler, second_stage_non_max_suppression_fn, second_stage_score_conversion_fn, second_stage_localization_loss_weight, second_stage_classification_loss_weight, second_stage_classification_loss, hard_example_miner, parallel_iterations=16, add_summaries=True, clip_anchors_to_image=False, use_static_shapes=False, resize_masks=False, freeze_batchnorm=False, return_raw_detections_during_predict=False):\n    \"\"\"RFCNMetaArch Constructor.\n\n    Args:\n      is_training: A boolean indicating whether the training version of the\n        computation graph should be constructed.\n      num_classes: Number of classes.  Note that num_classes *does not*\n        include the background category, so if groundtruth labels take values\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n        assigned classification targets can range from {0,... K}).\n      image_resizer_fn: A callable for image resizing.  This callable always\n        takes a rank-3 image tensor (corresponding to a single image) and\n        returns a rank-3 image tensor, possibly with new spatial dimensions.\n        See builders/image_resizer_builder.py.\n      feature_extractor: A FasterRCNNFeatureExtractor object.\n      number_of_stages:  Valid values are {1, 2}. If 1 will only construct the\n        Region Proposal Network (RPN) part of the model.\n      first_stage_anchor_generator: An anchor_generator.AnchorGenerator object\n        (note that currently we only support\n        grid_anchor_generator.GridAnchorGenerator objects)\n      first_stage_target_assigner: Target assigner to use for first stage of\n        R-FCN (RPN).\n      first_stage_atrous_rate: A single integer indicating the atrous rate for\n        the single convolution op which is applied to the `rpn_features_to_crop`\n        tensor to obtain a tensor to be used for box prediction. Some feature\n        extractors optionally allow for producing feature maps computed at\n        denser resolutions.  The atrous rate is used to compensate for the\n        denser feature maps by using an effectively larger receptive field.\n        (This should typically be set to 1).\n      first_stage_box_predictor_arg_scope_fn: Either a\n        Keras layer hyperparams object or a function to construct tf-slim\n        arg_scope for conv2d, separable_conv2d and fully_connected ops. Used\n        for the RPN box predictor. If it is a keras hyperparams object the\n        RPN box predictor will be a Keras model. If it is a function to\n        construct an arg scope it will be a tf-slim box predictor.\n      first_stage_box_predictor_kernel_size: Kernel size to use for the\n        convolution op just prior to RPN box predictions.\n      first_stage_box_predictor_depth: Output depth for the convolution op\n        just prior to RPN box predictions.\n      first_stage_minibatch_size: The \"batch size\" to use for computing the\n        objectness and location loss of the region proposal network. This\n        \"batch size\" refers to the number of anchors selected as contributing\n        to the loss function for any given image within the image batch and is\n        only called \"batch_size\" due to terminology from the Faster R-CNN paper.\n      first_stage_sampler: The sampler for the boxes used to calculate the RPN\n        loss after the first stage.\n      first_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\n        callable that takes `boxes`, `scores` and optional `clip_window`(with\n        all other inputs already set) and returns a dictionary containing\n        tensors with keys: `detection_boxes`, `detection_scores`,\n        `detection_classes`, `num_detections`. This is used to perform non max\n        suppression  on the boxes predicted by the Region Proposal Network\n        (RPN).\n        See `post_processing.batch_multiclass_non_max_suppression` for the type\n        and shape of these tensors.\n      first_stage_max_proposals: Maximum number of boxes to retain after\n        performing Non-Max Suppression (NMS) on the boxes predicted by the\n        Region Proposal Network (RPN).\n      first_stage_localization_loss_weight: A float\n      first_stage_objectness_loss_weight: A float\n      crop_and_resize_fn: A differentiable resampler to use for cropping RPN\n        proposal features.\n      second_stage_target_assigner: Target assigner to use for second stage of\n        R-FCN. If the model is configured with multiple prediction heads, this\n        target assigner is used to generate targets for all heads (with the\n        correct `unmatched_class_label`).\n      second_stage_rfcn_box_predictor: RFCN box predictor to use for\n        second stage.\n      second_stage_batch_size: The batch size used for computing the\n        classification and refined location loss of the box classifier.  This\n        \"batch size\" refers to the number of proposals selected as contributing\n        to the loss function for any given image within the image batch and is\n        only called \"batch_size\" due to terminology from the Faster R-CNN paper.\n      second_stage_sampler: The sampler for the boxes used for second stage\n        box classifier.\n      second_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\n        callable that takes `boxes`, `scores`, optional `clip_window` and\n        optional (kwarg) `mask` inputs (with all other inputs already set)\n        and returns a dictionary containing tensors with keys:\n        `detection_boxes`, `detection_scores`, `detection_classes`,\n        `num_detections`, and (optionally) `detection_masks`. See\n        `post_processing.batch_multiclass_non_max_suppression` for the type and\n        shape of these tensors.\n      second_stage_score_conversion_fn: Callable elementwise nonlinearity\n        (that takes tensors as inputs and returns tensors).  This is usually\n        used to convert logits to probabilities.\n      second_stage_localization_loss_weight: A float\n      second_stage_classification_loss_weight: A float\n      second_stage_classification_loss: A string indicating which loss function\n        to use, supports 'softmax' and 'sigmoid'.\n      hard_example_miner:  A losses.HardExampleMiner object (can be None).\n      parallel_iterations: (Optional) The number of iterations allowed to run\n        in parallel for calls to tf.map_fn.\n      add_summaries: boolean (default: True) controlling whether summary ops\n        should be added to tensorflow graph.\n      clip_anchors_to_image: The anchors generated are clip to the\n        window size without filtering the nonoverlapping anchors. This generates\n        a static number of anchors. This argument is unused.\n      use_static_shapes: If True, uses implementation of ops with static shape\n        guarantees.\n      resize_masks: Indicates whether the masks presend in the groundtruth\n        should be resized in the model with `image_resizer_fn`\n      freeze_batchnorm: Whether to freeze batch norm parameters during\n        training or not. When training with a small batch size (e.g. 1), it is\n        desirable to freeze batch norm update and use pretrained batch norm\n        params.\n      return_raw_detections_during_predict: Whether to return raw detection\n        boxes in the predict() method. These are decoded boxes that have not\n        been through postprocessing (i.e. NMS). Default False.\n\n    Raises:\n      ValueError: If `second_stage_batch_size` > `first_stage_max_proposals`\n      ValueError: If first_stage_anchor_generator is not of type\n        grid_anchor_generator.GridAnchorGenerator.\n    \"\"\"\n    super(RFCNMetaArch, self).__init__(is_training, num_classes, image_resizer_fn, feature_extractor, number_of_stages, first_stage_anchor_generator, first_stage_target_assigner, first_stage_atrous_rate, first_stage_box_predictor_arg_scope_fn, first_stage_box_predictor_kernel_size, first_stage_box_predictor_depth, first_stage_minibatch_size, first_stage_sampler, first_stage_non_max_suppression_fn, first_stage_max_proposals, first_stage_localization_loss_weight, first_stage_objectness_loss_weight, crop_and_resize_fn, None, None, None, second_stage_target_assigner, None, second_stage_batch_size, second_stage_sampler, second_stage_non_max_suppression_fn, second_stage_score_conversion_fn, second_stage_localization_loss_weight, second_stage_classification_loss_weight, second_stage_classification_loss, 1.0, hard_example_miner, parallel_iterations, add_summaries, clip_anchors_to_image, use_static_shapes, resize_masks, freeze_batchnorm=freeze_batchnorm, return_raw_detections_during_predict=return_raw_detections_during_predict)\n    self._rfcn_box_predictor = second_stage_rfcn_box_predictor",
        "mutated": [
            "def __init__(self, is_training, num_classes, image_resizer_fn, feature_extractor, number_of_stages, first_stage_anchor_generator, first_stage_target_assigner, first_stage_atrous_rate, first_stage_box_predictor_arg_scope_fn, first_stage_box_predictor_kernel_size, first_stage_box_predictor_depth, first_stage_minibatch_size, first_stage_sampler, first_stage_non_max_suppression_fn, first_stage_max_proposals, first_stage_localization_loss_weight, first_stage_objectness_loss_weight, crop_and_resize_fn, second_stage_target_assigner, second_stage_rfcn_box_predictor, second_stage_batch_size, second_stage_sampler, second_stage_non_max_suppression_fn, second_stage_score_conversion_fn, second_stage_localization_loss_weight, second_stage_classification_loss_weight, second_stage_classification_loss, hard_example_miner, parallel_iterations=16, add_summaries=True, clip_anchors_to_image=False, use_static_shapes=False, resize_masks=False, freeze_batchnorm=False, return_raw_detections_during_predict=False):\n    if False:\n        i = 10\n    'RFCNMetaArch Constructor.\\n\\n    Args:\\n      is_training: A boolean indicating whether the training version of the\\n        computation graph should be constructed.\\n      num_classes: Number of classes.  Note that num_classes *does not*\\n        include the background category, so if groundtruth labels take values\\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\\n        assigned classification targets can range from {0,... K}).\\n      image_resizer_fn: A callable for image resizing.  This callable always\\n        takes a rank-3 image tensor (corresponding to a single image) and\\n        returns a rank-3 image tensor, possibly with new spatial dimensions.\\n        See builders/image_resizer_builder.py.\\n      feature_extractor: A FasterRCNNFeatureExtractor object.\\n      number_of_stages:  Valid values are {1, 2}. If 1 will only construct the\\n        Region Proposal Network (RPN) part of the model.\\n      first_stage_anchor_generator: An anchor_generator.AnchorGenerator object\\n        (note that currently we only support\\n        grid_anchor_generator.GridAnchorGenerator objects)\\n      first_stage_target_assigner: Target assigner to use for first stage of\\n        R-FCN (RPN).\\n      first_stage_atrous_rate: A single integer indicating the atrous rate for\\n        the single convolution op which is applied to the `rpn_features_to_crop`\\n        tensor to obtain a tensor to be used for box prediction. Some feature\\n        extractors optionally allow for producing feature maps computed at\\n        denser resolutions.  The atrous rate is used to compensate for the\\n        denser feature maps by using an effectively larger receptive field.\\n        (This should typically be set to 1).\\n      first_stage_box_predictor_arg_scope_fn: Either a\\n        Keras layer hyperparams object or a function to construct tf-slim\\n        arg_scope for conv2d, separable_conv2d and fully_connected ops. Used\\n        for the RPN box predictor. If it is a keras hyperparams object the\\n        RPN box predictor will be a Keras model. If it is a function to\\n        construct an arg scope it will be a tf-slim box predictor.\\n      first_stage_box_predictor_kernel_size: Kernel size to use for the\\n        convolution op just prior to RPN box predictions.\\n      first_stage_box_predictor_depth: Output depth for the convolution op\\n        just prior to RPN box predictions.\\n      first_stage_minibatch_size: The \"batch size\" to use for computing the\\n        objectness and location loss of the region proposal network. This\\n        \"batch size\" refers to the number of anchors selected as contributing\\n        to the loss function for any given image within the image batch and is\\n        only called \"batch_size\" due to terminology from the Faster R-CNN paper.\\n      first_stage_sampler: The sampler for the boxes used to calculate the RPN\\n        loss after the first stage.\\n      first_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\\n        callable that takes `boxes`, `scores` and optional `clip_window`(with\\n        all other inputs already set) and returns a dictionary containing\\n        tensors with keys: `detection_boxes`, `detection_scores`,\\n        `detection_classes`, `num_detections`. This is used to perform non max\\n        suppression  on the boxes predicted by the Region Proposal Network\\n        (RPN).\\n        See `post_processing.batch_multiclass_non_max_suppression` for the type\\n        and shape of these tensors.\\n      first_stage_max_proposals: Maximum number of boxes to retain after\\n        performing Non-Max Suppression (NMS) on the boxes predicted by the\\n        Region Proposal Network (RPN).\\n      first_stage_localization_loss_weight: A float\\n      first_stage_objectness_loss_weight: A float\\n      crop_and_resize_fn: A differentiable resampler to use for cropping RPN\\n        proposal features.\\n      second_stage_target_assigner: Target assigner to use for second stage of\\n        R-FCN. If the model is configured with multiple prediction heads, this\\n        target assigner is used to generate targets for all heads (with the\\n        correct `unmatched_class_label`).\\n      second_stage_rfcn_box_predictor: RFCN box predictor to use for\\n        second stage.\\n      second_stage_batch_size: The batch size used for computing the\\n        classification and refined location loss of the box classifier.  This\\n        \"batch size\" refers to the number of proposals selected as contributing\\n        to the loss function for any given image within the image batch and is\\n        only called \"batch_size\" due to terminology from the Faster R-CNN paper.\\n      second_stage_sampler: The sampler for the boxes used for second stage\\n        box classifier.\\n      second_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\\n        callable that takes `boxes`, `scores`, optional `clip_window` and\\n        optional (kwarg) `mask` inputs (with all other inputs already set)\\n        and returns a dictionary containing tensors with keys:\\n        `detection_boxes`, `detection_scores`, `detection_classes`,\\n        `num_detections`, and (optionally) `detection_masks`. See\\n        `post_processing.batch_multiclass_non_max_suppression` for the type and\\n        shape of these tensors.\\n      second_stage_score_conversion_fn: Callable elementwise nonlinearity\\n        (that takes tensors as inputs and returns tensors).  This is usually\\n        used to convert logits to probabilities.\\n      second_stage_localization_loss_weight: A float\\n      second_stage_classification_loss_weight: A float\\n      second_stage_classification_loss: A string indicating which loss function\\n        to use, supports \\'softmax\\' and \\'sigmoid\\'.\\n      hard_example_miner:  A losses.HardExampleMiner object (can be None).\\n      parallel_iterations: (Optional) The number of iterations allowed to run\\n        in parallel for calls to tf.map_fn.\\n      add_summaries: boolean (default: True) controlling whether summary ops\\n        should be added to tensorflow graph.\\n      clip_anchors_to_image: The anchors generated are clip to the\\n        window size without filtering the nonoverlapping anchors. This generates\\n        a static number of anchors. This argument is unused.\\n      use_static_shapes: If True, uses implementation of ops with static shape\\n        guarantees.\\n      resize_masks: Indicates whether the masks presend in the groundtruth\\n        should be resized in the model with `image_resizer_fn`\\n      freeze_batchnorm: Whether to freeze batch norm parameters during\\n        training or not. When training with a small batch size (e.g. 1), it is\\n        desirable to freeze batch norm update and use pretrained batch norm\\n        params.\\n      return_raw_detections_during_predict: Whether to return raw detection\\n        boxes in the predict() method. These are decoded boxes that have not\\n        been through postprocessing (i.e. NMS). Default False.\\n\\n    Raises:\\n      ValueError: If `second_stage_batch_size` > `first_stage_max_proposals`\\n      ValueError: If first_stage_anchor_generator is not of type\\n        grid_anchor_generator.GridAnchorGenerator.\\n    '\n    super(RFCNMetaArch, self).__init__(is_training, num_classes, image_resizer_fn, feature_extractor, number_of_stages, first_stage_anchor_generator, first_stage_target_assigner, first_stage_atrous_rate, first_stage_box_predictor_arg_scope_fn, first_stage_box_predictor_kernel_size, first_stage_box_predictor_depth, first_stage_minibatch_size, first_stage_sampler, first_stage_non_max_suppression_fn, first_stage_max_proposals, first_stage_localization_loss_weight, first_stage_objectness_loss_weight, crop_and_resize_fn, None, None, None, second_stage_target_assigner, None, second_stage_batch_size, second_stage_sampler, second_stage_non_max_suppression_fn, second_stage_score_conversion_fn, second_stage_localization_loss_weight, second_stage_classification_loss_weight, second_stage_classification_loss, 1.0, hard_example_miner, parallel_iterations, add_summaries, clip_anchors_to_image, use_static_shapes, resize_masks, freeze_batchnorm=freeze_batchnorm, return_raw_detections_during_predict=return_raw_detections_during_predict)\n    self._rfcn_box_predictor = second_stage_rfcn_box_predictor",
            "def __init__(self, is_training, num_classes, image_resizer_fn, feature_extractor, number_of_stages, first_stage_anchor_generator, first_stage_target_assigner, first_stage_atrous_rate, first_stage_box_predictor_arg_scope_fn, first_stage_box_predictor_kernel_size, first_stage_box_predictor_depth, first_stage_minibatch_size, first_stage_sampler, first_stage_non_max_suppression_fn, first_stage_max_proposals, first_stage_localization_loss_weight, first_stage_objectness_loss_weight, crop_and_resize_fn, second_stage_target_assigner, second_stage_rfcn_box_predictor, second_stage_batch_size, second_stage_sampler, second_stage_non_max_suppression_fn, second_stage_score_conversion_fn, second_stage_localization_loss_weight, second_stage_classification_loss_weight, second_stage_classification_loss, hard_example_miner, parallel_iterations=16, add_summaries=True, clip_anchors_to_image=False, use_static_shapes=False, resize_masks=False, freeze_batchnorm=False, return_raw_detections_during_predict=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'RFCNMetaArch Constructor.\\n\\n    Args:\\n      is_training: A boolean indicating whether the training version of the\\n        computation graph should be constructed.\\n      num_classes: Number of classes.  Note that num_classes *does not*\\n        include the background category, so if groundtruth labels take values\\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\\n        assigned classification targets can range from {0,... K}).\\n      image_resizer_fn: A callable for image resizing.  This callable always\\n        takes a rank-3 image tensor (corresponding to a single image) and\\n        returns a rank-3 image tensor, possibly with new spatial dimensions.\\n        See builders/image_resizer_builder.py.\\n      feature_extractor: A FasterRCNNFeatureExtractor object.\\n      number_of_stages:  Valid values are {1, 2}. If 1 will only construct the\\n        Region Proposal Network (RPN) part of the model.\\n      first_stage_anchor_generator: An anchor_generator.AnchorGenerator object\\n        (note that currently we only support\\n        grid_anchor_generator.GridAnchorGenerator objects)\\n      first_stage_target_assigner: Target assigner to use for first stage of\\n        R-FCN (RPN).\\n      first_stage_atrous_rate: A single integer indicating the atrous rate for\\n        the single convolution op which is applied to the `rpn_features_to_crop`\\n        tensor to obtain a tensor to be used for box prediction. Some feature\\n        extractors optionally allow for producing feature maps computed at\\n        denser resolutions.  The atrous rate is used to compensate for the\\n        denser feature maps by using an effectively larger receptive field.\\n        (This should typically be set to 1).\\n      first_stage_box_predictor_arg_scope_fn: Either a\\n        Keras layer hyperparams object or a function to construct tf-slim\\n        arg_scope for conv2d, separable_conv2d and fully_connected ops. Used\\n        for the RPN box predictor. If it is a keras hyperparams object the\\n        RPN box predictor will be a Keras model. If it is a function to\\n        construct an arg scope it will be a tf-slim box predictor.\\n      first_stage_box_predictor_kernel_size: Kernel size to use for the\\n        convolution op just prior to RPN box predictions.\\n      first_stage_box_predictor_depth: Output depth for the convolution op\\n        just prior to RPN box predictions.\\n      first_stage_minibatch_size: The \"batch size\" to use for computing the\\n        objectness and location loss of the region proposal network. This\\n        \"batch size\" refers to the number of anchors selected as contributing\\n        to the loss function for any given image within the image batch and is\\n        only called \"batch_size\" due to terminology from the Faster R-CNN paper.\\n      first_stage_sampler: The sampler for the boxes used to calculate the RPN\\n        loss after the first stage.\\n      first_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\\n        callable that takes `boxes`, `scores` and optional `clip_window`(with\\n        all other inputs already set) and returns a dictionary containing\\n        tensors with keys: `detection_boxes`, `detection_scores`,\\n        `detection_classes`, `num_detections`. This is used to perform non max\\n        suppression  on the boxes predicted by the Region Proposal Network\\n        (RPN).\\n        See `post_processing.batch_multiclass_non_max_suppression` for the type\\n        and shape of these tensors.\\n      first_stage_max_proposals: Maximum number of boxes to retain after\\n        performing Non-Max Suppression (NMS) on the boxes predicted by the\\n        Region Proposal Network (RPN).\\n      first_stage_localization_loss_weight: A float\\n      first_stage_objectness_loss_weight: A float\\n      crop_and_resize_fn: A differentiable resampler to use for cropping RPN\\n        proposal features.\\n      second_stage_target_assigner: Target assigner to use for second stage of\\n        R-FCN. If the model is configured with multiple prediction heads, this\\n        target assigner is used to generate targets for all heads (with the\\n        correct `unmatched_class_label`).\\n      second_stage_rfcn_box_predictor: RFCN box predictor to use for\\n        second stage.\\n      second_stage_batch_size: The batch size used for computing the\\n        classification and refined location loss of the box classifier.  This\\n        \"batch size\" refers to the number of proposals selected as contributing\\n        to the loss function for any given image within the image batch and is\\n        only called \"batch_size\" due to terminology from the Faster R-CNN paper.\\n      second_stage_sampler: The sampler for the boxes used for second stage\\n        box classifier.\\n      second_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\\n        callable that takes `boxes`, `scores`, optional `clip_window` and\\n        optional (kwarg) `mask` inputs (with all other inputs already set)\\n        and returns a dictionary containing tensors with keys:\\n        `detection_boxes`, `detection_scores`, `detection_classes`,\\n        `num_detections`, and (optionally) `detection_masks`. See\\n        `post_processing.batch_multiclass_non_max_suppression` for the type and\\n        shape of these tensors.\\n      second_stage_score_conversion_fn: Callable elementwise nonlinearity\\n        (that takes tensors as inputs and returns tensors).  This is usually\\n        used to convert logits to probabilities.\\n      second_stage_localization_loss_weight: A float\\n      second_stage_classification_loss_weight: A float\\n      second_stage_classification_loss: A string indicating which loss function\\n        to use, supports \\'softmax\\' and \\'sigmoid\\'.\\n      hard_example_miner:  A losses.HardExampleMiner object (can be None).\\n      parallel_iterations: (Optional) The number of iterations allowed to run\\n        in parallel for calls to tf.map_fn.\\n      add_summaries: boolean (default: True) controlling whether summary ops\\n        should be added to tensorflow graph.\\n      clip_anchors_to_image: The anchors generated are clip to the\\n        window size without filtering the nonoverlapping anchors. This generates\\n        a static number of anchors. This argument is unused.\\n      use_static_shapes: If True, uses implementation of ops with static shape\\n        guarantees.\\n      resize_masks: Indicates whether the masks presend in the groundtruth\\n        should be resized in the model with `image_resizer_fn`\\n      freeze_batchnorm: Whether to freeze batch norm parameters during\\n        training or not. When training with a small batch size (e.g. 1), it is\\n        desirable to freeze batch norm update and use pretrained batch norm\\n        params.\\n      return_raw_detections_during_predict: Whether to return raw detection\\n        boxes in the predict() method. These are decoded boxes that have not\\n        been through postprocessing (i.e. NMS). Default False.\\n\\n    Raises:\\n      ValueError: If `second_stage_batch_size` > `first_stage_max_proposals`\\n      ValueError: If first_stage_anchor_generator is not of type\\n        grid_anchor_generator.GridAnchorGenerator.\\n    '\n    super(RFCNMetaArch, self).__init__(is_training, num_classes, image_resizer_fn, feature_extractor, number_of_stages, first_stage_anchor_generator, first_stage_target_assigner, first_stage_atrous_rate, first_stage_box_predictor_arg_scope_fn, first_stage_box_predictor_kernel_size, first_stage_box_predictor_depth, first_stage_minibatch_size, first_stage_sampler, first_stage_non_max_suppression_fn, first_stage_max_proposals, first_stage_localization_loss_weight, first_stage_objectness_loss_weight, crop_and_resize_fn, None, None, None, second_stage_target_assigner, None, second_stage_batch_size, second_stage_sampler, second_stage_non_max_suppression_fn, second_stage_score_conversion_fn, second_stage_localization_loss_weight, second_stage_classification_loss_weight, second_stage_classification_loss, 1.0, hard_example_miner, parallel_iterations, add_summaries, clip_anchors_to_image, use_static_shapes, resize_masks, freeze_batchnorm=freeze_batchnorm, return_raw_detections_during_predict=return_raw_detections_during_predict)\n    self._rfcn_box_predictor = second_stage_rfcn_box_predictor",
            "def __init__(self, is_training, num_classes, image_resizer_fn, feature_extractor, number_of_stages, first_stage_anchor_generator, first_stage_target_assigner, first_stage_atrous_rate, first_stage_box_predictor_arg_scope_fn, first_stage_box_predictor_kernel_size, first_stage_box_predictor_depth, first_stage_minibatch_size, first_stage_sampler, first_stage_non_max_suppression_fn, first_stage_max_proposals, first_stage_localization_loss_weight, first_stage_objectness_loss_weight, crop_and_resize_fn, second_stage_target_assigner, second_stage_rfcn_box_predictor, second_stage_batch_size, second_stage_sampler, second_stage_non_max_suppression_fn, second_stage_score_conversion_fn, second_stage_localization_loss_weight, second_stage_classification_loss_weight, second_stage_classification_loss, hard_example_miner, parallel_iterations=16, add_summaries=True, clip_anchors_to_image=False, use_static_shapes=False, resize_masks=False, freeze_batchnorm=False, return_raw_detections_during_predict=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'RFCNMetaArch Constructor.\\n\\n    Args:\\n      is_training: A boolean indicating whether the training version of the\\n        computation graph should be constructed.\\n      num_classes: Number of classes.  Note that num_classes *does not*\\n        include the background category, so if groundtruth labels take values\\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\\n        assigned classification targets can range from {0,... K}).\\n      image_resizer_fn: A callable for image resizing.  This callable always\\n        takes a rank-3 image tensor (corresponding to a single image) and\\n        returns a rank-3 image tensor, possibly with new spatial dimensions.\\n        See builders/image_resizer_builder.py.\\n      feature_extractor: A FasterRCNNFeatureExtractor object.\\n      number_of_stages:  Valid values are {1, 2}. If 1 will only construct the\\n        Region Proposal Network (RPN) part of the model.\\n      first_stage_anchor_generator: An anchor_generator.AnchorGenerator object\\n        (note that currently we only support\\n        grid_anchor_generator.GridAnchorGenerator objects)\\n      first_stage_target_assigner: Target assigner to use for first stage of\\n        R-FCN (RPN).\\n      first_stage_atrous_rate: A single integer indicating the atrous rate for\\n        the single convolution op which is applied to the `rpn_features_to_crop`\\n        tensor to obtain a tensor to be used for box prediction. Some feature\\n        extractors optionally allow for producing feature maps computed at\\n        denser resolutions.  The atrous rate is used to compensate for the\\n        denser feature maps by using an effectively larger receptive field.\\n        (This should typically be set to 1).\\n      first_stage_box_predictor_arg_scope_fn: Either a\\n        Keras layer hyperparams object or a function to construct tf-slim\\n        arg_scope for conv2d, separable_conv2d and fully_connected ops. Used\\n        for the RPN box predictor. If it is a keras hyperparams object the\\n        RPN box predictor will be a Keras model. If it is a function to\\n        construct an arg scope it will be a tf-slim box predictor.\\n      first_stage_box_predictor_kernel_size: Kernel size to use for the\\n        convolution op just prior to RPN box predictions.\\n      first_stage_box_predictor_depth: Output depth for the convolution op\\n        just prior to RPN box predictions.\\n      first_stage_minibatch_size: The \"batch size\" to use for computing the\\n        objectness and location loss of the region proposal network. This\\n        \"batch size\" refers to the number of anchors selected as contributing\\n        to the loss function for any given image within the image batch and is\\n        only called \"batch_size\" due to terminology from the Faster R-CNN paper.\\n      first_stage_sampler: The sampler for the boxes used to calculate the RPN\\n        loss after the first stage.\\n      first_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\\n        callable that takes `boxes`, `scores` and optional `clip_window`(with\\n        all other inputs already set) and returns a dictionary containing\\n        tensors with keys: `detection_boxes`, `detection_scores`,\\n        `detection_classes`, `num_detections`. This is used to perform non max\\n        suppression  on the boxes predicted by the Region Proposal Network\\n        (RPN).\\n        See `post_processing.batch_multiclass_non_max_suppression` for the type\\n        and shape of these tensors.\\n      first_stage_max_proposals: Maximum number of boxes to retain after\\n        performing Non-Max Suppression (NMS) on the boxes predicted by the\\n        Region Proposal Network (RPN).\\n      first_stage_localization_loss_weight: A float\\n      first_stage_objectness_loss_weight: A float\\n      crop_and_resize_fn: A differentiable resampler to use for cropping RPN\\n        proposal features.\\n      second_stage_target_assigner: Target assigner to use for second stage of\\n        R-FCN. If the model is configured with multiple prediction heads, this\\n        target assigner is used to generate targets for all heads (with the\\n        correct `unmatched_class_label`).\\n      second_stage_rfcn_box_predictor: RFCN box predictor to use for\\n        second stage.\\n      second_stage_batch_size: The batch size used for computing the\\n        classification and refined location loss of the box classifier.  This\\n        \"batch size\" refers to the number of proposals selected as contributing\\n        to the loss function for any given image within the image batch and is\\n        only called \"batch_size\" due to terminology from the Faster R-CNN paper.\\n      second_stage_sampler: The sampler for the boxes used for second stage\\n        box classifier.\\n      second_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\\n        callable that takes `boxes`, `scores`, optional `clip_window` and\\n        optional (kwarg) `mask` inputs (with all other inputs already set)\\n        and returns a dictionary containing tensors with keys:\\n        `detection_boxes`, `detection_scores`, `detection_classes`,\\n        `num_detections`, and (optionally) `detection_masks`. See\\n        `post_processing.batch_multiclass_non_max_suppression` for the type and\\n        shape of these tensors.\\n      second_stage_score_conversion_fn: Callable elementwise nonlinearity\\n        (that takes tensors as inputs and returns tensors).  This is usually\\n        used to convert logits to probabilities.\\n      second_stage_localization_loss_weight: A float\\n      second_stage_classification_loss_weight: A float\\n      second_stage_classification_loss: A string indicating which loss function\\n        to use, supports \\'softmax\\' and \\'sigmoid\\'.\\n      hard_example_miner:  A losses.HardExampleMiner object (can be None).\\n      parallel_iterations: (Optional) The number of iterations allowed to run\\n        in parallel for calls to tf.map_fn.\\n      add_summaries: boolean (default: True) controlling whether summary ops\\n        should be added to tensorflow graph.\\n      clip_anchors_to_image: The anchors generated are clip to the\\n        window size without filtering the nonoverlapping anchors. This generates\\n        a static number of anchors. This argument is unused.\\n      use_static_shapes: If True, uses implementation of ops with static shape\\n        guarantees.\\n      resize_masks: Indicates whether the masks presend in the groundtruth\\n        should be resized in the model with `image_resizer_fn`\\n      freeze_batchnorm: Whether to freeze batch norm parameters during\\n        training or not. When training with a small batch size (e.g. 1), it is\\n        desirable to freeze batch norm update and use pretrained batch norm\\n        params.\\n      return_raw_detections_during_predict: Whether to return raw detection\\n        boxes in the predict() method. These are decoded boxes that have not\\n        been through postprocessing (i.e. NMS). Default False.\\n\\n    Raises:\\n      ValueError: If `second_stage_batch_size` > `first_stage_max_proposals`\\n      ValueError: If first_stage_anchor_generator is not of type\\n        grid_anchor_generator.GridAnchorGenerator.\\n    '\n    super(RFCNMetaArch, self).__init__(is_training, num_classes, image_resizer_fn, feature_extractor, number_of_stages, first_stage_anchor_generator, first_stage_target_assigner, first_stage_atrous_rate, first_stage_box_predictor_arg_scope_fn, first_stage_box_predictor_kernel_size, first_stage_box_predictor_depth, first_stage_minibatch_size, first_stage_sampler, first_stage_non_max_suppression_fn, first_stage_max_proposals, first_stage_localization_loss_weight, first_stage_objectness_loss_weight, crop_and_resize_fn, None, None, None, second_stage_target_assigner, None, second_stage_batch_size, second_stage_sampler, second_stage_non_max_suppression_fn, second_stage_score_conversion_fn, second_stage_localization_loss_weight, second_stage_classification_loss_weight, second_stage_classification_loss, 1.0, hard_example_miner, parallel_iterations, add_summaries, clip_anchors_to_image, use_static_shapes, resize_masks, freeze_batchnorm=freeze_batchnorm, return_raw_detections_during_predict=return_raw_detections_during_predict)\n    self._rfcn_box_predictor = second_stage_rfcn_box_predictor",
            "def __init__(self, is_training, num_classes, image_resizer_fn, feature_extractor, number_of_stages, first_stage_anchor_generator, first_stage_target_assigner, first_stage_atrous_rate, first_stage_box_predictor_arg_scope_fn, first_stage_box_predictor_kernel_size, first_stage_box_predictor_depth, first_stage_minibatch_size, first_stage_sampler, first_stage_non_max_suppression_fn, first_stage_max_proposals, first_stage_localization_loss_weight, first_stage_objectness_loss_weight, crop_and_resize_fn, second_stage_target_assigner, second_stage_rfcn_box_predictor, second_stage_batch_size, second_stage_sampler, second_stage_non_max_suppression_fn, second_stage_score_conversion_fn, second_stage_localization_loss_weight, second_stage_classification_loss_weight, second_stage_classification_loss, hard_example_miner, parallel_iterations=16, add_summaries=True, clip_anchors_to_image=False, use_static_shapes=False, resize_masks=False, freeze_batchnorm=False, return_raw_detections_during_predict=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'RFCNMetaArch Constructor.\\n\\n    Args:\\n      is_training: A boolean indicating whether the training version of the\\n        computation graph should be constructed.\\n      num_classes: Number of classes.  Note that num_classes *does not*\\n        include the background category, so if groundtruth labels take values\\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\\n        assigned classification targets can range from {0,... K}).\\n      image_resizer_fn: A callable for image resizing.  This callable always\\n        takes a rank-3 image tensor (corresponding to a single image) and\\n        returns a rank-3 image tensor, possibly with new spatial dimensions.\\n        See builders/image_resizer_builder.py.\\n      feature_extractor: A FasterRCNNFeatureExtractor object.\\n      number_of_stages:  Valid values are {1, 2}. If 1 will only construct the\\n        Region Proposal Network (RPN) part of the model.\\n      first_stage_anchor_generator: An anchor_generator.AnchorGenerator object\\n        (note that currently we only support\\n        grid_anchor_generator.GridAnchorGenerator objects)\\n      first_stage_target_assigner: Target assigner to use for first stage of\\n        R-FCN (RPN).\\n      first_stage_atrous_rate: A single integer indicating the atrous rate for\\n        the single convolution op which is applied to the `rpn_features_to_crop`\\n        tensor to obtain a tensor to be used for box prediction. Some feature\\n        extractors optionally allow for producing feature maps computed at\\n        denser resolutions.  The atrous rate is used to compensate for the\\n        denser feature maps by using an effectively larger receptive field.\\n        (This should typically be set to 1).\\n      first_stage_box_predictor_arg_scope_fn: Either a\\n        Keras layer hyperparams object or a function to construct tf-slim\\n        arg_scope for conv2d, separable_conv2d and fully_connected ops. Used\\n        for the RPN box predictor. If it is a keras hyperparams object the\\n        RPN box predictor will be a Keras model. If it is a function to\\n        construct an arg scope it will be a tf-slim box predictor.\\n      first_stage_box_predictor_kernel_size: Kernel size to use for the\\n        convolution op just prior to RPN box predictions.\\n      first_stage_box_predictor_depth: Output depth for the convolution op\\n        just prior to RPN box predictions.\\n      first_stage_minibatch_size: The \"batch size\" to use for computing the\\n        objectness and location loss of the region proposal network. This\\n        \"batch size\" refers to the number of anchors selected as contributing\\n        to the loss function for any given image within the image batch and is\\n        only called \"batch_size\" due to terminology from the Faster R-CNN paper.\\n      first_stage_sampler: The sampler for the boxes used to calculate the RPN\\n        loss after the first stage.\\n      first_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\\n        callable that takes `boxes`, `scores` and optional `clip_window`(with\\n        all other inputs already set) and returns a dictionary containing\\n        tensors with keys: `detection_boxes`, `detection_scores`,\\n        `detection_classes`, `num_detections`. This is used to perform non max\\n        suppression  on the boxes predicted by the Region Proposal Network\\n        (RPN).\\n        See `post_processing.batch_multiclass_non_max_suppression` for the type\\n        and shape of these tensors.\\n      first_stage_max_proposals: Maximum number of boxes to retain after\\n        performing Non-Max Suppression (NMS) on the boxes predicted by the\\n        Region Proposal Network (RPN).\\n      first_stage_localization_loss_weight: A float\\n      first_stage_objectness_loss_weight: A float\\n      crop_and_resize_fn: A differentiable resampler to use for cropping RPN\\n        proposal features.\\n      second_stage_target_assigner: Target assigner to use for second stage of\\n        R-FCN. If the model is configured with multiple prediction heads, this\\n        target assigner is used to generate targets for all heads (with the\\n        correct `unmatched_class_label`).\\n      second_stage_rfcn_box_predictor: RFCN box predictor to use for\\n        second stage.\\n      second_stage_batch_size: The batch size used for computing the\\n        classification and refined location loss of the box classifier.  This\\n        \"batch size\" refers to the number of proposals selected as contributing\\n        to the loss function for any given image within the image batch and is\\n        only called \"batch_size\" due to terminology from the Faster R-CNN paper.\\n      second_stage_sampler: The sampler for the boxes used for second stage\\n        box classifier.\\n      second_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\\n        callable that takes `boxes`, `scores`, optional `clip_window` and\\n        optional (kwarg) `mask` inputs (with all other inputs already set)\\n        and returns a dictionary containing tensors with keys:\\n        `detection_boxes`, `detection_scores`, `detection_classes`,\\n        `num_detections`, and (optionally) `detection_masks`. See\\n        `post_processing.batch_multiclass_non_max_suppression` for the type and\\n        shape of these tensors.\\n      second_stage_score_conversion_fn: Callable elementwise nonlinearity\\n        (that takes tensors as inputs and returns tensors).  This is usually\\n        used to convert logits to probabilities.\\n      second_stage_localization_loss_weight: A float\\n      second_stage_classification_loss_weight: A float\\n      second_stage_classification_loss: A string indicating which loss function\\n        to use, supports \\'softmax\\' and \\'sigmoid\\'.\\n      hard_example_miner:  A losses.HardExampleMiner object (can be None).\\n      parallel_iterations: (Optional) The number of iterations allowed to run\\n        in parallel for calls to tf.map_fn.\\n      add_summaries: boolean (default: True) controlling whether summary ops\\n        should be added to tensorflow graph.\\n      clip_anchors_to_image: The anchors generated are clip to the\\n        window size without filtering the nonoverlapping anchors. This generates\\n        a static number of anchors. This argument is unused.\\n      use_static_shapes: If True, uses implementation of ops with static shape\\n        guarantees.\\n      resize_masks: Indicates whether the masks presend in the groundtruth\\n        should be resized in the model with `image_resizer_fn`\\n      freeze_batchnorm: Whether to freeze batch norm parameters during\\n        training or not. When training with a small batch size (e.g. 1), it is\\n        desirable to freeze batch norm update and use pretrained batch norm\\n        params.\\n      return_raw_detections_during_predict: Whether to return raw detection\\n        boxes in the predict() method. These are decoded boxes that have not\\n        been through postprocessing (i.e. NMS). Default False.\\n\\n    Raises:\\n      ValueError: If `second_stage_batch_size` > `first_stage_max_proposals`\\n      ValueError: If first_stage_anchor_generator is not of type\\n        grid_anchor_generator.GridAnchorGenerator.\\n    '\n    super(RFCNMetaArch, self).__init__(is_training, num_classes, image_resizer_fn, feature_extractor, number_of_stages, first_stage_anchor_generator, first_stage_target_assigner, first_stage_atrous_rate, first_stage_box_predictor_arg_scope_fn, first_stage_box_predictor_kernel_size, first_stage_box_predictor_depth, first_stage_minibatch_size, first_stage_sampler, first_stage_non_max_suppression_fn, first_stage_max_proposals, first_stage_localization_loss_weight, first_stage_objectness_loss_weight, crop_and_resize_fn, None, None, None, second_stage_target_assigner, None, second_stage_batch_size, second_stage_sampler, second_stage_non_max_suppression_fn, second_stage_score_conversion_fn, second_stage_localization_loss_weight, second_stage_classification_loss_weight, second_stage_classification_loss, 1.0, hard_example_miner, parallel_iterations, add_summaries, clip_anchors_to_image, use_static_shapes, resize_masks, freeze_batchnorm=freeze_batchnorm, return_raw_detections_during_predict=return_raw_detections_during_predict)\n    self._rfcn_box_predictor = second_stage_rfcn_box_predictor",
            "def __init__(self, is_training, num_classes, image_resizer_fn, feature_extractor, number_of_stages, first_stage_anchor_generator, first_stage_target_assigner, first_stage_atrous_rate, first_stage_box_predictor_arg_scope_fn, first_stage_box_predictor_kernel_size, first_stage_box_predictor_depth, first_stage_minibatch_size, first_stage_sampler, first_stage_non_max_suppression_fn, first_stage_max_proposals, first_stage_localization_loss_weight, first_stage_objectness_loss_weight, crop_and_resize_fn, second_stage_target_assigner, second_stage_rfcn_box_predictor, second_stage_batch_size, second_stage_sampler, second_stage_non_max_suppression_fn, second_stage_score_conversion_fn, second_stage_localization_loss_weight, second_stage_classification_loss_weight, second_stage_classification_loss, hard_example_miner, parallel_iterations=16, add_summaries=True, clip_anchors_to_image=False, use_static_shapes=False, resize_masks=False, freeze_batchnorm=False, return_raw_detections_during_predict=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'RFCNMetaArch Constructor.\\n\\n    Args:\\n      is_training: A boolean indicating whether the training version of the\\n        computation graph should be constructed.\\n      num_classes: Number of classes.  Note that num_classes *does not*\\n        include the background category, so if groundtruth labels take values\\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\\n        assigned classification targets can range from {0,... K}).\\n      image_resizer_fn: A callable for image resizing.  This callable always\\n        takes a rank-3 image tensor (corresponding to a single image) and\\n        returns a rank-3 image tensor, possibly with new spatial dimensions.\\n        See builders/image_resizer_builder.py.\\n      feature_extractor: A FasterRCNNFeatureExtractor object.\\n      number_of_stages:  Valid values are {1, 2}. If 1 will only construct the\\n        Region Proposal Network (RPN) part of the model.\\n      first_stage_anchor_generator: An anchor_generator.AnchorGenerator object\\n        (note that currently we only support\\n        grid_anchor_generator.GridAnchorGenerator objects)\\n      first_stage_target_assigner: Target assigner to use for first stage of\\n        R-FCN (RPN).\\n      first_stage_atrous_rate: A single integer indicating the atrous rate for\\n        the single convolution op which is applied to the `rpn_features_to_crop`\\n        tensor to obtain a tensor to be used for box prediction. Some feature\\n        extractors optionally allow for producing feature maps computed at\\n        denser resolutions.  The atrous rate is used to compensate for the\\n        denser feature maps by using an effectively larger receptive field.\\n        (This should typically be set to 1).\\n      first_stage_box_predictor_arg_scope_fn: Either a\\n        Keras layer hyperparams object or a function to construct tf-slim\\n        arg_scope for conv2d, separable_conv2d and fully_connected ops. Used\\n        for the RPN box predictor. If it is a keras hyperparams object the\\n        RPN box predictor will be a Keras model. If it is a function to\\n        construct an arg scope it will be a tf-slim box predictor.\\n      first_stage_box_predictor_kernel_size: Kernel size to use for the\\n        convolution op just prior to RPN box predictions.\\n      first_stage_box_predictor_depth: Output depth for the convolution op\\n        just prior to RPN box predictions.\\n      first_stage_minibatch_size: The \"batch size\" to use for computing the\\n        objectness and location loss of the region proposal network. This\\n        \"batch size\" refers to the number of anchors selected as contributing\\n        to the loss function for any given image within the image batch and is\\n        only called \"batch_size\" due to terminology from the Faster R-CNN paper.\\n      first_stage_sampler: The sampler for the boxes used to calculate the RPN\\n        loss after the first stage.\\n      first_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\\n        callable that takes `boxes`, `scores` and optional `clip_window`(with\\n        all other inputs already set) and returns a dictionary containing\\n        tensors with keys: `detection_boxes`, `detection_scores`,\\n        `detection_classes`, `num_detections`. This is used to perform non max\\n        suppression  on the boxes predicted by the Region Proposal Network\\n        (RPN).\\n        See `post_processing.batch_multiclass_non_max_suppression` for the type\\n        and shape of these tensors.\\n      first_stage_max_proposals: Maximum number of boxes to retain after\\n        performing Non-Max Suppression (NMS) on the boxes predicted by the\\n        Region Proposal Network (RPN).\\n      first_stage_localization_loss_weight: A float\\n      first_stage_objectness_loss_weight: A float\\n      crop_and_resize_fn: A differentiable resampler to use for cropping RPN\\n        proposal features.\\n      second_stage_target_assigner: Target assigner to use for second stage of\\n        R-FCN. If the model is configured with multiple prediction heads, this\\n        target assigner is used to generate targets for all heads (with the\\n        correct `unmatched_class_label`).\\n      second_stage_rfcn_box_predictor: RFCN box predictor to use for\\n        second stage.\\n      second_stage_batch_size: The batch size used for computing the\\n        classification and refined location loss of the box classifier.  This\\n        \"batch size\" refers to the number of proposals selected as contributing\\n        to the loss function for any given image within the image batch and is\\n        only called \"batch_size\" due to terminology from the Faster R-CNN paper.\\n      second_stage_sampler: The sampler for the boxes used for second stage\\n        box classifier.\\n      second_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\\n        callable that takes `boxes`, `scores`, optional `clip_window` and\\n        optional (kwarg) `mask` inputs (with all other inputs already set)\\n        and returns a dictionary containing tensors with keys:\\n        `detection_boxes`, `detection_scores`, `detection_classes`,\\n        `num_detections`, and (optionally) `detection_masks`. See\\n        `post_processing.batch_multiclass_non_max_suppression` for the type and\\n        shape of these tensors.\\n      second_stage_score_conversion_fn: Callable elementwise nonlinearity\\n        (that takes tensors as inputs and returns tensors).  This is usually\\n        used to convert logits to probabilities.\\n      second_stage_localization_loss_weight: A float\\n      second_stage_classification_loss_weight: A float\\n      second_stage_classification_loss: A string indicating which loss function\\n        to use, supports \\'softmax\\' and \\'sigmoid\\'.\\n      hard_example_miner:  A losses.HardExampleMiner object (can be None).\\n      parallel_iterations: (Optional) The number of iterations allowed to run\\n        in parallel for calls to tf.map_fn.\\n      add_summaries: boolean (default: True) controlling whether summary ops\\n        should be added to tensorflow graph.\\n      clip_anchors_to_image: The anchors generated are clip to the\\n        window size without filtering the nonoverlapping anchors. This generates\\n        a static number of anchors. This argument is unused.\\n      use_static_shapes: If True, uses implementation of ops with static shape\\n        guarantees.\\n      resize_masks: Indicates whether the masks presend in the groundtruth\\n        should be resized in the model with `image_resizer_fn`\\n      freeze_batchnorm: Whether to freeze batch norm parameters during\\n        training or not. When training with a small batch size (e.g. 1), it is\\n        desirable to freeze batch norm update and use pretrained batch norm\\n        params.\\n      return_raw_detections_during_predict: Whether to return raw detection\\n        boxes in the predict() method. These are decoded boxes that have not\\n        been through postprocessing (i.e. NMS). Default False.\\n\\n    Raises:\\n      ValueError: If `second_stage_batch_size` > `first_stage_max_proposals`\\n      ValueError: If first_stage_anchor_generator is not of type\\n        grid_anchor_generator.GridAnchorGenerator.\\n    '\n    super(RFCNMetaArch, self).__init__(is_training, num_classes, image_resizer_fn, feature_extractor, number_of_stages, first_stage_anchor_generator, first_stage_target_assigner, first_stage_atrous_rate, first_stage_box_predictor_arg_scope_fn, first_stage_box_predictor_kernel_size, first_stage_box_predictor_depth, first_stage_minibatch_size, first_stage_sampler, first_stage_non_max_suppression_fn, first_stage_max_proposals, first_stage_localization_loss_weight, first_stage_objectness_loss_weight, crop_and_resize_fn, None, None, None, second_stage_target_assigner, None, second_stage_batch_size, second_stage_sampler, second_stage_non_max_suppression_fn, second_stage_score_conversion_fn, second_stage_localization_loss_weight, second_stage_classification_loss_weight, second_stage_classification_loss, 1.0, hard_example_miner, parallel_iterations, add_summaries, clip_anchors_to_image, use_static_shapes, resize_masks, freeze_batchnorm=freeze_batchnorm, return_raw_detections_during_predict=return_raw_detections_during_predict)\n    self._rfcn_box_predictor = second_stage_rfcn_box_predictor"
        ]
    },
    {
        "func_name": "_predict_second_stage",
        "original": "def _predict_second_stage(self, rpn_box_encodings, rpn_objectness_predictions_with_background, rpn_features, anchors, image_shape, true_image_shapes):\n    \"\"\"Predicts the output tensors from 2nd stage of R-FCN.\n\n    Args:\n      rpn_box_encodings: 3-D float tensor of shape\n        [batch_size, num_valid_anchors, self._box_coder.code_size] containing\n        predicted boxes.\n      rpn_objectness_predictions_with_background: 3-D float tensor of shape\n        [batch_size, num_valid_anchors, 2] containing class\n        predictions (logits) for each of the anchors.  Note that this\n        tensor *includes* background class predictions (at class index 0).\n      rpn_features: A 4-D float32 tensor with shape\n        [batch_size, height, width, depth] representing image features from the\n        RPN.\n      anchors: 2-D float tensor of shape\n        [num_anchors, self._box_coder.code_size].\n      image_shape: A 1D int32 tensors of size [4] containing the image shape.\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n        of the form [height, width, channels] indicating the shapes\n        of true images in the resized images, as resized images can be padded\n        with zeros.\n\n    Returns:\n      prediction_dict: a dictionary holding \"raw\" prediction tensors:\n        1) refined_box_encodings: a 3-D tensor with shape\n          [total_num_proposals, num_classes, 4] representing predicted\n          (final) refined box encodings, where\n          total_num_proposals=batch_size*self._max_num_proposals\n        2) class_predictions_with_background: a 2-D tensor with shape\n          [total_num_proposals, num_classes + 1] containing class\n          predictions (logits) for each of the anchors, where\n          total_num_proposals=batch_size*self._max_num_proposals.\n          Note that this tensor *includes* background class predictions\n          (at class index 0).\n        3) num_proposals: An int32 tensor of shape [batch_size] representing the\n          number of proposals generated by the RPN. `num_proposals` allows us\n          to keep track of which entries are to be treated as zero paddings and\n          which are not since we always pad the number of proposals to be\n          `self.max_num_proposals` for each image.\n        4) proposal_boxes: A float32 tensor of shape\n          [batch_size, self.max_num_proposals, 4] representing\n          decoded proposal bounding boxes (in absolute coordinates).\n        5) proposal_boxes_normalized: A float32 tensor of shape\n          [batch_size, self.max_num_proposals, 4] representing decoded proposal\n          bounding boxes (in normalized coordinates). Can be used to override\n          the boxes proposed by the RPN, thus enabling one to extract box\n          classification and prediction for externally selected areas of the\n          image.\n        6) box_classifier_features: a 4-D float32 tensor, of shape\n          [batch_size, feature_map_height, feature_map_width, depth],\n          representing the box classifier features.\n    \"\"\"\n    image_shape_2d = tf.tile(tf.expand_dims(image_shape[1:], 0), [image_shape[0], 1])\n    (proposal_boxes_normalized, _, _, num_proposals, _, _) = self._postprocess_rpn(rpn_box_encodings, rpn_objectness_predictions_with_background, anchors, image_shape_2d, true_image_shapes)\n    box_classifier_features = self._extract_box_classifier_features(rpn_features)\n    if self._rfcn_box_predictor.is_keras_model:\n        box_predictions = self._rfcn_box_predictor([box_classifier_features], proposal_boxes=proposal_boxes_normalized)\n    else:\n        box_predictions = self._rfcn_box_predictor.predict([box_classifier_features], num_predictions_per_location=[1], scope=self.second_stage_box_predictor_scope, proposal_boxes=proposal_boxes_normalized)\n    refined_box_encodings = tf.squeeze(tf.concat(box_predictions[box_predictor.BOX_ENCODINGS], axis=1), axis=1)\n    class_predictions_with_background = tf.squeeze(tf.concat(box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1), axis=1)\n    absolute_proposal_boxes = ops.normalized_to_image_coordinates(proposal_boxes_normalized, image_shape, parallel_iterations=self._parallel_iterations)\n    prediction_dict = {'refined_box_encodings': refined_box_encodings, 'class_predictions_with_background': class_predictions_with_background, 'num_proposals': num_proposals, 'proposal_boxes': absolute_proposal_boxes, 'box_classifier_features': box_classifier_features, 'proposal_boxes_normalized': proposal_boxes_normalized, 'final_anchors': absolute_proposal_boxes}\n    if self._return_raw_detections_during_predict:\n        prediction_dict.update(self._raw_detections_and_feature_map_inds(refined_box_encodings, absolute_proposal_boxes))\n    return prediction_dict",
        "mutated": [
            "def _predict_second_stage(self, rpn_box_encodings, rpn_objectness_predictions_with_background, rpn_features, anchors, image_shape, true_image_shapes):\n    if False:\n        i = 10\n    'Predicts the output tensors from 2nd stage of R-FCN.\\n\\n    Args:\\n      rpn_box_encodings: 3-D float tensor of shape\\n        [batch_size, num_valid_anchors, self._box_coder.code_size] containing\\n        predicted boxes.\\n      rpn_objectness_predictions_with_background: 3-D float tensor of shape\\n        [batch_size, num_valid_anchors, 2] containing class\\n        predictions (logits) for each of the anchors.  Note that this\\n        tensor *includes* background class predictions (at class index 0).\\n      rpn_features: A 4-D float32 tensor with shape\\n        [batch_size, height, width, depth] representing image features from the\\n        RPN.\\n      anchors: 2-D float tensor of shape\\n        [num_anchors, self._box_coder.code_size].\\n      image_shape: A 1D int32 tensors of size [4] containing the image shape.\\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\\n        of the form [height, width, channels] indicating the shapes\\n        of true images in the resized images, as resized images can be padded\\n        with zeros.\\n\\n    Returns:\\n      prediction_dict: a dictionary holding \"raw\" prediction tensors:\\n        1) refined_box_encodings: a 3-D tensor with shape\\n          [total_num_proposals, num_classes, 4] representing predicted\\n          (final) refined box encodings, where\\n          total_num_proposals=batch_size*self._max_num_proposals\\n        2) class_predictions_with_background: a 2-D tensor with shape\\n          [total_num_proposals, num_classes + 1] containing class\\n          predictions (logits) for each of the anchors, where\\n          total_num_proposals=batch_size*self._max_num_proposals.\\n          Note that this tensor *includes* background class predictions\\n          (at class index 0).\\n        3) num_proposals: An int32 tensor of shape [batch_size] representing the\\n          number of proposals generated by the RPN. `num_proposals` allows us\\n          to keep track of which entries are to be treated as zero paddings and\\n          which are not since we always pad the number of proposals to be\\n          `self.max_num_proposals` for each image.\\n        4) proposal_boxes: A float32 tensor of shape\\n          [batch_size, self.max_num_proposals, 4] representing\\n          decoded proposal bounding boxes (in absolute coordinates).\\n        5) proposal_boxes_normalized: A float32 tensor of shape\\n          [batch_size, self.max_num_proposals, 4] representing decoded proposal\\n          bounding boxes (in normalized coordinates). Can be used to override\\n          the boxes proposed by the RPN, thus enabling one to extract box\\n          classification and prediction for externally selected areas of the\\n          image.\\n        6) box_classifier_features: a 4-D float32 tensor, of shape\\n          [batch_size, feature_map_height, feature_map_width, depth],\\n          representing the box classifier features.\\n    '\n    image_shape_2d = tf.tile(tf.expand_dims(image_shape[1:], 0), [image_shape[0], 1])\n    (proposal_boxes_normalized, _, _, num_proposals, _, _) = self._postprocess_rpn(rpn_box_encodings, rpn_objectness_predictions_with_background, anchors, image_shape_2d, true_image_shapes)\n    box_classifier_features = self._extract_box_classifier_features(rpn_features)\n    if self._rfcn_box_predictor.is_keras_model:\n        box_predictions = self._rfcn_box_predictor([box_classifier_features], proposal_boxes=proposal_boxes_normalized)\n    else:\n        box_predictions = self._rfcn_box_predictor.predict([box_classifier_features], num_predictions_per_location=[1], scope=self.second_stage_box_predictor_scope, proposal_boxes=proposal_boxes_normalized)\n    refined_box_encodings = tf.squeeze(tf.concat(box_predictions[box_predictor.BOX_ENCODINGS], axis=1), axis=1)\n    class_predictions_with_background = tf.squeeze(tf.concat(box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1), axis=1)\n    absolute_proposal_boxes = ops.normalized_to_image_coordinates(proposal_boxes_normalized, image_shape, parallel_iterations=self._parallel_iterations)\n    prediction_dict = {'refined_box_encodings': refined_box_encodings, 'class_predictions_with_background': class_predictions_with_background, 'num_proposals': num_proposals, 'proposal_boxes': absolute_proposal_boxes, 'box_classifier_features': box_classifier_features, 'proposal_boxes_normalized': proposal_boxes_normalized, 'final_anchors': absolute_proposal_boxes}\n    if self._return_raw_detections_during_predict:\n        prediction_dict.update(self._raw_detections_and_feature_map_inds(refined_box_encodings, absolute_proposal_boxes))\n    return prediction_dict",
            "def _predict_second_stage(self, rpn_box_encodings, rpn_objectness_predictions_with_background, rpn_features, anchors, image_shape, true_image_shapes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Predicts the output tensors from 2nd stage of R-FCN.\\n\\n    Args:\\n      rpn_box_encodings: 3-D float tensor of shape\\n        [batch_size, num_valid_anchors, self._box_coder.code_size] containing\\n        predicted boxes.\\n      rpn_objectness_predictions_with_background: 3-D float tensor of shape\\n        [batch_size, num_valid_anchors, 2] containing class\\n        predictions (logits) for each of the anchors.  Note that this\\n        tensor *includes* background class predictions (at class index 0).\\n      rpn_features: A 4-D float32 tensor with shape\\n        [batch_size, height, width, depth] representing image features from the\\n        RPN.\\n      anchors: 2-D float tensor of shape\\n        [num_anchors, self._box_coder.code_size].\\n      image_shape: A 1D int32 tensors of size [4] containing the image shape.\\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\\n        of the form [height, width, channels] indicating the shapes\\n        of true images in the resized images, as resized images can be padded\\n        with zeros.\\n\\n    Returns:\\n      prediction_dict: a dictionary holding \"raw\" prediction tensors:\\n        1) refined_box_encodings: a 3-D tensor with shape\\n          [total_num_proposals, num_classes, 4] representing predicted\\n          (final) refined box encodings, where\\n          total_num_proposals=batch_size*self._max_num_proposals\\n        2) class_predictions_with_background: a 2-D tensor with shape\\n          [total_num_proposals, num_classes + 1] containing class\\n          predictions (logits) for each of the anchors, where\\n          total_num_proposals=batch_size*self._max_num_proposals.\\n          Note that this tensor *includes* background class predictions\\n          (at class index 0).\\n        3) num_proposals: An int32 tensor of shape [batch_size] representing the\\n          number of proposals generated by the RPN. `num_proposals` allows us\\n          to keep track of which entries are to be treated as zero paddings and\\n          which are not since we always pad the number of proposals to be\\n          `self.max_num_proposals` for each image.\\n        4) proposal_boxes: A float32 tensor of shape\\n          [batch_size, self.max_num_proposals, 4] representing\\n          decoded proposal bounding boxes (in absolute coordinates).\\n        5) proposal_boxes_normalized: A float32 tensor of shape\\n          [batch_size, self.max_num_proposals, 4] representing decoded proposal\\n          bounding boxes (in normalized coordinates). Can be used to override\\n          the boxes proposed by the RPN, thus enabling one to extract box\\n          classification and prediction for externally selected areas of the\\n          image.\\n        6) box_classifier_features: a 4-D float32 tensor, of shape\\n          [batch_size, feature_map_height, feature_map_width, depth],\\n          representing the box classifier features.\\n    '\n    image_shape_2d = tf.tile(tf.expand_dims(image_shape[1:], 0), [image_shape[0], 1])\n    (proposal_boxes_normalized, _, _, num_proposals, _, _) = self._postprocess_rpn(rpn_box_encodings, rpn_objectness_predictions_with_background, anchors, image_shape_2d, true_image_shapes)\n    box_classifier_features = self._extract_box_classifier_features(rpn_features)\n    if self._rfcn_box_predictor.is_keras_model:\n        box_predictions = self._rfcn_box_predictor([box_classifier_features], proposal_boxes=proposal_boxes_normalized)\n    else:\n        box_predictions = self._rfcn_box_predictor.predict([box_classifier_features], num_predictions_per_location=[1], scope=self.second_stage_box_predictor_scope, proposal_boxes=proposal_boxes_normalized)\n    refined_box_encodings = tf.squeeze(tf.concat(box_predictions[box_predictor.BOX_ENCODINGS], axis=1), axis=1)\n    class_predictions_with_background = tf.squeeze(tf.concat(box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1), axis=1)\n    absolute_proposal_boxes = ops.normalized_to_image_coordinates(proposal_boxes_normalized, image_shape, parallel_iterations=self._parallel_iterations)\n    prediction_dict = {'refined_box_encodings': refined_box_encodings, 'class_predictions_with_background': class_predictions_with_background, 'num_proposals': num_proposals, 'proposal_boxes': absolute_proposal_boxes, 'box_classifier_features': box_classifier_features, 'proposal_boxes_normalized': proposal_boxes_normalized, 'final_anchors': absolute_proposal_boxes}\n    if self._return_raw_detections_during_predict:\n        prediction_dict.update(self._raw_detections_and_feature_map_inds(refined_box_encodings, absolute_proposal_boxes))\n    return prediction_dict",
            "def _predict_second_stage(self, rpn_box_encodings, rpn_objectness_predictions_with_background, rpn_features, anchors, image_shape, true_image_shapes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Predicts the output tensors from 2nd stage of R-FCN.\\n\\n    Args:\\n      rpn_box_encodings: 3-D float tensor of shape\\n        [batch_size, num_valid_anchors, self._box_coder.code_size] containing\\n        predicted boxes.\\n      rpn_objectness_predictions_with_background: 3-D float tensor of shape\\n        [batch_size, num_valid_anchors, 2] containing class\\n        predictions (logits) for each of the anchors.  Note that this\\n        tensor *includes* background class predictions (at class index 0).\\n      rpn_features: A 4-D float32 tensor with shape\\n        [batch_size, height, width, depth] representing image features from the\\n        RPN.\\n      anchors: 2-D float tensor of shape\\n        [num_anchors, self._box_coder.code_size].\\n      image_shape: A 1D int32 tensors of size [4] containing the image shape.\\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\\n        of the form [height, width, channels] indicating the shapes\\n        of true images in the resized images, as resized images can be padded\\n        with zeros.\\n\\n    Returns:\\n      prediction_dict: a dictionary holding \"raw\" prediction tensors:\\n        1) refined_box_encodings: a 3-D tensor with shape\\n          [total_num_proposals, num_classes, 4] representing predicted\\n          (final) refined box encodings, where\\n          total_num_proposals=batch_size*self._max_num_proposals\\n        2) class_predictions_with_background: a 2-D tensor with shape\\n          [total_num_proposals, num_classes + 1] containing class\\n          predictions (logits) for each of the anchors, where\\n          total_num_proposals=batch_size*self._max_num_proposals.\\n          Note that this tensor *includes* background class predictions\\n          (at class index 0).\\n        3) num_proposals: An int32 tensor of shape [batch_size] representing the\\n          number of proposals generated by the RPN. `num_proposals` allows us\\n          to keep track of which entries are to be treated as zero paddings and\\n          which are not since we always pad the number of proposals to be\\n          `self.max_num_proposals` for each image.\\n        4) proposal_boxes: A float32 tensor of shape\\n          [batch_size, self.max_num_proposals, 4] representing\\n          decoded proposal bounding boxes (in absolute coordinates).\\n        5) proposal_boxes_normalized: A float32 tensor of shape\\n          [batch_size, self.max_num_proposals, 4] representing decoded proposal\\n          bounding boxes (in normalized coordinates). Can be used to override\\n          the boxes proposed by the RPN, thus enabling one to extract box\\n          classification and prediction for externally selected areas of the\\n          image.\\n        6) box_classifier_features: a 4-D float32 tensor, of shape\\n          [batch_size, feature_map_height, feature_map_width, depth],\\n          representing the box classifier features.\\n    '\n    image_shape_2d = tf.tile(tf.expand_dims(image_shape[1:], 0), [image_shape[0], 1])\n    (proposal_boxes_normalized, _, _, num_proposals, _, _) = self._postprocess_rpn(rpn_box_encodings, rpn_objectness_predictions_with_background, anchors, image_shape_2d, true_image_shapes)\n    box_classifier_features = self._extract_box_classifier_features(rpn_features)\n    if self._rfcn_box_predictor.is_keras_model:\n        box_predictions = self._rfcn_box_predictor([box_classifier_features], proposal_boxes=proposal_boxes_normalized)\n    else:\n        box_predictions = self._rfcn_box_predictor.predict([box_classifier_features], num_predictions_per_location=[1], scope=self.second_stage_box_predictor_scope, proposal_boxes=proposal_boxes_normalized)\n    refined_box_encodings = tf.squeeze(tf.concat(box_predictions[box_predictor.BOX_ENCODINGS], axis=1), axis=1)\n    class_predictions_with_background = tf.squeeze(tf.concat(box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1), axis=1)\n    absolute_proposal_boxes = ops.normalized_to_image_coordinates(proposal_boxes_normalized, image_shape, parallel_iterations=self._parallel_iterations)\n    prediction_dict = {'refined_box_encodings': refined_box_encodings, 'class_predictions_with_background': class_predictions_with_background, 'num_proposals': num_proposals, 'proposal_boxes': absolute_proposal_boxes, 'box_classifier_features': box_classifier_features, 'proposal_boxes_normalized': proposal_boxes_normalized, 'final_anchors': absolute_proposal_boxes}\n    if self._return_raw_detections_during_predict:\n        prediction_dict.update(self._raw_detections_and_feature_map_inds(refined_box_encodings, absolute_proposal_boxes))\n    return prediction_dict",
            "def _predict_second_stage(self, rpn_box_encodings, rpn_objectness_predictions_with_background, rpn_features, anchors, image_shape, true_image_shapes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Predicts the output tensors from 2nd stage of R-FCN.\\n\\n    Args:\\n      rpn_box_encodings: 3-D float tensor of shape\\n        [batch_size, num_valid_anchors, self._box_coder.code_size] containing\\n        predicted boxes.\\n      rpn_objectness_predictions_with_background: 3-D float tensor of shape\\n        [batch_size, num_valid_anchors, 2] containing class\\n        predictions (logits) for each of the anchors.  Note that this\\n        tensor *includes* background class predictions (at class index 0).\\n      rpn_features: A 4-D float32 tensor with shape\\n        [batch_size, height, width, depth] representing image features from the\\n        RPN.\\n      anchors: 2-D float tensor of shape\\n        [num_anchors, self._box_coder.code_size].\\n      image_shape: A 1D int32 tensors of size [4] containing the image shape.\\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\\n        of the form [height, width, channels] indicating the shapes\\n        of true images in the resized images, as resized images can be padded\\n        with zeros.\\n\\n    Returns:\\n      prediction_dict: a dictionary holding \"raw\" prediction tensors:\\n        1) refined_box_encodings: a 3-D tensor with shape\\n          [total_num_proposals, num_classes, 4] representing predicted\\n          (final) refined box encodings, where\\n          total_num_proposals=batch_size*self._max_num_proposals\\n        2) class_predictions_with_background: a 2-D tensor with shape\\n          [total_num_proposals, num_classes + 1] containing class\\n          predictions (logits) for each of the anchors, where\\n          total_num_proposals=batch_size*self._max_num_proposals.\\n          Note that this tensor *includes* background class predictions\\n          (at class index 0).\\n        3) num_proposals: An int32 tensor of shape [batch_size] representing the\\n          number of proposals generated by the RPN. `num_proposals` allows us\\n          to keep track of which entries are to be treated as zero paddings and\\n          which are not since we always pad the number of proposals to be\\n          `self.max_num_proposals` for each image.\\n        4) proposal_boxes: A float32 tensor of shape\\n          [batch_size, self.max_num_proposals, 4] representing\\n          decoded proposal bounding boxes (in absolute coordinates).\\n        5) proposal_boxes_normalized: A float32 tensor of shape\\n          [batch_size, self.max_num_proposals, 4] representing decoded proposal\\n          bounding boxes (in normalized coordinates). Can be used to override\\n          the boxes proposed by the RPN, thus enabling one to extract box\\n          classification and prediction for externally selected areas of the\\n          image.\\n        6) box_classifier_features: a 4-D float32 tensor, of shape\\n          [batch_size, feature_map_height, feature_map_width, depth],\\n          representing the box classifier features.\\n    '\n    image_shape_2d = tf.tile(tf.expand_dims(image_shape[1:], 0), [image_shape[0], 1])\n    (proposal_boxes_normalized, _, _, num_proposals, _, _) = self._postprocess_rpn(rpn_box_encodings, rpn_objectness_predictions_with_background, anchors, image_shape_2d, true_image_shapes)\n    box_classifier_features = self._extract_box_classifier_features(rpn_features)\n    if self._rfcn_box_predictor.is_keras_model:\n        box_predictions = self._rfcn_box_predictor([box_classifier_features], proposal_boxes=proposal_boxes_normalized)\n    else:\n        box_predictions = self._rfcn_box_predictor.predict([box_classifier_features], num_predictions_per_location=[1], scope=self.second_stage_box_predictor_scope, proposal_boxes=proposal_boxes_normalized)\n    refined_box_encodings = tf.squeeze(tf.concat(box_predictions[box_predictor.BOX_ENCODINGS], axis=1), axis=1)\n    class_predictions_with_background = tf.squeeze(tf.concat(box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1), axis=1)\n    absolute_proposal_boxes = ops.normalized_to_image_coordinates(proposal_boxes_normalized, image_shape, parallel_iterations=self._parallel_iterations)\n    prediction_dict = {'refined_box_encodings': refined_box_encodings, 'class_predictions_with_background': class_predictions_with_background, 'num_proposals': num_proposals, 'proposal_boxes': absolute_proposal_boxes, 'box_classifier_features': box_classifier_features, 'proposal_boxes_normalized': proposal_boxes_normalized, 'final_anchors': absolute_proposal_boxes}\n    if self._return_raw_detections_during_predict:\n        prediction_dict.update(self._raw_detections_and_feature_map_inds(refined_box_encodings, absolute_proposal_boxes))\n    return prediction_dict",
            "def _predict_second_stage(self, rpn_box_encodings, rpn_objectness_predictions_with_background, rpn_features, anchors, image_shape, true_image_shapes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Predicts the output tensors from 2nd stage of R-FCN.\\n\\n    Args:\\n      rpn_box_encodings: 3-D float tensor of shape\\n        [batch_size, num_valid_anchors, self._box_coder.code_size] containing\\n        predicted boxes.\\n      rpn_objectness_predictions_with_background: 3-D float tensor of shape\\n        [batch_size, num_valid_anchors, 2] containing class\\n        predictions (logits) for each of the anchors.  Note that this\\n        tensor *includes* background class predictions (at class index 0).\\n      rpn_features: A 4-D float32 tensor with shape\\n        [batch_size, height, width, depth] representing image features from the\\n        RPN.\\n      anchors: 2-D float tensor of shape\\n        [num_anchors, self._box_coder.code_size].\\n      image_shape: A 1D int32 tensors of size [4] containing the image shape.\\n      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\\n        of the form [height, width, channels] indicating the shapes\\n        of true images in the resized images, as resized images can be padded\\n        with zeros.\\n\\n    Returns:\\n      prediction_dict: a dictionary holding \"raw\" prediction tensors:\\n        1) refined_box_encodings: a 3-D tensor with shape\\n          [total_num_proposals, num_classes, 4] representing predicted\\n          (final) refined box encodings, where\\n          total_num_proposals=batch_size*self._max_num_proposals\\n        2) class_predictions_with_background: a 2-D tensor with shape\\n          [total_num_proposals, num_classes + 1] containing class\\n          predictions (logits) for each of the anchors, where\\n          total_num_proposals=batch_size*self._max_num_proposals.\\n          Note that this tensor *includes* background class predictions\\n          (at class index 0).\\n        3) num_proposals: An int32 tensor of shape [batch_size] representing the\\n          number of proposals generated by the RPN. `num_proposals` allows us\\n          to keep track of which entries are to be treated as zero paddings and\\n          which are not since we always pad the number of proposals to be\\n          `self.max_num_proposals` for each image.\\n        4) proposal_boxes: A float32 tensor of shape\\n          [batch_size, self.max_num_proposals, 4] representing\\n          decoded proposal bounding boxes (in absolute coordinates).\\n        5) proposal_boxes_normalized: A float32 tensor of shape\\n          [batch_size, self.max_num_proposals, 4] representing decoded proposal\\n          bounding boxes (in normalized coordinates). Can be used to override\\n          the boxes proposed by the RPN, thus enabling one to extract box\\n          classification and prediction for externally selected areas of the\\n          image.\\n        6) box_classifier_features: a 4-D float32 tensor, of shape\\n          [batch_size, feature_map_height, feature_map_width, depth],\\n          representing the box classifier features.\\n    '\n    image_shape_2d = tf.tile(tf.expand_dims(image_shape[1:], 0), [image_shape[0], 1])\n    (proposal_boxes_normalized, _, _, num_proposals, _, _) = self._postprocess_rpn(rpn_box_encodings, rpn_objectness_predictions_with_background, anchors, image_shape_2d, true_image_shapes)\n    box_classifier_features = self._extract_box_classifier_features(rpn_features)\n    if self._rfcn_box_predictor.is_keras_model:\n        box_predictions = self._rfcn_box_predictor([box_classifier_features], proposal_boxes=proposal_boxes_normalized)\n    else:\n        box_predictions = self._rfcn_box_predictor.predict([box_classifier_features], num_predictions_per_location=[1], scope=self.second_stage_box_predictor_scope, proposal_boxes=proposal_boxes_normalized)\n    refined_box_encodings = tf.squeeze(tf.concat(box_predictions[box_predictor.BOX_ENCODINGS], axis=1), axis=1)\n    class_predictions_with_background = tf.squeeze(tf.concat(box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1), axis=1)\n    absolute_proposal_boxes = ops.normalized_to_image_coordinates(proposal_boxes_normalized, image_shape, parallel_iterations=self._parallel_iterations)\n    prediction_dict = {'refined_box_encodings': refined_box_encodings, 'class_predictions_with_background': class_predictions_with_background, 'num_proposals': num_proposals, 'proposal_boxes': absolute_proposal_boxes, 'box_classifier_features': box_classifier_features, 'proposal_boxes_normalized': proposal_boxes_normalized, 'final_anchors': absolute_proposal_boxes}\n    if self._return_raw_detections_during_predict:\n        prediction_dict.update(self._raw_detections_and_feature_map_inds(refined_box_encodings, absolute_proposal_boxes))\n    return prediction_dict"
        ]
    },
    {
        "func_name": "regularization_losses",
        "original": "def regularization_losses(self):\n    \"\"\"Returns a list of regularization losses for this model.\n\n    Returns a list of regularization losses for this model that the estimator\n    needs to use during training/optimization.\n\n    Returns:\n      A list of regularization loss tensors.\n    \"\"\"\n    reg_losses = super(RFCNMetaArch, self).regularization_losses()\n    if self._rfcn_box_predictor.is_keras_model:\n        reg_losses.extend(self._rfcn_box_predictor.losses)\n    return reg_losses",
        "mutated": [
            "def regularization_losses(self):\n    if False:\n        i = 10\n    'Returns a list of regularization losses for this model.\\n\\n    Returns a list of regularization losses for this model that the estimator\\n    needs to use during training/optimization.\\n\\n    Returns:\\n      A list of regularization loss tensors.\\n    '\n    reg_losses = super(RFCNMetaArch, self).regularization_losses()\n    if self._rfcn_box_predictor.is_keras_model:\n        reg_losses.extend(self._rfcn_box_predictor.losses)\n    return reg_losses",
            "def regularization_losses(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Returns a list of regularization losses for this model.\\n\\n    Returns a list of regularization losses for this model that the estimator\\n    needs to use during training/optimization.\\n\\n    Returns:\\n      A list of regularization loss tensors.\\n    '\n    reg_losses = super(RFCNMetaArch, self).regularization_losses()\n    if self._rfcn_box_predictor.is_keras_model:\n        reg_losses.extend(self._rfcn_box_predictor.losses)\n    return reg_losses",
            "def regularization_losses(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Returns a list of regularization losses for this model.\\n\\n    Returns a list of regularization losses for this model that the estimator\\n    needs to use during training/optimization.\\n\\n    Returns:\\n      A list of regularization loss tensors.\\n    '\n    reg_losses = super(RFCNMetaArch, self).regularization_losses()\n    if self._rfcn_box_predictor.is_keras_model:\n        reg_losses.extend(self._rfcn_box_predictor.losses)\n    return reg_losses",
            "def regularization_losses(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Returns a list of regularization losses for this model.\\n\\n    Returns a list of regularization losses for this model that the estimator\\n    needs to use during training/optimization.\\n\\n    Returns:\\n      A list of regularization loss tensors.\\n    '\n    reg_losses = super(RFCNMetaArch, self).regularization_losses()\n    if self._rfcn_box_predictor.is_keras_model:\n        reg_losses.extend(self._rfcn_box_predictor.losses)\n    return reg_losses",
            "def regularization_losses(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Returns a list of regularization losses for this model.\\n\\n    Returns a list of regularization losses for this model that the estimator\\n    needs to use during training/optimization.\\n\\n    Returns:\\n      A list of regularization loss tensors.\\n    '\n    reg_losses = super(RFCNMetaArch, self).regularization_losses()\n    if self._rfcn_box_predictor.is_keras_model:\n        reg_losses.extend(self._rfcn_box_predictor.losses)\n    return reg_losses"
        ]
    },
    {
        "func_name": "updates",
        "original": "def updates(self):\n    \"\"\"Returns a list of update operators for this model.\n\n    Returns a list of update operators for this model that must be executed at\n    each training step. The estimator's train op needs to have a control\n    dependency on these updates.\n\n    Returns:\n      A list of update operators.\n    \"\"\"\n    update_ops = super(RFCNMetaArch, self).updates()\n    if self._rfcn_box_predictor.is_keras_model:\n        update_ops.extend(self._rfcn_box_predictor.get_updates_for(None))\n        update_ops.extend(self._rfcn_box_predictor.get_updates_for(self._rfcn_box_predictor.inputs))\n    return update_ops",
        "mutated": [
            "def updates(self):\n    if False:\n        i = 10\n    \"Returns a list of update operators for this model.\\n\\n    Returns a list of update operators for this model that must be executed at\\n    each training step. The estimator's train op needs to have a control\\n    dependency on these updates.\\n\\n    Returns:\\n      A list of update operators.\\n    \"\n    update_ops = super(RFCNMetaArch, self).updates()\n    if self._rfcn_box_predictor.is_keras_model:\n        update_ops.extend(self._rfcn_box_predictor.get_updates_for(None))\n        update_ops.extend(self._rfcn_box_predictor.get_updates_for(self._rfcn_box_predictor.inputs))\n    return update_ops",
            "def updates(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Returns a list of update operators for this model.\\n\\n    Returns a list of update operators for this model that must be executed at\\n    each training step. The estimator's train op needs to have a control\\n    dependency on these updates.\\n\\n    Returns:\\n      A list of update operators.\\n    \"\n    update_ops = super(RFCNMetaArch, self).updates()\n    if self._rfcn_box_predictor.is_keras_model:\n        update_ops.extend(self._rfcn_box_predictor.get_updates_for(None))\n        update_ops.extend(self._rfcn_box_predictor.get_updates_for(self._rfcn_box_predictor.inputs))\n    return update_ops",
            "def updates(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Returns a list of update operators for this model.\\n\\n    Returns a list of update operators for this model that must be executed at\\n    each training step. The estimator's train op needs to have a control\\n    dependency on these updates.\\n\\n    Returns:\\n      A list of update operators.\\n    \"\n    update_ops = super(RFCNMetaArch, self).updates()\n    if self._rfcn_box_predictor.is_keras_model:\n        update_ops.extend(self._rfcn_box_predictor.get_updates_for(None))\n        update_ops.extend(self._rfcn_box_predictor.get_updates_for(self._rfcn_box_predictor.inputs))\n    return update_ops",
            "def updates(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Returns a list of update operators for this model.\\n\\n    Returns a list of update operators for this model that must be executed at\\n    each training step. The estimator's train op needs to have a control\\n    dependency on these updates.\\n\\n    Returns:\\n      A list of update operators.\\n    \"\n    update_ops = super(RFCNMetaArch, self).updates()\n    if self._rfcn_box_predictor.is_keras_model:\n        update_ops.extend(self._rfcn_box_predictor.get_updates_for(None))\n        update_ops.extend(self._rfcn_box_predictor.get_updates_for(self._rfcn_box_predictor.inputs))\n    return update_ops",
            "def updates(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Returns a list of update operators for this model.\\n\\n    Returns a list of update operators for this model that must be executed at\\n    each training step. The estimator's train op needs to have a control\\n    dependency on these updates.\\n\\n    Returns:\\n      A list of update operators.\\n    \"\n    update_ops = super(RFCNMetaArch, self).updates()\n    if self._rfcn_box_predictor.is_keras_model:\n        update_ops.extend(self._rfcn_box_predictor.get_updates_for(None))\n        update_ops.extend(self._rfcn_box_predictor.get_updates_for(self._rfcn_box_predictor.inputs))\n    return update_ops"
        ]
    }
]