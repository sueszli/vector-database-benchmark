[
    {
        "func_name": "reduce_tensor",
        "original": "def reduce_tensor(tensor):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt /= args.world_size\n    return rt",
        "mutated": [
            "def reduce_tensor(tensor):\n    if False:\n        i = 10\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt /= args.world_size\n    return rt",
            "def reduce_tensor(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt /= args.world_size\n    return rt",
            "def reduce_tensor(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt /= args.world_size\n    return rt",
            "def reduce_tensor(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt /= args.world_size\n    return rt",
            "def reduce_tensor(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt /= args.world_size\n    return rt"
        ]
    },
    {
        "func_name": "training_test",
        "original": "def training_test(args):\n    \"\"\"Run ExternalSource pipelines along RN18 network. Based on simplified RN50 Pytorch sample. \"\"\"\n    args.distributed = False\n    args.world_size = 1\n    args.gpu = 0\n    args.distributed_initialized = False\n    if 'WORLD_SIZE' in os.environ:\n        args.distributed = int(os.environ['WORLD_SIZE']) > 1\n    test_pipe_factories = get_pipe_factories(args.test_pipes, external_source_parallel_pipeline, file_reader_pipeline, external_source_pipeline)\n    for pipe_factory in test_pipe_factories:\n        pipe = pipe_factory(batch_size=args.batch_size, num_threads=args.workers, device_id=args.local_rank, data_path=args.data_path, prefetch_queue_depth=args.prefetch, reader_queue_depth=args.reader_queue_depth, py_start_method=args.worker_init, py_num_workers=args.py_workers, source_mode=args.source_mode, read_encoded=args.dali_decode)\n        pipe.start_py_workers()\n        if args.distributed and (not args.distributed_initialized):\n            args.gpu = args.local_rank\n            torch.cuda.set_device(args.gpu)\n            torch.distributed.init_process_group(backend='nccl', init_method='env://')\n            args.world_size = torch.distributed.get_world_size()\n            args.distributed_initialized = True\n        pipe.build()\n        model = models.resnet18().cuda()\n        if args.distributed:\n            model = DDP(model, device_ids=[args.gpu], output_device=args.gpu)\n        model.train()\n        loss_fun = nn.CrossEntropyLoss().cuda()\n        lr = 0.1 * args.batch_size / 256\n        optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9)\n        samples_no = pipe.epoch_size('Reader')\n        if args.benchmark_iters is None:\n            expected_iters = samples_no // args.batch_size + (samples_no % args.batch_size != 0)\n        else:\n            expected_iters = args.benchmark_iters\n        if pipe_factory == file_reader_pipeline:\n            iterator = DALIClassificationIterator([pipe], reader_name='Reader', last_batch_policy=LastBatchPolicy.DROP, auto_reset=True)\n        else:\n            iterator = DALIClassificationIterator([pipe], size=samples_no * args.world_size, auto_reset=True)\n        if args.local_rank == 0:\n            print('RUN {}'.format(pipe_factory.__name__))\n        losses = AverageMeter()\n        for i in range(args.epochs):\n            if args.local_rank == 0:\n                if i == 0:\n                    print('Warm up')\n                else:\n                    print('Test run ' + str(i))\n            end = time.time()\n            data_time = AverageMeter()\n            for (j, data) in enumerate(iterator):\n                inputs = data[0]['data']\n                target = data[0]['label'].squeeze(-1).cuda().long()\n                outputs = model(inputs)\n                loss = loss_fun(outputs, target)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                if j % 50 == 0 and j != 0:\n                    if args.distributed:\n                        reduced_loss = reduce_tensor(loss.data)\n                    else:\n                        reduced_loss = loss.data\n                    if args.local_rank == 0:\n                        print(reduced_loss.item())\n                    losses.update(reduced_loss.item())\n                    torch.cuda.synchronize()\n                    data_time.update((time.time() - end) / 50)\n                    end = time.time()\n                    if args.local_rank == 0:\n                        template_string = '{} {}/ {}, avg time: {} [s], worst time: {} [s], speed: {} [img/s], loss: {}, loss_avg: {}'\n                        print(template_string.format(pipe_factory.__name__, j, expected_iters, data_time.avg, data_time.max_val, args.batch_size * args.world_size / data_time.avg, reduced_loss.item(), losses.avg))\n                if j >= expected_iters:\n                    break\n        print('OK {}'.format(pipe_factory.__name__))",
        "mutated": [
            "def training_test(args):\n    if False:\n        i = 10\n    'Run ExternalSource pipelines along RN18 network. Based on simplified RN50 Pytorch sample. '\n    args.distributed = False\n    args.world_size = 1\n    args.gpu = 0\n    args.distributed_initialized = False\n    if 'WORLD_SIZE' in os.environ:\n        args.distributed = int(os.environ['WORLD_SIZE']) > 1\n    test_pipe_factories = get_pipe_factories(args.test_pipes, external_source_parallel_pipeline, file_reader_pipeline, external_source_pipeline)\n    for pipe_factory in test_pipe_factories:\n        pipe = pipe_factory(batch_size=args.batch_size, num_threads=args.workers, device_id=args.local_rank, data_path=args.data_path, prefetch_queue_depth=args.prefetch, reader_queue_depth=args.reader_queue_depth, py_start_method=args.worker_init, py_num_workers=args.py_workers, source_mode=args.source_mode, read_encoded=args.dali_decode)\n        pipe.start_py_workers()\n        if args.distributed and (not args.distributed_initialized):\n            args.gpu = args.local_rank\n            torch.cuda.set_device(args.gpu)\n            torch.distributed.init_process_group(backend='nccl', init_method='env://')\n            args.world_size = torch.distributed.get_world_size()\n            args.distributed_initialized = True\n        pipe.build()\n        model = models.resnet18().cuda()\n        if args.distributed:\n            model = DDP(model, device_ids=[args.gpu], output_device=args.gpu)\n        model.train()\n        loss_fun = nn.CrossEntropyLoss().cuda()\n        lr = 0.1 * args.batch_size / 256\n        optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9)\n        samples_no = pipe.epoch_size('Reader')\n        if args.benchmark_iters is None:\n            expected_iters = samples_no // args.batch_size + (samples_no % args.batch_size != 0)\n        else:\n            expected_iters = args.benchmark_iters\n        if pipe_factory == file_reader_pipeline:\n            iterator = DALIClassificationIterator([pipe], reader_name='Reader', last_batch_policy=LastBatchPolicy.DROP, auto_reset=True)\n        else:\n            iterator = DALIClassificationIterator([pipe], size=samples_no * args.world_size, auto_reset=True)\n        if args.local_rank == 0:\n            print('RUN {}'.format(pipe_factory.__name__))\n        losses = AverageMeter()\n        for i in range(args.epochs):\n            if args.local_rank == 0:\n                if i == 0:\n                    print('Warm up')\n                else:\n                    print('Test run ' + str(i))\n            end = time.time()\n            data_time = AverageMeter()\n            for (j, data) in enumerate(iterator):\n                inputs = data[0]['data']\n                target = data[0]['label'].squeeze(-1).cuda().long()\n                outputs = model(inputs)\n                loss = loss_fun(outputs, target)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                if j % 50 == 0 and j != 0:\n                    if args.distributed:\n                        reduced_loss = reduce_tensor(loss.data)\n                    else:\n                        reduced_loss = loss.data\n                    if args.local_rank == 0:\n                        print(reduced_loss.item())\n                    losses.update(reduced_loss.item())\n                    torch.cuda.synchronize()\n                    data_time.update((time.time() - end) / 50)\n                    end = time.time()\n                    if args.local_rank == 0:\n                        template_string = '{} {}/ {}, avg time: {} [s], worst time: {} [s], speed: {} [img/s], loss: {}, loss_avg: {}'\n                        print(template_string.format(pipe_factory.__name__, j, expected_iters, data_time.avg, data_time.max_val, args.batch_size * args.world_size / data_time.avg, reduced_loss.item(), losses.avg))\n                if j >= expected_iters:\n                    break\n        print('OK {}'.format(pipe_factory.__name__))",
            "def training_test(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Run ExternalSource pipelines along RN18 network. Based on simplified RN50 Pytorch sample. '\n    args.distributed = False\n    args.world_size = 1\n    args.gpu = 0\n    args.distributed_initialized = False\n    if 'WORLD_SIZE' in os.environ:\n        args.distributed = int(os.environ['WORLD_SIZE']) > 1\n    test_pipe_factories = get_pipe_factories(args.test_pipes, external_source_parallel_pipeline, file_reader_pipeline, external_source_pipeline)\n    for pipe_factory in test_pipe_factories:\n        pipe = pipe_factory(batch_size=args.batch_size, num_threads=args.workers, device_id=args.local_rank, data_path=args.data_path, prefetch_queue_depth=args.prefetch, reader_queue_depth=args.reader_queue_depth, py_start_method=args.worker_init, py_num_workers=args.py_workers, source_mode=args.source_mode, read_encoded=args.dali_decode)\n        pipe.start_py_workers()\n        if args.distributed and (not args.distributed_initialized):\n            args.gpu = args.local_rank\n            torch.cuda.set_device(args.gpu)\n            torch.distributed.init_process_group(backend='nccl', init_method='env://')\n            args.world_size = torch.distributed.get_world_size()\n            args.distributed_initialized = True\n        pipe.build()\n        model = models.resnet18().cuda()\n        if args.distributed:\n            model = DDP(model, device_ids=[args.gpu], output_device=args.gpu)\n        model.train()\n        loss_fun = nn.CrossEntropyLoss().cuda()\n        lr = 0.1 * args.batch_size / 256\n        optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9)\n        samples_no = pipe.epoch_size('Reader')\n        if args.benchmark_iters is None:\n            expected_iters = samples_no // args.batch_size + (samples_no % args.batch_size != 0)\n        else:\n            expected_iters = args.benchmark_iters\n        if pipe_factory == file_reader_pipeline:\n            iterator = DALIClassificationIterator([pipe], reader_name='Reader', last_batch_policy=LastBatchPolicy.DROP, auto_reset=True)\n        else:\n            iterator = DALIClassificationIterator([pipe], size=samples_no * args.world_size, auto_reset=True)\n        if args.local_rank == 0:\n            print('RUN {}'.format(pipe_factory.__name__))\n        losses = AverageMeter()\n        for i in range(args.epochs):\n            if args.local_rank == 0:\n                if i == 0:\n                    print('Warm up')\n                else:\n                    print('Test run ' + str(i))\n            end = time.time()\n            data_time = AverageMeter()\n            for (j, data) in enumerate(iterator):\n                inputs = data[0]['data']\n                target = data[0]['label'].squeeze(-1).cuda().long()\n                outputs = model(inputs)\n                loss = loss_fun(outputs, target)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                if j % 50 == 0 and j != 0:\n                    if args.distributed:\n                        reduced_loss = reduce_tensor(loss.data)\n                    else:\n                        reduced_loss = loss.data\n                    if args.local_rank == 0:\n                        print(reduced_loss.item())\n                    losses.update(reduced_loss.item())\n                    torch.cuda.synchronize()\n                    data_time.update((time.time() - end) / 50)\n                    end = time.time()\n                    if args.local_rank == 0:\n                        template_string = '{} {}/ {}, avg time: {} [s], worst time: {} [s], speed: {} [img/s], loss: {}, loss_avg: {}'\n                        print(template_string.format(pipe_factory.__name__, j, expected_iters, data_time.avg, data_time.max_val, args.batch_size * args.world_size / data_time.avg, reduced_loss.item(), losses.avg))\n                if j >= expected_iters:\n                    break\n        print('OK {}'.format(pipe_factory.__name__))",
            "def training_test(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Run ExternalSource pipelines along RN18 network. Based on simplified RN50 Pytorch sample. '\n    args.distributed = False\n    args.world_size = 1\n    args.gpu = 0\n    args.distributed_initialized = False\n    if 'WORLD_SIZE' in os.environ:\n        args.distributed = int(os.environ['WORLD_SIZE']) > 1\n    test_pipe_factories = get_pipe_factories(args.test_pipes, external_source_parallel_pipeline, file_reader_pipeline, external_source_pipeline)\n    for pipe_factory in test_pipe_factories:\n        pipe = pipe_factory(batch_size=args.batch_size, num_threads=args.workers, device_id=args.local_rank, data_path=args.data_path, prefetch_queue_depth=args.prefetch, reader_queue_depth=args.reader_queue_depth, py_start_method=args.worker_init, py_num_workers=args.py_workers, source_mode=args.source_mode, read_encoded=args.dali_decode)\n        pipe.start_py_workers()\n        if args.distributed and (not args.distributed_initialized):\n            args.gpu = args.local_rank\n            torch.cuda.set_device(args.gpu)\n            torch.distributed.init_process_group(backend='nccl', init_method='env://')\n            args.world_size = torch.distributed.get_world_size()\n            args.distributed_initialized = True\n        pipe.build()\n        model = models.resnet18().cuda()\n        if args.distributed:\n            model = DDP(model, device_ids=[args.gpu], output_device=args.gpu)\n        model.train()\n        loss_fun = nn.CrossEntropyLoss().cuda()\n        lr = 0.1 * args.batch_size / 256\n        optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9)\n        samples_no = pipe.epoch_size('Reader')\n        if args.benchmark_iters is None:\n            expected_iters = samples_no // args.batch_size + (samples_no % args.batch_size != 0)\n        else:\n            expected_iters = args.benchmark_iters\n        if pipe_factory == file_reader_pipeline:\n            iterator = DALIClassificationIterator([pipe], reader_name='Reader', last_batch_policy=LastBatchPolicy.DROP, auto_reset=True)\n        else:\n            iterator = DALIClassificationIterator([pipe], size=samples_no * args.world_size, auto_reset=True)\n        if args.local_rank == 0:\n            print('RUN {}'.format(pipe_factory.__name__))\n        losses = AverageMeter()\n        for i in range(args.epochs):\n            if args.local_rank == 0:\n                if i == 0:\n                    print('Warm up')\n                else:\n                    print('Test run ' + str(i))\n            end = time.time()\n            data_time = AverageMeter()\n            for (j, data) in enumerate(iterator):\n                inputs = data[0]['data']\n                target = data[0]['label'].squeeze(-1).cuda().long()\n                outputs = model(inputs)\n                loss = loss_fun(outputs, target)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                if j % 50 == 0 and j != 0:\n                    if args.distributed:\n                        reduced_loss = reduce_tensor(loss.data)\n                    else:\n                        reduced_loss = loss.data\n                    if args.local_rank == 0:\n                        print(reduced_loss.item())\n                    losses.update(reduced_loss.item())\n                    torch.cuda.synchronize()\n                    data_time.update((time.time() - end) / 50)\n                    end = time.time()\n                    if args.local_rank == 0:\n                        template_string = '{} {}/ {}, avg time: {} [s], worst time: {} [s], speed: {} [img/s], loss: {}, loss_avg: {}'\n                        print(template_string.format(pipe_factory.__name__, j, expected_iters, data_time.avg, data_time.max_val, args.batch_size * args.world_size / data_time.avg, reduced_loss.item(), losses.avg))\n                if j >= expected_iters:\n                    break\n        print('OK {}'.format(pipe_factory.__name__))",
            "def training_test(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Run ExternalSource pipelines along RN18 network. Based on simplified RN50 Pytorch sample. '\n    args.distributed = False\n    args.world_size = 1\n    args.gpu = 0\n    args.distributed_initialized = False\n    if 'WORLD_SIZE' in os.environ:\n        args.distributed = int(os.environ['WORLD_SIZE']) > 1\n    test_pipe_factories = get_pipe_factories(args.test_pipes, external_source_parallel_pipeline, file_reader_pipeline, external_source_pipeline)\n    for pipe_factory in test_pipe_factories:\n        pipe = pipe_factory(batch_size=args.batch_size, num_threads=args.workers, device_id=args.local_rank, data_path=args.data_path, prefetch_queue_depth=args.prefetch, reader_queue_depth=args.reader_queue_depth, py_start_method=args.worker_init, py_num_workers=args.py_workers, source_mode=args.source_mode, read_encoded=args.dali_decode)\n        pipe.start_py_workers()\n        if args.distributed and (not args.distributed_initialized):\n            args.gpu = args.local_rank\n            torch.cuda.set_device(args.gpu)\n            torch.distributed.init_process_group(backend='nccl', init_method='env://')\n            args.world_size = torch.distributed.get_world_size()\n            args.distributed_initialized = True\n        pipe.build()\n        model = models.resnet18().cuda()\n        if args.distributed:\n            model = DDP(model, device_ids=[args.gpu], output_device=args.gpu)\n        model.train()\n        loss_fun = nn.CrossEntropyLoss().cuda()\n        lr = 0.1 * args.batch_size / 256\n        optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9)\n        samples_no = pipe.epoch_size('Reader')\n        if args.benchmark_iters is None:\n            expected_iters = samples_no // args.batch_size + (samples_no % args.batch_size != 0)\n        else:\n            expected_iters = args.benchmark_iters\n        if pipe_factory == file_reader_pipeline:\n            iterator = DALIClassificationIterator([pipe], reader_name='Reader', last_batch_policy=LastBatchPolicy.DROP, auto_reset=True)\n        else:\n            iterator = DALIClassificationIterator([pipe], size=samples_no * args.world_size, auto_reset=True)\n        if args.local_rank == 0:\n            print('RUN {}'.format(pipe_factory.__name__))\n        losses = AverageMeter()\n        for i in range(args.epochs):\n            if args.local_rank == 0:\n                if i == 0:\n                    print('Warm up')\n                else:\n                    print('Test run ' + str(i))\n            end = time.time()\n            data_time = AverageMeter()\n            for (j, data) in enumerate(iterator):\n                inputs = data[0]['data']\n                target = data[0]['label'].squeeze(-1).cuda().long()\n                outputs = model(inputs)\n                loss = loss_fun(outputs, target)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                if j % 50 == 0 and j != 0:\n                    if args.distributed:\n                        reduced_loss = reduce_tensor(loss.data)\n                    else:\n                        reduced_loss = loss.data\n                    if args.local_rank == 0:\n                        print(reduced_loss.item())\n                    losses.update(reduced_loss.item())\n                    torch.cuda.synchronize()\n                    data_time.update((time.time() - end) / 50)\n                    end = time.time()\n                    if args.local_rank == 0:\n                        template_string = '{} {}/ {}, avg time: {} [s], worst time: {} [s], speed: {} [img/s], loss: {}, loss_avg: {}'\n                        print(template_string.format(pipe_factory.__name__, j, expected_iters, data_time.avg, data_time.max_val, args.batch_size * args.world_size / data_time.avg, reduced_loss.item(), losses.avg))\n                if j >= expected_iters:\n                    break\n        print('OK {}'.format(pipe_factory.__name__))",
            "def training_test(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Run ExternalSource pipelines along RN18 network. Based on simplified RN50 Pytorch sample. '\n    args.distributed = False\n    args.world_size = 1\n    args.gpu = 0\n    args.distributed_initialized = False\n    if 'WORLD_SIZE' in os.environ:\n        args.distributed = int(os.environ['WORLD_SIZE']) > 1\n    test_pipe_factories = get_pipe_factories(args.test_pipes, external_source_parallel_pipeline, file_reader_pipeline, external_source_pipeline)\n    for pipe_factory in test_pipe_factories:\n        pipe = pipe_factory(batch_size=args.batch_size, num_threads=args.workers, device_id=args.local_rank, data_path=args.data_path, prefetch_queue_depth=args.prefetch, reader_queue_depth=args.reader_queue_depth, py_start_method=args.worker_init, py_num_workers=args.py_workers, source_mode=args.source_mode, read_encoded=args.dali_decode)\n        pipe.start_py_workers()\n        if args.distributed and (not args.distributed_initialized):\n            args.gpu = args.local_rank\n            torch.cuda.set_device(args.gpu)\n            torch.distributed.init_process_group(backend='nccl', init_method='env://')\n            args.world_size = torch.distributed.get_world_size()\n            args.distributed_initialized = True\n        pipe.build()\n        model = models.resnet18().cuda()\n        if args.distributed:\n            model = DDP(model, device_ids=[args.gpu], output_device=args.gpu)\n        model.train()\n        loss_fun = nn.CrossEntropyLoss().cuda()\n        lr = 0.1 * args.batch_size / 256\n        optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9)\n        samples_no = pipe.epoch_size('Reader')\n        if args.benchmark_iters is None:\n            expected_iters = samples_no // args.batch_size + (samples_no % args.batch_size != 0)\n        else:\n            expected_iters = args.benchmark_iters\n        if pipe_factory == file_reader_pipeline:\n            iterator = DALIClassificationIterator([pipe], reader_name='Reader', last_batch_policy=LastBatchPolicy.DROP, auto_reset=True)\n        else:\n            iterator = DALIClassificationIterator([pipe], size=samples_no * args.world_size, auto_reset=True)\n        if args.local_rank == 0:\n            print('RUN {}'.format(pipe_factory.__name__))\n        losses = AverageMeter()\n        for i in range(args.epochs):\n            if args.local_rank == 0:\n                if i == 0:\n                    print('Warm up')\n                else:\n                    print('Test run ' + str(i))\n            end = time.time()\n            data_time = AverageMeter()\n            for (j, data) in enumerate(iterator):\n                inputs = data[0]['data']\n                target = data[0]['label'].squeeze(-1).cuda().long()\n                outputs = model(inputs)\n                loss = loss_fun(outputs, target)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                if j % 50 == 0 and j != 0:\n                    if args.distributed:\n                        reduced_loss = reduce_tensor(loss.data)\n                    else:\n                        reduced_loss = loss.data\n                    if args.local_rank == 0:\n                        print(reduced_loss.item())\n                    losses.update(reduced_loss.item())\n                    torch.cuda.synchronize()\n                    data_time.update((time.time() - end) / 50)\n                    end = time.time()\n                    if args.local_rank == 0:\n                        template_string = '{} {}/ {}, avg time: {} [s], worst time: {} [s], speed: {} [img/s], loss: {}, loss_avg: {}'\n                        print(template_string.format(pipe_factory.__name__, j, expected_iters, data_time.avg, data_time.max_val, args.batch_size * args.world_size / data_time.avg, reduced_loss.item(), losses.avg))\n                if j >= expected_iters:\n                    break\n        print('OK {}'.format(pipe_factory.__name__))"
        ]
    }
]