[
    {
        "func_name": "calculate",
        "original": "@classmethod\ndef calculate(cls, finished_upstreams: Iterator[TaskInstance]) -> _UpstreamTIStates:\n    \"\"\"Calculate states for a task instance.\n\n        ``counter`` is inclusive of ``setup_counter`` -- e.g. if there are 2 skipped upstreams, one\n        of which is a setup, then counter will show 2 skipped and setup counter will show 1.\n\n        :param ti: the ti that we want to calculate deps for\n        :param finished_tis: all the finished tasks of the dag_run\n        \"\"\"\n    counter: dict[str, int] = Counter()\n    setup_counter: dict[str, int] = Counter()\n    for ti in finished_upstreams:\n        curr_state = {ti.state: 1}\n        counter.update(curr_state)\n        if ti.task.is_setup:\n            setup_counter.update(curr_state)\n    return _UpstreamTIStates(success=counter.get(TaskInstanceState.SUCCESS, 0), skipped=counter.get(TaskInstanceState.SKIPPED, 0), failed=counter.get(TaskInstanceState.FAILED, 0), upstream_failed=counter.get(TaskInstanceState.UPSTREAM_FAILED, 0), removed=counter.get(TaskInstanceState.REMOVED, 0), done=sum(counter.values()), success_setup=setup_counter.get(TaskInstanceState.SUCCESS, 0), skipped_setup=setup_counter.get(TaskInstanceState.SKIPPED, 0))",
        "mutated": [
            "@classmethod\ndef calculate(cls, finished_upstreams: Iterator[TaskInstance]) -> _UpstreamTIStates:\n    if False:\n        i = 10\n    'Calculate states for a task instance.\\n\\n        ``counter`` is inclusive of ``setup_counter`` -- e.g. if there are 2 skipped upstreams, one\\n        of which is a setup, then counter will show 2 skipped and setup counter will show 1.\\n\\n        :param ti: the ti that we want to calculate deps for\\n        :param finished_tis: all the finished tasks of the dag_run\\n        '\n    counter: dict[str, int] = Counter()\n    setup_counter: dict[str, int] = Counter()\n    for ti in finished_upstreams:\n        curr_state = {ti.state: 1}\n        counter.update(curr_state)\n        if ti.task.is_setup:\n            setup_counter.update(curr_state)\n    return _UpstreamTIStates(success=counter.get(TaskInstanceState.SUCCESS, 0), skipped=counter.get(TaskInstanceState.SKIPPED, 0), failed=counter.get(TaskInstanceState.FAILED, 0), upstream_failed=counter.get(TaskInstanceState.UPSTREAM_FAILED, 0), removed=counter.get(TaskInstanceState.REMOVED, 0), done=sum(counter.values()), success_setup=setup_counter.get(TaskInstanceState.SUCCESS, 0), skipped_setup=setup_counter.get(TaskInstanceState.SKIPPED, 0))",
            "@classmethod\ndef calculate(cls, finished_upstreams: Iterator[TaskInstance]) -> _UpstreamTIStates:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Calculate states for a task instance.\\n\\n        ``counter`` is inclusive of ``setup_counter`` -- e.g. if there are 2 skipped upstreams, one\\n        of which is a setup, then counter will show 2 skipped and setup counter will show 1.\\n\\n        :param ti: the ti that we want to calculate deps for\\n        :param finished_tis: all the finished tasks of the dag_run\\n        '\n    counter: dict[str, int] = Counter()\n    setup_counter: dict[str, int] = Counter()\n    for ti in finished_upstreams:\n        curr_state = {ti.state: 1}\n        counter.update(curr_state)\n        if ti.task.is_setup:\n            setup_counter.update(curr_state)\n    return _UpstreamTIStates(success=counter.get(TaskInstanceState.SUCCESS, 0), skipped=counter.get(TaskInstanceState.SKIPPED, 0), failed=counter.get(TaskInstanceState.FAILED, 0), upstream_failed=counter.get(TaskInstanceState.UPSTREAM_FAILED, 0), removed=counter.get(TaskInstanceState.REMOVED, 0), done=sum(counter.values()), success_setup=setup_counter.get(TaskInstanceState.SUCCESS, 0), skipped_setup=setup_counter.get(TaskInstanceState.SKIPPED, 0))",
            "@classmethod\ndef calculate(cls, finished_upstreams: Iterator[TaskInstance]) -> _UpstreamTIStates:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Calculate states for a task instance.\\n\\n        ``counter`` is inclusive of ``setup_counter`` -- e.g. if there are 2 skipped upstreams, one\\n        of which is a setup, then counter will show 2 skipped and setup counter will show 1.\\n\\n        :param ti: the ti that we want to calculate deps for\\n        :param finished_tis: all the finished tasks of the dag_run\\n        '\n    counter: dict[str, int] = Counter()\n    setup_counter: dict[str, int] = Counter()\n    for ti in finished_upstreams:\n        curr_state = {ti.state: 1}\n        counter.update(curr_state)\n        if ti.task.is_setup:\n            setup_counter.update(curr_state)\n    return _UpstreamTIStates(success=counter.get(TaskInstanceState.SUCCESS, 0), skipped=counter.get(TaskInstanceState.SKIPPED, 0), failed=counter.get(TaskInstanceState.FAILED, 0), upstream_failed=counter.get(TaskInstanceState.UPSTREAM_FAILED, 0), removed=counter.get(TaskInstanceState.REMOVED, 0), done=sum(counter.values()), success_setup=setup_counter.get(TaskInstanceState.SUCCESS, 0), skipped_setup=setup_counter.get(TaskInstanceState.SKIPPED, 0))",
            "@classmethod\ndef calculate(cls, finished_upstreams: Iterator[TaskInstance]) -> _UpstreamTIStates:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Calculate states for a task instance.\\n\\n        ``counter`` is inclusive of ``setup_counter`` -- e.g. if there are 2 skipped upstreams, one\\n        of which is a setup, then counter will show 2 skipped and setup counter will show 1.\\n\\n        :param ti: the ti that we want to calculate deps for\\n        :param finished_tis: all the finished tasks of the dag_run\\n        '\n    counter: dict[str, int] = Counter()\n    setup_counter: dict[str, int] = Counter()\n    for ti in finished_upstreams:\n        curr_state = {ti.state: 1}\n        counter.update(curr_state)\n        if ti.task.is_setup:\n            setup_counter.update(curr_state)\n    return _UpstreamTIStates(success=counter.get(TaskInstanceState.SUCCESS, 0), skipped=counter.get(TaskInstanceState.SKIPPED, 0), failed=counter.get(TaskInstanceState.FAILED, 0), upstream_failed=counter.get(TaskInstanceState.UPSTREAM_FAILED, 0), removed=counter.get(TaskInstanceState.REMOVED, 0), done=sum(counter.values()), success_setup=setup_counter.get(TaskInstanceState.SUCCESS, 0), skipped_setup=setup_counter.get(TaskInstanceState.SKIPPED, 0))",
            "@classmethod\ndef calculate(cls, finished_upstreams: Iterator[TaskInstance]) -> _UpstreamTIStates:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Calculate states for a task instance.\\n\\n        ``counter`` is inclusive of ``setup_counter`` -- e.g. if there are 2 skipped upstreams, one\\n        of which is a setup, then counter will show 2 skipped and setup counter will show 1.\\n\\n        :param ti: the ti that we want to calculate deps for\\n        :param finished_tis: all the finished tasks of the dag_run\\n        '\n    counter: dict[str, int] = Counter()\n    setup_counter: dict[str, int] = Counter()\n    for ti in finished_upstreams:\n        curr_state = {ti.state: 1}\n        counter.update(curr_state)\n        if ti.task.is_setup:\n            setup_counter.update(curr_state)\n    return _UpstreamTIStates(success=counter.get(TaskInstanceState.SUCCESS, 0), skipped=counter.get(TaskInstanceState.SKIPPED, 0), failed=counter.get(TaskInstanceState.FAILED, 0), upstream_failed=counter.get(TaskInstanceState.UPSTREAM_FAILED, 0), removed=counter.get(TaskInstanceState.REMOVED, 0), done=sum(counter.values()), success_setup=setup_counter.get(TaskInstanceState.SUCCESS, 0), skipped_setup=setup_counter.get(TaskInstanceState.SKIPPED, 0))"
        ]
    },
    {
        "func_name": "_get_dep_statuses",
        "original": "def _get_dep_statuses(self, ti: TaskInstance, session: Session, dep_context: DepContext) -> Iterator[TIDepStatus]:\n    if not ti.task.upstream_task_ids:\n        yield self._passing_status(reason='The task instance did not have any upstream tasks.')\n        return\n    if ti.task.trigger_rule == TR.ALWAYS:\n        yield self._passing_status(reason='The task had a always trigger rule set.')\n        return\n    yield from self._evaluate_trigger_rule(ti=ti, dep_context=dep_context, session=session)",
        "mutated": [
            "def _get_dep_statuses(self, ti: TaskInstance, session: Session, dep_context: DepContext) -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n    if not ti.task.upstream_task_ids:\n        yield self._passing_status(reason='The task instance did not have any upstream tasks.')\n        return\n    if ti.task.trigger_rule == TR.ALWAYS:\n        yield self._passing_status(reason='The task had a always trigger rule set.')\n        return\n    yield from self._evaluate_trigger_rule(ti=ti, dep_context=dep_context, session=session)",
            "def _get_dep_statuses(self, ti: TaskInstance, session: Session, dep_context: DepContext) -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not ti.task.upstream_task_ids:\n        yield self._passing_status(reason='The task instance did not have any upstream tasks.')\n        return\n    if ti.task.trigger_rule == TR.ALWAYS:\n        yield self._passing_status(reason='The task had a always trigger rule set.')\n        return\n    yield from self._evaluate_trigger_rule(ti=ti, dep_context=dep_context, session=session)",
            "def _get_dep_statuses(self, ti: TaskInstance, session: Session, dep_context: DepContext) -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not ti.task.upstream_task_ids:\n        yield self._passing_status(reason='The task instance did not have any upstream tasks.')\n        return\n    if ti.task.trigger_rule == TR.ALWAYS:\n        yield self._passing_status(reason='The task had a always trigger rule set.')\n        return\n    yield from self._evaluate_trigger_rule(ti=ti, dep_context=dep_context, session=session)",
            "def _get_dep_statuses(self, ti: TaskInstance, session: Session, dep_context: DepContext) -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not ti.task.upstream_task_ids:\n        yield self._passing_status(reason='The task instance did not have any upstream tasks.')\n        return\n    if ti.task.trigger_rule == TR.ALWAYS:\n        yield self._passing_status(reason='The task had a always trigger rule set.')\n        return\n    yield from self._evaluate_trigger_rule(ti=ti, dep_context=dep_context, session=session)",
            "def _get_dep_statuses(self, ti: TaskInstance, session: Session, dep_context: DepContext) -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not ti.task.upstream_task_ids:\n        yield self._passing_status(reason='The task instance did not have any upstream tasks.')\n        return\n    if ti.task.trigger_rule == TR.ALWAYS:\n        yield self._passing_status(reason='The task had a always trigger rule set.')\n        return\n    yield from self._evaluate_trigger_rule(ti=ti, dep_context=dep_context, session=session)"
        ]
    },
    {
        "func_name": "_get_expanded_ti_count",
        "original": "@functools.lru_cache\ndef _get_expanded_ti_count() -> int:\n    \"\"\"Get how many tis the current task is supposed to be expanded into.\n\n            This extra closure allows us to query the database only when needed,\n            and at most once.\n            \"\"\"\n    return ti.task.get_mapped_ti_count(ti.run_id, session=session)",
        "mutated": [
            "@functools.lru_cache\ndef _get_expanded_ti_count() -> int:\n    if False:\n        i = 10\n    'Get how many tis the current task is supposed to be expanded into.\\n\\n            This extra closure allows us to query the database only when needed,\\n            and at most once.\\n            '\n    return ti.task.get_mapped_ti_count(ti.run_id, session=session)",
            "@functools.lru_cache\ndef _get_expanded_ti_count() -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Get how many tis the current task is supposed to be expanded into.\\n\\n            This extra closure allows us to query the database only when needed,\\n            and at most once.\\n            '\n    return ti.task.get_mapped_ti_count(ti.run_id, session=session)",
            "@functools.lru_cache\ndef _get_expanded_ti_count() -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Get how many tis the current task is supposed to be expanded into.\\n\\n            This extra closure allows us to query the database only when needed,\\n            and at most once.\\n            '\n    return ti.task.get_mapped_ti_count(ti.run_id, session=session)",
            "@functools.lru_cache\ndef _get_expanded_ti_count() -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Get how many tis the current task is supposed to be expanded into.\\n\\n            This extra closure allows us to query the database only when needed,\\n            and at most once.\\n            '\n    return ti.task.get_mapped_ti_count(ti.run_id, session=session)",
            "@functools.lru_cache\ndef _get_expanded_ti_count() -> int:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Get how many tis the current task is supposed to be expanded into.\\n\\n            This extra closure allows us to query the database only when needed,\\n            and at most once.\\n            '\n    return ti.task.get_mapped_ti_count(ti.run_id, session=session)"
        ]
    },
    {
        "func_name": "_get_relevant_upstream_map_indexes",
        "original": "@functools.lru_cache\ndef _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:\n    \"\"\"Get the given task's map indexes relevant to the current ti.\n\n            This extra closure allows us to query the database only when needed,\n            and at most once for each task (instead of once for each expanded\n            task instance of the same task).\n            \"\"\"\n    if TYPE_CHECKING:\n        assert isinstance(ti.task.dag, DAG)\n    try:\n        expanded_ti_count = _get_expanded_ti_count()\n    except (NotFullyPopulated, NotMapped):\n        return None\n    return ti.get_relevant_upstream_map_indexes(upstream=ti.task.dag.task_dict[upstream_id], ti_count=expanded_ti_count, session=session)",
        "mutated": [
            "@functools.lru_cache\ndef _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:\n    if False:\n        i = 10\n    \"Get the given task's map indexes relevant to the current ti.\\n\\n            This extra closure allows us to query the database only when needed,\\n            and at most once for each task (instead of once for each expanded\\n            task instance of the same task).\\n            \"\n    if TYPE_CHECKING:\n        assert isinstance(ti.task.dag, DAG)\n    try:\n        expanded_ti_count = _get_expanded_ti_count()\n    except (NotFullyPopulated, NotMapped):\n        return None\n    return ti.get_relevant_upstream_map_indexes(upstream=ti.task.dag.task_dict[upstream_id], ti_count=expanded_ti_count, session=session)",
            "@functools.lru_cache\ndef _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Get the given task's map indexes relevant to the current ti.\\n\\n            This extra closure allows us to query the database only when needed,\\n            and at most once for each task (instead of once for each expanded\\n            task instance of the same task).\\n            \"\n    if TYPE_CHECKING:\n        assert isinstance(ti.task.dag, DAG)\n    try:\n        expanded_ti_count = _get_expanded_ti_count()\n    except (NotFullyPopulated, NotMapped):\n        return None\n    return ti.get_relevant_upstream_map_indexes(upstream=ti.task.dag.task_dict[upstream_id], ti_count=expanded_ti_count, session=session)",
            "@functools.lru_cache\ndef _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Get the given task's map indexes relevant to the current ti.\\n\\n            This extra closure allows us to query the database only when needed,\\n            and at most once for each task (instead of once for each expanded\\n            task instance of the same task).\\n            \"\n    if TYPE_CHECKING:\n        assert isinstance(ti.task.dag, DAG)\n    try:\n        expanded_ti_count = _get_expanded_ti_count()\n    except (NotFullyPopulated, NotMapped):\n        return None\n    return ti.get_relevant_upstream_map_indexes(upstream=ti.task.dag.task_dict[upstream_id], ti_count=expanded_ti_count, session=session)",
            "@functools.lru_cache\ndef _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Get the given task's map indexes relevant to the current ti.\\n\\n            This extra closure allows us to query the database only when needed,\\n            and at most once for each task (instead of once for each expanded\\n            task instance of the same task).\\n            \"\n    if TYPE_CHECKING:\n        assert isinstance(ti.task.dag, DAG)\n    try:\n        expanded_ti_count = _get_expanded_ti_count()\n    except (NotFullyPopulated, NotMapped):\n        return None\n    return ti.get_relevant_upstream_map_indexes(upstream=ti.task.dag.task_dict[upstream_id], ti_count=expanded_ti_count, session=session)",
            "@functools.lru_cache\ndef _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Get the given task's map indexes relevant to the current ti.\\n\\n            This extra closure allows us to query the database only when needed,\\n            and at most once for each task (instead of once for each expanded\\n            task instance of the same task).\\n            \"\n    if TYPE_CHECKING:\n        assert isinstance(ti.task.dag, DAG)\n    try:\n        expanded_ti_count = _get_expanded_ti_count()\n    except (NotFullyPopulated, NotMapped):\n        return None\n    return ti.get_relevant_upstream_map_indexes(upstream=ti.task.dag.task_dict[upstream_id], ti_count=expanded_ti_count, session=session)"
        ]
    },
    {
        "func_name": "_is_relevant_upstream",
        "original": "def _is_relevant_upstream(upstream: TaskInstance, relevant_ids: set[str] | KeysView[str]) -> bool:\n    \"\"\"\n            Whether a task instance is a \"relevant upstream\" of the current task.\n\n            This will return false if upstream.task_id is not in relevant_ids,\n            or if both of the following are true:\n                1. upstream.task_id in relevant_ids is True\n                2. ti is in a mapped task group and upstream has a map index\n                  that ti does not depend on.\n            \"\"\"\n    if upstream.task_id not in relevant_ids:\n        return False\n    if ti.task.get_closest_mapped_task_group() is None:\n        return True\n    if upstream.map_index < 0:\n        return True\n    relevant = _get_relevant_upstream_map_indexes(upstream_id=upstream.task_id)\n    if relevant is None:\n        return True\n    if relevant == upstream.map_index:\n        return True\n    if isinstance(relevant, collections.abc.Container) and upstream.map_index in relevant:\n        return True\n    return False",
        "mutated": [
            "def _is_relevant_upstream(upstream: TaskInstance, relevant_ids: set[str] | KeysView[str]) -> bool:\n    if False:\n        i = 10\n    '\\n            Whether a task instance is a \"relevant upstream\" of the current task.\\n\\n            This will return false if upstream.task_id is not in relevant_ids,\\n            or if both of the following are true:\\n                1. upstream.task_id in relevant_ids is True\\n                2. ti is in a mapped task group and upstream has a map index\\n                  that ti does not depend on.\\n            '\n    if upstream.task_id not in relevant_ids:\n        return False\n    if ti.task.get_closest_mapped_task_group() is None:\n        return True\n    if upstream.map_index < 0:\n        return True\n    relevant = _get_relevant_upstream_map_indexes(upstream_id=upstream.task_id)\n    if relevant is None:\n        return True\n    if relevant == upstream.map_index:\n        return True\n    if isinstance(relevant, collections.abc.Container) and upstream.map_index in relevant:\n        return True\n    return False",
            "def _is_relevant_upstream(upstream: TaskInstance, relevant_ids: set[str] | KeysView[str]) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n            Whether a task instance is a \"relevant upstream\" of the current task.\\n\\n            This will return false if upstream.task_id is not in relevant_ids,\\n            or if both of the following are true:\\n                1. upstream.task_id in relevant_ids is True\\n                2. ti is in a mapped task group and upstream has a map index\\n                  that ti does not depend on.\\n            '\n    if upstream.task_id not in relevant_ids:\n        return False\n    if ti.task.get_closest_mapped_task_group() is None:\n        return True\n    if upstream.map_index < 0:\n        return True\n    relevant = _get_relevant_upstream_map_indexes(upstream_id=upstream.task_id)\n    if relevant is None:\n        return True\n    if relevant == upstream.map_index:\n        return True\n    if isinstance(relevant, collections.abc.Container) and upstream.map_index in relevant:\n        return True\n    return False",
            "def _is_relevant_upstream(upstream: TaskInstance, relevant_ids: set[str] | KeysView[str]) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n            Whether a task instance is a \"relevant upstream\" of the current task.\\n\\n            This will return false if upstream.task_id is not in relevant_ids,\\n            or if both of the following are true:\\n                1. upstream.task_id in relevant_ids is True\\n                2. ti is in a mapped task group and upstream has a map index\\n                  that ti does not depend on.\\n            '\n    if upstream.task_id not in relevant_ids:\n        return False\n    if ti.task.get_closest_mapped_task_group() is None:\n        return True\n    if upstream.map_index < 0:\n        return True\n    relevant = _get_relevant_upstream_map_indexes(upstream_id=upstream.task_id)\n    if relevant is None:\n        return True\n    if relevant == upstream.map_index:\n        return True\n    if isinstance(relevant, collections.abc.Container) and upstream.map_index in relevant:\n        return True\n    return False",
            "def _is_relevant_upstream(upstream: TaskInstance, relevant_ids: set[str] | KeysView[str]) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n            Whether a task instance is a \"relevant upstream\" of the current task.\\n\\n            This will return false if upstream.task_id is not in relevant_ids,\\n            or if both of the following are true:\\n                1. upstream.task_id in relevant_ids is True\\n                2. ti is in a mapped task group and upstream has a map index\\n                  that ti does not depend on.\\n            '\n    if upstream.task_id not in relevant_ids:\n        return False\n    if ti.task.get_closest_mapped_task_group() is None:\n        return True\n    if upstream.map_index < 0:\n        return True\n    relevant = _get_relevant_upstream_map_indexes(upstream_id=upstream.task_id)\n    if relevant is None:\n        return True\n    if relevant == upstream.map_index:\n        return True\n    if isinstance(relevant, collections.abc.Container) and upstream.map_index in relevant:\n        return True\n    return False",
            "def _is_relevant_upstream(upstream: TaskInstance, relevant_ids: set[str] | KeysView[str]) -> bool:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n            Whether a task instance is a \"relevant upstream\" of the current task.\\n\\n            This will return false if upstream.task_id is not in relevant_ids,\\n            or if both of the following are true:\\n                1. upstream.task_id in relevant_ids is True\\n                2. ti is in a mapped task group and upstream has a map index\\n                  that ti does not depend on.\\n            '\n    if upstream.task_id not in relevant_ids:\n        return False\n    if ti.task.get_closest_mapped_task_group() is None:\n        return True\n    if upstream.map_index < 0:\n        return True\n    relevant = _get_relevant_upstream_map_indexes(upstream_id=upstream.task_id)\n    if relevant is None:\n        return True\n    if relevant == upstream.map_index:\n        return True\n    if isinstance(relevant, collections.abc.Container) and upstream.map_index in relevant:\n        return True\n    return False"
        ]
    },
    {
        "func_name": "_iter_upstream_conditions",
        "original": "def _iter_upstream_conditions(relevant_tasks: dict) -> Iterator[ColumnOperators]:\n    from airflow.models.taskinstance import TaskInstance\n    if ti.task.get_closest_mapped_task_group() is None:\n        yield TaskInstance.task_id.in_(relevant_tasks.keys())\n        return\n    for upstream_id in relevant_tasks:\n        map_indexes = _get_relevant_upstream_map_indexes(upstream_id)\n        if map_indexes is None:\n            yield (TaskInstance.task_id == upstream_id)\n            continue\n        yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index < 0)\n        if isinstance(map_indexes, range) and map_indexes.step == 1:\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index >= map_indexes.start, TaskInstance.map_index < map_indexes.stop)\n        elif isinstance(map_indexes, collections.abc.Container):\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index.in_(map_indexes))\n        else:\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index == map_indexes)",
        "mutated": [
            "def _iter_upstream_conditions(relevant_tasks: dict) -> Iterator[ColumnOperators]:\n    if False:\n        i = 10\n    from airflow.models.taskinstance import TaskInstance\n    if ti.task.get_closest_mapped_task_group() is None:\n        yield TaskInstance.task_id.in_(relevant_tasks.keys())\n        return\n    for upstream_id in relevant_tasks:\n        map_indexes = _get_relevant_upstream_map_indexes(upstream_id)\n        if map_indexes is None:\n            yield (TaskInstance.task_id == upstream_id)\n            continue\n        yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index < 0)\n        if isinstance(map_indexes, range) and map_indexes.step == 1:\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index >= map_indexes.start, TaskInstance.map_index < map_indexes.stop)\n        elif isinstance(map_indexes, collections.abc.Container):\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index.in_(map_indexes))\n        else:\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index == map_indexes)",
            "def _iter_upstream_conditions(relevant_tasks: dict) -> Iterator[ColumnOperators]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from airflow.models.taskinstance import TaskInstance\n    if ti.task.get_closest_mapped_task_group() is None:\n        yield TaskInstance.task_id.in_(relevant_tasks.keys())\n        return\n    for upstream_id in relevant_tasks:\n        map_indexes = _get_relevant_upstream_map_indexes(upstream_id)\n        if map_indexes is None:\n            yield (TaskInstance.task_id == upstream_id)\n            continue\n        yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index < 0)\n        if isinstance(map_indexes, range) and map_indexes.step == 1:\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index >= map_indexes.start, TaskInstance.map_index < map_indexes.stop)\n        elif isinstance(map_indexes, collections.abc.Container):\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index.in_(map_indexes))\n        else:\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index == map_indexes)",
            "def _iter_upstream_conditions(relevant_tasks: dict) -> Iterator[ColumnOperators]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from airflow.models.taskinstance import TaskInstance\n    if ti.task.get_closest_mapped_task_group() is None:\n        yield TaskInstance.task_id.in_(relevant_tasks.keys())\n        return\n    for upstream_id in relevant_tasks:\n        map_indexes = _get_relevant_upstream_map_indexes(upstream_id)\n        if map_indexes is None:\n            yield (TaskInstance.task_id == upstream_id)\n            continue\n        yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index < 0)\n        if isinstance(map_indexes, range) and map_indexes.step == 1:\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index >= map_indexes.start, TaskInstance.map_index < map_indexes.stop)\n        elif isinstance(map_indexes, collections.abc.Container):\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index.in_(map_indexes))\n        else:\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index == map_indexes)",
            "def _iter_upstream_conditions(relevant_tasks: dict) -> Iterator[ColumnOperators]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from airflow.models.taskinstance import TaskInstance\n    if ti.task.get_closest_mapped_task_group() is None:\n        yield TaskInstance.task_id.in_(relevant_tasks.keys())\n        return\n    for upstream_id in relevant_tasks:\n        map_indexes = _get_relevant_upstream_map_indexes(upstream_id)\n        if map_indexes is None:\n            yield (TaskInstance.task_id == upstream_id)\n            continue\n        yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index < 0)\n        if isinstance(map_indexes, range) and map_indexes.step == 1:\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index >= map_indexes.start, TaskInstance.map_index < map_indexes.stop)\n        elif isinstance(map_indexes, collections.abc.Container):\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index.in_(map_indexes))\n        else:\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index == map_indexes)",
            "def _iter_upstream_conditions(relevant_tasks: dict) -> Iterator[ColumnOperators]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from airflow.models.taskinstance import TaskInstance\n    if ti.task.get_closest_mapped_task_group() is None:\n        yield TaskInstance.task_id.in_(relevant_tasks.keys())\n        return\n    for upstream_id in relevant_tasks:\n        map_indexes = _get_relevant_upstream_map_indexes(upstream_id)\n        if map_indexes is None:\n            yield (TaskInstance.task_id == upstream_id)\n            continue\n        yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index < 0)\n        if isinstance(map_indexes, range) and map_indexes.step == 1:\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index >= map_indexes.start, TaskInstance.map_index < map_indexes.stop)\n        elif isinstance(map_indexes, collections.abc.Container):\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index.in_(map_indexes))\n        else:\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index == map_indexes)"
        ]
    },
    {
        "func_name": "_evaluate_setup_constraint",
        "original": "def _evaluate_setup_constraint(*, relevant_setups) -> Iterator[tuple[TIDepStatus, bool]]:\n    \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n            :param ti: Task instance to evaluate the trigger rule of.\n            :param dep_context: The current dependency context.\n            :param session: Database session.\n            \"\"\"\n    task = ti.task\n    indirect_setups = {k: v for (k, v) in relevant_setups.items() if k not in task.upstream_task_ids}\n    finished_upstream_tis = (x for x in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=x, relevant_ids=indirect_setups.keys()))\n    upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n    success = upstream_states.success\n    skipped = upstream_states.skipped\n    failed = upstream_states.failed\n    upstream_failed = upstream_states.upstream_failed\n    removed = upstream_states.removed\n    if not any((needs_expansion(t) for t in indirect_setups.values())):\n        upstream = len(indirect_setups)\n    else:\n        task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=indirect_setups))).group_by(TaskInstance.task_id)).all()\n        upstream = sum((count for (_, count) in task_id_counts))\n    new_state = None\n    changed = False\n    if dep_context.flag_upstream_failed:\n        if upstream_failed or failed:\n            new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif skipped:\n            new_state = TaskInstanceState.SKIPPED\n        elif removed and success and (ti.map_index > -1):\n            if ti.map_index >= success:\n                new_state = TaskInstanceState.REMOVED\n    if new_state is not None:\n        if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n            past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n            if not past_depends_met:\n                yield (self._failing_status(reason='Task should be skipped but the past depends are not met'), changed)\n                return\n        changed = ti.set_state(new_state, session)\n    if changed:\n        dep_context.have_changed_ti_states = True\n    non_successes = upstream - success\n    if ti.map_index > -1:\n        non_successes -= removed\n    if non_successes > 0:\n        yield (self._failing_status(reason=f'All setup tasks must complete successfully. Relevant setups: {relevant_setups}: upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}'), changed)",
        "mutated": [
            "def _evaluate_setup_constraint(*, relevant_setups) -> Iterator[tuple[TIDepStatus, bool]]:\n    if False:\n        i = 10\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n            :param ti: Task instance to evaluate the trigger rule of.\\n            :param dep_context: The current dependency context.\\n            :param session: Database session.\\n            \"\n    task = ti.task\n    indirect_setups = {k: v for (k, v) in relevant_setups.items() if k not in task.upstream_task_ids}\n    finished_upstream_tis = (x for x in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=x, relevant_ids=indirect_setups.keys()))\n    upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n    success = upstream_states.success\n    skipped = upstream_states.skipped\n    failed = upstream_states.failed\n    upstream_failed = upstream_states.upstream_failed\n    removed = upstream_states.removed\n    if not any((needs_expansion(t) for t in indirect_setups.values())):\n        upstream = len(indirect_setups)\n    else:\n        task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=indirect_setups))).group_by(TaskInstance.task_id)).all()\n        upstream = sum((count for (_, count) in task_id_counts))\n    new_state = None\n    changed = False\n    if dep_context.flag_upstream_failed:\n        if upstream_failed or failed:\n            new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif skipped:\n            new_state = TaskInstanceState.SKIPPED\n        elif removed and success and (ti.map_index > -1):\n            if ti.map_index >= success:\n                new_state = TaskInstanceState.REMOVED\n    if new_state is not None:\n        if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n            past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n            if not past_depends_met:\n                yield (self._failing_status(reason='Task should be skipped but the past depends are not met'), changed)\n                return\n        changed = ti.set_state(new_state, session)\n    if changed:\n        dep_context.have_changed_ti_states = True\n    non_successes = upstream - success\n    if ti.map_index > -1:\n        non_successes -= removed\n    if non_successes > 0:\n        yield (self._failing_status(reason=f'All setup tasks must complete successfully. Relevant setups: {relevant_setups}: upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}'), changed)",
            "def _evaluate_setup_constraint(*, relevant_setups) -> Iterator[tuple[TIDepStatus, bool]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n            :param ti: Task instance to evaluate the trigger rule of.\\n            :param dep_context: The current dependency context.\\n            :param session: Database session.\\n            \"\n    task = ti.task\n    indirect_setups = {k: v for (k, v) in relevant_setups.items() if k not in task.upstream_task_ids}\n    finished_upstream_tis = (x for x in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=x, relevant_ids=indirect_setups.keys()))\n    upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n    success = upstream_states.success\n    skipped = upstream_states.skipped\n    failed = upstream_states.failed\n    upstream_failed = upstream_states.upstream_failed\n    removed = upstream_states.removed\n    if not any((needs_expansion(t) for t in indirect_setups.values())):\n        upstream = len(indirect_setups)\n    else:\n        task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=indirect_setups))).group_by(TaskInstance.task_id)).all()\n        upstream = sum((count for (_, count) in task_id_counts))\n    new_state = None\n    changed = False\n    if dep_context.flag_upstream_failed:\n        if upstream_failed or failed:\n            new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif skipped:\n            new_state = TaskInstanceState.SKIPPED\n        elif removed and success and (ti.map_index > -1):\n            if ti.map_index >= success:\n                new_state = TaskInstanceState.REMOVED\n    if new_state is not None:\n        if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n            past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n            if not past_depends_met:\n                yield (self._failing_status(reason='Task should be skipped but the past depends are not met'), changed)\n                return\n        changed = ti.set_state(new_state, session)\n    if changed:\n        dep_context.have_changed_ti_states = True\n    non_successes = upstream - success\n    if ti.map_index > -1:\n        non_successes -= removed\n    if non_successes > 0:\n        yield (self._failing_status(reason=f'All setup tasks must complete successfully. Relevant setups: {relevant_setups}: upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}'), changed)",
            "def _evaluate_setup_constraint(*, relevant_setups) -> Iterator[tuple[TIDepStatus, bool]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n            :param ti: Task instance to evaluate the trigger rule of.\\n            :param dep_context: The current dependency context.\\n            :param session: Database session.\\n            \"\n    task = ti.task\n    indirect_setups = {k: v for (k, v) in relevant_setups.items() if k not in task.upstream_task_ids}\n    finished_upstream_tis = (x for x in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=x, relevant_ids=indirect_setups.keys()))\n    upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n    success = upstream_states.success\n    skipped = upstream_states.skipped\n    failed = upstream_states.failed\n    upstream_failed = upstream_states.upstream_failed\n    removed = upstream_states.removed\n    if not any((needs_expansion(t) for t in indirect_setups.values())):\n        upstream = len(indirect_setups)\n    else:\n        task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=indirect_setups))).group_by(TaskInstance.task_id)).all()\n        upstream = sum((count for (_, count) in task_id_counts))\n    new_state = None\n    changed = False\n    if dep_context.flag_upstream_failed:\n        if upstream_failed or failed:\n            new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif skipped:\n            new_state = TaskInstanceState.SKIPPED\n        elif removed and success and (ti.map_index > -1):\n            if ti.map_index >= success:\n                new_state = TaskInstanceState.REMOVED\n    if new_state is not None:\n        if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n            past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n            if not past_depends_met:\n                yield (self._failing_status(reason='Task should be skipped but the past depends are not met'), changed)\n                return\n        changed = ti.set_state(new_state, session)\n    if changed:\n        dep_context.have_changed_ti_states = True\n    non_successes = upstream - success\n    if ti.map_index > -1:\n        non_successes -= removed\n    if non_successes > 0:\n        yield (self._failing_status(reason=f'All setup tasks must complete successfully. Relevant setups: {relevant_setups}: upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}'), changed)",
            "def _evaluate_setup_constraint(*, relevant_setups) -> Iterator[tuple[TIDepStatus, bool]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n            :param ti: Task instance to evaluate the trigger rule of.\\n            :param dep_context: The current dependency context.\\n            :param session: Database session.\\n            \"\n    task = ti.task\n    indirect_setups = {k: v for (k, v) in relevant_setups.items() if k not in task.upstream_task_ids}\n    finished_upstream_tis = (x for x in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=x, relevant_ids=indirect_setups.keys()))\n    upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n    success = upstream_states.success\n    skipped = upstream_states.skipped\n    failed = upstream_states.failed\n    upstream_failed = upstream_states.upstream_failed\n    removed = upstream_states.removed\n    if not any((needs_expansion(t) for t in indirect_setups.values())):\n        upstream = len(indirect_setups)\n    else:\n        task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=indirect_setups))).group_by(TaskInstance.task_id)).all()\n        upstream = sum((count for (_, count) in task_id_counts))\n    new_state = None\n    changed = False\n    if dep_context.flag_upstream_failed:\n        if upstream_failed or failed:\n            new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif skipped:\n            new_state = TaskInstanceState.SKIPPED\n        elif removed and success and (ti.map_index > -1):\n            if ti.map_index >= success:\n                new_state = TaskInstanceState.REMOVED\n    if new_state is not None:\n        if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n            past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n            if not past_depends_met:\n                yield (self._failing_status(reason='Task should be skipped but the past depends are not met'), changed)\n                return\n        changed = ti.set_state(new_state, session)\n    if changed:\n        dep_context.have_changed_ti_states = True\n    non_successes = upstream - success\n    if ti.map_index > -1:\n        non_successes -= removed\n    if non_successes > 0:\n        yield (self._failing_status(reason=f'All setup tasks must complete successfully. Relevant setups: {relevant_setups}: upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}'), changed)",
            "def _evaluate_setup_constraint(*, relevant_setups) -> Iterator[tuple[TIDepStatus, bool]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n            :param ti: Task instance to evaluate the trigger rule of.\\n            :param dep_context: The current dependency context.\\n            :param session: Database session.\\n            \"\n    task = ti.task\n    indirect_setups = {k: v for (k, v) in relevant_setups.items() if k not in task.upstream_task_ids}\n    finished_upstream_tis = (x for x in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=x, relevant_ids=indirect_setups.keys()))\n    upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n    success = upstream_states.success\n    skipped = upstream_states.skipped\n    failed = upstream_states.failed\n    upstream_failed = upstream_states.upstream_failed\n    removed = upstream_states.removed\n    if not any((needs_expansion(t) for t in indirect_setups.values())):\n        upstream = len(indirect_setups)\n    else:\n        task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=indirect_setups))).group_by(TaskInstance.task_id)).all()\n        upstream = sum((count for (_, count) in task_id_counts))\n    new_state = None\n    changed = False\n    if dep_context.flag_upstream_failed:\n        if upstream_failed or failed:\n            new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif skipped:\n            new_state = TaskInstanceState.SKIPPED\n        elif removed and success and (ti.map_index > -1):\n            if ti.map_index >= success:\n                new_state = TaskInstanceState.REMOVED\n    if new_state is not None:\n        if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n            past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n            if not past_depends_met:\n                yield (self._failing_status(reason='Task should be skipped but the past depends are not met'), changed)\n                return\n        changed = ti.set_state(new_state, session)\n    if changed:\n        dep_context.have_changed_ti_states = True\n    non_successes = upstream - success\n    if ti.map_index > -1:\n        non_successes -= removed\n    if non_successes > 0:\n        yield (self._failing_status(reason=f'All setup tasks must complete successfully. Relevant setups: {relevant_setups}: upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}'), changed)"
        ]
    },
    {
        "func_name": "_evaluate_direct_relatives",
        "original": "def _evaluate_direct_relatives() -> Iterator[TIDepStatus]:\n    \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n            :param ti: Task instance to evaluate the trigger rule of.\n            :param dep_context: The current dependency context.\n            :param session: Database session.\n            \"\"\"\n    task = ti.task\n    upstream_tasks = {t.task_id: t for t in task.upstream_list}\n    trigger_rule = task.trigger_rule\n    finished_upstream_tis = (finished_ti for finished_ti in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=finished_ti, relevant_ids=ti.task.upstream_task_ids))\n    upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n    success = upstream_states.success\n    skipped = upstream_states.skipped\n    failed = upstream_states.failed\n    upstream_failed = upstream_states.upstream_failed\n    removed = upstream_states.removed\n    done = upstream_states.done\n    success_setup = upstream_states.success_setup\n    skipped_setup = upstream_states.skipped_setup\n    if not any((needs_expansion(t) for t in upstream_tasks.values())):\n        upstream = len(upstream_tasks)\n        upstream_setup = sum((1 for x in upstream_tasks.values() if x.is_setup))\n    else:\n        task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=upstream_tasks))).group_by(TaskInstance.task_id)).all()\n        upstream = sum((count for (_, count) in task_id_counts))\n        upstream_setup = sum((c for (t, c) in task_id_counts if upstream_tasks[t].is_setup))\n    upstream_done = done >= upstream\n    changed = False\n    new_state = None\n    if dep_context.flag_upstream_failed:\n        if trigger_rule == TR.ALL_SUCCESS:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif removed and success and (ti.map_index > -1):\n                if ti.map_index >= success:\n                    new_state = TaskInstanceState.REMOVED\n        elif trigger_rule == TR.ALL_FAILED:\n            if success or skipped:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ONE_SUCCESS:\n            if upstream_done and done == skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif upstream_done and success <= 0:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif trigger_rule == TR.ONE_FAILED:\n            if upstream_done and (not (failed or upstream_failed)):\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ONE_DONE:\n            if upstream_done and (not (failed or success)):\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.NONE_FAILED:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped == upstream:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.NONE_SKIPPED:\n            if skipped:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ALL_SKIPPED:\n            if success or failed or upstream_failed:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n            if upstream_done and upstream_setup and (skipped_setup >= upstream_setup):\n                new_state = TaskInstanceState.SKIPPED\n            elif upstream_done and upstream_setup and (success_setup == 0):\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n    if new_state is not None:\n        if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n            past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n            if not past_depends_met:\n                yield self._failing_status(reason='Task should be skipped but the past depends are not met')\n                return\n        changed = ti.set_state(new_state, session)\n    if changed:\n        dep_context.have_changed_ti_states = True\n    if trigger_rule == TR.ONE_SUCCESS:\n        if success <= 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task success, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ONE_FAILED:\n        if not failed and (not upstream_failed):\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task failure, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ONE_DONE:\n        if success + failed <= 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}'requires at least one upstream task failure or successbut none were failed or success. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_SUCCESS:\n        num_failures = upstream - success\n        if ti.map_index > -1:\n            num_failures -= removed\n        if num_failures > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_FAILED:\n        num_success = upstream - failed - upstream_failed\n        if ti.map_index > -1:\n            num_success -= removed\n        if num_success > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have failed, but found {num_success} non-failure(s). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_DONE:\n        if not upstream_done:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.NONE_FAILED or trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n        num_failures = upstream - success - skipped\n        if ti.map_index > -1:\n            num_failures -= removed\n        if num_failures > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded or been skipped, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.NONE_SKIPPED:\n        if not upstream_done or skipped > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to not have been skipped, but found {skipped} task(s) skipped. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_SKIPPED:\n        num_non_skipped = upstream - skipped\n        if num_non_skipped > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have been skipped, but found {num_non_skipped} task(s) in non skipped state. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n        if not upstream_done:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif upstream_setup is None:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' cannot have mapped tasks as upstream. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif upstream_setup and (not success_setup):\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires at least one upstream setup task be successful, but found {upstream_setup - success_setup} task(s) that were not. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    else:\n        yield self._failing_status(reason=f\"No strategy to evaluate trigger rule '{trigger_rule}'.\")",
        "mutated": [
            "def _evaluate_direct_relatives() -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n            :param ti: Task instance to evaluate the trigger rule of.\\n            :param dep_context: The current dependency context.\\n            :param session: Database session.\\n            \"\n    task = ti.task\n    upstream_tasks = {t.task_id: t for t in task.upstream_list}\n    trigger_rule = task.trigger_rule\n    finished_upstream_tis = (finished_ti for finished_ti in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=finished_ti, relevant_ids=ti.task.upstream_task_ids))\n    upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n    success = upstream_states.success\n    skipped = upstream_states.skipped\n    failed = upstream_states.failed\n    upstream_failed = upstream_states.upstream_failed\n    removed = upstream_states.removed\n    done = upstream_states.done\n    success_setup = upstream_states.success_setup\n    skipped_setup = upstream_states.skipped_setup\n    if not any((needs_expansion(t) for t in upstream_tasks.values())):\n        upstream = len(upstream_tasks)\n        upstream_setup = sum((1 for x in upstream_tasks.values() if x.is_setup))\n    else:\n        task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=upstream_tasks))).group_by(TaskInstance.task_id)).all()\n        upstream = sum((count for (_, count) in task_id_counts))\n        upstream_setup = sum((c for (t, c) in task_id_counts if upstream_tasks[t].is_setup))\n    upstream_done = done >= upstream\n    changed = False\n    new_state = None\n    if dep_context.flag_upstream_failed:\n        if trigger_rule == TR.ALL_SUCCESS:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif removed and success and (ti.map_index > -1):\n                if ti.map_index >= success:\n                    new_state = TaskInstanceState.REMOVED\n        elif trigger_rule == TR.ALL_FAILED:\n            if success or skipped:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ONE_SUCCESS:\n            if upstream_done and done == skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif upstream_done and success <= 0:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif trigger_rule == TR.ONE_FAILED:\n            if upstream_done and (not (failed or upstream_failed)):\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ONE_DONE:\n            if upstream_done and (not (failed or success)):\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.NONE_FAILED:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped == upstream:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.NONE_SKIPPED:\n            if skipped:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ALL_SKIPPED:\n            if success or failed or upstream_failed:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n            if upstream_done and upstream_setup and (skipped_setup >= upstream_setup):\n                new_state = TaskInstanceState.SKIPPED\n            elif upstream_done and upstream_setup and (success_setup == 0):\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n    if new_state is not None:\n        if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n            past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n            if not past_depends_met:\n                yield self._failing_status(reason='Task should be skipped but the past depends are not met')\n                return\n        changed = ti.set_state(new_state, session)\n    if changed:\n        dep_context.have_changed_ti_states = True\n    if trigger_rule == TR.ONE_SUCCESS:\n        if success <= 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task success, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ONE_FAILED:\n        if not failed and (not upstream_failed):\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task failure, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ONE_DONE:\n        if success + failed <= 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}'requires at least one upstream task failure or successbut none were failed or success. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_SUCCESS:\n        num_failures = upstream - success\n        if ti.map_index > -1:\n            num_failures -= removed\n        if num_failures > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_FAILED:\n        num_success = upstream - failed - upstream_failed\n        if ti.map_index > -1:\n            num_success -= removed\n        if num_success > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have failed, but found {num_success} non-failure(s). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_DONE:\n        if not upstream_done:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.NONE_FAILED or trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n        num_failures = upstream - success - skipped\n        if ti.map_index > -1:\n            num_failures -= removed\n        if num_failures > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded or been skipped, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.NONE_SKIPPED:\n        if not upstream_done or skipped > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to not have been skipped, but found {skipped} task(s) skipped. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_SKIPPED:\n        num_non_skipped = upstream - skipped\n        if num_non_skipped > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have been skipped, but found {num_non_skipped} task(s) in non skipped state. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n        if not upstream_done:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif upstream_setup is None:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' cannot have mapped tasks as upstream. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif upstream_setup and (not success_setup):\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires at least one upstream setup task be successful, but found {upstream_setup - success_setup} task(s) that were not. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    else:\n        yield self._failing_status(reason=f\"No strategy to evaluate trigger rule '{trigger_rule}'.\")",
            "def _evaluate_direct_relatives() -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n            :param ti: Task instance to evaluate the trigger rule of.\\n            :param dep_context: The current dependency context.\\n            :param session: Database session.\\n            \"\n    task = ti.task\n    upstream_tasks = {t.task_id: t for t in task.upstream_list}\n    trigger_rule = task.trigger_rule\n    finished_upstream_tis = (finished_ti for finished_ti in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=finished_ti, relevant_ids=ti.task.upstream_task_ids))\n    upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n    success = upstream_states.success\n    skipped = upstream_states.skipped\n    failed = upstream_states.failed\n    upstream_failed = upstream_states.upstream_failed\n    removed = upstream_states.removed\n    done = upstream_states.done\n    success_setup = upstream_states.success_setup\n    skipped_setup = upstream_states.skipped_setup\n    if not any((needs_expansion(t) for t in upstream_tasks.values())):\n        upstream = len(upstream_tasks)\n        upstream_setup = sum((1 for x in upstream_tasks.values() if x.is_setup))\n    else:\n        task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=upstream_tasks))).group_by(TaskInstance.task_id)).all()\n        upstream = sum((count for (_, count) in task_id_counts))\n        upstream_setup = sum((c for (t, c) in task_id_counts if upstream_tasks[t].is_setup))\n    upstream_done = done >= upstream\n    changed = False\n    new_state = None\n    if dep_context.flag_upstream_failed:\n        if trigger_rule == TR.ALL_SUCCESS:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif removed and success and (ti.map_index > -1):\n                if ti.map_index >= success:\n                    new_state = TaskInstanceState.REMOVED\n        elif trigger_rule == TR.ALL_FAILED:\n            if success or skipped:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ONE_SUCCESS:\n            if upstream_done and done == skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif upstream_done and success <= 0:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif trigger_rule == TR.ONE_FAILED:\n            if upstream_done and (not (failed or upstream_failed)):\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ONE_DONE:\n            if upstream_done and (not (failed or success)):\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.NONE_FAILED:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped == upstream:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.NONE_SKIPPED:\n            if skipped:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ALL_SKIPPED:\n            if success or failed or upstream_failed:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n            if upstream_done and upstream_setup and (skipped_setup >= upstream_setup):\n                new_state = TaskInstanceState.SKIPPED\n            elif upstream_done and upstream_setup and (success_setup == 0):\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n    if new_state is not None:\n        if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n            past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n            if not past_depends_met:\n                yield self._failing_status(reason='Task should be skipped but the past depends are not met')\n                return\n        changed = ti.set_state(new_state, session)\n    if changed:\n        dep_context.have_changed_ti_states = True\n    if trigger_rule == TR.ONE_SUCCESS:\n        if success <= 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task success, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ONE_FAILED:\n        if not failed and (not upstream_failed):\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task failure, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ONE_DONE:\n        if success + failed <= 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}'requires at least one upstream task failure or successbut none were failed or success. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_SUCCESS:\n        num_failures = upstream - success\n        if ti.map_index > -1:\n            num_failures -= removed\n        if num_failures > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_FAILED:\n        num_success = upstream - failed - upstream_failed\n        if ti.map_index > -1:\n            num_success -= removed\n        if num_success > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have failed, but found {num_success} non-failure(s). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_DONE:\n        if not upstream_done:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.NONE_FAILED or trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n        num_failures = upstream - success - skipped\n        if ti.map_index > -1:\n            num_failures -= removed\n        if num_failures > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded or been skipped, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.NONE_SKIPPED:\n        if not upstream_done or skipped > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to not have been skipped, but found {skipped} task(s) skipped. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_SKIPPED:\n        num_non_skipped = upstream - skipped\n        if num_non_skipped > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have been skipped, but found {num_non_skipped} task(s) in non skipped state. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n        if not upstream_done:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif upstream_setup is None:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' cannot have mapped tasks as upstream. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif upstream_setup and (not success_setup):\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires at least one upstream setup task be successful, but found {upstream_setup - success_setup} task(s) that were not. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    else:\n        yield self._failing_status(reason=f\"No strategy to evaluate trigger rule '{trigger_rule}'.\")",
            "def _evaluate_direct_relatives() -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n            :param ti: Task instance to evaluate the trigger rule of.\\n            :param dep_context: The current dependency context.\\n            :param session: Database session.\\n            \"\n    task = ti.task\n    upstream_tasks = {t.task_id: t for t in task.upstream_list}\n    trigger_rule = task.trigger_rule\n    finished_upstream_tis = (finished_ti for finished_ti in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=finished_ti, relevant_ids=ti.task.upstream_task_ids))\n    upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n    success = upstream_states.success\n    skipped = upstream_states.skipped\n    failed = upstream_states.failed\n    upstream_failed = upstream_states.upstream_failed\n    removed = upstream_states.removed\n    done = upstream_states.done\n    success_setup = upstream_states.success_setup\n    skipped_setup = upstream_states.skipped_setup\n    if not any((needs_expansion(t) for t in upstream_tasks.values())):\n        upstream = len(upstream_tasks)\n        upstream_setup = sum((1 for x in upstream_tasks.values() if x.is_setup))\n    else:\n        task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=upstream_tasks))).group_by(TaskInstance.task_id)).all()\n        upstream = sum((count for (_, count) in task_id_counts))\n        upstream_setup = sum((c for (t, c) in task_id_counts if upstream_tasks[t].is_setup))\n    upstream_done = done >= upstream\n    changed = False\n    new_state = None\n    if dep_context.flag_upstream_failed:\n        if trigger_rule == TR.ALL_SUCCESS:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif removed and success and (ti.map_index > -1):\n                if ti.map_index >= success:\n                    new_state = TaskInstanceState.REMOVED\n        elif trigger_rule == TR.ALL_FAILED:\n            if success or skipped:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ONE_SUCCESS:\n            if upstream_done and done == skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif upstream_done and success <= 0:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif trigger_rule == TR.ONE_FAILED:\n            if upstream_done and (not (failed or upstream_failed)):\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ONE_DONE:\n            if upstream_done and (not (failed or success)):\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.NONE_FAILED:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped == upstream:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.NONE_SKIPPED:\n            if skipped:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ALL_SKIPPED:\n            if success or failed or upstream_failed:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n            if upstream_done and upstream_setup and (skipped_setup >= upstream_setup):\n                new_state = TaskInstanceState.SKIPPED\n            elif upstream_done and upstream_setup and (success_setup == 0):\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n    if new_state is not None:\n        if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n            past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n            if not past_depends_met:\n                yield self._failing_status(reason='Task should be skipped but the past depends are not met')\n                return\n        changed = ti.set_state(new_state, session)\n    if changed:\n        dep_context.have_changed_ti_states = True\n    if trigger_rule == TR.ONE_SUCCESS:\n        if success <= 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task success, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ONE_FAILED:\n        if not failed and (not upstream_failed):\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task failure, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ONE_DONE:\n        if success + failed <= 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}'requires at least one upstream task failure or successbut none were failed or success. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_SUCCESS:\n        num_failures = upstream - success\n        if ti.map_index > -1:\n            num_failures -= removed\n        if num_failures > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_FAILED:\n        num_success = upstream - failed - upstream_failed\n        if ti.map_index > -1:\n            num_success -= removed\n        if num_success > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have failed, but found {num_success} non-failure(s). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_DONE:\n        if not upstream_done:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.NONE_FAILED or trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n        num_failures = upstream - success - skipped\n        if ti.map_index > -1:\n            num_failures -= removed\n        if num_failures > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded or been skipped, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.NONE_SKIPPED:\n        if not upstream_done or skipped > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to not have been skipped, but found {skipped} task(s) skipped. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_SKIPPED:\n        num_non_skipped = upstream - skipped\n        if num_non_skipped > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have been skipped, but found {num_non_skipped} task(s) in non skipped state. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n        if not upstream_done:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif upstream_setup is None:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' cannot have mapped tasks as upstream. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif upstream_setup and (not success_setup):\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires at least one upstream setup task be successful, but found {upstream_setup - success_setup} task(s) that were not. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    else:\n        yield self._failing_status(reason=f\"No strategy to evaluate trigger rule '{trigger_rule}'.\")",
            "def _evaluate_direct_relatives() -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n            :param ti: Task instance to evaluate the trigger rule of.\\n            :param dep_context: The current dependency context.\\n            :param session: Database session.\\n            \"\n    task = ti.task\n    upstream_tasks = {t.task_id: t for t in task.upstream_list}\n    trigger_rule = task.trigger_rule\n    finished_upstream_tis = (finished_ti for finished_ti in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=finished_ti, relevant_ids=ti.task.upstream_task_ids))\n    upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n    success = upstream_states.success\n    skipped = upstream_states.skipped\n    failed = upstream_states.failed\n    upstream_failed = upstream_states.upstream_failed\n    removed = upstream_states.removed\n    done = upstream_states.done\n    success_setup = upstream_states.success_setup\n    skipped_setup = upstream_states.skipped_setup\n    if not any((needs_expansion(t) for t in upstream_tasks.values())):\n        upstream = len(upstream_tasks)\n        upstream_setup = sum((1 for x in upstream_tasks.values() if x.is_setup))\n    else:\n        task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=upstream_tasks))).group_by(TaskInstance.task_id)).all()\n        upstream = sum((count for (_, count) in task_id_counts))\n        upstream_setup = sum((c for (t, c) in task_id_counts if upstream_tasks[t].is_setup))\n    upstream_done = done >= upstream\n    changed = False\n    new_state = None\n    if dep_context.flag_upstream_failed:\n        if trigger_rule == TR.ALL_SUCCESS:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif removed and success and (ti.map_index > -1):\n                if ti.map_index >= success:\n                    new_state = TaskInstanceState.REMOVED\n        elif trigger_rule == TR.ALL_FAILED:\n            if success or skipped:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ONE_SUCCESS:\n            if upstream_done and done == skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif upstream_done and success <= 0:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif trigger_rule == TR.ONE_FAILED:\n            if upstream_done and (not (failed or upstream_failed)):\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ONE_DONE:\n            if upstream_done and (not (failed or success)):\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.NONE_FAILED:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped == upstream:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.NONE_SKIPPED:\n            if skipped:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ALL_SKIPPED:\n            if success or failed or upstream_failed:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n            if upstream_done and upstream_setup and (skipped_setup >= upstream_setup):\n                new_state = TaskInstanceState.SKIPPED\n            elif upstream_done and upstream_setup and (success_setup == 0):\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n    if new_state is not None:\n        if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n            past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n            if not past_depends_met:\n                yield self._failing_status(reason='Task should be skipped but the past depends are not met')\n                return\n        changed = ti.set_state(new_state, session)\n    if changed:\n        dep_context.have_changed_ti_states = True\n    if trigger_rule == TR.ONE_SUCCESS:\n        if success <= 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task success, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ONE_FAILED:\n        if not failed and (not upstream_failed):\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task failure, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ONE_DONE:\n        if success + failed <= 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}'requires at least one upstream task failure or successbut none were failed or success. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_SUCCESS:\n        num_failures = upstream - success\n        if ti.map_index > -1:\n            num_failures -= removed\n        if num_failures > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_FAILED:\n        num_success = upstream - failed - upstream_failed\n        if ti.map_index > -1:\n            num_success -= removed\n        if num_success > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have failed, but found {num_success} non-failure(s). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_DONE:\n        if not upstream_done:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.NONE_FAILED or trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n        num_failures = upstream - success - skipped\n        if ti.map_index > -1:\n            num_failures -= removed\n        if num_failures > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded or been skipped, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.NONE_SKIPPED:\n        if not upstream_done or skipped > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to not have been skipped, but found {skipped} task(s) skipped. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_SKIPPED:\n        num_non_skipped = upstream - skipped\n        if num_non_skipped > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have been skipped, but found {num_non_skipped} task(s) in non skipped state. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n        if not upstream_done:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif upstream_setup is None:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' cannot have mapped tasks as upstream. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif upstream_setup and (not success_setup):\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires at least one upstream setup task be successful, but found {upstream_setup - success_setup} task(s) that were not. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    else:\n        yield self._failing_status(reason=f\"No strategy to evaluate trigger rule '{trigger_rule}'.\")",
            "def _evaluate_direct_relatives() -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n            :param ti: Task instance to evaluate the trigger rule of.\\n            :param dep_context: The current dependency context.\\n            :param session: Database session.\\n            \"\n    task = ti.task\n    upstream_tasks = {t.task_id: t for t in task.upstream_list}\n    trigger_rule = task.trigger_rule\n    finished_upstream_tis = (finished_ti for finished_ti in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=finished_ti, relevant_ids=ti.task.upstream_task_ids))\n    upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n    success = upstream_states.success\n    skipped = upstream_states.skipped\n    failed = upstream_states.failed\n    upstream_failed = upstream_states.upstream_failed\n    removed = upstream_states.removed\n    done = upstream_states.done\n    success_setup = upstream_states.success_setup\n    skipped_setup = upstream_states.skipped_setup\n    if not any((needs_expansion(t) for t in upstream_tasks.values())):\n        upstream = len(upstream_tasks)\n        upstream_setup = sum((1 for x in upstream_tasks.values() if x.is_setup))\n    else:\n        task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=upstream_tasks))).group_by(TaskInstance.task_id)).all()\n        upstream = sum((count for (_, count) in task_id_counts))\n        upstream_setup = sum((c for (t, c) in task_id_counts if upstream_tasks[t].is_setup))\n    upstream_done = done >= upstream\n    changed = False\n    new_state = None\n    if dep_context.flag_upstream_failed:\n        if trigger_rule == TR.ALL_SUCCESS:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif removed and success and (ti.map_index > -1):\n                if ti.map_index >= success:\n                    new_state = TaskInstanceState.REMOVED\n        elif trigger_rule == TR.ALL_FAILED:\n            if success or skipped:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ONE_SUCCESS:\n            if upstream_done and done == skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif upstream_done and success <= 0:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif trigger_rule == TR.ONE_FAILED:\n            if upstream_done and (not (failed or upstream_failed)):\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ONE_DONE:\n            if upstream_done and (not (failed or success)):\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.NONE_FAILED:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n        elif trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped == upstream:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.NONE_SKIPPED:\n            if skipped:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ALL_SKIPPED:\n            if success or failed or upstream_failed:\n                new_state = TaskInstanceState.SKIPPED\n        elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n            if upstream_done and upstream_setup and (skipped_setup >= upstream_setup):\n                new_state = TaskInstanceState.SKIPPED\n            elif upstream_done and upstream_setup and (success_setup == 0):\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n    if new_state is not None:\n        if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n            past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n            if not past_depends_met:\n                yield self._failing_status(reason='Task should be skipped but the past depends are not met')\n                return\n        changed = ti.set_state(new_state, session)\n    if changed:\n        dep_context.have_changed_ti_states = True\n    if trigger_rule == TR.ONE_SUCCESS:\n        if success <= 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task success, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ONE_FAILED:\n        if not failed and (not upstream_failed):\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task failure, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ONE_DONE:\n        if success + failed <= 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}'requires at least one upstream task failure or successbut none were failed or success. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_SUCCESS:\n        num_failures = upstream - success\n        if ti.map_index > -1:\n            num_failures -= removed\n        if num_failures > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_FAILED:\n        num_success = upstream - failed - upstream_failed\n        if ti.map_index > -1:\n            num_success -= removed\n        if num_success > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have failed, but found {num_success} non-failure(s). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_DONE:\n        if not upstream_done:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.NONE_FAILED or trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n        num_failures = upstream - success - skipped\n        if ti.map_index > -1:\n            num_failures -= removed\n        if num_failures > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded or been skipped, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.NONE_SKIPPED:\n        if not upstream_done or skipped > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to not have been skipped, but found {skipped} task(s) skipped. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_SKIPPED:\n        num_non_skipped = upstream - skipped\n        if num_non_skipped > 0:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have been skipped, but found {num_non_skipped} task(s) in non skipped state. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n        if not upstream_done:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif upstream_setup is None:\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' cannot have mapped tasks as upstream. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif upstream_setup and (not success_setup):\n            yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires at least one upstream setup task be successful, but found {upstream_setup - success_setup} task(s) that were not. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n    else:\n        yield self._failing_status(reason=f\"No strategy to evaluate trigger rule '{trigger_rule}'.\")"
        ]
    },
    {
        "func_name": "_evaluate_trigger_rule",
        "original": "def _evaluate_trigger_rule(self, *, ti: TaskInstance, dep_context: DepContext, session: Session) -> Iterator[TIDepStatus]:\n    \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n        :param ti: Task instance to evaluate the trigger rule of.\n        :param dep_context: The current dependency context.\n        :param session: Database session.\n        \"\"\"\n    from airflow.models.abstractoperator import NotMapped\n    from airflow.models.expandinput import NotFullyPopulated\n    from airflow.models.operator import needs_expansion\n    from airflow.models.taskinstance import TaskInstance\n\n    @functools.lru_cache\n    def _get_expanded_ti_count() -> int:\n        \"\"\"Get how many tis the current task is supposed to be expanded into.\n\n            This extra closure allows us to query the database only when needed,\n            and at most once.\n            \"\"\"\n        return ti.task.get_mapped_ti_count(ti.run_id, session=session)\n\n    @functools.lru_cache\n    def _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:\n        \"\"\"Get the given task's map indexes relevant to the current ti.\n\n            This extra closure allows us to query the database only when needed,\n            and at most once for each task (instead of once for each expanded\n            task instance of the same task).\n            \"\"\"\n        if TYPE_CHECKING:\n            assert isinstance(ti.task.dag, DAG)\n        try:\n            expanded_ti_count = _get_expanded_ti_count()\n        except (NotFullyPopulated, NotMapped):\n            return None\n        return ti.get_relevant_upstream_map_indexes(upstream=ti.task.dag.task_dict[upstream_id], ti_count=expanded_ti_count, session=session)\n\n    def _is_relevant_upstream(upstream: TaskInstance, relevant_ids: set[str] | KeysView[str]) -> bool:\n        \"\"\"\n            Whether a task instance is a \"relevant upstream\" of the current task.\n\n            This will return false if upstream.task_id is not in relevant_ids,\n            or if both of the following are true:\n                1. upstream.task_id in relevant_ids is True\n                2. ti is in a mapped task group and upstream has a map index\n                  that ti does not depend on.\n            \"\"\"\n        if upstream.task_id not in relevant_ids:\n            return False\n        if ti.task.get_closest_mapped_task_group() is None:\n            return True\n        if upstream.map_index < 0:\n            return True\n        relevant = _get_relevant_upstream_map_indexes(upstream_id=upstream.task_id)\n        if relevant is None:\n            return True\n        if relevant == upstream.map_index:\n            return True\n        if isinstance(relevant, collections.abc.Container) and upstream.map_index in relevant:\n            return True\n        return False\n\n    def _iter_upstream_conditions(relevant_tasks: dict) -> Iterator[ColumnOperators]:\n        from airflow.models.taskinstance import TaskInstance\n        if ti.task.get_closest_mapped_task_group() is None:\n            yield TaskInstance.task_id.in_(relevant_tasks.keys())\n            return\n        for upstream_id in relevant_tasks:\n            map_indexes = _get_relevant_upstream_map_indexes(upstream_id)\n            if map_indexes is None:\n                yield (TaskInstance.task_id == upstream_id)\n                continue\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index < 0)\n            if isinstance(map_indexes, range) and map_indexes.step == 1:\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index >= map_indexes.start, TaskInstance.map_index < map_indexes.stop)\n            elif isinstance(map_indexes, collections.abc.Container):\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index.in_(map_indexes))\n            else:\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index == map_indexes)\n\n    def _evaluate_setup_constraint(*, relevant_setups) -> Iterator[tuple[TIDepStatus, bool]]:\n        \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n            :param ti: Task instance to evaluate the trigger rule of.\n            :param dep_context: The current dependency context.\n            :param session: Database session.\n            \"\"\"\n        task = ti.task\n        indirect_setups = {k: v for (k, v) in relevant_setups.items() if k not in task.upstream_task_ids}\n        finished_upstream_tis = (x for x in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=x, relevant_ids=indirect_setups.keys()))\n        upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n        success = upstream_states.success\n        skipped = upstream_states.skipped\n        failed = upstream_states.failed\n        upstream_failed = upstream_states.upstream_failed\n        removed = upstream_states.removed\n        if not any((needs_expansion(t) for t in indirect_setups.values())):\n            upstream = len(indirect_setups)\n        else:\n            task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=indirect_setups))).group_by(TaskInstance.task_id)).all()\n            upstream = sum((count for (_, count) in task_id_counts))\n        new_state = None\n        changed = False\n        if dep_context.flag_upstream_failed:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif removed and success and (ti.map_index > -1):\n                if ti.map_index >= success:\n                    new_state = TaskInstanceState.REMOVED\n        if new_state is not None:\n            if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n                past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n                if not past_depends_met:\n                    yield (self._failing_status(reason='Task should be skipped but the past depends are not met'), changed)\n                    return\n            changed = ti.set_state(new_state, session)\n        if changed:\n            dep_context.have_changed_ti_states = True\n        non_successes = upstream - success\n        if ti.map_index > -1:\n            non_successes -= removed\n        if non_successes > 0:\n            yield (self._failing_status(reason=f'All setup tasks must complete successfully. Relevant setups: {relevant_setups}: upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}'), changed)\n\n    def _evaluate_direct_relatives() -> Iterator[TIDepStatus]:\n        \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n            :param ti: Task instance to evaluate the trigger rule of.\n            :param dep_context: The current dependency context.\n            :param session: Database session.\n            \"\"\"\n        task = ti.task\n        upstream_tasks = {t.task_id: t for t in task.upstream_list}\n        trigger_rule = task.trigger_rule\n        finished_upstream_tis = (finished_ti for finished_ti in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=finished_ti, relevant_ids=ti.task.upstream_task_ids))\n        upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n        success = upstream_states.success\n        skipped = upstream_states.skipped\n        failed = upstream_states.failed\n        upstream_failed = upstream_states.upstream_failed\n        removed = upstream_states.removed\n        done = upstream_states.done\n        success_setup = upstream_states.success_setup\n        skipped_setup = upstream_states.skipped_setup\n        if not any((needs_expansion(t) for t in upstream_tasks.values())):\n            upstream = len(upstream_tasks)\n            upstream_setup = sum((1 for x in upstream_tasks.values() if x.is_setup))\n        else:\n            task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=upstream_tasks))).group_by(TaskInstance.task_id)).all()\n            upstream = sum((count for (_, count) in task_id_counts))\n            upstream_setup = sum((c for (t, c) in task_id_counts if upstream_tasks[t].is_setup))\n        upstream_done = done >= upstream\n        changed = False\n        new_state = None\n        if dep_context.flag_upstream_failed:\n            if trigger_rule == TR.ALL_SUCCESS:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n                elif skipped:\n                    new_state = TaskInstanceState.SKIPPED\n                elif removed and success and (ti.map_index > -1):\n                    if ti.map_index >= success:\n                        new_state = TaskInstanceState.REMOVED\n            elif trigger_rule == TR.ALL_FAILED:\n                if success or skipped:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ONE_SUCCESS:\n                if upstream_done and done == skipped:\n                    new_state = TaskInstanceState.SKIPPED\n                elif upstream_done and success <= 0:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif trigger_rule == TR.ONE_FAILED:\n                if upstream_done and (not (failed or upstream_failed)):\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ONE_DONE:\n                if upstream_done and (not (failed or success)):\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.NONE_FAILED:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n                elif skipped == upstream:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.NONE_SKIPPED:\n                if skipped:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ALL_SKIPPED:\n                if success or failed or upstream_failed:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n                if upstream_done and upstream_setup and (skipped_setup >= upstream_setup):\n                    new_state = TaskInstanceState.SKIPPED\n                elif upstream_done and upstream_setup and (success_setup == 0):\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n        if new_state is not None:\n            if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n                past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n                if not past_depends_met:\n                    yield self._failing_status(reason='Task should be skipped but the past depends are not met')\n                    return\n            changed = ti.set_state(new_state, session)\n        if changed:\n            dep_context.have_changed_ti_states = True\n        if trigger_rule == TR.ONE_SUCCESS:\n            if success <= 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task success, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ONE_FAILED:\n            if not failed and (not upstream_failed):\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task failure, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ONE_DONE:\n            if success + failed <= 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}'requires at least one upstream task failure or successbut none were failed or success. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_SUCCESS:\n            num_failures = upstream - success\n            if ti.map_index > -1:\n                num_failures -= removed\n            if num_failures > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_FAILED:\n            num_success = upstream - failed - upstream_failed\n            if ti.map_index > -1:\n                num_success -= removed\n            if num_success > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have failed, but found {num_success} non-failure(s). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_DONE:\n            if not upstream_done:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.NONE_FAILED or trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n            num_failures = upstream - success - skipped\n            if ti.map_index > -1:\n                num_failures -= removed\n            if num_failures > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded or been skipped, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.NONE_SKIPPED:\n            if not upstream_done or skipped > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to not have been skipped, but found {skipped} task(s) skipped. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_SKIPPED:\n            num_non_skipped = upstream - skipped\n            if num_non_skipped > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have been skipped, but found {num_non_skipped} task(s) in non skipped state. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n            if not upstream_done:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n            elif upstream_setup is None:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' cannot have mapped tasks as upstream. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n            elif upstream_setup and (not success_setup):\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires at least one upstream setup task be successful, but found {upstream_setup - success_setup} task(s) that were not. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        else:\n            yield self._failing_status(reason=f\"No strategy to evaluate trigger rule '{trigger_rule}'.\")\n    if not ti.task.is_teardown:\n        relevant_setups = {t.task_id: t for t in ti.task.get_upstreams_only_setups()}\n        if relevant_setups:\n            for (status, changed) in _evaluate_setup_constraint(relevant_setups=relevant_setups):\n                yield status\n                if not status.passed and changed:\n                    return\n    yield from _evaluate_direct_relatives()",
        "mutated": [
            "def _evaluate_trigger_rule(self, *, ti: TaskInstance, dep_context: DepContext, session: Session) -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n        :param ti: Task instance to evaluate the trigger rule of.\\n        :param dep_context: The current dependency context.\\n        :param session: Database session.\\n        \"\n    from airflow.models.abstractoperator import NotMapped\n    from airflow.models.expandinput import NotFullyPopulated\n    from airflow.models.operator import needs_expansion\n    from airflow.models.taskinstance import TaskInstance\n\n    @functools.lru_cache\n    def _get_expanded_ti_count() -> int:\n        \"\"\"Get how many tis the current task is supposed to be expanded into.\n\n            This extra closure allows us to query the database only when needed,\n            and at most once.\n            \"\"\"\n        return ti.task.get_mapped_ti_count(ti.run_id, session=session)\n\n    @functools.lru_cache\n    def _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:\n        \"\"\"Get the given task's map indexes relevant to the current ti.\n\n            This extra closure allows us to query the database only when needed,\n            and at most once for each task (instead of once for each expanded\n            task instance of the same task).\n            \"\"\"\n        if TYPE_CHECKING:\n            assert isinstance(ti.task.dag, DAG)\n        try:\n            expanded_ti_count = _get_expanded_ti_count()\n        except (NotFullyPopulated, NotMapped):\n            return None\n        return ti.get_relevant_upstream_map_indexes(upstream=ti.task.dag.task_dict[upstream_id], ti_count=expanded_ti_count, session=session)\n\n    def _is_relevant_upstream(upstream: TaskInstance, relevant_ids: set[str] | KeysView[str]) -> bool:\n        \"\"\"\n            Whether a task instance is a \"relevant upstream\" of the current task.\n\n            This will return false if upstream.task_id is not in relevant_ids,\n            or if both of the following are true:\n                1. upstream.task_id in relevant_ids is True\n                2. ti is in a mapped task group and upstream has a map index\n                  that ti does not depend on.\n            \"\"\"\n        if upstream.task_id not in relevant_ids:\n            return False\n        if ti.task.get_closest_mapped_task_group() is None:\n            return True\n        if upstream.map_index < 0:\n            return True\n        relevant = _get_relevant_upstream_map_indexes(upstream_id=upstream.task_id)\n        if relevant is None:\n            return True\n        if relevant == upstream.map_index:\n            return True\n        if isinstance(relevant, collections.abc.Container) and upstream.map_index in relevant:\n            return True\n        return False\n\n    def _iter_upstream_conditions(relevant_tasks: dict) -> Iterator[ColumnOperators]:\n        from airflow.models.taskinstance import TaskInstance\n        if ti.task.get_closest_mapped_task_group() is None:\n            yield TaskInstance.task_id.in_(relevant_tasks.keys())\n            return\n        for upstream_id in relevant_tasks:\n            map_indexes = _get_relevant_upstream_map_indexes(upstream_id)\n            if map_indexes is None:\n                yield (TaskInstance.task_id == upstream_id)\n                continue\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index < 0)\n            if isinstance(map_indexes, range) and map_indexes.step == 1:\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index >= map_indexes.start, TaskInstance.map_index < map_indexes.stop)\n            elif isinstance(map_indexes, collections.abc.Container):\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index.in_(map_indexes))\n            else:\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index == map_indexes)\n\n    def _evaluate_setup_constraint(*, relevant_setups) -> Iterator[tuple[TIDepStatus, bool]]:\n        \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n            :param ti: Task instance to evaluate the trigger rule of.\n            :param dep_context: The current dependency context.\n            :param session: Database session.\n            \"\"\"\n        task = ti.task\n        indirect_setups = {k: v for (k, v) in relevant_setups.items() if k not in task.upstream_task_ids}\n        finished_upstream_tis = (x for x in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=x, relevant_ids=indirect_setups.keys()))\n        upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n        success = upstream_states.success\n        skipped = upstream_states.skipped\n        failed = upstream_states.failed\n        upstream_failed = upstream_states.upstream_failed\n        removed = upstream_states.removed\n        if not any((needs_expansion(t) for t in indirect_setups.values())):\n            upstream = len(indirect_setups)\n        else:\n            task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=indirect_setups))).group_by(TaskInstance.task_id)).all()\n            upstream = sum((count for (_, count) in task_id_counts))\n        new_state = None\n        changed = False\n        if dep_context.flag_upstream_failed:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif removed and success and (ti.map_index > -1):\n                if ti.map_index >= success:\n                    new_state = TaskInstanceState.REMOVED\n        if new_state is not None:\n            if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n                past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n                if not past_depends_met:\n                    yield (self._failing_status(reason='Task should be skipped but the past depends are not met'), changed)\n                    return\n            changed = ti.set_state(new_state, session)\n        if changed:\n            dep_context.have_changed_ti_states = True\n        non_successes = upstream - success\n        if ti.map_index > -1:\n            non_successes -= removed\n        if non_successes > 0:\n            yield (self._failing_status(reason=f'All setup tasks must complete successfully. Relevant setups: {relevant_setups}: upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}'), changed)\n\n    def _evaluate_direct_relatives() -> Iterator[TIDepStatus]:\n        \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n            :param ti: Task instance to evaluate the trigger rule of.\n            :param dep_context: The current dependency context.\n            :param session: Database session.\n            \"\"\"\n        task = ti.task\n        upstream_tasks = {t.task_id: t for t in task.upstream_list}\n        trigger_rule = task.trigger_rule\n        finished_upstream_tis = (finished_ti for finished_ti in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=finished_ti, relevant_ids=ti.task.upstream_task_ids))\n        upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n        success = upstream_states.success\n        skipped = upstream_states.skipped\n        failed = upstream_states.failed\n        upstream_failed = upstream_states.upstream_failed\n        removed = upstream_states.removed\n        done = upstream_states.done\n        success_setup = upstream_states.success_setup\n        skipped_setup = upstream_states.skipped_setup\n        if not any((needs_expansion(t) for t in upstream_tasks.values())):\n            upstream = len(upstream_tasks)\n            upstream_setup = sum((1 for x in upstream_tasks.values() if x.is_setup))\n        else:\n            task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=upstream_tasks))).group_by(TaskInstance.task_id)).all()\n            upstream = sum((count for (_, count) in task_id_counts))\n            upstream_setup = sum((c for (t, c) in task_id_counts if upstream_tasks[t].is_setup))\n        upstream_done = done >= upstream\n        changed = False\n        new_state = None\n        if dep_context.flag_upstream_failed:\n            if trigger_rule == TR.ALL_SUCCESS:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n                elif skipped:\n                    new_state = TaskInstanceState.SKIPPED\n                elif removed and success and (ti.map_index > -1):\n                    if ti.map_index >= success:\n                        new_state = TaskInstanceState.REMOVED\n            elif trigger_rule == TR.ALL_FAILED:\n                if success or skipped:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ONE_SUCCESS:\n                if upstream_done and done == skipped:\n                    new_state = TaskInstanceState.SKIPPED\n                elif upstream_done and success <= 0:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif trigger_rule == TR.ONE_FAILED:\n                if upstream_done and (not (failed or upstream_failed)):\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ONE_DONE:\n                if upstream_done and (not (failed or success)):\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.NONE_FAILED:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n                elif skipped == upstream:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.NONE_SKIPPED:\n                if skipped:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ALL_SKIPPED:\n                if success or failed or upstream_failed:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n                if upstream_done and upstream_setup and (skipped_setup >= upstream_setup):\n                    new_state = TaskInstanceState.SKIPPED\n                elif upstream_done and upstream_setup and (success_setup == 0):\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n        if new_state is not None:\n            if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n                past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n                if not past_depends_met:\n                    yield self._failing_status(reason='Task should be skipped but the past depends are not met')\n                    return\n            changed = ti.set_state(new_state, session)\n        if changed:\n            dep_context.have_changed_ti_states = True\n        if trigger_rule == TR.ONE_SUCCESS:\n            if success <= 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task success, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ONE_FAILED:\n            if not failed and (not upstream_failed):\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task failure, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ONE_DONE:\n            if success + failed <= 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}'requires at least one upstream task failure or successbut none were failed or success. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_SUCCESS:\n            num_failures = upstream - success\n            if ti.map_index > -1:\n                num_failures -= removed\n            if num_failures > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_FAILED:\n            num_success = upstream - failed - upstream_failed\n            if ti.map_index > -1:\n                num_success -= removed\n            if num_success > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have failed, but found {num_success} non-failure(s). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_DONE:\n            if not upstream_done:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.NONE_FAILED or trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n            num_failures = upstream - success - skipped\n            if ti.map_index > -1:\n                num_failures -= removed\n            if num_failures > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded or been skipped, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.NONE_SKIPPED:\n            if not upstream_done or skipped > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to not have been skipped, but found {skipped} task(s) skipped. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_SKIPPED:\n            num_non_skipped = upstream - skipped\n            if num_non_skipped > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have been skipped, but found {num_non_skipped} task(s) in non skipped state. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n            if not upstream_done:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n            elif upstream_setup is None:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' cannot have mapped tasks as upstream. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n            elif upstream_setup and (not success_setup):\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires at least one upstream setup task be successful, but found {upstream_setup - success_setup} task(s) that were not. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        else:\n            yield self._failing_status(reason=f\"No strategy to evaluate trigger rule '{trigger_rule}'.\")\n    if not ti.task.is_teardown:\n        relevant_setups = {t.task_id: t for t in ti.task.get_upstreams_only_setups()}\n        if relevant_setups:\n            for (status, changed) in _evaluate_setup_constraint(relevant_setups=relevant_setups):\n                yield status\n                if not status.passed and changed:\n                    return\n    yield from _evaluate_direct_relatives()",
            "def _evaluate_trigger_rule(self, *, ti: TaskInstance, dep_context: DepContext, session: Session) -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n        :param ti: Task instance to evaluate the trigger rule of.\\n        :param dep_context: The current dependency context.\\n        :param session: Database session.\\n        \"\n    from airflow.models.abstractoperator import NotMapped\n    from airflow.models.expandinput import NotFullyPopulated\n    from airflow.models.operator import needs_expansion\n    from airflow.models.taskinstance import TaskInstance\n\n    @functools.lru_cache\n    def _get_expanded_ti_count() -> int:\n        \"\"\"Get how many tis the current task is supposed to be expanded into.\n\n            This extra closure allows us to query the database only when needed,\n            and at most once.\n            \"\"\"\n        return ti.task.get_mapped_ti_count(ti.run_id, session=session)\n\n    @functools.lru_cache\n    def _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:\n        \"\"\"Get the given task's map indexes relevant to the current ti.\n\n            This extra closure allows us to query the database only when needed,\n            and at most once for each task (instead of once for each expanded\n            task instance of the same task).\n            \"\"\"\n        if TYPE_CHECKING:\n            assert isinstance(ti.task.dag, DAG)\n        try:\n            expanded_ti_count = _get_expanded_ti_count()\n        except (NotFullyPopulated, NotMapped):\n            return None\n        return ti.get_relevant_upstream_map_indexes(upstream=ti.task.dag.task_dict[upstream_id], ti_count=expanded_ti_count, session=session)\n\n    def _is_relevant_upstream(upstream: TaskInstance, relevant_ids: set[str] | KeysView[str]) -> bool:\n        \"\"\"\n            Whether a task instance is a \"relevant upstream\" of the current task.\n\n            This will return false if upstream.task_id is not in relevant_ids,\n            or if both of the following are true:\n                1. upstream.task_id in relevant_ids is True\n                2. ti is in a mapped task group and upstream has a map index\n                  that ti does not depend on.\n            \"\"\"\n        if upstream.task_id not in relevant_ids:\n            return False\n        if ti.task.get_closest_mapped_task_group() is None:\n            return True\n        if upstream.map_index < 0:\n            return True\n        relevant = _get_relevant_upstream_map_indexes(upstream_id=upstream.task_id)\n        if relevant is None:\n            return True\n        if relevant == upstream.map_index:\n            return True\n        if isinstance(relevant, collections.abc.Container) and upstream.map_index in relevant:\n            return True\n        return False\n\n    def _iter_upstream_conditions(relevant_tasks: dict) -> Iterator[ColumnOperators]:\n        from airflow.models.taskinstance import TaskInstance\n        if ti.task.get_closest_mapped_task_group() is None:\n            yield TaskInstance.task_id.in_(relevant_tasks.keys())\n            return\n        for upstream_id in relevant_tasks:\n            map_indexes = _get_relevant_upstream_map_indexes(upstream_id)\n            if map_indexes is None:\n                yield (TaskInstance.task_id == upstream_id)\n                continue\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index < 0)\n            if isinstance(map_indexes, range) and map_indexes.step == 1:\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index >= map_indexes.start, TaskInstance.map_index < map_indexes.stop)\n            elif isinstance(map_indexes, collections.abc.Container):\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index.in_(map_indexes))\n            else:\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index == map_indexes)\n\n    def _evaluate_setup_constraint(*, relevant_setups) -> Iterator[tuple[TIDepStatus, bool]]:\n        \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n            :param ti: Task instance to evaluate the trigger rule of.\n            :param dep_context: The current dependency context.\n            :param session: Database session.\n            \"\"\"\n        task = ti.task\n        indirect_setups = {k: v for (k, v) in relevant_setups.items() if k not in task.upstream_task_ids}\n        finished_upstream_tis = (x for x in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=x, relevant_ids=indirect_setups.keys()))\n        upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n        success = upstream_states.success\n        skipped = upstream_states.skipped\n        failed = upstream_states.failed\n        upstream_failed = upstream_states.upstream_failed\n        removed = upstream_states.removed\n        if not any((needs_expansion(t) for t in indirect_setups.values())):\n            upstream = len(indirect_setups)\n        else:\n            task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=indirect_setups))).group_by(TaskInstance.task_id)).all()\n            upstream = sum((count for (_, count) in task_id_counts))\n        new_state = None\n        changed = False\n        if dep_context.flag_upstream_failed:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif removed and success and (ti.map_index > -1):\n                if ti.map_index >= success:\n                    new_state = TaskInstanceState.REMOVED\n        if new_state is not None:\n            if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n                past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n                if not past_depends_met:\n                    yield (self._failing_status(reason='Task should be skipped but the past depends are not met'), changed)\n                    return\n            changed = ti.set_state(new_state, session)\n        if changed:\n            dep_context.have_changed_ti_states = True\n        non_successes = upstream - success\n        if ti.map_index > -1:\n            non_successes -= removed\n        if non_successes > 0:\n            yield (self._failing_status(reason=f'All setup tasks must complete successfully. Relevant setups: {relevant_setups}: upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}'), changed)\n\n    def _evaluate_direct_relatives() -> Iterator[TIDepStatus]:\n        \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n            :param ti: Task instance to evaluate the trigger rule of.\n            :param dep_context: The current dependency context.\n            :param session: Database session.\n            \"\"\"\n        task = ti.task\n        upstream_tasks = {t.task_id: t for t in task.upstream_list}\n        trigger_rule = task.trigger_rule\n        finished_upstream_tis = (finished_ti for finished_ti in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=finished_ti, relevant_ids=ti.task.upstream_task_ids))\n        upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n        success = upstream_states.success\n        skipped = upstream_states.skipped\n        failed = upstream_states.failed\n        upstream_failed = upstream_states.upstream_failed\n        removed = upstream_states.removed\n        done = upstream_states.done\n        success_setup = upstream_states.success_setup\n        skipped_setup = upstream_states.skipped_setup\n        if not any((needs_expansion(t) for t in upstream_tasks.values())):\n            upstream = len(upstream_tasks)\n            upstream_setup = sum((1 for x in upstream_tasks.values() if x.is_setup))\n        else:\n            task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=upstream_tasks))).group_by(TaskInstance.task_id)).all()\n            upstream = sum((count for (_, count) in task_id_counts))\n            upstream_setup = sum((c for (t, c) in task_id_counts if upstream_tasks[t].is_setup))\n        upstream_done = done >= upstream\n        changed = False\n        new_state = None\n        if dep_context.flag_upstream_failed:\n            if trigger_rule == TR.ALL_SUCCESS:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n                elif skipped:\n                    new_state = TaskInstanceState.SKIPPED\n                elif removed and success and (ti.map_index > -1):\n                    if ti.map_index >= success:\n                        new_state = TaskInstanceState.REMOVED\n            elif trigger_rule == TR.ALL_FAILED:\n                if success or skipped:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ONE_SUCCESS:\n                if upstream_done and done == skipped:\n                    new_state = TaskInstanceState.SKIPPED\n                elif upstream_done and success <= 0:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif trigger_rule == TR.ONE_FAILED:\n                if upstream_done and (not (failed or upstream_failed)):\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ONE_DONE:\n                if upstream_done and (not (failed or success)):\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.NONE_FAILED:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n                elif skipped == upstream:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.NONE_SKIPPED:\n                if skipped:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ALL_SKIPPED:\n                if success or failed or upstream_failed:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n                if upstream_done and upstream_setup and (skipped_setup >= upstream_setup):\n                    new_state = TaskInstanceState.SKIPPED\n                elif upstream_done and upstream_setup and (success_setup == 0):\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n        if new_state is not None:\n            if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n                past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n                if not past_depends_met:\n                    yield self._failing_status(reason='Task should be skipped but the past depends are not met')\n                    return\n            changed = ti.set_state(new_state, session)\n        if changed:\n            dep_context.have_changed_ti_states = True\n        if trigger_rule == TR.ONE_SUCCESS:\n            if success <= 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task success, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ONE_FAILED:\n            if not failed and (not upstream_failed):\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task failure, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ONE_DONE:\n            if success + failed <= 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}'requires at least one upstream task failure or successbut none were failed or success. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_SUCCESS:\n            num_failures = upstream - success\n            if ti.map_index > -1:\n                num_failures -= removed\n            if num_failures > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_FAILED:\n            num_success = upstream - failed - upstream_failed\n            if ti.map_index > -1:\n                num_success -= removed\n            if num_success > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have failed, but found {num_success} non-failure(s). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_DONE:\n            if not upstream_done:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.NONE_FAILED or trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n            num_failures = upstream - success - skipped\n            if ti.map_index > -1:\n                num_failures -= removed\n            if num_failures > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded or been skipped, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.NONE_SKIPPED:\n            if not upstream_done or skipped > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to not have been skipped, but found {skipped} task(s) skipped. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_SKIPPED:\n            num_non_skipped = upstream - skipped\n            if num_non_skipped > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have been skipped, but found {num_non_skipped} task(s) in non skipped state. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n            if not upstream_done:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n            elif upstream_setup is None:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' cannot have mapped tasks as upstream. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n            elif upstream_setup and (not success_setup):\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires at least one upstream setup task be successful, but found {upstream_setup - success_setup} task(s) that were not. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        else:\n            yield self._failing_status(reason=f\"No strategy to evaluate trigger rule '{trigger_rule}'.\")\n    if not ti.task.is_teardown:\n        relevant_setups = {t.task_id: t for t in ti.task.get_upstreams_only_setups()}\n        if relevant_setups:\n            for (status, changed) in _evaluate_setup_constraint(relevant_setups=relevant_setups):\n                yield status\n                if not status.passed and changed:\n                    return\n    yield from _evaluate_direct_relatives()",
            "def _evaluate_trigger_rule(self, *, ti: TaskInstance, dep_context: DepContext, session: Session) -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n        :param ti: Task instance to evaluate the trigger rule of.\\n        :param dep_context: The current dependency context.\\n        :param session: Database session.\\n        \"\n    from airflow.models.abstractoperator import NotMapped\n    from airflow.models.expandinput import NotFullyPopulated\n    from airflow.models.operator import needs_expansion\n    from airflow.models.taskinstance import TaskInstance\n\n    @functools.lru_cache\n    def _get_expanded_ti_count() -> int:\n        \"\"\"Get how many tis the current task is supposed to be expanded into.\n\n            This extra closure allows us to query the database only when needed,\n            and at most once.\n            \"\"\"\n        return ti.task.get_mapped_ti_count(ti.run_id, session=session)\n\n    @functools.lru_cache\n    def _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:\n        \"\"\"Get the given task's map indexes relevant to the current ti.\n\n            This extra closure allows us to query the database only when needed,\n            and at most once for each task (instead of once for each expanded\n            task instance of the same task).\n            \"\"\"\n        if TYPE_CHECKING:\n            assert isinstance(ti.task.dag, DAG)\n        try:\n            expanded_ti_count = _get_expanded_ti_count()\n        except (NotFullyPopulated, NotMapped):\n            return None\n        return ti.get_relevant_upstream_map_indexes(upstream=ti.task.dag.task_dict[upstream_id], ti_count=expanded_ti_count, session=session)\n\n    def _is_relevant_upstream(upstream: TaskInstance, relevant_ids: set[str] | KeysView[str]) -> bool:\n        \"\"\"\n            Whether a task instance is a \"relevant upstream\" of the current task.\n\n            This will return false if upstream.task_id is not in relevant_ids,\n            or if both of the following are true:\n                1. upstream.task_id in relevant_ids is True\n                2. ti is in a mapped task group and upstream has a map index\n                  that ti does not depend on.\n            \"\"\"\n        if upstream.task_id not in relevant_ids:\n            return False\n        if ti.task.get_closest_mapped_task_group() is None:\n            return True\n        if upstream.map_index < 0:\n            return True\n        relevant = _get_relevant_upstream_map_indexes(upstream_id=upstream.task_id)\n        if relevant is None:\n            return True\n        if relevant == upstream.map_index:\n            return True\n        if isinstance(relevant, collections.abc.Container) and upstream.map_index in relevant:\n            return True\n        return False\n\n    def _iter_upstream_conditions(relevant_tasks: dict) -> Iterator[ColumnOperators]:\n        from airflow.models.taskinstance import TaskInstance\n        if ti.task.get_closest_mapped_task_group() is None:\n            yield TaskInstance.task_id.in_(relevant_tasks.keys())\n            return\n        for upstream_id in relevant_tasks:\n            map_indexes = _get_relevant_upstream_map_indexes(upstream_id)\n            if map_indexes is None:\n                yield (TaskInstance.task_id == upstream_id)\n                continue\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index < 0)\n            if isinstance(map_indexes, range) and map_indexes.step == 1:\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index >= map_indexes.start, TaskInstance.map_index < map_indexes.stop)\n            elif isinstance(map_indexes, collections.abc.Container):\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index.in_(map_indexes))\n            else:\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index == map_indexes)\n\n    def _evaluate_setup_constraint(*, relevant_setups) -> Iterator[tuple[TIDepStatus, bool]]:\n        \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n            :param ti: Task instance to evaluate the trigger rule of.\n            :param dep_context: The current dependency context.\n            :param session: Database session.\n            \"\"\"\n        task = ti.task\n        indirect_setups = {k: v for (k, v) in relevant_setups.items() if k not in task.upstream_task_ids}\n        finished_upstream_tis = (x for x in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=x, relevant_ids=indirect_setups.keys()))\n        upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n        success = upstream_states.success\n        skipped = upstream_states.skipped\n        failed = upstream_states.failed\n        upstream_failed = upstream_states.upstream_failed\n        removed = upstream_states.removed\n        if not any((needs_expansion(t) for t in indirect_setups.values())):\n            upstream = len(indirect_setups)\n        else:\n            task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=indirect_setups))).group_by(TaskInstance.task_id)).all()\n            upstream = sum((count for (_, count) in task_id_counts))\n        new_state = None\n        changed = False\n        if dep_context.flag_upstream_failed:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif removed and success and (ti.map_index > -1):\n                if ti.map_index >= success:\n                    new_state = TaskInstanceState.REMOVED\n        if new_state is not None:\n            if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n                past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n                if not past_depends_met:\n                    yield (self._failing_status(reason='Task should be skipped but the past depends are not met'), changed)\n                    return\n            changed = ti.set_state(new_state, session)\n        if changed:\n            dep_context.have_changed_ti_states = True\n        non_successes = upstream - success\n        if ti.map_index > -1:\n            non_successes -= removed\n        if non_successes > 0:\n            yield (self._failing_status(reason=f'All setup tasks must complete successfully. Relevant setups: {relevant_setups}: upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}'), changed)\n\n    def _evaluate_direct_relatives() -> Iterator[TIDepStatus]:\n        \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n            :param ti: Task instance to evaluate the trigger rule of.\n            :param dep_context: The current dependency context.\n            :param session: Database session.\n            \"\"\"\n        task = ti.task\n        upstream_tasks = {t.task_id: t for t in task.upstream_list}\n        trigger_rule = task.trigger_rule\n        finished_upstream_tis = (finished_ti for finished_ti in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=finished_ti, relevant_ids=ti.task.upstream_task_ids))\n        upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n        success = upstream_states.success\n        skipped = upstream_states.skipped\n        failed = upstream_states.failed\n        upstream_failed = upstream_states.upstream_failed\n        removed = upstream_states.removed\n        done = upstream_states.done\n        success_setup = upstream_states.success_setup\n        skipped_setup = upstream_states.skipped_setup\n        if not any((needs_expansion(t) for t in upstream_tasks.values())):\n            upstream = len(upstream_tasks)\n            upstream_setup = sum((1 for x in upstream_tasks.values() if x.is_setup))\n        else:\n            task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=upstream_tasks))).group_by(TaskInstance.task_id)).all()\n            upstream = sum((count for (_, count) in task_id_counts))\n            upstream_setup = sum((c for (t, c) in task_id_counts if upstream_tasks[t].is_setup))\n        upstream_done = done >= upstream\n        changed = False\n        new_state = None\n        if dep_context.flag_upstream_failed:\n            if trigger_rule == TR.ALL_SUCCESS:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n                elif skipped:\n                    new_state = TaskInstanceState.SKIPPED\n                elif removed and success and (ti.map_index > -1):\n                    if ti.map_index >= success:\n                        new_state = TaskInstanceState.REMOVED\n            elif trigger_rule == TR.ALL_FAILED:\n                if success or skipped:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ONE_SUCCESS:\n                if upstream_done and done == skipped:\n                    new_state = TaskInstanceState.SKIPPED\n                elif upstream_done and success <= 0:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif trigger_rule == TR.ONE_FAILED:\n                if upstream_done and (not (failed or upstream_failed)):\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ONE_DONE:\n                if upstream_done and (not (failed or success)):\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.NONE_FAILED:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n                elif skipped == upstream:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.NONE_SKIPPED:\n                if skipped:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ALL_SKIPPED:\n                if success or failed or upstream_failed:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n                if upstream_done and upstream_setup and (skipped_setup >= upstream_setup):\n                    new_state = TaskInstanceState.SKIPPED\n                elif upstream_done and upstream_setup and (success_setup == 0):\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n        if new_state is not None:\n            if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n                past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n                if not past_depends_met:\n                    yield self._failing_status(reason='Task should be skipped but the past depends are not met')\n                    return\n            changed = ti.set_state(new_state, session)\n        if changed:\n            dep_context.have_changed_ti_states = True\n        if trigger_rule == TR.ONE_SUCCESS:\n            if success <= 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task success, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ONE_FAILED:\n            if not failed and (not upstream_failed):\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task failure, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ONE_DONE:\n            if success + failed <= 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}'requires at least one upstream task failure or successbut none were failed or success. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_SUCCESS:\n            num_failures = upstream - success\n            if ti.map_index > -1:\n                num_failures -= removed\n            if num_failures > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_FAILED:\n            num_success = upstream - failed - upstream_failed\n            if ti.map_index > -1:\n                num_success -= removed\n            if num_success > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have failed, but found {num_success} non-failure(s). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_DONE:\n            if not upstream_done:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.NONE_FAILED or trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n            num_failures = upstream - success - skipped\n            if ti.map_index > -1:\n                num_failures -= removed\n            if num_failures > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded or been skipped, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.NONE_SKIPPED:\n            if not upstream_done or skipped > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to not have been skipped, but found {skipped} task(s) skipped. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_SKIPPED:\n            num_non_skipped = upstream - skipped\n            if num_non_skipped > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have been skipped, but found {num_non_skipped} task(s) in non skipped state. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n            if not upstream_done:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n            elif upstream_setup is None:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' cannot have mapped tasks as upstream. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n            elif upstream_setup and (not success_setup):\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires at least one upstream setup task be successful, but found {upstream_setup - success_setup} task(s) that were not. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        else:\n            yield self._failing_status(reason=f\"No strategy to evaluate trigger rule '{trigger_rule}'.\")\n    if not ti.task.is_teardown:\n        relevant_setups = {t.task_id: t for t in ti.task.get_upstreams_only_setups()}\n        if relevant_setups:\n            for (status, changed) in _evaluate_setup_constraint(relevant_setups=relevant_setups):\n                yield status\n                if not status.passed and changed:\n                    return\n    yield from _evaluate_direct_relatives()",
            "def _evaluate_trigger_rule(self, *, ti: TaskInstance, dep_context: DepContext, session: Session) -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n        :param ti: Task instance to evaluate the trigger rule of.\\n        :param dep_context: The current dependency context.\\n        :param session: Database session.\\n        \"\n    from airflow.models.abstractoperator import NotMapped\n    from airflow.models.expandinput import NotFullyPopulated\n    from airflow.models.operator import needs_expansion\n    from airflow.models.taskinstance import TaskInstance\n\n    @functools.lru_cache\n    def _get_expanded_ti_count() -> int:\n        \"\"\"Get how many tis the current task is supposed to be expanded into.\n\n            This extra closure allows us to query the database only when needed,\n            and at most once.\n            \"\"\"\n        return ti.task.get_mapped_ti_count(ti.run_id, session=session)\n\n    @functools.lru_cache\n    def _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:\n        \"\"\"Get the given task's map indexes relevant to the current ti.\n\n            This extra closure allows us to query the database only when needed,\n            and at most once for each task (instead of once for each expanded\n            task instance of the same task).\n            \"\"\"\n        if TYPE_CHECKING:\n            assert isinstance(ti.task.dag, DAG)\n        try:\n            expanded_ti_count = _get_expanded_ti_count()\n        except (NotFullyPopulated, NotMapped):\n            return None\n        return ti.get_relevant_upstream_map_indexes(upstream=ti.task.dag.task_dict[upstream_id], ti_count=expanded_ti_count, session=session)\n\n    def _is_relevant_upstream(upstream: TaskInstance, relevant_ids: set[str] | KeysView[str]) -> bool:\n        \"\"\"\n            Whether a task instance is a \"relevant upstream\" of the current task.\n\n            This will return false if upstream.task_id is not in relevant_ids,\n            or if both of the following are true:\n                1. upstream.task_id in relevant_ids is True\n                2. ti is in a mapped task group and upstream has a map index\n                  that ti does not depend on.\n            \"\"\"\n        if upstream.task_id not in relevant_ids:\n            return False\n        if ti.task.get_closest_mapped_task_group() is None:\n            return True\n        if upstream.map_index < 0:\n            return True\n        relevant = _get_relevant_upstream_map_indexes(upstream_id=upstream.task_id)\n        if relevant is None:\n            return True\n        if relevant == upstream.map_index:\n            return True\n        if isinstance(relevant, collections.abc.Container) and upstream.map_index in relevant:\n            return True\n        return False\n\n    def _iter_upstream_conditions(relevant_tasks: dict) -> Iterator[ColumnOperators]:\n        from airflow.models.taskinstance import TaskInstance\n        if ti.task.get_closest_mapped_task_group() is None:\n            yield TaskInstance.task_id.in_(relevant_tasks.keys())\n            return\n        for upstream_id in relevant_tasks:\n            map_indexes = _get_relevant_upstream_map_indexes(upstream_id)\n            if map_indexes is None:\n                yield (TaskInstance.task_id == upstream_id)\n                continue\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index < 0)\n            if isinstance(map_indexes, range) and map_indexes.step == 1:\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index >= map_indexes.start, TaskInstance.map_index < map_indexes.stop)\n            elif isinstance(map_indexes, collections.abc.Container):\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index.in_(map_indexes))\n            else:\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index == map_indexes)\n\n    def _evaluate_setup_constraint(*, relevant_setups) -> Iterator[tuple[TIDepStatus, bool]]:\n        \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n            :param ti: Task instance to evaluate the trigger rule of.\n            :param dep_context: The current dependency context.\n            :param session: Database session.\n            \"\"\"\n        task = ti.task\n        indirect_setups = {k: v for (k, v) in relevant_setups.items() if k not in task.upstream_task_ids}\n        finished_upstream_tis = (x for x in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=x, relevant_ids=indirect_setups.keys()))\n        upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n        success = upstream_states.success\n        skipped = upstream_states.skipped\n        failed = upstream_states.failed\n        upstream_failed = upstream_states.upstream_failed\n        removed = upstream_states.removed\n        if not any((needs_expansion(t) for t in indirect_setups.values())):\n            upstream = len(indirect_setups)\n        else:\n            task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=indirect_setups))).group_by(TaskInstance.task_id)).all()\n            upstream = sum((count for (_, count) in task_id_counts))\n        new_state = None\n        changed = False\n        if dep_context.flag_upstream_failed:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif removed and success and (ti.map_index > -1):\n                if ti.map_index >= success:\n                    new_state = TaskInstanceState.REMOVED\n        if new_state is not None:\n            if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n                past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n                if not past_depends_met:\n                    yield (self._failing_status(reason='Task should be skipped but the past depends are not met'), changed)\n                    return\n            changed = ti.set_state(new_state, session)\n        if changed:\n            dep_context.have_changed_ti_states = True\n        non_successes = upstream - success\n        if ti.map_index > -1:\n            non_successes -= removed\n        if non_successes > 0:\n            yield (self._failing_status(reason=f'All setup tasks must complete successfully. Relevant setups: {relevant_setups}: upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}'), changed)\n\n    def _evaluate_direct_relatives() -> Iterator[TIDepStatus]:\n        \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n            :param ti: Task instance to evaluate the trigger rule of.\n            :param dep_context: The current dependency context.\n            :param session: Database session.\n            \"\"\"\n        task = ti.task\n        upstream_tasks = {t.task_id: t for t in task.upstream_list}\n        trigger_rule = task.trigger_rule\n        finished_upstream_tis = (finished_ti for finished_ti in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=finished_ti, relevant_ids=ti.task.upstream_task_ids))\n        upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n        success = upstream_states.success\n        skipped = upstream_states.skipped\n        failed = upstream_states.failed\n        upstream_failed = upstream_states.upstream_failed\n        removed = upstream_states.removed\n        done = upstream_states.done\n        success_setup = upstream_states.success_setup\n        skipped_setup = upstream_states.skipped_setup\n        if not any((needs_expansion(t) for t in upstream_tasks.values())):\n            upstream = len(upstream_tasks)\n            upstream_setup = sum((1 for x in upstream_tasks.values() if x.is_setup))\n        else:\n            task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=upstream_tasks))).group_by(TaskInstance.task_id)).all()\n            upstream = sum((count for (_, count) in task_id_counts))\n            upstream_setup = sum((c for (t, c) in task_id_counts if upstream_tasks[t].is_setup))\n        upstream_done = done >= upstream\n        changed = False\n        new_state = None\n        if dep_context.flag_upstream_failed:\n            if trigger_rule == TR.ALL_SUCCESS:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n                elif skipped:\n                    new_state = TaskInstanceState.SKIPPED\n                elif removed and success and (ti.map_index > -1):\n                    if ti.map_index >= success:\n                        new_state = TaskInstanceState.REMOVED\n            elif trigger_rule == TR.ALL_FAILED:\n                if success or skipped:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ONE_SUCCESS:\n                if upstream_done and done == skipped:\n                    new_state = TaskInstanceState.SKIPPED\n                elif upstream_done and success <= 0:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif trigger_rule == TR.ONE_FAILED:\n                if upstream_done and (not (failed or upstream_failed)):\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ONE_DONE:\n                if upstream_done and (not (failed or success)):\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.NONE_FAILED:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n                elif skipped == upstream:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.NONE_SKIPPED:\n                if skipped:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ALL_SKIPPED:\n                if success or failed or upstream_failed:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n                if upstream_done and upstream_setup and (skipped_setup >= upstream_setup):\n                    new_state = TaskInstanceState.SKIPPED\n                elif upstream_done and upstream_setup and (success_setup == 0):\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n        if new_state is not None:\n            if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n                past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n                if not past_depends_met:\n                    yield self._failing_status(reason='Task should be skipped but the past depends are not met')\n                    return\n            changed = ti.set_state(new_state, session)\n        if changed:\n            dep_context.have_changed_ti_states = True\n        if trigger_rule == TR.ONE_SUCCESS:\n            if success <= 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task success, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ONE_FAILED:\n            if not failed and (not upstream_failed):\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task failure, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ONE_DONE:\n            if success + failed <= 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}'requires at least one upstream task failure or successbut none were failed or success. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_SUCCESS:\n            num_failures = upstream - success\n            if ti.map_index > -1:\n                num_failures -= removed\n            if num_failures > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_FAILED:\n            num_success = upstream - failed - upstream_failed\n            if ti.map_index > -1:\n                num_success -= removed\n            if num_success > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have failed, but found {num_success} non-failure(s). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_DONE:\n            if not upstream_done:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.NONE_FAILED or trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n            num_failures = upstream - success - skipped\n            if ti.map_index > -1:\n                num_failures -= removed\n            if num_failures > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded or been skipped, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.NONE_SKIPPED:\n            if not upstream_done or skipped > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to not have been skipped, but found {skipped} task(s) skipped. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_SKIPPED:\n            num_non_skipped = upstream - skipped\n            if num_non_skipped > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have been skipped, but found {num_non_skipped} task(s) in non skipped state. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n            if not upstream_done:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n            elif upstream_setup is None:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' cannot have mapped tasks as upstream. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n            elif upstream_setup and (not success_setup):\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires at least one upstream setup task be successful, but found {upstream_setup - success_setup} task(s) that were not. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        else:\n            yield self._failing_status(reason=f\"No strategy to evaluate trigger rule '{trigger_rule}'.\")\n    if not ti.task.is_teardown:\n        relevant_setups = {t.task_id: t for t in ti.task.get_upstreams_only_setups()}\n        if relevant_setups:\n            for (status, changed) in _evaluate_setup_constraint(relevant_setups=relevant_setups):\n                yield status\n                if not status.passed and changed:\n                    return\n    yield from _evaluate_direct_relatives()",
            "def _evaluate_trigger_rule(self, *, ti: TaskInstance, dep_context: DepContext, session: Session) -> Iterator[TIDepStatus]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Evaluate whether ``ti``'s trigger rule was met.\\n\\n        :param ti: Task instance to evaluate the trigger rule of.\\n        :param dep_context: The current dependency context.\\n        :param session: Database session.\\n        \"\n    from airflow.models.abstractoperator import NotMapped\n    from airflow.models.expandinput import NotFullyPopulated\n    from airflow.models.operator import needs_expansion\n    from airflow.models.taskinstance import TaskInstance\n\n    @functools.lru_cache\n    def _get_expanded_ti_count() -> int:\n        \"\"\"Get how many tis the current task is supposed to be expanded into.\n\n            This extra closure allows us to query the database only when needed,\n            and at most once.\n            \"\"\"\n        return ti.task.get_mapped_ti_count(ti.run_id, session=session)\n\n    @functools.lru_cache\n    def _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:\n        \"\"\"Get the given task's map indexes relevant to the current ti.\n\n            This extra closure allows us to query the database only when needed,\n            and at most once for each task (instead of once for each expanded\n            task instance of the same task).\n            \"\"\"\n        if TYPE_CHECKING:\n            assert isinstance(ti.task.dag, DAG)\n        try:\n            expanded_ti_count = _get_expanded_ti_count()\n        except (NotFullyPopulated, NotMapped):\n            return None\n        return ti.get_relevant_upstream_map_indexes(upstream=ti.task.dag.task_dict[upstream_id], ti_count=expanded_ti_count, session=session)\n\n    def _is_relevant_upstream(upstream: TaskInstance, relevant_ids: set[str] | KeysView[str]) -> bool:\n        \"\"\"\n            Whether a task instance is a \"relevant upstream\" of the current task.\n\n            This will return false if upstream.task_id is not in relevant_ids,\n            or if both of the following are true:\n                1. upstream.task_id in relevant_ids is True\n                2. ti is in a mapped task group and upstream has a map index\n                  that ti does not depend on.\n            \"\"\"\n        if upstream.task_id not in relevant_ids:\n            return False\n        if ti.task.get_closest_mapped_task_group() is None:\n            return True\n        if upstream.map_index < 0:\n            return True\n        relevant = _get_relevant_upstream_map_indexes(upstream_id=upstream.task_id)\n        if relevant is None:\n            return True\n        if relevant == upstream.map_index:\n            return True\n        if isinstance(relevant, collections.abc.Container) and upstream.map_index in relevant:\n            return True\n        return False\n\n    def _iter_upstream_conditions(relevant_tasks: dict) -> Iterator[ColumnOperators]:\n        from airflow.models.taskinstance import TaskInstance\n        if ti.task.get_closest_mapped_task_group() is None:\n            yield TaskInstance.task_id.in_(relevant_tasks.keys())\n            return\n        for upstream_id in relevant_tasks:\n            map_indexes = _get_relevant_upstream_map_indexes(upstream_id)\n            if map_indexes is None:\n                yield (TaskInstance.task_id == upstream_id)\n                continue\n            yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index < 0)\n            if isinstance(map_indexes, range) and map_indexes.step == 1:\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index >= map_indexes.start, TaskInstance.map_index < map_indexes.stop)\n            elif isinstance(map_indexes, collections.abc.Container):\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index.in_(map_indexes))\n            else:\n                yield and_(TaskInstance.task_id == upstream_id, TaskInstance.map_index == map_indexes)\n\n    def _evaluate_setup_constraint(*, relevant_setups) -> Iterator[tuple[TIDepStatus, bool]]:\n        \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n            :param ti: Task instance to evaluate the trigger rule of.\n            :param dep_context: The current dependency context.\n            :param session: Database session.\n            \"\"\"\n        task = ti.task\n        indirect_setups = {k: v for (k, v) in relevant_setups.items() if k not in task.upstream_task_ids}\n        finished_upstream_tis = (x for x in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=x, relevant_ids=indirect_setups.keys()))\n        upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n        success = upstream_states.success\n        skipped = upstream_states.skipped\n        failed = upstream_states.failed\n        upstream_failed = upstream_states.upstream_failed\n        removed = upstream_states.removed\n        if not any((needs_expansion(t) for t in indirect_setups.values())):\n            upstream = len(indirect_setups)\n        else:\n            task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=indirect_setups))).group_by(TaskInstance.task_id)).all()\n            upstream = sum((count for (_, count) in task_id_counts))\n        new_state = None\n        changed = False\n        if dep_context.flag_upstream_failed:\n            if upstream_failed or failed:\n                new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif skipped:\n                new_state = TaskInstanceState.SKIPPED\n            elif removed and success and (ti.map_index > -1):\n                if ti.map_index >= success:\n                    new_state = TaskInstanceState.REMOVED\n        if new_state is not None:\n            if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n                past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n                if not past_depends_met:\n                    yield (self._failing_status(reason='Task should be skipped but the past depends are not met'), changed)\n                    return\n            changed = ti.set_state(new_state, session)\n        if changed:\n            dep_context.have_changed_ti_states = True\n        non_successes = upstream - success\n        if ti.map_index > -1:\n            non_successes -= removed\n        if non_successes > 0:\n            yield (self._failing_status(reason=f'All setup tasks must complete successfully. Relevant setups: {relevant_setups}: upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}'), changed)\n\n    def _evaluate_direct_relatives() -> Iterator[TIDepStatus]:\n        \"\"\"Evaluate whether ``ti``'s trigger rule was met.\n\n            :param ti: Task instance to evaluate the trigger rule of.\n            :param dep_context: The current dependency context.\n            :param session: Database session.\n            \"\"\"\n        task = ti.task\n        upstream_tasks = {t.task_id: t for t in task.upstream_list}\n        trigger_rule = task.trigger_rule\n        finished_upstream_tis = (finished_ti for finished_ti in dep_context.ensure_finished_tis(ti.get_dagrun(session), session) if _is_relevant_upstream(upstream=finished_ti, relevant_ids=ti.task.upstream_task_ids))\n        upstream_states = _UpstreamTIStates.calculate(finished_upstream_tis)\n        success = upstream_states.success\n        skipped = upstream_states.skipped\n        failed = upstream_states.failed\n        upstream_failed = upstream_states.upstream_failed\n        removed = upstream_states.removed\n        done = upstream_states.done\n        success_setup = upstream_states.success_setup\n        skipped_setup = upstream_states.skipped_setup\n        if not any((needs_expansion(t) for t in upstream_tasks.values())):\n            upstream = len(upstream_tasks)\n            upstream_setup = sum((1 for x in upstream_tasks.values() if x.is_setup))\n        else:\n            task_id_counts = session.execute(select(TaskInstance.task_id, func.count(TaskInstance.task_id)).where(TaskInstance.dag_id == ti.dag_id, TaskInstance.run_id == ti.run_id).where(or_(*_iter_upstream_conditions(relevant_tasks=upstream_tasks))).group_by(TaskInstance.task_id)).all()\n            upstream = sum((count for (_, count) in task_id_counts))\n            upstream_setup = sum((c for (t, c) in task_id_counts if upstream_tasks[t].is_setup))\n        upstream_done = done >= upstream\n        changed = False\n        new_state = None\n        if dep_context.flag_upstream_failed:\n            if trigger_rule == TR.ALL_SUCCESS:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n                elif skipped:\n                    new_state = TaskInstanceState.SKIPPED\n                elif removed and success and (ti.map_index > -1):\n                    if ti.map_index >= success:\n                        new_state = TaskInstanceState.REMOVED\n            elif trigger_rule == TR.ALL_FAILED:\n                if success or skipped:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ONE_SUCCESS:\n                if upstream_done and done == skipped:\n                    new_state = TaskInstanceState.SKIPPED\n                elif upstream_done and success <= 0:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif trigger_rule == TR.ONE_FAILED:\n                if upstream_done and (not (failed or upstream_failed)):\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ONE_DONE:\n                if upstream_done and (not (failed or success)):\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.NONE_FAILED:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n            elif trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n                if upstream_failed or failed:\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n                elif skipped == upstream:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.NONE_SKIPPED:\n                if skipped:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ALL_SKIPPED:\n                if success or failed or upstream_failed:\n                    new_state = TaskInstanceState.SKIPPED\n            elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n                if upstream_done and upstream_setup and (skipped_setup >= upstream_setup):\n                    new_state = TaskInstanceState.SKIPPED\n                elif upstream_done and upstream_setup and (success_setup == 0):\n                    new_state = TaskInstanceState.UPSTREAM_FAILED\n        if new_state is not None:\n            if new_state == TaskInstanceState.SKIPPED and dep_context.wait_for_past_depends_before_skipping:\n                past_depends_met = ti.xcom_pull(task_ids=ti.task_id, key=PAST_DEPENDS_MET, session=session, default=False)\n                if not past_depends_met:\n                    yield self._failing_status(reason='Task should be skipped but the past depends are not met')\n                    return\n            changed = ti.set_state(new_state, session)\n        if changed:\n            dep_context.have_changed_ti_states = True\n        if trigger_rule == TR.ONE_SUCCESS:\n            if success <= 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task success, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ONE_FAILED:\n            if not failed and (not upstream_failed):\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires one upstream task failure, but none were found. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ONE_DONE:\n            if success + failed <= 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}'requires at least one upstream task failure or successbut none were failed or success. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_SUCCESS:\n            num_failures = upstream - success\n            if ti.map_index > -1:\n                num_failures -= removed\n            if num_failures > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_FAILED:\n            num_success = upstream - failed - upstream_failed\n            if ti.map_index > -1:\n                num_success -= removed\n            if num_success > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have failed, but found {num_success} non-failure(s). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_DONE:\n            if not upstream_done:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.NONE_FAILED or trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:\n            num_failures = upstream - success - skipped\n            if ti.map_index > -1:\n                num_failures -= removed\n            if num_failures > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have succeeded or been skipped, but found {num_failures} non-success(es). upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.NONE_SKIPPED:\n            if not upstream_done or skipped > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to not have been skipped, but found {skipped} task(s) skipped. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_SKIPPED:\n            num_non_skipped = upstream - skipped\n            if num_non_skipped > 0:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have been skipped, but found {num_non_skipped} task(s) in non skipped state. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        elif trigger_rule == TR.ALL_DONE_SETUP_SUCCESS:\n            if not upstream_done:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have completed, but found {len(upstream_tasks) - done} task(s) that were not done. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n            elif upstream_setup is None:\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' cannot have mapped tasks as upstream. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n            elif upstream_setup and (not success_setup):\n                yield self._failing_status(reason=f\"Task's trigger rule '{trigger_rule}' requires at least one upstream setup task be successful, but found {upstream_setup - success_setup} task(s) that were not. upstream_states={upstream_states}, upstream_task_ids={task.upstream_task_ids}\")\n        else:\n            yield self._failing_status(reason=f\"No strategy to evaluate trigger rule '{trigger_rule}'.\")\n    if not ti.task.is_teardown:\n        relevant_setups = {t.task_id: t for t in ti.task.get_upstreams_only_setups()}\n        if relevant_setups:\n            for (status, changed) in _evaluate_setup_constraint(relevant_setups=relevant_setups):\n                yield status\n                if not status.passed and changed:\n                    return\n    yield from _evaluate_direct_relatives()"
        ]
    }
]