[
    {
        "func_name": "save_pic",
        "original": "@utils.async_func\ndef save_pic(x_rec):\n    tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n    tracker.plot()\n    result_sample = x_rec * 0.5 + 0.5\n    result_sample = result_sample.cpu()\n    f = os.path.join(cfg.OUTPUT_DIR, 'sample_%d_%d.jpg' % (lod2batch.current_epoch + 1, lod2batch.iteration // 1000))\n    print('Saved to %s' % f)\n    save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))",
        "mutated": [
            "@utils.async_func\ndef save_pic(x_rec):\n    if False:\n        i = 10\n    tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n    tracker.plot()\n    result_sample = x_rec * 0.5 + 0.5\n    result_sample = result_sample.cpu()\n    f = os.path.join(cfg.OUTPUT_DIR, 'sample_%d_%d.jpg' % (lod2batch.current_epoch + 1, lod2batch.iteration // 1000))\n    print('Saved to %s' % f)\n    save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))",
            "@utils.async_func\ndef save_pic(x_rec):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n    tracker.plot()\n    result_sample = x_rec * 0.5 + 0.5\n    result_sample = result_sample.cpu()\n    f = os.path.join(cfg.OUTPUT_DIR, 'sample_%d_%d.jpg' % (lod2batch.current_epoch + 1, lod2batch.iteration // 1000))\n    print('Saved to %s' % f)\n    save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))",
            "@utils.async_func\ndef save_pic(x_rec):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n    tracker.plot()\n    result_sample = x_rec * 0.5 + 0.5\n    result_sample = result_sample.cpu()\n    f = os.path.join(cfg.OUTPUT_DIR, 'sample_%d_%d.jpg' % (lod2batch.current_epoch + 1, lod2batch.iteration // 1000))\n    print('Saved to %s' % f)\n    save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))",
            "@utils.async_func\ndef save_pic(x_rec):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n    tracker.plot()\n    result_sample = x_rec * 0.5 + 0.5\n    result_sample = result_sample.cpu()\n    f = os.path.join(cfg.OUTPUT_DIR, 'sample_%d_%d.jpg' % (lod2batch.current_epoch + 1, lod2batch.iteration // 1000))\n    print('Saved to %s' % f)\n    save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))",
            "@utils.async_func\ndef save_pic(x_rec):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n    tracker.plot()\n    result_sample = x_rec * 0.5 + 0.5\n    result_sample = result_sample.cpu()\n    f = os.path.join(cfg.OUTPUT_DIR, 'sample_%d_%d.jpg' % (lod2batch.current_epoch + 1, lod2batch.iteration // 1000))\n    print('Saved to %s' % f)\n    save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))"
        ]
    },
    {
        "func_name": "save_sample",
        "original": "def save_sample(lod2batch, tracker, sample, samplez, x, logger, model, cmodel, cfg, encoder_optimizer, decoder_optimizer):\n    os.makedirs('results', exist_ok=True)\n    logger.info('\\n[%d/%d] - ptime: %.2f, %s, blend: %.3f, lr: %.12f,  %.12f, max mem: %f\",' % (lod2batch.current_epoch + 1, cfg.TRAIN.TRAIN_EPOCHS, lod2batch.per_epoch_ptime, str(tracker), lod2batch.get_blend_factor(), encoder_optimizer.param_groups[0]['lr'], decoder_optimizer.param_groups[0]['lr'], torch.cuda.max_memory_allocated() / 1024.0 / 1024.0))\n    with torch.no_grad():\n        model.eval()\n        cmodel.eval()\n        sample = sample[:lod2batch.get_per_GPU_batch_size()]\n        samplez = samplez[:lod2batch.get_per_GPU_batch_size()]\n        needed_resolution = model.decoder.layer_to_resolution[lod2batch.lod]\n        sample_in = sample\n        while sample_in.shape[2] > needed_resolution:\n            sample_in = F.avg_pool2d(sample_in, 2, 2)\n        assert sample_in.shape[2] == needed_resolution\n        blend_factor = lod2batch.get_blend_factor()\n        if lod2batch.in_transition:\n            needed_resolution_prev = model.decoder.layer_to_resolution[lod2batch.lod - 1]\n            sample_in_prev = F.avg_pool2d(sample_in, 2, 2)\n            sample_in_prev_2x = F.interpolate(sample_in_prev, needed_resolution)\n            sample_in = sample_in * blend_factor + sample_in_prev_2x * (1.0 - blend_factor)\n        (Z, _) = model.encode(sample_in, lod2batch.lod, blend_factor)\n        if cfg.MODEL.Z_REGRESSION:\n            Z = model.mapping_f(Z[:, 0])\n        else:\n            Z = Z.repeat(1, model.mapping_f.num_layers, 1)\n        rec1 = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        rec2 = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        Z = model.mapping_f(samplez)\n        g_rec = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        Z = cmodel.mapping_f(samplez)\n        cg_rec = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        resultsample = torch.cat([sample_in, rec1, rec2, g_rec, cg_rec], dim=0)\n\n        @utils.async_func\n        def save_pic(x_rec):\n            tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n            tracker.plot()\n            result_sample = x_rec * 0.5 + 0.5\n            result_sample = result_sample.cpu()\n            f = os.path.join(cfg.OUTPUT_DIR, 'sample_%d_%d.jpg' % (lod2batch.current_epoch + 1, lod2batch.iteration // 1000))\n            print('Saved to %s' % f)\n            save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))\n        save_pic(resultsample)",
        "mutated": [
            "def save_sample(lod2batch, tracker, sample, samplez, x, logger, model, cmodel, cfg, encoder_optimizer, decoder_optimizer):\n    if False:\n        i = 10\n    os.makedirs('results', exist_ok=True)\n    logger.info('\\n[%d/%d] - ptime: %.2f, %s, blend: %.3f, lr: %.12f,  %.12f, max mem: %f\",' % (lod2batch.current_epoch + 1, cfg.TRAIN.TRAIN_EPOCHS, lod2batch.per_epoch_ptime, str(tracker), lod2batch.get_blend_factor(), encoder_optimizer.param_groups[0]['lr'], decoder_optimizer.param_groups[0]['lr'], torch.cuda.max_memory_allocated() / 1024.0 / 1024.0))\n    with torch.no_grad():\n        model.eval()\n        cmodel.eval()\n        sample = sample[:lod2batch.get_per_GPU_batch_size()]\n        samplez = samplez[:lod2batch.get_per_GPU_batch_size()]\n        needed_resolution = model.decoder.layer_to_resolution[lod2batch.lod]\n        sample_in = sample\n        while sample_in.shape[2] > needed_resolution:\n            sample_in = F.avg_pool2d(sample_in, 2, 2)\n        assert sample_in.shape[2] == needed_resolution\n        blend_factor = lod2batch.get_blend_factor()\n        if lod2batch.in_transition:\n            needed_resolution_prev = model.decoder.layer_to_resolution[lod2batch.lod - 1]\n            sample_in_prev = F.avg_pool2d(sample_in, 2, 2)\n            sample_in_prev_2x = F.interpolate(sample_in_prev, needed_resolution)\n            sample_in = sample_in * blend_factor + sample_in_prev_2x * (1.0 - blend_factor)\n        (Z, _) = model.encode(sample_in, lod2batch.lod, blend_factor)\n        if cfg.MODEL.Z_REGRESSION:\n            Z = model.mapping_f(Z[:, 0])\n        else:\n            Z = Z.repeat(1, model.mapping_f.num_layers, 1)\n        rec1 = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        rec2 = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        Z = model.mapping_f(samplez)\n        g_rec = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        Z = cmodel.mapping_f(samplez)\n        cg_rec = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        resultsample = torch.cat([sample_in, rec1, rec2, g_rec, cg_rec], dim=0)\n\n        @utils.async_func\n        def save_pic(x_rec):\n            tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n            tracker.plot()\n            result_sample = x_rec * 0.5 + 0.5\n            result_sample = result_sample.cpu()\n            f = os.path.join(cfg.OUTPUT_DIR, 'sample_%d_%d.jpg' % (lod2batch.current_epoch + 1, lod2batch.iteration // 1000))\n            print('Saved to %s' % f)\n            save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))\n        save_pic(resultsample)",
            "def save_sample(lod2batch, tracker, sample, samplez, x, logger, model, cmodel, cfg, encoder_optimizer, decoder_optimizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    os.makedirs('results', exist_ok=True)\n    logger.info('\\n[%d/%d] - ptime: %.2f, %s, blend: %.3f, lr: %.12f,  %.12f, max mem: %f\",' % (lod2batch.current_epoch + 1, cfg.TRAIN.TRAIN_EPOCHS, lod2batch.per_epoch_ptime, str(tracker), lod2batch.get_blend_factor(), encoder_optimizer.param_groups[0]['lr'], decoder_optimizer.param_groups[0]['lr'], torch.cuda.max_memory_allocated() / 1024.0 / 1024.0))\n    with torch.no_grad():\n        model.eval()\n        cmodel.eval()\n        sample = sample[:lod2batch.get_per_GPU_batch_size()]\n        samplez = samplez[:lod2batch.get_per_GPU_batch_size()]\n        needed_resolution = model.decoder.layer_to_resolution[lod2batch.lod]\n        sample_in = sample\n        while sample_in.shape[2] > needed_resolution:\n            sample_in = F.avg_pool2d(sample_in, 2, 2)\n        assert sample_in.shape[2] == needed_resolution\n        blend_factor = lod2batch.get_blend_factor()\n        if lod2batch.in_transition:\n            needed_resolution_prev = model.decoder.layer_to_resolution[lod2batch.lod - 1]\n            sample_in_prev = F.avg_pool2d(sample_in, 2, 2)\n            sample_in_prev_2x = F.interpolate(sample_in_prev, needed_resolution)\n            sample_in = sample_in * blend_factor + sample_in_prev_2x * (1.0 - blend_factor)\n        (Z, _) = model.encode(sample_in, lod2batch.lod, blend_factor)\n        if cfg.MODEL.Z_REGRESSION:\n            Z = model.mapping_f(Z[:, 0])\n        else:\n            Z = Z.repeat(1, model.mapping_f.num_layers, 1)\n        rec1 = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        rec2 = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        Z = model.mapping_f(samplez)\n        g_rec = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        Z = cmodel.mapping_f(samplez)\n        cg_rec = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        resultsample = torch.cat([sample_in, rec1, rec2, g_rec, cg_rec], dim=0)\n\n        @utils.async_func\n        def save_pic(x_rec):\n            tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n            tracker.plot()\n            result_sample = x_rec * 0.5 + 0.5\n            result_sample = result_sample.cpu()\n            f = os.path.join(cfg.OUTPUT_DIR, 'sample_%d_%d.jpg' % (lod2batch.current_epoch + 1, lod2batch.iteration // 1000))\n            print('Saved to %s' % f)\n            save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))\n        save_pic(resultsample)",
            "def save_sample(lod2batch, tracker, sample, samplez, x, logger, model, cmodel, cfg, encoder_optimizer, decoder_optimizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    os.makedirs('results', exist_ok=True)\n    logger.info('\\n[%d/%d] - ptime: %.2f, %s, blend: %.3f, lr: %.12f,  %.12f, max mem: %f\",' % (lod2batch.current_epoch + 1, cfg.TRAIN.TRAIN_EPOCHS, lod2batch.per_epoch_ptime, str(tracker), lod2batch.get_blend_factor(), encoder_optimizer.param_groups[0]['lr'], decoder_optimizer.param_groups[0]['lr'], torch.cuda.max_memory_allocated() / 1024.0 / 1024.0))\n    with torch.no_grad():\n        model.eval()\n        cmodel.eval()\n        sample = sample[:lod2batch.get_per_GPU_batch_size()]\n        samplez = samplez[:lod2batch.get_per_GPU_batch_size()]\n        needed_resolution = model.decoder.layer_to_resolution[lod2batch.lod]\n        sample_in = sample\n        while sample_in.shape[2] > needed_resolution:\n            sample_in = F.avg_pool2d(sample_in, 2, 2)\n        assert sample_in.shape[2] == needed_resolution\n        blend_factor = lod2batch.get_blend_factor()\n        if lod2batch.in_transition:\n            needed_resolution_prev = model.decoder.layer_to_resolution[lod2batch.lod - 1]\n            sample_in_prev = F.avg_pool2d(sample_in, 2, 2)\n            sample_in_prev_2x = F.interpolate(sample_in_prev, needed_resolution)\n            sample_in = sample_in * blend_factor + sample_in_prev_2x * (1.0 - blend_factor)\n        (Z, _) = model.encode(sample_in, lod2batch.lod, blend_factor)\n        if cfg.MODEL.Z_REGRESSION:\n            Z = model.mapping_f(Z[:, 0])\n        else:\n            Z = Z.repeat(1, model.mapping_f.num_layers, 1)\n        rec1 = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        rec2 = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        Z = model.mapping_f(samplez)\n        g_rec = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        Z = cmodel.mapping_f(samplez)\n        cg_rec = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        resultsample = torch.cat([sample_in, rec1, rec2, g_rec, cg_rec], dim=0)\n\n        @utils.async_func\n        def save_pic(x_rec):\n            tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n            tracker.plot()\n            result_sample = x_rec * 0.5 + 0.5\n            result_sample = result_sample.cpu()\n            f = os.path.join(cfg.OUTPUT_DIR, 'sample_%d_%d.jpg' % (lod2batch.current_epoch + 1, lod2batch.iteration // 1000))\n            print('Saved to %s' % f)\n            save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))\n        save_pic(resultsample)",
            "def save_sample(lod2batch, tracker, sample, samplez, x, logger, model, cmodel, cfg, encoder_optimizer, decoder_optimizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    os.makedirs('results', exist_ok=True)\n    logger.info('\\n[%d/%d] - ptime: %.2f, %s, blend: %.3f, lr: %.12f,  %.12f, max mem: %f\",' % (lod2batch.current_epoch + 1, cfg.TRAIN.TRAIN_EPOCHS, lod2batch.per_epoch_ptime, str(tracker), lod2batch.get_blend_factor(), encoder_optimizer.param_groups[0]['lr'], decoder_optimizer.param_groups[0]['lr'], torch.cuda.max_memory_allocated() / 1024.0 / 1024.0))\n    with torch.no_grad():\n        model.eval()\n        cmodel.eval()\n        sample = sample[:lod2batch.get_per_GPU_batch_size()]\n        samplez = samplez[:lod2batch.get_per_GPU_batch_size()]\n        needed_resolution = model.decoder.layer_to_resolution[lod2batch.lod]\n        sample_in = sample\n        while sample_in.shape[2] > needed_resolution:\n            sample_in = F.avg_pool2d(sample_in, 2, 2)\n        assert sample_in.shape[2] == needed_resolution\n        blend_factor = lod2batch.get_blend_factor()\n        if lod2batch.in_transition:\n            needed_resolution_prev = model.decoder.layer_to_resolution[lod2batch.lod - 1]\n            sample_in_prev = F.avg_pool2d(sample_in, 2, 2)\n            sample_in_prev_2x = F.interpolate(sample_in_prev, needed_resolution)\n            sample_in = sample_in * blend_factor + sample_in_prev_2x * (1.0 - blend_factor)\n        (Z, _) = model.encode(sample_in, lod2batch.lod, blend_factor)\n        if cfg.MODEL.Z_REGRESSION:\n            Z = model.mapping_f(Z[:, 0])\n        else:\n            Z = Z.repeat(1, model.mapping_f.num_layers, 1)\n        rec1 = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        rec2 = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        Z = model.mapping_f(samplez)\n        g_rec = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        Z = cmodel.mapping_f(samplez)\n        cg_rec = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        resultsample = torch.cat([sample_in, rec1, rec2, g_rec, cg_rec], dim=0)\n\n        @utils.async_func\n        def save_pic(x_rec):\n            tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n            tracker.plot()\n            result_sample = x_rec * 0.5 + 0.5\n            result_sample = result_sample.cpu()\n            f = os.path.join(cfg.OUTPUT_DIR, 'sample_%d_%d.jpg' % (lod2batch.current_epoch + 1, lod2batch.iteration // 1000))\n            print('Saved to %s' % f)\n            save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))\n        save_pic(resultsample)",
            "def save_sample(lod2batch, tracker, sample, samplez, x, logger, model, cmodel, cfg, encoder_optimizer, decoder_optimizer):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    os.makedirs('results', exist_ok=True)\n    logger.info('\\n[%d/%d] - ptime: %.2f, %s, blend: %.3f, lr: %.12f,  %.12f, max mem: %f\",' % (lod2batch.current_epoch + 1, cfg.TRAIN.TRAIN_EPOCHS, lod2batch.per_epoch_ptime, str(tracker), lod2batch.get_blend_factor(), encoder_optimizer.param_groups[0]['lr'], decoder_optimizer.param_groups[0]['lr'], torch.cuda.max_memory_allocated() / 1024.0 / 1024.0))\n    with torch.no_grad():\n        model.eval()\n        cmodel.eval()\n        sample = sample[:lod2batch.get_per_GPU_batch_size()]\n        samplez = samplez[:lod2batch.get_per_GPU_batch_size()]\n        needed_resolution = model.decoder.layer_to_resolution[lod2batch.lod]\n        sample_in = sample\n        while sample_in.shape[2] > needed_resolution:\n            sample_in = F.avg_pool2d(sample_in, 2, 2)\n        assert sample_in.shape[2] == needed_resolution\n        blend_factor = lod2batch.get_blend_factor()\n        if lod2batch.in_transition:\n            needed_resolution_prev = model.decoder.layer_to_resolution[lod2batch.lod - 1]\n            sample_in_prev = F.avg_pool2d(sample_in, 2, 2)\n            sample_in_prev_2x = F.interpolate(sample_in_prev, needed_resolution)\n            sample_in = sample_in * blend_factor + sample_in_prev_2x * (1.0 - blend_factor)\n        (Z, _) = model.encode(sample_in, lod2batch.lod, blend_factor)\n        if cfg.MODEL.Z_REGRESSION:\n            Z = model.mapping_f(Z[:, 0])\n        else:\n            Z = Z.repeat(1, model.mapping_f.num_layers, 1)\n        rec1 = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        rec2 = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        Z = model.mapping_f(samplez)\n        g_rec = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        Z = cmodel.mapping_f(samplez)\n        cg_rec = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        resultsample = torch.cat([sample_in, rec1, rec2, g_rec, cg_rec], dim=0)\n\n        @utils.async_func\n        def save_pic(x_rec):\n            tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n            tracker.plot()\n            result_sample = x_rec * 0.5 + 0.5\n            result_sample = result_sample.cpu()\n            f = os.path.join(cfg.OUTPUT_DIR, 'sample_%d_%d.jpg' % (lod2batch.current_epoch + 1, lod2batch.iteration // 1000))\n            print('Saved to %s' % f)\n            save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))\n        save_pic(resultsample)"
        ]
    },
    {
        "func_name": "train",
        "original": "def train(cfg, logger, local_rank, world_size, distributed):\n    torch.cuda.set_device(local_rank)\n    model = Model(startf=cfg.MODEL.START_CHANNEL_COUNT, layer_count=cfg.MODEL.LAYER_COUNT, maxf=cfg.MODEL.MAX_CHANNEL_COUNT, latent_size=cfg.MODEL.LATENT_SPACE_SIZE, dlatent_avg_beta=cfg.MODEL.DLATENT_AVG_BETA, style_mixing_prob=cfg.MODEL.STYLE_MIXING_PROB, mapping_layers=cfg.MODEL.MAPPING_LAYERS, channels=cfg.MODEL.CHANNELS, generator=cfg.MODEL.GENERATOR, encoder=cfg.MODEL.ENCODER, z_regression=cfg.MODEL.Z_REGRESSION)\n    model.cuda(local_rank)\n    model.train()\n    if local_rank == 0:\n        model_s = Model(startf=cfg.MODEL.START_CHANNEL_COUNT, layer_count=cfg.MODEL.LAYER_COUNT, maxf=cfg.MODEL.MAX_CHANNEL_COUNT, latent_size=cfg.MODEL.LATENT_SPACE_SIZE, truncation_psi=cfg.MODEL.TRUNCATIOM_PSI, truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF, mapping_layers=cfg.MODEL.MAPPING_LAYERS, channels=cfg.MODEL.CHANNELS, generator=cfg.MODEL.GENERATOR, encoder=cfg.MODEL.ENCODER, z_regression=cfg.MODEL.Z_REGRESSION)\n        model_s.cuda(local_rank)\n        model_s.eval()\n        model_s.requires_grad_(False)\n    if distributed:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], broadcast_buffers=False, bucket_cap_mb=25, find_unused_parameters=True)\n        model.device_ids = None\n        decoder = model.module.decoder\n        encoder = model.module.encoder\n        mapping_d = model.module.mapping_d\n        mapping_f = model.module.mapping_f\n        dlatent_avg = model.module.dlatent_avg\n    else:\n        decoder = model.decoder\n        encoder = model.encoder\n        mapping_d = model.mapping_d\n        mapping_f = model.mapping_f\n        dlatent_avg = model.dlatent_avg\n    count_param_override.print = lambda a: logger.info(a)\n    logger.info('Trainable parameters generator:')\n    count_parameters(decoder)\n    logger.info('Trainable parameters discriminator:')\n    count_parameters(encoder)\n    arguments = dict()\n    arguments['iteration'] = 0\n    decoder_optimizer = LREQAdam([{'params': decoder.parameters()}, {'params': mapping_f.parameters()}], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n    encoder_optimizer = LREQAdam([{'params': encoder.parameters()}, {'params': mapping_d.parameters()}], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n    scheduler = ComboMultiStepLR(optimizers={'encoder_optimizer': encoder_optimizer, 'decoder_optimizer': decoder_optimizer}, milestones=cfg.TRAIN.LEARNING_DECAY_STEPS, gamma=cfg.TRAIN.LEARNING_DECAY_RATE, reference_batch_size=32, base_lr=cfg.TRAIN.LEARNING_RATES)\n    model_dict = {'discriminator': encoder, 'generator': decoder, 'mapping_tl': mapping_d, 'mapping_fl': mapping_f, 'dlatent_avg': dlatent_avg}\n    if local_rank == 0:\n        model_dict['discriminator_s'] = model_s.encoder\n        model_dict['generator_s'] = model_s.decoder\n        model_dict['mapping_tl_s'] = model_s.mapping_d\n        model_dict['mapping_fl_s'] = model_s.mapping_f\n    tracker = LossTracker(cfg.OUTPUT_DIR)\n    checkpointer = Checkpointer(cfg, model_dict, {'encoder_optimizer': encoder_optimizer, 'decoder_optimizer': decoder_optimizer, 'scheduler': scheduler, 'tracker': tracker}, logger=logger, save=local_rank == 0)\n    extra_checkpoint_data = checkpointer.load()\n    logger.info('Starting from epoch: %d' % scheduler.start_epoch())\n    arguments.update(extra_checkpoint_data)\n    layer_to_resolution = decoder.layer_to_resolution\n    dataset = TFRecordsDataset(cfg, logger, rank=local_rank, world_size=world_size, buffer_size_mb=1024, channels=cfg.MODEL.CHANNELS)\n    rnd = np.random.RandomState(3456)\n    latents = rnd.randn(32, cfg.MODEL.LATENT_SPACE_SIZE)\n    samplez = torch.tensor(latents).float().cuda()\n    lod2batch = lod_driver.LODDriver(cfg, logger, world_size, dataset_size=len(dataset) * world_size)\n    if cfg.DATASET.SAMPLES_PATH != 'no_path':\n        path = cfg.DATASET.SAMPLES_PATH\n        src = []\n        with torch.no_grad():\n            for filename in list(os.listdir(path))[:32]:\n                img = np.asarray(Image.open(os.path.join(path, filename)))\n                if img.shape[2] == 4:\n                    img = img[:, :, :3]\n                im = img.transpose((2, 0, 1))\n                x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.0\n                if x.shape[0] == 4:\n                    x = x[:3]\n                src.append(x)\n            sample = torch.stack(src)\n    else:\n        dataset.reset(cfg.DATASET.MAX_RESOLUTION_LEVEL, 32)\n        sample = next(make_dataloader(cfg, logger, dataset, 32, local_rank))\n        sample = sample / 127.5 - 1.0\n    lod2batch.set_epoch(scheduler.start_epoch(), [encoder_optimizer, decoder_optimizer])\n    for epoch in range(scheduler.start_epoch(), cfg.TRAIN.TRAIN_EPOCHS):\n        model.train()\n        lod2batch.set_epoch(epoch, [encoder_optimizer, decoder_optimizer])\n        logger.info('Batch size: %d, Batch size per GPU: %d, LOD: %d - %dx%d, blend: %.3f, dataset size: %d' % (lod2batch.get_batch_size(), lod2batch.get_per_GPU_batch_size(), lod2batch.lod, 2 ** lod2batch.get_lod_power2(), 2 ** lod2batch.get_lod_power2(), lod2batch.get_blend_factor(), len(dataset) * world_size))\n        dataset.reset(lod2batch.get_lod_power2(), lod2batch.get_per_GPU_batch_size())\n        batches = make_dataloader(cfg, logger, dataset, lod2batch.get_per_GPU_batch_size(), local_rank)\n        scheduler.set_batch_size(lod2batch.get_batch_size(), lod2batch.lod)\n        model.train()\n        need_permute = False\n        epoch_start_time = time.time()\n        i = 0\n        for x_orig in tqdm(batches):\n            i += 1\n            with torch.no_grad():\n                if x_orig.shape[0] != lod2batch.get_per_GPU_batch_size():\n                    continue\n                if need_permute:\n                    x_orig = x_orig.permute(0, 3, 1, 2)\n                x_orig = x_orig / 127.5 - 1.0\n                blend_factor = lod2batch.get_blend_factor()\n                needed_resolution = layer_to_resolution[lod2batch.lod]\n                x = x_orig\n                if lod2batch.in_transition:\n                    needed_resolution_prev = layer_to_resolution[lod2batch.lod - 1]\n                    x_prev = F.avg_pool2d(x_orig, 2, 2)\n                    x_prev_2x = F.interpolate(x_prev, needed_resolution)\n                    x = x * blend_factor + x_prev_2x * (1.0 - blend_factor)\n            x.requires_grad = True\n            encoder_optimizer.zero_grad()\n            loss_d = model(x, lod2batch.lod, blend_factor, d_train=True, ae=False)\n            tracker.update(dict(loss_d=loss_d))\n            loss_d.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.zero_grad()\n            loss_g = model(x, lod2batch.lod, blend_factor, d_train=False, ae=False)\n            tracker.update(dict(loss_g=loss_g))\n            loss_g.backward()\n            decoder_optimizer.step()\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            lae = model(x, lod2batch.lod, blend_factor, d_train=True, ae=True)\n            tracker.update(dict(lae=lae))\n            lae.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n            if local_rank == 0:\n                betta = 0.5 ** (lod2batch.get_batch_size() / (10 * 1000.0))\n                model_s.lerp(model, betta)\n            epoch_end_time = time.time()\n            per_epoch_ptime = epoch_end_time - epoch_start_time\n            lod_for_saving_model = lod2batch.lod\n            lod2batch.step()\n            if local_rank == 0:\n                if lod2batch.is_time_to_save():\n                    checkpointer.save('model_tmp_intermediate_lod%d' % lod_for_saving_model)\n                if lod2batch.is_time_to_report():\n                    save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, model.module if hasattr(model, 'module') else model, cfg, encoder_optimizer, decoder_optimizer)\n        scheduler.step()\n        if local_rank == 0:\n            checkpointer.save('model_tmp_lod%d' % lod_for_saving_model)\n            save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, model.module if hasattr(model, 'module') else model, cfg, encoder_optimizer, decoder_optimizer)\n    logger.info('Training finish!... save training results')\n    if local_rank == 0:\n        checkpointer.save('model_final').wait()",
        "mutated": [
            "def train(cfg, logger, local_rank, world_size, distributed):\n    if False:\n        i = 10\n    torch.cuda.set_device(local_rank)\n    model = Model(startf=cfg.MODEL.START_CHANNEL_COUNT, layer_count=cfg.MODEL.LAYER_COUNT, maxf=cfg.MODEL.MAX_CHANNEL_COUNT, latent_size=cfg.MODEL.LATENT_SPACE_SIZE, dlatent_avg_beta=cfg.MODEL.DLATENT_AVG_BETA, style_mixing_prob=cfg.MODEL.STYLE_MIXING_PROB, mapping_layers=cfg.MODEL.MAPPING_LAYERS, channels=cfg.MODEL.CHANNELS, generator=cfg.MODEL.GENERATOR, encoder=cfg.MODEL.ENCODER, z_regression=cfg.MODEL.Z_REGRESSION)\n    model.cuda(local_rank)\n    model.train()\n    if local_rank == 0:\n        model_s = Model(startf=cfg.MODEL.START_CHANNEL_COUNT, layer_count=cfg.MODEL.LAYER_COUNT, maxf=cfg.MODEL.MAX_CHANNEL_COUNT, latent_size=cfg.MODEL.LATENT_SPACE_SIZE, truncation_psi=cfg.MODEL.TRUNCATIOM_PSI, truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF, mapping_layers=cfg.MODEL.MAPPING_LAYERS, channels=cfg.MODEL.CHANNELS, generator=cfg.MODEL.GENERATOR, encoder=cfg.MODEL.ENCODER, z_regression=cfg.MODEL.Z_REGRESSION)\n        model_s.cuda(local_rank)\n        model_s.eval()\n        model_s.requires_grad_(False)\n    if distributed:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], broadcast_buffers=False, bucket_cap_mb=25, find_unused_parameters=True)\n        model.device_ids = None\n        decoder = model.module.decoder\n        encoder = model.module.encoder\n        mapping_d = model.module.mapping_d\n        mapping_f = model.module.mapping_f\n        dlatent_avg = model.module.dlatent_avg\n    else:\n        decoder = model.decoder\n        encoder = model.encoder\n        mapping_d = model.mapping_d\n        mapping_f = model.mapping_f\n        dlatent_avg = model.dlatent_avg\n    count_param_override.print = lambda a: logger.info(a)\n    logger.info('Trainable parameters generator:')\n    count_parameters(decoder)\n    logger.info('Trainable parameters discriminator:')\n    count_parameters(encoder)\n    arguments = dict()\n    arguments['iteration'] = 0\n    decoder_optimizer = LREQAdam([{'params': decoder.parameters()}, {'params': mapping_f.parameters()}], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n    encoder_optimizer = LREQAdam([{'params': encoder.parameters()}, {'params': mapping_d.parameters()}], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n    scheduler = ComboMultiStepLR(optimizers={'encoder_optimizer': encoder_optimizer, 'decoder_optimizer': decoder_optimizer}, milestones=cfg.TRAIN.LEARNING_DECAY_STEPS, gamma=cfg.TRAIN.LEARNING_DECAY_RATE, reference_batch_size=32, base_lr=cfg.TRAIN.LEARNING_RATES)\n    model_dict = {'discriminator': encoder, 'generator': decoder, 'mapping_tl': mapping_d, 'mapping_fl': mapping_f, 'dlatent_avg': dlatent_avg}\n    if local_rank == 0:\n        model_dict['discriminator_s'] = model_s.encoder\n        model_dict['generator_s'] = model_s.decoder\n        model_dict['mapping_tl_s'] = model_s.mapping_d\n        model_dict['mapping_fl_s'] = model_s.mapping_f\n    tracker = LossTracker(cfg.OUTPUT_DIR)\n    checkpointer = Checkpointer(cfg, model_dict, {'encoder_optimizer': encoder_optimizer, 'decoder_optimizer': decoder_optimizer, 'scheduler': scheduler, 'tracker': tracker}, logger=logger, save=local_rank == 0)\n    extra_checkpoint_data = checkpointer.load()\n    logger.info('Starting from epoch: %d' % scheduler.start_epoch())\n    arguments.update(extra_checkpoint_data)\n    layer_to_resolution = decoder.layer_to_resolution\n    dataset = TFRecordsDataset(cfg, logger, rank=local_rank, world_size=world_size, buffer_size_mb=1024, channels=cfg.MODEL.CHANNELS)\n    rnd = np.random.RandomState(3456)\n    latents = rnd.randn(32, cfg.MODEL.LATENT_SPACE_SIZE)\n    samplez = torch.tensor(latents).float().cuda()\n    lod2batch = lod_driver.LODDriver(cfg, logger, world_size, dataset_size=len(dataset) * world_size)\n    if cfg.DATASET.SAMPLES_PATH != 'no_path':\n        path = cfg.DATASET.SAMPLES_PATH\n        src = []\n        with torch.no_grad():\n            for filename in list(os.listdir(path))[:32]:\n                img = np.asarray(Image.open(os.path.join(path, filename)))\n                if img.shape[2] == 4:\n                    img = img[:, :, :3]\n                im = img.transpose((2, 0, 1))\n                x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.0\n                if x.shape[0] == 4:\n                    x = x[:3]\n                src.append(x)\n            sample = torch.stack(src)\n    else:\n        dataset.reset(cfg.DATASET.MAX_RESOLUTION_LEVEL, 32)\n        sample = next(make_dataloader(cfg, logger, dataset, 32, local_rank))\n        sample = sample / 127.5 - 1.0\n    lod2batch.set_epoch(scheduler.start_epoch(), [encoder_optimizer, decoder_optimizer])\n    for epoch in range(scheduler.start_epoch(), cfg.TRAIN.TRAIN_EPOCHS):\n        model.train()\n        lod2batch.set_epoch(epoch, [encoder_optimizer, decoder_optimizer])\n        logger.info('Batch size: %d, Batch size per GPU: %d, LOD: %d - %dx%d, blend: %.3f, dataset size: %d' % (lod2batch.get_batch_size(), lod2batch.get_per_GPU_batch_size(), lod2batch.lod, 2 ** lod2batch.get_lod_power2(), 2 ** lod2batch.get_lod_power2(), lod2batch.get_blend_factor(), len(dataset) * world_size))\n        dataset.reset(lod2batch.get_lod_power2(), lod2batch.get_per_GPU_batch_size())\n        batches = make_dataloader(cfg, logger, dataset, lod2batch.get_per_GPU_batch_size(), local_rank)\n        scheduler.set_batch_size(lod2batch.get_batch_size(), lod2batch.lod)\n        model.train()\n        need_permute = False\n        epoch_start_time = time.time()\n        i = 0\n        for x_orig in tqdm(batches):\n            i += 1\n            with torch.no_grad():\n                if x_orig.shape[0] != lod2batch.get_per_GPU_batch_size():\n                    continue\n                if need_permute:\n                    x_orig = x_orig.permute(0, 3, 1, 2)\n                x_orig = x_orig / 127.5 - 1.0\n                blend_factor = lod2batch.get_blend_factor()\n                needed_resolution = layer_to_resolution[lod2batch.lod]\n                x = x_orig\n                if lod2batch.in_transition:\n                    needed_resolution_prev = layer_to_resolution[lod2batch.lod - 1]\n                    x_prev = F.avg_pool2d(x_orig, 2, 2)\n                    x_prev_2x = F.interpolate(x_prev, needed_resolution)\n                    x = x * blend_factor + x_prev_2x * (1.0 - blend_factor)\n            x.requires_grad = True\n            encoder_optimizer.zero_grad()\n            loss_d = model(x, lod2batch.lod, blend_factor, d_train=True, ae=False)\n            tracker.update(dict(loss_d=loss_d))\n            loss_d.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.zero_grad()\n            loss_g = model(x, lod2batch.lod, blend_factor, d_train=False, ae=False)\n            tracker.update(dict(loss_g=loss_g))\n            loss_g.backward()\n            decoder_optimizer.step()\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            lae = model(x, lod2batch.lod, blend_factor, d_train=True, ae=True)\n            tracker.update(dict(lae=lae))\n            lae.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n            if local_rank == 0:\n                betta = 0.5 ** (lod2batch.get_batch_size() / (10 * 1000.0))\n                model_s.lerp(model, betta)\n            epoch_end_time = time.time()\n            per_epoch_ptime = epoch_end_time - epoch_start_time\n            lod_for_saving_model = lod2batch.lod\n            lod2batch.step()\n            if local_rank == 0:\n                if lod2batch.is_time_to_save():\n                    checkpointer.save('model_tmp_intermediate_lod%d' % lod_for_saving_model)\n                if lod2batch.is_time_to_report():\n                    save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, model.module if hasattr(model, 'module') else model, cfg, encoder_optimizer, decoder_optimizer)\n        scheduler.step()\n        if local_rank == 0:\n            checkpointer.save('model_tmp_lod%d' % lod_for_saving_model)\n            save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, model.module if hasattr(model, 'module') else model, cfg, encoder_optimizer, decoder_optimizer)\n    logger.info('Training finish!... save training results')\n    if local_rank == 0:\n        checkpointer.save('model_final').wait()",
            "def train(cfg, logger, local_rank, world_size, distributed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    torch.cuda.set_device(local_rank)\n    model = Model(startf=cfg.MODEL.START_CHANNEL_COUNT, layer_count=cfg.MODEL.LAYER_COUNT, maxf=cfg.MODEL.MAX_CHANNEL_COUNT, latent_size=cfg.MODEL.LATENT_SPACE_SIZE, dlatent_avg_beta=cfg.MODEL.DLATENT_AVG_BETA, style_mixing_prob=cfg.MODEL.STYLE_MIXING_PROB, mapping_layers=cfg.MODEL.MAPPING_LAYERS, channels=cfg.MODEL.CHANNELS, generator=cfg.MODEL.GENERATOR, encoder=cfg.MODEL.ENCODER, z_regression=cfg.MODEL.Z_REGRESSION)\n    model.cuda(local_rank)\n    model.train()\n    if local_rank == 0:\n        model_s = Model(startf=cfg.MODEL.START_CHANNEL_COUNT, layer_count=cfg.MODEL.LAYER_COUNT, maxf=cfg.MODEL.MAX_CHANNEL_COUNT, latent_size=cfg.MODEL.LATENT_SPACE_SIZE, truncation_psi=cfg.MODEL.TRUNCATIOM_PSI, truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF, mapping_layers=cfg.MODEL.MAPPING_LAYERS, channels=cfg.MODEL.CHANNELS, generator=cfg.MODEL.GENERATOR, encoder=cfg.MODEL.ENCODER, z_regression=cfg.MODEL.Z_REGRESSION)\n        model_s.cuda(local_rank)\n        model_s.eval()\n        model_s.requires_grad_(False)\n    if distributed:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], broadcast_buffers=False, bucket_cap_mb=25, find_unused_parameters=True)\n        model.device_ids = None\n        decoder = model.module.decoder\n        encoder = model.module.encoder\n        mapping_d = model.module.mapping_d\n        mapping_f = model.module.mapping_f\n        dlatent_avg = model.module.dlatent_avg\n    else:\n        decoder = model.decoder\n        encoder = model.encoder\n        mapping_d = model.mapping_d\n        mapping_f = model.mapping_f\n        dlatent_avg = model.dlatent_avg\n    count_param_override.print = lambda a: logger.info(a)\n    logger.info('Trainable parameters generator:')\n    count_parameters(decoder)\n    logger.info('Trainable parameters discriminator:')\n    count_parameters(encoder)\n    arguments = dict()\n    arguments['iteration'] = 0\n    decoder_optimizer = LREQAdam([{'params': decoder.parameters()}, {'params': mapping_f.parameters()}], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n    encoder_optimizer = LREQAdam([{'params': encoder.parameters()}, {'params': mapping_d.parameters()}], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n    scheduler = ComboMultiStepLR(optimizers={'encoder_optimizer': encoder_optimizer, 'decoder_optimizer': decoder_optimizer}, milestones=cfg.TRAIN.LEARNING_DECAY_STEPS, gamma=cfg.TRAIN.LEARNING_DECAY_RATE, reference_batch_size=32, base_lr=cfg.TRAIN.LEARNING_RATES)\n    model_dict = {'discriminator': encoder, 'generator': decoder, 'mapping_tl': mapping_d, 'mapping_fl': mapping_f, 'dlatent_avg': dlatent_avg}\n    if local_rank == 0:\n        model_dict['discriminator_s'] = model_s.encoder\n        model_dict['generator_s'] = model_s.decoder\n        model_dict['mapping_tl_s'] = model_s.mapping_d\n        model_dict['mapping_fl_s'] = model_s.mapping_f\n    tracker = LossTracker(cfg.OUTPUT_DIR)\n    checkpointer = Checkpointer(cfg, model_dict, {'encoder_optimizer': encoder_optimizer, 'decoder_optimizer': decoder_optimizer, 'scheduler': scheduler, 'tracker': tracker}, logger=logger, save=local_rank == 0)\n    extra_checkpoint_data = checkpointer.load()\n    logger.info('Starting from epoch: %d' % scheduler.start_epoch())\n    arguments.update(extra_checkpoint_data)\n    layer_to_resolution = decoder.layer_to_resolution\n    dataset = TFRecordsDataset(cfg, logger, rank=local_rank, world_size=world_size, buffer_size_mb=1024, channels=cfg.MODEL.CHANNELS)\n    rnd = np.random.RandomState(3456)\n    latents = rnd.randn(32, cfg.MODEL.LATENT_SPACE_SIZE)\n    samplez = torch.tensor(latents).float().cuda()\n    lod2batch = lod_driver.LODDriver(cfg, logger, world_size, dataset_size=len(dataset) * world_size)\n    if cfg.DATASET.SAMPLES_PATH != 'no_path':\n        path = cfg.DATASET.SAMPLES_PATH\n        src = []\n        with torch.no_grad():\n            for filename in list(os.listdir(path))[:32]:\n                img = np.asarray(Image.open(os.path.join(path, filename)))\n                if img.shape[2] == 4:\n                    img = img[:, :, :3]\n                im = img.transpose((2, 0, 1))\n                x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.0\n                if x.shape[0] == 4:\n                    x = x[:3]\n                src.append(x)\n            sample = torch.stack(src)\n    else:\n        dataset.reset(cfg.DATASET.MAX_RESOLUTION_LEVEL, 32)\n        sample = next(make_dataloader(cfg, logger, dataset, 32, local_rank))\n        sample = sample / 127.5 - 1.0\n    lod2batch.set_epoch(scheduler.start_epoch(), [encoder_optimizer, decoder_optimizer])\n    for epoch in range(scheduler.start_epoch(), cfg.TRAIN.TRAIN_EPOCHS):\n        model.train()\n        lod2batch.set_epoch(epoch, [encoder_optimizer, decoder_optimizer])\n        logger.info('Batch size: %d, Batch size per GPU: %d, LOD: %d - %dx%d, blend: %.3f, dataset size: %d' % (lod2batch.get_batch_size(), lod2batch.get_per_GPU_batch_size(), lod2batch.lod, 2 ** lod2batch.get_lod_power2(), 2 ** lod2batch.get_lod_power2(), lod2batch.get_blend_factor(), len(dataset) * world_size))\n        dataset.reset(lod2batch.get_lod_power2(), lod2batch.get_per_GPU_batch_size())\n        batches = make_dataloader(cfg, logger, dataset, lod2batch.get_per_GPU_batch_size(), local_rank)\n        scheduler.set_batch_size(lod2batch.get_batch_size(), lod2batch.lod)\n        model.train()\n        need_permute = False\n        epoch_start_time = time.time()\n        i = 0\n        for x_orig in tqdm(batches):\n            i += 1\n            with torch.no_grad():\n                if x_orig.shape[0] != lod2batch.get_per_GPU_batch_size():\n                    continue\n                if need_permute:\n                    x_orig = x_orig.permute(0, 3, 1, 2)\n                x_orig = x_orig / 127.5 - 1.0\n                blend_factor = lod2batch.get_blend_factor()\n                needed_resolution = layer_to_resolution[lod2batch.lod]\n                x = x_orig\n                if lod2batch.in_transition:\n                    needed_resolution_prev = layer_to_resolution[lod2batch.lod - 1]\n                    x_prev = F.avg_pool2d(x_orig, 2, 2)\n                    x_prev_2x = F.interpolate(x_prev, needed_resolution)\n                    x = x * blend_factor + x_prev_2x * (1.0 - blend_factor)\n            x.requires_grad = True\n            encoder_optimizer.zero_grad()\n            loss_d = model(x, lod2batch.lod, blend_factor, d_train=True, ae=False)\n            tracker.update(dict(loss_d=loss_d))\n            loss_d.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.zero_grad()\n            loss_g = model(x, lod2batch.lod, blend_factor, d_train=False, ae=False)\n            tracker.update(dict(loss_g=loss_g))\n            loss_g.backward()\n            decoder_optimizer.step()\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            lae = model(x, lod2batch.lod, blend_factor, d_train=True, ae=True)\n            tracker.update(dict(lae=lae))\n            lae.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n            if local_rank == 0:\n                betta = 0.5 ** (lod2batch.get_batch_size() / (10 * 1000.0))\n                model_s.lerp(model, betta)\n            epoch_end_time = time.time()\n            per_epoch_ptime = epoch_end_time - epoch_start_time\n            lod_for_saving_model = lod2batch.lod\n            lod2batch.step()\n            if local_rank == 0:\n                if lod2batch.is_time_to_save():\n                    checkpointer.save('model_tmp_intermediate_lod%d' % lod_for_saving_model)\n                if lod2batch.is_time_to_report():\n                    save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, model.module if hasattr(model, 'module') else model, cfg, encoder_optimizer, decoder_optimizer)\n        scheduler.step()\n        if local_rank == 0:\n            checkpointer.save('model_tmp_lod%d' % lod_for_saving_model)\n            save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, model.module if hasattr(model, 'module') else model, cfg, encoder_optimizer, decoder_optimizer)\n    logger.info('Training finish!... save training results')\n    if local_rank == 0:\n        checkpointer.save('model_final').wait()",
            "def train(cfg, logger, local_rank, world_size, distributed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    torch.cuda.set_device(local_rank)\n    model = Model(startf=cfg.MODEL.START_CHANNEL_COUNT, layer_count=cfg.MODEL.LAYER_COUNT, maxf=cfg.MODEL.MAX_CHANNEL_COUNT, latent_size=cfg.MODEL.LATENT_SPACE_SIZE, dlatent_avg_beta=cfg.MODEL.DLATENT_AVG_BETA, style_mixing_prob=cfg.MODEL.STYLE_MIXING_PROB, mapping_layers=cfg.MODEL.MAPPING_LAYERS, channels=cfg.MODEL.CHANNELS, generator=cfg.MODEL.GENERATOR, encoder=cfg.MODEL.ENCODER, z_regression=cfg.MODEL.Z_REGRESSION)\n    model.cuda(local_rank)\n    model.train()\n    if local_rank == 0:\n        model_s = Model(startf=cfg.MODEL.START_CHANNEL_COUNT, layer_count=cfg.MODEL.LAYER_COUNT, maxf=cfg.MODEL.MAX_CHANNEL_COUNT, latent_size=cfg.MODEL.LATENT_SPACE_SIZE, truncation_psi=cfg.MODEL.TRUNCATIOM_PSI, truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF, mapping_layers=cfg.MODEL.MAPPING_LAYERS, channels=cfg.MODEL.CHANNELS, generator=cfg.MODEL.GENERATOR, encoder=cfg.MODEL.ENCODER, z_regression=cfg.MODEL.Z_REGRESSION)\n        model_s.cuda(local_rank)\n        model_s.eval()\n        model_s.requires_grad_(False)\n    if distributed:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], broadcast_buffers=False, bucket_cap_mb=25, find_unused_parameters=True)\n        model.device_ids = None\n        decoder = model.module.decoder\n        encoder = model.module.encoder\n        mapping_d = model.module.mapping_d\n        mapping_f = model.module.mapping_f\n        dlatent_avg = model.module.dlatent_avg\n    else:\n        decoder = model.decoder\n        encoder = model.encoder\n        mapping_d = model.mapping_d\n        mapping_f = model.mapping_f\n        dlatent_avg = model.dlatent_avg\n    count_param_override.print = lambda a: logger.info(a)\n    logger.info('Trainable parameters generator:')\n    count_parameters(decoder)\n    logger.info('Trainable parameters discriminator:')\n    count_parameters(encoder)\n    arguments = dict()\n    arguments['iteration'] = 0\n    decoder_optimizer = LREQAdam([{'params': decoder.parameters()}, {'params': mapping_f.parameters()}], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n    encoder_optimizer = LREQAdam([{'params': encoder.parameters()}, {'params': mapping_d.parameters()}], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n    scheduler = ComboMultiStepLR(optimizers={'encoder_optimizer': encoder_optimizer, 'decoder_optimizer': decoder_optimizer}, milestones=cfg.TRAIN.LEARNING_DECAY_STEPS, gamma=cfg.TRAIN.LEARNING_DECAY_RATE, reference_batch_size=32, base_lr=cfg.TRAIN.LEARNING_RATES)\n    model_dict = {'discriminator': encoder, 'generator': decoder, 'mapping_tl': mapping_d, 'mapping_fl': mapping_f, 'dlatent_avg': dlatent_avg}\n    if local_rank == 0:\n        model_dict['discriminator_s'] = model_s.encoder\n        model_dict['generator_s'] = model_s.decoder\n        model_dict['mapping_tl_s'] = model_s.mapping_d\n        model_dict['mapping_fl_s'] = model_s.mapping_f\n    tracker = LossTracker(cfg.OUTPUT_DIR)\n    checkpointer = Checkpointer(cfg, model_dict, {'encoder_optimizer': encoder_optimizer, 'decoder_optimizer': decoder_optimizer, 'scheduler': scheduler, 'tracker': tracker}, logger=logger, save=local_rank == 0)\n    extra_checkpoint_data = checkpointer.load()\n    logger.info('Starting from epoch: %d' % scheduler.start_epoch())\n    arguments.update(extra_checkpoint_data)\n    layer_to_resolution = decoder.layer_to_resolution\n    dataset = TFRecordsDataset(cfg, logger, rank=local_rank, world_size=world_size, buffer_size_mb=1024, channels=cfg.MODEL.CHANNELS)\n    rnd = np.random.RandomState(3456)\n    latents = rnd.randn(32, cfg.MODEL.LATENT_SPACE_SIZE)\n    samplez = torch.tensor(latents).float().cuda()\n    lod2batch = lod_driver.LODDriver(cfg, logger, world_size, dataset_size=len(dataset) * world_size)\n    if cfg.DATASET.SAMPLES_PATH != 'no_path':\n        path = cfg.DATASET.SAMPLES_PATH\n        src = []\n        with torch.no_grad():\n            for filename in list(os.listdir(path))[:32]:\n                img = np.asarray(Image.open(os.path.join(path, filename)))\n                if img.shape[2] == 4:\n                    img = img[:, :, :3]\n                im = img.transpose((2, 0, 1))\n                x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.0\n                if x.shape[0] == 4:\n                    x = x[:3]\n                src.append(x)\n            sample = torch.stack(src)\n    else:\n        dataset.reset(cfg.DATASET.MAX_RESOLUTION_LEVEL, 32)\n        sample = next(make_dataloader(cfg, logger, dataset, 32, local_rank))\n        sample = sample / 127.5 - 1.0\n    lod2batch.set_epoch(scheduler.start_epoch(), [encoder_optimizer, decoder_optimizer])\n    for epoch in range(scheduler.start_epoch(), cfg.TRAIN.TRAIN_EPOCHS):\n        model.train()\n        lod2batch.set_epoch(epoch, [encoder_optimizer, decoder_optimizer])\n        logger.info('Batch size: %d, Batch size per GPU: %d, LOD: %d - %dx%d, blend: %.3f, dataset size: %d' % (lod2batch.get_batch_size(), lod2batch.get_per_GPU_batch_size(), lod2batch.lod, 2 ** lod2batch.get_lod_power2(), 2 ** lod2batch.get_lod_power2(), lod2batch.get_blend_factor(), len(dataset) * world_size))\n        dataset.reset(lod2batch.get_lod_power2(), lod2batch.get_per_GPU_batch_size())\n        batches = make_dataloader(cfg, logger, dataset, lod2batch.get_per_GPU_batch_size(), local_rank)\n        scheduler.set_batch_size(lod2batch.get_batch_size(), lod2batch.lod)\n        model.train()\n        need_permute = False\n        epoch_start_time = time.time()\n        i = 0\n        for x_orig in tqdm(batches):\n            i += 1\n            with torch.no_grad():\n                if x_orig.shape[0] != lod2batch.get_per_GPU_batch_size():\n                    continue\n                if need_permute:\n                    x_orig = x_orig.permute(0, 3, 1, 2)\n                x_orig = x_orig / 127.5 - 1.0\n                blend_factor = lod2batch.get_blend_factor()\n                needed_resolution = layer_to_resolution[lod2batch.lod]\n                x = x_orig\n                if lod2batch.in_transition:\n                    needed_resolution_prev = layer_to_resolution[lod2batch.lod - 1]\n                    x_prev = F.avg_pool2d(x_orig, 2, 2)\n                    x_prev_2x = F.interpolate(x_prev, needed_resolution)\n                    x = x * blend_factor + x_prev_2x * (1.0 - blend_factor)\n            x.requires_grad = True\n            encoder_optimizer.zero_grad()\n            loss_d = model(x, lod2batch.lod, blend_factor, d_train=True, ae=False)\n            tracker.update(dict(loss_d=loss_d))\n            loss_d.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.zero_grad()\n            loss_g = model(x, lod2batch.lod, blend_factor, d_train=False, ae=False)\n            tracker.update(dict(loss_g=loss_g))\n            loss_g.backward()\n            decoder_optimizer.step()\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            lae = model(x, lod2batch.lod, blend_factor, d_train=True, ae=True)\n            tracker.update(dict(lae=lae))\n            lae.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n            if local_rank == 0:\n                betta = 0.5 ** (lod2batch.get_batch_size() / (10 * 1000.0))\n                model_s.lerp(model, betta)\n            epoch_end_time = time.time()\n            per_epoch_ptime = epoch_end_time - epoch_start_time\n            lod_for_saving_model = lod2batch.lod\n            lod2batch.step()\n            if local_rank == 0:\n                if lod2batch.is_time_to_save():\n                    checkpointer.save('model_tmp_intermediate_lod%d' % lod_for_saving_model)\n                if lod2batch.is_time_to_report():\n                    save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, model.module if hasattr(model, 'module') else model, cfg, encoder_optimizer, decoder_optimizer)\n        scheduler.step()\n        if local_rank == 0:\n            checkpointer.save('model_tmp_lod%d' % lod_for_saving_model)\n            save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, model.module if hasattr(model, 'module') else model, cfg, encoder_optimizer, decoder_optimizer)\n    logger.info('Training finish!... save training results')\n    if local_rank == 0:\n        checkpointer.save('model_final').wait()",
            "def train(cfg, logger, local_rank, world_size, distributed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    torch.cuda.set_device(local_rank)\n    model = Model(startf=cfg.MODEL.START_CHANNEL_COUNT, layer_count=cfg.MODEL.LAYER_COUNT, maxf=cfg.MODEL.MAX_CHANNEL_COUNT, latent_size=cfg.MODEL.LATENT_SPACE_SIZE, dlatent_avg_beta=cfg.MODEL.DLATENT_AVG_BETA, style_mixing_prob=cfg.MODEL.STYLE_MIXING_PROB, mapping_layers=cfg.MODEL.MAPPING_LAYERS, channels=cfg.MODEL.CHANNELS, generator=cfg.MODEL.GENERATOR, encoder=cfg.MODEL.ENCODER, z_regression=cfg.MODEL.Z_REGRESSION)\n    model.cuda(local_rank)\n    model.train()\n    if local_rank == 0:\n        model_s = Model(startf=cfg.MODEL.START_CHANNEL_COUNT, layer_count=cfg.MODEL.LAYER_COUNT, maxf=cfg.MODEL.MAX_CHANNEL_COUNT, latent_size=cfg.MODEL.LATENT_SPACE_SIZE, truncation_psi=cfg.MODEL.TRUNCATIOM_PSI, truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF, mapping_layers=cfg.MODEL.MAPPING_LAYERS, channels=cfg.MODEL.CHANNELS, generator=cfg.MODEL.GENERATOR, encoder=cfg.MODEL.ENCODER, z_regression=cfg.MODEL.Z_REGRESSION)\n        model_s.cuda(local_rank)\n        model_s.eval()\n        model_s.requires_grad_(False)\n    if distributed:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], broadcast_buffers=False, bucket_cap_mb=25, find_unused_parameters=True)\n        model.device_ids = None\n        decoder = model.module.decoder\n        encoder = model.module.encoder\n        mapping_d = model.module.mapping_d\n        mapping_f = model.module.mapping_f\n        dlatent_avg = model.module.dlatent_avg\n    else:\n        decoder = model.decoder\n        encoder = model.encoder\n        mapping_d = model.mapping_d\n        mapping_f = model.mapping_f\n        dlatent_avg = model.dlatent_avg\n    count_param_override.print = lambda a: logger.info(a)\n    logger.info('Trainable parameters generator:')\n    count_parameters(decoder)\n    logger.info('Trainable parameters discriminator:')\n    count_parameters(encoder)\n    arguments = dict()\n    arguments['iteration'] = 0\n    decoder_optimizer = LREQAdam([{'params': decoder.parameters()}, {'params': mapping_f.parameters()}], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n    encoder_optimizer = LREQAdam([{'params': encoder.parameters()}, {'params': mapping_d.parameters()}], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n    scheduler = ComboMultiStepLR(optimizers={'encoder_optimizer': encoder_optimizer, 'decoder_optimizer': decoder_optimizer}, milestones=cfg.TRAIN.LEARNING_DECAY_STEPS, gamma=cfg.TRAIN.LEARNING_DECAY_RATE, reference_batch_size=32, base_lr=cfg.TRAIN.LEARNING_RATES)\n    model_dict = {'discriminator': encoder, 'generator': decoder, 'mapping_tl': mapping_d, 'mapping_fl': mapping_f, 'dlatent_avg': dlatent_avg}\n    if local_rank == 0:\n        model_dict['discriminator_s'] = model_s.encoder\n        model_dict['generator_s'] = model_s.decoder\n        model_dict['mapping_tl_s'] = model_s.mapping_d\n        model_dict['mapping_fl_s'] = model_s.mapping_f\n    tracker = LossTracker(cfg.OUTPUT_DIR)\n    checkpointer = Checkpointer(cfg, model_dict, {'encoder_optimizer': encoder_optimizer, 'decoder_optimizer': decoder_optimizer, 'scheduler': scheduler, 'tracker': tracker}, logger=logger, save=local_rank == 0)\n    extra_checkpoint_data = checkpointer.load()\n    logger.info('Starting from epoch: %d' % scheduler.start_epoch())\n    arguments.update(extra_checkpoint_data)\n    layer_to_resolution = decoder.layer_to_resolution\n    dataset = TFRecordsDataset(cfg, logger, rank=local_rank, world_size=world_size, buffer_size_mb=1024, channels=cfg.MODEL.CHANNELS)\n    rnd = np.random.RandomState(3456)\n    latents = rnd.randn(32, cfg.MODEL.LATENT_SPACE_SIZE)\n    samplez = torch.tensor(latents).float().cuda()\n    lod2batch = lod_driver.LODDriver(cfg, logger, world_size, dataset_size=len(dataset) * world_size)\n    if cfg.DATASET.SAMPLES_PATH != 'no_path':\n        path = cfg.DATASET.SAMPLES_PATH\n        src = []\n        with torch.no_grad():\n            for filename in list(os.listdir(path))[:32]:\n                img = np.asarray(Image.open(os.path.join(path, filename)))\n                if img.shape[2] == 4:\n                    img = img[:, :, :3]\n                im = img.transpose((2, 0, 1))\n                x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.0\n                if x.shape[0] == 4:\n                    x = x[:3]\n                src.append(x)\n            sample = torch.stack(src)\n    else:\n        dataset.reset(cfg.DATASET.MAX_RESOLUTION_LEVEL, 32)\n        sample = next(make_dataloader(cfg, logger, dataset, 32, local_rank))\n        sample = sample / 127.5 - 1.0\n    lod2batch.set_epoch(scheduler.start_epoch(), [encoder_optimizer, decoder_optimizer])\n    for epoch in range(scheduler.start_epoch(), cfg.TRAIN.TRAIN_EPOCHS):\n        model.train()\n        lod2batch.set_epoch(epoch, [encoder_optimizer, decoder_optimizer])\n        logger.info('Batch size: %d, Batch size per GPU: %d, LOD: %d - %dx%d, blend: %.3f, dataset size: %d' % (lod2batch.get_batch_size(), lod2batch.get_per_GPU_batch_size(), lod2batch.lod, 2 ** lod2batch.get_lod_power2(), 2 ** lod2batch.get_lod_power2(), lod2batch.get_blend_factor(), len(dataset) * world_size))\n        dataset.reset(lod2batch.get_lod_power2(), lod2batch.get_per_GPU_batch_size())\n        batches = make_dataloader(cfg, logger, dataset, lod2batch.get_per_GPU_batch_size(), local_rank)\n        scheduler.set_batch_size(lod2batch.get_batch_size(), lod2batch.lod)\n        model.train()\n        need_permute = False\n        epoch_start_time = time.time()\n        i = 0\n        for x_orig in tqdm(batches):\n            i += 1\n            with torch.no_grad():\n                if x_orig.shape[0] != lod2batch.get_per_GPU_batch_size():\n                    continue\n                if need_permute:\n                    x_orig = x_orig.permute(0, 3, 1, 2)\n                x_orig = x_orig / 127.5 - 1.0\n                blend_factor = lod2batch.get_blend_factor()\n                needed_resolution = layer_to_resolution[lod2batch.lod]\n                x = x_orig\n                if lod2batch.in_transition:\n                    needed_resolution_prev = layer_to_resolution[lod2batch.lod - 1]\n                    x_prev = F.avg_pool2d(x_orig, 2, 2)\n                    x_prev_2x = F.interpolate(x_prev, needed_resolution)\n                    x = x * blend_factor + x_prev_2x * (1.0 - blend_factor)\n            x.requires_grad = True\n            encoder_optimizer.zero_grad()\n            loss_d = model(x, lod2batch.lod, blend_factor, d_train=True, ae=False)\n            tracker.update(dict(loss_d=loss_d))\n            loss_d.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.zero_grad()\n            loss_g = model(x, lod2batch.lod, blend_factor, d_train=False, ae=False)\n            tracker.update(dict(loss_g=loss_g))\n            loss_g.backward()\n            decoder_optimizer.step()\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            lae = model(x, lod2batch.lod, blend_factor, d_train=True, ae=True)\n            tracker.update(dict(lae=lae))\n            lae.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n            if local_rank == 0:\n                betta = 0.5 ** (lod2batch.get_batch_size() / (10 * 1000.0))\n                model_s.lerp(model, betta)\n            epoch_end_time = time.time()\n            per_epoch_ptime = epoch_end_time - epoch_start_time\n            lod_for_saving_model = lod2batch.lod\n            lod2batch.step()\n            if local_rank == 0:\n                if lod2batch.is_time_to_save():\n                    checkpointer.save('model_tmp_intermediate_lod%d' % lod_for_saving_model)\n                if lod2batch.is_time_to_report():\n                    save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, model.module if hasattr(model, 'module') else model, cfg, encoder_optimizer, decoder_optimizer)\n        scheduler.step()\n        if local_rank == 0:\n            checkpointer.save('model_tmp_lod%d' % lod_for_saving_model)\n            save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, model.module if hasattr(model, 'module') else model, cfg, encoder_optimizer, decoder_optimizer)\n    logger.info('Training finish!... save training results')\n    if local_rank == 0:\n        checkpointer.save('model_final').wait()",
            "def train(cfg, logger, local_rank, world_size, distributed):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    torch.cuda.set_device(local_rank)\n    model = Model(startf=cfg.MODEL.START_CHANNEL_COUNT, layer_count=cfg.MODEL.LAYER_COUNT, maxf=cfg.MODEL.MAX_CHANNEL_COUNT, latent_size=cfg.MODEL.LATENT_SPACE_SIZE, dlatent_avg_beta=cfg.MODEL.DLATENT_AVG_BETA, style_mixing_prob=cfg.MODEL.STYLE_MIXING_PROB, mapping_layers=cfg.MODEL.MAPPING_LAYERS, channels=cfg.MODEL.CHANNELS, generator=cfg.MODEL.GENERATOR, encoder=cfg.MODEL.ENCODER, z_regression=cfg.MODEL.Z_REGRESSION)\n    model.cuda(local_rank)\n    model.train()\n    if local_rank == 0:\n        model_s = Model(startf=cfg.MODEL.START_CHANNEL_COUNT, layer_count=cfg.MODEL.LAYER_COUNT, maxf=cfg.MODEL.MAX_CHANNEL_COUNT, latent_size=cfg.MODEL.LATENT_SPACE_SIZE, truncation_psi=cfg.MODEL.TRUNCATIOM_PSI, truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF, mapping_layers=cfg.MODEL.MAPPING_LAYERS, channels=cfg.MODEL.CHANNELS, generator=cfg.MODEL.GENERATOR, encoder=cfg.MODEL.ENCODER, z_regression=cfg.MODEL.Z_REGRESSION)\n        model_s.cuda(local_rank)\n        model_s.eval()\n        model_s.requires_grad_(False)\n    if distributed:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], broadcast_buffers=False, bucket_cap_mb=25, find_unused_parameters=True)\n        model.device_ids = None\n        decoder = model.module.decoder\n        encoder = model.module.encoder\n        mapping_d = model.module.mapping_d\n        mapping_f = model.module.mapping_f\n        dlatent_avg = model.module.dlatent_avg\n    else:\n        decoder = model.decoder\n        encoder = model.encoder\n        mapping_d = model.mapping_d\n        mapping_f = model.mapping_f\n        dlatent_avg = model.dlatent_avg\n    count_param_override.print = lambda a: logger.info(a)\n    logger.info('Trainable parameters generator:')\n    count_parameters(decoder)\n    logger.info('Trainable parameters discriminator:')\n    count_parameters(encoder)\n    arguments = dict()\n    arguments['iteration'] = 0\n    decoder_optimizer = LREQAdam([{'params': decoder.parameters()}, {'params': mapping_f.parameters()}], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n    encoder_optimizer = LREQAdam([{'params': encoder.parameters()}, {'params': mapping_d.parameters()}], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n    scheduler = ComboMultiStepLR(optimizers={'encoder_optimizer': encoder_optimizer, 'decoder_optimizer': decoder_optimizer}, milestones=cfg.TRAIN.LEARNING_DECAY_STEPS, gamma=cfg.TRAIN.LEARNING_DECAY_RATE, reference_batch_size=32, base_lr=cfg.TRAIN.LEARNING_RATES)\n    model_dict = {'discriminator': encoder, 'generator': decoder, 'mapping_tl': mapping_d, 'mapping_fl': mapping_f, 'dlatent_avg': dlatent_avg}\n    if local_rank == 0:\n        model_dict['discriminator_s'] = model_s.encoder\n        model_dict['generator_s'] = model_s.decoder\n        model_dict['mapping_tl_s'] = model_s.mapping_d\n        model_dict['mapping_fl_s'] = model_s.mapping_f\n    tracker = LossTracker(cfg.OUTPUT_DIR)\n    checkpointer = Checkpointer(cfg, model_dict, {'encoder_optimizer': encoder_optimizer, 'decoder_optimizer': decoder_optimizer, 'scheduler': scheduler, 'tracker': tracker}, logger=logger, save=local_rank == 0)\n    extra_checkpoint_data = checkpointer.load()\n    logger.info('Starting from epoch: %d' % scheduler.start_epoch())\n    arguments.update(extra_checkpoint_data)\n    layer_to_resolution = decoder.layer_to_resolution\n    dataset = TFRecordsDataset(cfg, logger, rank=local_rank, world_size=world_size, buffer_size_mb=1024, channels=cfg.MODEL.CHANNELS)\n    rnd = np.random.RandomState(3456)\n    latents = rnd.randn(32, cfg.MODEL.LATENT_SPACE_SIZE)\n    samplez = torch.tensor(latents).float().cuda()\n    lod2batch = lod_driver.LODDriver(cfg, logger, world_size, dataset_size=len(dataset) * world_size)\n    if cfg.DATASET.SAMPLES_PATH != 'no_path':\n        path = cfg.DATASET.SAMPLES_PATH\n        src = []\n        with torch.no_grad():\n            for filename in list(os.listdir(path))[:32]:\n                img = np.asarray(Image.open(os.path.join(path, filename)))\n                if img.shape[2] == 4:\n                    img = img[:, :, :3]\n                im = img.transpose((2, 0, 1))\n                x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.0\n                if x.shape[0] == 4:\n                    x = x[:3]\n                src.append(x)\n            sample = torch.stack(src)\n    else:\n        dataset.reset(cfg.DATASET.MAX_RESOLUTION_LEVEL, 32)\n        sample = next(make_dataloader(cfg, logger, dataset, 32, local_rank))\n        sample = sample / 127.5 - 1.0\n    lod2batch.set_epoch(scheduler.start_epoch(), [encoder_optimizer, decoder_optimizer])\n    for epoch in range(scheduler.start_epoch(), cfg.TRAIN.TRAIN_EPOCHS):\n        model.train()\n        lod2batch.set_epoch(epoch, [encoder_optimizer, decoder_optimizer])\n        logger.info('Batch size: %d, Batch size per GPU: %d, LOD: %d - %dx%d, blend: %.3f, dataset size: %d' % (lod2batch.get_batch_size(), lod2batch.get_per_GPU_batch_size(), lod2batch.lod, 2 ** lod2batch.get_lod_power2(), 2 ** lod2batch.get_lod_power2(), lod2batch.get_blend_factor(), len(dataset) * world_size))\n        dataset.reset(lod2batch.get_lod_power2(), lod2batch.get_per_GPU_batch_size())\n        batches = make_dataloader(cfg, logger, dataset, lod2batch.get_per_GPU_batch_size(), local_rank)\n        scheduler.set_batch_size(lod2batch.get_batch_size(), lod2batch.lod)\n        model.train()\n        need_permute = False\n        epoch_start_time = time.time()\n        i = 0\n        for x_orig in tqdm(batches):\n            i += 1\n            with torch.no_grad():\n                if x_orig.shape[0] != lod2batch.get_per_GPU_batch_size():\n                    continue\n                if need_permute:\n                    x_orig = x_orig.permute(0, 3, 1, 2)\n                x_orig = x_orig / 127.5 - 1.0\n                blend_factor = lod2batch.get_blend_factor()\n                needed_resolution = layer_to_resolution[lod2batch.lod]\n                x = x_orig\n                if lod2batch.in_transition:\n                    needed_resolution_prev = layer_to_resolution[lod2batch.lod - 1]\n                    x_prev = F.avg_pool2d(x_orig, 2, 2)\n                    x_prev_2x = F.interpolate(x_prev, needed_resolution)\n                    x = x * blend_factor + x_prev_2x * (1.0 - blend_factor)\n            x.requires_grad = True\n            encoder_optimizer.zero_grad()\n            loss_d = model(x, lod2batch.lod, blend_factor, d_train=True, ae=False)\n            tracker.update(dict(loss_d=loss_d))\n            loss_d.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.zero_grad()\n            loss_g = model(x, lod2batch.lod, blend_factor, d_train=False, ae=False)\n            tracker.update(dict(loss_g=loss_g))\n            loss_g.backward()\n            decoder_optimizer.step()\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            lae = model(x, lod2batch.lod, blend_factor, d_train=True, ae=True)\n            tracker.update(dict(lae=lae))\n            lae.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n            if local_rank == 0:\n                betta = 0.5 ** (lod2batch.get_batch_size() / (10 * 1000.0))\n                model_s.lerp(model, betta)\n            epoch_end_time = time.time()\n            per_epoch_ptime = epoch_end_time - epoch_start_time\n            lod_for_saving_model = lod2batch.lod\n            lod2batch.step()\n            if local_rank == 0:\n                if lod2batch.is_time_to_save():\n                    checkpointer.save('model_tmp_intermediate_lod%d' % lod_for_saving_model)\n                if lod2batch.is_time_to_report():\n                    save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, model.module if hasattr(model, 'module') else model, cfg, encoder_optimizer, decoder_optimizer)\n        scheduler.step()\n        if local_rank == 0:\n            checkpointer.save('model_tmp_lod%d' % lod_for_saving_model)\n            save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, model.module if hasattr(model, 'module') else model, cfg, encoder_optimizer, decoder_optimizer)\n    logger.info('Training finish!... save training results')\n    if local_rank == 0:\n        checkpointer.save('model_final').wait()"
        ]
    }
]