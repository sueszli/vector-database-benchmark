[
    {
        "func_name": "add_checkpointing_args",
        "original": "def add_checkpointing_args(parser):\n    parser.add_argument('--megatron-path', type=str, default=None, help='Base directory of Megatron repository')\n    parser.add_argument('--convert_checkpoint_from_megatron_to_transformers', action='store_true', help='If True, convert a Megatron checkpoint to a Transformers checkpoint. If False, convert a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--load_path', type=str, required=True, help='Path to the checkpoint to convert.')\n    parser.add_argument('--save_path', type=str, required=True, help='Path to the converted checkpoint.')\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    return parser",
        "mutated": [
            "def add_checkpointing_args(parser):\n    if False:\n        i = 10\n    parser.add_argument('--megatron-path', type=str, default=None, help='Base directory of Megatron repository')\n    parser.add_argument('--convert_checkpoint_from_megatron_to_transformers', action='store_true', help='If True, convert a Megatron checkpoint to a Transformers checkpoint. If False, convert a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--load_path', type=str, required=True, help='Path to the checkpoint to convert.')\n    parser.add_argument('--save_path', type=str, required=True, help='Path to the converted checkpoint.')\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    return parser",
            "def add_checkpointing_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser.add_argument('--megatron-path', type=str, default=None, help='Base directory of Megatron repository')\n    parser.add_argument('--convert_checkpoint_from_megatron_to_transformers', action='store_true', help='If True, convert a Megatron checkpoint to a Transformers checkpoint. If False, convert a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--load_path', type=str, required=True, help='Path to the checkpoint to convert.')\n    parser.add_argument('--save_path', type=str, required=True, help='Path to the converted checkpoint.')\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    return parser",
            "def add_checkpointing_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser.add_argument('--megatron-path', type=str, default=None, help='Base directory of Megatron repository')\n    parser.add_argument('--convert_checkpoint_from_megatron_to_transformers', action='store_true', help='If True, convert a Megatron checkpoint to a Transformers checkpoint. If False, convert a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--load_path', type=str, required=True, help='Path to the checkpoint to convert.')\n    parser.add_argument('--save_path', type=str, required=True, help='Path to the converted checkpoint.')\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    return parser",
            "def add_checkpointing_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser.add_argument('--megatron-path', type=str, default=None, help='Base directory of Megatron repository')\n    parser.add_argument('--convert_checkpoint_from_megatron_to_transformers', action='store_true', help='If True, convert a Megatron checkpoint to a Transformers checkpoint. If False, convert a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--load_path', type=str, required=True, help='Path to the checkpoint to convert.')\n    parser.add_argument('--save_path', type=str, required=True, help='Path to the converted checkpoint.')\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    return parser",
            "def add_checkpointing_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser.add_argument('--megatron-path', type=str, default=None, help='Base directory of Megatron repository')\n    parser.add_argument('--convert_checkpoint_from_megatron_to_transformers', action='store_true', help='If True, convert a Megatron checkpoint to a Transformers checkpoint. If False, convert a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--load_path', type=str, required=True, help='Path to the checkpoint to convert.')\n    parser.add_argument('--save_path', type=str, required=True, help='Path to the converted checkpoint.')\n    parser.add_argument('--print-checkpoint-structure', action='store_true')\n    return parser"
        ]
    },
    {
        "func_name": "add_megatron_checkpoint_args",
        "original": "def add_megatron_checkpoint_args(parser):\n    parser.add_argument('--target_tensor_model_parallel_size', type=int, default=1, help='The tensor model parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_pipeline_model_parallel_size', type=int, default=1, help='The pipeline model parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_data_parallel_size', type=int, default=1, help='The data parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_params_dtype', type=str, default='fp32', help='The dtype of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--make_vocab_size_divisible_by', type=int, default=128, help='Pad the vocab size to be divisible by this value. This is added for computational efficieny reasons. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--use_distributed_optimizer', action='store_true', help='If True, use the distributed optimizer. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    return parser",
        "mutated": [
            "def add_megatron_checkpoint_args(parser):\n    if False:\n        i = 10\n    parser.add_argument('--target_tensor_model_parallel_size', type=int, default=1, help='The tensor model parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_pipeline_model_parallel_size', type=int, default=1, help='The pipeline model parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_data_parallel_size', type=int, default=1, help='The data parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_params_dtype', type=str, default='fp32', help='The dtype of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--make_vocab_size_divisible_by', type=int, default=128, help='Pad the vocab size to be divisible by this value. This is added for computational efficieny reasons. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--use_distributed_optimizer', action='store_true', help='If True, use the distributed optimizer. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    return parser",
            "def add_megatron_checkpoint_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser.add_argument('--target_tensor_model_parallel_size', type=int, default=1, help='The tensor model parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_pipeline_model_parallel_size', type=int, default=1, help='The pipeline model parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_data_parallel_size', type=int, default=1, help='The data parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_params_dtype', type=str, default='fp32', help='The dtype of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--make_vocab_size_divisible_by', type=int, default=128, help='Pad the vocab size to be divisible by this value. This is added for computational efficieny reasons. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--use_distributed_optimizer', action='store_true', help='If True, use the distributed optimizer. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    return parser",
            "def add_megatron_checkpoint_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser.add_argument('--target_tensor_model_parallel_size', type=int, default=1, help='The tensor model parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_pipeline_model_parallel_size', type=int, default=1, help='The pipeline model parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_data_parallel_size', type=int, default=1, help='The data parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_params_dtype', type=str, default='fp32', help='The dtype of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--make_vocab_size_divisible_by', type=int, default=128, help='Pad the vocab size to be divisible by this value. This is added for computational efficieny reasons. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--use_distributed_optimizer', action='store_true', help='If True, use the distributed optimizer. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    return parser",
            "def add_megatron_checkpoint_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser.add_argument('--target_tensor_model_parallel_size', type=int, default=1, help='The tensor model parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_pipeline_model_parallel_size', type=int, default=1, help='The pipeline model parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_data_parallel_size', type=int, default=1, help='The data parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_params_dtype', type=str, default='fp32', help='The dtype of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--make_vocab_size_divisible_by', type=int, default=128, help='Pad the vocab size to be divisible by this value. This is added for computational efficieny reasons. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--use_distributed_optimizer', action='store_true', help='If True, use the distributed optimizer. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    return parser",
            "def add_megatron_checkpoint_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser.add_argument('--target_tensor_model_parallel_size', type=int, default=1, help='The tensor model parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_pipeline_model_parallel_size', type=int, default=1, help='The pipeline model parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_data_parallel_size', type=int, default=1, help='The data parallel size of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--target_params_dtype', type=str, default='fp32', help='The dtype of the converted checkpoint. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--make_vocab_size_divisible_by', type=int, default=128, help='Pad the vocab size to be divisible by this value. This is added for computational efficieny reasons. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    parser.add_argument('--use_distributed_optimizer', action='store_true', help='If True, use the distributed optimizer. Only used when converting a Transformers checkpoint to a Megatron checkpoint.')\n    return parser"
        ]
    },
    {
        "func_name": "add_transformers_checkpoint_args",
        "original": "def add_transformers_checkpoint_args(parser):\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='The name of the pre-trained tokenizer to save. If not None, the tokenizer will be saved. Only used when converting a Megatron checkpoint to a Transformers checkpoint.')\n    parser.add_argument('--max_shard_size', type=str, default='10GB', help='The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size lower than this size. If expressed as a string, needs to be digits followed by a unit (like `5MB`). Only used when converting a Megatron checkpoint to a Transformers checkpoint.')\n    return parser",
        "mutated": [
            "def add_transformers_checkpoint_args(parser):\n    if False:\n        i = 10\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='The name of the pre-trained tokenizer to save. If not None, the tokenizer will be saved. Only used when converting a Megatron checkpoint to a Transformers checkpoint.')\n    parser.add_argument('--max_shard_size', type=str, default='10GB', help='The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size lower than this size. If expressed as a string, needs to be digits followed by a unit (like `5MB`). Only used when converting a Megatron checkpoint to a Transformers checkpoint.')\n    return parser",
            "def add_transformers_checkpoint_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='The name of the pre-trained tokenizer to save. If not None, the tokenizer will be saved. Only used when converting a Megatron checkpoint to a Transformers checkpoint.')\n    parser.add_argument('--max_shard_size', type=str, default='10GB', help='The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size lower than this size. If expressed as a string, needs to be digits followed by a unit (like `5MB`). Only used when converting a Megatron checkpoint to a Transformers checkpoint.')\n    return parser",
            "def add_transformers_checkpoint_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='The name of the pre-trained tokenizer to save. If not None, the tokenizer will be saved. Only used when converting a Megatron checkpoint to a Transformers checkpoint.')\n    parser.add_argument('--max_shard_size', type=str, default='10GB', help='The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size lower than this size. If expressed as a string, needs to be digits followed by a unit (like `5MB`). Only used when converting a Megatron checkpoint to a Transformers checkpoint.')\n    return parser",
            "def add_transformers_checkpoint_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='The name of the pre-trained tokenizer to save. If not None, the tokenizer will be saved. Only used when converting a Megatron checkpoint to a Transformers checkpoint.')\n    parser.add_argument('--max_shard_size', type=str, default='10GB', help='The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size lower than this size. If expressed as a string, needs to be digits followed by a unit (like `5MB`). Only used when converting a Megatron checkpoint to a Transformers checkpoint.')\n    return parser",
            "def add_transformers_checkpoint_args(parser):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser.add_argument('--tokenizer_name', type=str, default=None, help='The name of the pre-trained tokenizer to save. If not None, the tokenizer will be saved. Only used when converting a Megatron checkpoint to a Transformers checkpoint.')\n    parser.add_argument('--max_shard_size', type=str, default='10GB', help='The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size lower than this size. If expressed as a string, needs to be digits followed by a unit (like `5MB`). Only used when converting a Megatron checkpoint to a Transformers checkpoint.')\n    return parser"
        ]
    },
    {
        "func_name": "recursive_print",
        "original": "def recursive_print(name, val, spaces=0):\n    \"\"\"\n    Recursively print the structure of a checkpoint. This function is taken from `convert_megatron_gpt2_checkpoint.py`\n\n    Args:\n        name (str): the name of the current tensor parameter\n        val (Tuple(int)): the shape of the current tensor parameter\n        spaces (int): the number of spaces to print before the output for a nested structure\n    \"\"\"\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
        "mutated": [
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n    '\\n    Recursively print the structure of a checkpoint. This function is taken from `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        name (str): the name of the current tensor parameter\\n        val (Tuple(int)): the shape of the current tensor parameter\\n        spaces (int): the number of spaces to print before the output for a nested structure\\n    '\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Recursively print the structure of a checkpoint. This function is taken from `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        name (str): the name of the current tensor parameter\\n        val (Tuple(int)): the shape of the current tensor parameter\\n        spaces (int): the number of spaces to print before the output for a nested structure\\n    '\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Recursively print the structure of a checkpoint. This function is taken from `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        name (str): the name of the current tensor parameter\\n        val (Tuple(int)): the shape of the current tensor parameter\\n        spaces (int): the number of spaces to print before the output for a nested structure\\n    '\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Recursively print the structure of a checkpoint. This function is taken from `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        name (str): the name of the current tensor parameter\\n        val (Tuple(int)): the shape of the current tensor parameter\\n        spaces (int): the number of spaces to print before the output for a nested structure\\n    '\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)",
            "def recursive_print(name, val, spaces=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Recursively print the structure of a checkpoint. This function is taken from `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        name (str): the name of the current tensor parameter\\n        val (Tuple(int)): the shape of the current tensor parameter\\n        spaces (int): the number of spaces to print before the output for a nested structure\\n    '\n    if name is None:\n        msg = None\n    else:\n        fmt = '.' * max(0, spaces - 2) + '# {:' + str(50 - spaces) + 's}'\n        msg = fmt.format(name)\n    if isinstance(val, dict):\n        if msg is not None:\n            print(msg)\n        for k in val.keys():\n            recursive_print(k, val[k], spaces + 2)\n    elif isinstance(val, torch.Tensor):\n        print(msg, ':', val.size())\n    else:\n        print(msg, ':', val)"
        ]
    },
    {
        "func_name": "megatron_to_transformers_fix_query_key_value_ordering",
        "original": "def megatron_to_transformers_fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    \"\"\"\n    Permutes layout of param tensor to [num_splits * num_heads * hidden_size, :] for compatibility with later versions\n    of NVIDIA Megatron-LM. The inverse operation is performed inside Megatron-LM to read checkpoints:\n    https://github.com/NVIDIA/Megatron-LM/blob/v2.4/megatron/checkpointing.py#L209 If param is the weight tensor of the\n    self-attention block, the returned tensor will have to be transposed one more time to be read by HuggingFace GPT2.\n    This function is taken from `convert_megatron_gpt2_checkpoint.py`\n\n    Args:\n        param (torch.Tensor): the tensor to permute\n        checkpoint_version (int): the version of the checkpoint.\n        num_splits (int): the number of projections, usually 3 for (Query, Key, Value)\n        num_heads (int): the number of attention heads\n        hidden_size (int): the hidden size per head\n    \"\"\"\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
        "mutated": [
            "def megatron_to_transformers_fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n    '\\n    Permutes layout of param tensor to [num_splits * num_heads * hidden_size, :] for compatibility with later versions\\n    of NVIDIA Megatron-LM. The inverse operation is performed inside Megatron-LM to read checkpoints:\\n    https://github.com/NVIDIA/Megatron-LM/blob/v2.4/megatron/checkpointing.py#L209 If param is the weight tensor of the\\n    self-attention block, the returned tensor will have to be transposed one more time to be read by HuggingFace GPT2.\\n    This function is taken from `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        param (torch.Tensor): the tensor to permute\\n        checkpoint_version (int): the version of the checkpoint.\\n        num_splits (int): the number of projections, usually 3 for (Query, Key, Value)\\n        num_heads (int): the number of attention heads\\n        hidden_size (int): the hidden size per head\\n    '\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def megatron_to_transformers_fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Permutes layout of param tensor to [num_splits * num_heads * hidden_size, :] for compatibility with later versions\\n    of NVIDIA Megatron-LM. The inverse operation is performed inside Megatron-LM to read checkpoints:\\n    https://github.com/NVIDIA/Megatron-LM/blob/v2.4/megatron/checkpointing.py#L209 If param is the weight tensor of the\\n    self-attention block, the returned tensor will have to be transposed one more time to be read by HuggingFace GPT2.\\n    This function is taken from `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        param (torch.Tensor): the tensor to permute\\n        checkpoint_version (int): the version of the checkpoint.\\n        num_splits (int): the number of projections, usually 3 for (Query, Key, Value)\\n        num_heads (int): the number of attention heads\\n        hidden_size (int): the hidden size per head\\n    '\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def megatron_to_transformers_fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Permutes layout of param tensor to [num_splits * num_heads * hidden_size, :] for compatibility with later versions\\n    of NVIDIA Megatron-LM. The inverse operation is performed inside Megatron-LM to read checkpoints:\\n    https://github.com/NVIDIA/Megatron-LM/blob/v2.4/megatron/checkpointing.py#L209 If param is the weight tensor of the\\n    self-attention block, the returned tensor will have to be transposed one more time to be read by HuggingFace GPT2.\\n    This function is taken from `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        param (torch.Tensor): the tensor to permute\\n        checkpoint_version (int): the version of the checkpoint.\\n        num_splits (int): the number of projections, usually 3 for (Query, Key, Value)\\n        num_heads (int): the number of attention heads\\n        hidden_size (int): the hidden size per head\\n    '\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def megatron_to_transformers_fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Permutes layout of param tensor to [num_splits * num_heads * hidden_size, :] for compatibility with later versions\\n    of NVIDIA Megatron-LM. The inverse operation is performed inside Megatron-LM to read checkpoints:\\n    https://github.com/NVIDIA/Megatron-LM/blob/v2.4/megatron/checkpointing.py#L209 If param is the weight tensor of the\\n    self-attention block, the returned tensor will have to be transposed one more time to be read by HuggingFace GPT2.\\n    This function is taken from `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        param (torch.Tensor): the tensor to permute\\n        checkpoint_version (int): the version of the checkpoint.\\n        num_splits (int): the number of projections, usually 3 for (Query, Key, Value)\\n        num_heads (int): the number of attention heads\\n        hidden_size (int): the hidden size per head\\n    '\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def megatron_to_transformers_fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Permutes layout of param tensor to [num_splits * num_heads * hidden_size, :] for compatibility with later versions\\n    of NVIDIA Megatron-LM. The inverse operation is performed inside Megatron-LM to read checkpoints:\\n    https://github.com/NVIDIA/Megatron-LM/blob/v2.4/megatron/checkpointing.py#L209 If param is the weight tensor of the\\n    self-attention block, the returned tensor will have to be transposed one more time to be read by HuggingFace GPT2.\\n    This function is taken from `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        param (torch.Tensor): the tensor to permute\\n        checkpoint_version (int): the version of the checkpoint.\\n        num_splits (int): the number of projections, usually 3 for (Query, Key, Value)\\n        num_heads (int): the number of attention heads\\n        hidden_size (int): the hidden size per head\\n    '\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        saved_shape = (num_heads, hidden_size, num_splits) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        saved_shape = (num_heads, num_splits, hidden_size) + input_shape[1:]\n        param = param.view(*saved_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param"
        ]
    },
    {
        "func_name": "transformers_to_megatron_fix_query_key_value_ordering",
        "original": "def transformers_to_megatron_fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    \"\"\"\n    Permutes layout of param tensor to the one compatible with respective NVIDIA Megatron-LM chekpoint versions. Input\n    is [num_splits * num_heads * hidden_size, :] and output is [num_heads * hidden_size * num_splits, :] for version\n    1.0 and [num_heads * num_splits * hidden_size, :] for version 2.0 and later. If param is the weight tensor of the\n    self-attention block, the param needs to be already transposed before calling this function.\n\n    Args:\n        param (torch.Tensor): the tensor to permute\n        checkpoint_version (int): the version of the checkpoint.\n        num_splits (int): the number of projections, usually 3 for (Query, Key, Value)\n        num_heads (int): the number of attention heads\n        hidden_size (int): the hidden size per head\n    \"\"\"\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        current_shape = (num_splits, num_heads, hidden_size) + input_shape[1:]\n        param = param.view(*current_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        current_shape = (num_splits, num_heads, hidden_size) + input_shape[1:]\n        param = param.view(*current_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
        "mutated": [
            "def transformers_to_megatron_fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n    '\\n    Permutes layout of param tensor to the one compatible with respective NVIDIA Megatron-LM chekpoint versions. Input\\n    is [num_splits * num_heads * hidden_size, :] and output is [num_heads * hidden_size * num_splits, :] for version\\n    1.0 and [num_heads * num_splits * hidden_size, :] for version 2.0 and later. If param is the weight tensor of the\\n    self-attention block, the param needs to be already transposed before calling this function.\\n\\n    Args:\\n        param (torch.Tensor): the tensor to permute\\n        checkpoint_version (int): the version of the checkpoint.\\n        num_splits (int): the number of projections, usually 3 for (Query, Key, Value)\\n        num_heads (int): the number of attention heads\\n        hidden_size (int): the hidden size per head\\n    '\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        current_shape = (num_splits, num_heads, hidden_size) + input_shape[1:]\n        param = param.view(*current_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        current_shape = (num_splits, num_heads, hidden_size) + input_shape[1:]\n        param = param.view(*current_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def transformers_to_megatron_fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Permutes layout of param tensor to the one compatible with respective NVIDIA Megatron-LM chekpoint versions. Input\\n    is [num_splits * num_heads * hidden_size, :] and output is [num_heads * hidden_size * num_splits, :] for version\\n    1.0 and [num_heads * num_splits * hidden_size, :] for version 2.0 and later. If param is the weight tensor of the\\n    self-attention block, the param needs to be already transposed before calling this function.\\n\\n    Args:\\n        param (torch.Tensor): the tensor to permute\\n        checkpoint_version (int): the version of the checkpoint.\\n        num_splits (int): the number of projections, usually 3 for (Query, Key, Value)\\n        num_heads (int): the number of attention heads\\n        hidden_size (int): the hidden size per head\\n    '\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        current_shape = (num_splits, num_heads, hidden_size) + input_shape[1:]\n        param = param.view(*current_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        current_shape = (num_splits, num_heads, hidden_size) + input_shape[1:]\n        param = param.view(*current_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def transformers_to_megatron_fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Permutes layout of param tensor to the one compatible with respective NVIDIA Megatron-LM chekpoint versions. Input\\n    is [num_splits * num_heads * hidden_size, :] and output is [num_heads * hidden_size * num_splits, :] for version\\n    1.0 and [num_heads * num_splits * hidden_size, :] for version 2.0 and later. If param is the weight tensor of the\\n    self-attention block, the param needs to be already transposed before calling this function.\\n\\n    Args:\\n        param (torch.Tensor): the tensor to permute\\n        checkpoint_version (int): the version of the checkpoint.\\n        num_splits (int): the number of projections, usually 3 for (Query, Key, Value)\\n        num_heads (int): the number of attention heads\\n        hidden_size (int): the hidden size per head\\n    '\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        current_shape = (num_splits, num_heads, hidden_size) + input_shape[1:]\n        param = param.view(*current_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        current_shape = (num_splits, num_heads, hidden_size) + input_shape[1:]\n        param = param.view(*current_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def transformers_to_megatron_fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Permutes layout of param tensor to the one compatible with respective NVIDIA Megatron-LM chekpoint versions. Input\\n    is [num_splits * num_heads * hidden_size, :] and output is [num_heads * hidden_size * num_splits, :] for version\\n    1.0 and [num_heads * num_splits * hidden_size, :] for version 2.0 and later. If param is the weight tensor of the\\n    self-attention block, the param needs to be already transposed before calling this function.\\n\\n    Args:\\n        param (torch.Tensor): the tensor to permute\\n        checkpoint_version (int): the version of the checkpoint.\\n        num_splits (int): the number of projections, usually 3 for (Query, Key, Value)\\n        num_heads (int): the number of attention heads\\n        hidden_size (int): the hidden size per head\\n    '\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        current_shape = (num_splits, num_heads, hidden_size) + input_shape[1:]\n        param = param.view(*current_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        current_shape = (num_splits, num_heads, hidden_size) + input_shape[1:]\n        param = param.view(*current_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param",
            "def transformers_to_megatron_fix_query_key_value_ordering(param, checkpoint_version, num_splits, num_heads, hidden_size):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Permutes layout of param tensor to the one compatible with respective NVIDIA Megatron-LM chekpoint versions. Input\\n    is [num_splits * num_heads * hidden_size, :] and output is [num_heads * hidden_size * num_splits, :] for version\\n    1.0 and [num_heads * num_splits * hidden_size, :] for version 2.0 and later. If param is the weight tensor of the\\n    self-attention block, the param needs to be already transposed before calling this function.\\n\\n    Args:\\n        param (torch.Tensor): the tensor to permute\\n        checkpoint_version (int): the version of the checkpoint.\\n        num_splits (int): the number of projections, usually 3 for (Query, Key, Value)\\n        num_heads (int): the number of attention heads\\n        hidden_size (int): the hidden size per head\\n    '\n    input_shape = param.size()\n    if checkpoint_version == 1.0:\n        current_shape = (num_splits, num_heads, hidden_size) + input_shape[1:]\n        param = param.view(*current_shape)\n        param = param.transpose(0, 2)\n        param = param.transpose(1, 2).contiguous()\n    elif checkpoint_version >= 2.0:\n        current_shape = (num_splits, num_heads, hidden_size) + input_shape[1:]\n        param = param.view(*current_shape)\n        param = param.transpose(0, 1).contiguous()\n    param = param.view(*input_shape)\n    return param"
        ]
    },
    {
        "func_name": "merge_transformers_sharded_states",
        "original": "def merge_transformers_sharded_states(path, num_checkpoints):\n    \"\"\"\n    Merge sharded checkpoints from transformers into a single checkpoint.\n\n    Args:\n        path (str): the path to the sharded checkpoints\n        num_checkpoints (int): the number of checkpoints to merge\n    \"\"\"\n    state_dict = {}\n    for i in range(1, num_checkpoints + 1):\n        checkpoint_path = os.path.join(path, f'pytorch_model-{i:05d}-of-{num_checkpoints:05d}.bin')\n        current_chunk = torch.load(checkpoint_path, map_location='cpu')\n        state_dict.update(current_chunk)\n    return state_dict",
        "mutated": [
            "def merge_transformers_sharded_states(path, num_checkpoints):\n    if False:\n        i = 10\n    '\\n    Merge sharded checkpoints from transformers into a single checkpoint.\\n\\n    Args:\\n        path (str): the path to the sharded checkpoints\\n        num_checkpoints (int): the number of checkpoints to merge\\n    '\n    state_dict = {}\n    for i in range(1, num_checkpoints + 1):\n        checkpoint_path = os.path.join(path, f'pytorch_model-{i:05d}-of-{num_checkpoints:05d}.bin')\n        current_chunk = torch.load(checkpoint_path, map_location='cpu')\n        state_dict.update(current_chunk)\n    return state_dict",
            "def merge_transformers_sharded_states(path, num_checkpoints):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Merge sharded checkpoints from transformers into a single checkpoint.\\n\\n    Args:\\n        path (str): the path to the sharded checkpoints\\n        num_checkpoints (int): the number of checkpoints to merge\\n    '\n    state_dict = {}\n    for i in range(1, num_checkpoints + 1):\n        checkpoint_path = os.path.join(path, f'pytorch_model-{i:05d}-of-{num_checkpoints:05d}.bin')\n        current_chunk = torch.load(checkpoint_path, map_location='cpu')\n        state_dict.update(current_chunk)\n    return state_dict",
            "def merge_transformers_sharded_states(path, num_checkpoints):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Merge sharded checkpoints from transformers into a single checkpoint.\\n\\n    Args:\\n        path (str): the path to the sharded checkpoints\\n        num_checkpoints (int): the number of checkpoints to merge\\n    '\n    state_dict = {}\n    for i in range(1, num_checkpoints + 1):\n        checkpoint_path = os.path.join(path, f'pytorch_model-{i:05d}-of-{num_checkpoints:05d}.bin')\n        current_chunk = torch.load(checkpoint_path, map_location='cpu')\n        state_dict.update(current_chunk)\n    return state_dict",
            "def merge_transformers_sharded_states(path, num_checkpoints):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Merge sharded checkpoints from transformers into a single checkpoint.\\n\\n    Args:\\n        path (str): the path to the sharded checkpoints\\n        num_checkpoints (int): the number of checkpoints to merge\\n    '\n    state_dict = {}\n    for i in range(1, num_checkpoints + 1):\n        checkpoint_path = os.path.join(path, f'pytorch_model-{i:05d}-of-{num_checkpoints:05d}.bin')\n        current_chunk = torch.load(checkpoint_path, map_location='cpu')\n        state_dict.update(current_chunk)\n    return state_dict",
            "def merge_transformers_sharded_states(path, num_checkpoints):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Merge sharded checkpoints from transformers into a single checkpoint.\\n\\n    Args:\\n        path (str): the path to the sharded checkpoints\\n        num_checkpoints (int): the number of checkpoints to merge\\n    '\n    state_dict = {}\n    for i in range(1, num_checkpoints + 1):\n        checkpoint_path = os.path.join(path, f'pytorch_model-{i:05d}-of-{num_checkpoints:05d}.bin')\n        current_chunk = torch.load(checkpoint_path, map_location='cpu')\n        state_dict.update(current_chunk)\n    return state_dict"
        ]
    },
    {
        "func_name": "get_megatron_sharded_states",
        "original": "def get_megatron_sharded_states(args, tp_size, pp_size, pp_rank):\n    \"\"\"\n    Get sharded checkpoints from NVIDIA Megatron-LM checkpoint based on the provided tensor parallel size, pipeline\n    parallel size and pipeline parallel rank.\n\n    Args:\n        args (argparse.Namespace): the arguments to the script\n        tp_size (int): the tensor parallel size\n        pp_size (int): the pipeline parallel size\n        pp_rank (int): the pipeline parallel rank\n    \"\"\"\n    tp_state_dicts = []\n    for i in range(tp_size):\n        sub_dir_name = f'mp_rank_{i:02d}' if pp_size == 1 else f'mp_rank_{i:02d}_{pp_rank:03d}'\n        for checkpoint_name in ['model_optim_rng.pt', 'model_rng.pt']:\n            checkpoint_path = os.path.join(args.load_path, sub_dir_name, checkpoint_name)\n            if os.path.isfile(checkpoint_path):\n                break\n        state_dict = torch.load(checkpoint_path, map_location='cpu')\n        tp_state_dicts.append(state_dict)\n    return tp_state_dicts",
        "mutated": [
            "def get_megatron_sharded_states(args, tp_size, pp_size, pp_rank):\n    if False:\n        i = 10\n    '\\n    Get sharded checkpoints from NVIDIA Megatron-LM checkpoint based on the provided tensor parallel size, pipeline\\n    parallel size and pipeline parallel rank.\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n        tp_size (int): the tensor parallel size\\n        pp_size (int): the pipeline parallel size\\n        pp_rank (int): the pipeline parallel rank\\n    '\n    tp_state_dicts = []\n    for i in range(tp_size):\n        sub_dir_name = f'mp_rank_{i:02d}' if pp_size == 1 else f'mp_rank_{i:02d}_{pp_rank:03d}'\n        for checkpoint_name in ['model_optim_rng.pt', 'model_rng.pt']:\n            checkpoint_path = os.path.join(args.load_path, sub_dir_name, checkpoint_name)\n            if os.path.isfile(checkpoint_path):\n                break\n        state_dict = torch.load(checkpoint_path, map_location='cpu')\n        tp_state_dicts.append(state_dict)\n    return tp_state_dicts",
            "def get_megatron_sharded_states(args, tp_size, pp_size, pp_rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Get sharded checkpoints from NVIDIA Megatron-LM checkpoint based on the provided tensor parallel size, pipeline\\n    parallel size and pipeline parallel rank.\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n        tp_size (int): the tensor parallel size\\n        pp_size (int): the pipeline parallel size\\n        pp_rank (int): the pipeline parallel rank\\n    '\n    tp_state_dicts = []\n    for i in range(tp_size):\n        sub_dir_name = f'mp_rank_{i:02d}' if pp_size == 1 else f'mp_rank_{i:02d}_{pp_rank:03d}'\n        for checkpoint_name in ['model_optim_rng.pt', 'model_rng.pt']:\n            checkpoint_path = os.path.join(args.load_path, sub_dir_name, checkpoint_name)\n            if os.path.isfile(checkpoint_path):\n                break\n        state_dict = torch.load(checkpoint_path, map_location='cpu')\n        tp_state_dicts.append(state_dict)\n    return tp_state_dicts",
            "def get_megatron_sharded_states(args, tp_size, pp_size, pp_rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Get sharded checkpoints from NVIDIA Megatron-LM checkpoint based on the provided tensor parallel size, pipeline\\n    parallel size and pipeline parallel rank.\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n        tp_size (int): the tensor parallel size\\n        pp_size (int): the pipeline parallel size\\n        pp_rank (int): the pipeline parallel rank\\n    '\n    tp_state_dicts = []\n    for i in range(tp_size):\n        sub_dir_name = f'mp_rank_{i:02d}' if pp_size == 1 else f'mp_rank_{i:02d}_{pp_rank:03d}'\n        for checkpoint_name in ['model_optim_rng.pt', 'model_rng.pt']:\n            checkpoint_path = os.path.join(args.load_path, sub_dir_name, checkpoint_name)\n            if os.path.isfile(checkpoint_path):\n                break\n        state_dict = torch.load(checkpoint_path, map_location='cpu')\n        tp_state_dicts.append(state_dict)\n    return tp_state_dicts",
            "def get_megatron_sharded_states(args, tp_size, pp_size, pp_rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Get sharded checkpoints from NVIDIA Megatron-LM checkpoint based on the provided tensor parallel size, pipeline\\n    parallel size and pipeline parallel rank.\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n        tp_size (int): the tensor parallel size\\n        pp_size (int): the pipeline parallel size\\n        pp_rank (int): the pipeline parallel rank\\n    '\n    tp_state_dicts = []\n    for i in range(tp_size):\n        sub_dir_name = f'mp_rank_{i:02d}' if pp_size == 1 else f'mp_rank_{i:02d}_{pp_rank:03d}'\n        for checkpoint_name in ['model_optim_rng.pt', 'model_rng.pt']:\n            checkpoint_path = os.path.join(args.load_path, sub_dir_name, checkpoint_name)\n            if os.path.isfile(checkpoint_path):\n                break\n        state_dict = torch.load(checkpoint_path, map_location='cpu')\n        tp_state_dicts.append(state_dict)\n    return tp_state_dicts",
            "def get_megatron_sharded_states(args, tp_size, pp_size, pp_rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Get sharded checkpoints from NVIDIA Megatron-LM checkpoint based on the provided tensor parallel size, pipeline\\n    parallel size and pipeline parallel rank.\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n        tp_size (int): the tensor parallel size\\n        pp_size (int): the pipeline parallel size\\n        pp_rank (int): the pipeline parallel rank\\n    '\n    tp_state_dicts = []\n    for i in range(tp_size):\n        sub_dir_name = f'mp_rank_{i:02d}' if pp_size == 1 else f'mp_rank_{i:02d}_{pp_rank:03d}'\n        for checkpoint_name in ['model_optim_rng.pt', 'model_rng.pt']:\n            checkpoint_path = os.path.join(args.load_path, sub_dir_name, checkpoint_name)\n            if os.path.isfile(checkpoint_path):\n                break\n        state_dict = torch.load(checkpoint_path, map_location='cpu')\n        tp_state_dicts.append(state_dict)\n    return tp_state_dicts"
        ]
    },
    {
        "func_name": "get_element_from_dict_by_path",
        "original": "def get_element_from_dict_by_path(d, path):\n    \"\"\"\n    Get element from dictionary by path. If element is not present, recursively add empty dictionaries.\n\n    Args:\n        d (dict): the dictionary to get the element from\n        path (list): the path to the element which is delimited by \".\"\n    \"\"\"\n    path = path.split('.')\n    for k in path:\n        if k not in d:\n            d[k] = {}\n        d = d[k]\n    return d",
        "mutated": [
            "def get_element_from_dict_by_path(d, path):\n    if False:\n        i = 10\n    '\\n    Get element from dictionary by path. If element is not present, recursively add empty dictionaries.\\n\\n    Args:\\n        d (dict): the dictionary to get the element from\\n        path (list): the path to the element which is delimited by \".\"\\n    '\n    path = path.split('.')\n    for k in path:\n        if k not in d:\n            d[k] = {}\n        d = d[k]\n    return d",
            "def get_element_from_dict_by_path(d, path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Get element from dictionary by path. If element is not present, recursively add empty dictionaries.\\n\\n    Args:\\n        d (dict): the dictionary to get the element from\\n        path (list): the path to the element which is delimited by \".\"\\n    '\n    path = path.split('.')\n    for k in path:\n        if k not in d:\n            d[k] = {}\n        d = d[k]\n    return d",
            "def get_element_from_dict_by_path(d, path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Get element from dictionary by path. If element is not present, recursively add empty dictionaries.\\n\\n    Args:\\n        d (dict): the dictionary to get the element from\\n        path (list): the path to the element which is delimited by \".\"\\n    '\n    path = path.split('.')\n    for k in path:\n        if k not in d:\n            d[k] = {}\n        d = d[k]\n    return d",
            "def get_element_from_dict_by_path(d, path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Get element from dictionary by path. If element is not present, recursively add empty dictionaries.\\n\\n    Args:\\n        d (dict): the dictionary to get the element from\\n        path (list): the path to the element which is delimited by \".\"\\n    '\n    path = path.split('.')\n    for k in path:\n        if k not in d:\n            d[k] = {}\n        d = d[k]\n    return d",
            "def get_element_from_dict_by_path(d, path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Get element from dictionary by path. If element is not present, recursively add empty dictionaries.\\n\\n    Args:\\n        d (dict): the dictionary to get the element from\\n        path (list): the path to the element which is delimited by \".\"\\n    '\n    path = path.split('.')\n    for k in path:\n        if k not in d:\n            d[k] = {}\n        d = d[k]\n    return d"
        ]
    },
    {
        "func_name": "convert_checkpoint_from_megatron_to_transformers",
        "original": "def convert_checkpoint_from_megatron_to_transformers(args):\n    \"\"\"\n    Convert NVIDIA Megatron-LM checkpoint to HuggingFace Transformers checkpoint. This handles Megatron checkpoints\n    with different tensor parallelism and pipeline parallelism sizes. It saves the converted checkpoint into shards\n    using HuggingFace Transformers checkpoint sharding functionality. This greatly extends the functionality of\n    `convert_megatron_gpt2_checkpoint.py`\n\n    Args:\n        args (argparse.Namespace): the arguments to the script\n    \"\"\"\n    sub_dirs = os.listdir(args.load_path)\n    possible_sub_dirs = ['mp_rank_00', 'mp_rank_00_000']\n    for sub_dir in possible_sub_dirs:\n        if sub_dir in sub_dirs:\n            rank0_checkpoint_name = os.listdir(os.path.join(args.load_path, sub_dir))[0]\n            rank0_checkpoint_path = os.path.join(args.load_path, sub_dir, rank0_checkpoint_name)\n            break\n    print(f'Loading Megatron-LM checkpoint arguments from: {rank0_checkpoint_path}')\n    state_dict = torch.load(rank0_checkpoint_path, map_location='cpu')\n    megatron_args = state_dict.get('args', None)\n    if megatron_args is None:\n        raise ValueError('Megatron-LM checkpoint does not contain arguments. This utility only supports Megatron-LM checkpoints containing all the megatron arguments. This is because it loads all config related to model architecture, the tensor and pipeline model parallel size from the checkpoint insead of user having to manually specify all the details. Please save Megatron-LM checkpoint along with all the megatron arguments to use this utility.')\n    if megatron_args is not None:\n        if megatron_args.bias_gelu_fusion:\n            activation_function = 'gelu_fast'\n        elif megatron_args.openai_gelu:\n            activation_function = 'gelu_new'\n        else:\n            activation_function = 'gelu'\n    else:\n        activation_function = 'gelu_new'\n    vocab_size = megatron_args.padded_vocab_size if getattr(megatron_args, 'orig_vocab_size', None) is None else megatron_args.orig_vocab_size\n    print(vocab_size)\n    config = GPT2Config(vocab_size=vocab_size, n_positions=megatron_args.max_position_embeddings, n_embd=megatron_args.hidden_size, n_layer=megatron_args.num_layers, n_head=megatron_args.num_attention_heads, n_inner=megatron_args.ffn_hidden_size, activation_function=activation_function, resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, scale_attn_weights=True, use_cache=True, bos_token_id=vocab_size - 1, eos_token_id=vocab_size - 1, architectures=['GPT2LMHeadModel'])\n    output_state_dict = {}\n    checkpoint_version = state_dict.get('checkpoint_version', 0.0)\n    tp_size = megatron_args.tensor_model_parallel_size\n    pp_size = megatron_args.pipeline_model_parallel_size\n    dtype = torch.float32\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    print('Converting')\n    print('Converting embeddings')\n    tp_state_dicts = get_megatron_sharded_states(args, tp_size, pp_size, 0)\n    position_embeddings = get_element_from_dict_by_path(tp_state_dicts[0], 'model.language_model.embedding.position_embeddings.weight')\n    output_state_dict['transformer.wpe.weight'] = position_embeddings.to(dtype)\n    word_embeddings = torch.cat([get_element_from_dict_by_path(tp_state_dicts[tp_rank], 'model.language_model.embedding.word_embeddings.weight') for tp_rank in range(tp_size)], dim=0)\n    word_embeddings = word_embeddings[:vocab_size].to(dtype)\n    output_state_dict['transformer.wte.weight'] = word_embeddings\n    print('Converting transformer layers')\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    n_positions = config.n_positions\n    num_layers = config.num_hidden_layers // pp_size\n    for pp_rank in range(pp_size):\n        if pp_size > 0:\n            print(f'Converting pipeline parallel rank {pp_rank}')\n            tp_state_dicts = get_megatron_sharded_states(args, tp_size, pp_size, pp_rank)\n        path = 'model.language_model.transformer' if 'transformer' in get_element_from_dict_by_path(tp_state_dicts[0], 'model.language_model').keys() else 'model.language_model.encoder'\n        for (key, val) in get_element_from_dict_by_path(tp_state_dicts[0], path).items():\n            m = layer_re.match(key)\n            if m is None:\n                break\n            layer_idx = int(m.group(1)) + pp_rank * num_layers\n            op_name = m.group(2)\n            weight_or_bias = m.group(3)\n            layer_name = f'transformer.h.{layer_idx}'\n            if op_name + '.' + weight_or_bias not in tensor_parallel_params:\n                params = val.to(dtype)\n            else:\n                dim = 1 if op_name in ['self_attention.dense', 'mlp.dense_4h_to_h', 'attention.dense'] else 0\n                params = torch.cat([val] + [get_element_from_dict_by_path(tp_state_dicts[tp_rank], f'{path}')[key] for tp_rank in range(1, tp_size)], dim=dim).to(dtype)\n            if op_name.endswith('layernorm'):\n                ln_name = 'ln_1' if op_name.startswith('input') else 'ln_2'\n                output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = params\n            elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n                causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=dtype)).view(1, 1, n_positions, n_positions)\n                output_state_dict[layer_name + '.attn.bias'] = causal_mask\n                masked_bias = torch.tensor(-10000.0, dtype=dtype)\n                output_state_dict[layer_name + '.attn.masked_bias'] = masked_bias\n                out_val = megatron_to_transformers_fix_query_key_value_ordering(params, checkpoint_version, 3, heads, hidden_size_per_head)\n                out_val = out_val.transpose(0, 1).contiguous()\n                output_state_dict[layer_name + '.attn.c_attn.weight'] = out_val\n            elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n                out_val = megatron_to_transformers_fix_query_key_value_ordering(params, checkpoint_version, 3, heads, hidden_size_per_head)\n                output_state_dict[layer_name + '.attn.c_attn.bias'] = out_val\n            elif weight_or_bias == 'weight':\n                out_name = megatron_to_transformers[op_name]\n                output_state_dict[layer_name + out_name + 'weight'] = params.transpose(0, 1)\n            elif weight_or_bias == 'bias':\n                out_name = megatron_to_transformers[op_name]\n                output_state_dict[layer_name + out_name + 'bias'] = params\n    if config.n_layer != layer_idx + 1:\n        raise ValueError(f'Expected {config.n_layer} layers but found {layer_idx + 1}')\n    print('Converting final layernorm')\n    params = get_element_from_dict_by_path(tp_state_dicts[0], str(path))\n    output_state_dict['transformer.ln_f.weight'] = params['final_layernorm.weight'].to(dtype)\n    output_state_dict['transformer.ln_f.bias'] = params['final_layernorm.bias'].to(dtype)\n    print('Converting LM head')\n    output_state_dict['lm_head.weight'] = word_embeddings.to(dtype)\n    print('Conversion from Megatron-LM to Transformers is done!')\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    if args.tokenizer_name is None:\n        tokenizer_name = 'gpt2'\n    else:\n        tokenizer_name = args.tokenizer_name\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    tokenizer_class = type(tokenizer).__name__\n    config.tokenizer_class = tokenizer_class\n    print('Saving config')\n    config.save_pretrained(args.save_path)\n    if args.tokenizer_name is not None:\n        print(f'Adding {tokenizer_class} tokenizer files')\n        tokenizer.save_pretrained(args.save_path)\n    max_shard_size = int(args.max_shard_size) if args.max_shard_size.isdigit() else args.max_shard_size\n    (shards, index) = shard_checkpoint(output_state_dict, max_shard_size=max_shard_size)\n    for (shard_file, shard) in shards.items():\n        torch.save(shard, os.path.join(args.save_path, shard_file))\n    if index is None:\n        print(f'Model weights saved in {os.path.join(args.save_path, WEIGHTS_NAME)}')\n    else:\n        save_index_file = os.path.join(args.save_path, WEIGHTS_INDEX_NAME)\n        with open(save_index_file, 'w', encoding='utf-8') as f:\n            content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n            f.write(content)\n        print(f'The model is bigger than the maximum size per checkpoint ({args.max_shard_size}) and is going to be split in {len(shards)} checkpoint shards. You can find where each parameters has been saved in the index located at {save_index_file}.')",
        "mutated": [
            "def convert_checkpoint_from_megatron_to_transformers(args):\n    if False:\n        i = 10\n    '\\n    Convert NVIDIA Megatron-LM checkpoint to HuggingFace Transformers checkpoint. This handles Megatron checkpoints\\n    with different tensor parallelism and pipeline parallelism sizes. It saves the converted checkpoint into shards\\n    using HuggingFace Transformers checkpoint sharding functionality. This greatly extends the functionality of\\n    `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n    '\n    sub_dirs = os.listdir(args.load_path)\n    possible_sub_dirs = ['mp_rank_00', 'mp_rank_00_000']\n    for sub_dir in possible_sub_dirs:\n        if sub_dir in sub_dirs:\n            rank0_checkpoint_name = os.listdir(os.path.join(args.load_path, sub_dir))[0]\n            rank0_checkpoint_path = os.path.join(args.load_path, sub_dir, rank0_checkpoint_name)\n            break\n    print(f'Loading Megatron-LM checkpoint arguments from: {rank0_checkpoint_path}')\n    state_dict = torch.load(rank0_checkpoint_path, map_location='cpu')\n    megatron_args = state_dict.get('args', None)\n    if megatron_args is None:\n        raise ValueError('Megatron-LM checkpoint does not contain arguments. This utility only supports Megatron-LM checkpoints containing all the megatron arguments. This is because it loads all config related to model architecture, the tensor and pipeline model parallel size from the checkpoint insead of user having to manually specify all the details. Please save Megatron-LM checkpoint along with all the megatron arguments to use this utility.')\n    if megatron_args is not None:\n        if megatron_args.bias_gelu_fusion:\n            activation_function = 'gelu_fast'\n        elif megatron_args.openai_gelu:\n            activation_function = 'gelu_new'\n        else:\n            activation_function = 'gelu'\n    else:\n        activation_function = 'gelu_new'\n    vocab_size = megatron_args.padded_vocab_size if getattr(megatron_args, 'orig_vocab_size', None) is None else megatron_args.orig_vocab_size\n    print(vocab_size)\n    config = GPT2Config(vocab_size=vocab_size, n_positions=megatron_args.max_position_embeddings, n_embd=megatron_args.hidden_size, n_layer=megatron_args.num_layers, n_head=megatron_args.num_attention_heads, n_inner=megatron_args.ffn_hidden_size, activation_function=activation_function, resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, scale_attn_weights=True, use_cache=True, bos_token_id=vocab_size - 1, eos_token_id=vocab_size - 1, architectures=['GPT2LMHeadModel'])\n    output_state_dict = {}\n    checkpoint_version = state_dict.get('checkpoint_version', 0.0)\n    tp_size = megatron_args.tensor_model_parallel_size\n    pp_size = megatron_args.pipeline_model_parallel_size\n    dtype = torch.float32\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    print('Converting')\n    print('Converting embeddings')\n    tp_state_dicts = get_megatron_sharded_states(args, tp_size, pp_size, 0)\n    position_embeddings = get_element_from_dict_by_path(tp_state_dicts[0], 'model.language_model.embedding.position_embeddings.weight')\n    output_state_dict['transformer.wpe.weight'] = position_embeddings.to(dtype)\n    word_embeddings = torch.cat([get_element_from_dict_by_path(tp_state_dicts[tp_rank], 'model.language_model.embedding.word_embeddings.weight') for tp_rank in range(tp_size)], dim=0)\n    word_embeddings = word_embeddings[:vocab_size].to(dtype)\n    output_state_dict['transformer.wte.weight'] = word_embeddings\n    print('Converting transformer layers')\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    n_positions = config.n_positions\n    num_layers = config.num_hidden_layers // pp_size\n    for pp_rank in range(pp_size):\n        if pp_size > 0:\n            print(f'Converting pipeline parallel rank {pp_rank}')\n            tp_state_dicts = get_megatron_sharded_states(args, tp_size, pp_size, pp_rank)\n        path = 'model.language_model.transformer' if 'transformer' in get_element_from_dict_by_path(tp_state_dicts[0], 'model.language_model').keys() else 'model.language_model.encoder'\n        for (key, val) in get_element_from_dict_by_path(tp_state_dicts[0], path).items():\n            m = layer_re.match(key)\n            if m is None:\n                break\n            layer_idx = int(m.group(1)) + pp_rank * num_layers\n            op_name = m.group(2)\n            weight_or_bias = m.group(3)\n            layer_name = f'transformer.h.{layer_idx}'\n            if op_name + '.' + weight_or_bias not in tensor_parallel_params:\n                params = val.to(dtype)\n            else:\n                dim = 1 if op_name in ['self_attention.dense', 'mlp.dense_4h_to_h', 'attention.dense'] else 0\n                params = torch.cat([val] + [get_element_from_dict_by_path(tp_state_dicts[tp_rank], f'{path}')[key] for tp_rank in range(1, tp_size)], dim=dim).to(dtype)\n            if op_name.endswith('layernorm'):\n                ln_name = 'ln_1' if op_name.startswith('input') else 'ln_2'\n                output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = params\n            elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n                causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=dtype)).view(1, 1, n_positions, n_positions)\n                output_state_dict[layer_name + '.attn.bias'] = causal_mask\n                masked_bias = torch.tensor(-10000.0, dtype=dtype)\n                output_state_dict[layer_name + '.attn.masked_bias'] = masked_bias\n                out_val = megatron_to_transformers_fix_query_key_value_ordering(params, checkpoint_version, 3, heads, hidden_size_per_head)\n                out_val = out_val.transpose(0, 1).contiguous()\n                output_state_dict[layer_name + '.attn.c_attn.weight'] = out_val\n            elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n                out_val = megatron_to_transformers_fix_query_key_value_ordering(params, checkpoint_version, 3, heads, hidden_size_per_head)\n                output_state_dict[layer_name + '.attn.c_attn.bias'] = out_val\n            elif weight_or_bias == 'weight':\n                out_name = megatron_to_transformers[op_name]\n                output_state_dict[layer_name + out_name + 'weight'] = params.transpose(0, 1)\n            elif weight_or_bias == 'bias':\n                out_name = megatron_to_transformers[op_name]\n                output_state_dict[layer_name + out_name + 'bias'] = params\n    if config.n_layer != layer_idx + 1:\n        raise ValueError(f'Expected {config.n_layer} layers but found {layer_idx + 1}')\n    print('Converting final layernorm')\n    params = get_element_from_dict_by_path(tp_state_dicts[0], str(path))\n    output_state_dict['transformer.ln_f.weight'] = params['final_layernorm.weight'].to(dtype)\n    output_state_dict['transformer.ln_f.bias'] = params['final_layernorm.bias'].to(dtype)\n    print('Converting LM head')\n    output_state_dict['lm_head.weight'] = word_embeddings.to(dtype)\n    print('Conversion from Megatron-LM to Transformers is done!')\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    if args.tokenizer_name is None:\n        tokenizer_name = 'gpt2'\n    else:\n        tokenizer_name = args.tokenizer_name\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    tokenizer_class = type(tokenizer).__name__\n    config.tokenizer_class = tokenizer_class\n    print('Saving config')\n    config.save_pretrained(args.save_path)\n    if args.tokenizer_name is not None:\n        print(f'Adding {tokenizer_class} tokenizer files')\n        tokenizer.save_pretrained(args.save_path)\n    max_shard_size = int(args.max_shard_size) if args.max_shard_size.isdigit() else args.max_shard_size\n    (shards, index) = shard_checkpoint(output_state_dict, max_shard_size=max_shard_size)\n    for (shard_file, shard) in shards.items():\n        torch.save(shard, os.path.join(args.save_path, shard_file))\n    if index is None:\n        print(f'Model weights saved in {os.path.join(args.save_path, WEIGHTS_NAME)}')\n    else:\n        save_index_file = os.path.join(args.save_path, WEIGHTS_INDEX_NAME)\n        with open(save_index_file, 'w', encoding='utf-8') as f:\n            content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n            f.write(content)\n        print(f'The model is bigger than the maximum size per checkpoint ({args.max_shard_size}) and is going to be split in {len(shards)} checkpoint shards. You can find where each parameters has been saved in the index located at {save_index_file}.')",
            "def convert_checkpoint_from_megatron_to_transformers(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Convert NVIDIA Megatron-LM checkpoint to HuggingFace Transformers checkpoint. This handles Megatron checkpoints\\n    with different tensor parallelism and pipeline parallelism sizes. It saves the converted checkpoint into shards\\n    using HuggingFace Transformers checkpoint sharding functionality. This greatly extends the functionality of\\n    `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n    '\n    sub_dirs = os.listdir(args.load_path)\n    possible_sub_dirs = ['mp_rank_00', 'mp_rank_00_000']\n    for sub_dir in possible_sub_dirs:\n        if sub_dir in sub_dirs:\n            rank0_checkpoint_name = os.listdir(os.path.join(args.load_path, sub_dir))[0]\n            rank0_checkpoint_path = os.path.join(args.load_path, sub_dir, rank0_checkpoint_name)\n            break\n    print(f'Loading Megatron-LM checkpoint arguments from: {rank0_checkpoint_path}')\n    state_dict = torch.load(rank0_checkpoint_path, map_location='cpu')\n    megatron_args = state_dict.get('args', None)\n    if megatron_args is None:\n        raise ValueError('Megatron-LM checkpoint does not contain arguments. This utility only supports Megatron-LM checkpoints containing all the megatron arguments. This is because it loads all config related to model architecture, the tensor and pipeline model parallel size from the checkpoint insead of user having to manually specify all the details. Please save Megatron-LM checkpoint along with all the megatron arguments to use this utility.')\n    if megatron_args is not None:\n        if megatron_args.bias_gelu_fusion:\n            activation_function = 'gelu_fast'\n        elif megatron_args.openai_gelu:\n            activation_function = 'gelu_new'\n        else:\n            activation_function = 'gelu'\n    else:\n        activation_function = 'gelu_new'\n    vocab_size = megatron_args.padded_vocab_size if getattr(megatron_args, 'orig_vocab_size', None) is None else megatron_args.orig_vocab_size\n    print(vocab_size)\n    config = GPT2Config(vocab_size=vocab_size, n_positions=megatron_args.max_position_embeddings, n_embd=megatron_args.hidden_size, n_layer=megatron_args.num_layers, n_head=megatron_args.num_attention_heads, n_inner=megatron_args.ffn_hidden_size, activation_function=activation_function, resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, scale_attn_weights=True, use_cache=True, bos_token_id=vocab_size - 1, eos_token_id=vocab_size - 1, architectures=['GPT2LMHeadModel'])\n    output_state_dict = {}\n    checkpoint_version = state_dict.get('checkpoint_version', 0.0)\n    tp_size = megatron_args.tensor_model_parallel_size\n    pp_size = megatron_args.pipeline_model_parallel_size\n    dtype = torch.float32\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    print('Converting')\n    print('Converting embeddings')\n    tp_state_dicts = get_megatron_sharded_states(args, tp_size, pp_size, 0)\n    position_embeddings = get_element_from_dict_by_path(tp_state_dicts[0], 'model.language_model.embedding.position_embeddings.weight')\n    output_state_dict['transformer.wpe.weight'] = position_embeddings.to(dtype)\n    word_embeddings = torch.cat([get_element_from_dict_by_path(tp_state_dicts[tp_rank], 'model.language_model.embedding.word_embeddings.weight') for tp_rank in range(tp_size)], dim=0)\n    word_embeddings = word_embeddings[:vocab_size].to(dtype)\n    output_state_dict['transformer.wte.weight'] = word_embeddings\n    print('Converting transformer layers')\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    n_positions = config.n_positions\n    num_layers = config.num_hidden_layers // pp_size\n    for pp_rank in range(pp_size):\n        if pp_size > 0:\n            print(f'Converting pipeline parallel rank {pp_rank}')\n            tp_state_dicts = get_megatron_sharded_states(args, tp_size, pp_size, pp_rank)\n        path = 'model.language_model.transformer' if 'transformer' in get_element_from_dict_by_path(tp_state_dicts[0], 'model.language_model').keys() else 'model.language_model.encoder'\n        for (key, val) in get_element_from_dict_by_path(tp_state_dicts[0], path).items():\n            m = layer_re.match(key)\n            if m is None:\n                break\n            layer_idx = int(m.group(1)) + pp_rank * num_layers\n            op_name = m.group(2)\n            weight_or_bias = m.group(3)\n            layer_name = f'transformer.h.{layer_idx}'\n            if op_name + '.' + weight_or_bias not in tensor_parallel_params:\n                params = val.to(dtype)\n            else:\n                dim = 1 if op_name in ['self_attention.dense', 'mlp.dense_4h_to_h', 'attention.dense'] else 0\n                params = torch.cat([val] + [get_element_from_dict_by_path(tp_state_dicts[tp_rank], f'{path}')[key] for tp_rank in range(1, tp_size)], dim=dim).to(dtype)\n            if op_name.endswith('layernorm'):\n                ln_name = 'ln_1' if op_name.startswith('input') else 'ln_2'\n                output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = params\n            elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n                causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=dtype)).view(1, 1, n_positions, n_positions)\n                output_state_dict[layer_name + '.attn.bias'] = causal_mask\n                masked_bias = torch.tensor(-10000.0, dtype=dtype)\n                output_state_dict[layer_name + '.attn.masked_bias'] = masked_bias\n                out_val = megatron_to_transformers_fix_query_key_value_ordering(params, checkpoint_version, 3, heads, hidden_size_per_head)\n                out_val = out_val.transpose(0, 1).contiguous()\n                output_state_dict[layer_name + '.attn.c_attn.weight'] = out_val\n            elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n                out_val = megatron_to_transformers_fix_query_key_value_ordering(params, checkpoint_version, 3, heads, hidden_size_per_head)\n                output_state_dict[layer_name + '.attn.c_attn.bias'] = out_val\n            elif weight_or_bias == 'weight':\n                out_name = megatron_to_transformers[op_name]\n                output_state_dict[layer_name + out_name + 'weight'] = params.transpose(0, 1)\n            elif weight_or_bias == 'bias':\n                out_name = megatron_to_transformers[op_name]\n                output_state_dict[layer_name + out_name + 'bias'] = params\n    if config.n_layer != layer_idx + 1:\n        raise ValueError(f'Expected {config.n_layer} layers but found {layer_idx + 1}')\n    print('Converting final layernorm')\n    params = get_element_from_dict_by_path(tp_state_dicts[0], str(path))\n    output_state_dict['transformer.ln_f.weight'] = params['final_layernorm.weight'].to(dtype)\n    output_state_dict['transformer.ln_f.bias'] = params['final_layernorm.bias'].to(dtype)\n    print('Converting LM head')\n    output_state_dict['lm_head.weight'] = word_embeddings.to(dtype)\n    print('Conversion from Megatron-LM to Transformers is done!')\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    if args.tokenizer_name is None:\n        tokenizer_name = 'gpt2'\n    else:\n        tokenizer_name = args.tokenizer_name\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    tokenizer_class = type(tokenizer).__name__\n    config.tokenizer_class = tokenizer_class\n    print('Saving config')\n    config.save_pretrained(args.save_path)\n    if args.tokenizer_name is not None:\n        print(f'Adding {tokenizer_class} tokenizer files')\n        tokenizer.save_pretrained(args.save_path)\n    max_shard_size = int(args.max_shard_size) if args.max_shard_size.isdigit() else args.max_shard_size\n    (shards, index) = shard_checkpoint(output_state_dict, max_shard_size=max_shard_size)\n    for (shard_file, shard) in shards.items():\n        torch.save(shard, os.path.join(args.save_path, shard_file))\n    if index is None:\n        print(f'Model weights saved in {os.path.join(args.save_path, WEIGHTS_NAME)}')\n    else:\n        save_index_file = os.path.join(args.save_path, WEIGHTS_INDEX_NAME)\n        with open(save_index_file, 'w', encoding='utf-8') as f:\n            content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n            f.write(content)\n        print(f'The model is bigger than the maximum size per checkpoint ({args.max_shard_size}) and is going to be split in {len(shards)} checkpoint shards. You can find where each parameters has been saved in the index located at {save_index_file}.')",
            "def convert_checkpoint_from_megatron_to_transformers(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Convert NVIDIA Megatron-LM checkpoint to HuggingFace Transformers checkpoint. This handles Megatron checkpoints\\n    with different tensor parallelism and pipeline parallelism sizes. It saves the converted checkpoint into shards\\n    using HuggingFace Transformers checkpoint sharding functionality. This greatly extends the functionality of\\n    `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n    '\n    sub_dirs = os.listdir(args.load_path)\n    possible_sub_dirs = ['mp_rank_00', 'mp_rank_00_000']\n    for sub_dir in possible_sub_dirs:\n        if sub_dir in sub_dirs:\n            rank0_checkpoint_name = os.listdir(os.path.join(args.load_path, sub_dir))[0]\n            rank0_checkpoint_path = os.path.join(args.load_path, sub_dir, rank0_checkpoint_name)\n            break\n    print(f'Loading Megatron-LM checkpoint arguments from: {rank0_checkpoint_path}')\n    state_dict = torch.load(rank0_checkpoint_path, map_location='cpu')\n    megatron_args = state_dict.get('args', None)\n    if megatron_args is None:\n        raise ValueError('Megatron-LM checkpoint does not contain arguments. This utility only supports Megatron-LM checkpoints containing all the megatron arguments. This is because it loads all config related to model architecture, the tensor and pipeline model parallel size from the checkpoint insead of user having to manually specify all the details. Please save Megatron-LM checkpoint along with all the megatron arguments to use this utility.')\n    if megatron_args is not None:\n        if megatron_args.bias_gelu_fusion:\n            activation_function = 'gelu_fast'\n        elif megatron_args.openai_gelu:\n            activation_function = 'gelu_new'\n        else:\n            activation_function = 'gelu'\n    else:\n        activation_function = 'gelu_new'\n    vocab_size = megatron_args.padded_vocab_size if getattr(megatron_args, 'orig_vocab_size', None) is None else megatron_args.orig_vocab_size\n    print(vocab_size)\n    config = GPT2Config(vocab_size=vocab_size, n_positions=megatron_args.max_position_embeddings, n_embd=megatron_args.hidden_size, n_layer=megatron_args.num_layers, n_head=megatron_args.num_attention_heads, n_inner=megatron_args.ffn_hidden_size, activation_function=activation_function, resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, scale_attn_weights=True, use_cache=True, bos_token_id=vocab_size - 1, eos_token_id=vocab_size - 1, architectures=['GPT2LMHeadModel'])\n    output_state_dict = {}\n    checkpoint_version = state_dict.get('checkpoint_version', 0.0)\n    tp_size = megatron_args.tensor_model_parallel_size\n    pp_size = megatron_args.pipeline_model_parallel_size\n    dtype = torch.float32\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    print('Converting')\n    print('Converting embeddings')\n    tp_state_dicts = get_megatron_sharded_states(args, tp_size, pp_size, 0)\n    position_embeddings = get_element_from_dict_by_path(tp_state_dicts[0], 'model.language_model.embedding.position_embeddings.weight')\n    output_state_dict['transformer.wpe.weight'] = position_embeddings.to(dtype)\n    word_embeddings = torch.cat([get_element_from_dict_by_path(tp_state_dicts[tp_rank], 'model.language_model.embedding.word_embeddings.weight') for tp_rank in range(tp_size)], dim=0)\n    word_embeddings = word_embeddings[:vocab_size].to(dtype)\n    output_state_dict['transformer.wte.weight'] = word_embeddings\n    print('Converting transformer layers')\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    n_positions = config.n_positions\n    num_layers = config.num_hidden_layers // pp_size\n    for pp_rank in range(pp_size):\n        if pp_size > 0:\n            print(f'Converting pipeline parallel rank {pp_rank}')\n            tp_state_dicts = get_megatron_sharded_states(args, tp_size, pp_size, pp_rank)\n        path = 'model.language_model.transformer' if 'transformer' in get_element_from_dict_by_path(tp_state_dicts[0], 'model.language_model').keys() else 'model.language_model.encoder'\n        for (key, val) in get_element_from_dict_by_path(tp_state_dicts[0], path).items():\n            m = layer_re.match(key)\n            if m is None:\n                break\n            layer_idx = int(m.group(1)) + pp_rank * num_layers\n            op_name = m.group(2)\n            weight_or_bias = m.group(3)\n            layer_name = f'transformer.h.{layer_idx}'\n            if op_name + '.' + weight_or_bias not in tensor_parallel_params:\n                params = val.to(dtype)\n            else:\n                dim = 1 if op_name in ['self_attention.dense', 'mlp.dense_4h_to_h', 'attention.dense'] else 0\n                params = torch.cat([val] + [get_element_from_dict_by_path(tp_state_dicts[tp_rank], f'{path}')[key] for tp_rank in range(1, tp_size)], dim=dim).to(dtype)\n            if op_name.endswith('layernorm'):\n                ln_name = 'ln_1' if op_name.startswith('input') else 'ln_2'\n                output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = params\n            elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n                causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=dtype)).view(1, 1, n_positions, n_positions)\n                output_state_dict[layer_name + '.attn.bias'] = causal_mask\n                masked_bias = torch.tensor(-10000.0, dtype=dtype)\n                output_state_dict[layer_name + '.attn.masked_bias'] = masked_bias\n                out_val = megatron_to_transformers_fix_query_key_value_ordering(params, checkpoint_version, 3, heads, hidden_size_per_head)\n                out_val = out_val.transpose(0, 1).contiguous()\n                output_state_dict[layer_name + '.attn.c_attn.weight'] = out_val\n            elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n                out_val = megatron_to_transformers_fix_query_key_value_ordering(params, checkpoint_version, 3, heads, hidden_size_per_head)\n                output_state_dict[layer_name + '.attn.c_attn.bias'] = out_val\n            elif weight_or_bias == 'weight':\n                out_name = megatron_to_transformers[op_name]\n                output_state_dict[layer_name + out_name + 'weight'] = params.transpose(0, 1)\n            elif weight_or_bias == 'bias':\n                out_name = megatron_to_transformers[op_name]\n                output_state_dict[layer_name + out_name + 'bias'] = params\n    if config.n_layer != layer_idx + 1:\n        raise ValueError(f'Expected {config.n_layer} layers but found {layer_idx + 1}')\n    print('Converting final layernorm')\n    params = get_element_from_dict_by_path(tp_state_dicts[0], str(path))\n    output_state_dict['transformer.ln_f.weight'] = params['final_layernorm.weight'].to(dtype)\n    output_state_dict['transformer.ln_f.bias'] = params['final_layernorm.bias'].to(dtype)\n    print('Converting LM head')\n    output_state_dict['lm_head.weight'] = word_embeddings.to(dtype)\n    print('Conversion from Megatron-LM to Transformers is done!')\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    if args.tokenizer_name is None:\n        tokenizer_name = 'gpt2'\n    else:\n        tokenizer_name = args.tokenizer_name\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    tokenizer_class = type(tokenizer).__name__\n    config.tokenizer_class = tokenizer_class\n    print('Saving config')\n    config.save_pretrained(args.save_path)\n    if args.tokenizer_name is not None:\n        print(f'Adding {tokenizer_class} tokenizer files')\n        tokenizer.save_pretrained(args.save_path)\n    max_shard_size = int(args.max_shard_size) if args.max_shard_size.isdigit() else args.max_shard_size\n    (shards, index) = shard_checkpoint(output_state_dict, max_shard_size=max_shard_size)\n    for (shard_file, shard) in shards.items():\n        torch.save(shard, os.path.join(args.save_path, shard_file))\n    if index is None:\n        print(f'Model weights saved in {os.path.join(args.save_path, WEIGHTS_NAME)}')\n    else:\n        save_index_file = os.path.join(args.save_path, WEIGHTS_INDEX_NAME)\n        with open(save_index_file, 'w', encoding='utf-8') as f:\n            content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n            f.write(content)\n        print(f'The model is bigger than the maximum size per checkpoint ({args.max_shard_size}) and is going to be split in {len(shards)} checkpoint shards. You can find where each parameters has been saved in the index located at {save_index_file}.')",
            "def convert_checkpoint_from_megatron_to_transformers(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Convert NVIDIA Megatron-LM checkpoint to HuggingFace Transformers checkpoint. This handles Megatron checkpoints\\n    with different tensor parallelism and pipeline parallelism sizes. It saves the converted checkpoint into shards\\n    using HuggingFace Transformers checkpoint sharding functionality. This greatly extends the functionality of\\n    `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n    '\n    sub_dirs = os.listdir(args.load_path)\n    possible_sub_dirs = ['mp_rank_00', 'mp_rank_00_000']\n    for sub_dir in possible_sub_dirs:\n        if sub_dir in sub_dirs:\n            rank0_checkpoint_name = os.listdir(os.path.join(args.load_path, sub_dir))[0]\n            rank0_checkpoint_path = os.path.join(args.load_path, sub_dir, rank0_checkpoint_name)\n            break\n    print(f'Loading Megatron-LM checkpoint arguments from: {rank0_checkpoint_path}')\n    state_dict = torch.load(rank0_checkpoint_path, map_location='cpu')\n    megatron_args = state_dict.get('args', None)\n    if megatron_args is None:\n        raise ValueError('Megatron-LM checkpoint does not contain arguments. This utility only supports Megatron-LM checkpoints containing all the megatron arguments. This is because it loads all config related to model architecture, the tensor and pipeline model parallel size from the checkpoint insead of user having to manually specify all the details. Please save Megatron-LM checkpoint along with all the megatron arguments to use this utility.')\n    if megatron_args is not None:\n        if megatron_args.bias_gelu_fusion:\n            activation_function = 'gelu_fast'\n        elif megatron_args.openai_gelu:\n            activation_function = 'gelu_new'\n        else:\n            activation_function = 'gelu'\n    else:\n        activation_function = 'gelu_new'\n    vocab_size = megatron_args.padded_vocab_size if getattr(megatron_args, 'orig_vocab_size', None) is None else megatron_args.orig_vocab_size\n    print(vocab_size)\n    config = GPT2Config(vocab_size=vocab_size, n_positions=megatron_args.max_position_embeddings, n_embd=megatron_args.hidden_size, n_layer=megatron_args.num_layers, n_head=megatron_args.num_attention_heads, n_inner=megatron_args.ffn_hidden_size, activation_function=activation_function, resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, scale_attn_weights=True, use_cache=True, bos_token_id=vocab_size - 1, eos_token_id=vocab_size - 1, architectures=['GPT2LMHeadModel'])\n    output_state_dict = {}\n    checkpoint_version = state_dict.get('checkpoint_version', 0.0)\n    tp_size = megatron_args.tensor_model_parallel_size\n    pp_size = megatron_args.pipeline_model_parallel_size\n    dtype = torch.float32\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    print('Converting')\n    print('Converting embeddings')\n    tp_state_dicts = get_megatron_sharded_states(args, tp_size, pp_size, 0)\n    position_embeddings = get_element_from_dict_by_path(tp_state_dicts[0], 'model.language_model.embedding.position_embeddings.weight')\n    output_state_dict['transformer.wpe.weight'] = position_embeddings.to(dtype)\n    word_embeddings = torch.cat([get_element_from_dict_by_path(tp_state_dicts[tp_rank], 'model.language_model.embedding.word_embeddings.weight') for tp_rank in range(tp_size)], dim=0)\n    word_embeddings = word_embeddings[:vocab_size].to(dtype)\n    output_state_dict['transformer.wte.weight'] = word_embeddings\n    print('Converting transformer layers')\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    n_positions = config.n_positions\n    num_layers = config.num_hidden_layers // pp_size\n    for pp_rank in range(pp_size):\n        if pp_size > 0:\n            print(f'Converting pipeline parallel rank {pp_rank}')\n            tp_state_dicts = get_megatron_sharded_states(args, tp_size, pp_size, pp_rank)\n        path = 'model.language_model.transformer' if 'transformer' in get_element_from_dict_by_path(tp_state_dicts[0], 'model.language_model').keys() else 'model.language_model.encoder'\n        for (key, val) in get_element_from_dict_by_path(tp_state_dicts[0], path).items():\n            m = layer_re.match(key)\n            if m is None:\n                break\n            layer_idx = int(m.group(1)) + pp_rank * num_layers\n            op_name = m.group(2)\n            weight_or_bias = m.group(3)\n            layer_name = f'transformer.h.{layer_idx}'\n            if op_name + '.' + weight_or_bias not in tensor_parallel_params:\n                params = val.to(dtype)\n            else:\n                dim = 1 if op_name in ['self_attention.dense', 'mlp.dense_4h_to_h', 'attention.dense'] else 0\n                params = torch.cat([val] + [get_element_from_dict_by_path(tp_state_dicts[tp_rank], f'{path}')[key] for tp_rank in range(1, tp_size)], dim=dim).to(dtype)\n            if op_name.endswith('layernorm'):\n                ln_name = 'ln_1' if op_name.startswith('input') else 'ln_2'\n                output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = params\n            elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n                causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=dtype)).view(1, 1, n_positions, n_positions)\n                output_state_dict[layer_name + '.attn.bias'] = causal_mask\n                masked_bias = torch.tensor(-10000.0, dtype=dtype)\n                output_state_dict[layer_name + '.attn.masked_bias'] = masked_bias\n                out_val = megatron_to_transformers_fix_query_key_value_ordering(params, checkpoint_version, 3, heads, hidden_size_per_head)\n                out_val = out_val.transpose(0, 1).contiguous()\n                output_state_dict[layer_name + '.attn.c_attn.weight'] = out_val\n            elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n                out_val = megatron_to_transformers_fix_query_key_value_ordering(params, checkpoint_version, 3, heads, hidden_size_per_head)\n                output_state_dict[layer_name + '.attn.c_attn.bias'] = out_val\n            elif weight_or_bias == 'weight':\n                out_name = megatron_to_transformers[op_name]\n                output_state_dict[layer_name + out_name + 'weight'] = params.transpose(0, 1)\n            elif weight_or_bias == 'bias':\n                out_name = megatron_to_transformers[op_name]\n                output_state_dict[layer_name + out_name + 'bias'] = params\n    if config.n_layer != layer_idx + 1:\n        raise ValueError(f'Expected {config.n_layer} layers but found {layer_idx + 1}')\n    print('Converting final layernorm')\n    params = get_element_from_dict_by_path(tp_state_dicts[0], str(path))\n    output_state_dict['transformer.ln_f.weight'] = params['final_layernorm.weight'].to(dtype)\n    output_state_dict['transformer.ln_f.bias'] = params['final_layernorm.bias'].to(dtype)\n    print('Converting LM head')\n    output_state_dict['lm_head.weight'] = word_embeddings.to(dtype)\n    print('Conversion from Megatron-LM to Transformers is done!')\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    if args.tokenizer_name is None:\n        tokenizer_name = 'gpt2'\n    else:\n        tokenizer_name = args.tokenizer_name\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    tokenizer_class = type(tokenizer).__name__\n    config.tokenizer_class = tokenizer_class\n    print('Saving config')\n    config.save_pretrained(args.save_path)\n    if args.tokenizer_name is not None:\n        print(f'Adding {tokenizer_class} tokenizer files')\n        tokenizer.save_pretrained(args.save_path)\n    max_shard_size = int(args.max_shard_size) if args.max_shard_size.isdigit() else args.max_shard_size\n    (shards, index) = shard_checkpoint(output_state_dict, max_shard_size=max_shard_size)\n    for (shard_file, shard) in shards.items():\n        torch.save(shard, os.path.join(args.save_path, shard_file))\n    if index is None:\n        print(f'Model weights saved in {os.path.join(args.save_path, WEIGHTS_NAME)}')\n    else:\n        save_index_file = os.path.join(args.save_path, WEIGHTS_INDEX_NAME)\n        with open(save_index_file, 'w', encoding='utf-8') as f:\n            content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n            f.write(content)\n        print(f'The model is bigger than the maximum size per checkpoint ({args.max_shard_size}) and is going to be split in {len(shards)} checkpoint shards. You can find where each parameters has been saved in the index located at {save_index_file}.')",
            "def convert_checkpoint_from_megatron_to_transformers(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Convert NVIDIA Megatron-LM checkpoint to HuggingFace Transformers checkpoint. This handles Megatron checkpoints\\n    with different tensor parallelism and pipeline parallelism sizes. It saves the converted checkpoint into shards\\n    using HuggingFace Transformers checkpoint sharding functionality. This greatly extends the functionality of\\n    `convert_megatron_gpt2_checkpoint.py`\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n    '\n    sub_dirs = os.listdir(args.load_path)\n    possible_sub_dirs = ['mp_rank_00', 'mp_rank_00_000']\n    for sub_dir in possible_sub_dirs:\n        if sub_dir in sub_dirs:\n            rank0_checkpoint_name = os.listdir(os.path.join(args.load_path, sub_dir))[0]\n            rank0_checkpoint_path = os.path.join(args.load_path, sub_dir, rank0_checkpoint_name)\n            break\n    print(f'Loading Megatron-LM checkpoint arguments from: {rank0_checkpoint_path}')\n    state_dict = torch.load(rank0_checkpoint_path, map_location='cpu')\n    megatron_args = state_dict.get('args', None)\n    if megatron_args is None:\n        raise ValueError('Megatron-LM checkpoint does not contain arguments. This utility only supports Megatron-LM checkpoints containing all the megatron arguments. This is because it loads all config related to model architecture, the tensor and pipeline model parallel size from the checkpoint insead of user having to manually specify all the details. Please save Megatron-LM checkpoint along with all the megatron arguments to use this utility.')\n    if megatron_args is not None:\n        if megatron_args.bias_gelu_fusion:\n            activation_function = 'gelu_fast'\n        elif megatron_args.openai_gelu:\n            activation_function = 'gelu_new'\n        else:\n            activation_function = 'gelu'\n    else:\n        activation_function = 'gelu_new'\n    vocab_size = megatron_args.padded_vocab_size if getattr(megatron_args, 'orig_vocab_size', None) is None else megatron_args.orig_vocab_size\n    print(vocab_size)\n    config = GPT2Config(vocab_size=vocab_size, n_positions=megatron_args.max_position_embeddings, n_embd=megatron_args.hidden_size, n_layer=megatron_args.num_layers, n_head=megatron_args.num_attention_heads, n_inner=megatron_args.ffn_hidden_size, activation_function=activation_function, resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, scale_attn_weights=True, use_cache=True, bos_token_id=vocab_size - 1, eos_token_id=vocab_size - 1, architectures=['GPT2LMHeadModel'])\n    output_state_dict = {}\n    checkpoint_version = state_dict.get('checkpoint_version', 0.0)\n    tp_size = megatron_args.tensor_model_parallel_size\n    pp_size = megatron_args.pipeline_model_parallel_size\n    dtype = torch.float32\n    layer_re = re.compile('layers\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    print('Converting')\n    print('Converting embeddings')\n    tp_state_dicts = get_megatron_sharded_states(args, tp_size, pp_size, 0)\n    position_embeddings = get_element_from_dict_by_path(tp_state_dicts[0], 'model.language_model.embedding.position_embeddings.weight')\n    output_state_dict['transformer.wpe.weight'] = position_embeddings.to(dtype)\n    word_embeddings = torch.cat([get_element_from_dict_by_path(tp_state_dicts[tp_rank], 'model.language_model.embedding.word_embeddings.weight') for tp_rank in range(tp_size)], dim=0)\n    word_embeddings = word_embeddings[:vocab_size].to(dtype)\n    output_state_dict['transformer.wte.weight'] = word_embeddings\n    print('Converting transformer layers')\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    n_positions = config.n_positions\n    num_layers = config.num_hidden_layers // pp_size\n    for pp_rank in range(pp_size):\n        if pp_size > 0:\n            print(f'Converting pipeline parallel rank {pp_rank}')\n            tp_state_dicts = get_megatron_sharded_states(args, tp_size, pp_size, pp_rank)\n        path = 'model.language_model.transformer' if 'transformer' in get_element_from_dict_by_path(tp_state_dicts[0], 'model.language_model').keys() else 'model.language_model.encoder'\n        for (key, val) in get_element_from_dict_by_path(tp_state_dicts[0], path).items():\n            m = layer_re.match(key)\n            if m is None:\n                break\n            layer_idx = int(m.group(1)) + pp_rank * num_layers\n            op_name = m.group(2)\n            weight_or_bias = m.group(3)\n            layer_name = f'transformer.h.{layer_idx}'\n            if op_name + '.' + weight_or_bias not in tensor_parallel_params:\n                params = val.to(dtype)\n            else:\n                dim = 1 if op_name in ['self_attention.dense', 'mlp.dense_4h_to_h', 'attention.dense'] else 0\n                params = torch.cat([val] + [get_element_from_dict_by_path(tp_state_dicts[tp_rank], f'{path}')[key] for tp_rank in range(1, tp_size)], dim=dim).to(dtype)\n            if op_name.endswith('layernorm'):\n                ln_name = 'ln_1' if op_name.startswith('input') else 'ln_2'\n                output_state_dict[layer_name + '.' + ln_name + '.' + weight_or_bias] = params\n            elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'weight':\n                causal_mask = torch.tril(torch.ones((n_positions, n_positions), dtype=dtype)).view(1, 1, n_positions, n_positions)\n                output_state_dict[layer_name + '.attn.bias'] = causal_mask\n                masked_bias = torch.tensor(-10000.0, dtype=dtype)\n                output_state_dict[layer_name + '.attn.masked_bias'] = masked_bias\n                out_val = megatron_to_transformers_fix_query_key_value_ordering(params, checkpoint_version, 3, heads, hidden_size_per_head)\n                out_val = out_val.transpose(0, 1).contiguous()\n                output_state_dict[layer_name + '.attn.c_attn.weight'] = out_val\n            elif (op_name == 'attention.query_key_value' or op_name == 'self_attention.query_key_value') and weight_or_bias == 'bias':\n                out_val = megatron_to_transformers_fix_query_key_value_ordering(params, checkpoint_version, 3, heads, hidden_size_per_head)\n                output_state_dict[layer_name + '.attn.c_attn.bias'] = out_val\n            elif weight_or_bias == 'weight':\n                out_name = megatron_to_transformers[op_name]\n                output_state_dict[layer_name + out_name + 'weight'] = params.transpose(0, 1)\n            elif weight_or_bias == 'bias':\n                out_name = megatron_to_transformers[op_name]\n                output_state_dict[layer_name + out_name + 'bias'] = params\n    if config.n_layer != layer_idx + 1:\n        raise ValueError(f'Expected {config.n_layer} layers but found {layer_idx + 1}')\n    print('Converting final layernorm')\n    params = get_element_from_dict_by_path(tp_state_dicts[0], str(path))\n    output_state_dict['transformer.ln_f.weight'] = params['final_layernorm.weight'].to(dtype)\n    output_state_dict['transformer.ln_f.bias'] = params['final_layernorm.bias'].to(dtype)\n    print('Converting LM head')\n    output_state_dict['lm_head.weight'] = word_embeddings.to(dtype)\n    print('Conversion from Megatron-LM to Transformers is done!')\n    if args.print_checkpoint_structure:\n        recursive_print(None, output_state_dict)\n    if args.tokenizer_name is None:\n        tokenizer_name = 'gpt2'\n    else:\n        tokenizer_name = args.tokenizer_name\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    tokenizer_class = type(tokenizer).__name__\n    config.tokenizer_class = tokenizer_class\n    print('Saving config')\n    config.save_pretrained(args.save_path)\n    if args.tokenizer_name is not None:\n        print(f'Adding {tokenizer_class} tokenizer files')\n        tokenizer.save_pretrained(args.save_path)\n    max_shard_size = int(args.max_shard_size) if args.max_shard_size.isdigit() else args.max_shard_size\n    (shards, index) = shard_checkpoint(output_state_dict, max_shard_size=max_shard_size)\n    for (shard_file, shard) in shards.items():\n        torch.save(shard, os.path.join(args.save_path, shard_file))\n    if index is None:\n        print(f'Model weights saved in {os.path.join(args.save_path, WEIGHTS_NAME)}')\n    else:\n        save_index_file = os.path.join(args.save_path, WEIGHTS_INDEX_NAME)\n        with open(save_index_file, 'w', encoding='utf-8') as f:\n            content = json.dumps(index, indent=2, sort_keys=True) + '\\n'\n            f.write(content)\n        print(f'The model is bigger than the maximum size per checkpoint ({args.max_shard_size}) and is going to be split in {len(shards)} checkpoint shards. You can find where each parameters has been saved in the index located at {save_index_file}.')"
        ]
    },
    {
        "func_name": "convert_checkpoint_from_transformers_to_megatron",
        "original": "def convert_checkpoint_from_transformers_to_megatron(args):\n    \"\"\"\n    Convert a checkpoint from HuggingFace Transformers to Megatron-LM. This allows converted checkpoints with variable\n    tensor parallelism and pipeline parallelism sizes. It takes as input a checkpoint from HuggingFace Transformers\n    which can have multiple shards.\n\n    Args:\n        args (argparse.Namespace): the arguments to the script\n\n    \"\"\"\n    os.makedirs(args.save_path, exist_ok=True)\n    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n    if args.megatron_path is not None:\n        sys.path.insert(0, args.megatron_path)\n    try:\n        from megatron.tokenizer.tokenizer import _vocab_size_with_padding\n    except ModuleNotFoundError:\n        print('Unable to import Megatron, please specify the path to Megatron using --megatron-path. Exiting.')\n        exit(1)\n    sub_dirs = [x for x in os.listdir(args.load_path) if x.startswith('pytorch_model')]\n    if len(sub_dirs) == 1:\n        checkpoint_name = 'pytorch_model.bin'\n        state_dict = torch.load(os.path.join(args.load_path, checkpoint_name), map_location='cpu')\n    else:\n        num_checkpoints = len(sub_dirs) - 1\n        state_dict = merge_transformers_sharded_states(args.load_path, num_checkpoints)\n    config = GPT2Config.from_pretrained(args.load_path)\n    tracker_filepath = os.path.join(args.save_path, 'latest_checkpointed_iteration.txt')\n    with open(tracker_filepath, 'w') as f:\n        f.write('release')\n    release_dir = os.path.join(args.save_path, 'release')\n    os.makedirs(release_dir, exist_ok=True)\n    megatron_args = {'orig_vocab_size': config.vocab_size, 'max_position_embeddings': config.n_positions, 'hidden_size': config.n_embd, 'num_layers': config.n_layer, 'num_attention_heads': config.n_head, 'ffn_hidden_size': config.n_inner, 'tensor_model_parallel_size': args.target_tensor_model_parallel_size, 'pipeline_model_parallel_size': args.target_pipeline_model_parallel_size, 'data_parallel_size': args.target_data_parallel_size, 'make_vocab_size_divisible_by': args.make_vocab_size_divisible_by, 'rank': 0, 'tokenizer_type': 'GPT2BPETokenizer'}\n    if config.activation_function == 'gelu':\n        megatron_args['bias_gelu_fusion'] = False\n        megatron_args['openai_gelu'] = False\n    elif config.activation_function == 'gelu_fast':\n        megatron_args['bias_gelu_fusion'] = True\n        megatron_args['openai_gelu'] = False\n    elif config.activation_function == 'gelu_new':\n        megatron_args['bias_gelu_fusion'] = False\n        megatron_args['openai_gelu'] = True\n    margs = types.SimpleNamespace()\n    for (k, v) in megatron_args.items():\n        setattr(margs, k, v)\n    if args.target_params_dtype == 'fp16':\n        dtype = torch.float16\n    elif args.target_params_dtype == 'bf16':\n        dtype = torch.bfloat16\n    else:\n        dtype = torch.float32\n    setattr(margs, 'params_dtype', dtype)\n    dummy_optim_state_dict = {}\n    dummy_optim_state_dict['optimizer'] = {'step': 0, 'param_groups': [{'lr': 0.0, 'beta1': 0.0, 'beta2': 0.0, 'eps': 0.0, 'weight_decay': 0.0, 'correct_bias': False, 'params': []}]}\n    if args.use_distributed_optimizer:\n        for i in range(args.target_pipeline_model_parallel_size):\n            for j in range(args.target_tensor_model_parallel_size):\n                for k in range(args.target_data_parallel_size):\n                    if args.target_pipeline_model_parallel_size == 1:\n                        checkpoint_dir = f'mp_rank_{j:02d}_{k:03d}'\n                    else:\n                        checkpoint_dir = f'mp_rank_{j:02d}_{i:03d}_{k:03d}'\n                    checkpoint_dir = os.path.join(release_dir, checkpoint_dir)\n                    os.makedirs(checkpoint_dir, exist_ok=True)\n                    torch.save(dummy_optim_state_dict, os.path.join(checkpoint_dir, 'optim.pt'))\n    print('Converting')\n    output_state_dict = []\n    for i in range(args.target_tensor_model_parallel_size):\n        output_state_dict.append({})\n    print('converting embedding layer')\n    pos_embedding = state_dict['transformer.wpe.weight'].to(dtype)\n    word_embedding = state_dict['transformer.wte.weight'].to(dtype)\n    orig_vocab_size = config.vocab_size\n    padded_vocab_size = _vocab_size_with_padding(orig_vocab_size, margs)\n    setattr(margs, 'padded_vocab_size', padded_vocab_size)\n    if orig_vocab_size > padded_vocab_size:\n        full_word_embed = word_embedding[0:padded_vocab_size, :]\n    elif orig_vocab_size < padded_vocab_size:\n        padding_size = padded_vocab_size - orig_vocab_size\n        full_word_embed = torch.cat((word_embedding, word_embedding[-1].unsqueeze(0).expand(padding_size, -1)))\n    else:\n        full_word_embed = word_embedding\n    out_word_embed = torch.chunk(full_word_embed, args.target_tensor_model_parallel_size, dim=0)\n    for i in range(args.target_tensor_model_parallel_size):\n        pos_emb_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.embedding.position_embeddings')\n        pos_emb_dict['weight'] = pos_embedding\n        word_emb_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.embedding.word_embeddings')\n        word_emb_dict['weight'] = out_word_embed[i].clone()\n    print('converting transformer layers')\n    if config.num_attention_heads % args.target_tensor_model_parallel_size != 0:\n        raise ValueError(f'Number of attention heads ({config.num_attention_heads}) must be divisible by number of tensor parallelism ({args.target_tensor_model_parallel_size})')\n    if config.num_hidden_layers % args.target_pipeline_model_parallel_size != 0:\n        raise ValueError(f'Number of layers ({config.num_hidden_layers}) must be divisible by number of pipeline parallelism ({args.target_pipeline_model_parallel_size})')\n    num_layers = config.num_hidden_layers // args.target_pipeline_model_parallel_size\n    layer_re = re.compile('transformer.h\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    for pp_rank in range(args.target_pipeline_model_parallel_size):\n        layer_offset = pp_rank * num_layers\n        if pp_rank > 0:\n            output_state_dict = []\n            for i in range(args.target_tensor_model_parallel_size):\n                output_state_dict.append({})\n        for layer in range(num_layers):\n            pp_layer_id = layer + layer_offset\n            layers_to_copy = [layer_name for layer_name in state_dict.keys() if layer_name.startswith(f'transformer.h.{pp_layer_id}.')]\n            for layer_name in layers_to_copy:\n                m = layer_re.match(layer_name)\n                if m is None:\n                    break\n                _ = int(m.group(1))\n                op_name = m.group(2)\n                weight_or_bias = m.group(3)\n                params = state_dict[layer_name].to(dtype)\n                if op_name.startswith('ln'):\n                    out_name = 'input_layernorm' if op_name.endswith('1') else 'post_attention_layernorm'\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                elif op_name.startswith('attn.c_attn') and weight_or_bias == 'weight':\n                    params = params.transpose(0, 1).contiguous()\n                    params = transformers_to_megatron_fix_query_key_value_ordering(params, 3.0, 3, heads, hidden_size_per_head)\n                    layer_name = f'layers.{layer}.self_attention.query_key_value.{weight_or_bias}'\n                elif op_name.startswith('attn.c_attn') and weight_or_bias == 'bias':\n                    params = transformers_to_megatron_fix_query_key_value_ordering(params, 3.0, 3, heads, hidden_size_per_head)\n                    layer_name = f'layers.{layer}.self_attention.query_key_value.{weight_or_bias}'\n                elif weight_or_bias == 'weight':\n                    out_name = transformers_to_megatron.get(op_name, None)\n                    if out_name is None:\n                        continue\n                    params = params.transpose(0, 1)\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                elif weight_or_bias == 'bias':\n                    out_name = transformers_to_megatron.get(op_name, None)\n                    if out_name is None:\n                        continue\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                else:\n                    continue\n                if op_name + '.' + weight_or_bias in tensor_parallel_params:\n                    dim = 1 if op_name in ['attn.c_proj', 'mlp.c_proj'] else 0\n                    params = torch.chunk(params, args.target_tensor_model_parallel_size, dim=dim)\n                for i in range(args.target_tensor_model_parallel_size):\n                    params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.encoder')\n                    params_dict[layer_name] = params[i].clone() if op_name + '.' + weight_or_bias in tensor_parallel_params else params\n        if pp_rank == args.target_pipeline_model_parallel_size - 1:\n            for weight_or_bias in ['weight', 'bias']:\n                params = state_dict[f'transformer.ln_f.{weight_or_bias}'].to(dtype)\n                layer_name = f'final_layernorm.{weight_or_bias}'\n                for i in range(args.target_tensor_model_parallel_size):\n                    params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.encoder')\n                    params_dict[layer_name] = params\n            for i in range(args.target_tensor_model_parallel_size):\n                params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.word_embeddings_for_head')\n                params_dict['weight'] = out_word_embed[i].clone()\n        for tp_rank in range(args.target_tensor_model_parallel_size):\n            output_state_dict[tp_rank]['checkpoint_version'] = 3.0\n            output_state_dict[tp_rank]['args'] = margs\n            checkpoint_dir = f'mp_rank_{tp_rank:02d}' if args.target_pipeline_model_parallel_size == 1 else f'mp_rank_{tp_rank:02d}_{pp_rank:03d}'\n            if args.use_distributed_optimizer:\n                checkpoint_name = 'model_rng.pt'\n            else:\n                checkpoint_name = 'model_optim_rng.pt'\n                output_state_dict[tp_rank]['optimizer'] = dummy_optim_state_dict['optimizer']\n            checkpoint_dir = os.path.join(release_dir, checkpoint_dir)\n            os.makedirs(checkpoint_dir, exist_ok=True)\n            checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n            if args.print_checkpoint_structure:\n                print(f'Checkpoint structure of model state dict shard belonging to TP rank {tp_rank} and PP rank {pp_rank}:')\n                recursive_print(None, output_state_dict[tp_rank])\n            torch.save(output_state_dict[tp_rank], checkpoint_path)",
        "mutated": [
            "def convert_checkpoint_from_transformers_to_megatron(args):\n    if False:\n        i = 10\n    '\\n    Convert a checkpoint from HuggingFace Transformers to Megatron-LM. This allows converted checkpoints with variable\\n    tensor parallelism and pipeline parallelism sizes. It takes as input a checkpoint from HuggingFace Transformers\\n    which can have multiple shards.\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n\\n    '\n    os.makedirs(args.save_path, exist_ok=True)\n    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n    if args.megatron_path is not None:\n        sys.path.insert(0, args.megatron_path)\n    try:\n        from megatron.tokenizer.tokenizer import _vocab_size_with_padding\n    except ModuleNotFoundError:\n        print('Unable to import Megatron, please specify the path to Megatron using --megatron-path. Exiting.')\n        exit(1)\n    sub_dirs = [x for x in os.listdir(args.load_path) if x.startswith('pytorch_model')]\n    if len(sub_dirs) == 1:\n        checkpoint_name = 'pytorch_model.bin'\n        state_dict = torch.load(os.path.join(args.load_path, checkpoint_name), map_location='cpu')\n    else:\n        num_checkpoints = len(sub_dirs) - 1\n        state_dict = merge_transformers_sharded_states(args.load_path, num_checkpoints)\n    config = GPT2Config.from_pretrained(args.load_path)\n    tracker_filepath = os.path.join(args.save_path, 'latest_checkpointed_iteration.txt')\n    with open(tracker_filepath, 'w') as f:\n        f.write('release')\n    release_dir = os.path.join(args.save_path, 'release')\n    os.makedirs(release_dir, exist_ok=True)\n    megatron_args = {'orig_vocab_size': config.vocab_size, 'max_position_embeddings': config.n_positions, 'hidden_size': config.n_embd, 'num_layers': config.n_layer, 'num_attention_heads': config.n_head, 'ffn_hidden_size': config.n_inner, 'tensor_model_parallel_size': args.target_tensor_model_parallel_size, 'pipeline_model_parallel_size': args.target_pipeline_model_parallel_size, 'data_parallel_size': args.target_data_parallel_size, 'make_vocab_size_divisible_by': args.make_vocab_size_divisible_by, 'rank': 0, 'tokenizer_type': 'GPT2BPETokenizer'}\n    if config.activation_function == 'gelu':\n        megatron_args['bias_gelu_fusion'] = False\n        megatron_args['openai_gelu'] = False\n    elif config.activation_function == 'gelu_fast':\n        megatron_args['bias_gelu_fusion'] = True\n        megatron_args['openai_gelu'] = False\n    elif config.activation_function == 'gelu_new':\n        megatron_args['bias_gelu_fusion'] = False\n        megatron_args['openai_gelu'] = True\n    margs = types.SimpleNamespace()\n    for (k, v) in megatron_args.items():\n        setattr(margs, k, v)\n    if args.target_params_dtype == 'fp16':\n        dtype = torch.float16\n    elif args.target_params_dtype == 'bf16':\n        dtype = torch.bfloat16\n    else:\n        dtype = torch.float32\n    setattr(margs, 'params_dtype', dtype)\n    dummy_optim_state_dict = {}\n    dummy_optim_state_dict['optimizer'] = {'step': 0, 'param_groups': [{'lr': 0.0, 'beta1': 0.0, 'beta2': 0.0, 'eps': 0.0, 'weight_decay': 0.0, 'correct_bias': False, 'params': []}]}\n    if args.use_distributed_optimizer:\n        for i in range(args.target_pipeline_model_parallel_size):\n            for j in range(args.target_tensor_model_parallel_size):\n                for k in range(args.target_data_parallel_size):\n                    if args.target_pipeline_model_parallel_size == 1:\n                        checkpoint_dir = f'mp_rank_{j:02d}_{k:03d}'\n                    else:\n                        checkpoint_dir = f'mp_rank_{j:02d}_{i:03d}_{k:03d}'\n                    checkpoint_dir = os.path.join(release_dir, checkpoint_dir)\n                    os.makedirs(checkpoint_dir, exist_ok=True)\n                    torch.save(dummy_optim_state_dict, os.path.join(checkpoint_dir, 'optim.pt'))\n    print('Converting')\n    output_state_dict = []\n    for i in range(args.target_tensor_model_parallel_size):\n        output_state_dict.append({})\n    print('converting embedding layer')\n    pos_embedding = state_dict['transformer.wpe.weight'].to(dtype)\n    word_embedding = state_dict['transformer.wte.weight'].to(dtype)\n    orig_vocab_size = config.vocab_size\n    padded_vocab_size = _vocab_size_with_padding(orig_vocab_size, margs)\n    setattr(margs, 'padded_vocab_size', padded_vocab_size)\n    if orig_vocab_size > padded_vocab_size:\n        full_word_embed = word_embedding[0:padded_vocab_size, :]\n    elif orig_vocab_size < padded_vocab_size:\n        padding_size = padded_vocab_size - orig_vocab_size\n        full_word_embed = torch.cat((word_embedding, word_embedding[-1].unsqueeze(0).expand(padding_size, -1)))\n    else:\n        full_word_embed = word_embedding\n    out_word_embed = torch.chunk(full_word_embed, args.target_tensor_model_parallel_size, dim=0)\n    for i in range(args.target_tensor_model_parallel_size):\n        pos_emb_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.embedding.position_embeddings')\n        pos_emb_dict['weight'] = pos_embedding\n        word_emb_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.embedding.word_embeddings')\n        word_emb_dict['weight'] = out_word_embed[i].clone()\n    print('converting transformer layers')\n    if config.num_attention_heads % args.target_tensor_model_parallel_size != 0:\n        raise ValueError(f'Number of attention heads ({config.num_attention_heads}) must be divisible by number of tensor parallelism ({args.target_tensor_model_parallel_size})')\n    if config.num_hidden_layers % args.target_pipeline_model_parallel_size != 0:\n        raise ValueError(f'Number of layers ({config.num_hidden_layers}) must be divisible by number of pipeline parallelism ({args.target_pipeline_model_parallel_size})')\n    num_layers = config.num_hidden_layers // args.target_pipeline_model_parallel_size\n    layer_re = re.compile('transformer.h\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    for pp_rank in range(args.target_pipeline_model_parallel_size):\n        layer_offset = pp_rank * num_layers\n        if pp_rank > 0:\n            output_state_dict = []\n            for i in range(args.target_tensor_model_parallel_size):\n                output_state_dict.append({})\n        for layer in range(num_layers):\n            pp_layer_id = layer + layer_offset\n            layers_to_copy = [layer_name for layer_name in state_dict.keys() if layer_name.startswith(f'transformer.h.{pp_layer_id}.')]\n            for layer_name in layers_to_copy:\n                m = layer_re.match(layer_name)\n                if m is None:\n                    break\n                _ = int(m.group(1))\n                op_name = m.group(2)\n                weight_or_bias = m.group(3)\n                params = state_dict[layer_name].to(dtype)\n                if op_name.startswith('ln'):\n                    out_name = 'input_layernorm' if op_name.endswith('1') else 'post_attention_layernorm'\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                elif op_name.startswith('attn.c_attn') and weight_or_bias == 'weight':\n                    params = params.transpose(0, 1).contiguous()\n                    params = transformers_to_megatron_fix_query_key_value_ordering(params, 3.0, 3, heads, hidden_size_per_head)\n                    layer_name = f'layers.{layer}.self_attention.query_key_value.{weight_or_bias}'\n                elif op_name.startswith('attn.c_attn') and weight_or_bias == 'bias':\n                    params = transformers_to_megatron_fix_query_key_value_ordering(params, 3.0, 3, heads, hidden_size_per_head)\n                    layer_name = f'layers.{layer}.self_attention.query_key_value.{weight_or_bias}'\n                elif weight_or_bias == 'weight':\n                    out_name = transformers_to_megatron.get(op_name, None)\n                    if out_name is None:\n                        continue\n                    params = params.transpose(0, 1)\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                elif weight_or_bias == 'bias':\n                    out_name = transformers_to_megatron.get(op_name, None)\n                    if out_name is None:\n                        continue\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                else:\n                    continue\n                if op_name + '.' + weight_or_bias in tensor_parallel_params:\n                    dim = 1 if op_name in ['attn.c_proj', 'mlp.c_proj'] else 0\n                    params = torch.chunk(params, args.target_tensor_model_parallel_size, dim=dim)\n                for i in range(args.target_tensor_model_parallel_size):\n                    params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.encoder')\n                    params_dict[layer_name] = params[i].clone() if op_name + '.' + weight_or_bias in tensor_parallel_params else params\n        if pp_rank == args.target_pipeline_model_parallel_size - 1:\n            for weight_or_bias in ['weight', 'bias']:\n                params = state_dict[f'transformer.ln_f.{weight_or_bias}'].to(dtype)\n                layer_name = f'final_layernorm.{weight_or_bias}'\n                for i in range(args.target_tensor_model_parallel_size):\n                    params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.encoder')\n                    params_dict[layer_name] = params\n            for i in range(args.target_tensor_model_parallel_size):\n                params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.word_embeddings_for_head')\n                params_dict['weight'] = out_word_embed[i].clone()\n        for tp_rank in range(args.target_tensor_model_parallel_size):\n            output_state_dict[tp_rank]['checkpoint_version'] = 3.0\n            output_state_dict[tp_rank]['args'] = margs\n            checkpoint_dir = f'mp_rank_{tp_rank:02d}' if args.target_pipeline_model_parallel_size == 1 else f'mp_rank_{tp_rank:02d}_{pp_rank:03d}'\n            if args.use_distributed_optimizer:\n                checkpoint_name = 'model_rng.pt'\n            else:\n                checkpoint_name = 'model_optim_rng.pt'\n                output_state_dict[tp_rank]['optimizer'] = dummy_optim_state_dict['optimizer']\n            checkpoint_dir = os.path.join(release_dir, checkpoint_dir)\n            os.makedirs(checkpoint_dir, exist_ok=True)\n            checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n            if args.print_checkpoint_structure:\n                print(f'Checkpoint structure of model state dict shard belonging to TP rank {tp_rank} and PP rank {pp_rank}:')\n                recursive_print(None, output_state_dict[tp_rank])\n            torch.save(output_state_dict[tp_rank], checkpoint_path)",
            "def convert_checkpoint_from_transformers_to_megatron(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Convert a checkpoint from HuggingFace Transformers to Megatron-LM. This allows converted checkpoints with variable\\n    tensor parallelism and pipeline parallelism sizes. It takes as input a checkpoint from HuggingFace Transformers\\n    which can have multiple shards.\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n\\n    '\n    os.makedirs(args.save_path, exist_ok=True)\n    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n    if args.megatron_path is not None:\n        sys.path.insert(0, args.megatron_path)\n    try:\n        from megatron.tokenizer.tokenizer import _vocab_size_with_padding\n    except ModuleNotFoundError:\n        print('Unable to import Megatron, please specify the path to Megatron using --megatron-path. Exiting.')\n        exit(1)\n    sub_dirs = [x for x in os.listdir(args.load_path) if x.startswith('pytorch_model')]\n    if len(sub_dirs) == 1:\n        checkpoint_name = 'pytorch_model.bin'\n        state_dict = torch.load(os.path.join(args.load_path, checkpoint_name), map_location='cpu')\n    else:\n        num_checkpoints = len(sub_dirs) - 1\n        state_dict = merge_transformers_sharded_states(args.load_path, num_checkpoints)\n    config = GPT2Config.from_pretrained(args.load_path)\n    tracker_filepath = os.path.join(args.save_path, 'latest_checkpointed_iteration.txt')\n    with open(tracker_filepath, 'w') as f:\n        f.write('release')\n    release_dir = os.path.join(args.save_path, 'release')\n    os.makedirs(release_dir, exist_ok=True)\n    megatron_args = {'orig_vocab_size': config.vocab_size, 'max_position_embeddings': config.n_positions, 'hidden_size': config.n_embd, 'num_layers': config.n_layer, 'num_attention_heads': config.n_head, 'ffn_hidden_size': config.n_inner, 'tensor_model_parallel_size': args.target_tensor_model_parallel_size, 'pipeline_model_parallel_size': args.target_pipeline_model_parallel_size, 'data_parallel_size': args.target_data_parallel_size, 'make_vocab_size_divisible_by': args.make_vocab_size_divisible_by, 'rank': 0, 'tokenizer_type': 'GPT2BPETokenizer'}\n    if config.activation_function == 'gelu':\n        megatron_args['bias_gelu_fusion'] = False\n        megatron_args['openai_gelu'] = False\n    elif config.activation_function == 'gelu_fast':\n        megatron_args['bias_gelu_fusion'] = True\n        megatron_args['openai_gelu'] = False\n    elif config.activation_function == 'gelu_new':\n        megatron_args['bias_gelu_fusion'] = False\n        megatron_args['openai_gelu'] = True\n    margs = types.SimpleNamespace()\n    for (k, v) in megatron_args.items():\n        setattr(margs, k, v)\n    if args.target_params_dtype == 'fp16':\n        dtype = torch.float16\n    elif args.target_params_dtype == 'bf16':\n        dtype = torch.bfloat16\n    else:\n        dtype = torch.float32\n    setattr(margs, 'params_dtype', dtype)\n    dummy_optim_state_dict = {}\n    dummy_optim_state_dict['optimizer'] = {'step': 0, 'param_groups': [{'lr': 0.0, 'beta1': 0.0, 'beta2': 0.0, 'eps': 0.0, 'weight_decay': 0.0, 'correct_bias': False, 'params': []}]}\n    if args.use_distributed_optimizer:\n        for i in range(args.target_pipeline_model_parallel_size):\n            for j in range(args.target_tensor_model_parallel_size):\n                for k in range(args.target_data_parallel_size):\n                    if args.target_pipeline_model_parallel_size == 1:\n                        checkpoint_dir = f'mp_rank_{j:02d}_{k:03d}'\n                    else:\n                        checkpoint_dir = f'mp_rank_{j:02d}_{i:03d}_{k:03d}'\n                    checkpoint_dir = os.path.join(release_dir, checkpoint_dir)\n                    os.makedirs(checkpoint_dir, exist_ok=True)\n                    torch.save(dummy_optim_state_dict, os.path.join(checkpoint_dir, 'optim.pt'))\n    print('Converting')\n    output_state_dict = []\n    for i in range(args.target_tensor_model_parallel_size):\n        output_state_dict.append({})\n    print('converting embedding layer')\n    pos_embedding = state_dict['transformer.wpe.weight'].to(dtype)\n    word_embedding = state_dict['transformer.wte.weight'].to(dtype)\n    orig_vocab_size = config.vocab_size\n    padded_vocab_size = _vocab_size_with_padding(orig_vocab_size, margs)\n    setattr(margs, 'padded_vocab_size', padded_vocab_size)\n    if orig_vocab_size > padded_vocab_size:\n        full_word_embed = word_embedding[0:padded_vocab_size, :]\n    elif orig_vocab_size < padded_vocab_size:\n        padding_size = padded_vocab_size - orig_vocab_size\n        full_word_embed = torch.cat((word_embedding, word_embedding[-1].unsqueeze(0).expand(padding_size, -1)))\n    else:\n        full_word_embed = word_embedding\n    out_word_embed = torch.chunk(full_word_embed, args.target_tensor_model_parallel_size, dim=0)\n    for i in range(args.target_tensor_model_parallel_size):\n        pos_emb_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.embedding.position_embeddings')\n        pos_emb_dict['weight'] = pos_embedding\n        word_emb_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.embedding.word_embeddings')\n        word_emb_dict['weight'] = out_word_embed[i].clone()\n    print('converting transformer layers')\n    if config.num_attention_heads % args.target_tensor_model_parallel_size != 0:\n        raise ValueError(f'Number of attention heads ({config.num_attention_heads}) must be divisible by number of tensor parallelism ({args.target_tensor_model_parallel_size})')\n    if config.num_hidden_layers % args.target_pipeline_model_parallel_size != 0:\n        raise ValueError(f'Number of layers ({config.num_hidden_layers}) must be divisible by number of pipeline parallelism ({args.target_pipeline_model_parallel_size})')\n    num_layers = config.num_hidden_layers // args.target_pipeline_model_parallel_size\n    layer_re = re.compile('transformer.h\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    for pp_rank in range(args.target_pipeline_model_parallel_size):\n        layer_offset = pp_rank * num_layers\n        if pp_rank > 0:\n            output_state_dict = []\n            for i in range(args.target_tensor_model_parallel_size):\n                output_state_dict.append({})\n        for layer in range(num_layers):\n            pp_layer_id = layer + layer_offset\n            layers_to_copy = [layer_name for layer_name in state_dict.keys() if layer_name.startswith(f'transformer.h.{pp_layer_id}.')]\n            for layer_name in layers_to_copy:\n                m = layer_re.match(layer_name)\n                if m is None:\n                    break\n                _ = int(m.group(1))\n                op_name = m.group(2)\n                weight_or_bias = m.group(3)\n                params = state_dict[layer_name].to(dtype)\n                if op_name.startswith('ln'):\n                    out_name = 'input_layernorm' if op_name.endswith('1') else 'post_attention_layernorm'\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                elif op_name.startswith('attn.c_attn') and weight_or_bias == 'weight':\n                    params = params.transpose(0, 1).contiguous()\n                    params = transformers_to_megatron_fix_query_key_value_ordering(params, 3.0, 3, heads, hidden_size_per_head)\n                    layer_name = f'layers.{layer}.self_attention.query_key_value.{weight_or_bias}'\n                elif op_name.startswith('attn.c_attn') and weight_or_bias == 'bias':\n                    params = transformers_to_megatron_fix_query_key_value_ordering(params, 3.0, 3, heads, hidden_size_per_head)\n                    layer_name = f'layers.{layer}.self_attention.query_key_value.{weight_or_bias}'\n                elif weight_or_bias == 'weight':\n                    out_name = transformers_to_megatron.get(op_name, None)\n                    if out_name is None:\n                        continue\n                    params = params.transpose(0, 1)\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                elif weight_or_bias == 'bias':\n                    out_name = transformers_to_megatron.get(op_name, None)\n                    if out_name is None:\n                        continue\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                else:\n                    continue\n                if op_name + '.' + weight_or_bias in tensor_parallel_params:\n                    dim = 1 if op_name in ['attn.c_proj', 'mlp.c_proj'] else 0\n                    params = torch.chunk(params, args.target_tensor_model_parallel_size, dim=dim)\n                for i in range(args.target_tensor_model_parallel_size):\n                    params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.encoder')\n                    params_dict[layer_name] = params[i].clone() if op_name + '.' + weight_or_bias in tensor_parallel_params else params\n        if pp_rank == args.target_pipeline_model_parallel_size - 1:\n            for weight_or_bias in ['weight', 'bias']:\n                params = state_dict[f'transformer.ln_f.{weight_or_bias}'].to(dtype)\n                layer_name = f'final_layernorm.{weight_or_bias}'\n                for i in range(args.target_tensor_model_parallel_size):\n                    params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.encoder')\n                    params_dict[layer_name] = params\n            for i in range(args.target_tensor_model_parallel_size):\n                params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.word_embeddings_for_head')\n                params_dict['weight'] = out_word_embed[i].clone()\n        for tp_rank in range(args.target_tensor_model_parallel_size):\n            output_state_dict[tp_rank]['checkpoint_version'] = 3.0\n            output_state_dict[tp_rank]['args'] = margs\n            checkpoint_dir = f'mp_rank_{tp_rank:02d}' if args.target_pipeline_model_parallel_size == 1 else f'mp_rank_{tp_rank:02d}_{pp_rank:03d}'\n            if args.use_distributed_optimizer:\n                checkpoint_name = 'model_rng.pt'\n            else:\n                checkpoint_name = 'model_optim_rng.pt'\n                output_state_dict[tp_rank]['optimizer'] = dummy_optim_state_dict['optimizer']\n            checkpoint_dir = os.path.join(release_dir, checkpoint_dir)\n            os.makedirs(checkpoint_dir, exist_ok=True)\n            checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n            if args.print_checkpoint_structure:\n                print(f'Checkpoint structure of model state dict shard belonging to TP rank {tp_rank} and PP rank {pp_rank}:')\n                recursive_print(None, output_state_dict[tp_rank])\n            torch.save(output_state_dict[tp_rank], checkpoint_path)",
            "def convert_checkpoint_from_transformers_to_megatron(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Convert a checkpoint from HuggingFace Transformers to Megatron-LM. This allows converted checkpoints with variable\\n    tensor parallelism and pipeline parallelism sizes. It takes as input a checkpoint from HuggingFace Transformers\\n    which can have multiple shards.\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n\\n    '\n    os.makedirs(args.save_path, exist_ok=True)\n    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n    if args.megatron_path is not None:\n        sys.path.insert(0, args.megatron_path)\n    try:\n        from megatron.tokenizer.tokenizer import _vocab_size_with_padding\n    except ModuleNotFoundError:\n        print('Unable to import Megatron, please specify the path to Megatron using --megatron-path. Exiting.')\n        exit(1)\n    sub_dirs = [x for x in os.listdir(args.load_path) if x.startswith('pytorch_model')]\n    if len(sub_dirs) == 1:\n        checkpoint_name = 'pytorch_model.bin'\n        state_dict = torch.load(os.path.join(args.load_path, checkpoint_name), map_location='cpu')\n    else:\n        num_checkpoints = len(sub_dirs) - 1\n        state_dict = merge_transformers_sharded_states(args.load_path, num_checkpoints)\n    config = GPT2Config.from_pretrained(args.load_path)\n    tracker_filepath = os.path.join(args.save_path, 'latest_checkpointed_iteration.txt')\n    with open(tracker_filepath, 'w') as f:\n        f.write('release')\n    release_dir = os.path.join(args.save_path, 'release')\n    os.makedirs(release_dir, exist_ok=True)\n    megatron_args = {'orig_vocab_size': config.vocab_size, 'max_position_embeddings': config.n_positions, 'hidden_size': config.n_embd, 'num_layers': config.n_layer, 'num_attention_heads': config.n_head, 'ffn_hidden_size': config.n_inner, 'tensor_model_parallel_size': args.target_tensor_model_parallel_size, 'pipeline_model_parallel_size': args.target_pipeline_model_parallel_size, 'data_parallel_size': args.target_data_parallel_size, 'make_vocab_size_divisible_by': args.make_vocab_size_divisible_by, 'rank': 0, 'tokenizer_type': 'GPT2BPETokenizer'}\n    if config.activation_function == 'gelu':\n        megatron_args['bias_gelu_fusion'] = False\n        megatron_args['openai_gelu'] = False\n    elif config.activation_function == 'gelu_fast':\n        megatron_args['bias_gelu_fusion'] = True\n        megatron_args['openai_gelu'] = False\n    elif config.activation_function == 'gelu_new':\n        megatron_args['bias_gelu_fusion'] = False\n        megatron_args['openai_gelu'] = True\n    margs = types.SimpleNamespace()\n    for (k, v) in megatron_args.items():\n        setattr(margs, k, v)\n    if args.target_params_dtype == 'fp16':\n        dtype = torch.float16\n    elif args.target_params_dtype == 'bf16':\n        dtype = torch.bfloat16\n    else:\n        dtype = torch.float32\n    setattr(margs, 'params_dtype', dtype)\n    dummy_optim_state_dict = {}\n    dummy_optim_state_dict['optimizer'] = {'step': 0, 'param_groups': [{'lr': 0.0, 'beta1': 0.0, 'beta2': 0.0, 'eps': 0.0, 'weight_decay': 0.0, 'correct_bias': False, 'params': []}]}\n    if args.use_distributed_optimizer:\n        for i in range(args.target_pipeline_model_parallel_size):\n            for j in range(args.target_tensor_model_parallel_size):\n                for k in range(args.target_data_parallel_size):\n                    if args.target_pipeline_model_parallel_size == 1:\n                        checkpoint_dir = f'mp_rank_{j:02d}_{k:03d}'\n                    else:\n                        checkpoint_dir = f'mp_rank_{j:02d}_{i:03d}_{k:03d}'\n                    checkpoint_dir = os.path.join(release_dir, checkpoint_dir)\n                    os.makedirs(checkpoint_dir, exist_ok=True)\n                    torch.save(dummy_optim_state_dict, os.path.join(checkpoint_dir, 'optim.pt'))\n    print('Converting')\n    output_state_dict = []\n    for i in range(args.target_tensor_model_parallel_size):\n        output_state_dict.append({})\n    print('converting embedding layer')\n    pos_embedding = state_dict['transformer.wpe.weight'].to(dtype)\n    word_embedding = state_dict['transformer.wte.weight'].to(dtype)\n    orig_vocab_size = config.vocab_size\n    padded_vocab_size = _vocab_size_with_padding(orig_vocab_size, margs)\n    setattr(margs, 'padded_vocab_size', padded_vocab_size)\n    if orig_vocab_size > padded_vocab_size:\n        full_word_embed = word_embedding[0:padded_vocab_size, :]\n    elif orig_vocab_size < padded_vocab_size:\n        padding_size = padded_vocab_size - orig_vocab_size\n        full_word_embed = torch.cat((word_embedding, word_embedding[-1].unsqueeze(0).expand(padding_size, -1)))\n    else:\n        full_word_embed = word_embedding\n    out_word_embed = torch.chunk(full_word_embed, args.target_tensor_model_parallel_size, dim=0)\n    for i in range(args.target_tensor_model_parallel_size):\n        pos_emb_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.embedding.position_embeddings')\n        pos_emb_dict['weight'] = pos_embedding\n        word_emb_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.embedding.word_embeddings')\n        word_emb_dict['weight'] = out_word_embed[i].clone()\n    print('converting transformer layers')\n    if config.num_attention_heads % args.target_tensor_model_parallel_size != 0:\n        raise ValueError(f'Number of attention heads ({config.num_attention_heads}) must be divisible by number of tensor parallelism ({args.target_tensor_model_parallel_size})')\n    if config.num_hidden_layers % args.target_pipeline_model_parallel_size != 0:\n        raise ValueError(f'Number of layers ({config.num_hidden_layers}) must be divisible by number of pipeline parallelism ({args.target_pipeline_model_parallel_size})')\n    num_layers = config.num_hidden_layers // args.target_pipeline_model_parallel_size\n    layer_re = re.compile('transformer.h\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    for pp_rank in range(args.target_pipeline_model_parallel_size):\n        layer_offset = pp_rank * num_layers\n        if pp_rank > 0:\n            output_state_dict = []\n            for i in range(args.target_tensor_model_parallel_size):\n                output_state_dict.append({})\n        for layer in range(num_layers):\n            pp_layer_id = layer + layer_offset\n            layers_to_copy = [layer_name for layer_name in state_dict.keys() if layer_name.startswith(f'transformer.h.{pp_layer_id}.')]\n            for layer_name in layers_to_copy:\n                m = layer_re.match(layer_name)\n                if m is None:\n                    break\n                _ = int(m.group(1))\n                op_name = m.group(2)\n                weight_or_bias = m.group(3)\n                params = state_dict[layer_name].to(dtype)\n                if op_name.startswith('ln'):\n                    out_name = 'input_layernorm' if op_name.endswith('1') else 'post_attention_layernorm'\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                elif op_name.startswith('attn.c_attn') and weight_or_bias == 'weight':\n                    params = params.transpose(0, 1).contiguous()\n                    params = transformers_to_megatron_fix_query_key_value_ordering(params, 3.0, 3, heads, hidden_size_per_head)\n                    layer_name = f'layers.{layer}.self_attention.query_key_value.{weight_or_bias}'\n                elif op_name.startswith('attn.c_attn') and weight_or_bias == 'bias':\n                    params = transformers_to_megatron_fix_query_key_value_ordering(params, 3.0, 3, heads, hidden_size_per_head)\n                    layer_name = f'layers.{layer}.self_attention.query_key_value.{weight_or_bias}'\n                elif weight_or_bias == 'weight':\n                    out_name = transformers_to_megatron.get(op_name, None)\n                    if out_name is None:\n                        continue\n                    params = params.transpose(0, 1)\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                elif weight_or_bias == 'bias':\n                    out_name = transformers_to_megatron.get(op_name, None)\n                    if out_name is None:\n                        continue\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                else:\n                    continue\n                if op_name + '.' + weight_or_bias in tensor_parallel_params:\n                    dim = 1 if op_name in ['attn.c_proj', 'mlp.c_proj'] else 0\n                    params = torch.chunk(params, args.target_tensor_model_parallel_size, dim=dim)\n                for i in range(args.target_tensor_model_parallel_size):\n                    params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.encoder')\n                    params_dict[layer_name] = params[i].clone() if op_name + '.' + weight_or_bias in tensor_parallel_params else params\n        if pp_rank == args.target_pipeline_model_parallel_size - 1:\n            for weight_or_bias in ['weight', 'bias']:\n                params = state_dict[f'transformer.ln_f.{weight_or_bias}'].to(dtype)\n                layer_name = f'final_layernorm.{weight_or_bias}'\n                for i in range(args.target_tensor_model_parallel_size):\n                    params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.encoder')\n                    params_dict[layer_name] = params\n            for i in range(args.target_tensor_model_parallel_size):\n                params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.word_embeddings_for_head')\n                params_dict['weight'] = out_word_embed[i].clone()\n        for tp_rank in range(args.target_tensor_model_parallel_size):\n            output_state_dict[tp_rank]['checkpoint_version'] = 3.0\n            output_state_dict[tp_rank]['args'] = margs\n            checkpoint_dir = f'mp_rank_{tp_rank:02d}' if args.target_pipeline_model_parallel_size == 1 else f'mp_rank_{tp_rank:02d}_{pp_rank:03d}'\n            if args.use_distributed_optimizer:\n                checkpoint_name = 'model_rng.pt'\n            else:\n                checkpoint_name = 'model_optim_rng.pt'\n                output_state_dict[tp_rank]['optimizer'] = dummy_optim_state_dict['optimizer']\n            checkpoint_dir = os.path.join(release_dir, checkpoint_dir)\n            os.makedirs(checkpoint_dir, exist_ok=True)\n            checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n            if args.print_checkpoint_structure:\n                print(f'Checkpoint structure of model state dict shard belonging to TP rank {tp_rank} and PP rank {pp_rank}:')\n                recursive_print(None, output_state_dict[tp_rank])\n            torch.save(output_state_dict[tp_rank], checkpoint_path)",
            "def convert_checkpoint_from_transformers_to_megatron(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Convert a checkpoint from HuggingFace Transformers to Megatron-LM. This allows converted checkpoints with variable\\n    tensor parallelism and pipeline parallelism sizes. It takes as input a checkpoint from HuggingFace Transformers\\n    which can have multiple shards.\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n\\n    '\n    os.makedirs(args.save_path, exist_ok=True)\n    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n    if args.megatron_path is not None:\n        sys.path.insert(0, args.megatron_path)\n    try:\n        from megatron.tokenizer.tokenizer import _vocab_size_with_padding\n    except ModuleNotFoundError:\n        print('Unable to import Megatron, please specify the path to Megatron using --megatron-path. Exiting.')\n        exit(1)\n    sub_dirs = [x for x in os.listdir(args.load_path) if x.startswith('pytorch_model')]\n    if len(sub_dirs) == 1:\n        checkpoint_name = 'pytorch_model.bin'\n        state_dict = torch.load(os.path.join(args.load_path, checkpoint_name), map_location='cpu')\n    else:\n        num_checkpoints = len(sub_dirs) - 1\n        state_dict = merge_transformers_sharded_states(args.load_path, num_checkpoints)\n    config = GPT2Config.from_pretrained(args.load_path)\n    tracker_filepath = os.path.join(args.save_path, 'latest_checkpointed_iteration.txt')\n    with open(tracker_filepath, 'w') as f:\n        f.write('release')\n    release_dir = os.path.join(args.save_path, 'release')\n    os.makedirs(release_dir, exist_ok=True)\n    megatron_args = {'orig_vocab_size': config.vocab_size, 'max_position_embeddings': config.n_positions, 'hidden_size': config.n_embd, 'num_layers': config.n_layer, 'num_attention_heads': config.n_head, 'ffn_hidden_size': config.n_inner, 'tensor_model_parallel_size': args.target_tensor_model_parallel_size, 'pipeline_model_parallel_size': args.target_pipeline_model_parallel_size, 'data_parallel_size': args.target_data_parallel_size, 'make_vocab_size_divisible_by': args.make_vocab_size_divisible_by, 'rank': 0, 'tokenizer_type': 'GPT2BPETokenizer'}\n    if config.activation_function == 'gelu':\n        megatron_args['bias_gelu_fusion'] = False\n        megatron_args['openai_gelu'] = False\n    elif config.activation_function == 'gelu_fast':\n        megatron_args['bias_gelu_fusion'] = True\n        megatron_args['openai_gelu'] = False\n    elif config.activation_function == 'gelu_new':\n        megatron_args['bias_gelu_fusion'] = False\n        megatron_args['openai_gelu'] = True\n    margs = types.SimpleNamespace()\n    for (k, v) in megatron_args.items():\n        setattr(margs, k, v)\n    if args.target_params_dtype == 'fp16':\n        dtype = torch.float16\n    elif args.target_params_dtype == 'bf16':\n        dtype = torch.bfloat16\n    else:\n        dtype = torch.float32\n    setattr(margs, 'params_dtype', dtype)\n    dummy_optim_state_dict = {}\n    dummy_optim_state_dict['optimizer'] = {'step': 0, 'param_groups': [{'lr': 0.0, 'beta1': 0.0, 'beta2': 0.0, 'eps': 0.0, 'weight_decay': 0.0, 'correct_bias': False, 'params': []}]}\n    if args.use_distributed_optimizer:\n        for i in range(args.target_pipeline_model_parallel_size):\n            for j in range(args.target_tensor_model_parallel_size):\n                for k in range(args.target_data_parallel_size):\n                    if args.target_pipeline_model_parallel_size == 1:\n                        checkpoint_dir = f'mp_rank_{j:02d}_{k:03d}'\n                    else:\n                        checkpoint_dir = f'mp_rank_{j:02d}_{i:03d}_{k:03d}'\n                    checkpoint_dir = os.path.join(release_dir, checkpoint_dir)\n                    os.makedirs(checkpoint_dir, exist_ok=True)\n                    torch.save(dummy_optim_state_dict, os.path.join(checkpoint_dir, 'optim.pt'))\n    print('Converting')\n    output_state_dict = []\n    for i in range(args.target_tensor_model_parallel_size):\n        output_state_dict.append({})\n    print('converting embedding layer')\n    pos_embedding = state_dict['transformer.wpe.weight'].to(dtype)\n    word_embedding = state_dict['transformer.wte.weight'].to(dtype)\n    orig_vocab_size = config.vocab_size\n    padded_vocab_size = _vocab_size_with_padding(orig_vocab_size, margs)\n    setattr(margs, 'padded_vocab_size', padded_vocab_size)\n    if orig_vocab_size > padded_vocab_size:\n        full_word_embed = word_embedding[0:padded_vocab_size, :]\n    elif orig_vocab_size < padded_vocab_size:\n        padding_size = padded_vocab_size - orig_vocab_size\n        full_word_embed = torch.cat((word_embedding, word_embedding[-1].unsqueeze(0).expand(padding_size, -1)))\n    else:\n        full_word_embed = word_embedding\n    out_word_embed = torch.chunk(full_word_embed, args.target_tensor_model_parallel_size, dim=0)\n    for i in range(args.target_tensor_model_parallel_size):\n        pos_emb_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.embedding.position_embeddings')\n        pos_emb_dict['weight'] = pos_embedding\n        word_emb_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.embedding.word_embeddings')\n        word_emb_dict['weight'] = out_word_embed[i].clone()\n    print('converting transformer layers')\n    if config.num_attention_heads % args.target_tensor_model_parallel_size != 0:\n        raise ValueError(f'Number of attention heads ({config.num_attention_heads}) must be divisible by number of tensor parallelism ({args.target_tensor_model_parallel_size})')\n    if config.num_hidden_layers % args.target_pipeline_model_parallel_size != 0:\n        raise ValueError(f'Number of layers ({config.num_hidden_layers}) must be divisible by number of pipeline parallelism ({args.target_pipeline_model_parallel_size})')\n    num_layers = config.num_hidden_layers // args.target_pipeline_model_parallel_size\n    layer_re = re.compile('transformer.h\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    for pp_rank in range(args.target_pipeline_model_parallel_size):\n        layer_offset = pp_rank * num_layers\n        if pp_rank > 0:\n            output_state_dict = []\n            for i in range(args.target_tensor_model_parallel_size):\n                output_state_dict.append({})\n        for layer in range(num_layers):\n            pp_layer_id = layer + layer_offset\n            layers_to_copy = [layer_name for layer_name in state_dict.keys() if layer_name.startswith(f'transformer.h.{pp_layer_id}.')]\n            for layer_name in layers_to_copy:\n                m = layer_re.match(layer_name)\n                if m is None:\n                    break\n                _ = int(m.group(1))\n                op_name = m.group(2)\n                weight_or_bias = m.group(3)\n                params = state_dict[layer_name].to(dtype)\n                if op_name.startswith('ln'):\n                    out_name = 'input_layernorm' if op_name.endswith('1') else 'post_attention_layernorm'\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                elif op_name.startswith('attn.c_attn') and weight_or_bias == 'weight':\n                    params = params.transpose(0, 1).contiguous()\n                    params = transformers_to_megatron_fix_query_key_value_ordering(params, 3.0, 3, heads, hidden_size_per_head)\n                    layer_name = f'layers.{layer}.self_attention.query_key_value.{weight_or_bias}'\n                elif op_name.startswith('attn.c_attn') and weight_or_bias == 'bias':\n                    params = transformers_to_megatron_fix_query_key_value_ordering(params, 3.0, 3, heads, hidden_size_per_head)\n                    layer_name = f'layers.{layer}.self_attention.query_key_value.{weight_or_bias}'\n                elif weight_or_bias == 'weight':\n                    out_name = transformers_to_megatron.get(op_name, None)\n                    if out_name is None:\n                        continue\n                    params = params.transpose(0, 1)\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                elif weight_or_bias == 'bias':\n                    out_name = transformers_to_megatron.get(op_name, None)\n                    if out_name is None:\n                        continue\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                else:\n                    continue\n                if op_name + '.' + weight_or_bias in tensor_parallel_params:\n                    dim = 1 if op_name in ['attn.c_proj', 'mlp.c_proj'] else 0\n                    params = torch.chunk(params, args.target_tensor_model_parallel_size, dim=dim)\n                for i in range(args.target_tensor_model_parallel_size):\n                    params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.encoder')\n                    params_dict[layer_name] = params[i].clone() if op_name + '.' + weight_or_bias in tensor_parallel_params else params\n        if pp_rank == args.target_pipeline_model_parallel_size - 1:\n            for weight_or_bias in ['weight', 'bias']:\n                params = state_dict[f'transformer.ln_f.{weight_or_bias}'].to(dtype)\n                layer_name = f'final_layernorm.{weight_or_bias}'\n                for i in range(args.target_tensor_model_parallel_size):\n                    params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.encoder')\n                    params_dict[layer_name] = params\n            for i in range(args.target_tensor_model_parallel_size):\n                params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.word_embeddings_for_head')\n                params_dict['weight'] = out_word_embed[i].clone()\n        for tp_rank in range(args.target_tensor_model_parallel_size):\n            output_state_dict[tp_rank]['checkpoint_version'] = 3.0\n            output_state_dict[tp_rank]['args'] = margs\n            checkpoint_dir = f'mp_rank_{tp_rank:02d}' if args.target_pipeline_model_parallel_size == 1 else f'mp_rank_{tp_rank:02d}_{pp_rank:03d}'\n            if args.use_distributed_optimizer:\n                checkpoint_name = 'model_rng.pt'\n            else:\n                checkpoint_name = 'model_optim_rng.pt'\n                output_state_dict[tp_rank]['optimizer'] = dummy_optim_state_dict['optimizer']\n            checkpoint_dir = os.path.join(release_dir, checkpoint_dir)\n            os.makedirs(checkpoint_dir, exist_ok=True)\n            checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n            if args.print_checkpoint_structure:\n                print(f'Checkpoint structure of model state dict shard belonging to TP rank {tp_rank} and PP rank {pp_rank}:')\n                recursive_print(None, output_state_dict[tp_rank])\n            torch.save(output_state_dict[tp_rank], checkpoint_path)",
            "def convert_checkpoint_from_transformers_to_megatron(args):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Convert a checkpoint from HuggingFace Transformers to Megatron-LM. This allows converted checkpoints with variable\\n    tensor parallelism and pipeline parallelism sizes. It takes as input a checkpoint from HuggingFace Transformers\\n    which can have multiple shards.\\n\\n    Args:\\n        args (argparse.Namespace): the arguments to the script\\n\\n    '\n    os.makedirs(args.save_path, exist_ok=True)\n    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n    if args.megatron_path is not None:\n        sys.path.insert(0, args.megatron_path)\n    try:\n        from megatron.tokenizer.tokenizer import _vocab_size_with_padding\n    except ModuleNotFoundError:\n        print('Unable to import Megatron, please specify the path to Megatron using --megatron-path. Exiting.')\n        exit(1)\n    sub_dirs = [x for x in os.listdir(args.load_path) if x.startswith('pytorch_model')]\n    if len(sub_dirs) == 1:\n        checkpoint_name = 'pytorch_model.bin'\n        state_dict = torch.load(os.path.join(args.load_path, checkpoint_name), map_location='cpu')\n    else:\n        num_checkpoints = len(sub_dirs) - 1\n        state_dict = merge_transformers_sharded_states(args.load_path, num_checkpoints)\n    config = GPT2Config.from_pretrained(args.load_path)\n    tracker_filepath = os.path.join(args.save_path, 'latest_checkpointed_iteration.txt')\n    with open(tracker_filepath, 'w') as f:\n        f.write('release')\n    release_dir = os.path.join(args.save_path, 'release')\n    os.makedirs(release_dir, exist_ok=True)\n    megatron_args = {'orig_vocab_size': config.vocab_size, 'max_position_embeddings': config.n_positions, 'hidden_size': config.n_embd, 'num_layers': config.n_layer, 'num_attention_heads': config.n_head, 'ffn_hidden_size': config.n_inner, 'tensor_model_parallel_size': args.target_tensor_model_parallel_size, 'pipeline_model_parallel_size': args.target_pipeline_model_parallel_size, 'data_parallel_size': args.target_data_parallel_size, 'make_vocab_size_divisible_by': args.make_vocab_size_divisible_by, 'rank': 0, 'tokenizer_type': 'GPT2BPETokenizer'}\n    if config.activation_function == 'gelu':\n        megatron_args['bias_gelu_fusion'] = False\n        megatron_args['openai_gelu'] = False\n    elif config.activation_function == 'gelu_fast':\n        megatron_args['bias_gelu_fusion'] = True\n        megatron_args['openai_gelu'] = False\n    elif config.activation_function == 'gelu_new':\n        megatron_args['bias_gelu_fusion'] = False\n        megatron_args['openai_gelu'] = True\n    margs = types.SimpleNamespace()\n    for (k, v) in megatron_args.items():\n        setattr(margs, k, v)\n    if args.target_params_dtype == 'fp16':\n        dtype = torch.float16\n    elif args.target_params_dtype == 'bf16':\n        dtype = torch.bfloat16\n    else:\n        dtype = torch.float32\n    setattr(margs, 'params_dtype', dtype)\n    dummy_optim_state_dict = {}\n    dummy_optim_state_dict['optimizer'] = {'step': 0, 'param_groups': [{'lr': 0.0, 'beta1': 0.0, 'beta2': 0.0, 'eps': 0.0, 'weight_decay': 0.0, 'correct_bias': False, 'params': []}]}\n    if args.use_distributed_optimizer:\n        for i in range(args.target_pipeline_model_parallel_size):\n            for j in range(args.target_tensor_model_parallel_size):\n                for k in range(args.target_data_parallel_size):\n                    if args.target_pipeline_model_parallel_size == 1:\n                        checkpoint_dir = f'mp_rank_{j:02d}_{k:03d}'\n                    else:\n                        checkpoint_dir = f'mp_rank_{j:02d}_{i:03d}_{k:03d}'\n                    checkpoint_dir = os.path.join(release_dir, checkpoint_dir)\n                    os.makedirs(checkpoint_dir, exist_ok=True)\n                    torch.save(dummy_optim_state_dict, os.path.join(checkpoint_dir, 'optim.pt'))\n    print('Converting')\n    output_state_dict = []\n    for i in range(args.target_tensor_model_parallel_size):\n        output_state_dict.append({})\n    print('converting embedding layer')\n    pos_embedding = state_dict['transformer.wpe.weight'].to(dtype)\n    word_embedding = state_dict['transformer.wte.weight'].to(dtype)\n    orig_vocab_size = config.vocab_size\n    padded_vocab_size = _vocab_size_with_padding(orig_vocab_size, margs)\n    setattr(margs, 'padded_vocab_size', padded_vocab_size)\n    if orig_vocab_size > padded_vocab_size:\n        full_word_embed = word_embedding[0:padded_vocab_size, :]\n    elif orig_vocab_size < padded_vocab_size:\n        padding_size = padded_vocab_size - orig_vocab_size\n        full_word_embed = torch.cat((word_embedding, word_embedding[-1].unsqueeze(0).expand(padding_size, -1)))\n    else:\n        full_word_embed = word_embedding\n    out_word_embed = torch.chunk(full_word_embed, args.target_tensor_model_parallel_size, dim=0)\n    for i in range(args.target_tensor_model_parallel_size):\n        pos_emb_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.embedding.position_embeddings')\n        pos_emb_dict['weight'] = pos_embedding\n        word_emb_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.embedding.word_embeddings')\n        word_emb_dict['weight'] = out_word_embed[i].clone()\n    print('converting transformer layers')\n    if config.num_attention_heads % args.target_tensor_model_parallel_size != 0:\n        raise ValueError(f'Number of attention heads ({config.num_attention_heads}) must be divisible by number of tensor parallelism ({args.target_tensor_model_parallel_size})')\n    if config.num_hidden_layers % args.target_pipeline_model_parallel_size != 0:\n        raise ValueError(f'Number of layers ({config.num_hidden_layers}) must be divisible by number of pipeline parallelism ({args.target_pipeline_model_parallel_size})')\n    num_layers = config.num_hidden_layers // args.target_pipeline_model_parallel_size\n    layer_re = re.compile('transformer.h\\\\.(\\\\d+)\\\\.([a-z0-9_.]+)\\\\.([a-z]+)')\n    heads = config.n_head\n    hidden_size_per_head = config.n_embd // config.n_head\n    for pp_rank in range(args.target_pipeline_model_parallel_size):\n        layer_offset = pp_rank * num_layers\n        if pp_rank > 0:\n            output_state_dict = []\n            for i in range(args.target_tensor_model_parallel_size):\n                output_state_dict.append({})\n        for layer in range(num_layers):\n            pp_layer_id = layer + layer_offset\n            layers_to_copy = [layer_name for layer_name in state_dict.keys() if layer_name.startswith(f'transformer.h.{pp_layer_id}.')]\n            for layer_name in layers_to_copy:\n                m = layer_re.match(layer_name)\n                if m is None:\n                    break\n                _ = int(m.group(1))\n                op_name = m.group(2)\n                weight_or_bias = m.group(3)\n                params = state_dict[layer_name].to(dtype)\n                if op_name.startswith('ln'):\n                    out_name = 'input_layernorm' if op_name.endswith('1') else 'post_attention_layernorm'\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                elif op_name.startswith('attn.c_attn') and weight_or_bias == 'weight':\n                    params = params.transpose(0, 1).contiguous()\n                    params = transformers_to_megatron_fix_query_key_value_ordering(params, 3.0, 3, heads, hidden_size_per_head)\n                    layer_name = f'layers.{layer}.self_attention.query_key_value.{weight_or_bias}'\n                elif op_name.startswith('attn.c_attn') and weight_or_bias == 'bias':\n                    params = transformers_to_megatron_fix_query_key_value_ordering(params, 3.0, 3, heads, hidden_size_per_head)\n                    layer_name = f'layers.{layer}.self_attention.query_key_value.{weight_or_bias}'\n                elif weight_or_bias == 'weight':\n                    out_name = transformers_to_megatron.get(op_name, None)\n                    if out_name is None:\n                        continue\n                    params = params.transpose(0, 1)\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                elif weight_or_bias == 'bias':\n                    out_name = transformers_to_megatron.get(op_name, None)\n                    if out_name is None:\n                        continue\n                    layer_name = f'layers.{layer}.{out_name}.{weight_or_bias}'\n                else:\n                    continue\n                if op_name + '.' + weight_or_bias in tensor_parallel_params:\n                    dim = 1 if op_name in ['attn.c_proj', 'mlp.c_proj'] else 0\n                    params = torch.chunk(params, args.target_tensor_model_parallel_size, dim=dim)\n                for i in range(args.target_tensor_model_parallel_size):\n                    params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.encoder')\n                    params_dict[layer_name] = params[i].clone() if op_name + '.' + weight_or_bias in tensor_parallel_params else params\n        if pp_rank == args.target_pipeline_model_parallel_size - 1:\n            for weight_or_bias in ['weight', 'bias']:\n                params = state_dict[f'transformer.ln_f.{weight_or_bias}'].to(dtype)\n                layer_name = f'final_layernorm.{weight_or_bias}'\n                for i in range(args.target_tensor_model_parallel_size):\n                    params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.language_model.encoder')\n                    params_dict[layer_name] = params\n            for i in range(args.target_tensor_model_parallel_size):\n                params_dict = get_element_from_dict_by_path(output_state_dict[i], 'model.word_embeddings_for_head')\n                params_dict['weight'] = out_word_embed[i].clone()\n        for tp_rank in range(args.target_tensor_model_parallel_size):\n            output_state_dict[tp_rank]['checkpoint_version'] = 3.0\n            output_state_dict[tp_rank]['args'] = margs\n            checkpoint_dir = f'mp_rank_{tp_rank:02d}' if args.target_pipeline_model_parallel_size == 1 else f'mp_rank_{tp_rank:02d}_{pp_rank:03d}'\n            if args.use_distributed_optimizer:\n                checkpoint_name = 'model_rng.pt'\n            else:\n                checkpoint_name = 'model_optim_rng.pt'\n                output_state_dict[tp_rank]['optimizer'] = dummy_optim_state_dict['optimizer']\n            checkpoint_dir = os.path.join(release_dir, checkpoint_dir)\n            os.makedirs(checkpoint_dir, exist_ok=True)\n            checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n            if args.print_checkpoint_structure:\n                print(f'Checkpoint structure of model state dict shard belonging to TP rank {tp_rank} and PP rank {pp_rank}:')\n                recursive_print(None, output_state_dict[tp_rank])\n            torch.save(output_state_dict[tp_rank], checkpoint_path)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    parser = argparse.ArgumentParser()\n    parser = add_checkpointing_args(parser)\n    parser = add_megatron_checkpoint_args(parser)\n    parser = add_transformers_checkpoint_args(parser)\n    args = parser.parse_args()\n    if args.convert_checkpoint_from_megatron_to_transformers:\n        convert_checkpoint_from_megatron_to_transformers(args)\n    else:\n        convert_checkpoint_from_transformers_to_megatron(args)",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser()\n    parser = add_checkpointing_args(parser)\n    parser = add_megatron_checkpoint_args(parser)\n    parser = add_transformers_checkpoint_args(parser)\n    args = parser.parse_args()\n    if args.convert_checkpoint_from_megatron_to_transformers:\n        convert_checkpoint_from_megatron_to_transformers(args)\n    else:\n        convert_checkpoint_from_transformers_to_megatron(args)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser()\n    parser = add_checkpointing_args(parser)\n    parser = add_megatron_checkpoint_args(parser)\n    parser = add_transformers_checkpoint_args(parser)\n    args = parser.parse_args()\n    if args.convert_checkpoint_from_megatron_to_transformers:\n        convert_checkpoint_from_megatron_to_transformers(args)\n    else:\n        convert_checkpoint_from_transformers_to_megatron(args)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser()\n    parser = add_checkpointing_args(parser)\n    parser = add_megatron_checkpoint_args(parser)\n    parser = add_transformers_checkpoint_args(parser)\n    args = parser.parse_args()\n    if args.convert_checkpoint_from_megatron_to_transformers:\n        convert_checkpoint_from_megatron_to_transformers(args)\n    else:\n        convert_checkpoint_from_transformers_to_megatron(args)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser()\n    parser = add_checkpointing_args(parser)\n    parser = add_megatron_checkpoint_args(parser)\n    parser = add_transformers_checkpoint_args(parser)\n    args = parser.parse_args()\n    if args.convert_checkpoint_from_megatron_to_transformers:\n        convert_checkpoint_from_megatron_to_transformers(args)\n    else:\n        convert_checkpoint_from_transformers_to_megatron(args)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser()\n    parser = add_checkpointing_args(parser)\n    parser = add_megatron_checkpoint_args(parser)\n    parser = add_transformers_checkpoint_args(parser)\n    args = parser.parse_args()\n    if args.convert_checkpoint_from_megatron_to_transformers:\n        convert_checkpoint_from_megatron_to_transformers(args)\n    else:\n        convert_checkpoint_from_transformers_to_megatron(args)"
        ]
    }
]