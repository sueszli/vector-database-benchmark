[
    {
        "func_name": "fused_layer_norm",
        "original": "def fused_layer_norm(x, norm_weight, norm_bias, epsilon, residual_alpha=1.0, begin_norm_axis=1, bias=None, residual=None, quant_scale=-1, quant_round_type=0, quant_max_bound=0, quant_min_bound=0):\n    \"\"\"\n    Apply Fused LayerNorm kernel. Also support LayerNorm(bias + residual_alpha * residual + x) fused pattern.\n\n    when norm_weight and norm_bias is None, it return fused (bias + residual_alpha * residual + x)\n\n    Args:\n        x (Tensor): the input Tensor..\n        norm_weight (Tensor): the weight Tensor to affine output.\n        norm_bias (Tensor): the bias Tensor to affine output.\n        epsilon (float): a small float number to avoid divide 0.\n        residual_alpha (float): a scale factor for residual. default is 1.\n        begin_norm_axis (int): the begin axis to normalize. default is 1.\n        bias (optional|Tensor): the previous layers's bias to fused.\n        residual (optional|Tensor): the residual input to fused.\n        quant_scale (float): the quant scale.\n        quant_round_type (float): the quant round type.\n        quant_max_bound (float): the quant max bound to clip.\n        quant_min_bound (float): the quant min bound to clip.\n\n\n    Returns:\n        Tensor: the output Tensor.\n\n    Examples:\n        .. code-block:: python\n\n            >>> # doctest: +REQUIRES(env:GPU)\n            >>> import paddle\n            >>> paddle.device.set_device('gpu')\n\n            >>> paddle_x = paddle.cast(paddle.randn(shape=[32, 256]), dtype=paddle.float16)\n            >>> paddle_weight = paddle.cast(paddle.randn(shape=[256]), dtype=paddle.float32)\n            >>> paddle_bias = paddle.cast(paddle.randn(shape=[256]), dtype=paddle.float32)\n            >>> epsilon = 1e-6\n            >>> paddle_layernorm = paddle.incubate.nn.functional.fused_layer_norm(paddle_x, paddle_weight, paddle_bias, epsilon, 1)\n    \"\"\"\n    if in_dynamic_mode():\n        return _C_ops.fused_bias_residual_layernorm(x, bias, residual, norm_weight, norm_bias, epsilon, residual_alpha, begin_norm_axis, quant_scale, quant_round_type, quant_max_bound, quant_min_bound)\n    helper = LayerHelper('fused_layernorm', **locals())\n    out = None\n    if quant_scale <= 0:\n        out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    else:\n        out = helper.create_variable_for_type_inference(dtype=paddle.int8)\n    outputs_dict = {}\n    outputs_dict['out'] = out\n    outputs_dict['mean'] = helper.create_variable_for_type_inference(dtype=paddle.float32)\n    outputs_dict['variance'] = helper.create_variable_for_type_inference(dtype=paddle.float32)\n    residual_out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    outputs_dict['residual_out'] = residual_out\n    inputs = {'x': x}\n    if norm_weight is not None:\n        inputs['norm_weight'] = norm_weight\n    if norm_bias is not None:\n        inputs['norm_bias'] = norm_bias\n    if residual is not None:\n        inputs['residual'] = residual\n    if bias is not None:\n        inputs['bias'] = bias\n    helper.append_op(type='fused_bias_residual_layernorm', inputs=inputs, attrs={'epsilon': epsilon, 'residual_alpha': residual_alpha, 'begin_norm_axis': begin_norm_axis, 'quant_scale': quant_scale, 'quant_round_type': quant_round_type, 'quant_max_bound': quant_max_bound, 'quant_min_bound': quant_min_bound}, outputs=outputs_dict)\n    return (out, residual_out) if residual is not None else out",
        "mutated": [
            "def fused_layer_norm(x, norm_weight, norm_bias, epsilon, residual_alpha=1.0, begin_norm_axis=1, bias=None, residual=None, quant_scale=-1, quant_round_type=0, quant_max_bound=0, quant_min_bound=0):\n    if False:\n        i = 10\n    \"\\n    Apply Fused LayerNorm kernel. Also support LayerNorm(bias + residual_alpha * residual + x) fused pattern.\\n\\n    when norm_weight and norm_bias is None, it return fused (bias + residual_alpha * residual + x)\\n\\n    Args:\\n        x (Tensor): the input Tensor..\\n        norm_weight (Tensor): the weight Tensor to affine output.\\n        norm_bias (Tensor): the bias Tensor to affine output.\\n        epsilon (float): a small float number to avoid divide 0.\\n        residual_alpha (float): a scale factor for residual. default is 1.\\n        begin_norm_axis (int): the begin axis to normalize. default is 1.\\n        bias (optional|Tensor): the previous layers's bias to fused.\\n        residual (optional|Tensor): the residual input to fused.\\n        quant_scale (float): the quant scale.\\n        quant_round_type (float): the quant round type.\\n        quant_max_bound (float): the quant max bound to clip.\\n        quant_min_bound (float): the quant min bound to clip.\\n\\n\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import paddle\\n            >>> paddle.device.set_device('gpu')\\n\\n            >>> paddle_x = paddle.cast(paddle.randn(shape=[32, 256]), dtype=paddle.float16)\\n            >>> paddle_weight = paddle.cast(paddle.randn(shape=[256]), dtype=paddle.float32)\\n            >>> paddle_bias = paddle.cast(paddle.randn(shape=[256]), dtype=paddle.float32)\\n            >>> epsilon = 1e-6\\n            >>> paddle_layernorm = paddle.incubate.nn.functional.fused_layer_norm(paddle_x, paddle_weight, paddle_bias, epsilon, 1)\\n    \"\n    if in_dynamic_mode():\n        return _C_ops.fused_bias_residual_layernorm(x, bias, residual, norm_weight, norm_bias, epsilon, residual_alpha, begin_norm_axis, quant_scale, quant_round_type, quant_max_bound, quant_min_bound)\n    helper = LayerHelper('fused_layernorm', **locals())\n    out = None\n    if quant_scale <= 0:\n        out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    else:\n        out = helper.create_variable_for_type_inference(dtype=paddle.int8)\n    outputs_dict = {}\n    outputs_dict['out'] = out\n    outputs_dict['mean'] = helper.create_variable_for_type_inference(dtype=paddle.float32)\n    outputs_dict['variance'] = helper.create_variable_for_type_inference(dtype=paddle.float32)\n    residual_out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    outputs_dict['residual_out'] = residual_out\n    inputs = {'x': x}\n    if norm_weight is not None:\n        inputs['norm_weight'] = norm_weight\n    if norm_bias is not None:\n        inputs['norm_bias'] = norm_bias\n    if residual is not None:\n        inputs['residual'] = residual\n    if bias is not None:\n        inputs['bias'] = bias\n    helper.append_op(type='fused_bias_residual_layernorm', inputs=inputs, attrs={'epsilon': epsilon, 'residual_alpha': residual_alpha, 'begin_norm_axis': begin_norm_axis, 'quant_scale': quant_scale, 'quant_round_type': quant_round_type, 'quant_max_bound': quant_max_bound, 'quant_min_bound': quant_min_bound}, outputs=outputs_dict)\n    return (out, residual_out) if residual is not None else out",
            "def fused_layer_norm(x, norm_weight, norm_bias, epsilon, residual_alpha=1.0, begin_norm_axis=1, bias=None, residual=None, quant_scale=-1, quant_round_type=0, quant_max_bound=0, quant_min_bound=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Apply Fused LayerNorm kernel. Also support LayerNorm(bias + residual_alpha * residual + x) fused pattern.\\n\\n    when norm_weight and norm_bias is None, it return fused (bias + residual_alpha * residual + x)\\n\\n    Args:\\n        x (Tensor): the input Tensor..\\n        norm_weight (Tensor): the weight Tensor to affine output.\\n        norm_bias (Tensor): the bias Tensor to affine output.\\n        epsilon (float): a small float number to avoid divide 0.\\n        residual_alpha (float): a scale factor for residual. default is 1.\\n        begin_norm_axis (int): the begin axis to normalize. default is 1.\\n        bias (optional|Tensor): the previous layers's bias to fused.\\n        residual (optional|Tensor): the residual input to fused.\\n        quant_scale (float): the quant scale.\\n        quant_round_type (float): the quant round type.\\n        quant_max_bound (float): the quant max bound to clip.\\n        quant_min_bound (float): the quant min bound to clip.\\n\\n\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import paddle\\n            >>> paddle.device.set_device('gpu')\\n\\n            >>> paddle_x = paddle.cast(paddle.randn(shape=[32, 256]), dtype=paddle.float16)\\n            >>> paddle_weight = paddle.cast(paddle.randn(shape=[256]), dtype=paddle.float32)\\n            >>> paddle_bias = paddle.cast(paddle.randn(shape=[256]), dtype=paddle.float32)\\n            >>> epsilon = 1e-6\\n            >>> paddle_layernorm = paddle.incubate.nn.functional.fused_layer_norm(paddle_x, paddle_weight, paddle_bias, epsilon, 1)\\n    \"\n    if in_dynamic_mode():\n        return _C_ops.fused_bias_residual_layernorm(x, bias, residual, norm_weight, norm_bias, epsilon, residual_alpha, begin_norm_axis, quant_scale, quant_round_type, quant_max_bound, quant_min_bound)\n    helper = LayerHelper('fused_layernorm', **locals())\n    out = None\n    if quant_scale <= 0:\n        out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    else:\n        out = helper.create_variable_for_type_inference(dtype=paddle.int8)\n    outputs_dict = {}\n    outputs_dict['out'] = out\n    outputs_dict['mean'] = helper.create_variable_for_type_inference(dtype=paddle.float32)\n    outputs_dict['variance'] = helper.create_variable_for_type_inference(dtype=paddle.float32)\n    residual_out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    outputs_dict['residual_out'] = residual_out\n    inputs = {'x': x}\n    if norm_weight is not None:\n        inputs['norm_weight'] = norm_weight\n    if norm_bias is not None:\n        inputs['norm_bias'] = norm_bias\n    if residual is not None:\n        inputs['residual'] = residual\n    if bias is not None:\n        inputs['bias'] = bias\n    helper.append_op(type='fused_bias_residual_layernorm', inputs=inputs, attrs={'epsilon': epsilon, 'residual_alpha': residual_alpha, 'begin_norm_axis': begin_norm_axis, 'quant_scale': quant_scale, 'quant_round_type': quant_round_type, 'quant_max_bound': quant_max_bound, 'quant_min_bound': quant_min_bound}, outputs=outputs_dict)\n    return (out, residual_out) if residual is not None else out",
            "def fused_layer_norm(x, norm_weight, norm_bias, epsilon, residual_alpha=1.0, begin_norm_axis=1, bias=None, residual=None, quant_scale=-1, quant_round_type=0, quant_max_bound=0, quant_min_bound=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Apply Fused LayerNorm kernel. Also support LayerNorm(bias + residual_alpha * residual + x) fused pattern.\\n\\n    when norm_weight and norm_bias is None, it return fused (bias + residual_alpha * residual + x)\\n\\n    Args:\\n        x (Tensor): the input Tensor..\\n        norm_weight (Tensor): the weight Tensor to affine output.\\n        norm_bias (Tensor): the bias Tensor to affine output.\\n        epsilon (float): a small float number to avoid divide 0.\\n        residual_alpha (float): a scale factor for residual. default is 1.\\n        begin_norm_axis (int): the begin axis to normalize. default is 1.\\n        bias (optional|Tensor): the previous layers's bias to fused.\\n        residual (optional|Tensor): the residual input to fused.\\n        quant_scale (float): the quant scale.\\n        quant_round_type (float): the quant round type.\\n        quant_max_bound (float): the quant max bound to clip.\\n        quant_min_bound (float): the quant min bound to clip.\\n\\n\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import paddle\\n            >>> paddle.device.set_device('gpu')\\n\\n            >>> paddle_x = paddle.cast(paddle.randn(shape=[32, 256]), dtype=paddle.float16)\\n            >>> paddle_weight = paddle.cast(paddle.randn(shape=[256]), dtype=paddle.float32)\\n            >>> paddle_bias = paddle.cast(paddle.randn(shape=[256]), dtype=paddle.float32)\\n            >>> epsilon = 1e-6\\n            >>> paddle_layernorm = paddle.incubate.nn.functional.fused_layer_norm(paddle_x, paddle_weight, paddle_bias, epsilon, 1)\\n    \"\n    if in_dynamic_mode():\n        return _C_ops.fused_bias_residual_layernorm(x, bias, residual, norm_weight, norm_bias, epsilon, residual_alpha, begin_norm_axis, quant_scale, quant_round_type, quant_max_bound, quant_min_bound)\n    helper = LayerHelper('fused_layernorm', **locals())\n    out = None\n    if quant_scale <= 0:\n        out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    else:\n        out = helper.create_variable_for_type_inference(dtype=paddle.int8)\n    outputs_dict = {}\n    outputs_dict['out'] = out\n    outputs_dict['mean'] = helper.create_variable_for_type_inference(dtype=paddle.float32)\n    outputs_dict['variance'] = helper.create_variable_for_type_inference(dtype=paddle.float32)\n    residual_out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    outputs_dict['residual_out'] = residual_out\n    inputs = {'x': x}\n    if norm_weight is not None:\n        inputs['norm_weight'] = norm_weight\n    if norm_bias is not None:\n        inputs['norm_bias'] = norm_bias\n    if residual is not None:\n        inputs['residual'] = residual\n    if bias is not None:\n        inputs['bias'] = bias\n    helper.append_op(type='fused_bias_residual_layernorm', inputs=inputs, attrs={'epsilon': epsilon, 'residual_alpha': residual_alpha, 'begin_norm_axis': begin_norm_axis, 'quant_scale': quant_scale, 'quant_round_type': quant_round_type, 'quant_max_bound': quant_max_bound, 'quant_min_bound': quant_min_bound}, outputs=outputs_dict)\n    return (out, residual_out) if residual is not None else out",
            "def fused_layer_norm(x, norm_weight, norm_bias, epsilon, residual_alpha=1.0, begin_norm_axis=1, bias=None, residual=None, quant_scale=-1, quant_round_type=0, quant_max_bound=0, quant_min_bound=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Apply Fused LayerNorm kernel. Also support LayerNorm(bias + residual_alpha * residual + x) fused pattern.\\n\\n    when norm_weight and norm_bias is None, it return fused (bias + residual_alpha * residual + x)\\n\\n    Args:\\n        x (Tensor): the input Tensor..\\n        norm_weight (Tensor): the weight Tensor to affine output.\\n        norm_bias (Tensor): the bias Tensor to affine output.\\n        epsilon (float): a small float number to avoid divide 0.\\n        residual_alpha (float): a scale factor for residual. default is 1.\\n        begin_norm_axis (int): the begin axis to normalize. default is 1.\\n        bias (optional|Tensor): the previous layers's bias to fused.\\n        residual (optional|Tensor): the residual input to fused.\\n        quant_scale (float): the quant scale.\\n        quant_round_type (float): the quant round type.\\n        quant_max_bound (float): the quant max bound to clip.\\n        quant_min_bound (float): the quant min bound to clip.\\n\\n\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import paddle\\n            >>> paddle.device.set_device('gpu')\\n\\n            >>> paddle_x = paddle.cast(paddle.randn(shape=[32, 256]), dtype=paddle.float16)\\n            >>> paddle_weight = paddle.cast(paddle.randn(shape=[256]), dtype=paddle.float32)\\n            >>> paddle_bias = paddle.cast(paddle.randn(shape=[256]), dtype=paddle.float32)\\n            >>> epsilon = 1e-6\\n            >>> paddle_layernorm = paddle.incubate.nn.functional.fused_layer_norm(paddle_x, paddle_weight, paddle_bias, epsilon, 1)\\n    \"\n    if in_dynamic_mode():\n        return _C_ops.fused_bias_residual_layernorm(x, bias, residual, norm_weight, norm_bias, epsilon, residual_alpha, begin_norm_axis, quant_scale, quant_round_type, quant_max_bound, quant_min_bound)\n    helper = LayerHelper('fused_layernorm', **locals())\n    out = None\n    if quant_scale <= 0:\n        out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    else:\n        out = helper.create_variable_for_type_inference(dtype=paddle.int8)\n    outputs_dict = {}\n    outputs_dict['out'] = out\n    outputs_dict['mean'] = helper.create_variable_for_type_inference(dtype=paddle.float32)\n    outputs_dict['variance'] = helper.create_variable_for_type_inference(dtype=paddle.float32)\n    residual_out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    outputs_dict['residual_out'] = residual_out\n    inputs = {'x': x}\n    if norm_weight is not None:\n        inputs['norm_weight'] = norm_weight\n    if norm_bias is not None:\n        inputs['norm_bias'] = norm_bias\n    if residual is not None:\n        inputs['residual'] = residual\n    if bias is not None:\n        inputs['bias'] = bias\n    helper.append_op(type='fused_bias_residual_layernorm', inputs=inputs, attrs={'epsilon': epsilon, 'residual_alpha': residual_alpha, 'begin_norm_axis': begin_norm_axis, 'quant_scale': quant_scale, 'quant_round_type': quant_round_type, 'quant_max_bound': quant_max_bound, 'quant_min_bound': quant_min_bound}, outputs=outputs_dict)\n    return (out, residual_out) if residual is not None else out",
            "def fused_layer_norm(x, norm_weight, norm_bias, epsilon, residual_alpha=1.0, begin_norm_axis=1, bias=None, residual=None, quant_scale=-1, quant_round_type=0, quant_max_bound=0, quant_min_bound=0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Apply Fused LayerNorm kernel. Also support LayerNorm(bias + residual_alpha * residual + x) fused pattern.\\n\\n    when norm_weight and norm_bias is None, it return fused (bias + residual_alpha * residual + x)\\n\\n    Args:\\n        x (Tensor): the input Tensor..\\n        norm_weight (Tensor): the weight Tensor to affine output.\\n        norm_bias (Tensor): the bias Tensor to affine output.\\n        epsilon (float): a small float number to avoid divide 0.\\n        residual_alpha (float): a scale factor for residual. default is 1.\\n        begin_norm_axis (int): the begin axis to normalize. default is 1.\\n        bias (optional|Tensor): the previous layers's bias to fused.\\n        residual (optional|Tensor): the residual input to fused.\\n        quant_scale (float): the quant scale.\\n        quant_round_type (float): the quant round type.\\n        quant_max_bound (float): the quant max bound to clip.\\n        quant_min_bound (float): the quant min bound to clip.\\n\\n\\n    Returns:\\n        Tensor: the output Tensor.\\n\\n    Examples:\\n        .. code-block:: python\\n\\n            >>> # doctest: +REQUIRES(env:GPU)\\n            >>> import paddle\\n            >>> paddle.device.set_device('gpu')\\n\\n            >>> paddle_x = paddle.cast(paddle.randn(shape=[32, 256]), dtype=paddle.float16)\\n            >>> paddle_weight = paddle.cast(paddle.randn(shape=[256]), dtype=paddle.float32)\\n            >>> paddle_bias = paddle.cast(paddle.randn(shape=[256]), dtype=paddle.float32)\\n            >>> epsilon = 1e-6\\n            >>> paddle_layernorm = paddle.incubate.nn.functional.fused_layer_norm(paddle_x, paddle_weight, paddle_bias, epsilon, 1)\\n    \"\n    if in_dynamic_mode():\n        return _C_ops.fused_bias_residual_layernorm(x, bias, residual, norm_weight, norm_bias, epsilon, residual_alpha, begin_norm_axis, quant_scale, quant_round_type, quant_max_bound, quant_min_bound)\n    helper = LayerHelper('fused_layernorm', **locals())\n    out = None\n    if quant_scale <= 0:\n        out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    else:\n        out = helper.create_variable_for_type_inference(dtype=paddle.int8)\n    outputs_dict = {}\n    outputs_dict['out'] = out\n    outputs_dict['mean'] = helper.create_variable_for_type_inference(dtype=paddle.float32)\n    outputs_dict['variance'] = helper.create_variable_for_type_inference(dtype=paddle.float32)\n    residual_out = helper.create_variable_for_type_inference(dtype=x.dtype)\n    outputs_dict['residual_out'] = residual_out\n    inputs = {'x': x}\n    if norm_weight is not None:\n        inputs['norm_weight'] = norm_weight\n    if norm_bias is not None:\n        inputs['norm_bias'] = norm_bias\n    if residual is not None:\n        inputs['residual'] = residual\n    if bias is not None:\n        inputs['bias'] = bias\n    helper.append_op(type='fused_bias_residual_layernorm', inputs=inputs, attrs={'epsilon': epsilon, 'residual_alpha': residual_alpha, 'begin_norm_axis': begin_norm_axis, 'quant_scale': quant_scale, 'quant_round_type': quant_round_type, 'quant_max_bound': quant_max_bound, 'quant_min_bound': quant_min_bound}, outputs=outputs_dict)\n    return (out, residual_out) if residual is not None else out"
        ]
    }
]