[
    {
        "func_name": "tensor_parallel_sync_filter_fn",
        "original": "def tensor_parallel_sync_filter_fn(param, pos_emb=True, layer_norm=True, bias=True):\n    \"\"\"\n    Layer fliter function for tensor parallelism transformer.\n\n    In tensor parallelism of transformer like model, there is 4 kind of param\n    that are supposed to be the same in all tensor parallel peers:\n        * position embedding\n        * scale of layer norm\n        * bias of layer norm\n        * bias of row parallel linear\n\n    set corresponding input args to select specific layers.\n    NOTE  adopting the param name pattern for different transformer blocks.\n    \"\"\"\n    p_name = param.name\n    if pos_emb and p_name.startswith('pos_embedding'):\n        return True\n    elif layer_norm and p_name.endswith('_layer_norm_bias'):\n        return True\n    elif layer_norm and p_name.endswith('_layer_norm_scale'):\n        return True\n    elif bias and '.b_' in p_name and (param.is_distributed is False):\n        return True\n    else:\n        return False",
        "mutated": [
            "def tensor_parallel_sync_filter_fn(param, pos_emb=True, layer_norm=True, bias=True):\n    if False:\n        i = 10\n    '\\n    Layer fliter function for tensor parallelism transformer.\\n\\n    In tensor parallelism of transformer like model, there is 4 kind of param\\n    that are supposed to be the same in all tensor parallel peers:\\n        * position embedding\\n        * scale of layer norm\\n        * bias of layer norm\\n        * bias of row parallel linear\\n\\n    set corresponding input args to select specific layers.\\n    NOTE  adopting the param name pattern for different transformer blocks.\\n    '\n    p_name = param.name\n    if pos_emb and p_name.startswith('pos_embedding'):\n        return True\n    elif layer_norm and p_name.endswith('_layer_norm_bias'):\n        return True\n    elif layer_norm and p_name.endswith('_layer_norm_scale'):\n        return True\n    elif bias and '.b_' in p_name and (param.is_distributed is False):\n        return True\n    else:\n        return False",
            "def tensor_parallel_sync_filter_fn(param, pos_emb=True, layer_norm=True, bias=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Layer fliter function for tensor parallelism transformer.\\n\\n    In tensor parallelism of transformer like model, there is 4 kind of param\\n    that are supposed to be the same in all tensor parallel peers:\\n        * position embedding\\n        * scale of layer norm\\n        * bias of layer norm\\n        * bias of row parallel linear\\n\\n    set corresponding input args to select specific layers.\\n    NOTE  adopting the param name pattern for different transformer blocks.\\n    '\n    p_name = param.name\n    if pos_emb and p_name.startswith('pos_embedding'):\n        return True\n    elif layer_norm and p_name.endswith('_layer_norm_bias'):\n        return True\n    elif layer_norm and p_name.endswith('_layer_norm_scale'):\n        return True\n    elif bias and '.b_' in p_name and (param.is_distributed is False):\n        return True\n    else:\n        return False",
            "def tensor_parallel_sync_filter_fn(param, pos_emb=True, layer_norm=True, bias=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Layer fliter function for tensor parallelism transformer.\\n\\n    In tensor parallelism of transformer like model, there is 4 kind of param\\n    that are supposed to be the same in all tensor parallel peers:\\n        * position embedding\\n        * scale of layer norm\\n        * bias of layer norm\\n        * bias of row parallel linear\\n\\n    set corresponding input args to select specific layers.\\n    NOTE  adopting the param name pattern for different transformer blocks.\\n    '\n    p_name = param.name\n    if pos_emb and p_name.startswith('pos_embedding'):\n        return True\n    elif layer_norm and p_name.endswith('_layer_norm_bias'):\n        return True\n    elif layer_norm and p_name.endswith('_layer_norm_scale'):\n        return True\n    elif bias and '.b_' in p_name and (param.is_distributed is False):\n        return True\n    else:\n        return False",
            "def tensor_parallel_sync_filter_fn(param, pos_emb=True, layer_norm=True, bias=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Layer fliter function for tensor parallelism transformer.\\n\\n    In tensor parallelism of transformer like model, there is 4 kind of param\\n    that are supposed to be the same in all tensor parallel peers:\\n        * position embedding\\n        * scale of layer norm\\n        * bias of layer norm\\n        * bias of row parallel linear\\n\\n    set corresponding input args to select specific layers.\\n    NOTE  adopting the param name pattern for different transformer blocks.\\n    '\n    p_name = param.name\n    if pos_emb and p_name.startswith('pos_embedding'):\n        return True\n    elif layer_norm and p_name.endswith('_layer_norm_bias'):\n        return True\n    elif layer_norm and p_name.endswith('_layer_norm_scale'):\n        return True\n    elif bias and '.b_' in p_name and (param.is_distributed is False):\n        return True\n    else:\n        return False",
            "def tensor_parallel_sync_filter_fn(param, pos_emb=True, layer_norm=True, bias=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Layer fliter function for tensor parallelism transformer.\\n\\n    In tensor parallelism of transformer like model, there is 4 kind of param\\n    that are supposed to be the same in all tensor parallel peers:\\n        * position embedding\\n        * scale of layer norm\\n        * bias of layer norm\\n        * bias of row parallel linear\\n\\n    set corresponding input args to select specific layers.\\n    NOTE  adopting the param name pattern for different transformer blocks.\\n    '\n    p_name = param.name\n    if pos_emb and p_name.startswith('pos_embedding'):\n        return True\n    elif layer_norm and p_name.endswith('_layer_norm_bias'):\n        return True\n    elif layer_norm and p_name.endswith('_layer_norm_scale'):\n        return True\n    elif bias and '.b_' in p_name and (param.is_distributed is False):\n        return True\n    else:\n        return False"
        ]
    },
    {
        "func_name": "resolute_tensor_parallel_ring_id",
        "original": "def resolute_tensor_parallel_ring_id(program):\n    ops = program.global_block().ops\n    ring_id = None\n    for op in ops:\n        if op.type == 'c_identity':\n            if ring_id is None:\n                ring_id = int(op.attr('ring_id'))\n            else:\n                assert ring_id == int(op.attr('ring_id')), 'Found two different ring_id for Tensor Parallel: ring_id={} and ring_id={}.'.format(ring_id, int(op.attr('ring_id')))\n    assert ring_id is not None, 'Could NOT found ring_id for Tensor Parallel.'\n    return ring_id",
        "mutated": [
            "def resolute_tensor_parallel_ring_id(program):\n    if False:\n        i = 10\n    ops = program.global_block().ops\n    ring_id = None\n    for op in ops:\n        if op.type == 'c_identity':\n            if ring_id is None:\n                ring_id = int(op.attr('ring_id'))\n            else:\n                assert ring_id == int(op.attr('ring_id')), 'Found two different ring_id for Tensor Parallel: ring_id={} and ring_id={}.'.format(ring_id, int(op.attr('ring_id')))\n    assert ring_id is not None, 'Could NOT found ring_id for Tensor Parallel.'\n    return ring_id",
            "def resolute_tensor_parallel_ring_id(program):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ops = program.global_block().ops\n    ring_id = None\n    for op in ops:\n        if op.type == 'c_identity':\n            if ring_id is None:\n                ring_id = int(op.attr('ring_id'))\n            else:\n                assert ring_id == int(op.attr('ring_id')), 'Found two different ring_id for Tensor Parallel: ring_id={} and ring_id={}.'.format(ring_id, int(op.attr('ring_id')))\n    assert ring_id is not None, 'Could NOT found ring_id for Tensor Parallel.'\n    return ring_id",
            "def resolute_tensor_parallel_ring_id(program):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ops = program.global_block().ops\n    ring_id = None\n    for op in ops:\n        if op.type == 'c_identity':\n            if ring_id is None:\n                ring_id = int(op.attr('ring_id'))\n            else:\n                assert ring_id == int(op.attr('ring_id')), 'Found two different ring_id for Tensor Parallel: ring_id={} and ring_id={}.'.format(ring_id, int(op.attr('ring_id')))\n    assert ring_id is not None, 'Could NOT found ring_id for Tensor Parallel.'\n    return ring_id",
            "def resolute_tensor_parallel_ring_id(program):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ops = program.global_block().ops\n    ring_id = None\n    for op in ops:\n        if op.type == 'c_identity':\n            if ring_id is None:\n                ring_id = int(op.attr('ring_id'))\n            else:\n                assert ring_id == int(op.attr('ring_id')), 'Found two different ring_id for Tensor Parallel: ring_id={} and ring_id={}.'.format(ring_id, int(op.attr('ring_id')))\n    assert ring_id is not None, 'Could NOT found ring_id for Tensor Parallel.'\n    return ring_id",
            "def resolute_tensor_parallel_ring_id(program):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ops = program.global_block().ops\n    ring_id = None\n    for op in ops:\n        if op.type == 'c_identity':\n            if ring_id is None:\n                ring_id = int(op.attr('ring_id'))\n            else:\n                assert ring_id == int(op.attr('ring_id')), 'Found two different ring_id for Tensor Parallel: ring_id={} and ring_id={}.'.format(ring_id, int(op.attr('ring_id')))\n    assert ring_id is not None, 'Could NOT found ring_id for Tensor Parallel.'\n    return ring_id"
        ]
    },
    {
        "func_name": "copy_parameters",
        "original": "def copy_parameters(block_, params):\n    for param in params:\n        new_p = Parameter(block=block_, shape=param.shape, dtype=param.dtype, type=param.type, lod_level=param.lod_level if param.type == core.VarDesc.VarType.LOD_TENSOR else None, stop_gradient=param.stop_gradient, trainable=param.trainable, optimize_attr=param.optimize_attr, regularizer=param.regularizer, error_clip=param.error_clip, name=param.name)\n        assert param.is_distributed is False, f'Try to sync Distribted Parameter: {param}'\n        new_p.is_distributed = False\n    block_.vars[new_p.name] = new_p",
        "mutated": [
            "def copy_parameters(block_, params):\n    if False:\n        i = 10\n    for param in params:\n        new_p = Parameter(block=block_, shape=param.shape, dtype=param.dtype, type=param.type, lod_level=param.lod_level if param.type == core.VarDesc.VarType.LOD_TENSOR else None, stop_gradient=param.stop_gradient, trainable=param.trainable, optimize_attr=param.optimize_attr, regularizer=param.regularizer, error_clip=param.error_clip, name=param.name)\n        assert param.is_distributed is False, f'Try to sync Distribted Parameter: {param}'\n        new_p.is_distributed = False\n    block_.vars[new_p.name] = new_p",
            "def copy_parameters(block_, params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for param in params:\n        new_p = Parameter(block=block_, shape=param.shape, dtype=param.dtype, type=param.type, lod_level=param.lod_level if param.type == core.VarDesc.VarType.LOD_TENSOR else None, stop_gradient=param.stop_gradient, trainable=param.trainable, optimize_attr=param.optimize_attr, regularizer=param.regularizer, error_clip=param.error_clip, name=param.name)\n        assert param.is_distributed is False, f'Try to sync Distribted Parameter: {param}'\n        new_p.is_distributed = False\n    block_.vars[new_p.name] = new_p",
            "def copy_parameters(block_, params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for param in params:\n        new_p = Parameter(block=block_, shape=param.shape, dtype=param.dtype, type=param.type, lod_level=param.lod_level if param.type == core.VarDesc.VarType.LOD_TENSOR else None, stop_gradient=param.stop_gradient, trainable=param.trainable, optimize_attr=param.optimize_attr, regularizer=param.regularizer, error_clip=param.error_clip, name=param.name)\n        assert param.is_distributed is False, f'Try to sync Distribted Parameter: {param}'\n        new_p.is_distributed = False\n    block_.vars[new_p.name] = new_p",
            "def copy_parameters(block_, params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for param in params:\n        new_p = Parameter(block=block_, shape=param.shape, dtype=param.dtype, type=param.type, lod_level=param.lod_level if param.type == core.VarDesc.VarType.LOD_TENSOR else None, stop_gradient=param.stop_gradient, trainable=param.trainable, optimize_attr=param.optimize_attr, regularizer=param.regularizer, error_clip=param.error_clip, name=param.name)\n        assert param.is_distributed is False, f'Try to sync Distribted Parameter: {param}'\n        new_p.is_distributed = False\n    block_.vars[new_p.name] = new_p",
            "def copy_parameters(block_, params):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for param in params:\n        new_p = Parameter(block=block_, shape=param.shape, dtype=param.dtype, type=param.type, lod_level=param.lod_level if param.type == core.VarDesc.VarType.LOD_TENSOR else None, stop_gradient=param.stop_gradient, trainable=param.trainable, optimize_attr=param.optimize_attr, regularizer=param.regularizer, error_clip=param.error_clip, name=param.name)\n        assert param.is_distributed is False, f'Try to sync Distribted Parameter: {param}'\n        new_p.is_distributed = False\n    block_.vars[new_p.name] = new_p"
        ]
    },
    {
        "func_name": "insert_sync_op",
        "original": "def insert_sync_op(block, idx, tp_degree, sync_mode, sync_ring_id, src_rank, varname, op_role):\n    if sync_mode == 'broadcast':\n        block._insert_op_without_sync(idx, type='c_broadcast', inputs={'X': varname}, outputs={'Out': varname}, attrs={'ring_id': sync_ring_id, 'root': src_rank, 'use_calc_stream': True, OP_ROLE_KEY: op_role})\n    elif sync_mode == 'average':\n        block._insert_op_without_sync(idx, type='scale', inputs={'X': varname}, outputs={'Out': varname}, attrs={'scale': 1.0 / tp_degree, OP_ROLE_KEY: op_role})\n        block._insert_op_without_sync(idx, type='c_allreduce_sum', inputs={'X': varname}, outputs={'Out': varname}, attrs={'ring_id': sync_ring_id, 'use_calc_stream': True, OP_ROLE_KEY: op_role})\n    else:\n        raise NotImplementedError(f'Sync mode of [{sync_mode}] is NOT supported.')",
        "mutated": [
            "def insert_sync_op(block, idx, tp_degree, sync_mode, sync_ring_id, src_rank, varname, op_role):\n    if False:\n        i = 10\n    if sync_mode == 'broadcast':\n        block._insert_op_without_sync(idx, type='c_broadcast', inputs={'X': varname}, outputs={'Out': varname}, attrs={'ring_id': sync_ring_id, 'root': src_rank, 'use_calc_stream': True, OP_ROLE_KEY: op_role})\n    elif sync_mode == 'average':\n        block._insert_op_without_sync(idx, type='scale', inputs={'X': varname}, outputs={'Out': varname}, attrs={'scale': 1.0 / tp_degree, OP_ROLE_KEY: op_role})\n        block._insert_op_without_sync(idx, type='c_allreduce_sum', inputs={'X': varname}, outputs={'Out': varname}, attrs={'ring_id': sync_ring_id, 'use_calc_stream': True, OP_ROLE_KEY: op_role})\n    else:\n        raise NotImplementedError(f'Sync mode of [{sync_mode}] is NOT supported.')",
            "def insert_sync_op(block, idx, tp_degree, sync_mode, sync_ring_id, src_rank, varname, op_role):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if sync_mode == 'broadcast':\n        block._insert_op_without_sync(idx, type='c_broadcast', inputs={'X': varname}, outputs={'Out': varname}, attrs={'ring_id': sync_ring_id, 'root': src_rank, 'use_calc_stream': True, OP_ROLE_KEY: op_role})\n    elif sync_mode == 'average':\n        block._insert_op_without_sync(idx, type='scale', inputs={'X': varname}, outputs={'Out': varname}, attrs={'scale': 1.0 / tp_degree, OP_ROLE_KEY: op_role})\n        block._insert_op_without_sync(idx, type='c_allreduce_sum', inputs={'X': varname}, outputs={'Out': varname}, attrs={'ring_id': sync_ring_id, 'use_calc_stream': True, OP_ROLE_KEY: op_role})\n    else:\n        raise NotImplementedError(f'Sync mode of [{sync_mode}] is NOT supported.')",
            "def insert_sync_op(block, idx, tp_degree, sync_mode, sync_ring_id, src_rank, varname, op_role):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if sync_mode == 'broadcast':\n        block._insert_op_without_sync(idx, type='c_broadcast', inputs={'X': varname}, outputs={'Out': varname}, attrs={'ring_id': sync_ring_id, 'root': src_rank, 'use_calc_stream': True, OP_ROLE_KEY: op_role})\n    elif sync_mode == 'average':\n        block._insert_op_without_sync(idx, type='scale', inputs={'X': varname}, outputs={'Out': varname}, attrs={'scale': 1.0 / tp_degree, OP_ROLE_KEY: op_role})\n        block._insert_op_without_sync(idx, type='c_allreduce_sum', inputs={'X': varname}, outputs={'Out': varname}, attrs={'ring_id': sync_ring_id, 'use_calc_stream': True, OP_ROLE_KEY: op_role})\n    else:\n        raise NotImplementedError(f'Sync mode of [{sync_mode}] is NOT supported.')",
            "def insert_sync_op(block, idx, tp_degree, sync_mode, sync_ring_id, src_rank, varname, op_role):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if sync_mode == 'broadcast':\n        block._insert_op_without_sync(idx, type='c_broadcast', inputs={'X': varname}, outputs={'Out': varname}, attrs={'ring_id': sync_ring_id, 'root': src_rank, 'use_calc_stream': True, OP_ROLE_KEY: op_role})\n    elif sync_mode == 'average':\n        block._insert_op_without_sync(idx, type='scale', inputs={'X': varname}, outputs={'Out': varname}, attrs={'scale': 1.0 / tp_degree, OP_ROLE_KEY: op_role})\n        block._insert_op_without_sync(idx, type='c_allreduce_sum', inputs={'X': varname}, outputs={'Out': varname}, attrs={'ring_id': sync_ring_id, 'use_calc_stream': True, OP_ROLE_KEY: op_role})\n    else:\n        raise NotImplementedError(f'Sync mode of [{sync_mode}] is NOT supported.')",
            "def insert_sync_op(block, idx, tp_degree, sync_mode, sync_ring_id, src_rank, varname, op_role):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if sync_mode == 'broadcast':\n        block._insert_op_without_sync(idx, type='c_broadcast', inputs={'X': varname}, outputs={'Out': varname}, attrs={'ring_id': sync_ring_id, 'root': src_rank, 'use_calc_stream': True, OP_ROLE_KEY: op_role})\n    elif sync_mode == 'average':\n        block._insert_op_without_sync(idx, type='scale', inputs={'X': varname}, outputs={'Out': varname}, attrs={'scale': 1.0 / tp_degree, OP_ROLE_KEY: op_role})\n        block._insert_op_without_sync(idx, type='c_allreduce_sum', inputs={'X': varname}, outputs={'Out': varname}, attrs={'ring_id': sync_ring_id, 'use_calc_stream': True, OP_ROLE_KEY: op_role})\n    else:\n        raise NotImplementedError(f'Sync mode of [{sync_mode}] is NOT supported.')"
        ]
    },
    {
        "func_name": "insert_synchronization",
        "original": "def insert_synchronization(block, params_to_sync, tp_degree, sync_ring_id, sync_param, sync_grad, sync_moment, sync_mode, src_rank):\n    unsync_param_names = [p.name for p in params_to_sync]\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type in _supported_optimizer_type:\n            assert 'Param' in op.input_names\n            assert len(op.input('Param')) == 1\n            param_name = op.input('Param')[0]\n            op_role = op.attr(OP_ROLE_KEY)\n            if param_name in unsync_param_names:\n                unsync_param_names.remove(param_name)\n                if sync_param:\n                    assert 'ParamOut' in op.output_names and op.output('ParamOut')[0] == param_name\n                    insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, param_name, op_role)\n                    if 'MasterParamOut' in op.output_names and len(op.output('MasterParamOut')) == 1:\n                        sync_var = op.output('MasterParamOut')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                if sync_moment:\n                    if 'Moment1Out' in op.output_names and len(op.output('Moment1Out')) == 1:\n                        sync_var = op.output('Moment1Out')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                    if 'Moment2Out' in op.output_names and len(op.output('Moment2Out')) == 1:\n                        sync_var = op.output('Moment2Out')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                if sync_grad:\n                    assert 'Grad' in op.input_names and len(op.input('Grad')) == 1\n                    sync_var = op.input('Grad')[0]\n                    insert_sync_op(block, idx, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n    assert len(unsync_param_names) == 0, f'The following param is unsync by some error: {unsync_param_names}'",
        "mutated": [
            "def insert_synchronization(block, params_to_sync, tp_degree, sync_ring_id, sync_param, sync_grad, sync_moment, sync_mode, src_rank):\n    if False:\n        i = 10\n    unsync_param_names = [p.name for p in params_to_sync]\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type in _supported_optimizer_type:\n            assert 'Param' in op.input_names\n            assert len(op.input('Param')) == 1\n            param_name = op.input('Param')[0]\n            op_role = op.attr(OP_ROLE_KEY)\n            if param_name in unsync_param_names:\n                unsync_param_names.remove(param_name)\n                if sync_param:\n                    assert 'ParamOut' in op.output_names and op.output('ParamOut')[0] == param_name\n                    insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, param_name, op_role)\n                    if 'MasterParamOut' in op.output_names and len(op.output('MasterParamOut')) == 1:\n                        sync_var = op.output('MasterParamOut')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                if sync_moment:\n                    if 'Moment1Out' in op.output_names and len(op.output('Moment1Out')) == 1:\n                        sync_var = op.output('Moment1Out')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                    if 'Moment2Out' in op.output_names and len(op.output('Moment2Out')) == 1:\n                        sync_var = op.output('Moment2Out')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                if sync_grad:\n                    assert 'Grad' in op.input_names and len(op.input('Grad')) == 1\n                    sync_var = op.input('Grad')[0]\n                    insert_sync_op(block, idx, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n    assert len(unsync_param_names) == 0, f'The following param is unsync by some error: {unsync_param_names}'",
            "def insert_synchronization(block, params_to_sync, tp_degree, sync_ring_id, sync_param, sync_grad, sync_moment, sync_mode, src_rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    unsync_param_names = [p.name for p in params_to_sync]\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type in _supported_optimizer_type:\n            assert 'Param' in op.input_names\n            assert len(op.input('Param')) == 1\n            param_name = op.input('Param')[0]\n            op_role = op.attr(OP_ROLE_KEY)\n            if param_name in unsync_param_names:\n                unsync_param_names.remove(param_name)\n                if sync_param:\n                    assert 'ParamOut' in op.output_names and op.output('ParamOut')[0] == param_name\n                    insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, param_name, op_role)\n                    if 'MasterParamOut' in op.output_names and len(op.output('MasterParamOut')) == 1:\n                        sync_var = op.output('MasterParamOut')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                if sync_moment:\n                    if 'Moment1Out' in op.output_names and len(op.output('Moment1Out')) == 1:\n                        sync_var = op.output('Moment1Out')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                    if 'Moment2Out' in op.output_names and len(op.output('Moment2Out')) == 1:\n                        sync_var = op.output('Moment2Out')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                if sync_grad:\n                    assert 'Grad' in op.input_names and len(op.input('Grad')) == 1\n                    sync_var = op.input('Grad')[0]\n                    insert_sync_op(block, idx, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n    assert len(unsync_param_names) == 0, f'The following param is unsync by some error: {unsync_param_names}'",
            "def insert_synchronization(block, params_to_sync, tp_degree, sync_ring_id, sync_param, sync_grad, sync_moment, sync_mode, src_rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    unsync_param_names = [p.name for p in params_to_sync]\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type in _supported_optimizer_type:\n            assert 'Param' in op.input_names\n            assert len(op.input('Param')) == 1\n            param_name = op.input('Param')[0]\n            op_role = op.attr(OP_ROLE_KEY)\n            if param_name in unsync_param_names:\n                unsync_param_names.remove(param_name)\n                if sync_param:\n                    assert 'ParamOut' in op.output_names and op.output('ParamOut')[0] == param_name\n                    insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, param_name, op_role)\n                    if 'MasterParamOut' in op.output_names and len(op.output('MasterParamOut')) == 1:\n                        sync_var = op.output('MasterParamOut')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                if sync_moment:\n                    if 'Moment1Out' in op.output_names and len(op.output('Moment1Out')) == 1:\n                        sync_var = op.output('Moment1Out')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                    if 'Moment2Out' in op.output_names and len(op.output('Moment2Out')) == 1:\n                        sync_var = op.output('Moment2Out')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                if sync_grad:\n                    assert 'Grad' in op.input_names and len(op.input('Grad')) == 1\n                    sync_var = op.input('Grad')[0]\n                    insert_sync_op(block, idx, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n    assert len(unsync_param_names) == 0, f'The following param is unsync by some error: {unsync_param_names}'",
            "def insert_synchronization(block, params_to_sync, tp_degree, sync_ring_id, sync_param, sync_grad, sync_moment, sync_mode, src_rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    unsync_param_names = [p.name for p in params_to_sync]\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type in _supported_optimizer_type:\n            assert 'Param' in op.input_names\n            assert len(op.input('Param')) == 1\n            param_name = op.input('Param')[0]\n            op_role = op.attr(OP_ROLE_KEY)\n            if param_name in unsync_param_names:\n                unsync_param_names.remove(param_name)\n                if sync_param:\n                    assert 'ParamOut' in op.output_names and op.output('ParamOut')[0] == param_name\n                    insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, param_name, op_role)\n                    if 'MasterParamOut' in op.output_names and len(op.output('MasterParamOut')) == 1:\n                        sync_var = op.output('MasterParamOut')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                if sync_moment:\n                    if 'Moment1Out' in op.output_names and len(op.output('Moment1Out')) == 1:\n                        sync_var = op.output('Moment1Out')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                    if 'Moment2Out' in op.output_names and len(op.output('Moment2Out')) == 1:\n                        sync_var = op.output('Moment2Out')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                if sync_grad:\n                    assert 'Grad' in op.input_names and len(op.input('Grad')) == 1\n                    sync_var = op.input('Grad')[0]\n                    insert_sync_op(block, idx, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n    assert len(unsync_param_names) == 0, f'The following param is unsync by some error: {unsync_param_names}'",
            "def insert_synchronization(block, params_to_sync, tp_degree, sync_ring_id, sync_param, sync_grad, sync_moment, sync_mode, src_rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    unsync_param_names = [p.name for p in params_to_sync]\n    for (idx, op) in reversed(list(enumerate(block.ops))):\n        if op.type in _supported_optimizer_type:\n            assert 'Param' in op.input_names\n            assert len(op.input('Param')) == 1\n            param_name = op.input('Param')[0]\n            op_role = op.attr(OP_ROLE_KEY)\n            if param_name in unsync_param_names:\n                unsync_param_names.remove(param_name)\n                if sync_param:\n                    assert 'ParamOut' in op.output_names and op.output('ParamOut')[0] == param_name\n                    insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, param_name, op_role)\n                    if 'MasterParamOut' in op.output_names and len(op.output('MasterParamOut')) == 1:\n                        sync_var = op.output('MasterParamOut')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                if sync_moment:\n                    if 'Moment1Out' in op.output_names and len(op.output('Moment1Out')) == 1:\n                        sync_var = op.output('Moment1Out')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                    if 'Moment2Out' in op.output_names and len(op.output('Moment2Out')) == 1:\n                        sync_var = op.output('Moment2Out')[0]\n                        insert_sync_op(block, idx + 1, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n                if sync_grad:\n                    assert 'Grad' in op.input_names and len(op.input('Grad')) == 1\n                    sync_var = op.input('Grad')[0]\n                    insert_sync_op(block, idx, tp_degree, sync_mode, sync_ring_id, src_rank, sync_var, op_role)\n    assert len(unsync_param_names) == 0, f'The following param is unsync by some error: {unsync_param_names}'"
        ]
    },
    {
        "func_name": "add_extra_synchronization",
        "original": "def add_extra_synchronization(program, params_filter_fn=tensor_parallel_sync_filter_fn, tp_degree=8, sync_mode='broadcast', sync_param=True, sync_grad=False, sync_moment=False, src_rank=0, sync_ring_id=None):\n    \"\"\"\n    Inplace add extra synchronization for input program.\n\n    program(Paddle.Program): distributed train program.\n\n    params_filter_fn(callable): function to filter out parameter for synchronization.\n\n    sync_mode(string): select from\n        \"broadcast\": parameter is sync by broadcasted from 'src_rank' to all other ranks.\n        \"average\": paramter is sync by average amonge all ranks\n\n    src_rank(int): the src used in broadcast sync_mode.\n\n    sync_param(bool): extra synchronize parameters.\n\n    sync_grad(bool): extra synchronize gradients.\n\n    sync_grad(bool): extra synchronize optimizer momentum.\n\n    sync_ring_id(int): communicator id use for synchronization, if it is None, use the ring_id of tensor parallel.\n    \"\"\"\n    logger.info('Constructing Extra Parameter Synchronization.')\n    logger.info(f'Tensor Parallel Degree: {tp_degree}, Synchronization mode: {sync_mode}')\n    if program._pipeline_opt is not None:\n        assert program._pipeline_opt['section_program'] is not None, 'Pipeline is enable but section_program is None'\n        program = program._pipeline_opt['section_program']\n    params_to_sync = []\n    all_params = program.global_block().all_parameters()\n    for param in all_params:\n        if params_filter_fn(param):\n            params_to_sync.append(param)\n    logger.info('The following param are goning to be synchronization everytime the optimizer update phase of the program is runned: ')\n    logger.info([p.name for p in params_to_sync])\n    if sync_ring_id is None:\n        sync_ring_id = resolute_tensor_parallel_ring_id(program)\n    block = program.global_block()\n    insert_synchronization(block, params_to_sync, tp_degree, sync_ring_id, sync_param, sync_grad, sync_moment, sync_mode, src_rank)",
        "mutated": [
            "def add_extra_synchronization(program, params_filter_fn=tensor_parallel_sync_filter_fn, tp_degree=8, sync_mode='broadcast', sync_param=True, sync_grad=False, sync_moment=False, src_rank=0, sync_ring_id=None):\n    if False:\n        i = 10\n    '\\n    Inplace add extra synchronization for input program.\\n\\n    program(Paddle.Program): distributed train program.\\n\\n    params_filter_fn(callable): function to filter out parameter for synchronization.\\n\\n    sync_mode(string): select from\\n        \"broadcast\": parameter is sync by broadcasted from \\'src_rank\\' to all other ranks.\\n        \"average\": paramter is sync by average amonge all ranks\\n\\n    src_rank(int): the src used in broadcast sync_mode.\\n\\n    sync_param(bool): extra synchronize parameters.\\n\\n    sync_grad(bool): extra synchronize gradients.\\n\\n    sync_grad(bool): extra synchronize optimizer momentum.\\n\\n    sync_ring_id(int): communicator id use for synchronization, if it is None, use the ring_id of tensor parallel.\\n    '\n    logger.info('Constructing Extra Parameter Synchronization.')\n    logger.info(f'Tensor Parallel Degree: {tp_degree}, Synchronization mode: {sync_mode}')\n    if program._pipeline_opt is not None:\n        assert program._pipeline_opt['section_program'] is not None, 'Pipeline is enable but section_program is None'\n        program = program._pipeline_opt['section_program']\n    params_to_sync = []\n    all_params = program.global_block().all_parameters()\n    for param in all_params:\n        if params_filter_fn(param):\n            params_to_sync.append(param)\n    logger.info('The following param are goning to be synchronization everytime the optimizer update phase of the program is runned: ')\n    logger.info([p.name for p in params_to_sync])\n    if sync_ring_id is None:\n        sync_ring_id = resolute_tensor_parallel_ring_id(program)\n    block = program.global_block()\n    insert_synchronization(block, params_to_sync, tp_degree, sync_ring_id, sync_param, sync_grad, sync_moment, sync_mode, src_rank)",
            "def add_extra_synchronization(program, params_filter_fn=tensor_parallel_sync_filter_fn, tp_degree=8, sync_mode='broadcast', sync_param=True, sync_grad=False, sync_moment=False, src_rank=0, sync_ring_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Inplace add extra synchronization for input program.\\n\\n    program(Paddle.Program): distributed train program.\\n\\n    params_filter_fn(callable): function to filter out parameter for synchronization.\\n\\n    sync_mode(string): select from\\n        \"broadcast\": parameter is sync by broadcasted from \\'src_rank\\' to all other ranks.\\n        \"average\": paramter is sync by average amonge all ranks\\n\\n    src_rank(int): the src used in broadcast sync_mode.\\n\\n    sync_param(bool): extra synchronize parameters.\\n\\n    sync_grad(bool): extra synchronize gradients.\\n\\n    sync_grad(bool): extra synchronize optimizer momentum.\\n\\n    sync_ring_id(int): communicator id use for synchronization, if it is None, use the ring_id of tensor parallel.\\n    '\n    logger.info('Constructing Extra Parameter Synchronization.')\n    logger.info(f'Tensor Parallel Degree: {tp_degree}, Synchronization mode: {sync_mode}')\n    if program._pipeline_opt is not None:\n        assert program._pipeline_opt['section_program'] is not None, 'Pipeline is enable but section_program is None'\n        program = program._pipeline_opt['section_program']\n    params_to_sync = []\n    all_params = program.global_block().all_parameters()\n    for param in all_params:\n        if params_filter_fn(param):\n            params_to_sync.append(param)\n    logger.info('The following param are goning to be synchronization everytime the optimizer update phase of the program is runned: ')\n    logger.info([p.name for p in params_to_sync])\n    if sync_ring_id is None:\n        sync_ring_id = resolute_tensor_parallel_ring_id(program)\n    block = program.global_block()\n    insert_synchronization(block, params_to_sync, tp_degree, sync_ring_id, sync_param, sync_grad, sync_moment, sync_mode, src_rank)",
            "def add_extra_synchronization(program, params_filter_fn=tensor_parallel_sync_filter_fn, tp_degree=8, sync_mode='broadcast', sync_param=True, sync_grad=False, sync_moment=False, src_rank=0, sync_ring_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Inplace add extra synchronization for input program.\\n\\n    program(Paddle.Program): distributed train program.\\n\\n    params_filter_fn(callable): function to filter out parameter for synchronization.\\n\\n    sync_mode(string): select from\\n        \"broadcast\": parameter is sync by broadcasted from \\'src_rank\\' to all other ranks.\\n        \"average\": paramter is sync by average amonge all ranks\\n\\n    src_rank(int): the src used in broadcast sync_mode.\\n\\n    sync_param(bool): extra synchronize parameters.\\n\\n    sync_grad(bool): extra synchronize gradients.\\n\\n    sync_grad(bool): extra synchronize optimizer momentum.\\n\\n    sync_ring_id(int): communicator id use for synchronization, if it is None, use the ring_id of tensor parallel.\\n    '\n    logger.info('Constructing Extra Parameter Synchronization.')\n    logger.info(f'Tensor Parallel Degree: {tp_degree}, Synchronization mode: {sync_mode}')\n    if program._pipeline_opt is not None:\n        assert program._pipeline_opt['section_program'] is not None, 'Pipeline is enable but section_program is None'\n        program = program._pipeline_opt['section_program']\n    params_to_sync = []\n    all_params = program.global_block().all_parameters()\n    for param in all_params:\n        if params_filter_fn(param):\n            params_to_sync.append(param)\n    logger.info('The following param are goning to be synchronization everytime the optimizer update phase of the program is runned: ')\n    logger.info([p.name for p in params_to_sync])\n    if sync_ring_id is None:\n        sync_ring_id = resolute_tensor_parallel_ring_id(program)\n    block = program.global_block()\n    insert_synchronization(block, params_to_sync, tp_degree, sync_ring_id, sync_param, sync_grad, sync_moment, sync_mode, src_rank)",
            "def add_extra_synchronization(program, params_filter_fn=tensor_parallel_sync_filter_fn, tp_degree=8, sync_mode='broadcast', sync_param=True, sync_grad=False, sync_moment=False, src_rank=0, sync_ring_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Inplace add extra synchronization for input program.\\n\\n    program(Paddle.Program): distributed train program.\\n\\n    params_filter_fn(callable): function to filter out parameter for synchronization.\\n\\n    sync_mode(string): select from\\n        \"broadcast\": parameter is sync by broadcasted from \\'src_rank\\' to all other ranks.\\n        \"average\": paramter is sync by average amonge all ranks\\n\\n    src_rank(int): the src used in broadcast sync_mode.\\n\\n    sync_param(bool): extra synchronize parameters.\\n\\n    sync_grad(bool): extra synchronize gradients.\\n\\n    sync_grad(bool): extra synchronize optimizer momentum.\\n\\n    sync_ring_id(int): communicator id use for synchronization, if it is None, use the ring_id of tensor parallel.\\n    '\n    logger.info('Constructing Extra Parameter Synchronization.')\n    logger.info(f'Tensor Parallel Degree: {tp_degree}, Synchronization mode: {sync_mode}')\n    if program._pipeline_opt is not None:\n        assert program._pipeline_opt['section_program'] is not None, 'Pipeline is enable but section_program is None'\n        program = program._pipeline_opt['section_program']\n    params_to_sync = []\n    all_params = program.global_block().all_parameters()\n    for param in all_params:\n        if params_filter_fn(param):\n            params_to_sync.append(param)\n    logger.info('The following param are goning to be synchronization everytime the optimizer update phase of the program is runned: ')\n    logger.info([p.name for p in params_to_sync])\n    if sync_ring_id is None:\n        sync_ring_id = resolute_tensor_parallel_ring_id(program)\n    block = program.global_block()\n    insert_synchronization(block, params_to_sync, tp_degree, sync_ring_id, sync_param, sync_grad, sync_moment, sync_mode, src_rank)",
            "def add_extra_synchronization(program, params_filter_fn=tensor_parallel_sync_filter_fn, tp_degree=8, sync_mode='broadcast', sync_param=True, sync_grad=False, sync_moment=False, src_rank=0, sync_ring_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Inplace add extra synchronization for input program.\\n\\n    program(Paddle.Program): distributed train program.\\n\\n    params_filter_fn(callable): function to filter out parameter for synchronization.\\n\\n    sync_mode(string): select from\\n        \"broadcast\": parameter is sync by broadcasted from \\'src_rank\\' to all other ranks.\\n        \"average\": paramter is sync by average amonge all ranks\\n\\n    src_rank(int): the src used in broadcast sync_mode.\\n\\n    sync_param(bool): extra synchronize parameters.\\n\\n    sync_grad(bool): extra synchronize gradients.\\n\\n    sync_grad(bool): extra synchronize optimizer momentum.\\n\\n    sync_ring_id(int): communicator id use for synchronization, if it is None, use the ring_id of tensor parallel.\\n    '\n    logger.info('Constructing Extra Parameter Synchronization.')\n    logger.info(f'Tensor Parallel Degree: {tp_degree}, Synchronization mode: {sync_mode}')\n    if program._pipeline_opt is not None:\n        assert program._pipeline_opt['section_program'] is not None, 'Pipeline is enable but section_program is None'\n        program = program._pipeline_opt['section_program']\n    params_to_sync = []\n    all_params = program.global_block().all_parameters()\n    for param in all_params:\n        if params_filter_fn(param):\n            params_to_sync.append(param)\n    logger.info('The following param are goning to be synchronization everytime the optimizer update phase of the program is runned: ')\n    logger.info([p.name for p in params_to_sync])\n    if sync_ring_id is None:\n        sync_ring_id = resolute_tensor_parallel_ring_id(program)\n    block = program.global_block()\n    insert_synchronization(block, params_to_sync, tp_degree, sync_ring_id, sync_param, sync_grad, sync_moment, sync_mode, src_rank)"
        ]
    }
]