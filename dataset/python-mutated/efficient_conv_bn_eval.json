[
    {
        "func_name": "efficient_conv_bn_eval",
        "original": "def efficient_conv_bn_eval(bn: nn.modules.batchnorm._BatchNorm, conv: nn.modules.conv._ConvNd, x: torch.Tensor):\n    \"\"\"\n    Implementation based on https://arxiv.org/abs/2305.11624\n    \"Tune-Mode ConvBN Blocks For Efficient Transfer Learning\"\n    It leverages the associative law between convolution and affine transform,\n    i.e., normalize (weight conv feature) = (normalize weight) conv feature.\n    It works for Eval mode of ConvBN blocks during validation, and can be used\n    for **training** as well, but only if one sets `bn.training=False`. It\n     reduces memory footprint and computation cost, at the cost of slightly\n     reduced numerical stability.\n    Args:\n        bn (nn.modules.batchnorm._BatchNorm): a BatchNorm module.\n        conv (nn.modules.conv._ConvNd): a conv module\n        x (torch.Tensor): Input feature map.\n    \"\"\"\n    assert bn.running_var is not None\n    weight_on_the_fly = conv.weight\n    if conv.bias is not None:\n        bias_on_the_fly = conv.bias\n    else:\n        bias_on_the_fly = torch.zeros_like(bn.running_var)\n    if bn.weight is not None:\n        bn_weight = bn.weight\n    else:\n        bn_weight = torch.ones_like(bn.running_var)\n    if bn.bias is not None:\n        bn_bias = bn.bias\n    else:\n        bn_bias = torch.zeros_like(bn.running_var)\n    target_shape = [-1] + [1] * (conv.weight.ndim - 1)\n    if isinstance(conv, nn.modules.conv._ConvTransposeNd):\n        target_shape[:2] = [target_shape[1], target_shape[0]]\n    weight_coeff = torch.rsqrt(bn.running_var + bn.eps).reshape(target_shape)\n    coefff_on_the_fly = bn_weight.view_as(weight_coeff) * weight_coeff\n    weight_on_the_fly = weight_on_the_fly * coefff_on_the_fly\n    bias_on_the_fly = bn_bias + coefff_on_the_fly.flatten() * (bias_on_the_fly - bn.running_mean)\n    input = x\n    params = {'weight': weight_on_the_fly, 'bias': bias_on_the_fly}\n    output = functional_call(conv, params, input)\n    return output",
        "mutated": [
            "def efficient_conv_bn_eval(bn: nn.modules.batchnorm._BatchNorm, conv: nn.modules.conv._ConvNd, x: torch.Tensor):\n    if False:\n        i = 10\n    '\\n    Implementation based on https://arxiv.org/abs/2305.11624\\n    \"Tune-Mode ConvBN Blocks For Efficient Transfer Learning\"\\n    It leverages the associative law between convolution and affine transform,\\n    i.e., normalize (weight conv feature) = (normalize weight) conv feature.\\n    It works for Eval mode of ConvBN blocks during validation, and can be used\\n    for **training** as well, but only if one sets `bn.training=False`. It\\n     reduces memory footprint and computation cost, at the cost of slightly\\n     reduced numerical stability.\\n    Args:\\n        bn (nn.modules.batchnorm._BatchNorm): a BatchNorm module.\\n        conv (nn.modules.conv._ConvNd): a conv module\\n        x (torch.Tensor): Input feature map.\\n    '\n    assert bn.running_var is not None\n    weight_on_the_fly = conv.weight\n    if conv.bias is not None:\n        bias_on_the_fly = conv.bias\n    else:\n        bias_on_the_fly = torch.zeros_like(bn.running_var)\n    if bn.weight is not None:\n        bn_weight = bn.weight\n    else:\n        bn_weight = torch.ones_like(bn.running_var)\n    if bn.bias is not None:\n        bn_bias = bn.bias\n    else:\n        bn_bias = torch.zeros_like(bn.running_var)\n    target_shape = [-1] + [1] * (conv.weight.ndim - 1)\n    if isinstance(conv, nn.modules.conv._ConvTransposeNd):\n        target_shape[:2] = [target_shape[1], target_shape[0]]\n    weight_coeff = torch.rsqrt(bn.running_var + bn.eps).reshape(target_shape)\n    coefff_on_the_fly = bn_weight.view_as(weight_coeff) * weight_coeff\n    weight_on_the_fly = weight_on_the_fly * coefff_on_the_fly\n    bias_on_the_fly = bn_bias + coefff_on_the_fly.flatten() * (bias_on_the_fly - bn.running_mean)\n    input = x\n    params = {'weight': weight_on_the_fly, 'bias': bias_on_the_fly}\n    output = functional_call(conv, params, input)\n    return output",
            "def efficient_conv_bn_eval(bn: nn.modules.batchnorm._BatchNorm, conv: nn.modules.conv._ConvNd, x: torch.Tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Implementation based on https://arxiv.org/abs/2305.11624\\n    \"Tune-Mode ConvBN Blocks For Efficient Transfer Learning\"\\n    It leverages the associative law between convolution and affine transform,\\n    i.e., normalize (weight conv feature) = (normalize weight) conv feature.\\n    It works for Eval mode of ConvBN blocks during validation, and can be used\\n    for **training** as well, but only if one sets `bn.training=False`. It\\n     reduces memory footprint and computation cost, at the cost of slightly\\n     reduced numerical stability.\\n    Args:\\n        bn (nn.modules.batchnorm._BatchNorm): a BatchNorm module.\\n        conv (nn.modules.conv._ConvNd): a conv module\\n        x (torch.Tensor): Input feature map.\\n    '\n    assert bn.running_var is not None\n    weight_on_the_fly = conv.weight\n    if conv.bias is not None:\n        bias_on_the_fly = conv.bias\n    else:\n        bias_on_the_fly = torch.zeros_like(bn.running_var)\n    if bn.weight is not None:\n        bn_weight = bn.weight\n    else:\n        bn_weight = torch.ones_like(bn.running_var)\n    if bn.bias is not None:\n        bn_bias = bn.bias\n    else:\n        bn_bias = torch.zeros_like(bn.running_var)\n    target_shape = [-1] + [1] * (conv.weight.ndim - 1)\n    if isinstance(conv, nn.modules.conv._ConvTransposeNd):\n        target_shape[:2] = [target_shape[1], target_shape[0]]\n    weight_coeff = torch.rsqrt(bn.running_var + bn.eps).reshape(target_shape)\n    coefff_on_the_fly = bn_weight.view_as(weight_coeff) * weight_coeff\n    weight_on_the_fly = weight_on_the_fly * coefff_on_the_fly\n    bias_on_the_fly = bn_bias + coefff_on_the_fly.flatten() * (bias_on_the_fly - bn.running_mean)\n    input = x\n    params = {'weight': weight_on_the_fly, 'bias': bias_on_the_fly}\n    output = functional_call(conv, params, input)\n    return output",
            "def efficient_conv_bn_eval(bn: nn.modules.batchnorm._BatchNorm, conv: nn.modules.conv._ConvNd, x: torch.Tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Implementation based on https://arxiv.org/abs/2305.11624\\n    \"Tune-Mode ConvBN Blocks For Efficient Transfer Learning\"\\n    It leverages the associative law between convolution and affine transform,\\n    i.e., normalize (weight conv feature) = (normalize weight) conv feature.\\n    It works for Eval mode of ConvBN blocks during validation, and can be used\\n    for **training** as well, but only if one sets `bn.training=False`. It\\n     reduces memory footprint and computation cost, at the cost of slightly\\n     reduced numerical stability.\\n    Args:\\n        bn (nn.modules.batchnorm._BatchNorm): a BatchNorm module.\\n        conv (nn.modules.conv._ConvNd): a conv module\\n        x (torch.Tensor): Input feature map.\\n    '\n    assert bn.running_var is not None\n    weight_on_the_fly = conv.weight\n    if conv.bias is not None:\n        bias_on_the_fly = conv.bias\n    else:\n        bias_on_the_fly = torch.zeros_like(bn.running_var)\n    if bn.weight is not None:\n        bn_weight = bn.weight\n    else:\n        bn_weight = torch.ones_like(bn.running_var)\n    if bn.bias is not None:\n        bn_bias = bn.bias\n    else:\n        bn_bias = torch.zeros_like(bn.running_var)\n    target_shape = [-1] + [1] * (conv.weight.ndim - 1)\n    if isinstance(conv, nn.modules.conv._ConvTransposeNd):\n        target_shape[:2] = [target_shape[1], target_shape[0]]\n    weight_coeff = torch.rsqrt(bn.running_var + bn.eps).reshape(target_shape)\n    coefff_on_the_fly = bn_weight.view_as(weight_coeff) * weight_coeff\n    weight_on_the_fly = weight_on_the_fly * coefff_on_the_fly\n    bias_on_the_fly = bn_bias + coefff_on_the_fly.flatten() * (bias_on_the_fly - bn.running_mean)\n    input = x\n    params = {'weight': weight_on_the_fly, 'bias': bias_on_the_fly}\n    output = functional_call(conv, params, input)\n    return output",
            "def efficient_conv_bn_eval(bn: nn.modules.batchnorm._BatchNorm, conv: nn.modules.conv._ConvNd, x: torch.Tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Implementation based on https://arxiv.org/abs/2305.11624\\n    \"Tune-Mode ConvBN Blocks For Efficient Transfer Learning\"\\n    It leverages the associative law between convolution and affine transform,\\n    i.e., normalize (weight conv feature) = (normalize weight) conv feature.\\n    It works for Eval mode of ConvBN blocks during validation, and can be used\\n    for **training** as well, but only if one sets `bn.training=False`. It\\n     reduces memory footprint and computation cost, at the cost of slightly\\n     reduced numerical stability.\\n    Args:\\n        bn (nn.modules.batchnorm._BatchNorm): a BatchNorm module.\\n        conv (nn.modules.conv._ConvNd): a conv module\\n        x (torch.Tensor): Input feature map.\\n    '\n    assert bn.running_var is not None\n    weight_on_the_fly = conv.weight\n    if conv.bias is not None:\n        bias_on_the_fly = conv.bias\n    else:\n        bias_on_the_fly = torch.zeros_like(bn.running_var)\n    if bn.weight is not None:\n        bn_weight = bn.weight\n    else:\n        bn_weight = torch.ones_like(bn.running_var)\n    if bn.bias is not None:\n        bn_bias = bn.bias\n    else:\n        bn_bias = torch.zeros_like(bn.running_var)\n    target_shape = [-1] + [1] * (conv.weight.ndim - 1)\n    if isinstance(conv, nn.modules.conv._ConvTransposeNd):\n        target_shape[:2] = [target_shape[1], target_shape[0]]\n    weight_coeff = torch.rsqrt(bn.running_var + bn.eps).reshape(target_shape)\n    coefff_on_the_fly = bn_weight.view_as(weight_coeff) * weight_coeff\n    weight_on_the_fly = weight_on_the_fly * coefff_on_the_fly\n    bias_on_the_fly = bn_bias + coefff_on_the_fly.flatten() * (bias_on_the_fly - bn.running_mean)\n    input = x\n    params = {'weight': weight_on_the_fly, 'bias': bias_on_the_fly}\n    output = functional_call(conv, params, input)\n    return output",
            "def efficient_conv_bn_eval(bn: nn.modules.batchnorm._BatchNorm, conv: nn.modules.conv._ConvNd, x: torch.Tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Implementation based on https://arxiv.org/abs/2305.11624\\n    \"Tune-Mode ConvBN Blocks For Efficient Transfer Learning\"\\n    It leverages the associative law between convolution and affine transform,\\n    i.e., normalize (weight conv feature) = (normalize weight) conv feature.\\n    It works for Eval mode of ConvBN blocks during validation, and can be used\\n    for **training** as well, but only if one sets `bn.training=False`. It\\n     reduces memory footprint and computation cost, at the cost of slightly\\n     reduced numerical stability.\\n    Args:\\n        bn (nn.modules.batchnorm._BatchNorm): a BatchNorm module.\\n        conv (nn.modules.conv._ConvNd): a conv module\\n        x (torch.Tensor): Input feature map.\\n    '\n    assert bn.running_var is not None\n    weight_on_the_fly = conv.weight\n    if conv.bias is not None:\n        bias_on_the_fly = conv.bias\n    else:\n        bias_on_the_fly = torch.zeros_like(bn.running_var)\n    if bn.weight is not None:\n        bn_weight = bn.weight\n    else:\n        bn_weight = torch.ones_like(bn.running_var)\n    if bn.bias is not None:\n        bn_bias = bn.bias\n    else:\n        bn_bias = torch.zeros_like(bn.running_var)\n    target_shape = [-1] + [1] * (conv.weight.ndim - 1)\n    if isinstance(conv, nn.modules.conv._ConvTransposeNd):\n        target_shape[:2] = [target_shape[1], target_shape[0]]\n    weight_coeff = torch.rsqrt(bn.running_var + bn.eps).reshape(target_shape)\n    coefff_on_the_fly = bn_weight.view_as(weight_coeff) * weight_coeff\n    weight_on_the_fly = weight_on_the_fly * coefff_on_the_fly\n    bias_on_the_fly = bn_bias + coefff_on_the_fly.flatten() * (bias_on_the_fly - bn.running_mean)\n    input = x\n    params = {'weight': weight_on_the_fly, 'bias': bias_on_the_fly}\n    output = functional_call(conv, params, input)\n    return output"
        ]
    },
    {
        "func_name": "efficient_conv_bn_eval_graph_transform",
        "original": "@register_graph_pattern(CallModuleVarArgs([nn.modules.batchnorm._BatchNorm, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm]), pass_dict=efficient_conv_bn_eval_pass, extra_check=lambda match: not inductor_config.freezing and inductor_config.efficient_conv_bn_eval_fx_passes)\ndef efficient_conv_bn_eval_graph_transform(match: Match, *args, **kwargs):\n    bn_node = match.nodes[0]\n    graph = match.graph\n    gm = graph.owning_module\n    bn_mod = getattr(gm, bn_node.target)\n    if not bn_mod.track_running_stats or bn_mod.training:\n        return\n    if bn_node.args:\n        input_node = bn_node.args[0]\n    else:\n        input_node = bn_node.kwargs['input']\n    if input_node.op != 'call_module':\n        return\n    if not hasattr(gm, input_node.target):\n        return\n    input_mod = getattr(gm, input_node.target)\n    supported_convs = [nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d]\n    if not any((isinstance(input_mod, cls) for cls in supported_convs)):\n        return\n    conv_node = input_node\n    if len(conv_node.users) > 1:\n        return\n    counters['inductor']['efficient_conv_bn_eval'] += 1\n    with graph.inserting_before(conv_node):\n        conv_get_node = graph.create_node(op='get_attr', target=conv_node.target, name='get_conv')\n        bn_get_node = graph.create_node(op='get_attr', target=bn_node.target, name='get_bn')\n        if conv_node.args:\n            conv_input = conv_node.args[0]\n        else:\n            conv_input = conv_node.kwargs['input']\n        args = (bn_get_node, conv_get_node, conv_input)\n        new_node = graph.create_node(op='call_function', target=efficient_conv_bn_eval, args=args, name='efficient_conv_bn_eval')\n    bn_node.replace_all_uses_with(new_node)\n    graph.erase_node(bn_node)\n    graph.erase_node(conv_node)",
        "mutated": [
            "@register_graph_pattern(CallModuleVarArgs([nn.modules.batchnorm._BatchNorm, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm]), pass_dict=efficient_conv_bn_eval_pass, extra_check=lambda match: not inductor_config.freezing and inductor_config.efficient_conv_bn_eval_fx_passes)\ndef efficient_conv_bn_eval_graph_transform(match: Match, *args, **kwargs):\n    if False:\n        i = 10\n    bn_node = match.nodes[0]\n    graph = match.graph\n    gm = graph.owning_module\n    bn_mod = getattr(gm, bn_node.target)\n    if not bn_mod.track_running_stats or bn_mod.training:\n        return\n    if bn_node.args:\n        input_node = bn_node.args[0]\n    else:\n        input_node = bn_node.kwargs['input']\n    if input_node.op != 'call_module':\n        return\n    if not hasattr(gm, input_node.target):\n        return\n    input_mod = getattr(gm, input_node.target)\n    supported_convs = [nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d]\n    if not any((isinstance(input_mod, cls) for cls in supported_convs)):\n        return\n    conv_node = input_node\n    if len(conv_node.users) > 1:\n        return\n    counters['inductor']['efficient_conv_bn_eval'] += 1\n    with graph.inserting_before(conv_node):\n        conv_get_node = graph.create_node(op='get_attr', target=conv_node.target, name='get_conv')\n        bn_get_node = graph.create_node(op='get_attr', target=bn_node.target, name='get_bn')\n        if conv_node.args:\n            conv_input = conv_node.args[0]\n        else:\n            conv_input = conv_node.kwargs['input']\n        args = (bn_get_node, conv_get_node, conv_input)\n        new_node = graph.create_node(op='call_function', target=efficient_conv_bn_eval, args=args, name='efficient_conv_bn_eval')\n    bn_node.replace_all_uses_with(new_node)\n    graph.erase_node(bn_node)\n    graph.erase_node(conv_node)",
            "@register_graph_pattern(CallModuleVarArgs([nn.modules.batchnorm._BatchNorm, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm]), pass_dict=efficient_conv_bn_eval_pass, extra_check=lambda match: not inductor_config.freezing and inductor_config.efficient_conv_bn_eval_fx_passes)\ndef efficient_conv_bn_eval_graph_transform(match: Match, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    bn_node = match.nodes[0]\n    graph = match.graph\n    gm = graph.owning_module\n    bn_mod = getattr(gm, bn_node.target)\n    if not bn_mod.track_running_stats or bn_mod.training:\n        return\n    if bn_node.args:\n        input_node = bn_node.args[0]\n    else:\n        input_node = bn_node.kwargs['input']\n    if input_node.op != 'call_module':\n        return\n    if not hasattr(gm, input_node.target):\n        return\n    input_mod = getattr(gm, input_node.target)\n    supported_convs = [nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d]\n    if not any((isinstance(input_mod, cls) for cls in supported_convs)):\n        return\n    conv_node = input_node\n    if len(conv_node.users) > 1:\n        return\n    counters['inductor']['efficient_conv_bn_eval'] += 1\n    with graph.inserting_before(conv_node):\n        conv_get_node = graph.create_node(op='get_attr', target=conv_node.target, name='get_conv')\n        bn_get_node = graph.create_node(op='get_attr', target=bn_node.target, name='get_bn')\n        if conv_node.args:\n            conv_input = conv_node.args[0]\n        else:\n            conv_input = conv_node.kwargs['input']\n        args = (bn_get_node, conv_get_node, conv_input)\n        new_node = graph.create_node(op='call_function', target=efficient_conv_bn_eval, args=args, name='efficient_conv_bn_eval')\n    bn_node.replace_all_uses_with(new_node)\n    graph.erase_node(bn_node)\n    graph.erase_node(conv_node)",
            "@register_graph_pattern(CallModuleVarArgs([nn.modules.batchnorm._BatchNorm, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm]), pass_dict=efficient_conv_bn_eval_pass, extra_check=lambda match: not inductor_config.freezing and inductor_config.efficient_conv_bn_eval_fx_passes)\ndef efficient_conv_bn_eval_graph_transform(match: Match, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    bn_node = match.nodes[0]\n    graph = match.graph\n    gm = graph.owning_module\n    bn_mod = getattr(gm, bn_node.target)\n    if not bn_mod.track_running_stats or bn_mod.training:\n        return\n    if bn_node.args:\n        input_node = bn_node.args[0]\n    else:\n        input_node = bn_node.kwargs['input']\n    if input_node.op != 'call_module':\n        return\n    if not hasattr(gm, input_node.target):\n        return\n    input_mod = getattr(gm, input_node.target)\n    supported_convs = [nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d]\n    if not any((isinstance(input_mod, cls) for cls in supported_convs)):\n        return\n    conv_node = input_node\n    if len(conv_node.users) > 1:\n        return\n    counters['inductor']['efficient_conv_bn_eval'] += 1\n    with graph.inserting_before(conv_node):\n        conv_get_node = graph.create_node(op='get_attr', target=conv_node.target, name='get_conv')\n        bn_get_node = graph.create_node(op='get_attr', target=bn_node.target, name='get_bn')\n        if conv_node.args:\n            conv_input = conv_node.args[0]\n        else:\n            conv_input = conv_node.kwargs['input']\n        args = (bn_get_node, conv_get_node, conv_input)\n        new_node = graph.create_node(op='call_function', target=efficient_conv_bn_eval, args=args, name='efficient_conv_bn_eval')\n    bn_node.replace_all_uses_with(new_node)\n    graph.erase_node(bn_node)\n    graph.erase_node(conv_node)",
            "@register_graph_pattern(CallModuleVarArgs([nn.modules.batchnorm._BatchNorm, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm]), pass_dict=efficient_conv_bn_eval_pass, extra_check=lambda match: not inductor_config.freezing and inductor_config.efficient_conv_bn_eval_fx_passes)\ndef efficient_conv_bn_eval_graph_transform(match: Match, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    bn_node = match.nodes[0]\n    graph = match.graph\n    gm = graph.owning_module\n    bn_mod = getattr(gm, bn_node.target)\n    if not bn_mod.track_running_stats or bn_mod.training:\n        return\n    if bn_node.args:\n        input_node = bn_node.args[0]\n    else:\n        input_node = bn_node.kwargs['input']\n    if input_node.op != 'call_module':\n        return\n    if not hasattr(gm, input_node.target):\n        return\n    input_mod = getattr(gm, input_node.target)\n    supported_convs = [nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d]\n    if not any((isinstance(input_mod, cls) for cls in supported_convs)):\n        return\n    conv_node = input_node\n    if len(conv_node.users) > 1:\n        return\n    counters['inductor']['efficient_conv_bn_eval'] += 1\n    with graph.inserting_before(conv_node):\n        conv_get_node = graph.create_node(op='get_attr', target=conv_node.target, name='get_conv')\n        bn_get_node = graph.create_node(op='get_attr', target=bn_node.target, name='get_bn')\n        if conv_node.args:\n            conv_input = conv_node.args[0]\n        else:\n            conv_input = conv_node.kwargs['input']\n        args = (bn_get_node, conv_get_node, conv_input)\n        new_node = graph.create_node(op='call_function', target=efficient_conv_bn_eval, args=args, name='efficient_conv_bn_eval')\n    bn_node.replace_all_uses_with(new_node)\n    graph.erase_node(bn_node)\n    graph.erase_node(conv_node)",
            "@register_graph_pattern(CallModuleVarArgs([nn.modules.batchnorm._BatchNorm, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm]), pass_dict=efficient_conv_bn_eval_pass, extra_check=lambda match: not inductor_config.freezing and inductor_config.efficient_conv_bn_eval_fx_passes)\ndef efficient_conv_bn_eval_graph_transform(match: Match, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    bn_node = match.nodes[0]\n    graph = match.graph\n    gm = graph.owning_module\n    bn_mod = getattr(gm, bn_node.target)\n    if not bn_mod.track_running_stats or bn_mod.training:\n        return\n    if bn_node.args:\n        input_node = bn_node.args[0]\n    else:\n        input_node = bn_node.kwargs['input']\n    if input_node.op != 'call_module':\n        return\n    if not hasattr(gm, input_node.target):\n        return\n    input_mod = getattr(gm, input_node.target)\n    supported_convs = [nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d]\n    if not any((isinstance(input_mod, cls) for cls in supported_convs)):\n        return\n    conv_node = input_node\n    if len(conv_node.users) > 1:\n        return\n    counters['inductor']['efficient_conv_bn_eval'] += 1\n    with graph.inserting_before(conv_node):\n        conv_get_node = graph.create_node(op='get_attr', target=conv_node.target, name='get_conv')\n        bn_get_node = graph.create_node(op='get_attr', target=bn_node.target, name='get_bn')\n        if conv_node.args:\n            conv_input = conv_node.args[0]\n        else:\n            conv_input = conv_node.kwargs['input']\n        args = (bn_get_node, conv_get_node, conv_input)\n        new_node = graph.create_node(op='call_function', target=efficient_conv_bn_eval, args=args, name='efficient_conv_bn_eval')\n    bn_node.replace_all_uses_with(new_node)\n    graph.erase_node(bn_node)\n    graph.erase_node(conv_node)"
        ]
    }
]