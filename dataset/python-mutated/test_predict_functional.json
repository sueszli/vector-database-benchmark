[
    {
        "func_name": "pctl",
        "original": "def pctl(q):\n    return lambda x: np.percentile(x, 100 * q)",
        "mutated": [
            "def pctl(q):\n    if False:\n        i = 10\n    return lambda x: np.percentile(x, 100 * q)",
            "def pctl(q):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return lambda x: np.percentile(x, 100 * q)",
            "def pctl(q):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return lambda x: np.percentile(x, 100 * q)",
            "def pctl(q):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return lambda x: np.percentile(x, 100 * q)",
            "def pctl(q):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return lambda x: np.percentile(x, 100 * q)"
        ]
    },
    {
        "func_name": "setup_class",
        "original": "@classmethod\ndef setup_class(cls):\n    if pdf_output:\n        from matplotlib.backends.backend_pdf import PdfPages\n        cls.pdf = PdfPages('predict_functional.pdf')",
        "mutated": [
            "@classmethod\ndef setup_class(cls):\n    if False:\n        i = 10\n    if pdf_output:\n        from matplotlib.backends.backend_pdf import PdfPages\n        cls.pdf = PdfPages('predict_functional.pdf')",
            "@classmethod\ndef setup_class(cls):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if pdf_output:\n        from matplotlib.backends.backend_pdf import PdfPages\n        cls.pdf = PdfPages('predict_functional.pdf')",
            "@classmethod\ndef setup_class(cls):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if pdf_output:\n        from matplotlib.backends.backend_pdf import PdfPages\n        cls.pdf = PdfPages('predict_functional.pdf')",
            "@classmethod\ndef setup_class(cls):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if pdf_output:\n        from matplotlib.backends.backend_pdf import PdfPages\n        cls.pdf = PdfPages('predict_functional.pdf')",
            "@classmethod\ndef setup_class(cls):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if pdf_output:\n        from matplotlib.backends.backend_pdf import PdfPages\n        cls.pdf = PdfPages('predict_functional.pdf')"
        ]
    },
    {
        "func_name": "teardown_class",
        "original": "@classmethod\ndef teardown_class(cls):\n    if pdf_output:\n        cls.pdf.close()",
        "mutated": [
            "@classmethod\ndef teardown_class(cls):\n    if False:\n        i = 10\n    if pdf_output:\n        cls.pdf.close()",
            "@classmethod\ndef teardown_class(cls):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if pdf_output:\n        cls.pdf.close()",
            "@classmethod\ndef teardown_class(cls):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if pdf_output:\n        cls.pdf.close()",
            "@classmethod\ndef teardown_class(cls):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if pdf_output:\n        cls.pdf.close()",
            "@classmethod\ndef teardown_class(cls):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if pdf_output:\n        cls.pdf.close()"
        ]
    },
    {
        "func_name": "close_or_save",
        "original": "def close_or_save(self, fig):\n    if pdf_output:\n        self.pdf.savefig(fig)",
        "mutated": [
            "def close_or_save(self, fig):\n    if False:\n        i = 10\n    if pdf_output:\n        self.pdf.savefig(fig)",
            "def close_or_save(self, fig):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if pdf_output:\n        self.pdf.savefig(fig)",
            "def close_or_save(self, fig):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if pdf_output:\n        self.pdf.savefig(fig)",
            "def close_or_save(self, fig):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if pdf_output:\n        self.pdf.savefig(fig)",
            "def close_or_save(self, fig):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if pdf_output:\n        self.pdf.savefig(fig)"
        ]
    },
    {
        "func_name": "test_formula",
        "original": "@pytest.mark.matplotlib\ndef test_formula(self, close_figures):\n    np.random.seed(542)\n    n = 500\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    x4 = np.random.randint(0, 5, size=n)\n    x4 = np.asarray(['ABCDE'[i] for i in x4])\n    x5 = np.random.normal(size=n)\n    y = 0.3 * x2 ** 2 + (x4 == 'B') + 0.1 * (x4 == 'B') * x2 ** 2 + x5 + np.random.normal(size=n)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4, 'x5': x5})\n    fml = 'y ~ x1 + bs(x2, df=4) + x3 + x2*x3 + I(x1**2) + C(x4) + C(x4)*bs(x2, df=4) + x5'\n    model = sm.OLS.from_formula(fml, data=df)\n    result = model.fit()\n    summaries = {'x1': np.mean, 'x3': pctl(0.75), 'x5': np.mean}\n    values = {'x4': 'B'}\n    (pr1, ci1, fvals1) = predict_functional(result, 'x2', summaries, values)\n    values = {'x4': 'C'}\n    (pr2, ci2, fvals2) = predict_functional(result, 'x2', summaries, values)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x4=B')\n    plt.plot(fvals2, pr2, '-', label='x4=C')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x4=B')\n    plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n    plt.plot(fvals2, pr2, '-', label='x4=C')\n    plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)",
        "mutated": [
            "@pytest.mark.matplotlib\ndef test_formula(self, close_figures):\n    if False:\n        i = 10\n    np.random.seed(542)\n    n = 500\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    x4 = np.random.randint(0, 5, size=n)\n    x4 = np.asarray(['ABCDE'[i] for i in x4])\n    x5 = np.random.normal(size=n)\n    y = 0.3 * x2 ** 2 + (x4 == 'B') + 0.1 * (x4 == 'B') * x2 ** 2 + x5 + np.random.normal(size=n)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4, 'x5': x5})\n    fml = 'y ~ x1 + bs(x2, df=4) + x3 + x2*x3 + I(x1**2) + C(x4) + C(x4)*bs(x2, df=4) + x5'\n    model = sm.OLS.from_formula(fml, data=df)\n    result = model.fit()\n    summaries = {'x1': np.mean, 'x3': pctl(0.75), 'x5': np.mean}\n    values = {'x4': 'B'}\n    (pr1, ci1, fvals1) = predict_functional(result, 'x2', summaries, values)\n    values = {'x4': 'C'}\n    (pr2, ci2, fvals2) = predict_functional(result, 'x2', summaries, values)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x4=B')\n    plt.plot(fvals2, pr2, '-', label='x4=C')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x4=B')\n    plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n    plt.plot(fvals2, pr2, '-', label='x4=C')\n    plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_formula(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np.random.seed(542)\n    n = 500\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    x4 = np.random.randint(0, 5, size=n)\n    x4 = np.asarray(['ABCDE'[i] for i in x4])\n    x5 = np.random.normal(size=n)\n    y = 0.3 * x2 ** 2 + (x4 == 'B') + 0.1 * (x4 == 'B') * x2 ** 2 + x5 + np.random.normal(size=n)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4, 'x5': x5})\n    fml = 'y ~ x1 + bs(x2, df=4) + x3 + x2*x3 + I(x1**2) + C(x4) + C(x4)*bs(x2, df=4) + x5'\n    model = sm.OLS.from_formula(fml, data=df)\n    result = model.fit()\n    summaries = {'x1': np.mean, 'x3': pctl(0.75), 'x5': np.mean}\n    values = {'x4': 'B'}\n    (pr1, ci1, fvals1) = predict_functional(result, 'x2', summaries, values)\n    values = {'x4': 'C'}\n    (pr2, ci2, fvals2) = predict_functional(result, 'x2', summaries, values)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x4=B')\n    plt.plot(fvals2, pr2, '-', label='x4=C')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x4=B')\n    plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n    plt.plot(fvals2, pr2, '-', label='x4=C')\n    plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_formula(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np.random.seed(542)\n    n = 500\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    x4 = np.random.randint(0, 5, size=n)\n    x4 = np.asarray(['ABCDE'[i] for i in x4])\n    x5 = np.random.normal(size=n)\n    y = 0.3 * x2 ** 2 + (x4 == 'B') + 0.1 * (x4 == 'B') * x2 ** 2 + x5 + np.random.normal(size=n)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4, 'x5': x5})\n    fml = 'y ~ x1 + bs(x2, df=4) + x3 + x2*x3 + I(x1**2) + C(x4) + C(x4)*bs(x2, df=4) + x5'\n    model = sm.OLS.from_formula(fml, data=df)\n    result = model.fit()\n    summaries = {'x1': np.mean, 'x3': pctl(0.75), 'x5': np.mean}\n    values = {'x4': 'B'}\n    (pr1, ci1, fvals1) = predict_functional(result, 'x2', summaries, values)\n    values = {'x4': 'C'}\n    (pr2, ci2, fvals2) = predict_functional(result, 'x2', summaries, values)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x4=B')\n    plt.plot(fvals2, pr2, '-', label='x4=C')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x4=B')\n    plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n    plt.plot(fvals2, pr2, '-', label='x4=C')\n    plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_formula(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np.random.seed(542)\n    n = 500\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    x4 = np.random.randint(0, 5, size=n)\n    x4 = np.asarray(['ABCDE'[i] for i in x4])\n    x5 = np.random.normal(size=n)\n    y = 0.3 * x2 ** 2 + (x4 == 'B') + 0.1 * (x4 == 'B') * x2 ** 2 + x5 + np.random.normal(size=n)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4, 'x5': x5})\n    fml = 'y ~ x1 + bs(x2, df=4) + x3 + x2*x3 + I(x1**2) + C(x4) + C(x4)*bs(x2, df=4) + x5'\n    model = sm.OLS.from_formula(fml, data=df)\n    result = model.fit()\n    summaries = {'x1': np.mean, 'x3': pctl(0.75), 'x5': np.mean}\n    values = {'x4': 'B'}\n    (pr1, ci1, fvals1) = predict_functional(result, 'x2', summaries, values)\n    values = {'x4': 'C'}\n    (pr2, ci2, fvals2) = predict_functional(result, 'x2', summaries, values)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x4=B')\n    plt.plot(fvals2, pr2, '-', label='x4=C')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x4=B')\n    plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n    plt.plot(fvals2, pr2, '-', label='x4=C')\n    plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_formula(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np.random.seed(542)\n    n = 500\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    x4 = np.random.randint(0, 5, size=n)\n    x4 = np.asarray(['ABCDE'[i] for i in x4])\n    x5 = np.random.normal(size=n)\n    y = 0.3 * x2 ** 2 + (x4 == 'B') + 0.1 * (x4 == 'B') * x2 ** 2 + x5 + np.random.normal(size=n)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4, 'x5': x5})\n    fml = 'y ~ x1 + bs(x2, df=4) + x3 + x2*x3 + I(x1**2) + C(x4) + C(x4)*bs(x2, df=4) + x5'\n    model = sm.OLS.from_formula(fml, data=df)\n    result = model.fit()\n    summaries = {'x1': np.mean, 'x3': pctl(0.75), 'x5': np.mean}\n    values = {'x4': 'B'}\n    (pr1, ci1, fvals1) = predict_functional(result, 'x2', summaries, values)\n    values = {'x4': 'C'}\n    (pr2, ci2, fvals2) = predict_functional(result, 'x2', summaries, values)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x4=B')\n    plt.plot(fvals2, pr2, '-', label='x4=C')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x4=B')\n    plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n    plt.plot(fvals2, pr2, '-', label='x4=C')\n    plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)"
        ]
    },
    {
        "func_name": "test_lm_contrast",
        "original": "@pytest.mark.matplotlib\ndef test_lm_contrast(self, close_figures):\n    np.random.seed(542)\n    n = 200\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    y = x1 + 2 * x2 + x3 - x1 * x2 + x2 * x3 + np.random.normal(size=n)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + x2 + x3 + x1*x2 + x2*x3'\n    model = sm.OLS.from_formula(fml, data=df)\n    result = model.fit()\n    values = {'x2': 1, 'x3': 1}\n    values2 = {'x2': 0, 'x3': 0}\n    (pr, cb, fvals) = predict_functional(result, 'x1', values=values, values2=values2, ci_method='scheffe')\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.67, 0.8])\n    plt.plot(fvals, pr, '-', label='Estimate', color='orange', lw=4)\n    plt.plot(fvals, 4 - fvals, '-', label='Truth', color='lime', lw=4)\n    plt.fill_between(fvals, cb[:, 0], cb[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Mean contrast', size=15)\n    plt.title('Linear model contrast')\n    self.close_or_save(fig)",
        "mutated": [
            "@pytest.mark.matplotlib\ndef test_lm_contrast(self, close_figures):\n    if False:\n        i = 10\n    np.random.seed(542)\n    n = 200\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    y = x1 + 2 * x2 + x3 - x1 * x2 + x2 * x3 + np.random.normal(size=n)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + x2 + x3 + x1*x2 + x2*x3'\n    model = sm.OLS.from_formula(fml, data=df)\n    result = model.fit()\n    values = {'x2': 1, 'x3': 1}\n    values2 = {'x2': 0, 'x3': 0}\n    (pr, cb, fvals) = predict_functional(result, 'x1', values=values, values2=values2, ci_method='scheffe')\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.67, 0.8])\n    plt.plot(fvals, pr, '-', label='Estimate', color='orange', lw=4)\n    plt.plot(fvals, 4 - fvals, '-', label='Truth', color='lime', lw=4)\n    plt.fill_between(fvals, cb[:, 0], cb[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Mean contrast', size=15)\n    plt.title('Linear model contrast')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_lm_contrast(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np.random.seed(542)\n    n = 200\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    y = x1 + 2 * x2 + x3 - x1 * x2 + x2 * x3 + np.random.normal(size=n)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + x2 + x3 + x1*x2 + x2*x3'\n    model = sm.OLS.from_formula(fml, data=df)\n    result = model.fit()\n    values = {'x2': 1, 'x3': 1}\n    values2 = {'x2': 0, 'x3': 0}\n    (pr, cb, fvals) = predict_functional(result, 'x1', values=values, values2=values2, ci_method='scheffe')\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.67, 0.8])\n    plt.plot(fvals, pr, '-', label='Estimate', color='orange', lw=4)\n    plt.plot(fvals, 4 - fvals, '-', label='Truth', color='lime', lw=4)\n    plt.fill_between(fvals, cb[:, 0], cb[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Mean contrast', size=15)\n    plt.title('Linear model contrast')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_lm_contrast(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np.random.seed(542)\n    n = 200\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    y = x1 + 2 * x2 + x3 - x1 * x2 + x2 * x3 + np.random.normal(size=n)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + x2 + x3 + x1*x2 + x2*x3'\n    model = sm.OLS.from_formula(fml, data=df)\n    result = model.fit()\n    values = {'x2': 1, 'x3': 1}\n    values2 = {'x2': 0, 'x3': 0}\n    (pr, cb, fvals) = predict_functional(result, 'x1', values=values, values2=values2, ci_method='scheffe')\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.67, 0.8])\n    plt.plot(fvals, pr, '-', label='Estimate', color='orange', lw=4)\n    plt.plot(fvals, 4 - fvals, '-', label='Truth', color='lime', lw=4)\n    plt.fill_between(fvals, cb[:, 0], cb[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Mean contrast', size=15)\n    plt.title('Linear model contrast')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_lm_contrast(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np.random.seed(542)\n    n = 200\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    y = x1 + 2 * x2 + x3 - x1 * x2 + x2 * x3 + np.random.normal(size=n)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + x2 + x3 + x1*x2 + x2*x3'\n    model = sm.OLS.from_formula(fml, data=df)\n    result = model.fit()\n    values = {'x2': 1, 'x3': 1}\n    values2 = {'x2': 0, 'x3': 0}\n    (pr, cb, fvals) = predict_functional(result, 'x1', values=values, values2=values2, ci_method='scheffe')\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.67, 0.8])\n    plt.plot(fvals, pr, '-', label='Estimate', color='orange', lw=4)\n    plt.plot(fvals, 4 - fvals, '-', label='Truth', color='lime', lw=4)\n    plt.fill_between(fvals, cb[:, 0], cb[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Mean contrast', size=15)\n    plt.title('Linear model contrast')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_lm_contrast(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np.random.seed(542)\n    n = 200\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    y = x1 + 2 * x2 + x3 - x1 * x2 + x2 * x3 + np.random.normal(size=n)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + x2 + x3 + x1*x2 + x2*x3'\n    model = sm.OLS.from_formula(fml, data=df)\n    result = model.fit()\n    values = {'x2': 1, 'x3': 1}\n    values2 = {'x2': 0, 'x3': 0}\n    (pr, cb, fvals) = predict_functional(result, 'x1', values=values, values2=values2, ci_method='scheffe')\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.67, 0.8])\n    plt.plot(fvals, pr, '-', label='Estimate', color='orange', lw=4)\n    plt.plot(fvals, 4 - fvals, '-', label='Truth', color='lime', lw=4)\n    plt.fill_between(fvals, cb[:, 0], cb[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Mean contrast', size=15)\n    plt.title('Linear model contrast')\n    self.close_or_save(fig)"
        ]
    },
    {
        "func_name": "test_glm_formula_contrast",
        "original": "@pytest.mark.matplotlib\ndef test_glm_formula_contrast(self, close_figures):\n    np.random.seed(542)\n    n = 50\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    mn = 5 + 0.1 * x1 + 0.1 * x2 + 0.1 * x3 - 0.1 * x1 * x2\n    y = np.random.poisson(np.exp(mn), size=len(mn))\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + x2 + x3 + x1*x2'\n    model = sm.GLM.from_formula(fml, data=df, family=sm.families.Poisson())\n    result = model.fit()\n    values = {'x2': 1, 'x3': 1}\n    values2 = {'x2': 0, 'x3': 0}\n    (pr, cb, fvals) = predict_functional(result, 'x1', values=values, values2=values2, ci_method='simultaneous')\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.67, 0.8])\n    plt.plot(fvals, pr, '-', label='Estimate', color='orange', lw=4)\n    plt.plot(fvals, 0.2 - 0.1 * fvals, '-', label='Truth', color='lime', lw=4)\n    plt.fill_between(fvals, cb[:, 0], cb[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Linear predictor contrast', size=15)\n    plt.title('Poisson regression contrast')\n    self.close_or_save(fig)",
        "mutated": [
            "@pytest.mark.matplotlib\ndef test_glm_formula_contrast(self, close_figures):\n    if False:\n        i = 10\n    np.random.seed(542)\n    n = 50\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    mn = 5 + 0.1 * x1 + 0.1 * x2 + 0.1 * x3 - 0.1 * x1 * x2\n    y = np.random.poisson(np.exp(mn), size=len(mn))\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + x2 + x3 + x1*x2'\n    model = sm.GLM.from_formula(fml, data=df, family=sm.families.Poisson())\n    result = model.fit()\n    values = {'x2': 1, 'x3': 1}\n    values2 = {'x2': 0, 'x3': 0}\n    (pr, cb, fvals) = predict_functional(result, 'x1', values=values, values2=values2, ci_method='simultaneous')\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.67, 0.8])\n    plt.plot(fvals, pr, '-', label='Estimate', color='orange', lw=4)\n    plt.plot(fvals, 0.2 - 0.1 * fvals, '-', label='Truth', color='lime', lw=4)\n    plt.fill_between(fvals, cb[:, 0], cb[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Linear predictor contrast', size=15)\n    plt.title('Poisson regression contrast')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_glm_formula_contrast(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np.random.seed(542)\n    n = 50\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    mn = 5 + 0.1 * x1 + 0.1 * x2 + 0.1 * x3 - 0.1 * x1 * x2\n    y = np.random.poisson(np.exp(mn), size=len(mn))\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + x2 + x3 + x1*x2'\n    model = sm.GLM.from_formula(fml, data=df, family=sm.families.Poisson())\n    result = model.fit()\n    values = {'x2': 1, 'x3': 1}\n    values2 = {'x2': 0, 'x3': 0}\n    (pr, cb, fvals) = predict_functional(result, 'x1', values=values, values2=values2, ci_method='simultaneous')\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.67, 0.8])\n    plt.plot(fvals, pr, '-', label='Estimate', color='orange', lw=4)\n    plt.plot(fvals, 0.2 - 0.1 * fvals, '-', label='Truth', color='lime', lw=4)\n    plt.fill_between(fvals, cb[:, 0], cb[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Linear predictor contrast', size=15)\n    plt.title('Poisson regression contrast')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_glm_formula_contrast(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np.random.seed(542)\n    n = 50\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    mn = 5 + 0.1 * x1 + 0.1 * x2 + 0.1 * x3 - 0.1 * x1 * x2\n    y = np.random.poisson(np.exp(mn), size=len(mn))\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + x2 + x3 + x1*x2'\n    model = sm.GLM.from_formula(fml, data=df, family=sm.families.Poisson())\n    result = model.fit()\n    values = {'x2': 1, 'x3': 1}\n    values2 = {'x2': 0, 'x3': 0}\n    (pr, cb, fvals) = predict_functional(result, 'x1', values=values, values2=values2, ci_method='simultaneous')\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.67, 0.8])\n    plt.plot(fvals, pr, '-', label='Estimate', color='orange', lw=4)\n    plt.plot(fvals, 0.2 - 0.1 * fvals, '-', label='Truth', color='lime', lw=4)\n    plt.fill_between(fvals, cb[:, 0], cb[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Linear predictor contrast', size=15)\n    plt.title('Poisson regression contrast')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_glm_formula_contrast(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np.random.seed(542)\n    n = 50\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    mn = 5 + 0.1 * x1 + 0.1 * x2 + 0.1 * x3 - 0.1 * x1 * x2\n    y = np.random.poisson(np.exp(mn), size=len(mn))\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + x2 + x3 + x1*x2'\n    model = sm.GLM.from_formula(fml, data=df, family=sm.families.Poisson())\n    result = model.fit()\n    values = {'x2': 1, 'x3': 1}\n    values2 = {'x2': 0, 'x3': 0}\n    (pr, cb, fvals) = predict_functional(result, 'x1', values=values, values2=values2, ci_method='simultaneous')\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.67, 0.8])\n    plt.plot(fvals, pr, '-', label='Estimate', color='orange', lw=4)\n    plt.plot(fvals, 0.2 - 0.1 * fvals, '-', label='Truth', color='lime', lw=4)\n    plt.fill_between(fvals, cb[:, 0], cb[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Linear predictor contrast', size=15)\n    plt.title('Poisson regression contrast')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_glm_formula_contrast(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np.random.seed(542)\n    n = 50\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    mn = 5 + 0.1 * x1 + 0.1 * x2 + 0.1 * x3 - 0.1 * x1 * x2\n    y = np.random.poisson(np.exp(mn), size=len(mn))\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + x2 + x3 + x1*x2'\n    model = sm.GLM.from_formula(fml, data=df, family=sm.families.Poisson())\n    result = model.fit()\n    values = {'x2': 1, 'x3': 1}\n    values2 = {'x2': 0, 'x3': 0}\n    (pr, cb, fvals) = predict_functional(result, 'x1', values=values, values2=values2, ci_method='simultaneous')\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.67, 0.8])\n    plt.plot(fvals, pr, '-', label='Estimate', color='orange', lw=4)\n    plt.plot(fvals, 0.2 - 0.1 * fvals, '-', label='Truth', color='lime', lw=4)\n    plt.fill_between(fvals, cb[:, 0], cb[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Linear predictor contrast', size=15)\n    plt.title('Poisson regression contrast')\n    self.close_or_save(fig)"
        ]
    },
    {
        "func_name": "test_scb",
        "original": "@pytest.mark.matplotlib\ndef test_scb(self, close_figures):\n    np.random.seed(473)\n    n = 100\n    x = np.random.normal(size=(n, 4))\n    x[:, 0] = 1\n    for fam_name in ('poisson', 'binomial', 'gaussian'):\n        if fam_name == 'poisson':\n            y = np.random.poisson(20, size=n)\n            fam = sm.families.Poisson()\n            true_mean = 20\n            true_lp = np.log(20)\n        elif fam_name == 'binomial':\n            y = 1 * (np.random.uniform(size=n) < 0.5)\n            fam = sm.families.Binomial()\n            true_mean = 0.5\n            true_lp = 0\n        elif fam_name == 'gaussian':\n            y = np.random.normal(size=n)\n            fam = sm.families.Gaussian()\n            true_mean = 0\n            true_lp = 0\n        model = sm.GLM(y, x, family=fam)\n        result = model.fit()\n        for linear in (False, True):\n            true = true_lp if linear else true_mean\n            values = {'const': 1, 'x2': 0}\n            summaries = {'x3': np.mean}\n            (pred1, cb1, fvals1) = predict_functional(result, 'x1', values=values, summaries=summaries, linear=linear)\n            (pred2, cb2, fvals2) = predict_functional(result, 'x1', values=values, summaries=summaries, ci_method='simultaneous', linear=linear)\n            plt.clf()\n            fig = plt.figure()\n            ax = plt.axes([0.1, 0.1, 0.58, 0.8])\n            plt.plot(fvals1, pred1, '-', color='black', label='Estimate')\n            plt.plot(fvals1, true * np.ones(len(pred1)), '-', color='purple', label='Truth')\n            plt.plot(fvals1, cb1[:, 0], color='blue', label='Pointwise CB')\n            plt.plot(fvals1, cb1[:, 1], color='blue')\n            plt.plot(fvals2, cb2[:, 0], color='green', label='Simultaneous CB')\n            plt.plot(fvals2, cb2[:, 1], color='green')\n            (ha, lb) = ax.get_legend_handles_labels()\n            leg = plt.figlegend(ha, lb, loc='center right')\n            leg.draw_frame(False)\n            plt.xlabel('Focus variable', size=15)\n            if linear:\n                plt.ylabel('Linear predictor', size=15)\n            else:\n                plt.ylabel('Fitted mean', size=15)\n            plt.title('%s family prediction' % fam_name.capitalize())\n            self.close_or_save(fig)",
        "mutated": [
            "@pytest.mark.matplotlib\ndef test_scb(self, close_figures):\n    if False:\n        i = 10\n    np.random.seed(473)\n    n = 100\n    x = np.random.normal(size=(n, 4))\n    x[:, 0] = 1\n    for fam_name in ('poisson', 'binomial', 'gaussian'):\n        if fam_name == 'poisson':\n            y = np.random.poisson(20, size=n)\n            fam = sm.families.Poisson()\n            true_mean = 20\n            true_lp = np.log(20)\n        elif fam_name == 'binomial':\n            y = 1 * (np.random.uniform(size=n) < 0.5)\n            fam = sm.families.Binomial()\n            true_mean = 0.5\n            true_lp = 0\n        elif fam_name == 'gaussian':\n            y = np.random.normal(size=n)\n            fam = sm.families.Gaussian()\n            true_mean = 0\n            true_lp = 0\n        model = sm.GLM(y, x, family=fam)\n        result = model.fit()\n        for linear in (False, True):\n            true = true_lp if linear else true_mean\n            values = {'const': 1, 'x2': 0}\n            summaries = {'x3': np.mean}\n            (pred1, cb1, fvals1) = predict_functional(result, 'x1', values=values, summaries=summaries, linear=linear)\n            (pred2, cb2, fvals2) = predict_functional(result, 'x1', values=values, summaries=summaries, ci_method='simultaneous', linear=linear)\n            plt.clf()\n            fig = plt.figure()\n            ax = plt.axes([0.1, 0.1, 0.58, 0.8])\n            plt.plot(fvals1, pred1, '-', color='black', label='Estimate')\n            plt.plot(fvals1, true * np.ones(len(pred1)), '-', color='purple', label='Truth')\n            plt.plot(fvals1, cb1[:, 0], color='blue', label='Pointwise CB')\n            plt.plot(fvals1, cb1[:, 1], color='blue')\n            plt.plot(fvals2, cb2[:, 0], color='green', label='Simultaneous CB')\n            plt.plot(fvals2, cb2[:, 1], color='green')\n            (ha, lb) = ax.get_legend_handles_labels()\n            leg = plt.figlegend(ha, lb, loc='center right')\n            leg.draw_frame(False)\n            plt.xlabel('Focus variable', size=15)\n            if linear:\n                plt.ylabel('Linear predictor', size=15)\n            else:\n                plt.ylabel('Fitted mean', size=15)\n            plt.title('%s family prediction' % fam_name.capitalize())\n            self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_scb(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np.random.seed(473)\n    n = 100\n    x = np.random.normal(size=(n, 4))\n    x[:, 0] = 1\n    for fam_name in ('poisson', 'binomial', 'gaussian'):\n        if fam_name == 'poisson':\n            y = np.random.poisson(20, size=n)\n            fam = sm.families.Poisson()\n            true_mean = 20\n            true_lp = np.log(20)\n        elif fam_name == 'binomial':\n            y = 1 * (np.random.uniform(size=n) < 0.5)\n            fam = sm.families.Binomial()\n            true_mean = 0.5\n            true_lp = 0\n        elif fam_name == 'gaussian':\n            y = np.random.normal(size=n)\n            fam = sm.families.Gaussian()\n            true_mean = 0\n            true_lp = 0\n        model = sm.GLM(y, x, family=fam)\n        result = model.fit()\n        for linear in (False, True):\n            true = true_lp if linear else true_mean\n            values = {'const': 1, 'x2': 0}\n            summaries = {'x3': np.mean}\n            (pred1, cb1, fvals1) = predict_functional(result, 'x1', values=values, summaries=summaries, linear=linear)\n            (pred2, cb2, fvals2) = predict_functional(result, 'x1', values=values, summaries=summaries, ci_method='simultaneous', linear=linear)\n            plt.clf()\n            fig = plt.figure()\n            ax = plt.axes([0.1, 0.1, 0.58, 0.8])\n            plt.plot(fvals1, pred1, '-', color='black', label='Estimate')\n            plt.plot(fvals1, true * np.ones(len(pred1)), '-', color='purple', label='Truth')\n            plt.plot(fvals1, cb1[:, 0], color='blue', label='Pointwise CB')\n            plt.plot(fvals1, cb1[:, 1], color='blue')\n            plt.plot(fvals2, cb2[:, 0], color='green', label='Simultaneous CB')\n            plt.plot(fvals2, cb2[:, 1], color='green')\n            (ha, lb) = ax.get_legend_handles_labels()\n            leg = plt.figlegend(ha, lb, loc='center right')\n            leg.draw_frame(False)\n            plt.xlabel('Focus variable', size=15)\n            if linear:\n                plt.ylabel('Linear predictor', size=15)\n            else:\n                plt.ylabel('Fitted mean', size=15)\n            plt.title('%s family prediction' % fam_name.capitalize())\n            self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_scb(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np.random.seed(473)\n    n = 100\n    x = np.random.normal(size=(n, 4))\n    x[:, 0] = 1\n    for fam_name in ('poisson', 'binomial', 'gaussian'):\n        if fam_name == 'poisson':\n            y = np.random.poisson(20, size=n)\n            fam = sm.families.Poisson()\n            true_mean = 20\n            true_lp = np.log(20)\n        elif fam_name == 'binomial':\n            y = 1 * (np.random.uniform(size=n) < 0.5)\n            fam = sm.families.Binomial()\n            true_mean = 0.5\n            true_lp = 0\n        elif fam_name == 'gaussian':\n            y = np.random.normal(size=n)\n            fam = sm.families.Gaussian()\n            true_mean = 0\n            true_lp = 0\n        model = sm.GLM(y, x, family=fam)\n        result = model.fit()\n        for linear in (False, True):\n            true = true_lp if linear else true_mean\n            values = {'const': 1, 'x2': 0}\n            summaries = {'x3': np.mean}\n            (pred1, cb1, fvals1) = predict_functional(result, 'x1', values=values, summaries=summaries, linear=linear)\n            (pred2, cb2, fvals2) = predict_functional(result, 'x1', values=values, summaries=summaries, ci_method='simultaneous', linear=linear)\n            plt.clf()\n            fig = plt.figure()\n            ax = plt.axes([0.1, 0.1, 0.58, 0.8])\n            plt.plot(fvals1, pred1, '-', color='black', label='Estimate')\n            plt.plot(fvals1, true * np.ones(len(pred1)), '-', color='purple', label='Truth')\n            plt.plot(fvals1, cb1[:, 0], color='blue', label='Pointwise CB')\n            plt.plot(fvals1, cb1[:, 1], color='blue')\n            plt.plot(fvals2, cb2[:, 0], color='green', label='Simultaneous CB')\n            plt.plot(fvals2, cb2[:, 1], color='green')\n            (ha, lb) = ax.get_legend_handles_labels()\n            leg = plt.figlegend(ha, lb, loc='center right')\n            leg.draw_frame(False)\n            plt.xlabel('Focus variable', size=15)\n            if linear:\n                plt.ylabel('Linear predictor', size=15)\n            else:\n                plt.ylabel('Fitted mean', size=15)\n            plt.title('%s family prediction' % fam_name.capitalize())\n            self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_scb(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np.random.seed(473)\n    n = 100\n    x = np.random.normal(size=(n, 4))\n    x[:, 0] = 1\n    for fam_name in ('poisson', 'binomial', 'gaussian'):\n        if fam_name == 'poisson':\n            y = np.random.poisson(20, size=n)\n            fam = sm.families.Poisson()\n            true_mean = 20\n            true_lp = np.log(20)\n        elif fam_name == 'binomial':\n            y = 1 * (np.random.uniform(size=n) < 0.5)\n            fam = sm.families.Binomial()\n            true_mean = 0.5\n            true_lp = 0\n        elif fam_name == 'gaussian':\n            y = np.random.normal(size=n)\n            fam = sm.families.Gaussian()\n            true_mean = 0\n            true_lp = 0\n        model = sm.GLM(y, x, family=fam)\n        result = model.fit()\n        for linear in (False, True):\n            true = true_lp if linear else true_mean\n            values = {'const': 1, 'x2': 0}\n            summaries = {'x3': np.mean}\n            (pred1, cb1, fvals1) = predict_functional(result, 'x1', values=values, summaries=summaries, linear=linear)\n            (pred2, cb2, fvals2) = predict_functional(result, 'x1', values=values, summaries=summaries, ci_method='simultaneous', linear=linear)\n            plt.clf()\n            fig = plt.figure()\n            ax = plt.axes([0.1, 0.1, 0.58, 0.8])\n            plt.plot(fvals1, pred1, '-', color='black', label='Estimate')\n            plt.plot(fvals1, true * np.ones(len(pred1)), '-', color='purple', label='Truth')\n            plt.plot(fvals1, cb1[:, 0], color='blue', label='Pointwise CB')\n            plt.plot(fvals1, cb1[:, 1], color='blue')\n            plt.plot(fvals2, cb2[:, 0], color='green', label='Simultaneous CB')\n            plt.plot(fvals2, cb2[:, 1], color='green')\n            (ha, lb) = ax.get_legend_handles_labels()\n            leg = plt.figlegend(ha, lb, loc='center right')\n            leg.draw_frame(False)\n            plt.xlabel('Focus variable', size=15)\n            if linear:\n                plt.ylabel('Linear predictor', size=15)\n            else:\n                plt.ylabel('Fitted mean', size=15)\n            plt.title('%s family prediction' % fam_name.capitalize())\n            self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_scb(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np.random.seed(473)\n    n = 100\n    x = np.random.normal(size=(n, 4))\n    x[:, 0] = 1\n    for fam_name in ('poisson', 'binomial', 'gaussian'):\n        if fam_name == 'poisson':\n            y = np.random.poisson(20, size=n)\n            fam = sm.families.Poisson()\n            true_mean = 20\n            true_lp = np.log(20)\n        elif fam_name == 'binomial':\n            y = 1 * (np.random.uniform(size=n) < 0.5)\n            fam = sm.families.Binomial()\n            true_mean = 0.5\n            true_lp = 0\n        elif fam_name == 'gaussian':\n            y = np.random.normal(size=n)\n            fam = sm.families.Gaussian()\n            true_mean = 0\n            true_lp = 0\n        model = sm.GLM(y, x, family=fam)\n        result = model.fit()\n        for linear in (False, True):\n            true = true_lp if linear else true_mean\n            values = {'const': 1, 'x2': 0}\n            summaries = {'x3': np.mean}\n            (pred1, cb1, fvals1) = predict_functional(result, 'x1', values=values, summaries=summaries, linear=linear)\n            (pred2, cb2, fvals2) = predict_functional(result, 'x1', values=values, summaries=summaries, ci_method='simultaneous', linear=linear)\n            plt.clf()\n            fig = plt.figure()\n            ax = plt.axes([0.1, 0.1, 0.58, 0.8])\n            plt.plot(fvals1, pred1, '-', color='black', label='Estimate')\n            plt.plot(fvals1, true * np.ones(len(pred1)), '-', color='purple', label='Truth')\n            plt.plot(fvals1, cb1[:, 0], color='blue', label='Pointwise CB')\n            plt.plot(fvals1, cb1[:, 1], color='blue')\n            plt.plot(fvals2, cb2[:, 0], color='green', label='Simultaneous CB')\n            plt.plot(fvals2, cb2[:, 1], color='green')\n            (ha, lb) = ax.get_legend_handles_labels()\n            leg = plt.figlegend(ha, lb, loc='center right')\n            leg.draw_frame(False)\n            plt.xlabel('Focus variable', size=15)\n            if linear:\n                plt.ylabel('Linear predictor', size=15)\n            else:\n                plt.ylabel('Fitted mean', size=15)\n            plt.title('%s family prediction' % fam_name.capitalize())\n            self.close_or_save(fig)"
        ]
    },
    {
        "func_name": "test_glm_formula",
        "original": "@pytest.mark.matplotlib\ndef test_glm_formula(self, close_figures):\n    np.random.seed(542)\n    n = 500\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.randint(0, 3, size=n)\n    x3 = np.asarray(['ABC'[i] for i in x3])\n    lin_pred = -1 + 0.5 * x1 ** 2 + (x3 == 'B')\n    prob = 1 / (1 + np.exp(-lin_pred))\n    y = 1 * (np.random.uniform(size=n) < prob)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + I(x1**2) + x2 + C(x3)'\n    model = sm.GLM.from_formula(fml, family=sm.families.Binomial(), data=df)\n    result = model.fit()\n    summaries = {'x2': np.mean}\n    for linear in (False, True):\n        values = {'x3': 'B'}\n        (pr1, ci1, fvals1) = predict_functional(result, 'x1', summaries, values, linear=linear)\n        values = {'x3': 'C'}\n        (pr2, ci2, fvals2) = predict_functional(result, 'x1', summaries, values, linear=linear)\n        exact1 = -1 + 0.5 * fvals1 ** 2 + 1\n        exact2 = -1 + 0.5 * fvals2 ** 2\n        if not linear:\n            exact1 = 1 / (1 + np.exp(-exact1))\n            exact2 = 1 / (1 + np.exp(-exact2))\n        plt.clf()\n        fig = plt.figure()\n        ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n        plt.plot(fvals1, pr1, '-', label='x3=B')\n        plt.plot(fvals2, pr2, '-', label='x3=C')\n        plt.plot(fvals1, exact1, '-', label='x3=B (exact)')\n        plt.plot(fvals2, exact2, '-', label='x3=C (exact)')\n        (ha, lb) = ax.get_legend_handles_labels()\n        plt.figlegend(ha, lb, loc='center right')\n        plt.xlabel('Focus variable', size=15)\n        if linear:\n            plt.ylabel('Fitted linear predictor', size=15)\n        else:\n            plt.ylabel('Fitted probability', size=15)\n        plt.title('Binomial GLM prediction')\n        self.close_or_save(fig)\n        plt.clf()\n        fig = plt.figure()\n        ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n        plt.plot(fvals1, pr1, '-', label='x3=B', color='orange')\n        plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n        plt.plot(fvals2, pr2, '-', label='x3=C', color='lime')\n        plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n        (ha, lb) = ax.get_legend_handles_labels()\n        plt.figlegend(ha, lb, loc='center right')\n        plt.xlabel('Focus variable', size=15)\n        if linear:\n            plt.ylabel('Fitted linear predictor', size=15)\n        else:\n            plt.ylabel('Fitted probability', size=15)\n        plt.title('Binomial GLM prediction')\n        self.close_or_save(fig)",
        "mutated": [
            "@pytest.mark.matplotlib\ndef test_glm_formula(self, close_figures):\n    if False:\n        i = 10\n    np.random.seed(542)\n    n = 500\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.randint(0, 3, size=n)\n    x3 = np.asarray(['ABC'[i] for i in x3])\n    lin_pred = -1 + 0.5 * x1 ** 2 + (x3 == 'B')\n    prob = 1 / (1 + np.exp(-lin_pred))\n    y = 1 * (np.random.uniform(size=n) < prob)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + I(x1**2) + x2 + C(x3)'\n    model = sm.GLM.from_formula(fml, family=sm.families.Binomial(), data=df)\n    result = model.fit()\n    summaries = {'x2': np.mean}\n    for linear in (False, True):\n        values = {'x3': 'B'}\n        (pr1, ci1, fvals1) = predict_functional(result, 'x1', summaries, values, linear=linear)\n        values = {'x3': 'C'}\n        (pr2, ci2, fvals2) = predict_functional(result, 'x1', summaries, values, linear=linear)\n        exact1 = -1 + 0.5 * fvals1 ** 2 + 1\n        exact2 = -1 + 0.5 * fvals2 ** 2\n        if not linear:\n            exact1 = 1 / (1 + np.exp(-exact1))\n            exact2 = 1 / (1 + np.exp(-exact2))\n        plt.clf()\n        fig = plt.figure()\n        ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n        plt.plot(fvals1, pr1, '-', label='x3=B')\n        plt.plot(fvals2, pr2, '-', label='x3=C')\n        plt.plot(fvals1, exact1, '-', label='x3=B (exact)')\n        plt.plot(fvals2, exact2, '-', label='x3=C (exact)')\n        (ha, lb) = ax.get_legend_handles_labels()\n        plt.figlegend(ha, lb, loc='center right')\n        plt.xlabel('Focus variable', size=15)\n        if linear:\n            plt.ylabel('Fitted linear predictor', size=15)\n        else:\n            plt.ylabel('Fitted probability', size=15)\n        plt.title('Binomial GLM prediction')\n        self.close_or_save(fig)\n        plt.clf()\n        fig = plt.figure()\n        ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n        plt.plot(fvals1, pr1, '-', label='x3=B', color='orange')\n        plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n        plt.plot(fvals2, pr2, '-', label='x3=C', color='lime')\n        plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n        (ha, lb) = ax.get_legend_handles_labels()\n        plt.figlegend(ha, lb, loc='center right')\n        plt.xlabel('Focus variable', size=15)\n        if linear:\n            plt.ylabel('Fitted linear predictor', size=15)\n        else:\n            plt.ylabel('Fitted probability', size=15)\n        plt.title('Binomial GLM prediction')\n        self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_glm_formula(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np.random.seed(542)\n    n = 500\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.randint(0, 3, size=n)\n    x3 = np.asarray(['ABC'[i] for i in x3])\n    lin_pred = -1 + 0.5 * x1 ** 2 + (x3 == 'B')\n    prob = 1 / (1 + np.exp(-lin_pred))\n    y = 1 * (np.random.uniform(size=n) < prob)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + I(x1**2) + x2 + C(x3)'\n    model = sm.GLM.from_formula(fml, family=sm.families.Binomial(), data=df)\n    result = model.fit()\n    summaries = {'x2': np.mean}\n    for linear in (False, True):\n        values = {'x3': 'B'}\n        (pr1, ci1, fvals1) = predict_functional(result, 'x1', summaries, values, linear=linear)\n        values = {'x3': 'C'}\n        (pr2, ci2, fvals2) = predict_functional(result, 'x1', summaries, values, linear=linear)\n        exact1 = -1 + 0.5 * fvals1 ** 2 + 1\n        exact2 = -1 + 0.5 * fvals2 ** 2\n        if not linear:\n            exact1 = 1 / (1 + np.exp(-exact1))\n            exact2 = 1 / (1 + np.exp(-exact2))\n        plt.clf()\n        fig = plt.figure()\n        ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n        plt.plot(fvals1, pr1, '-', label='x3=B')\n        plt.plot(fvals2, pr2, '-', label='x3=C')\n        plt.plot(fvals1, exact1, '-', label='x3=B (exact)')\n        plt.plot(fvals2, exact2, '-', label='x3=C (exact)')\n        (ha, lb) = ax.get_legend_handles_labels()\n        plt.figlegend(ha, lb, loc='center right')\n        plt.xlabel('Focus variable', size=15)\n        if linear:\n            plt.ylabel('Fitted linear predictor', size=15)\n        else:\n            plt.ylabel('Fitted probability', size=15)\n        plt.title('Binomial GLM prediction')\n        self.close_or_save(fig)\n        plt.clf()\n        fig = plt.figure()\n        ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n        plt.plot(fvals1, pr1, '-', label='x3=B', color='orange')\n        plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n        plt.plot(fvals2, pr2, '-', label='x3=C', color='lime')\n        plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n        (ha, lb) = ax.get_legend_handles_labels()\n        plt.figlegend(ha, lb, loc='center right')\n        plt.xlabel('Focus variable', size=15)\n        if linear:\n            plt.ylabel('Fitted linear predictor', size=15)\n        else:\n            plt.ylabel('Fitted probability', size=15)\n        plt.title('Binomial GLM prediction')\n        self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_glm_formula(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np.random.seed(542)\n    n = 500\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.randint(0, 3, size=n)\n    x3 = np.asarray(['ABC'[i] for i in x3])\n    lin_pred = -1 + 0.5 * x1 ** 2 + (x3 == 'B')\n    prob = 1 / (1 + np.exp(-lin_pred))\n    y = 1 * (np.random.uniform(size=n) < prob)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + I(x1**2) + x2 + C(x3)'\n    model = sm.GLM.from_formula(fml, family=sm.families.Binomial(), data=df)\n    result = model.fit()\n    summaries = {'x2': np.mean}\n    for linear in (False, True):\n        values = {'x3': 'B'}\n        (pr1, ci1, fvals1) = predict_functional(result, 'x1', summaries, values, linear=linear)\n        values = {'x3': 'C'}\n        (pr2, ci2, fvals2) = predict_functional(result, 'x1', summaries, values, linear=linear)\n        exact1 = -1 + 0.5 * fvals1 ** 2 + 1\n        exact2 = -1 + 0.5 * fvals2 ** 2\n        if not linear:\n            exact1 = 1 / (1 + np.exp(-exact1))\n            exact2 = 1 / (1 + np.exp(-exact2))\n        plt.clf()\n        fig = plt.figure()\n        ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n        plt.plot(fvals1, pr1, '-', label='x3=B')\n        plt.plot(fvals2, pr2, '-', label='x3=C')\n        plt.plot(fvals1, exact1, '-', label='x3=B (exact)')\n        plt.plot(fvals2, exact2, '-', label='x3=C (exact)')\n        (ha, lb) = ax.get_legend_handles_labels()\n        plt.figlegend(ha, lb, loc='center right')\n        plt.xlabel('Focus variable', size=15)\n        if linear:\n            plt.ylabel('Fitted linear predictor', size=15)\n        else:\n            plt.ylabel('Fitted probability', size=15)\n        plt.title('Binomial GLM prediction')\n        self.close_or_save(fig)\n        plt.clf()\n        fig = plt.figure()\n        ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n        plt.plot(fvals1, pr1, '-', label='x3=B', color='orange')\n        plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n        plt.plot(fvals2, pr2, '-', label='x3=C', color='lime')\n        plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n        (ha, lb) = ax.get_legend_handles_labels()\n        plt.figlegend(ha, lb, loc='center right')\n        plt.xlabel('Focus variable', size=15)\n        if linear:\n            plt.ylabel('Fitted linear predictor', size=15)\n        else:\n            plt.ylabel('Fitted probability', size=15)\n        plt.title('Binomial GLM prediction')\n        self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_glm_formula(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np.random.seed(542)\n    n = 500\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.randint(0, 3, size=n)\n    x3 = np.asarray(['ABC'[i] for i in x3])\n    lin_pred = -1 + 0.5 * x1 ** 2 + (x3 == 'B')\n    prob = 1 / (1 + np.exp(-lin_pred))\n    y = 1 * (np.random.uniform(size=n) < prob)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + I(x1**2) + x2 + C(x3)'\n    model = sm.GLM.from_formula(fml, family=sm.families.Binomial(), data=df)\n    result = model.fit()\n    summaries = {'x2': np.mean}\n    for linear in (False, True):\n        values = {'x3': 'B'}\n        (pr1, ci1, fvals1) = predict_functional(result, 'x1', summaries, values, linear=linear)\n        values = {'x3': 'C'}\n        (pr2, ci2, fvals2) = predict_functional(result, 'x1', summaries, values, linear=linear)\n        exact1 = -1 + 0.5 * fvals1 ** 2 + 1\n        exact2 = -1 + 0.5 * fvals2 ** 2\n        if not linear:\n            exact1 = 1 / (1 + np.exp(-exact1))\n            exact2 = 1 / (1 + np.exp(-exact2))\n        plt.clf()\n        fig = plt.figure()\n        ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n        plt.plot(fvals1, pr1, '-', label='x3=B')\n        plt.plot(fvals2, pr2, '-', label='x3=C')\n        plt.plot(fvals1, exact1, '-', label='x3=B (exact)')\n        plt.plot(fvals2, exact2, '-', label='x3=C (exact)')\n        (ha, lb) = ax.get_legend_handles_labels()\n        plt.figlegend(ha, lb, loc='center right')\n        plt.xlabel('Focus variable', size=15)\n        if linear:\n            plt.ylabel('Fitted linear predictor', size=15)\n        else:\n            plt.ylabel('Fitted probability', size=15)\n        plt.title('Binomial GLM prediction')\n        self.close_or_save(fig)\n        plt.clf()\n        fig = plt.figure()\n        ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n        plt.plot(fvals1, pr1, '-', label='x3=B', color='orange')\n        plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n        plt.plot(fvals2, pr2, '-', label='x3=C', color='lime')\n        plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n        (ha, lb) = ax.get_legend_handles_labels()\n        plt.figlegend(ha, lb, loc='center right')\n        plt.xlabel('Focus variable', size=15)\n        if linear:\n            plt.ylabel('Fitted linear predictor', size=15)\n        else:\n            plt.ylabel('Fitted probability', size=15)\n        plt.title('Binomial GLM prediction')\n        self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_glm_formula(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np.random.seed(542)\n    n = 500\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.randint(0, 3, size=n)\n    x3 = np.asarray(['ABC'[i] for i in x3])\n    lin_pred = -1 + 0.5 * x1 ** 2 + (x3 == 'B')\n    prob = 1 / (1 + np.exp(-lin_pred))\n    y = 1 * (np.random.uniform(size=n) < prob)\n    df = pd.DataFrame({'y': y, 'x1': x1, 'x2': x2, 'x3': x3})\n    fml = 'y ~ x1 + I(x1**2) + x2 + C(x3)'\n    model = sm.GLM.from_formula(fml, family=sm.families.Binomial(), data=df)\n    result = model.fit()\n    summaries = {'x2': np.mean}\n    for linear in (False, True):\n        values = {'x3': 'B'}\n        (pr1, ci1, fvals1) = predict_functional(result, 'x1', summaries, values, linear=linear)\n        values = {'x3': 'C'}\n        (pr2, ci2, fvals2) = predict_functional(result, 'x1', summaries, values, linear=linear)\n        exact1 = -1 + 0.5 * fvals1 ** 2 + 1\n        exact2 = -1 + 0.5 * fvals2 ** 2\n        if not linear:\n            exact1 = 1 / (1 + np.exp(-exact1))\n            exact2 = 1 / (1 + np.exp(-exact2))\n        plt.clf()\n        fig = plt.figure()\n        ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n        plt.plot(fvals1, pr1, '-', label='x3=B')\n        plt.plot(fvals2, pr2, '-', label='x3=C')\n        plt.plot(fvals1, exact1, '-', label='x3=B (exact)')\n        plt.plot(fvals2, exact2, '-', label='x3=C (exact)')\n        (ha, lb) = ax.get_legend_handles_labels()\n        plt.figlegend(ha, lb, loc='center right')\n        plt.xlabel('Focus variable', size=15)\n        if linear:\n            plt.ylabel('Fitted linear predictor', size=15)\n        else:\n            plt.ylabel('Fitted probability', size=15)\n        plt.title('Binomial GLM prediction')\n        self.close_or_save(fig)\n        plt.clf()\n        fig = plt.figure()\n        ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n        plt.plot(fvals1, pr1, '-', label='x3=B', color='orange')\n        plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n        plt.plot(fvals2, pr2, '-', label='x3=C', color='lime')\n        plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n        (ha, lb) = ax.get_legend_handles_labels()\n        plt.figlegend(ha, lb, loc='center right')\n        plt.xlabel('Focus variable', size=15)\n        if linear:\n            plt.ylabel('Fitted linear predictor', size=15)\n        else:\n            plt.ylabel('Fitted probability', size=15)\n        plt.title('Binomial GLM prediction')\n        self.close_or_save(fig)"
        ]
    },
    {
        "func_name": "test_noformula_prediction",
        "original": "@pytest.mark.matplotlib\ndef test_noformula_prediction(self, close_figures):\n    np.random.seed(6434)\n    n = 200\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    y = x1 - x2 + np.random.normal(size=n)\n    exog = np.vstack((x1, x2, x3)).T\n    model = sm.OLS(y, exog)\n    result = model.fit()\n    summaries = {'x3': pctl(0.75)}\n    values = {'x2': 1}\n    (pr1, ci1, fvals1) = predict_functional(result, 'x1', summaries, values)\n    values = {'x2': -1}\n    (pr2, ci2, fvals2) = predict_functional(result, 'x1', summaries, values)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x2=1', lw=4, alpha=0.6, color='orange')\n    plt.plot(fvals2, pr2, '-', label='x2=-1', lw=4, alpha=0.6, color='lime')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x2=1', lw=4, alpha=0.6, color='orange')\n    plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n    plt.plot(fvals1, pr2, '-', label='x2=1', lw=4, alpha=0.6, color='lime')\n    plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)",
        "mutated": [
            "@pytest.mark.matplotlib\ndef test_noformula_prediction(self, close_figures):\n    if False:\n        i = 10\n    np.random.seed(6434)\n    n = 200\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    y = x1 - x2 + np.random.normal(size=n)\n    exog = np.vstack((x1, x2, x3)).T\n    model = sm.OLS(y, exog)\n    result = model.fit()\n    summaries = {'x3': pctl(0.75)}\n    values = {'x2': 1}\n    (pr1, ci1, fvals1) = predict_functional(result, 'x1', summaries, values)\n    values = {'x2': -1}\n    (pr2, ci2, fvals2) = predict_functional(result, 'x1', summaries, values)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x2=1', lw=4, alpha=0.6, color='orange')\n    plt.plot(fvals2, pr2, '-', label='x2=-1', lw=4, alpha=0.6, color='lime')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x2=1', lw=4, alpha=0.6, color='orange')\n    plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n    plt.plot(fvals1, pr2, '-', label='x2=1', lw=4, alpha=0.6, color='lime')\n    plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_noformula_prediction(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np.random.seed(6434)\n    n = 200\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    y = x1 - x2 + np.random.normal(size=n)\n    exog = np.vstack((x1, x2, x3)).T\n    model = sm.OLS(y, exog)\n    result = model.fit()\n    summaries = {'x3': pctl(0.75)}\n    values = {'x2': 1}\n    (pr1, ci1, fvals1) = predict_functional(result, 'x1', summaries, values)\n    values = {'x2': -1}\n    (pr2, ci2, fvals2) = predict_functional(result, 'x1', summaries, values)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x2=1', lw=4, alpha=0.6, color='orange')\n    plt.plot(fvals2, pr2, '-', label='x2=-1', lw=4, alpha=0.6, color='lime')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x2=1', lw=4, alpha=0.6, color='orange')\n    plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n    plt.plot(fvals1, pr2, '-', label='x2=1', lw=4, alpha=0.6, color='lime')\n    plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_noformula_prediction(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np.random.seed(6434)\n    n = 200\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    y = x1 - x2 + np.random.normal(size=n)\n    exog = np.vstack((x1, x2, x3)).T\n    model = sm.OLS(y, exog)\n    result = model.fit()\n    summaries = {'x3': pctl(0.75)}\n    values = {'x2': 1}\n    (pr1, ci1, fvals1) = predict_functional(result, 'x1', summaries, values)\n    values = {'x2': -1}\n    (pr2, ci2, fvals2) = predict_functional(result, 'x1', summaries, values)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x2=1', lw=4, alpha=0.6, color='orange')\n    plt.plot(fvals2, pr2, '-', label='x2=-1', lw=4, alpha=0.6, color='lime')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x2=1', lw=4, alpha=0.6, color='orange')\n    plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n    plt.plot(fvals1, pr2, '-', label='x2=1', lw=4, alpha=0.6, color='lime')\n    plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_noformula_prediction(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np.random.seed(6434)\n    n = 200\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    y = x1 - x2 + np.random.normal(size=n)\n    exog = np.vstack((x1, x2, x3)).T\n    model = sm.OLS(y, exog)\n    result = model.fit()\n    summaries = {'x3': pctl(0.75)}\n    values = {'x2': 1}\n    (pr1, ci1, fvals1) = predict_functional(result, 'x1', summaries, values)\n    values = {'x2': -1}\n    (pr2, ci2, fvals2) = predict_functional(result, 'x1', summaries, values)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x2=1', lw=4, alpha=0.6, color='orange')\n    plt.plot(fvals2, pr2, '-', label='x2=-1', lw=4, alpha=0.6, color='lime')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x2=1', lw=4, alpha=0.6, color='orange')\n    plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n    plt.plot(fvals1, pr2, '-', label='x2=1', lw=4, alpha=0.6, color='lime')\n    plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)",
            "@pytest.mark.matplotlib\ndef test_noformula_prediction(self, close_figures):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np.random.seed(6434)\n    n = 200\n    x1 = np.random.normal(size=n)\n    x2 = np.random.normal(size=n)\n    x3 = np.random.normal(size=n)\n    y = x1 - x2 + np.random.normal(size=n)\n    exog = np.vstack((x1, x2, x3)).T\n    model = sm.OLS(y, exog)\n    result = model.fit()\n    summaries = {'x3': pctl(0.75)}\n    values = {'x2': 1}\n    (pr1, ci1, fvals1) = predict_functional(result, 'x1', summaries, values)\n    values = {'x2': -1}\n    (pr2, ci2, fvals2) = predict_functional(result, 'x1', summaries, values)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x2=1', lw=4, alpha=0.6, color='orange')\n    plt.plot(fvals2, pr2, '-', label='x2=-1', lw=4, alpha=0.6, color='lime')\n    (ha, lb) = ax.get_legend_handles_labels()\n    leg = plt.figlegend(ha, lb, loc='center right')\n    leg.draw_frame(False)\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)\n    plt.clf()\n    fig = plt.figure()\n    ax = plt.axes([0.1, 0.1, 0.7, 0.8])\n    plt.plot(fvals1, pr1, '-', label='x2=1', lw=4, alpha=0.6, color='orange')\n    plt.fill_between(fvals1, ci1[:, 0], ci1[:, 1], color='grey')\n    plt.plot(fvals1, pr2, '-', label='x2=1', lw=4, alpha=0.6, color='lime')\n    plt.fill_between(fvals2, ci2[:, 0], ci2[:, 1], color='grey')\n    (ha, lb) = ax.get_legend_handles_labels()\n    plt.figlegend(ha, lb, loc='center right')\n    plt.xlabel('Focus variable', size=15)\n    plt.ylabel('Fitted mean', size=15)\n    plt.title('Linear model prediction')\n    self.close_or_save(fig)"
        ]
    }
]