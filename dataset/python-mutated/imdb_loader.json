[
    {
        "func_name": "_read_words",
        "original": "def _read_words(filename, use_prefix=True):\n    all_words = []\n    sequence_example = tf.train.SequenceExample()\n    for r in tf.python_io.tf_record_iterator(filename):\n        sequence_example.ParseFromString(r)\n        if FLAGS.prefix_label and use_prefix:\n            label = sequence_example.context.feature['class'].int64_list.value[0]\n            review_words = [EOS_INDEX + 1 + label]\n        else:\n            review_words = []\n        review_words.extend([f.int64_list.value[0] for f in sequence_example.feature_lists.feature_list['token_id'].feature])\n        all_words.append(review_words)\n    return all_words",
        "mutated": [
            "def _read_words(filename, use_prefix=True):\n    if False:\n        i = 10\n    all_words = []\n    sequence_example = tf.train.SequenceExample()\n    for r in tf.python_io.tf_record_iterator(filename):\n        sequence_example.ParseFromString(r)\n        if FLAGS.prefix_label and use_prefix:\n            label = sequence_example.context.feature['class'].int64_list.value[0]\n            review_words = [EOS_INDEX + 1 + label]\n        else:\n            review_words = []\n        review_words.extend([f.int64_list.value[0] for f in sequence_example.feature_lists.feature_list['token_id'].feature])\n        all_words.append(review_words)\n    return all_words",
            "def _read_words(filename, use_prefix=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    all_words = []\n    sequence_example = tf.train.SequenceExample()\n    for r in tf.python_io.tf_record_iterator(filename):\n        sequence_example.ParseFromString(r)\n        if FLAGS.prefix_label and use_prefix:\n            label = sequence_example.context.feature['class'].int64_list.value[0]\n            review_words = [EOS_INDEX + 1 + label]\n        else:\n            review_words = []\n        review_words.extend([f.int64_list.value[0] for f in sequence_example.feature_lists.feature_list['token_id'].feature])\n        all_words.append(review_words)\n    return all_words",
            "def _read_words(filename, use_prefix=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    all_words = []\n    sequence_example = tf.train.SequenceExample()\n    for r in tf.python_io.tf_record_iterator(filename):\n        sequence_example.ParseFromString(r)\n        if FLAGS.prefix_label and use_prefix:\n            label = sequence_example.context.feature['class'].int64_list.value[0]\n            review_words = [EOS_INDEX + 1 + label]\n        else:\n            review_words = []\n        review_words.extend([f.int64_list.value[0] for f in sequence_example.feature_lists.feature_list['token_id'].feature])\n        all_words.append(review_words)\n    return all_words",
            "def _read_words(filename, use_prefix=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    all_words = []\n    sequence_example = tf.train.SequenceExample()\n    for r in tf.python_io.tf_record_iterator(filename):\n        sequence_example.ParseFromString(r)\n        if FLAGS.prefix_label and use_prefix:\n            label = sequence_example.context.feature['class'].int64_list.value[0]\n            review_words = [EOS_INDEX + 1 + label]\n        else:\n            review_words = []\n        review_words.extend([f.int64_list.value[0] for f in sequence_example.feature_lists.feature_list['token_id'].feature])\n        all_words.append(review_words)\n    return all_words",
            "def _read_words(filename, use_prefix=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    all_words = []\n    sequence_example = tf.train.SequenceExample()\n    for r in tf.python_io.tf_record_iterator(filename):\n        sequence_example.ParseFromString(r)\n        if FLAGS.prefix_label and use_prefix:\n            label = sequence_example.context.feature['class'].int64_list.value[0]\n            review_words = [EOS_INDEX + 1 + label]\n        else:\n            review_words = []\n        review_words.extend([f.int64_list.value[0] for f in sequence_example.feature_lists.feature_list['token_id'].feature])\n        all_words.append(review_words)\n    return all_words"
        ]
    },
    {
        "func_name": "build_vocab",
        "original": "def build_vocab(vocab_file):\n    word_to_id = {}\n    with tf.gfile.GFile(vocab_file, 'r') as f:\n        index = 0\n        for word in f:\n            word_to_id[word.strip()] = index\n            index += 1\n        word_to_id['<eos>'] = EOS_INDEX\n    return word_to_id",
        "mutated": [
            "def build_vocab(vocab_file):\n    if False:\n        i = 10\n    word_to_id = {}\n    with tf.gfile.GFile(vocab_file, 'r') as f:\n        index = 0\n        for word in f:\n            word_to_id[word.strip()] = index\n            index += 1\n        word_to_id['<eos>'] = EOS_INDEX\n    return word_to_id",
            "def build_vocab(vocab_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    word_to_id = {}\n    with tf.gfile.GFile(vocab_file, 'r') as f:\n        index = 0\n        for word in f:\n            word_to_id[word.strip()] = index\n            index += 1\n        word_to_id['<eos>'] = EOS_INDEX\n    return word_to_id",
            "def build_vocab(vocab_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    word_to_id = {}\n    with tf.gfile.GFile(vocab_file, 'r') as f:\n        index = 0\n        for word in f:\n            word_to_id[word.strip()] = index\n            index += 1\n        word_to_id['<eos>'] = EOS_INDEX\n    return word_to_id",
            "def build_vocab(vocab_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    word_to_id = {}\n    with tf.gfile.GFile(vocab_file, 'r') as f:\n        index = 0\n        for word in f:\n            word_to_id[word.strip()] = index\n            index += 1\n        word_to_id['<eos>'] = EOS_INDEX\n    return word_to_id",
            "def build_vocab(vocab_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    word_to_id = {}\n    with tf.gfile.GFile(vocab_file, 'r') as f:\n        index = 0\n        for word in f:\n            word_to_id[word.strip()] = index\n            index += 1\n        word_to_id['<eos>'] = EOS_INDEX\n    return word_to_id"
        ]
    },
    {
        "func_name": "imdb_raw_data",
        "original": "def imdb_raw_data(data_path=None):\n    \"\"\"Load IMDB raw data from data directory \"data_path\".\n  Reads IMDB tf record files containing integer ids,\n  and performs mini-batching of the inputs.\n  Args:\n    data_path: string path to the directory where simple-examples.tgz has\n      been extracted.\n  Returns:\n    tuple (train_data, valid_data)\n    where each of the data objects can be passed to IMDBIterator.\n  \"\"\"\n    train_path = os.path.join(data_path, 'train_lm.tfrecords')\n    valid_path = os.path.join(data_path, 'test_lm.tfrecords')\n    train_data = _read_words(train_path)\n    valid_data = _read_words(valid_path)\n    return (train_data, valid_data)",
        "mutated": [
            "def imdb_raw_data(data_path=None):\n    if False:\n        i = 10\n    'Load IMDB raw data from data directory \"data_path\".\\n  Reads IMDB tf record files containing integer ids,\\n  and performs mini-batching of the inputs.\\n  Args:\\n    data_path: string path to the directory where simple-examples.tgz has\\n      been extracted.\\n  Returns:\\n    tuple (train_data, valid_data)\\n    where each of the data objects can be passed to IMDBIterator.\\n  '\n    train_path = os.path.join(data_path, 'train_lm.tfrecords')\n    valid_path = os.path.join(data_path, 'test_lm.tfrecords')\n    train_data = _read_words(train_path)\n    valid_data = _read_words(valid_path)\n    return (train_data, valid_data)",
            "def imdb_raw_data(data_path=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Load IMDB raw data from data directory \"data_path\".\\n  Reads IMDB tf record files containing integer ids,\\n  and performs mini-batching of the inputs.\\n  Args:\\n    data_path: string path to the directory where simple-examples.tgz has\\n      been extracted.\\n  Returns:\\n    tuple (train_data, valid_data)\\n    where each of the data objects can be passed to IMDBIterator.\\n  '\n    train_path = os.path.join(data_path, 'train_lm.tfrecords')\n    valid_path = os.path.join(data_path, 'test_lm.tfrecords')\n    train_data = _read_words(train_path)\n    valid_data = _read_words(valid_path)\n    return (train_data, valid_data)",
            "def imdb_raw_data(data_path=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Load IMDB raw data from data directory \"data_path\".\\n  Reads IMDB tf record files containing integer ids,\\n  and performs mini-batching of the inputs.\\n  Args:\\n    data_path: string path to the directory where simple-examples.tgz has\\n      been extracted.\\n  Returns:\\n    tuple (train_data, valid_data)\\n    where each of the data objects can be passed to IMDBIterator.\\n  '\n    train_path = os.path.join(data_path, 'train_lm.tfrecords')\n    valid_path = os.path.join(data_path, 'test_lm.tfrecords')\n    train_data = _read_words(train_path)\n    valid_data = _read_words(valid_path)\n    return (train_data, valid_data)",
            "def imdb_raw_data(data_path=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Load IMDB raw data from data directory \"data_path\".\\n  Reads IMDB tf record files containing integer ids,\\n  and performs mini-batching of the inputs.\\n  Args:\\n    data_path: string path to the directory where simple-examples.tgz has\\n      been extracted.\\n  Returns:\\n    tuple (train_data, valid_data)\\n    where each of the data objects can be passed to IMDBIterator.\\n  '\n    train_path = os.path.join(data_path, 'train_lm.tfrecords')\n    valid_path = os.path.join(data_path, 'test_lm.tfrecords')\n    train_data = _read_words(train_path)\n    valid_data = _read_words(valid_path)\n    return (train_data, valid_data)",
            "def imdb_raw_data(data_path=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Load IMDB raw data from data directory \"data_path\".\\n  Reads IMDB tf record files containing integer ids,\\n  and performs mini-batching of the inputs.\\n  Args:\\n    data_path: string path to the directory where simple-examples.tgz has\\n      been extracted.\\n  Returns:\\n    tuple (train_data, valid_data)\\n    where each of the data objects can be passed to IMDBIterator.\\n  '\n    train_path = os.path.join(data_path, 'train_lm.tfrecords')\n    valid_path = os.path.join(data_path, 'test_lm.tfrecords')\n    train_data = _read_words(train_path)\n    valid_data = _read_words(valid_path)\n    return (train_data, valid_data)"
        ]
    },
    {
        "func_name": "imdb_iterator",
        "original": "def imdb_iterator(raw_data, batch_size, num_steps, epoch_size_override=None):\n    \"\"\"Iterate on the raw IMDB data.\n\n  This generates batch_size pointers into the raw IMDB data, and allows\n  minibatch iteration along these pointers.\n\n  Args:\n    raw_data: one of the raw data outputs from imdb_raw_data.\n    batch_size: int, the batch size.\n    num_steps: int, the number of unrolls.\n\n  Yields:\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n    The second element of the tuple is the same data time-shifted to the\n    right by one. The third is a set of weights with 1 indicating a word was\n    present and 0 not.\n\n  Raises:\n    ValueError: if batch_size or num_steps are too high.\n  \"\"\"\n    del epoch_size_override\n    data_len = len(raw_data)\n    num_batches = data_len // batch_size - 1\n    for batch in range(num_batches):\n        x = np.zeros([batch_size, num_steps], dtype=np.int32)\n        y = np.zeros([batch_size, num_steps], dtype=np.int32)\n        w = np.zeros([batch_size, num_steps], dtype=np.float)\n        for i in range(batch_size):\n            data_index = batch * batch_size + i\n            example = raw_data[data_index]\n            if len(example) > num_steps:\n                final_x = example[:num_steps]\n                final_y = example[1:num_steps + 1]\n                w[i] = 1\n            else:\n                to_fill_in = num_steps - len(example)\n                final_x = example + [EOS_INDEX] * to_fill_in\n                final_y = final_x[1:] + [EOS_INDEX]\n                w[i] = [1] * len(example) + [0] * to_fill_in\n            x[i] = final_x\n            y[i] = final_y\n        yield (x, y, w)",
        "mutated": [
            "def imdb_iterator(raw_data, batch_size, num_steps, epoch_size_override=None):\n    if False:\n        i = 10\n    'Iterate on the raw IMDB data.\\n\\n  This generates batch_size pointers into the raw IMDB data, and allows\\n  minibatch iteration along these pointers.\\n\\n  Args:\\n    raw_data: one of the raw data outputs from imdb_raw_data.\\n    batch_size: int, the batch size.\\n    num_steps: int, the number of unrolls.\\n\\n  Yields:\\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\\n    The second element of the tuple is the same data time-shifted to the\\n    right by one. The third is a set of weights with 1 indicating a word was\\n    present and 0 not.\\n\\n  Raises:\\n    ValueError: if batch_size or num_steps are too high.\\n  '\n    del epoch_size_override\n    data_len = len(raw_data)\n    num_batches = data_len // batch_size - 1\n    for batch in range(num_batches):\n        x = np.zeros([batch_size, num_steps], dtype=np.int32)\n        y = np.zeros([batch_size, num_steps], dtype=np.int32)\n        w = np.zeros([batch_size, num_steps], dtype=np.float)\n        for i in range(batch_size):\n            data_index = batch * batch_size + i\n            example = raw_data[data_index]\n            if len(example) > num_steps:\n                final_x = example[:num_steps]\n                final_y = example[1:num_steps + 1]\n                w[i] = 1\n            else:\n                to_fill_in = num_steps - len(example)\n                final_x = example + [EOS_INDEX] * to_fill_in\n                final_y = final_x[1:] + [EOS_INDEX]\n                w[i] = [1] * len(example) + [0] * to_fill_in\n            x[i] = final_x\n            y[i] = final_y\n        yield (x, y, w)",
            "def imdb_iterator(raw_data, batch_size, num_steps, epoch_size_override=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Iterate on the raw IMDB data.\\n\\n  This generates batch_size pointers into the raw IMDB data, and allows\\n  minibatch iteration along these pointers.\\n\\n  Args:\\n    raw_data: one of the raw data outputs from imdb_raw_data.\\n    batch_size: int, the batch size.\\n    num_steps: int, the number of unrolls.\\n\\n  Yields:\\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\\n    The second element of the tuple is the same data time-shifted to the\\n    right by one. The third is a set of weights with 1 indicating a word was\\n    present and 0 not.\\n\\n  Raises:\\n    ValueError: if batch_size or num_steps are too high.\\n  '\n    del epoch_size_override\n    data_len = len(raw_data)\n    num_batches = data_len // batch_size - 1\n    for batch in range(num_batches):\n        x = np.zeros([batch_size, num_steps], dtype=np.int32)\n        y = np.zeros([batch_size, num_steps], dtype=np.int32)\n        w = np.zeros([batch_size, num_steps], dtype=np.float)\n        for i in range(batch_size):\n            data_index = batch * batch_size + i\n            example = raw_data[data_index]\n            if len(example) > num_steps:\n                final_x = example[:num_steps]\n                final_y = example[1:num_steps + 1]\n                w[i] = 1\n            else:\n                to_fill_in = num_steps - len(example)\n                final_x = example + [EOS_INDEX] * to_fill_in\n                final_y = final_x[1:] + [EOS_INDEX]\n                w[i] = [1] * len(example) + [0] * to_fill_in\n            x[i] = final_x\n            y[i] = final_y\n        yield (x, y, w)",
            "def imdb_iterator(raw_data, batch_size, num_steps, epoch_size_override=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Iterate on the raw IMDB data.\\n\\n  This generates batch_size pointers into the raw IMDB data, and allows\\n  minibatch iteration along these pointers.\\n\\n  Args:\\n    raw_data: one of the raw data outputs from imdb_raw_data.\\n    batch_size: int, the batch size.\\n    num_steps: int, the number of unrolls.\\n\\n  Yields:\\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\\n    The second element of the tuple is the same data time-shifted to the\\n    right by one. The third is a set of weights with 1 indicating a word was\\n    present and 0 not.\\n\\n  Raises:\\n    ValueError: if batch_size or num_steps are too high.\\n  '\n    del epoch_size_override\n    data_len = len(raw_data)\n    num_batches = data_len // batch_size - 1\n    for batch in range(num_batches):\n        x = np.zeros([batch_size, num_steps], dtype=np.int32)\n        y = np.zeros([batch_size, num_steps], dtype=np.int32)\n        w = np.zeros([batch_size, num_steps], dtype=np.float)\n        for i in range(batch_size):\n            data_index = batch * batch_size + i\n            example = raw_data[data_index]\n            if len(example) > num_steps:\n                final_x = example[:num_steps]\n                final_y = example[1:num_steps + 1]\n                w[i] = 1\n            else:\n                to_fill_in = num_steps - len(example)\n                final_x = example + [EOS_INDEX] * to_fill_in\n                final_y = final_x[1:] + [EOS_INDEX]\n                w[i] = [1] * len(example) + [0] * to_fill_in\n            x[i] = final_x\n            y[i] = final_y\n        yield (x, y, w)",
            "def imdb_iterator(raw_data, batch_size, num_steps, epoch_size_override=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Iterate on the raw IMDB data.\\n\\n  This generates batch_size pointers into the raw IMDB data, and allows\\n  minibatch iteration along these pointers.\\n\\n  Args:\\n    raw_data: one of the raw data outputs from imdb_raw_data.\\n    batch_size: int, the batch size.\\n    num_steps: int, the number of unrolls.\\n\\n  Yields:\\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\\n    The second element of the tuple is the same data time-shifted to the\\n    right by one. The third is a set of weights with 1 indicating a word was\\n    present and 0 not.\\n\\n  Raises:\\n    ValueError: if batch_size or num_steps are too high.\\n  '\n    del epoch_size_override\n    data_len = len(raw_data)\n    num_batches = data_len // batch_size - 1\n    for batch in range(num_batches):\n        x = np.zeros([batch_size, num_steps], dtype=np.int32)\n        y = np.zeros([batch_size, num_steps], dtype=np.int32)\n        w = np.zeros([batch_size, num_steps], dtype=np.float)\n        for i in range(batch_size):\n            data_index = batch * batch_size + i\n            example = raw_data[data_index]\n            if len(example) > num_steps:\n                final_x = example[:num_steps]\n                final_y = example[1:num_steps + 1]\n                w[i] = 1\n            else:\n                to_fill_in = num_steps - len(example)\n                final_x = example + [EOS_INDEX] * to_fill_in\n                final_y = final_x[1:] + [EOS_INDEX]\n                w[i] = [1] * len(example) + [0] * to_fill_in\n            x[i] = final_x\n            y[i] = final_y\n        yield (x, y, w)",
            "def imdb_iterator(raw_data, batch_size, num_steps, epoch_size_override=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Iterate on the raw IMDB data.\\n\\n  This generates batch_size pointers into the raw IMDB data, and allows\\n  minibatch iteration along these pointers.\\n\\n  Args:\\n    raw_data: one of the raw data outputs from imdb_raw_data.\\n    batch_size: int, the batch size.\\n    num_steps: int, the number of unrolls.\\n\\n  Yields:\\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\\n    The second element of the tuple is the same data time-shifted to the\\n    right by one. The third is a set of weights with 1 indicating a word was\\n    present and 0 not.\\n\\n  Raises:\\n    ValueError: if batch_size or num_steps are too high.\\n  '\n    del epoch_size_override\n    data_len = len(raw_data)\n    num_batches = data_len // batch_size - 1\n    for batch in range(num_batches):\n        x = np.zeros([batch_size, num_steps], dtype=np.int32)\n        y = np.zeros([batch_size, num_steps], dtype=np.int32)\n        w = np.zeros([batch_size, num_steps], dtype=np.float)\n        for i in range(batch_size):\n            data_index = batch * batch_size + i\n            example = raw_data[data_index]\n            if len(example) > num_steps:\n                final_x = example[:num_steps]\n                final_y = example[1:num_steps + 1]\n                w[i] = 1\n            else:\n                to_fill_in = num_steps - len(example)\n                final_x = example + [EOS_INDEX] * to_fill_in\n                final_y = final_x[1:] + [EOS_INDEX]\n                w[i] = [1] * len(example) + [0] * to_fill_in\n            x[i] = final_x\n            y[i] = final_y\n        yield (x, y, w)"
        ]
    }
]