[
    {
        "func_name": "pairwise_squared_distance",
        "original": "def pairwise_squared_distance(feature):\n    \"\"\"Computes the squared pairwise distance matrix.\n\n  output[i, j] = || feature[i, :] - feature[j, :] ||_2^2\n\n  Args:\n    feature: 2-D Tensor of size [number of data, feature dimension]\n\n  Returns:\n    pairwise_squared_distances: 2-D Tensor of size\n      [number of data, number of data]\n  \"\"\"\n    pairwise_squared_distances = tf.add(tf.reduce_sum(tf.square(feature), axis=1, keep_dims=True), tf.reduce_sum(tf.square(tf.transpose(feature)), axis=0, keep_dims=True)) - 2.0 * tf.matmul(feature, tf.transpose(feature))\n    pairwise_squared_distances = tf.maximum(pairwise_squared_distances, 0.0)\n    return pairwise_squared_distances",
        "mutated": [
            "def pairwise_squared_distance(feature):\n    if False:\n        i = 10\n    'Computes the squared pairwise distance matrix.\\n\\n  output[i, j] = || feature[i, :] - feature[j, :] ||_2^2\\n\\n  Args:\\n    feature: 2-D Tensor of size [number of data, feature dimension]\\n\\n  Returns:\\n    pairwise_squared_distances: 2-D Tensor of size\\n      [number of data, number of data]\\n  '\n    pairwise_squared_distances = tf.add(tf.reduce_sum(tf.square(feature), axis=1, keep_dims=True), tf.reduce_sum(tf.square(tf.transpose(feature)), axis=0, keep_dims=True)) - 2.0 * tf.matmul(feature, tf.transpose(feature))\n    pairwise_squared_distances = tf.maximum(pairwise_squared_distances, 0.0)\n    return pairwise_squared_distances",
            "def pairwise_squared_distance(feature):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes the squared pairwise distance matrix.\\n\\n  output[i, j] = || feature[i, :] - feature[j, :] ||_2^2\\n\\n  Args:\\n    feature: 2-D Tensor of size [number of data, feature dimension]\\n\\n  Returns:\\n    pairwise_squared_distances: 2-D Tensor of size\\n      [number of data, number of data]\\n  '\n    pairwise_squared_distances = tf.add(tf.reduce_sum(tf.square(feature), axis=1, keep_dims=True), tf.reduce_sum(tf.square(tf.transpose(feature)), axis=0, keep_dims=True)) - 2.0 * tf.matmul(feature, tf.transpose(feature))\n    pairwise_squared_distances = tf.maximum(pairwise_squared_distances, 0.0)\n    return pairwise_squared_distances",
            "def pairwise_squared_distance(feature):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes the squared pairwise distance matrix.\\n\\n  output[i, j] = || feature[i, :] - feature[j, :] ||_2^2\\n\\n  Args:\\n    feature: 2-D Tensor of size [number of data, feature dimension]\\n\\n  Returns:\\n    pairwise_squared_distances: 2-D Tensor of size\\n      [number of data, number of data]\\n  '\n    pairwise_squared_distances = tf.add(tf.reduce_sum(tf.square(feature), axis=1, keep_dims=True), tf.reduce_sum(tf.square(tf.transpose(feature)), axis=0, keep_dims=True)) - 2.0 * tf.matmul(feature, tf.transpose(feature))\n    pairwise_squared_distances = tf.maximum(pairwise_squared_distances, 0.0)\n    return pairwise_squared_distances",
            "def pairwise_squared_distance(feature):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes the squared pairwise distance matrix.\\n\\n  output[i, j] = || feature[i, :] - feature[j, :] ||_2^2\\n\\n  Args:\\n    feature: 2-D Tensor of size [number of data, feature dimension]\\n\\n  Returns:\\n    pairwise_squared_distances: 2-D Tensor of size\\n      [number of data, number of data]\\n  '\n    pairwise_squared_distances = tf.add(tf.reduce_sum(tf.square(feature), axis=1, keep_dims=True), tf.reduce_sum(tf.square(tf.transpose(feature)), axis=0, keep_dims=True)) - 2.0 * tf.matmul(feature, tf.transpose(feature))\n    pairwise_squared_distances = tf.maximum(pairwise_squared_distances, 0.0)\n    return pairwise_squared_distances",
            "def pairwise_squared_distance(feature):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes the squared pairwise distance matrix.\\n\\n  output[i, j] = || feature[i, :] - feature[j, :] ||_2^2\\n\\n  Args:\\n    feature: 2-D Tensor of size [number of data, feature dimension]\\n\\n  Returns:\\n    pairwise_squared_distances: 2-D Tensor of size\\n      [number of data, number of data]\\n  '\n    pairwise_squared_distances = tf.add(tf.reduce_sum(tf.square(feature), axis=1, keep_dims=True), tf.reduce_sum(tf.square(tf.transpose(feature)), axis=0, keep_dims=True)) - 2.0 * tf.matmul(feature, tf.transpose(feature))\n    pairwise_squared_distances = tf.maximum(pairwise_squared_distances, 0.0)\n    return pairwise_squared_distances"
        ]
    },
    {
        "func_name": "masked_maximum",
        "original": "def masked_maximum(data, mask, dim=1):\n    \"\"\"Computes the axis wise maximum over chosen elements.\n\n  Args:\n    data: N-D Tensor.\n    mask: N-D Tensor of zeros or ones.\n    dim: The dimension over which to compute the maximum.\n\n  Returns:\n    masked_maximums: N-D Tensor.\n      The maximized dimension is of size 1 after the operation.\n  \"\"\"\n    axis_minimums = tf.reduce_min(data, dim, keep_dims=True)\n    masked_maximums = tf.reduce_max(tf.multiply(data - axis_minimums, mask), dim, keep_dims=True) + axis_minimums\n    return masked_maximums",
        "mutated": [
            "def masked_maximum(data, mask, dim=1):\n    if False:\n        i = 10\n    'Computes the axis wise maximum over chosen elements.\\n\\n  Args:\\n    data: N-D Tensor.\\n    mask: N-D Tensor of zeros or ones.\\n    dim: The dimension over which to compute the maximum.\\n\\n  Returns:\\n    masked_maximums: N-D Tensor.\\n      The maximized dimension is of size 1 after the operation.\\n  '\n    axis_minimums = tf.reduce_min(data, dim, keep_dims=True)\n    masked_maximums = tf.reduce_max(tf.multiply(data - axis_minimums, mask), dim, keep_dims=True) + axis_minimums\n    return masked_maximums",
            "def masked_maximum(data, mask, dim=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes the axis wise maximum over chosen elements.\\n\\n  Args:\\n    data: N-D Tensor.\\n    mask: N-D Tensor of zeros or ones.\\n    dim: The dimension over which to compute the maximum.\\n\\n  Returns:\\n    masked_maximums: N-D Tensor.\\n      The maximized dimension is of size 1 after the operation.\\n  '\n    axis_minimums = tf.reduce_min(data, dim, keep_dims=True)\n    masked_maximums = tf.reduce_max(tf.multiply(data - axis_minimums, mask), dim, keep_dims=True) + axis_minimums\n    return masked_maximums",
            "def masked_maximum(data, mask, dim=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes the axis wise maximum over chosen elements.\\n\\n  Args:\\n    data: N-D Tensor.\\n    mask: N-D Tensor of zeros or ones.\\n    dim: The dimension over which to compute the maximum.\\n\\n  Returns:\\n    masked_maximums: N-D Tensor.\\n      The maximized dimension is of size 1 after the operation.\\n  '\n    axis_minimums = tf.reduce_min(data, dim, keep_dims=True)\n    masked_maximums = tf.reduce_max(tf.multiply(data - axis_minimums, mask), dim, keep_dims=True) + axis_minimums\n    return masked_maximums",
            "def masked_maximum(data, mask, dim=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes the axis wise maximum over chosen elements.\\n\\n  Args:\\n    data: N-D Tensor.\\n    mask: N-D Tensor of zeros or ones.\\n    dim: The dimension over which to compute the maximum.\\n\\n  Returns:\\n    masked_maximums: N-D Tensor.\\n      The maximized dimension is of size 1 after the operation.\\n  '\n    axis_minimums = tf.reduce_min(data, dim, keep_dims=True)\n    masked_maximums = tf.reduce_max(tf.multiply(data - axis_minimums, mask), dim, keep_dims=True) + axis_minimums\n    return masked_maximums",
            "def masked_maximum(data, mask, dim=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes the axis wise maximum over chosen elements.\\n\\n  Args:\\n    data: N-D Tensor.\\n    mask: N-D Tensor of zeros or ones.\\n    dim: The dimension over which to compute the maximum.\\n\\n  Returns:\\n    masked_maximums: N-D Tensor.\\n      The maximized dimension is of size 1 after the operation.\\n  '\n    axis_minimums = tf.reduce_min(data, dim, keep_dims=True)\n    masked_maximums = tf.reduce_max(tf.multiply(data - axis_minimums, mask), dim, keep_dims=True) + axis_minimums\n    return masked_maximums"
        ]
    },
    {
        "func_name": "masked_minimum",
        "original": "def masked_minimum(data, mask, dim=1):\n    \"\"\"Computes the axis wise minimum over chosen elements.\n\n  Args:\n    data: 2-D Tensor of size [n, m].\n    mask: 2-D Boolean Tensor of size [n, m].\n    dim: The dimension over which to compute the minimum.\n\n  Returns:\n    masked_minimums: N-D Tensor.\n      The minimized dimension is of size 1 after the operation.\n  \"\"\"\n    axis_maximums = tf.reduce_max(data, dim, keep_dims=True)\n    masked_minimums = tf.reduce_min(tf.multiply(data - axis_maximums, mask), dim, keep_dims=True) + axis_maximums\n    return masked_minimums",
        "mutated": [
            "def masked_minimum(data, mask, dim=1):\n    if False:\n        i = 10\n    'Computes the axis wise minimum over chosen elements.\\n\\n  Args:\\n    data: 2-D Tensor of size [n, m].\\n    mask: 2-D Boolean Tensor of size [n, m].\\n    dim: The dimension over which to compute the minimum.\\n\\n  Returns:\\n    masked_minimums: N-D Tensor.\\n      The minimized dimension is of size 1 after the operation.\\n  '\n    axis_maximums = tf.reduce_max(data, dim, keep_dims=True)\n    masked_minimums = tf.reduce_min(tf.multiply(data - axis_maximums, mask), dim, keep_dims=True) + axis_maximums\n    return masked_minimums",
            "def masked_minimum(data, mask, dim=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes the axis wise minimum over chosen elements.\\n\\n  Args:\\n    data: 2-D Tensor of size [n, m].\\n    mask: 2-D Boolean Tensor of size [n, m].\\n    dim: The dimension over which to compute the minimum.\\n\\n  Returns:\\n    masked_minimums: N-D Tensor.\\n      The minimized dimension is of size 1 after the operation.\\n  '\n    axis_maximums = tf.reduce_max(data, dim, keep_dims=True)\n    masked_minimums = tf.reduce_min(tf.multiply(data - axis_maximums, mask), dim, keep_dims=True) + axis_maximums\n    return masked_minimums",
            "def masked_minimum(data, mask, dim=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes the axis wise minimum over chosen elements.\\n\\n  Args:\\n    data: 2-D Tensor of size [n, m].\\n    mask: 2-D Boolean Tensor of size [n, m].\\n    dim: The dimension over which to compute the minimum.\\n\\n  Returns:\\n    masked_minimums: N-D Tensor.\\n      The minimized dimension is of size 1 after the operation.\\n  '\n    axis_maximums = tf.reduce_max(data, dim, keep_dims=True)\n    masked_minimums = tf.reduce_min(tf.multiply(data - axis_maximums, mask), dim, keep_dims=True) + axis_maximums\n    return masked_minimums",
            "def masked_minimum(data, mask, dim=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes the axis wise minimum over chosen elements.\\n\\n  Args:\\n    data: 2-D Tensor of size [n, m].\\n    mask: 2-D Boolean Tensor of size [n, m].\\n    dim: The dimension over which to compute the minimum.\\n\\n  Returns:\\n    masked_minimums: N-D Tensor.\\n      The minimized dimension is of size 1 after the operation.\\n  '\n    axis_maximums = tf.reduce_max(data, dim, keep_dims=True)\n    masked_minimums = tf.reduce_min(tf.multiply(data - axis_maximums, mask), dim, keep_dims=True) + axis_maximums\n    return masked_minimums",
            "def masked_minimum(data, mask, dim=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes the axis wise minimum over chosen elements.\\n\\n  Args:\\n    data: 2-D Tensor of size [n, m].\\n    mask: 2-D Boolean Tensor of size [n, m].\\n    dim: The dimension over which to compute the minimum.\\n\\n  Returns:\\n    masked_minimums: N-D Tensor.\\n      The minimized dimension is of size 1 after the operation.\\n  '\n    axis_maximums = tf.reduce_max(data, dim, keep_dims=True)\n    masked_minimums = tf.reduce_min(tf.multiply(data - axis_maximums, mask), dim, keep_dims=True) + axis_maximums\n    return masked_minimums"
        ]
    },
    {
        "func_name": "singleview_tcn_loss",
        "original": "def singleview_tcn_loss(embeddings, timesteps, pos_radius, neg_radius, margin=1.0, sequence_ids=None, multiseq=False):\n    \"\"\"Computes the single view triplet loss with semi-hard negative mining.\n\n  The loss encourages the positive distances (between a pair of embeddings with\n  the same labels) to be smaller than the minimum negative distance among\n  which are at least greater than the positive distance plus the margin constant\n  (called semi-hard negative) in the mini-batch. If no such negative exists,\n  uses the largest negative distance instead.\n\n  Anchor, positive, negative selection is as follow:\n  Anchors: We consider every embedding timestep as an anchor.\n  Positives: pos_radius defines a radius (in timesteps) around each anchor from\n    which positives can be drawn. E.g. An anchor with t=10 and a pos_radius of\n    2 produces a set of 4 (anchor,pos) pairs [(a=10, p=8), ... (a=10, p=12)].\n  Negatives: neg_radius defines a boundary (in timesteps) around each anchor,\n    outside of which negatives can be drawn. E.g. An anchor with t=10 and a\n    neg_radius of 4 means negatives can be any t_neg where t_neg < 6 and\n    t_neg > 14.\n\n  Args:\n    embeddings: 2-D Tensor of embedding vectors.\n    timesteps: 1-D Tensor with shape [batch_size, 1] of sequence timesteps.\n    pos_radius: int32; the size of the window (in timesteps) around each anchor\n      timestep that a positive can be drawn from.\n    neg_radius: int32; the size of the window (in timesteps) around each anchor\n      timestep that defines a negative boundary. Negatives can only be chosen\n      where negative timestep t is < negative boundary min or > negative\n      boundary max.\n    margin: Float; the triplet loss margin hyperparameter.\n    sequence_ids: (Optional) 1-D Tensor with shape [batch_size, 1] of sequence\n      ids. Together (sequence_id, sequence_timestep) give us a unique index for\n      each image if we have multiple sequences in a batch.\n    multiseq: Boolean, whether or not the batch is composed of multiple\n      sequences (with possibly colliding timesteps).\n\n  Returns:\n    triplet_loss: tf.float32 scalar.\n  \"\"\"\n    assert neg_radius > pos_radius\n    tshape = tf.shape(timesteps)\n    assert tshape.shape == 2 or tshape.shape == 1\n    if tshape.shape == 1:\n        timesteps = tf.reshape(timesteps, [tshape[0], 1])\n    pdist_matrix = pairwise_squared_distance(embeddings)\n    pos_radius = tf.cast(pos_radius, tf.int32)\n    if multiseq:\n        tshape = tf.shape(sequence_ids)\n        assert tshape.shape == 2 or tshape.shape == 1\n        if tshape.shape == 1:\n            sequence_ids = tf.reshape(sequence_ids, [tshape[0], 1])\n        sequence_adjacency = tf.equal(sequence_ids, tf.transpose(sequence_ids))\n        sequence_adjacency_not = tf.logical_not(sequence_adjacency)\n        in_pos_range = tf.logical_and(tf.less_equal(tf.abs(timesteps - tf.transpose(timesteps)), pos_radius), sequence_adjacency)\n        in_neg_range = tf.logical_or(tf.greater(tf.abs(timesteps - tf.transpose(timesteps)), neg_radius), sequence_adjacency_not)\n    else:\n        in_pos_range = tf.less_equal(tf.abs(timesteps - tf.transpose(timesteps)), pos_radius)\n        in_neg_range = tf.greater(tf.abs(timesteps - tf.transpose(timesteps)), neg_radius)\n    batch_size = tf.size(timesteps)\n    pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n    mask = tf.logical_and(tf.tile(in_neg_range, [batch_size, 1]), tf.greater(pdist_matrix_tile, tf.reshape(tf.transpose(pdist_matrix), [-1, 1])))\n    mask_final = tf.reshape(tf.greater(tf.reduce_sum(tf.cast(mask, dtype=tf.float32), 1, keep_dims=True), 0.0), [batch_size, batch_size])\n    mask_final = tf.transpose(mask_final)\n    in_neg_range = tf.cast(in_neg_range, dtype=tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    negatives_outside = tf.reshape(masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n    negatives_outside = tf.transpose(negatives_outside)\n    negatives_inside = tf.tile(masked_maximum(pdist_matrix, in_neg_range), [1, batch_size])\n    semi_hard_negatives = tf.where(mask_final, negatives_outside, negatives_inside)\n    loss_mat = tf.add(margin, pdist_matrix - semi_hard_negatives)\n    mask_positives = tf.cast(in_pos_range, dtype=tf.float32) - tf.diag(tf.ones([batch_size]))\n    num_positives = tf.reduce_sum(mask_positives)\n    triplet_loss = tf.truediv(tf.reduce_sum(tf.maximum(tf.multiply(loss_mat, mask_positives), 0.0)), num_positives, name='triplet_svtcn_loss')\n    return triplet_loss",
        "mutated": [
            "def singleview_tcn_loss(embeddings, timesteps, pos_radius, neg_radius, margin=1.0, sequence_ids=None, multiseq=False):\n    if False:\n        i = 10\n    'Computes the single view triplet loss with semi-hard negative mining.\\n\\n  The loss encourages the positive distances (between a pair of embeddings with\\n  the same labels) to be smaller than the minimum negative distance among\\n  which are at least greater than the positive distance plus the margin constant\\n  (called semi-hard negative) in the mini-batch. If no such negative exists,\\n  uses the largest negative distance instead.\\n\\n  Anchor, positive, negative selection is as follow:\\n  Anchors: We consider every embedding timestep as an anchor.\\n  Positives: pos_radius defines a radius (in timesteps) around each anchor from\\n    which positives can be drawn. E.g. An anchor with t=10 and a pos_radius of\\n    2 produces a set of 4 (anchor,pos) pairs [(a=10, p=8), ... (a=10, p=12)].\\n  Negatives: neg_radius defines a boundary (in timesteps) around each anchor,\\n    outside of which negatives can be drawn. E.g. An anchor with t=10 and a\\n    neg_radius of 4 means negatives can be any t_neg where t_neg < 6 and\\n    t_neg > 14.\\n\\n  Args:\\n    embeddings: 2-D Tensor of embedding vectors.\\n    timesteps: 1-D Tensor with shape [batch_size, 1] of sequence timesteps.\\n    pos_radius: int32; the size of the window (in timesteps) around each anchor\\n      timestep that a positive can be drawn from.\\n    neg_radius: int32; the size of the window (in timesteps) around each anchor\\n      timestep that defines a negative boundary. Negatives can only be chosen\\n      where negative timestep t is < negative boundary min or > negative\\n      boundary max.\\n    margin: Float; the triplet loss margin hyperparameter.\\n    sequence_ids: (Optional) 1-D Tensor with shape [batch_size, 1] of sequence\\n      ids. Together (sequence_id, sequence_timestep) give us a unique index for\\n      each image if we have multiple sequences in a batch.\\n    multiseq: Boolean, whether or not the batch is composed of multiple\\n      sequences (with possibly colliding timesteps).\\n\\n  Returns:\\n    triplet_loss: tf.float32 scalar.\\n  '\n    assert neg_radius > pos_radius\n    tshape = tf.shape(timesteps)\n    assert tshape.shape == 2 or tshape.shape == 1\n    if tshape.shape == 1:\n        timesteps = tf.reshape(timesteps, [tshape[0], 1])\n    pdist_matrix = pairwise_squared_distance(embeddings)\n    pos_radius = tf.cast(pos_radius, tf.int32)\n    if multiseq:\n        tshape = tf.shape(sequence_ids)\n        assert tshape.shape == 2 or tshape.shape == 1\n        if tshape.shape == 1:\n            sequence_ids = tf.reshape(sequence_ids, [tshape[0], 1])\n        sequence_adjacency = tf.equal(sequence_ids, tf.transpose(sequence_ids))\n        sequence_adjacency_not = tf.logical_not(sequence_adjacency)\n        in_pos_range = tf.logical_and(tf.less_equal(tf.abs(timesteps - tf.transpose(timesteps)), pos_radius), sequence_adjacency)\n        in_neg_range = tf.logical_or(tf.greater(tf.abs(timesteps - tf.transpose(timesteps)), neg_radius), sequence_adjacency_not)\n    else:\n        in_pos_range = tf.less_equal(tf.abs(timesteps - tf.transpose(timesteps)), pos_radius)\n        in_neg_range = tf.greater(tf.abs(timesteps - tf.transpose(timesteps)), neg_radius)\n    batch_size = tf.size(timesteps)\n    pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n    mask = tf.logical_and(tf.tile(in_neg_range, [batch_size, 1]), tf.greater(pdist_matrix_tile, tf.reshape(tf.transpose(pdist_matrix), [-1, 1])))\n    mask_final = tf.reshape(tf.greater(tf.reduce_sum(tf.cast(mask, dtype=tf.float32), 1, keep_dims=True), 0.0), [batch_size, batch_size])\n    mask_final = tf.transpose(mask_final)\n    in_neg_range = tf.cast(in_neg_range, dtype=tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    negatives_outside = tf.reshape(masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n    negatives_outside = tf.transpose(negatives_outside)\n    negatives_inside = tf.tile(masked_maximum(pdist_matrix, in_neg_range), [1, batch_size])\n    semi_hard_negatives = tf.where(mask_final, negatives_outside, negatives_inside)\n    loss_mat = tf.add(margin, pdist_matrix - semi_hard_negatives)\n    mask_positives = tf.cast(in_pos_range, dtype=tf.float32) - tf.diag(tf.ones([batch_size]))\n    num_positives = tf.reduce_sum(mask_positives)\n    triplet_loss = tf.truediv(tf.reduce_sum(tf.maximum(tf.multiply(loss_mat, mask_positives), 0.0)), num_positives, name='triplet_svtcn_loss')\n    return triplet_loss",
            "def singleview_tcn_loss(embeddings, timesteps, pos_radius, neg_radius, margin=1.0, sequence_ids=None, multiseq=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes the single view triplet loss with semi-hard negative mining.\\n\\n  The loss encourages the positive distances (between a pair of embeddings with\\n  the same labels) to be smaller than the minimum negative distance among\\n  which are at least greater than the positive distance plus the margin constant\\n  (called semi-hard negative) in the mini-batch. If no such negative exists,\\n  uses the largest negative distance instead.\\n\\n  Anchor, positive, negative selection is as follow:\\n  Anchors: We consider every embedding timestep as an anchor.\\n  Positives: pos_radius defines a radius (in timesteps) around each anchor from\\n    which positives can be drawn. E.g. An anchor with t=10 and a pos_radius of\\n    2 produces a set of 4 (anchor,pos) pairs [(a=10, p=8), ... (a=10, p=12)].\\n  Negatives: neg_radius defines a boundary (in timesteps) around each anchor,\\n    outside of which negatives can be drawn. E.g. An anchor with t=10 and a\\n    neg_radius of 4 means negatives can be any t_neg where t_neg < 6 and\\n    t_neg > 14.\\n\\n  Args:\\n    embeddings: 2-D Tensor of embedding vectors.\\n    timesteps: 1-D Tensor with shape [batch_size, 1] of sequence timesteps.\\n    pos_radius: int32; the size of the window (in timesteps) around each anchor\\n      timestep that a positive can be drawn from.\\n    neg_radius: int32; the size of the window (in timesteps) around each anchor\\n      timestep that defines a negative boundary. Negatives can only be chosen\\n      where negative timestep t is < negative boundary min or > negative\\n      boundary max.\\n    margin: Float; the triplet loss margin hyperparameter.\\n    sequence_ids: (Optional) 1-D Tensor with shape [batch_size, 1] of sequence\\n      ids. Together (sequence_id, sequence_timestep) give us a unique index for\\n      each image if we have multiple sequences in a batch.\\n    multiseq: Boolean, whether or not the batch is composed of multiple\\n      sequences (with possibly colliding timesteps).\\n\\n  Returns:\\n    triplet_loss: tf.float32 scalar.\\n  '\n    assert neg_radius > pos_radius\n    tshape = tf.shape(timesteps)\n    assert tshape.shape == 2 or tshape.shape == 1\n    if tshape.shape == 1:\n        timesteps = tf.reshape(timesteps, [tshape[0], 1])\n    pdist_matrix = pairwise_squared_distance(embeddings)\n    pos_radius = tf.cast(pos_radius, tf.int32)\n    if multiseq:\n        tshape = tf.shape(sequence_ids)\n        assert tshape.shape == 2 or tshape.shape == 1\n        if tshape.shape == 1:\n            sequence_ids = tf.reshape(sequence_ids, [tshape[0], 1])\n        sequence_adjacency = tf.equal(sequence_ids, tf.transpose(sequence_ids))\n        sequence_adjacency_not = tf.logical_not(sequence_adjacency)\n        in_pos_range = tf.logical_and(tf.less_equal(tf.abs(timesteps - tf.transpose(timesteps)), pos_radius), sequence_adjacency)\n        in_neg_range = tf.logical_or(tf.greater(tf.abs(timesteps - tf.transpose(timesteps)), neg_radius), sequence_adjacency_not)\n    else:\n        in_pos_range = tf.less_equal(tf.abs(timesteps - tf.transpose(timesteps)), pos_radius)\n        in_neg_range = tf.greater(tf.abs(timesteps - tf.transpose(timesteps)), neg_radius)\n    batch_size = tf.size(timesteps)\n    pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n    mask = tf.logical_and(tf.tile(in_neg_range, [batch_size, 1]), tf.greater(pdist_matrix_tile, tf.reshape(tf.transpose(pdist_matrix), [-1, 1])))\n    mask_final = tf.reshape(tf.greater(tf.reduce_sum(tf.cast(mask, dtype=tf.float32), 1, keep_dims=True), 0.0), [batch_size, batch_size])\n    mask_final = tf.transpose(mask_final)\n    in_neg_range = tf.cast(in_neg_range, dtype=tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    negatives_outside = tf.reshape(masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n    negatives_outside = tf.transpose(negatives_outside)\n    negatives_inside = tf.tile(masked_maximum(pdist_matrix, in_neg_range), [1, batch_size])\n    semi_hard_negatives = tf.where(mask_final, negatives_outside, negatives_inside)\n    loss_mat = tf.add(margin, pdist_matrix - semi_hard_negatives)\n    mask_positives = tf.cast(in_pos_range, dtype=tf.float32) - tf.diag(tf.ones([batch_size]))\n    num_positives = tf.reduce_sum(mask_positives)\n    triplet_loss = tf.truediv(tf.reduce_sum(tf.maximum(tf.multiply(loss_mat, mask_positives), 0.0)), num_positives, name='triplet_svtcn_loss')\n    return triplet_loss",
            "def singleview_tcn_loss(embeddings, timesteps, pos_radius, neg_radius, margin=1.0, sequence_ids=None, multiseq=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes the single view triplet loss with semi-hard negative mining.\\n\\n  The loss encourages the positive distances (between a pair of embeddings with\\n  the same labels) to be smaller than the minimum negative distance among\\n  which are at least greater than the positive distance plus the margin constant\\n  (called semi-hard negative) in the mini-batch. If no such negative exists,\\n  uses the largest negative distance instead.\\n\\n  Anchor, positive, negative selection is as follow:\\n  Anchors: We consider every embedding timestep as an anchor.\\n  Positives: pos_radius defines a radius (in timesteps) around each anchor from\\n    which positives can be drawn. E.g. An anchor with t=10 and a pos_radius of\\n    2 produces a set of 4 (anchor,pos) pairs [(a=10, p=8), ... (a=10, p=12)].\\n  Negatives: neg_radius defines a boundary (in timesteps) around each anchor,\\n    outside of which negatives can be drawn. E.g. An anchor with t=10 and a\\n    neg_radius of 4 means negatives can be any t_neg where t_neg < 6 and\\n    t_neg > 14.\\n\\n  Args:\\n    embeddings: 2-D Tensor of embedding vectors.\\n    timesteps: 1-D Tensor with shape [batch_size, 1] of sequence timesteps.\\n    pos_radius: int32; the size of the window (in timesteps) around each anchor\\n      timestep that a positive can be drawn from.\\n    neg_radius: int32; the size of the window (in timesteps) around each anchor\\n      timestep that defines a negative boundary. Negatives can only be chosen\\n      where negative timestep t is < negative boundary min or > negative\\n      boundary max.\\n    margin: Float; the triplet loss margin hyperparameter.\\n    sequence_ids: (Optional) 1-D Tensor with shape [batch_size, 1] of sequence\\n      ids. Together (sequence_id, sequence_timestep) give us a unique index for\\n      each image if we have multiple sequences in a batch.\\n    multiseq: Boolean, whether or not the batch is composed of multiple\\n      sequences (with possibly colliding timesteps).\\n\\n  Returns:\\n    triplet_loss: tf.float32 scalar.\\n  '\n    assert neg_radius > pos_radius\n    tshape = tf.shape(timesteps)\n    assert tshape.shape == 2 or tshape.shape == 1\n    if tshape.shape == 1:\n        timesteps = tf.reshape(timesteps, [tshape[0], 1])\n    pdist_matrix = pairwise_squared_distance(embeddings)\n    pos_radius = tf.cast(pos_radius, tf.int32)\n    if multiseq:\n        tshape = tf.shape(sequence_ids)\n        assert tshape.shape == 2 or tshape.shape == 1\n        if tshape.shape == 1:\n            sequence_ids = tf.reshape(sequence_ids, [tshape[0], 1])\n        sequence_adjacency = tf.equal(sequence_ids, tf.transpose(sequence_ids))\n        sequence_adjacency_not = tf.logical_not(sequence_adjacency)\n        in_pos_range = tf.logical_and(tf.less_equal(tf.abs(timesteps - tf.transpose(timesteps)), pos_radius), sequence_adjacency)\n        in_neg_range = tf.logical_or(tf.greater(tf.abs(timesteps - tf.transpose(timesteps)), neg_radius), sequence_adjacency_not)\n    else:\n        in_pos_range = tf.less_equal(tf.abs(timesteps - tf.transpose(timesteps)), pos_radius)\n        in_neg_range = tf.greater(tf.abs(timesteps - tf.transpose(timesteps)), neg_radius)\n    batch_size = tf.size(timesteps)\n    pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n    mask = tf.logical_and(tf.tile(in_neg_range, [batch_size, 1]), tf.greater(pdist_matrix_tile, tf.reshape(tf.transpose(pdist_matrix), [-1, 1])))\n    mask_final = tf.reshape(tf.greater(tf.reduce_sum(tf.cast(mask, dtype=tf.float32), 1, keep_dims=True), 0.0), [batch_size, batch_size])\n    mask_final = tf.transpose(mask_final)\n    in_neg_range = tf.cast(in_neg_range, dtype=tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    negatives_outside = tf.reshape(masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n    negatives_outside = tf.transpose(negatives_outside)\n    negatives_inside = tf.tile(masked_maximum(pdist_matrix, in_neg_range), [1, batch_size])\n    semi_hard_negatives = tf.where(mask_final, negatives_outside, negatives_inside)\n    loss_mat = tf.add(margin, pdist_matrix - semi_hard_negatives)\n    mask_positives = tf.cast(in_pos_range, dtype=tf.float32) - tf.diag(tf.ones([batch_size]))\n    num_positives = tf.reduce_sum(mask_positives)\n    triplet_loss = tf.truediv(tf.reduce_sum(tf.maximum(tf.multiply(loss_mat, mask_positives), 0.0)), num_positives, name='triplet_svtcn_loss')\n    return triplet_loss",
            "def singleview_tcn_loss(embeddings, timesteps, pos_radius, neg_radius, margin=1.0, sequence_ids=None, multiseq=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes the single view triplet loss with semi-hard negative mining.\\n\\n  The loss encourages the positive distances (between a pair of embeddings with\\n  the same labels) to be smaller than the minimum negative distance among\\n  which are at least greater than the positive distance plus the margin constant\\n  (called semi-hard negative) in the mini-batch. If no such negative exists,\\n  uses the largest negative distance instead.\\n\\n  Anchor, positive, negative selection is as follow:\\n  Anchors: We consider every embedding timestep as an anchor.\\n  Positives: pos_radius defines a radius (in timesteps) around each anchor from\\n    which positives can be drawn. E.g. An anchor with t=10 and a pos_radius of\\n    2 produces a set of 4 (anchor,pos) pairs [(a=10, p=8), ... (a=10, p=12)].\\n  Negatives: neg_radius defines a boundary (in timesteps) around each anchor,\\n    outside of which negatives can be drawn. E.g. An anchor with t=10 and a\\n    neg_radius of 4 means negatives can be any t_neg where t_neg < 6 and\\n    t_neg > 14.\\n\\n  Args:\\n    embeddings: 2-D Tensor of embedding vectors.\\n    timesteps: 1-D Tensor with shape [batch_size, 1] of sequence timesteps.\\n    pos_radius: int32; the size of the window (in timesteps) around each anchor\\n      timestep that a positive can be drawn from.\\n    neg_radius: int32; the size of the window (in timesteps) around each anchor\\n      timestep that defines a negative boundary. Negatives can only be chosen\\n      where negative timestep t is < negative boundary min or > negative\\n      boundary max.\\n    margin: Float; the triplet loss margin hyperparameter.\\n    sequence_ids: (Optional) 1-D Tensor with shape [batch_size, 1] of sequence\\n      ids. Together (sequence_id, sequence_timestep) give us a unique index for\\n      each image if we have multiple sequences in a batch.\\n    multiseq: Boolean, whether or not the batch is composed of multiple\\n      sequences (with possibly colliding timesteps).\\n\\n  Returns:\\n    triplet_loss: tf.float32 scalar.\\n  '\n    assert neg_radius > pos_radius\n    tshape = tf.shape(timesteps)\n    assert tshape.shape == 2 or tshape.shape == 1\n    if tshape.shape == 1:\n        timesteps = tf.reshape(timesteps, [tshape[0], 1])\n    pdist_matrix = pairwise_squared_distance(embeddings)\n    pos_radius = tf.cast(pos_radius, tf.int32)\n    if multiseq:\n        tshape = tf.shape(sequence_ids)\n        assert tshape.shape == 2 or tshape.shape == 1\n        if tshape.shape == 1:\n            sequence_ids = tf.reshape(sequence_ids, [tshape[0], 1])\n        sequence_adjacency = tf.equal(sequence_ids, tf.transpose(sequence_ids))\n        sequence_adjacency_not = tf.logical_not(sequence_adjacency)\n        in_pos_range = tf.logical_and(tf.less_equal(tf.abs(timesteps - tf.transpose(timesteps)), pos_radius), sequence_adjacency)\n        in_neg_range = tf.logical_or(tf.greater(tf.abs(timesteps - tf.transpose(timesteps)), neg_radius), sequence_adjacency_not)\n    else:\n        in_pos_range = tf.less_equal(tf.abs(timesteps - tf.transpose(timesteps)), pos_radius)\n        in_neg_range = tf.greater(tf.abs(timesteps - tf.transpose(timesteps)), neg_radius)\n    batch_size = tf.size(timesteps)\n    pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n    mask = tf.logical_and(tf.tile(in_neg_range, [batch_size, 1]), tf.greater(pdist_matrix_tile, tf.reshape(tf.transpose(pdist_matrix), [-1, 1])))\n    mask_final = tf.reshape(tf.greater(tf.reduce_sum(tf.cast(mask, dtype=tf.float32), 1, keep_dims=True), 0.0), [batch_size, batch_size])\n    mask_final = tf.transpose(mask_final)\n    in_neg_range = tf.cast(in_neg_range, dtype=tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    negatives_outside = tf.reshape(masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n    negatives_outside = tf.transpose(negatives_outside)\n    negatives_inside = tf.tile(masked_maximum(pdist_matrix, in_neg_range), [1, batch_size])\n    semi_hard_negatives = tf.where(mask_final, negatives_outside, negatives_inside)\n    loss_mat = tf.add(margin, pdist_matrix - semi_hard_negatives)\n    mask_positives = tf.cast(in_pos_range, dtype=tf.float32) - tf.diag(tf.ones([batch_size]))\n    num_positives = tf.reduce_sum(mask_positives)\n    triplet_loss = tf.truediv(tf.reduce_sum(tf.maximum(tf.multiply(loss_mat, mask_positives), 0.0)), num_positives, name='triplet_svtcn_loss')\n    return triplet_loss",
            "def singleview_tcn_loss(embeddings, timesteps, pos_radius, neg_radius, margin=1.0, sequence_ids=None, multiseq=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes the single view triplet loss with semi-hard negative mining.\\n\\n  The loss encourages the positive distances (between a pair of embeddings with\\n  the same labels) to be smaller than the minimum negative distance among\\n  which are at least greater than the positive distance plus the margin constant\\n  (called semi-hard negative) in the mini-batch. If no such negative exists,\\n  uses the largest negative distance instead.\\n\\n  Anchor, positive, negative selection is as follow:\\n  Anchors: We consider every embedding timestep as an anchor.\\n  Positives: pos_radius defines a radius (in timesteps) around each anchor from\\n    which positives can be drawn. E.g. An anchor with t=10 and a pos_radius of\\n    2 produces a set of 4 (anchor,pos) pairs [(a=10, p=8), ... (a=10, p=12)].\\n  Negatives: neg_radius defines a boundary (in timesteps) around each anchor,\\n    outside of which negatives can be drawn. E.g. An anchor with t=10 and a\\n    neg_radius of 4 means negatives can be any t_neg where t_neg < 6 and\\n    t_neg > 14.\\n\\n  Args:\\n    embeddings: 2-D Tensor of embedding vectors.\\n    timesteps: 1-D Tensor with shape [batch_size, 1] of sequence timesteps.\\n    pos_radius: int32; the size of the window (in timesteps) around each anchor\\n      timestep that a positive can be drawn from.\\n    neg_radius: int32; the size of the window (in timesteps) around each anchor\\n      timestep that defines a negative boundary. Negatives can only be chosen\\n      where negative timestep t is < negative boundary min or > negative\\n      boundary max.\\n    margin: Float; the triplet loss margin hyperparameter.\\n    sequence_ids: (Optional) 1-D Tensor with shape [batch_size, 1] of sequence\\n      ids. Together (sequence_id, sequence_timestep) give us a unique index for\\n      each image if we have multiple sequences in a batch.\\n    multiseq: Boolean, whether or not the batch is composed of multiple\\n      sequences (with possibly colliding timesteps).\\n\\n  Returns:\\n    triplet_loss: tf.float32 scalar.\\n  '\n    assert neg_radius > pos_radius\n    tshape = tf.shape(timesteps)\n    assert tshape.shape == 2 or tshape.shape == 1\n    if tshape.shape == 1:\n        timesteps = tf.reshape(timesteps, [tshape[0], 1])\n    pdist_matrix = pairwise_squared_distance(embeddings)\n    pos_radius = tf.cast(pos_radius, tf.int32)\n    if multiseq:\n        tshape = tf.shape(sequence_ids)\n        assert tshape.shape == 2 or tshape.shape == 1\n        if tshape.shape == 1:\n            sequence_ids = tf.reshape(sequence_ids, [tshape[0], 1])\n        sequence_adjacency = tf.equal(sequence_ids, tf.transpose(sequence_ids))\n        sequence_adjacency_not = tf.logical_not(sequence_adjacency)\n        in_pos_range = tf.logical_and(tf.less_equal(tf.abs(timesteps - tf.transpose(timesteps)), pos_radius), sequence_adjacency)\n        in_neg_range = tf.logical_or(tf.greater(tf.abs(timesteps - tf.transpose(timesteps)), neg_radius), sequence_adjacency_not)\n    else:\n        in_pos_range = tf.less_equal(tf.abs(timesteps - tf.transpose(timesteps)), pos_radius)\n        in_neg_range = tf.greater(tf.abs(timesteps - tf.transpose(timesteps)), neg_radius)\n    batch_size = tf.size(timesteps)\n    pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n    mask = tf.logical_and(tf.tile(in_neg_range, [batch_size, 1]), tf.greater(pdist_matrix_tile, tf.reshape(tf.transpose(pdist_matrix), [-1, 1])))\n    mask_final = tf.reshape(tf.greater(tf.reduce_sum(tf.cast(mask, dtype=tf.float32), 1, keep_dims=True), 0.0), [batch_size, batch_size])\n    mask_final = tf.transpose(mask_final)\n    in_neg_range = tf.cast(in_neg_range, dtype=tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    negatives_outside = tf.reshape(masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n    negatives_outside = tf.transpose(negatives_outside)\n    negatives_inside = tf.tile(masked_maximum(pdist_matrix, in_neg_range), [1, batch_size])\n    semi_hard_negatives = tf.where(mask_final, negatives_outside, negatives_inside)\n    loss_mat = tf.add(margin, pdist_matrix - semi_hard_negatives)\n    mask_positives = tf.cast(in_pos_range, dtype=tf.float32) - tf.diag(tf.ones([batch_size]))\n    num_positives = tf.reduce_sum(mask_positives)\n    triplet_loss = tf.truediv(tf.reduce_sum(tf.maximum(tf.multiply(loss_mat, mask_positives), 0.0)), num_positives, name='triplet_svtcn_loss')\n    return triplet_loss"
        ]
    }
]