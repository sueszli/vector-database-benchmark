[
    {
        "func_name": "smooth_moving_avg",
        "original": "def smooth_moving_avg(signal, window=11):\n    window = int(window)\n    if signal.ndim != 1:\n        raise ValueError('')\n    if signal.size < window:\n        raise ValueError('Input vector needs to be bigger than window size.')\n    if window < 3:\n        return signal\n    s = np.r_[2 * signal[0] - signal[window - 1::-1], signal, 2 * signal[-1] - signal[-1:-window:-1]]\n    w = np.ones(window, 'd')\n    y = np.convolve(w / w.sum(), s, mode='same')\n    return y[window:-window + 1]",
        "mutated": [
            "def smooth_moving_avg(signal, window=11):\n    if False:\n        i = 10\n    window = int(window)\n    if signal.ndim != 1:\n        raise ValueError('')\n    if signal.size < window:\n        raise ValueError('Input vector needs to be bigger than window size.')\n    if window < 3:\n        return signal\n    s = np.r_[2 * signal[0] - signal[window - 1::-1], signal, 2 * signal[-1] - signal[-1:-window:-1]]\n    w = np.ones(window, 'd')\n    y = np.convolve(w / w.sum(), s, mode='same')\n    return y[window:-window + 1]",
            "def smooth_moving_avg(signal, window=11):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    window = int(window)\n    if signal.ndim != 1:\n        raise ValueError('')\n    if signal.size < window:\n        raise ValueError('Input vector needs to be bigger than window size.')\n    if window < 3:\n        return signal\n    s = np.r_[2 * signal[0] - signal[window - 1::-1], signal, 2 * signal[-1] - signal[-1:-window:-1]]\n    w = np.ones(window, 'd')\n    y = np.convolve(w / w.sum(), s, mode='same')\n    return y[window:-window + 1]",
            "def smooth_moving_avg(signal, window=11):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    window = int(window)\n    if signal.ndim != 1:\n        raise ValueError('')\n    if signal.size < window:\n        raise ValueError('Input vector needs to be bigger than window size.')\n    if window < 3:\n        return signal\n    s = np.r_[2 * signal[0] - signal[window - 1::-1], signal, 2 * signal[-1] - signal[-1:-window:-1]]\n    w = np.ones(window, 'd')\n    y = np.convolve(w / w.sum(), s, mode='same')\n    return y[window:-window + 1]",
            "def smooth_moving_avg(signal, window=11):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    window = int(window)\n    if signal.ndim != 1:\n        raise ValueError('')\n    if signal.size < window:\n        raise ValueError('Input vector needs to be bigger than window size.')\n    if window < 3:\n        return signal\n    s = np.r_[2 * signal[0] - signal[window - 1::-1], signal, 2 * signal[-1] - signal[-1:-window:-1]]\n    w = np.ones(window, 'd')\n    y = np.convolve(w / w.sum(), s, mode='same')\n    return y[window:-window + 1]",
            "def smooth_moving_avg(signal, window=11):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    window = int(window)\n    if signal.ndim != 1:\n        raise ValueError('')\n    if signal.size < window:\n        raise ValueError('Input vector needs to be bigger than window size.')\n    if window < 3:\n        return signal\n    s = np.r_[2 * signal[0] - signal[window - 1::-1], signal, 2 * signal[-1] - signal[-1:-window:-1]]\n    w = np.ones(window, 'd')\n    y = np.convolve(w / w.sum(), s, mode='same')\n    return y[window:-window + 1]"
        ]
    },
    {
        "func_name": "self_similarity_matrix",
        "original": "def self_similarity_matrix(feature_vectors):\n    \"\"\"\n    This function computes the self-similarity matrix for a sequence\n    of feature vectors.\n    ARGUMENTS:\n     - feature_vectors:    a np matrix (nDims x nVectors) whose i-th column\n                           corresponds to the i-th feature vector\n\n    RETURNS:\n     - sim_matrix:         the self-similarity matrix (nVectors x nVectors)\n    \"\"\"\n    scaler = StandardScaler()\n    norm_feature_vectors = scaler.fit_transform(feature_vectors.T).T\n    sim_matrix = 1.0 - distance.squareform(distance.pdist(norm_feature_vectors.T, 'cosine'))\n    return sim_matrix",
        "mutated": [
            "def self_similarity_matrix(feature_vectors):\n    if False:\n        i = 10\n    '\\n    This function computes the self-similarity matrix for a sequence\\n    of feature vectors.\\n    ARGUMENTS:\\n     - feature_vectors:    a np matrix (nDims x nVectors) whose i-th column\\n                           corresponds to the i-th feature vector\\n\\n    RETURNS:\\n     - sim_matrix:         the self-similarity matrix (nVectors x nVectors)\\n    '\n    scaler = StandardScaler()\n    norm_feature_vectors = scaler.fit_transform(feature_vectors.T).T\n    sim_matrix = 1.0 - distance.squareform(distance.pdist(norm_feature_vectors.T, 'cosine'))\n    return sim_matrix",
            "def self_similarity_matrix(feature_vectors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    This function computes the self-similarity matrix for a sequence\\n    of feature vectors.\\n    ARGUMENTS:\\n     - feature_vectors:    a np matrix (nDims x nVectors) whose i-th column\\n                           corresponds to the i-th feature vector\\n\\n    RETURNS:\\n     - sim_matrix:         the self-similarity matrix (nVectors x nVectors)\\n    '\n    scaler = StandardScaler()\n    norm_feature_vectors = scaler.fit_transform(feature_vectors.T).T\n    sim_matrix = 1.0 - distance.squareform(distance.pdist(norm_feature_vectors.T, 'cosine'))\n    return sim_matrix",
            "def self_similarity_matrix(feature_vectors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    This function computes the self-similarity matrix for a sequence\\n    of feature vectors.\\n    ARGUMENTS:\\n     - feature_vectors:    a np matrix (nDims x nVectors) whose i-th column\\n                           corresponds to the i-th feature vector\\n\\n    RETURNS:\\n     - sim_matrix:         the self-similarity matrix (nVectors x nVectors)\\n    '\n    scaler = StandardScaler()\n    norm_feature_vectors = scaler.fit_transform(feature_vectors.T).T\n    sim_matrix = 1.0 - distance.squareform(distance.pdist(norm_feature_vectors.T, 'cosine'))\n    return sim_matrix",
            "def self_similarity_matrix(feature_vectors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    This function computes the self-similarity matrix for a sequence\\n    of feature vectors.\\n    ARGUMENTS:\\n     - feature_vectors:    a np matrix (nDims x nVectors) whose i-th column\\n                           corresponds to the i-th feature vector\\n\\n    RETURNS:\\n     - sim_matrix:         the self-similarity matrix (nVectors x nVectors)\\n    '\n    scaler = StandardScaler()\n    norm_feature_vectors = scaler.fit_transform(feature_vectors.T).T\n    sim_matrix = 1.0 - distance.squareform(distance.pdist(norm_feature_vectors.T, 'cosine'))\n    return sim_matrix",
            "def self_similarity_matrix(feature_vectors):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    This function computes the self-similarity matrix for a sequence\\n    of feature vectors.\\n    ARGUMENTS:\\n     - feature_vectors:    a np matrix (nDims x nVectors) whose i-th column\\n                           corresponds to the i-th feature vector\\n\\n    RETURNS:\\n     - sim_matrix:         the self-similarity matrix (nVectors x nVectors)\\n    '\n    scaler = StandardScaler()\n    norm_feature_vectors = scaler.fit_transform(feature_vectors.T).T\n    sim_matrix = 1.0 - distance.squareform(distance.pdist(norm_feature_vectors.T, 'cosine'))\n    return sim_matrix"
        ]
    },
    {
        "func_name": "labels_to_segments",
        "original": "def labels_to_segments(labels, window):\n    \"\"\"\n    ARGUMENTS:\n     - labels:     a sequence of class labels (per time window)\n     - window:     window duration (in seconds)\n\n    RETURNS:\n     - segments:   a sequence of segment's limits: segs[i,0] is start and\n                   segs[i,1] are start and end point of segment i\n     - classes:    a sequence of class flags: class[i] is the class ID of\n                   the i-th segment\n    \"\"\"\n    if len(labels) == 1:\n        segs = [0, window]\n        classes = labels\n        return (segs, classes)\n    num_segs = 0\n    index = 0\n    classes = []\n    segment_list = []\n    cur_label = labels[index]\n    while index < len(labels) - 1:\n        previous_value = cur_label\n        while True:\n            index += 1\n            compare_flag = labels[index]\n            if (compare_flag != cur_label) | (index == len(labels) - 1):\n                num_segs += 1\n                cur_label = labels[index]\n                segment_list.append(index * window)\n                classes.append(previous_value)\n                break\n    segments = np.zeros((len(segment_list), 2))\n    for i in range(len(segment_list)):\n        if i > 0:\n            segments[i, 0] = segment_list[i - 1]\n        segments[i, 1] = segment_list[i]\n    return (segments, classes)",
        "mutated": [
            "def labels_to_segments(labels, window):\n    if False:\n        i = 10\n    \"\\n    ARGUMENTS:\\n     - labels:     a sequence of class labels (per time window)\\n     - window:     window duration (in seconds)\\n\\n    RETURNS:\\n     - segments:   a sequence of segment's limits: segs[i,0] is start and\\n                   segs[i,1] are start and end point of segment i\\n     - classes:    a sequence of class flags: class[i] is the class ID of\\n                   the i-th segment\\n    \"\n    if len(labels) == 1:\n        segs = [0, window]\n        classes = labels\n        return (segs, classes)\n    num_segs = 0\n    index = 0\n    classes = []\n    segment_list = []\n    cur_label = labels[index]\n    while index < len(labels) - 1:\n        previous_value = cur_label\n        while True:\n            index += 1\n            compare_flag = labels[index]\n            if (compare_flag != cur_label) | (index == len(labels) - 1):\n                num_segs += 1\n                cur_label = labels[index]\n                segment_list.append(index * window)\n                classes.append(previous_value)\n                break\n    segments = np.zeros((len(segment_list), 2))\n    for i in range(len(segment_list)):\n        if i > 0:\n            segments[i, 0] = segment_list[i - 1]\n        segments[i, 1] = segment_list[i]\n    return (segments, classes)",
            "def labels_to_segments(labels, window):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    ARGUMENTS:\\n     - labels:     a sequence of class labels (per time window)\\n     - window:     window duration (in seconds)\\n\\n    RETURNS:\\n     - segments:   a sequence of segment's limits: segs[i,0] is start and\\n                   segs[i,1] are start and end point of segment i\\n     - classes:    a sequence of class flags: class[i] is the class ID of\\n                   the i-th segment\\n    \"\n    if len(labels) == 1:\n        segs = [0, window]\n        classes = labels\n        return (segs, classes)\n    num_segs = 0\n    index = 0\n    classes = []\n    segment_list = []\n    cur_label = labels[index]\n    while index < len(labels) - 1:\n        previous_value = cur_label\n        while True:\n            index += 1\n            compare_flag = labels[index]\n            if (compare_flag != cur_label) | (index == len(labels) - 1):\n                num_segs += 1\n                cur_label = labels[index]\n                segment_list.append(index * window)\n                classes.append(previous_value)\n                break\n    segments = np.zeros((len(segment_list), 2))\n    for i in range(len(segment_list)):\n        if i > 0:\n            segments[i, 0] = segment_list[i - 1]\n        segments[i, 1] = segment_list[i]\n    return (segments, classes)",
            "def labels_to_segments(labels, window):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    ARGUMENTS:\\n     - labels:     a sequence of class labels (per time window)\\n     - window:     window duration (in seconds)\\n\\n    RETURNS:\\n     - segments:   a sequence of segment's limits: segs[i,0] is start and\\n                   segs[i,1] are start and end point of segment i\\n     - classes:    a sequence of class flags: class[i] is the class ID of\\n                   the i-th segment\\n    \"\n    if len(labels) == 1:\n        segs = [0, window]\n        classes = labels\n        return (segs, classes)\n    num_segs = 0\n    index = 0\n    classes = []\n    segment_list = []\n    cur_label = labels[index]\n    while index < len(labels) - 1:\n        previous_value = cur_label\n        while True:\n            index += 1\n            compare_flag = labels[index]\n            if (compare_flag != cur_label) | (index == len(labels) - 1):\n                num_segs += 1\n                cur_label = labels[index]\n                segment_list.append(index * window)\n                classes.append(previous_value)\n                break\n    segments = np.zeros((len(segment_list), 2))\n    for i in range(len(segment_list)):\n        if i > 0:\n            segments[i, 0] = segment_list[i - 1]\n        segments[i, 1] = segment_list[i]\n    return (segments, classes)",
            "def labels_to_segments(labels, window):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    ARGUMENTS:\\n     - labels:     a sequence of class labels (per time window)\\n     - window:     window duration (in seconds)\\n\\n    RETURNS:\\n     - segments:   a sequence of segment's limits: segs[i,0] is start and\\n                   segs[i,1] are start and end point of segment i\\n     - classes:    a sequence of class flags: class[i] is the class ID of\\n                   the i-th segment\\n    \"\n    if len(labels) == 1:\n        segs = [0, window]\n        classes = labels\n        return (segs, classes)\n    num_segs = 0\n    index = 0\n    classes = []\n    segment_list = []\n    cur_label = labels[index]\n    while index < len(labels) - 1:\n        previous_value = cur_label\n        while True:\n            index += 1\n            compare_flag = labels[index]\n            if (compare_flag != cur_label) | (index == len(labels) - 1):\n                num_segs += 1\n                cur_label = labels[index]\n                segment_list.append(index * window)\n                classes.append(previous_value)\n                break\n    segments = np.zeros((len(segment_list), 2))\n    for i in range(len(segment_list)):\n        if i > 0:\n            segments[i, 0] = segment_list[i - 1]\n        segments[i, 1] = segment_list[i]\n    return (segments, classes)",
            "def labels_to_segments(labels, window):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    ARGUMENTS:\\n     - labels:     a sequence of class labels (per time window)\\n     - window:     window duration (in seconds)\\n\\n    RETURNS:\\n     - segments:   a sequence of segment's limits: segs[i,0] is start and\\n                   segs[i,1] are start and end point of segment i\\n     - classes:    a sequence of class flags: class[i] is the class ID of\\n                   the i-th segment\\n    \"\n    if len(labels) == 1:\n        segs = [0, window]\n        classes = labels\n        return (segs, classes)\n    num_segs = 0\n    index = 0\n    classes = []\n    segment_list = []\n    cur_label = labels[index]\n    while index < len(labels) - 1:\n        previous_value = cur_label\n        while True:\n            index += 1\n            compare_flag = labels[index]\n            if (compare_flag != cur_label) | (index == len(labels) - 1):\n                num_segs += 1\n                cur_label = labels[index]\n                segment_list.append(index * window)\n                classes.append(previous_value)\n                break\n    segments = np.zeros((len(segment_list), 2))\n    for i in range(len(segment_list)):\n        if i > 0:\n            segments[i, 0] = segment_list[i - 1]\n        segments[i, 1] = segment_list[i]\n    return (segments, classes)"
        ]
    },
    {
        "func_name": "segments_to_labels",
        "original": "def segments_to_labels(start_times, end_times, labels, window):\n    \"\"\"\n    This function converts segment endpoints and respective segment\n    labels to fix-sized class labels.\n    ARGUMENTS:\n     - start_times:  segment start points (in seconds)\n     - end_times:    segment endpoints (in seconds)\n     - labels:       segment labels\n     - window:      fix-sized window (in seconds)\n    RETURNS:\n     - flags:    np array of class indices\n     - class_names:    list of classnames (strings)\n    \"\"\"\n    flags = []\n    class_names = list(set(labels))\n    index = window / 2.0\n    while index < end_times[-1]:\n        for i in range(len(start_times)):\n            if start_times[i] < index <= end_times[i]:\n                break\n        flags.append(class_names.index(labels[i]))\n        index += window\n    return (np.array(flags), class_names)",
        "mutated": [
            "def segments_to_labels(start_times, end_times, labels, window):\n    if False:\n        i = 10\n    '\\n    This function converts segment endpoints and respective segment\\n    labels to fix-sized class labels.\\n    ARGUMENTS:\\n     - start_times:  segment start points (in seconds)\\n     - end_times:    segment endpoints (in seconds)\\n     - labels:       segment labels\\n     - window:      fix-sized window (in seconds)\\n    RETURNS:\\n     - flags:    np array of class indices\\n     - class_names:    list of classnames (strings)\\n    '\n    flags = []\n    class_names = list(set(labels))\n    index = window / 2.0\n    while index < end_times[-1]:\n        for i in range(len(start_times)):\n            if start_times[i] < index <= end_times[i]:\n                break\n        flags.append(class_names.index(labels[i]))\n        index += window\n    return (np.array(flags), class_names)",
            "def segments_to_labels(start_times, end_times, labels, window):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    This function converts segment endpoints and respective segment\\n    labels to fix-sized class labels.\\n    ARGUMENTS:\\n     - start_times:  segment start points (in seconds)\\n     - end_times:    segment endpoints (in seconds)\\n     - labels:       segment labels\\n     - window:      fix-sized window (in seconds)\\n    RETURNS:\\n     - flags:    np array of class indices\\n     - class_names:    list of classnames (strings)\\n    '\n    flags = []\n    class_names = list(set(labels))\n    index = window / 2.0\n    while index < end_times[-1]:\n        for i in range(len(start_times)):\n            if start_times[i] < index <= end_times[i]:\n                break\n        flags.append(class_names.index(labels[i]))\n        index += window\n    return (np.array(flags), class_names)",
            "def segments_to_labels(start_times, end_times, labels, window):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    This function converts segment endpoints and respective segment\\n    labels to fix-sized class labels.\\n    ARGUMENTS:\\n     - start_times:  segment start points (in seconds)\\n     - end_times:    segment endpoints (in seconds)\\n     - labels:       segment labels\\n     - window:      fix-sized window (in seconds)\\n    RETURNS:\\n     - flags:    np array of class indices\\n     - class_names:    list of classnames (strings)\\n    '\n    flags = []\n    class_names = list(set(labels))\n    index = window / 2.0\n    while index < end_times[-1]:\n        for i in range(len(start_times)):\n            if start_times[i] < index <= end_times[i]:\n                break\n        flags.append(class_names.index(labels[i]))\n        index += window\n    return (np.array(flags), class_names)",
            "def segments_to_labels(start_times, end_times, labels, window):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    This function converts segment endpoints and respective segment\\n    labels to fix-sized class labels.\\n    ARGUMENTS:\\n     - start_times:  segment start points (in seconds)\\n     - end_times:    segment endpoints (in seconds)\\n     - labels:       segment labels\\n     - window:      fix-sized window (in seconds)\\n    RETURNS:\\n     - flags:    np array of class indices\\n     - class_names:    list of classnames (strings)\\n    '\n    flags = []\n    class_names = list(set(labels))\n    index = window / 2.0\n    while index < end_times[-1]:\n        for i in range(len(start_times)):\n            if start_times[i] < index <= end_times[i]:\n                break\n        flags.append(class_names.index(labels[i]))\n        index += window\n    return (np.array(flags), class_names)",
            "def segments_to_labels(start_times, end_times, labels, window):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    This function converts segment endpoints and respective segment\\n    labels to fix-sized class labels.\\n    ARGUMENTS:\\n     - start_times:  segment start points (in seconds)\\n     - end_times:    segment endpoints (in seconds)\\n     - labels:       segment labels\\n     - window:      fix-sized window (in seconds)\\n    RETURNS:\\n     - flags:    np array of class indices\\n     - class_names:    list of classnames (strings)\\n    '\n    flags = []\n    class_names = list(set(labels))\n    index = window / 2.0\n    while index < end_times[-1]:\n        for i in range(len(start_times)):\n            if start_times[i] < index <= end_times[i]:\n                break\n        flags.append(class_names.index(labels[i]))\n        index += window\n    return (np.array(flags), class_names)"
        ]
    },
    {
        "func_name": "compute_metrics",
        "original": "def compute_metrics(confusion_matrix, class_names):\n    \"\"\"\n    This function computes the precision, recall and f1 measures,\n    given a confusion matrix\n    \"\"\"\n    f1 = []\n    recall = []\n    precision = []\n    n_classes = confusion_matrix.shape[0]\n    if len(class_names) != n_classes:\n        print('Error in computePreRec! Confusion matrix and class_names list must be of the same size!')\n    else:\n        for (i, c) in enumerate(class_names):\n            precision.append(confusion_matrix[i, i] / np.sum(confusion_matrix[:, i]))\n            recall.append(confusion_matrix[i, i] / np.sum(confusion_matrix[i, :]))\n            f1.append(2 * precision[-1] * recall[-1] / (precision[-1] + recall[-1]))\n    return (recall, precision, f1)",
        "mutated": [
            "def compute_metrics(confusion_matrix, class_names):\n    if False:\n        i = 10\n    '\\n    This function computes the precision, recall and f1 measures,\\n    given a confusion matrix\\n    '\n    f1 = []\n    recall = []\n    precision = []\n    n_classes = confusion_matrix.shape[0]\n    if len(class_names) != n_classes:\n        print('Error in computePreRec! Confusion matrix and class_names list must be of the same size!')\n    else:\n        for (i, c) in enumerate(class_names):\n            precision.append(confusion_matrix[i, i] / np.sum(confusion_matrix[:, i]))\n            recall.append(confusion_matrix[i, i] / np.sum(confusion_matrix[i, :]))\n            f1.append(2 * precision[-1] * recall[-1] / (precision[-1] + recall[-1]))\n    return (recall, precision, f1)",
            "def compute_metrics(confusion_matrix, class_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    This function computes the precision, recall and f1 measures,\\n    given a confusion matrix\\n    '\n    f1 = []\n    recall = []\n    precision = []\n    n_classes = confusion_matrix.shape[0]\n    if len(class_names) != n_classes:\n        print('Error in computePreRec! Confusion matrix and class_names list must be of the same size!')\n    else:\n        for (i, c) in enumerate(class_names):\n            precision.append(confusion_matrix[i, i] / np.sum(confusion_matrix[:, i]))\n            recall.append(confusion_matrix[i, i] / np.sum(confusion_matrix[i, :]))\n            f1.append(2 * precision[-1] * recall[-1] / (precision[-1] + recall[-1]))\n    return (recall, precision, f1)",
            "def compute_metrics(confusion_matrix, class_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    This function computes the precision, recall and f1 measures,\\n    given a confusion matrix\\n    '\n    f1 = []\n    recall = []\n    precision = []\n    n_classes = confusion_matrix.shape[0]\n    if len(class_names) != n_classes:\n        print('Error in computePreRec! Confusion matrix and class_names list must be of the same size!')\n    else:\n        for (i, c) in enumerate(class_names):\n            precision.append(confusion_matrix[i, i] / np.sum(confusion_matrix[:, i]))\n            recall.append(confusion_matrix[i, i] / np.sum(confusion_matrix[i, :]))\n            f1.append(2 * precision[-1] * recall[-1] / (precision[-1] + recall[-1]))\n    return (recall, precision, f1)",
            "def compute_metrics(confusion_matrix, class_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    This function computes the precision, recall and f1 measures,\\n    given a confusion matrix\\n    '\n    f1 = []\n    recall = []\n    precision = []\n    n_classes = confusion_matrix.shape[0]\n    if len(class_names) != n_classes:\n        print('Error in computePreRec! Confusion matrix and class_names list must be of the same size!')\n    else:\n        for (i, c) in enumerate(class_names):\n            precision.append(confusion_matrix[i, i] / np.sum(confusion_matrix[:, i]))\n            recall.append(confusion_matrix[i, i] / np.sum(confusion_matrix[i, :]))\n            f1.append(2 * precision[-1] * recall[-1] / (precision[-1] + recall[-1]))\n    return (recall, precision, f1)",
            "def compute_metrics(confusion_matrix, class_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    This function computes the precision, recall and f1 measures,\\n    given a confusion matrix\\n    '\n    f1 = []\n    recall = []\n    precision = []\n    n_classes = confusion_matrix.shape[0]\n    if len(class_names) != n_classes:\n        print('Error in computePreRec! Confusion matrix and class_names list must be of the same size!')\n    else:\n        for (i, c) in enumerate(class_names):\n            precision.append(confusion_matrix[i, i] / np.sum(confusion_matrix[:, i]))\n            recall.append(confusion_matrix[i, i] / np.sum(confusion_matrix[i, :]))\n            f1.append(2 * precision[-1] * recall[-1] / (precision[-1] + recall[-1]))\n    return (recall, precision, f1)"
        ]
    },
    {
        "func_name": "read_segmentation_gt",
        "original": "def read_segmentation_gt(gt_file):\n    \"\"\"\n    This function reads a segmentation ground truth file,\n    following a simple CSV format with the following columns:\n    <segment start>,<segment end>,<class label>\n\n    ARGUMENTS:\n     - gt_file:       the path of the CSV segment file\n    RETURNS:\n     - seg_start:     a np array of segments' start positions\n     - seg_end:       a np array of segments' ending positions\n     - seg_label:     a list of respective class labels (strings)\n    \"\"\"\n    with open(gt_file, 'rt') as f_handle:\n        reader = csv.reader(f_handle, delimiter='\\t')\n        start_times = []\n        end_times = []\n        labels = []\n        for row in reader:\n            if len(row) == 3:\n                start_times.append(float(row[0]))\n                end_times.append(float(row[1]))\n                labels.append(row[2])\n    return (np.array(start_times), np.array(end_times), labels)",
        "mutated": [
            "def read_segmentation_gt(gt_file):\n    if False:\n        i = 10\n    \"\\n    This function reads a segmentation ground truth file,\\n    following a simple CSV format with the following columns:\\n    <segment start>,<segment end>,<class label>\\n\\n    ARGUMENTS:\\n     - gt_file:       the path of the CSV segment file\\n    RETURNS:\\n     - seg_start:     a np array of segments' start positions\\n     - seg_end:       a np array of segments' ending positions\\n     - seg_label:     a list of respective class labels (strings)\\n    \"\n    with open(gt_file, 'rt') as f_handle:\n        reader = csv.reader(f_handle, delimiter='\\t')\n        start_times = []\n        end_times = []\n        labels = []\n        for row in reader:\n            if len(row) == 3:\n                start_times.append(float(row[0]))\n                end_times.append(float(row[1]))\n                labels.append(row[2])\n    return (np.array(start_times), np.array(end_times), labels)",
            "def read_segmentation_gt(gt_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    This function reads a segmentation ground truth file,\\n    following a simple CSV format with the following columns:\\n    <segment start>,<segment end>,<class label>\\n\\n    ARGUMENTS:\\n     - gt_file:       the path of the CSV segment file\\n    RETURNS:\\n     - seg_start:     a np array of segments' start positions\\n     - seg_end:       a np array of segments' ending positions\\n     - seg_label:     a list of respective class labels (strings)\\n    \"\n    with open(gt_file, 'rt') as f_handle:\n        reader = csv.reader(f_handle, delimiter='\\t')\n        start_times = []\n        end_times = []\n        labels = []\n        for row in reader:\n            if len(row) == 3:\n                start_times.append(float(row[0]))\n                end_times.append(float(row[1]))\n                labels.append(row[2])\n    return (np.array(start_times), np.array(end_times), labels)",
            "def read_segmentation_gt(gt_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    This function reads a segmentation ground truth file,\\n    following a simple CSV format with the following columns:\\n    <segment start>,<segment end>,<class label>\\n\\n    ARGUMENTS:\\n     - gt_file:       the path of the CSV segment file\\n    RETURNS:\\n     - seg_start:     a np array of segments' start positions\\n     - seg_end:       a np array of segments' ending positions\\n     - seg_label:     a list of respective class labels (strings)\\n    \"\n    with open(gt_file, 'rt') as f_handle:\n        reader = csv.reader(f_handle, delimiter='\\t')\n        start_times = []\n        end_times = []\n        labels = []\n        for row in reader:\n            if len(row) == 3:\n                start_times.append(float(row[0]))\n                end_times.append(float(row[1]))\n                labels.append(row[2])\n    return (np.array(start_times), np.array(end_times), labels)",
            "def read_segmentation_gt(gt_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    This function reads a segmentation ground truth file,\\n    following a simple CSV format with the following columns:\\n    <segment start>,<segment end>,<class label>\\n\\n    ARGUMENTS:\\n     - gt_file:       the path of the CSV segment file\\n    RETURNS:\\n     - seg_start:     a np array of segments' start positions\\n     - seg_end:       a np array of segments' ending positions\\n     - seg_label:     a list of respective class labels (strings)\\n    \"\n    with open(gt_file, 'rt') as f_handle:\n        reader = csv.reader(f_handle, delimiter='\\t')\n        start_times = []\n        end_times = []\n        labels = []\n        for row in reader:\n            if len(row) == 3:\n                start_times.append(float(row[0]))\n                end_times.append(float(row[1]))\n                labels.append(row[2])\n    return (np.array(start_times), np.array(end_times), labels)",
            "def read_segmentation_gt(gt_file):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    This function reads a segmentation ground truth file,\\n    following a simple CSV format with the following columns:\\n    <segment start>,<segment end>,<class label>\\n\\n    ARGUMENTS:\\n     - gt_file:       the path of the CSV segment file\\n    RETURNS:\\n     - seg_start:     a np array of segments' start positions\\n     - seg_end:       a np array of segments' ending positions\\n     - seg_label:     a list of respective class labels (strings)\\n    \"\n    with open(gt_file, 'rt') as f_handle:\n        reader = csv.reader(f_handle, delimiter='\\t')\n        start_times = []\n        end_times = []\n        labels = []\n        for row in reader:\n            if len(row) == 3:\n                start_times.append(float(row[0]))\n                end_times.append(float(row[1]))\n                labels.append(row[2])\n    return (np.array(start_times), np.array(end_times), labels)"
        ]
    },
    {
        "func_name": "plot_segmentation_results",
        "original": "def plot_segmentation_results(flags_ind, flags_ind_gt, class_names, mt_step, evaluate_only=False):\n    \"\"\"\n    This function plots statistics on the classification-segmentation results \n    produced either by the fix-sized supervised method or the HMM method.\n    It also computes the overall accuracy achieved by the respective method \n    if ground-truth is available.\n    \"\"\"\n    flags = [class_names[int(f)] for f in flags_ind]\n    (segments, classes) = labels_to_segments(flags, mt_step)\n    min_len = min(flags_ind.shape[0], flags_ind_gt.shape[0])\n    if min_len > 0:\n        accuracy = np.sum(flags_ind[0:min_len] == flags_ind_gt[0:min_len]) / float(min_len)\n    else:\n        accuracy = -1\n    if not evaluate_only:\n        duration = segments[-1, 1]\n        s_percentages = np.zeros((len(class_names),))\n        percentages = np.zeros((len(class_names),))\n        av_durations = np.zeros((len(class_names),))\n        for i_seg in range(segments.shape[0]):\n            s_percentages[class_names.index(classes[i_seg])] += segments[i_seg, 1] - segments[i_seg, 0]\n        for i in range(s_percentages.shape[0]):\n            percentages[i] = 100.0 * s_percentages[i] / duration\n            class_sum = sum((1 for c in classes if c == class_names[i]))\n            if class_sum > 0:\n                av_durations[i] = s_percentages[i] / class_sum\n            else:\n                av_durations[i] = 0.0\n        for i in range(percentages.shape[0]):\n            print(class_names[i], percentages[i], av_durations[i])\n        font = {'size': 10}\n        plt.rc('font', **font)\n        fig = plt.figure()\n        ax1 = fig.add_subplot(211)\n        ax1.set_yticks(np.array(range(len(class_names))))\n        ax1.axis((0, duration, -1, len(class_names)))\n        ax1.set_yticklabels(class_names)\n        ax1.plot(np.array(range(len(flags_ind))) * mt_step + mt_step / 2.0, flags_ind)\n        if flags_ind_gt.shape[0] > 0:\n            ax1.plot(np.array(range(len(flags_ind_gt))) * mt_step + mt_step / 2.0, flags_ind_gt + 0.05, '--r')\n        plt.xlabel('time (seconds)')\n        if accuracy >= 0:\n            plt.title('Accuracy = {0:.1f}%'.format(100.0 * accuracy))\n        ax2 = fig.add_subplot(223)\n        plt.title('Classes percentage durations')\n        ax2.axis((0, len(class_names) + 1, 0, 100))\n        ax2.set_xticks(np.array(range(len(class_names) + 1)))\n        ax2.set_xticklabels([' '] + class_names)\n        print(np.array(range(len(class_names))), percentages)\n        ax2.bar(np.array(range(len(class_names))) + 0.5, percentages)\n        ax3 = fig.add_subplot(224)\n        plt.title('Segment average duration per class')\n        ax3.axis((0, len(class_names) + 1, 0, av_durations.max()))\n        ax3.set_xticks(np.array(range(len(class_names) + 1)))\n        ax3.set_xticklabels([' '] + class_names)\n        ax3.bar(np.array(range(len(class_names))) + 0.5, av_durations)\n        fig.tight_layout()\n        plt.show()\n    return accuracy",
        "mutated": [
            "def plot_segmentation_results(flags_ind, flags_ind_gt, class_names, mt_step, evaluate_only=False):\n    if False:\n        i = 10\n    '\\n    This function plots statistics on the classification-segmentation results \\n    produced either by the fix-sized supervised method or the HMM method.\\n    It also computes the overall accuracy achieved by the respective method \\n    if ground-truth is available.\\n    '\n    flags = [class_names[int(f)] for f in flags_ind]\n    (segments, classes) = labels_to_segments(flags, mt_step)\n    min_len = min(flags_ind.shape[0], flags_ind_gt.shape[0])\n    if min_len > 0:\n        accuracy = np.sum(flags_ind[0:min_len] == flags_ind_gt[0:min_len]) / float(min_len)\n    else:\n        accuracy = -1\n    if not evaluate_only:\n        duration = segments[-1, 1]\n        s_percentages = np.zeros((len(class_names),))\n        percentages = np.zeros((len(class_names),))\n        av_durations = np.zeros((len(class_names),))\n        for i_seg in range(segments.shape[0]):\n            s_percentages[class_names.index(classes[i_seg])] += segments[i_seg, 1] - segments[i_seg, 0]\n        for i in range(s_percentages.shape[0]):\n            percentages[i] = 100.0 * s_percentages[i] / duration\n            class_sum = sum((1 for c in classes if c == class_names[i]))\n            if class_sum > 0:\n                av_durations[i] = s_percentages[i] / class_sum\n            else:\n                av_durations[i] = 0.0\n        for i in range(percentages.shape[0]):\n            print(class_names[i], percentages[i], av_durations[i])\n        font = {'size': 10}\n        plt.rc('font', **font)\n        fig = plt.figure()\n        ax1 = fig.add_subplot(211)\n        ax1.set_yticks(np.array(range(len(class_names))))\n        ax1.axis((0, duration, -1, len(class_names)))\n        ax1.set_yticklabels(class_names)\n        ax1.plot(np.array(range(len(flags_ind))) * mt_step + mt_step / 2.0, flags_ind)\n        if flags_ind_gt.shape[0] > 0:\n            ax1.plot(np.array(range(len(flags_ind_gt))) * mt_step + mt_step / 2.0, flags_ind_gt + 0.05, '--r')\n        plt.xlabel('time (seconds)')\n        if accuracy >= 0:\n            plt.title('Accuracy = {0:.1f}%'.format(100.0 * accuracy))\n        ax2 = fig.add_subplot(223)\n        plt.title('Classes percentage durations')\n        ax2.axis((0, len(class_names) + 1, 0, 100))\n        ax2.set_xticks(np.array(range(len(class_names) + 1)))\n        ax2.set_xticklabels([' '] + class_names)\n        print(np.array(range(len(class_names))), percentages)\n        ax2.bar(np.array(range(len(class_names))) + 0.5, percentages)\n        ax3 = fig.add_subplot(224)\n        plt.title('Segment average duration per class')\n        ax3.axis((0, len(class_names) + 1, 0, av_durations.max()))\n        ax3.set_xticks(np.array(range(len(class_names) + 1)))\n        ax3.set_xticklabels([' '] + class_names)\n        ax3.bar(np.array(range(len(class_names))) + 0.5, av_durations)\n        fig.tight_layout()\n        plt.show()\n    return accuracy",
            "def plot_segmentation_results(flags_ind, flags_ind_gt, class_names, mt_step, evaluate_only=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    This function plots statistics on the classification-segmentation results \\n    produced either by the fix-sized supervised method or the HMM method.\\n    It also computes the overall accuracy achieved by the respective method \\n    if ground-truth is available.\\n    '\n    flags = [class_names[int(f)] for f in flags_ind]\n    (segments, classes) = labels_to_segments(flags, mt_step)\n    min_len = min(flags_ind.shape[0], flags_ind_gt.shape[0])\n    if min_len > 0:\n        accuracy = np.sum(flags_ind[0:min_len] == flags_ind_gt[0:min_len]) / float(min_len)\n    else:\n        accuracy = -1\n    if not evaluate_only:\n        duration = segments[-1, 1]\n        s_percentages = np.zeros((len(class_names),))\n        percentages = np.zeros((len(class_names),))\n        av_durations = np.zeros((len(class_names),))\n        for i_seg in range(segments.shape[0]):\n            s_percentages[class_names.index(classes[i_seg])] += segments[i_seg, 1] - segments[i_seg, 0]\n        for i in range(s_percentages.shape[0]):\n            percentages[i] = 100.0 * s_percentages[i] / duration\n            class_sum = sum((1 for c in classes if c == class_names[i]))\n            if class_sum > 0:\n                av_durations[i] = s_percentages[i] / class_sum\n            else:\n                av_durations[i] = 0.0\n        for i in range(percentages.shape[0]):\n            print(class_names[i], percentages[i], av_durations[i])\n        font = {'size': 10}\n        plt.rc('font', **font)\n        fig = plt.figure()\n        ax1 = fig.add_subplot(211)\n        ax1.set_yticks(np.array(range(len(class_names))))\n        ax1.axis((0, duration, -1, len(class_names)))\n        ax1.set_yticklabels(class_names)\n        ax1.plot(np.array(range(len(flags_ind))) * mt_step + mt_step / 2.0, flags_ind)\n        if flags_ind_gt.shape[0] > 0:\n            ax1.plot(np.array(range(len(flags_ind_gt))) * mt_step + mt_step / 2.0, flags_ind_gt + 0.05, '--r')\n        plt.xlabel('time (seconds)')\n        if accuracy >= 0:\n            plt.title('Accuracy = {0:.1f}%'.format(100.0 * accuracy))\n        ax2 = fig.add_subplot(223)\n        plt.title('Classes percentage durations')\n        ax2.axis((0, len(class_names) + 1, 0, 100))\n        ax2.set_xticks(np.array(range(len(class_names) + 1)))\n        ax2.set_xticklabels([' '] + class_names)\n        print(np.array(range(len(class_names))), percentages)\n        ax2.bar(np.array(range(len(class_names))) + 0.5, percentages)\n        ax3 = fig.add_subplot(224)\n        plt.title('Segment average duration per class')\n        ax3.axis((0, len(class_names) + 1, 0, av_durations.max()))\n        ax3.set_xticks(np.array(range(len(class_names) + 1)))\n        ax3.set_xticklabels([' '] + class_names)\n        ax3.bar(np.array(range(len(class_names))) + 0.5, av_durations)\n        fig.tight_layout()\n        plt.show()\n    return accuracy",
            "def plot_segmentation_results(flags_ind, flags_ind_gt, class_names, mt_step, evaluate_only=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    This function plots statistics on the classification-segmentation results \\n    produced either by the fix-sized supervised method or the HMM method.\\n    It also computes the overall accuracy achieved by the respective method \\n    if ground-truth is available.\\n    '\n    flags = [class_names[int(f)] for f in flags_ind]\n    (segments, classes) = labels_to_segments(flags, mt_step)\n    min_len = min(flags_ind.shape[0], flags_ind_gt.shape[0])\n    if min_len > 0:\n        accuracy = np.sum(flags_ind[0:min_len] == flags_ind_gt[0:min_len]) / float(min_len)\n    else:\n        accuracy = -1\n    if not evaluate_only:\n        duration = segments[-1, 1]\n        s_percentages = np.zeros((len(class_names),))\n        percentages = np.zeros((len(class_names),))\n        av_durations = np.zeros((len(class_names),))\n        for i_seg in range(segments.shape[0]):\n            s_percentages[class_names.index(classes[i_seg])] += segments[i_seg, 1] - segments[i_seg, 0]\n        for i in range(s_percentages.shape[0]):\n            percentages[i] = 100.0 * s_percentages[i] / duration\n            class_sum = sum((1 for c in classes if c == class_names[i]))\n            if class_sum > 0:\n                av_durations[i] = s_percentages[i] / class_sum\n            else:\n                av_durations[i] = 0.0\n        for i in range(percentages.shape[0]):\n            print(class_names[i], percentages[i], av_durations[i])\n        font = {'size': 10}\n        plt.rc('font', **font)\n        fig = plt.figure()\n        ax1 = fig.add_subplot(211)\n        ax1.set_yticks(np.array(range(len(class_names))))\n        ax1.axis((0, duration, -1, len(class_names)))\n        ax1.set_yticklabels(class_names)\n        ax1.plot(np.array(range(len(flags_ind))) * mt_step + mt_step / 2.0, flags_ind)\n        if flags_ind_gt.shape[0] > 0:\n            ax1.plot(np.array(range(len(flags_ind_gt))) * mt_step + mt_step / 2.0, flags_ind_gt + 0.05, '--r')\n        plt.xlabel('time (seconds)')\n        if accuracy >= 0:\n            plt.title('Accuracy = {0:.1f}%'.format(100.0 * accuracy))\n        ax2 = fig.add_subplot(223)\n        plt.title('Classes percentage durations')\n        ax2.axis((0, len(class_names) + 1, 0, 100))\n        ax2.set_xticks(np.array(range(len(class_names) + 1)))\n        ax2.set_xticklabels([' '] + class_names)\n        print(np.array(range(len(class_names))), percentages)\n        ax2.bar(np.array(range(len(class_names))) + 0.5, percentages)\n        ax3 = fig.add_subplot(224)\n        plt.title('Segment average duration per class')\n        ax3.axis((0, len(class_names) + 1, 0, av_durations.max()))\n        ax3.set_xticks(np.array(range(len(class_names) + 1)))\n        ax3.set_xticklabels([' '] + class_names)\n        ax3.bar(np.array(range(len(class_names))) + 0.5, av_durations)\n        fig.tight_layout()\n        plt.show()\n    return accuracy",
            "def plot_segmentation_results(flags_ind, flags_ind_gt, class_names, mt_step, evaluate_only=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    This function plots statistics on the classification-segmentation results \\n    produced either by the fix-sized supervised method or the HMM method.\\n    It also computes the overall accuracy achieved by the respective method \\n    if ground-truth is available.\\n    '\n    flags = [class_names[int(f)] for f in flags_ind]\n    (segments, classes) = labels_to_segments(flags, mt_step)\n    min_len = min(flags_ind.shape[0], flags_ind_gt.shape[0])\n    if min_len > 0:\n        accuracy = np.sum(flags_ind[0:min_len] == flags_ind_gt[0:min_len]) / float(min_len)\n    else:\n        accuracy = -1\n    if not evaluate_only:\n        duration = segments[-1, 1]\n        s_percentages = np.zeros((len(class_names),))\n        percentages = np.zeros((len(class_names),))\n        av_durations = np.zeros((len(class_names),))\n        for i_seg in range(segments.shape[0]):\n            s_percentages[class_names.index(classes[i_seg])] += segments[i_seg, 1] - segments[i_seg, 0]\n        for i in range(s_percentages.shape[0]):\n            percentages[i] = 100.0 * s_percentages[i] / duration\n            class_sum = sum((1 for c in classes if c == class_names[i]))\n            if class_sum > 0:\n                av_durations[i] = s_percentages[i] / class_sum\n            else:\n                av_durations[i] = 0.0\n        for i in range(percentages.shape[0]):\n            print(class_names[i], percentages[i], av_durations[i])\n        font = {'size': 10}\n        plt.rc('font', **font)\n        fig = plt.figure()\n        ax1 = fig.add_subplot(211)\n        ax1.set_yticks(np.array(range(len(class_names))))\n        ax1.axis((0, duration, -1, len(class_names)))\n        ax1.set_yticklabels(class_names)\n        ax1.plot(np.array(range(len(flags_ind))) * mt_step + mt_step / 2.0, flags_ind)\n        if flags_ind_gt.shape[0] > 0:\n            ax1.plot(np.array(range(len(flags_ind_gt))) * mt_step + mt_step / 2.0, flags_ind_gt + 0.05, '--r')\n        plt.xlabel('time (seconds)')\n        if accuracy >= 0:\n            plt.title('Accuracy = {0:.1f}%'.format(100.0 * accuracy))\n        ax2 = fig.add_subplot(223)\n        plt.title('Classes percentage durations')\n        ax2.axis((0, len(class_names) + 1, 0, 100))\n        ax2.set_xticks(np.array(range(len(class_names) + 1)))\n        ax2.set_xticklabels([' '] + class_names)\n        print(np.array(range(len(class_names))), percentages)\n        ax2.bar(np.array(range(len(class_names))) + 0.5, percentages)\n        ax3 = fig.add_subplot(224)\n        plt.title('Segment average duration per class')\n        ax3.axis((0, len(class_names) + 1, 0, av_durations.max()))\n        ax3.set_xticks(np.array(range(len(class_names) + 1)))\n        ax3.set_xticklabels([' '] + class_names)\n        ax3.bar(np.array(range(len(class_names))) + 0.5, av_durations)\n        fig.tight_layout()\n        plt.show()\n    return accuracy",
            "def plot_segmentation_results(flags_ind, flags_ind_gt, class_names, mt_step, evaluate_only=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    This function plots statistics on the classification-segmentation results \\n    produced either by the fix-sized supervised method or the HMM method.\\n    It also computes the overall accuracy achieved by the respective method \\n    if ground-truth is available.\\n    '\n    flags = [class_names[int(f)] for f in flags_ind]\n    (segments, classes) = labels_to_segments(flags, mt_step)\n    min_len = min(flags_ind.shape[0], flags_ind_gt.shape[0])\n    if min_len > 0:\n        accuracy = np.sum(flags_ind[0:min_len] == flags_ind_gt[0:min_len]) / float(min_len)\n    else:\n        accuracy = -1\n    if not evaluate_only:\n        duration = segments[-1, 1]\n        s_percentages = np.zeros((len(class_names),))\n        percentages = np.zeros((len(class_names),))\n        av_durations = np.zeros((len(class_names),))\n        for i_seg in range(segments.shape[0]):\n            s_percentages[class_names.index(classes[i_seg])] += segments[i_seg, 1] - segments[i_seg, 0]\n        for i in range(s_percentages.shape[0]):\n            percentages[i] = 100.0 * s_percentages[i] / duration\n            class_sum = sum((1 for c in classes if c == class_names[i]))\n            if class_sum > 0:\n                av_durations[i] = s_percentages[i] / class_sum\n            else:\n                av_durations[i] = 0.0\n        for i in range(percentages.shape[0]):\n            print(class_names[i], percentages[i], av_durations[i])\n        font = {'size': 10}\n        plt.rc('font', **font)\n        fig = plt.figure()\n        ax1 = fig.add_subplot(211)\n        ax1.set_yticks(np.array(range(len(class_names))))\n        ax1.axis((0, duration, -1, len(class_names)))\n        ax1.set_yticklabels(class_names)\n        ax1.plot(np.array(range(len(flags_ind))) * mt_step + mt_step / 2.0, flags_ind)\n        if flags_ind_gt.shape[0] > 0:\n            ax1.plot(np.array(range(len(flags_ind_gt))) * mt_step + mt_step / 2.0, flags_ind_gt + 0.05, '--r')\n        plt.xlabel('time (seconds)')\n        if accuracy >= 0:\n            plt.title('Accuracy = {0:.1f}%'.format(100.0 * accuracy))\n        ax2 = fig.add_subplot(223)\n        plt.title('Classes percentage durations')\n        ax2.axis((0, len(class_names) + 1, 0, 100))\n        ax2.set_xticks(np.array(range(len(class_names) + 1)))\n        ax2.set_xticklabels([' '] + class_names)\n        print(np.array(range(len(class_names))), percentages)\n        ax2.bar(np.array(range(len(class_names))) + 0.5, percentages)\n        ax3 = fig.add_subplot(224)\n        plt.title('Segment average duration per class')\n        ax3.axis((0, len(class_names) + 1, 0, av_durations.max()))\n        ax3.set_xticks(np.array(range(len(class_names) + 1)))\n        ax3.set_xticklabels([' '] + class_names)\n        ax3.bar(np.array(range(len(class_names))) + 0.5, av_durations)\n        fig.tight_layout()\n        plt.show()\n    return accuracy"
        ]
    },
    {
        "func_name": "evaluate_speaker_diarization",
        "original": "def evaluate_speaker_diarization(labels, labels_gt):\n    min_len = min(labels.shape[0], labels_gt.shape[0])\n    labels = labels[0:min_len]\n    labels_gt = labels_gt[0:min_len]\n    unique_flags = np.unique(labels)\n    unique_flags_gt = np.unique(labels_gt)\n    contigency_matrix = np.zeros((unique_flags.shape[0], unique_flags_gt.shape[0]))\n    for i in range(min_len):\n        contigency_matrix[int(np.nonzero(unique_flags == labels[i])[0]), int(np.nonzero(unique_flags_gt == labels_gt[i])[0])] += 1.0\n    (columns, rows) = contigency_matrix.shape\n    row_sum = np.sum(contigency_matrix, axis=0)\n    column_sum = np.sum(contigency_matrix, axis=1)\n    matrix_sum = np.sum(contigency_matrix)\n    purity_clust = np.zeros((columns,))\n    purity_speak = np.zeros((rows,))\n    for i in range(columns):\n        purity_clust[i] = np.max(contigency_matrix[i, :]) / column_sum[i]\n    for j in range(rows):\n        purity_speak[j] = np.max(contigency_matrix[:, j]) / row_sum[j]\n    purity_cluster_m = np.sum(purity_clust * column_sum) / matrix_sum\n    purity_speaker_m = np.sum(purity_speak * row_sum) / matrix_sum\n    return (purity_cluster_m, purity_speaker_m)",
        "mutated": [
            "def evaluate_speaker_diarization(labels, labels_gt):\n    if False:\n        i = 10\n    min_len = min(labels.shape[0], labels_gt.shape[0])\n    labels = labels[0:min_len]\n    labels_gt = labels_gt[0:min_len]\n    unique_flags = np.unique(labels)\n    unique_flags_gt = np.unique(labels_gt)\n    contigency_matrix = np.zeros((unique_flags.shape[0], unique_flags_gt.shape[0]))\n    for i in range(min_len):\n        contigency_matrix[int(np.nonzero(unique_flags == labels[i])[0]), int(np.nonzero(unique_flags_gt == labels_gt[i])[0])] += 1.0\n    (columns, rows) = contigency_matrix.shape\n    row_sum = np.sum(contigency_matrix, axis=0)\n    column_sum = np.sum(contigency_matrix, axis=1)\n    matrix_sum = np.sum(contigency_matrix)\n    purity_clust = np.zeros((columns,))\n    purity_speak = np.zeros((rows,))\n    for i in range(columns):\n        purity_clust[i] = np.max(contigency_matrix[i, :]) / column_sum[i]\n    for j in range(rows):\n        purity_speak[j] = np.max(contigency_matrix[:, j]) / row_sum[j]\n    purity_cluster_m = np.sum(purity_clust * column_sum) / matrix_sum\n    purity_speaker_m = np.sum(purity_speak * row_sum) / matrix_sum\n    return (purity_cluster_m, purity_speaker_m)",
            "def evaluate_speaker_diarization(labels, labels_gt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    min_len = min(labels.shape[0], labels_gt.shape[0])\n    labels = labels[0:min_len]\n    labels_gt = labels_gt[0:min_len]\n    unique_flags = np.unique(labels)\n    unique_flags_gt = np.unique(labels_gt)\n    contigency_matrix = np.zeros((unique_flags.shape[0], unique_flags_gt.shape[0]))\n    for i in range(min_len):\n        contigency_matrix[int(np.nonzero(unique_flags == labels[i])[0]), int(np.nonzero(unique_flags_gt == labels_gt[i])[0])] += 1.0\n    (columns, rows) = contigency_matrix.shape\n    row_sum = np.sum(contigency_matrix, axis=0)\n    column_sum = np.sum(contigency_matrix, axis=1)\n    matrix_sum = np.sum(contigency_matrix)\n    purity_clust = np.zeros((columns,))\n    purity_speak = np.zeros((rows,))\n    for i in range(columns):\n        purity_clust[i] = np.max(contigency_matrix[i, :]) / column_sum[i]\n    for j in range(rows):\n        purity_speak[j] = np.max(contigency_matrix[:, j]) / row_sum[j]\n    purity_cluster_m = np.sum(purity_clust * column_sum) / matrix_sum\n    purity_speaker_m = np.sum(purity_speak * row_sum) / matrix_sum\n    return (purity_cluster_m, purity_speaker_m)",
            "def evaluate_speaker_diarization(labels, labels_gt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    min_len = min(labels.shape[0], labels_gt.shape[0])\n    labels = labels[0:min_len]\n    labels_gt = labels_gt[0:min_len]\n    unique_flags = np.unique(labels)\n    unique_flags_gt = np.unique(labels_gt)\n    contigency_matrix = np.zeros((unique_flags.shape[0], unique_flags_gt.shape[0]))\n    for i in range(min_len):\n        contigency_matrix[int(np.nonzero(unique_flags == labels[i])[0]), int(np.nonzero(unique_flags_gt == labels_gt[i])[0])] += 1.0\n    (columns, rows) = contigency_matrix.shape\n    row_sum = np.sum(contigency_matrix, axis=0)\n    column_sum = np.sum(contigency_matrix, axis=1)\n    matrix_sum = np.sum(contigency_matrix)\n    purity_clust = np.zeros((columns,))\n    purity_speak = np.zeros((rows,))\n    for i in range(columns):\n        purity_clust[i] = np.max(contigency_matrix[i, :]) / column_sum[i]\n    for j in range(rows):\n        purity_speak[j] = np.max(contigency_matrix[:, j]) / row_sum[j]\n    purity_cluster_m = np.sum(purity_clust * column_sum) / matrix_sum\n    purity_speaker_m = np.sum(purity_speak * row_sum) / matrix_sum\n    return (purity_cluster_m, purity_speaker_m)",
            "def evaluate_speaker_diarization(labels, labels_gt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    min_len = min(labels.shape[0], labels_gt.shape[0])\n    labels = labels[0:min_len]\n    labels_gt = labels_gt[0:min_len]\n    unique_flags = np.unique(labels)\n    unique_flags_gt = np.unique(labels_gt)\n    contigency_matrix = np.zeros((unique_flags.shape[0], unique_flags_gt.shape[0]))\n    for i in range(min_len):\n        contigency_matrix[int(np.nonzero(unique_flags == labels[i])[0]), int(np.nonzero(unique_flags_gt == labels_gt[i])[0])] += 1.0\n    (columns, rows) = contigency_matrix.shape\n    row_sum = np.sum(contigency_matrix, axis=0)\n    column_sum = np.sum(contigency_matrix, axis=1)\n    matrix_sum = np.sum(contigency_matrix)\n    purity_clust = np.zeros((columns,))\n    purity_speak = np.zeros((rows,))\n    for i in range(columns):\n        purity_clust[i] = np.max(contigency_matrix[i, :]) / column_sum[i]\n    for j in range(rows):\n        purity_speak[j] = np.max(contigency_matrix[:, j]) / row_sum[j]\n    purity_cluster_m = np.sum(purity_clust * column_sum) / matrix_sum\n    purity_speaker_m = np.sum(purity_speak * row_sum) / matrix_sum\n    return (purity_cluster_m, purity_speaker_m)",
            "def evaluate_speaker_diarization(labels, labels_gt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    min_len = min(labels.shape[0], labels_gt.shape[0])\n    labels = labels[0:min_len]\n    labels_gt = labels_gt[0:min_len]\n    unique_flags = np.unique(labels)\n    unique_flags_gt = np.unique(labels_gt)\n    contigency_matrix = np.zeros((unique_flags.shape[0], unique_flags_gt.shape[0]))\n    for i in range(min_len):\n        contigency_matrix[int(np.nonzero(unique_flags == labels[i])[0]), int(np.nonzero(unique_flags_gt == labels_gt[i])[0])] += 1.0\n    (columns, rows) = contigency_matrix.shape\n    row_sum = np.sum(contigency_matrix, axis=0)\n    column_sum = np.sum(contigency_matrix, axis=1)\n    matrix_sum = np.sum(contigency_matrix)\n    purity_clust = np.zeros((columns,))\n    purity_speak = np.zeros((rows,))\n    for i in range(columns):\n        purity_clust[i] = np.max(contigency_matrix[i, :]) / column_sum[i]\n    for j in range(rows):\n        purity_speak[j] = np.max(contigency_matrix[:, j]) / row_sum[j]\n    purity_cluster_m = np.sum(purity_clust * column_sum) / matrix_sum\n    purity_speaker_m = np.sum(purity_speak * row_sum) / matrix_sum\n    return (purity_cluster_m, purity_speaker_m)"
        ]
    },
    {
        "func_name": "train_hmm_compute_statistics",
        "original": "def train_hmm_compute_statistics(features, labels):\n    \"\"\"\n    This function computes the statistics used to train\n    an HMM joint segmentation-classification model\n    using a sequence of sequential features and respective labels\n\n    ARGUMENTS:\n     - features:  a np matrix of feature vectors (numOfDimensions x n_wins)\n     - labels:    a np array of class indices (n_wins x 1)\n    RETURNS:\n     - class_priors:            matrix of prior class probabilities\n                                (n_classes x 1)\n     - transmutation_matrix:    transition matrix (n_classes x n_classes)\n     - means:                   means matrix (numOfDimensions x 1)\n     - cov:                     deviation matrix (numOfDimensions x 1)\n    \"\"\"\n    unique_labels = np.unique(labels)\n    n_comps = len(unique_labels)\n    n_feats = features.shape[0]\n    if features.shape[1] < labels.shape[0]:\n        print('trainHMM warning: number of short-term feature vectors must be greater or equal to the labels length!')\n        labels = labels[0:features.shape[1]]\n    class_priors = np.zeros((n_comps,))\n    for (i, u_label) in enumerate(unique_labels):\n        class_priors[i] = np.count_nonzero(labels == u_label)\n    class_priors = class_priors / class_priors.sum()\n    transmutation_matrix = np.zeros((n_comps, n_comps))\n    for i in range(labels.shape[0] - 1):\n        transmutation_matrix[int(labels[i]), int(labels[i + 1])] += 1\n    for i in range(n_comps):\n        transmutation_matrix[i, :] /= transmutation_matrix[i, :].sum()\n    means = np.zeros((n_comps, n_feats))\n    for i in range(n_comps):\n        means[i, :] = np.array(features[:, np.nonzero(labels == unique_labels[i])[0]].mean(axis=1))\n    cov = np.zeros((n_comps, n_feats))\n    for i in range(n_comps):\n        '\\n        cov[i, :, :] = np.cov(features[:, np.nonzero(labels == u_labels[i])[0]])\\n        '\n        cov[i, :] = np.std(features[:, np.nonzero(labels == unique_labels[i])[0]], axis=1)\n    return (class_priors, transmutation_matrix, means, cov)",
        "mutated": [
            "def train_hmm_compute_statistics(features, labels):\n    if False:\n        i = 10\n    '\\n    This function computes the statistics used to train\\n    an HMM joint segmentation-classification model\\n    using a sequence of sequential features and respective labels\\n\\n    ARGUMENTS:\\n     - features:  a np matrix of feature vectors (numOfDimensions x n_wins)\\n     - labels:    a np array of class indices (n_wins x 1)\\n    RETURNS:\\n     - class_priors:            matrix of prior class probabilities\\n                                (n_classes x 1)\\n     - transmutation_matrix:    transition matrix (n_classes x n_classes)\\n     - means:                   means matrix (numOfDimensions x 1)\\n     - cov:                     deviation matrix (numOfDimensions x 1)\\n    '\n    unique_labels = np.unique(labels)\n    n_comps = len(unique_labels)\n    n_feats = features.shape[0]\n    if features.shape[1] < labels.shape[0]:\n        print('trainHMM warning: number of short-term feature vectors must be greater or equal to the labels length!')\n        labels = labels[0:features.shape[1]]\n    class_priors = np.zeros((n_comps,))\n    for (i, u_label) in enumerate(unique_labels):\n        class_priors[i] = np.count_nonzero(labels == u_label)\n    class_priors = class_priors / class_priors.sum()\n    transmutation_matrix = np.zeros((n_comps, n_comps))\n    for i in range(labels.shape[0] - 1):\n        transmutation_matrix[int(labels[i]), int(labels[i + 1])] += 1\n    for i in range(n_comps):\n        transmutation_matrix[i, :] /= transmutation_matrix[i, :].sum()\n    means = np.zeros((n_comps, n_feats))\n    for i in range(n_comps):\n        means[i, :] = np.array(features[:, np.nonzero(labels == unique_labels[i])[0]].mean(axis=1))\n    cov = np.zeros((n_comps, n_feats))\n    for i in range(n_comps):\n        '\\n        cov[i, :, :] = np.cov(features[:, np.nonzero(labels == u_labels[i])[0]])\\n        '\n        cov[i, :] = np.std(features[:, np.nonzero(labels == unique_labels[i])[0]], axis=1)\n    return (class_priors, transmutation_matrix, means, cov)",
            "def train_hmm_compute_statistics(features, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    This function computes the statistics used to train\\n    an HMM joint segmentation-classification model\\n    using a sequence of sequential features and respective labels\\n\\n    ARGUMENTS:\\n     - features:  a np matrix of feature vectors (numOfDimensions x n_wins)\\n     - labels:    a np array of class indices (n_wins x 1)\\n    RETURNS:\\n     - class_priors:            matrix of prior class probabilities\\n                                (n_classes x 1)\\n     - transmutation_matrix:    transition matrix (n_classes x n_classes)\\n     - means:                   means matrix (numOfDimensions x 1)\\n     - cov:                     deviation matrix (numOfDimensions x 1)\\n    '\n    unique_labels = np.unique(labels)\n    n_comps = len(unique_labels)\n    n_feats = features.shape[0]\n    if features.shape[1] < labels.shape[0]:\n        print('trainHMM warning: number of short-term feature vectors must be greater or equal to the labels length!')\n        labels = labels[0:features.shape[1]]\n    class_priors = np.zeros((n_comps,))\n    for (i, u_label) in enumerate(unique_labels):\n        class_priors[i] = np.count_nonzero(labels == u_label)\n    class_priors = class_priors / class_priors.sum()\n    transmutation_matrix = np.zeros((n_comps, n_comps))\n    for i in range(labels.shape[0] - 1):\n        transmutation_matrix[int(labels[i]), int(labels[i + 1])] += 1\n    for i in range(n_comps):\n        transmutation_matrix[i, :] /= transmutation_matrix[i, :].sum()\n    means = np.zeros((n_comps, n_feats))\n    for i in range(n_comps):\n        means[i, :] = np.array(features[:, np.nonzero(labels == unique_labels[i])[0]].mean(axis=1))\n    cov = np.zeros((n_comps, n_feats))\n    for i in range(n_comps):\n        '\\n        cov[i, :, :] = np.cov(features[:, np.nonzero(labels == u_labels[i])[0]])\\n        '\n        cov[i, :] = np.std(features[:, np.nonzero(labels == unique_labels[i])[0]], axis=1)\n    return (class_priors, transmutation_matrix, means, cov)",
            "def train_hmm_compute_statistics(features, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    This function computes the statistics used to train\\n    an HMM joint segmentation-classification model\\n    using a sequence of sequential features and respective labels\\n\\n    ARGUMENTS:\\n     - features:  a np matrix of feature vectors (numOfDimensions x n_wins)\\n     - labels:    a np array of class indices (n_wins x 1)\\n    RETURNS:\\n     - class_priors:            matrix of prior class probabilities\\n                                (n_classes x 1)\\n     - transmutation_matrix:    transition matrix (n_classes x n_classes)\\n     - means:                   means matrix (numOfDimensions x 1)\\n     - cov:                     deviation matrix (numOfDimensions x 1)\\n    '\n    unique_labels = np.unique(labels)\n    n_comps = len(unique_labels)\n    n_feats = features.shape[0]\n    if features.shape[1] < labels.shape[0]:\n        print('trainHMM warning: number of short-term feature vectors must be greater or equal to the labels length!')\n        labels = labels[0:features.shape[1]]\n    class_priors = np.zeros((n_comps,))\n    for (i, u_label) in enumerate(unique_labels):\n        class_priors[i] = np.count_nonzero(labels == u_label)\n    class_priors = class_priors / class_priors.sum()\n    transmutation_matrix = np.zeros((n_comps, n_comps))\n    for i in range(labels.shape[0] - 1):\n        transmutation_matrix[int(labels[i]), int(labels[i + 1])] += 1\n    for i in range(n_comps):\n        transmutation_matrix[i, :] /= transmutation_matrix[i, :].sum()\n    means = np.zeros((n_comps, n_feats))\n    for i in range(n_comps):\n        means[i, :] = np.array(features[:, np.nonzero(labels == unique_labels[i])[0]].mean(axis=1))\n    cov = np.zeros((n_comps, n_feats))\n    for i in range(n_comps):\n        '\\n        cov[i, :, :] = np.cov(features[:, np.nonzero(labels == u_labels[i])[0]])\\n        '\n        cov[i, :] = np.std(features[:, np.nonzero(labels == unique_labels[i])[0]], axis=1)\n    return (class_priors, transmutation_matrix, means, cov)",
            "def train_hmm_compute_statistics(features, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    This function computes the statistics used to train\\n    an HMM joint segmentation-classification model\\n    using a sequence of sequential features and respective labels\\n\\n    ARGUMENTS:\\n     - features:  a np matrix of feature vectors (numOfDimensions x n_wins)\\n     - labels:    a np array of class indices (n_wins x 1)\\n    RETURNS:\\n     - class_priors:            matrix of prior class probabilities\\n                                (n_classes x 1)\\n     - transmutation_matrix:    transition matrix (n_classes x n_classes)\\n     - means:                   means matrix (numOfDimensions x 1)\\n     - cov:                     deviation matrix (numOfDimensions x 1)\\n    '\n    unique_labels = np.unique(labels)\n    n_comps = len(unique_labels)\n    n_feats = features.shape[0]\n    if features.shape[1] < labels.shape[0]:\n        print('trainHMM warning: number of short-term feature vectors must be greater or equal to the labels length!')\n        labels = labels[0:features.shape[1]]\n    class_priors = np.zeros((n_comps,))\n    for (i, u_label) in enumerate(unique_labels):\n        class_priors[i] = np.count_nonzero(labels == u_label)\n    class_priors = class_priors / class_priors.sum()\n    transmutation_matrix = np.zeros((n_comps, n_comps))\n    for i in range(labels.shape[0] - 1):\n        transmutation_matrix[int(labels[i]), int(labels[i + 1])] += 1\n    for i in range(n_comps):\n        transmutation_matrix[i, :] /= transmutation_matrix[i, :].sum()\n    means = np.zeros((n_comps, n_feats))\n    for i in range(n_comps):\n        means[i, :] = np.array(features[:, np.nonzero(labels == unique_labels[i])[0]].mean(axis=1))\n    cov = np.zeros((n_comps, n_feats))\n    for i in range(n_comps):\n        '\\n        cov[i, :, :] = np.cov(features[:, np.nonzero(labels == u_labels[i])[0]])\\n        '\n        cov[i, :] = np.std(features[:, np.nonzero(labels == unique_labels[i])[0]], axis=1)\n    return (class_priors, transmutation_matrix, means, cov)",
            "def train_hmm_compute_statistics(features, labels):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    This function computes the statistics used to train\\n    an HMM joint segmentation-classification model\\n    using a sequence of sequential features and respective labels\\n\\n    ARGUMENTS:\\n     - features:  a np matrix of feature vectors (numOfDimensions x n_wins)\\n     - labels:    a np array of class indices (n_wins x 1)\\n    RETURNS:\\n     - class_priors:            matrix of prior class probabilities\\n                                (n_classes x 1)\\n     - transmutation_matrix:    transition matrix (n_classes x n_classes)\\n     - means:                   means matrix (numOfDimensions x 1)\\n     - cov:                     deviation matrix (numOfDimensions x 1)\\n    '\n    unique_labels = np.unique(labels)\n    n_comps = len(unique_labels)\n    n_feats = features.shape[0]\n    if features.shape[1] < labels.shape[0]:\n        print('trainHMM warning: number of short-term feature vectors must be greater or equal to the labels length!')\n        labels = labels[0:features.shape[1]]\n    class_priors = np.zeros((n_comps,))\n    for (i, u_label) in enumerate(unique_labels):\n        class_priors[i] = np.count_nonzero(labels == u_label)\n    class_priors = class_priors / class_priors.sum()\n    transmutation_matrix = np.zeros((n_comps, n_comps))\n    for i in range(labels.shape[0] - 1):\n        transmutation_matrix[int(labels[i]), int(labels[i + 1])] += 1\n    for i in range(n_comps):\n        transmutation_matrix[i, :] /= transmutation_matrix[i, :].sum()\n    means = np.zeros((n_comps, n_feats))\n    for i in range(n_comps):\n        means[i, :] = np.array(features[:, np.nonzero(labels == unique_labels[i])[0]].mean(axis=1))\n    cov = np.zeros((n_comps, n_feats))\n    for i in range(n_comps):\n        '\\n        cov[i, :, :] = np.cov(features[:, np.nonzero(labels == u_labels[i])[0]])\\n        '\n        cov[i, :] = np.std(features[:, np.nonzero(labels == unique_labels[i])[0]], axis=1)\n    return (class_priors, transmutation_matrix, means, cov)"
        ]
    },
    {
        "func_name": "train_hmm_from_file",
        "original": "def train_hmm_from_file(wav_file, gt_file, hmm_model_name, mid_window, mid_step):\n    \"\"\"\n    This function trains a HMM model for segmentation-classification\n    using a single annotated audio file\n    ARGUMENTS:\n     - wav_file:        the path of the audio filename\n     - gt_file:         the path of the ground truth filename\n                       (a csv file of the form <segment start in seconds>,\n                       <segment end in seconds>,<segment label> in each row\n     - hmm_model_name:   the name of the HMM model to be stored\n     - mt_win:          mid-term window size\n     - mt_step:         mid-term window step\n    RETURNS:\n     - hmm:            an object to the resulting HMM\n     - class_names:     a list of class_names\n\n    After training, hmm, class_names, along with the mt_win and mt_step\n    values are stored in the hmm_model_name file\n    \"\"\"\n    (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n    (flags, class_names) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(wav_file)\n    (features, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    (class_priors, transumation_matrix, means, cov) = train_hmm_compute_statistics(features, flags)\n    hmm = hmmlearn.hmm.GaussianHMM(class_priors.shape[0], 'diag')\n    hmm.covars_ = cov\n    hmm.means_ = means\n    hmm.startprob_ = class_priors\n    hmm.transmat_ = transumation_matrix\n    save_hmm(hmm_model_name, hmm, class_names, mid_window, mid_step)\n    return (hmm, class_names)",
        "mutated": [
            "def train_hmm_from_file(wav_file, gt_file, hmm_model_name, mid_window, mid_step):\n    if False:\n        i = 10\n    '\\n    This function trains a HMM model for segmentation-classification\\n    using a single annotated audio file\\n    ARGUMENTS:\\n     - wav_file:        the path of the audio filename\\n     - gt_file:         the path of the ground truth filename\\n                       (a csv file of the form <segment start in seconds>,\\n                       <segment end in seconds>,<segment label> in each row\\n     - hmm_model_name:   the name of the HMM model to be stored\\n     - mt_win:          mid-term window size\\n     - mt_step:         mid-term window step\\n    RETURNS:\\n     - hmm:            an object to the resulting HMM\\n     - class_names:     a list of class_names\\n\\n    After training, hmm, class_names, along with the mt_win and mt_step\\n    values are stored in the hmm_model_name file\\n    '\n    (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n    (flags, class_names) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(wav_file)\n    (features, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    (class_priors, transumation_matrix, means, cov) = train_hmm_compute_statistics(features, flags)\n    hmm = hmmlearn.hmm.GaussianHMM(class_priors.shape[0], 'diag')\n    hmm.covars_ = cov\n    hmm.means_ = means\n    hmm.startprob_ = class_priors\n    hmm.transmat_ = transumation_matrix\n    save_hmm(hmm_model_name, hmm, class_names, mid_window, mid_step)\n    return (hmm, class_names)",
            "def train_hmm_from_file(wav_file, gt_file, hmm_model_name, mid_window, mid_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    This function trains a HMM model for segmentation-classification\\n    using a single annotated audio file\\n    ARGUMENTS:\\n     - wav_file:        the path of the audio filename\\n     - gt_file:         the path of the ground truth filename\\n                       (a csv file of the form <segment start in seconds>,\\n                       <segment end in seconds>,<segment label> in each row\\n     - hmm_model_name:   the name of the HMM model to be stored\\n     - mt_win:          mid-term window size\\n     - mt_step:         mid-term window step\\n    RETURNS:\\n     - hmm:            an object to the resulting HMM\\n     - class_names:     a list of class_names\\n\\n    After training, hmm, class_names, along with the mt_win and mt_step\\n    values are stored in the hmm_model_name file\\n    '\n    (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n    (flags, class_names) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(wav_file)\n    (features, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    (class_priors, transumation_matrix, means, cov) = train_hmm_compute_statistics(features, flags)\n    hmm = hmmlearn.hmm.GaussianHMM(class_priors.shape[0], 'diag')\n    hmm.covars_ = cov\n    hmm.means_ = means\n    hmm.startprob_ = class_priors\n    hmm.transmat_ = transumation_matrix\n    save_hmm(hmm_model_name, hmm, class_names, mid_window, mid_step)\n    return (hmm, class_names)",
            "def train_hmm_from_file(wav_file, gt_file, hmm_model_name, mid_window, mid_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    This function trains a HMM model for segmentation-classification\\n    using a single annotated audio file\\n    ARGUMENTS:\\n     - wav_file:        the path of the audio filename\\n     - gt_file:         the path of the ground truth filename\\n                       (a csv file of the form <segment start in seconds>,\\n                       <segment end in seconds>,<segment label> in each row\\n     - hmm_model_name:   the name of the HMM model to be stored\\n     - mt_win:          mid-term window size\\n     - mt_step:         mid-term window step\\n    RETURNS:\\n     - hmm:            an object to the resulting HMM\\n     - class_names:     a list of class_names\\n\\n    After training, hmm, class_names, along with the mt_win and mt_step\\n    values are stored in the hmm_model_name file\\n    '\n    (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n    (flags, class_names) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(wav_file)\n    (features, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    (class_priors, transumation_matrix, means, cov) = train_hmm_compute_statistics(features, flags)\n    hmm = hmmlearn.hmm.GaussianHMM(class_priors.shape[0], 'diag')\n    hmm.covars_ = cov\n    hmm.means_ = means\n    hmm.startprob_ = class_priors\n    hmm.transmat_ = transumation_matrix\n    save_hmm(hmm_model_name, hmm, class_names, mid_window, mid_step)\n    return (hmm, class_names)",
            "def train_hmm_from_file(wav_file, gt_file, hmm_model_name, mid_window, mid_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    This function trains a HMM model for segmentation-classification\\n    using a single annotated audio file\\n    ARGUMENTS:\\n     - wav_file:        the path of the audio filename\\n     - gt_file:         the path of the ground truth filename\\n                       (a csv file of the form <segment start in seconds>,\\n                       <segment end in seconds>,<segment label> in each row\\n     - hmm_model_name:   the name of the HMM model to be stored\\n     - mt_win:          mid-term window size\\n     - mt_step:         mid-term window step\\n    RETURNS:\\n     - hmm:            an object to the resulting HMM\\n     - class_names:     a list of class_names\\n\\n    After training, hmm, class_names, along with the mt_win and mt_step\\n    values are stored in the hmm_model_name file\\n    '\n    (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n    (flags, class_names) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(wav_file)\n    (features, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    (class_priors, transumation_matrix, means, cov) = train_hmm_compute_statistics(features, flags)\n    hmm = hmmlearn.hmm.GaussianHMM(class_priors.shape[0], 'diag')\n    hmm.covars_ = cov\n    hmm.means_ = means\n    hmm.startprob_ = class_priors\n    hmm.transmat_ = transumation_matrix\n    save_hmm(hmm_model_name, hmm, class_names, mid_window, mid_step)\n    return (hmm, class_names)",
            "def train_hmm_from_file(wav_file, gt_file, hmm_model_name, mid_window, mid_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    This function trains a HMM model for segmentation-classification\\n    using a single annotated audio file\\n    ARGUMENTS:\\n     - wav_file:        the path of the audio filename\\n     - gt_file:         the path of the ground truth filename\\n                       (a csv file of the form <segment start in seconds>,\\n                       <segment end in seconds>,<segment label> in each row\\n     - hmm_model_name:   the name of the HMM model to be stored\\n     - mt_win:          mid-term window size\\n     - mt_step:         mid-term window step\\n    RETURNS:\\n     - hmm:            an object to the resulting HMM\\n     - class_names:     a list of class_names\\n\\n    After training, hmm, class_names, along with the mt_win and mt_step\\n    values are stored in the hmm_model_name file\\n    '\n    (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n    (flags, class_names) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(wav_file)\n    (features, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    (class_priors, transumation_matrix, means, cov) = train_hmm_compute_statistics(features, flags)\n    hmm = hmmlearn.hmm.GaussianHMM(class_priors.shape[0], 'diag')\n    hmm.covars_ = cov\n    hmm.means_ = means\n    hmm.startprob_ = class_priors\n    hmm.transmat_ = transumation_matrix\n    save_hmm(hmm_model_name, hmm, class_names, mid_window, mid_step)\n    return (hmm, class_names)"
        ]
    },
    {
        "func_name": "train_hmm_from_directory",
        "original": "def train_hmm_from_directory(folder_path, hmm_model_name, mid_window, mid_step):\n    \"\"\"\n    This function trains a HMM model for segmentation-classification using\n    a where WAV files and .segment (ground-truth files) are stored\n    ARGUMENTS:\n     - folder_path:     the path of the data diretory\n     - hmm_model_name:  the name of the HMM model to be stored\n     - mt_win:          mid-term window size\n     - mt_step:         mid-term window step\n    RETURNS:\n     - hmm:            an object to the resulting HMM\n     - class_names:    a list of class_names\n\n    After training, hmm, class_names, along with the mt_win\n    and mt_step values are stored in the hmm_model_name file\n    \"\"\"\n    flags_all = np.array([])\n    class_names_all = []\n    for (i, f) in enumerate(glob.glob(folder_path + os.sep + '*.wav')):\n        wav_file = f\n        gt_file = f.replace('.wav', '.segments')\n        if os.path.isfile(gt_file):\n            (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n            (flags, class_names) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n            for c in class_names:\n                if c not in class_names_all:\n                    class_names_all.append(c)\n            (sampling_rate, signal) = audioBasicIO.read_audio_file(wav_file)\n            (feature_vector, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n            flag_len = len(flags)\n            feat_cols = feature_vector.shape[1]\n            min_sm = min(feat_cols, flag_len)\n            feature_vector = feature_vector[:, 0:min_sm]\n            flags = flags[0:min_sm]\n            flags_new = []\n            for (j, fl) in enumerate(flags):\n                flags_new.append(class_names_all.index(class_names_all[flags[j]]))\n            flags_all = np.append(flags_all, np.array(flags_new))\n            if i == 0:\n                f_all = feature_vector\n            else:\n                f_all = np.concatenate((f_all, feature_vector), axis=1)\n    (class_priors, transmutation_matrix, means, cov) = train_hmm_compute_statistics(f_all, flags_all)\n    hmm = hmmlearn.hmm.GaussianHMM(class_priors.shape[0], 'diag')\n    hmm.covars_ = cov\n    hmm.means_ = means\n    hmm.startprob_ = class_priors\n    hmm.transmat_ = transmutation_matrix\n    save_hmm(hmm_model_name, hmm, class_names_all, mid_window, mid_step)\n    return (hmm, class_names_all)",
        "mutated": [
            "def train_hmm_from_directory(folder_path, hmm_model_name, mid_window, mid_step):\n    if False:\n        i = 10\n    '\\n    This function trains a HMM model for segmentation-classification using\\n    a where WAV files and .segment (ground-truth files) are stored\\n    ARGUMENTS:\\n     - folder_path:     the path of the data diretory\\n     - hmm_model_name:  the name of the HMM model to be stored\\n     - mt_win:          mid-term window size\\n     - mt_step:         mid-term window step\\n    RETURNS:\\n     - hmm:            an object to the resulting HMM\\n     - class_names:    a list of class_names\\n\\n    After training, hmm, class_names, along with the mt_win\\n    and mt_step values are stored in the hmm_model_name file\\n    '\n    flags_all = np.array([])\n    class_names_all = []\n    for (i, f) in enumerate(glob.glob(folder_path + os.sep + '*.wav')):\n        wav_file = f\n        gt_file = f.replace('.wav', '.segments')\n        if os.path.isfile(gt_file):\n            (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n            (flags, class_names) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n            for c in class_names:\n                if c not in class_names_all:\n                    class_names_all.append(c)\n            (sampling_rate, signal) = audioBasicIO.read_audio_file(wav_file)\n            (feature_vector, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n            flag_len = len(flags)\n            feat_cols = feature_vector.shape[1]\n            min_sm = min(feat_cols, flag_len)\n            feature_vector = feature_vector[:, 0:min_sm]\n            flags = flags[0:min_sm]\n            flags_new = []\n            for (j, fl) in enumerate(flags):\n                flags_new.append(class_names_all.index(class_names_all[flags[j]]))\n            flags_all = np.append(flags_all, np.array(flags_new))\n            if i == 0:\n                f_all = feature_vector\n            else:\n                f_all = np.concatenate((f_all, feature_vector), axis=1)\n    (class_priors, transmutation_matrix, means, cov) = train_hmm_compute_statistics(f_all, flags_all)\n    hmm = hmmlearn.hmm.GaussianHMM(class_priors.shape[0], 'diag')\n    hmm.covars_ = cov\n    hmm.means_ = means\n    hmm.startprob_ = class_priors\n    hmm.transmat_ = transmutation_matrix\n    save_hmm(hmm_model_name, hmm, class_names_all, mid_window, mid_step)\n    return (hmm, class_names_all)",
            "def train_hmm_from_directory(folder_path, hmm_model_name, mid_window, mid_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    This function trains a HMM model for segmentation-classification using\\n    a where WAV files and .segment (ground-truth files) are stored\\n    ARGUMENTS:\\n     - folder_path:     the path of the data diretory\\n     - hmm_model_name:  the name of the HMM model to be stored\\n     - mt_win:          mid-term window size\\n     - mt_step:         mid-term window step\\n    RETURNS:\\n     - hmm:            an object to the resulting HMM\\n     - class_names:    a list of class_names\\n\\n    After training, hmm, class_names, along with the mt_win\\n    and mt_step values are stored in the hmm_model_name file\\n    '\n    flags_all = np.array([])\n    class_names_all = []\n    for (i, f) in enumerate(glob.glob(folder_path + os.sep + '*.wav')):\n        wav_file = f\n        gt_file = f.replace('.wav', '.segments')\n        if os.path.isfile(gt_file):\n            (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n            (flags, class_names) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n            for c in class_names:\n                if c not in class_names_all:\n                    class_names_all.append(c)\n            (sampling_rate, signal) = audioBasicIO.read_audio_file(wav_file)\n            (feature_vector, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n            flag_len = len(flags)\n            feat_cols = feature_vector.shape[1]\n            min_sm = min(feat_cols, flag_len)\n            feature_vector = feature_vector[:, 0:min_sm]\n            flags = flags[0:min_sm]\n            flags_new = []\n            for (j, fl) in enumerate(flags):\n                flags_new.append(class_names_all.index(class_names_all[flags[j]]))\n            flags_all = np.append(flags_all, np.array(flags_new))\n            if i == 0:\n                f_all = feature_vector\n            else:\n                f_all = np.concatenate((f_all, feature_vector), axis=1)\n    (class_priors, transmutation_matrix, means, cov) = train_hmm_compute_statistics(f_all, flags_all)\n    hmm = hmmlearn.hmm.GaussianHMM(class_priors.shape[0], 'diag')\n    hmm.covars_ = cov\n    hmm.means_ = means\n    hmm.startprob_ = class_priors\n    hmm.transmat_ = transmutation_matrix\n    save_hmm(hmm_model_name, hmm, class_names_all, mid_window, mid_step)\n    return (hmm, class_names_all)",
            "def train_hmm_from_directory(folder_path, hmm_model_name, mid_window, mid_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    This function trains a HMM model for segmentation-classification using\\n    a where WAV files and .segment (ground-truth files) are stored\\n    ARGUMENTS:\\n     - folder_path:     the path of the data diretory\\n     - hmm_model_name:  the name of the HMM model to be stored\\n     - mt_win:          mid-term window size\\n     - mt_step:         mid-term window step\\n    RETURNS:\\n     - hmm:            an object to the resulting HMM\\n     - class_names:    a list of class_names\\n\\n    After training, hmm, class_names, along with the mt_win\\n    and mt_step values are stored in the hmm_model_name file\\n    '\n    flags_all = np.array([])\n    class_names_all = []\n    for (i, f) in enumerate(glob.glob(folder_path + os.sep + '*.wav')):\n        wav_file = f\n        gt_file = f.replace('.wav', '.segments')\n        if os.path.isfile(gt_file):\n            (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n            (flags, class_names) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n            for c in class_names:\n                if c not in class_names_all:\n                    class_names_all.append(c)\n            (sampling_rate, signal) = audioBasicIO.read_audio_file(wav_file)\n            (feature_vector, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n            flag_len = len(flags)\n            feat_cols = feature_vector.shape[1]\n            min_sm = min(feat_cols, flag_len)\n            feature_vector = feature_vector[:, 0:min_sm]\n            flags = flags[0:min_sm]\n            flags_new = []\n            for (j, fl) in enumerate(flags):\n                flags_new.append(class_names_all.index(class_names_all[flags[j]]))\n            flags_all = np.append(flags_all, np.array(flags_new))\n            if i == 0:\n                f_all = feature_vector\n            else:\n                f_all = np.concatenate((f_all, feature_vector), axis=1)\n    (class_priors, transmutation_matrix, means, cov) = train_hmm_compute_statistics(f_all, flags_all)\n    hmm = hmmlearn.hmm.GaussianHMM(class_priors.shape[0], 'diag')\n    hmm.covars_ = cov\n    hmm.means_ = means\n    hmm.startprob_ = class_priors\n    hmm.transmat_ = transmutation_matrix\n    save_hmm(hmm_model_name, hmm, class_names_all, mid_window, mid_step)\n    return (hmm, class_names_all)",
            "def train_hmm_from_directory(folder_path, hmm_model_name, mid_window, mid_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    This function trains a HMM model for segmentation-classification using\\n    a where WAV files and .segment (ground-truth files) are stored\\n    ARGUMENTS:\\n     - folder_path:     the path of the data diretory\\n     - hmm_model_name:  the name of the HMM model to be stored\\n     - mt_win:          mid-term window size\\n     - mt_step:         mid-term window step\\n    RETURNS:\\n     - hmm:            an object to the resulting HMM\\n     - class_names:    a list of class_names\\n\\n    After training, hmm, class_names, along with the mt_win\\n    and mt_step values are stored in the hmm_model_name file\\n    '\n    flags_all = np.array([])\n    class_names_all = []\n    for (i, f) in enumerate(glob.glob(folder_path + os.sep + '*.wav')):\n        wav_file = f\n        gt_file = f.replace('.wav', '.segments')\n        if os.path.isfile(gt_file):\n            (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n            (flags, class_names) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n            for c in class_names:\n                if c not in class_names_all:\n                    class_names_all.append(c)\n            (sampling_rate, signal) = audioBasicIO.read_audio_file(wav_file)\n            (feature_vector, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n            flag_len = len(flags)\n            feat_cols = feature_vector.shape[1]\n            min_sm = min(feat_cols, flag_len)\n            feature_vector = feature_vector[:, 0:min_sm]\n            flags = flags[0:min_sm]\n            flags_new = []\n            for (j, fl) in enumerate(flags):\n                flags_new.append(class_names_all.index(class_names_all[flags[j]]))\n            flags_all = np.append(flags_all, np.array(flags_new))\n            if i == 0:\n                f_all = feature_vector\n            else:\n                f_all = np.concatenate((f_all, feature_vector), axis=1)\n    (class_priors, transmutation_matrix, means, cov) = train_hmm_compute_statistics(f_all, flags_all)\n    hmm = hmmlearn.hmm.GaussianHMM(class_priors.shape[0], 'diag')\n    hmm.covars_ = cov\n    hmm.means_ = means\n    hmm.startprob_ = class_priors\n    hmm.transmat_ = transmutation_matrix\n    save_hmm(hmm_model_name, hmm, class_names_all, mid_window, mid_step)\n    return (hmm, class_names_all)",
            "def train_hmm_from_directory(folder_path, hmm_model_name, mid_window, mid_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    This function trains a HMM model for segmentation-classification using\\n    a where WAV files and .segment (ground-truth files) are stored\\n    ARGUMENTS:\\n     - folder_path:     the path of the data diretory\\n     - hmm_model_name:  the name of the HMM model to be stored\\n     - mt_win:          mid-term window size\\n     - mt_step:         mid-term window step\\n    RETURNS:\\n     - hmm:            an object to the resulting HMM\\n     - class_names:    a list of class_names\\n\\n    After training, hmm, class_names, along with the mt_win\\n    and mt_step values are stored in the hmm_model_name file\\n    '\n    flags_all = np.array([])\n    class_names_all = []\n    for (i, f) in enumerate(glob.glob(folder_path + os.sep + '*.wav')):\n        wav_file = f\n        gt_file = f.replace('.wav', '.segments')\n        if os.path.isfile(gt_file):\n            (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n            (flags, class_names) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n            for c in class_names:\n                if c not in class_names_all:\n                    class_names_all.append(c)\n            (sampling_rate, signal) = audioBasicIO.read_audio_file(wav_file)\n            (feature_vector, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n            flag_len = len(flags)\n            feat_cols = feature_vector.shape[1]\n            min_sm = min(feat_cols, flag_len)\n            feature_vector = feature_vector[:, 0:min_sm]\n            flags = flags[0:min_sm]\n            flags_new = []\n            for (j, fl) in enumerate(flags):\n                flags_new.append(class_names_all.index(class_names_all[flags[j]]))\n            flags_all = np.append(flags_all, np.array(flags_new))\n            if i == 0:\n                f_all = feature_vector\n            else:\n                f_all = np.concatenate((f_all, feature_vector), axis=1)\n    (class_priors, transmutation_matrix, means, cov) = train_hmm_compute_statistics(f_all, flags_all)\n    hmm = hmmlearn.hmm.GaussianHMM(class_priors.shape[0], 'diag')\n    hmm.covars_ = cov\n    hmm.means_ = means\n    hmm.startprob_ = class_priors\n    hmm.transmat_ = transmutation_matrix\n    save_hmm(hmm_model_name, hmm, class_names_all, mid_window, mid_step)\n    return (hmm, class_names_all)"
        ]
    },
    {
        "func_name": "save_hmm",
        "original": "def save_hmm(hmm_model_name, model, classes, mid_window, mid_step):\n    \"\"\"Save HMM model\"\"\"\n    with open(hmm_model_name, 'wb') as f_handle:\n        cpickle.dump(model, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(classes, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(mid_window, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(mid_step, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)",
        "mutated": [
            "def save_hmm(hmm_model_name, model, classes, mid_window, mid_step):\n    if False:\n        i = 10\n    'Save HMM model'\n    with open(hmm_model_name, 'wb') as f_handle:\n        cpickle.dump(model, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(classes, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(mid_window, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(mid_step, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)",
            "def save_hmm(hmm_model_name, model, classes, mid_window, mid_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Save HMM model'\n    with open(hmm_model_name, 'wb') as f_handle:\n        cpickle.dump(model, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(classes, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(mid_window, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(mid_step, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)",
            "def save_hmm(hmm_model_name, model, classes, mid_window, mid_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Save HMM model'\n    with open(hmm_model_name, 'wb') as f_handle:\n        cpickle.dump(model, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(classes, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(mid_window, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(mid_step, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)",
            "def save_hmm(hmm_model_name, model, classes, mid_window, mid_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Save HMM model'\n    with open(hmm_model_name, 'wb') as f_handle:\n        cpickle.dump(model, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(classes, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(mid_window, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(mid_step, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)",
            "def save_hmm(hmm_model_name, model, classes, mid_window, mid_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Save HMM model'\n    with open(hmm_model_name, 'wb') as f_handle:\n        cpickle.dump(model, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(classes, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(mid_window, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)\n        cpickle.dump(mid_step, f_handle, protocol=cpickle.HIGHEST_PROTOCOL)"
        ]
    },
    {
        "func_name": "hmm_segmentation",
        "original": "def hmm_segmentation(audio_file, hmm_model_name, plot_results=False, gt_file=''):\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(audio_file)\n    with open(hmm_model_name, 'rb') as f_handle:\n        hmm = cpickle.load(f_handle)\n        class_names = cpickle.load(f_handle)\n        mid_window = cpickle.load(f_handle)\n        mid_step = cpickle.load(f_handle)\n    (features, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    labels = hmm.predict(features.T)\n    (labels_gt, class_names_gt, accuracy, cm) = load_ground_truth(gt_file, labels, class_names, mid_step, plot_results)\n    return (labels, class_names, accuracy, cm)",
        "mutated": [
            "def hmm_segmentation(audio_file, hmm_model_name, plot_results=False, gt_file=''):\n    if False:\n        i = 10\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(audio_file)\n    with open(hmm_model_name, 'rb') as f_handle:\n        hmm = cpickle.load(f_handle)\n        class_names = cpickle.load(f_handle)\n        mid_window = cpickle.load(f_handle)\n        mid_step = cpickle.load(f_handle)\n    (features, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    labels = hmm.predict(features.T)\n    (labels_gt, class_names_gt, accuracy, cm) = load_ground_truth(gt_file, labels, class_names, mid_step, plot_results)\n    return (labels, class_names, accuracy, cm)",
            "def hmm_segmentation(audio_file, hmm_model_name, plot_results=False, gt_file=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(audio_file)\n    with open(hmm_model_name, 'rb') as f_handle:\n        hmm = cpickle.load(f_handle)\n        class_names = cpickle.load(f_handle)\n        mid_window = cpickle.load(f_handle)\n        mid_step = cpickle.load(f_handle)\n    (features, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    labels = hmm.predict(features.T)\n    (labels_gt, class_names_gt, accuracy, cm) = load_ground_truth(gt_file, labels, class_names, mid_step, plot_results)\n    return (labels, class_names, accuracy, cm)",
            "def hmm_segmentation(audio_file, hmm_model_name, plot_results=False, gt_file=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(audio_file)\n    with open(hmm_model_name, 'rb') as f_handle:\n        hmm = cpickle.load(f_handle)\n        class_names = cpickle.load(f_handle)\n        mid_window = cpickle.load(f_handle)\n        mid_step = cpickle.load(f_handle)\n    (features, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    labels = hmm.predict(features.T)\n    (labels_gt, class_names_gt, accuracy, cm) = load_ground_truth(gt_file, labels, class_names, mid_step, plot_results)\n    return (labels, class_names, accuracy, cm)",
            "def hmm_segmentation(audio_file, hmm_model_name, plot_results=False, gt_file=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(audio_file)\n    with open(hmm_model_name, 'rb') as f_handle:\n        hmm = cpickle.load(f_handle)\n        class_names = cpickle.load(f_handle)\n        mid_window = cpickle.load(f_handle)\n        mid_step = cpickle.load(f_handle)\n    (features, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    labels = hmm.predict(features.T)\n    (labels_gt, class_names_gt, accuracy, cm) = load_ground_truth(gt_file, labels, class_names, mid_step, plot_results)\n    return (labels, class_names, accuracy, cm)",
            "def hmm_segmentation(audio_file, hmm_model_name, plot_results=False, gt_file=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(audio_file)\n    with open(hmm_model_name, 'rb') as f_handle:\n        hmm = cpickle.load(f_handle)\n        class_names = cpickle.load(f_handle)\n        mid_window = cpickle.load(f_handle)\n        mid_step = cpickle.load(f_handle)\n    (features, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    labels = hmm.predict(features.T)\n    (labels_gt, class_names_gt, accuracy, cm) = load_ground_truth(gt_file, labels, class_names, mid_step, plot_results)\n    return (labels, class_names, accuracy, cm)"
        ]
    },
    {
        "func_name": "load_ground_truth_segments",
        "original": "def load_ground_truth_segments(gt_file, mt_step):\n    (seg_start, seg_end, seg_labels) = read_segmentation_gt(gt_file)\n    (labels, class_names) = segments_to_labels(seg_start, seg_end, seg_labels, mt_step)\n    labels_temp = []\n    for (index, label) in enumerate(labels):\n        if class_names[labels[index]] in class_names:\n            labels_temp.append(class_names.index(class_names[labels[index]]))\n        else:\n            labels_temp.append(-1)\n    labels = np.array(labels_temp)\n    return (labels, class_names)",
        "mutated": [
            "def load_ground_truth_segments(gt_file, mt_step):\n    if False:\n        i = 10\n    (seg_start, seg_end, seg_labels) = read_segmentation_gt(gt_file)\n    (labels, class_names) = segments_to_labels(seg_start, seg_end, seg_labels, mt_step)\n    labels_temp = []\n    for (index, label) in enumerate(labels):\n        if class_names[labels[index]] in class_names:\n            labels_temp.append(class_names.index(class_names[labels[index]]))\n        else:\n            labels_temp.append(-1)\n    labels = np.array(labels_temp)\n    return (labels, class_names)",
            "def load_ground_truth_segments(gt_file, mt_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (seg_start, seg_end, seg_labels) = read_segmentation_gt(gt_file)\n    (labels, class_names) = segments_to_labels(seg_start, seg_end, seg_labels, mt_step)\n    labels_temp = []\n    for (index, label) in enumerate(labels):\n        if class_names[labels[index]] in class_names:\n            labels_temp.append(class_names.index(class_names[labels[index]]))\n        else:\n            labels_temp.append(-1)\n    labels = np.array(labels_temp)\n    return (labels, class_names)",
            "def load_ground_truth_segments(gt_file, mt_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (seg_start, seg_end, seg_labels) = read_segmentation_gt(gt_file)\n    (labels, class_names) = segments_to_labels(seg_start, seg_end, seg_labels, mt_step)\n    labels_temp = []\n    for (index, label) in enumerate(labels):\n        if class_names[labels[index]] in class_names:\n            labels_temp.append(class_names.index(class_names[labels[index]]))\n        else:\n            labels_temp.append(-1)\n    labels = np.array(labels_temp)\n    return (labels, class_names)",
            "def load_ground_truth_segments(gt_file, mt_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (seg_start, seg_end, seg_labels) = read_segmentation_gt(gt_file)\n    (labels, class_names) = segments_to_labels(seg_start, seg_end, seg_labels, mt_step)\n    labels_temp = []\n    for (index, label) in enumerate(labels):\n        if class_names[labels[index]] in class_names:\n            labels_temp.append(class_names.index(class_names[labels[index]]))\n        else:\n            labels_temp.append(-1)\n    labels = np.array(labels_temp)\n    return (labels, class_names)",
            "def load_ground_truth_segments(gt_file, mt_step):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (seg_start, seg_end, seg_labels) = read_segmentation_gt(gt_file)\n    (labels, class_names) = segments_to_labels(seg_start, seg_end, seg_labels, mt_step)\n    labels_temp = []\n    for (index, label) in enumerate(labels):\n        if class_names[labels[index]] in class_names:\n            labels_temp.append(class_names.index(class_names[labels[index]]))\n        else:\n            labels_temp.append(-1)\n    labels = np.array(labels_temp)\n    return (labels, class_names)"
        ]
    },
    {
        "func_name": "calculate_confusion_matrix",
        "original": "def calculate_confusion_matrix(predictions, ground_truth, classes):\n    cm = np.zeros((len(classes), len(classes)))\n    for index in range(min(predictions.shape[0], ground_truth.shape[0])):\n        cm[int(ground_truth[index]), int(predictions[index])] += 1\n    return cm",
        "mutated": [
            "def calculate_confusion_matrix(predictions, ground_truth, classes):\n    if False:\n        i = 10\n    cm = np.zeros((len(classes), len(classes)))\n    for index in range(min(predictions.shape[0], ground_truth.shape[0])):\n        cm[int(ground_truth[index]), int(predictions[index])] += 1\n    return cm",
            "def calculate_confusion_matrix(predictions, ground_truth, classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    cm = np.zeros((len(classes), len(classes)))\n    for index in range(min(predictions.shape[0], ground_truth.shape[0])):\n        cm[int(ground_truth[index]), int(predictions[index])] += 1\n    return cm",
            "def calculate_confusion_matrix(predictions, ground_truth, classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    cm = np.zeros((len(classes), len(classes)))\n    for index in range(min(predictions.shape[0], ground_truth.shape[0])):\n        cm[int(ground_truth[index]), int(predictions[index])] += 1\n    return cm",
            "def calculate_confusion_matrix(predictions, ground_truth, classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    cm = np.zeros((len(classes), len(classes)))\n    for index in range(min(predictions.shape[0], ground_truth.shape[0])):\n        cm[int(ground_truth[index]), int(predictions[index])] += 1\n    return cm",
            "def calculate_confusion_matrix(predictions, ground_truth, classes):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    cm = np.zeros((len(classes), len(classes)))\n    for index in range(min(predictions.shape[0], ground_truth.shape[0])):\n        cm[int(ground_truth[index]), int(predictions[index])] += 1\n    return cm"
        ]
    },
    {
        "func_name": "mid_term_file_classification",
        "original": "def mid_term_file_classification(input_file, model_name, model_type, plot_results=False, gt_file=''):\n    \"\"\"\n    This function performs mid-term classification of an audio stream.\n    Towards this end, supervised knowledge is used,\n    i.e. a pre-trained classifier.\n    ARGUMENTS:\n        - input_file:        path of the input WAV file\n        - model_name:        name of the classification model\n        - model_type:        svm or knn depending on the classifier type\n        - plot_results:      True if results are to be plotted using\n                             matplotlib along with a set of statistics\n        - gt_file:           path to the ground truth file, if exists, \n                             for calculating classification performance\n    RETURNS:\n    labels, class_names, accuracy, cm\n          - labels:         a sequence of segment's labels: segs[i] is the label\n                            of the i-th segment\n          - class_names:    a string sequence of class_names used in classification:\n                            class_names[i] is the name of classes[i]\n          - accuracy:       the accuracy of the classification.\n          - cm:             the confusion matrix of this classification\n    \"\"\"\n    labels = []\n    accuracy = 0.0\n    class_names = []\n    cm = np.array([])\n    if not os.path.isfile(model_name):\n        print('mtFileClassificationError: input model_type not found!')\n        return (labels, class_names, accuracy, cm)\n    if model_type == 'knn':\n        (classifier, mean, std, class_names, mt_win, mid_step, st_win, st_step, compute_beat) = at.load_model_knn(model_name)\n    else:\n        (classifier, mean, std, class_names, mt_win, mid_step, st_win, st_step, compute_beat) = at.load_model(model_name)\n    if compute_beat:\n        print('Model ' + model_name + ' contains long-term music features (beat etc) and cannot be used in segmentation')\n        return (labels, class_names, accuracy, cm)\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(input_file)\n    if sampling_rate == 0:\n        return (labels, class_names, accuracy, cm)\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (mt_feats, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mt_win * sampling_rate, mid_step * sampling_rate, round(sampling_rate * st_win), round(sampling_rate * st_step))\n    posterior_matrix = []\n    for col_index in range(mt_feats.shape[1]):\n        feature_vector = (mt_feats[:, col_index] - mean) / std\n        (label_predicted, posterior) = at.classifier_wrapper(classifier, model_type, feature_vector)\n        labels.append(label_predicted)\n        posterior_matrix.append(np.max(posterior))\n    labels = np.array(labels)\n    (segs, classes) = labels_to_segments(labels, mid_step)\n    for i in range(len(segs)):\n        print(segs[i], classes[i])\n    segs[-1] = len(signal) / float(sampling_rate)\n    (labels_gt, class_names_gt, accuracy, cm) = load_ground_truth(gt_file, labels, class_names, mid_step, plot_results)\n    return (labels, class_names, accuracy, cm)",
        "mutated": [
            "def mid_term_file_classification(input_file, model_name, model_type, plot_results=False, gt_file=''):\n    if False:\n        i = 10\n    \"\\n    This function performs mid-term classification of an audio stream.\\n    Towards this end, supervised knowledge is used,\\n    i.e. a pre-trained classifier.\\n    ARGUMENTS:\\n        - input_file:        path of the input WAV file\\n        - model_name:        name of the classification model\\n        - model_type:        svm or knn depending on the classifier type\\n        - plot_results:      True if results are to be plotted using\\n                             matplotlib along with a set of statistics\\n        - gt_file:           path to the ground truth file, if exists, \\n                             for calculating classification performance\\n    RETURNS:\\n    labels, class_names, accuracy, cm\\n          - labels:         a sequence of segment's labels: segs[i] is the label\\n                            of the i-th segment\\n          - class_names:    a string sequence of class_names used in classification:\\n                            class_names[i] is the name of classes[i]\\n          - accuracy:       the accuracy of the classification.\\n          - cm:             the confusion matrix of this classification\\n    \"\n    labels = []\n    accuracy = 0.0\n    class_names = []\n    cm = np.array([])\n    if not os.path.isfile(model_name):\n        print('mtFileClassificationError: input model_type not found!')\n        return (labels, class_names, accuracy, cm)\n    if model_type == 'knn':\n        (classifier, mean, std, class_names, mt_win, mid_step, st_win, st_step, compute_beat) = at.load_model_knn(model_name)\n    else:\n        (classifier, mean, std, class_names, mt_win, mid_step, st_win, st_step, compute_beat) = at.load_model(model_name)\n    if compute_beat:\n        print('Model ' + model_name + ' contains long-term music features (beat etc) and cannot be used in segmentation')\n        return (labels, class_names, accuracy, cm)\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(input_file)\n    if sampling_rate == 0:\n        return (labels, class_names, accuracy, cm)\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (mt_feats, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mt_win * sampling_rate, mid_step * sampling_rate, round(sampling_rate * st_win), round(sampling_rate * st_step))\n    posterior_matrix = []\n    for col_index in range(mt_feats.shape[1]):\n        feature_vector = (mt_feats[:, col_index] - mean) / std\n        (label_predicted, posterior) = at.classifier_wrapper(classifier, model_type, feature_vector)\n        labels.append(label_predicted)\n        posterior_matrix.append(np.max(posterior))\n    labels = np.array(labels)\n    (segs, classes) = labels_to_segments(labels, mid_step)\n    for i in range(len(segs)):\n        print(segs[i], classes[i])\n    segs[-1] = len(signal) / float(sampling_rate)\n    (labels_gt, class_names_gt, accuracy, cm) = load_ground_truth(gt_file, labels, class_names, mid_step, plot_results)\n    return (labels, class_names, accuracy, cm)",
            "def mid_term_file_classification(input_file, model_name, model_type, plot_results=False, gt_file=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    This function performs mid-term classification of an audio stream.\\n    Towards this end, supervised knowledge is used,\\n    i.e. a pre-trained classifier.\\n    ARGUMENTS:\\n        - input_file:        path of the input WAV file\\n        - model_name:        name of the classification model\\n        - model_type:        svm or knn depending on the classifier type\\n        - plot_results:      True if results are to be plotted using\\n                             matplotlib along with a set of statistics\\n        - gt_file:           path to the ground truth file, if exists, \\n                             for calculating classification performance\\n    RETURNS:\\n    labels, class_names, accuracy, cm\\n          - labels:         a sequence of segment's labels: segs[i] is the label\\n                            of the i-th segment\\n          - class_names:    a string sequence of class_names used in classification:\\n                            class_names[i] is the name of classes[i]\\n          - accuracy:       the accuracy of the classification.\\n          - cm:             the confusion matrix of this classification\\n    \"\n    labels = []\n    accuracy = 0.0\n    class_names = []\n    cm = np.array([])\n    if not os.path.isfile(model_name):\n        print('mtFileClassificationError: input model_type not found!')\n        return (labels, class_names, accuracy, cm)\n    if model_type == 'knn':\n        (classifier, mean, std, class_names, mt_win, mid_step, st_win, st_step, compute_beat) = at.load_model_knn(model_name)\n    else:\n        (classifier, mean, std, class_names, mt_win, mid_step, st_win, st_step, compute_beat) = at.load_model(model_name)\n    if compute_beat:\n        print('Model ' + model_name + ' contains long-term music features (beat etc) and cannot be used in segmentation')\n        return (labels, class_names, accuracy, cm)\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(input_file)\n    if sampling_rate == 0:\n        return (labels, class_names, accuracy, cm)\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (mt_feats, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mt_win * sampling_rate, mid_step * sampling_rate, round(sampling_rate * st_win), round(sampling_rate * st_step))\n    posterior_matrix = []\n    for col_index in range(mt_feats.shape[1]):\n        feature_vector = (mt_feats[:, col_index] - mean) / std\n        (label_predicted, posterior) = at.classifier_wrapper(classifier, model_type, feature_vector)\n        labels.append(label_predicted)\n        posterior_matrix.append(np.max(posterior))\n    labels = np.array(labels)\n    (segs, classes) = labels_to_segments(labels, mid_step)\n    for i in range(len(segs)):\n        print(segs[i], classes[i])\n    segs[-1] = len(signal) / float(sampling_rate)\n    (labels_gt, class_names_gt, accuracy, cm) = load_ground_truth(gt_file, labels, class_names, mid_step, plot_results)\n    return (labels, class_names, accuracy, cm)",
            "def mid_term_file_classification(input_file, model_name, model_type, plot_results=False, gt_file=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    This function performs mid-term classification of an audio stream.\\n    Towards this end, supervised knowledge is used,\\n    i.e. a pre-trained classifier.\\n    ARGUMENTS:\\n        - input_file:        path of the input WAV file\\n        - model_name:        name of the classification model\\n        - model_type:        svm or knn depending on the classifier type\\n        - plot_results:      True if results are to be plotted using\\n                             matplotlib along with a set of statistics\\n        - gt_file:           path to the ground truth file, if exists, \\n                             for calculating classification performance\\n    RETURNS:\\n    labels, class_names, accuracy, cm\\n          - labels:         a sequence of segment's labels: segs[i] is the label\\n                            of the i-th segment\\n          - class_names:    a string sequence of class_names used in classification:\\n                            class_names[i] is the name of classes[i]\\n          - accuracy:       the accuracy of the classification.\\n          - cm:             the confusion matrix of this classification\\n    \"\n    labels = []\n    accuracy = 0.0\n    class_names = []\n    cm = np.array([])\n    if not os.path.isfile(model_name):\n        print('mtFileClassificationError: input model_type not found!')\n        return (labels, class_names, accuracy, cm)\n    if model_type == 'knn':\n        (classifier, mean, std, class_names, mt_win, mid_step, st_win, st_step, compute_beat) = at.load_model_knn(model_name)\n    else:\n        (classifier, mean, std, class_names, mt_win, mid_step, st_win, st_step, compute_beat) = at.load_model(model_name)\n    if compute_beat:\n        print('Model ' + model_name + ' contains long-term music features (beat etc) and cannot be used in segmentation')\n        return (labels, class_names, accuracy, cm)\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(input_file)\n    if sampling_rate == 0:\n        return (labels, class_names, accuracy, cm)\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (mt_feats, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mt_win * sampling_rate, mid_step * sampling_rate, round(sampling_rate * st_win), round(sampling_rate * st_step))\n    posterior_matrix = []\n    for col_index in range(mt_feats.shape[1]):\n        feature_vector = (mt_feats[:, col_index] - mean) / std\n        (label_predicted, posterior) = at.classifier_wrapper(classifier, model_type, feature_vector)\n        labels.append(label_predicted)\n        posterior_matrix.append(np.max(posterior))\n    labels = np.array(labels)\n    (segs, classes) = labels_to_segments(labels, mid_step)\n    for i in range(len(segs)):\n        print(segs[i], classes[i])\n    segs[-1] = len(signal) / float(sampling_rate)\n    (labels_gt, class_names_gt, accuracy, cm) = load_ground_truth(gt_file, labels, class_names, mid_step, plot_results)\n    return (labels, class_names, accuracy, cm)",
            "def mid_term_file_classification(input_file, model_name, model_type, plot_results=False, gt_file=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    This function performs mid-term classification of an audio stream.\\n    Towards this end, supervised knowledge is used,\\n    i.e. a pre-trained classifier.\\n    ARGUMENTS:\\n        - input_file:        path of the input WAV file\\n        - model_name:        name of the classification model\\n        - model_type:        svm or knn depending on the classifier type\\n        - plot_results:      True if results are to be plotted using\\n                             matplotlib along with a set of statistics\\n        - gt_file:           path to the ground truth file, if exists, \\n                             for calculating classification performance\\n    RETURNS:\\n    labels, class_names, accuracy, cm\\n          - labels:         a sequence of segment's labels: segs[i] is the label\\n                            of the i-th segment\\n          - class_names:    a string sequence of class_names used in classification:\\n                            class_names[i] is the name of classes[i]\\n          - accuracy:       the accuracy of the classification.\\n          - cm:             the confusion matrix of this classification\\n    \"\n    labels = []\n    accuracy = 0.0\n    class_names = []\n    cm = np.array([])\n    if not os.path.isfile(model_name):\n        print('mtFileClassificationError: input model_type not found!')\n        return (labels, class_names, accuracy, cm)\n    if model_type == 'knn':\n        (classifier, mean, std, class_names, mt_win, mid_step, st_win, st_step, compute_beat) = at.load_model_knn(model_name)\n    else:\n        (classifier, mean, std, class_names, mt_win, mid_step, st_win, st_step, compute_beat) = at.load_model(model_name)\n    if compute_beat:\n        print('Model ' + model_name + ' contains long-term music features (beat etc) and cannot be used in segmentation')\n        return (labels, class_names, accuracy, cm)\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(input_file)\n    if sampling_rate == 0:\n        return (labels, class_names, accuracy, cm)\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (mt_feats, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mt_win * sampling_rate, mid_step * sampling_rate, round(sampling_rate * st_win), round(sampling_rate * st_step))\n    posterior_matrix = []\n    for col_index in range(mt_feats.shape[1]):\n        feature_vector = (mt_feats[:, col_index] - mean) / std\n        (label_predicted, posterior) = at.classifier_wrapper(classifier, model_type, feature_vector)\n        labels.append(label_predicted)\n        posterior_matrix.append(np.max(posterior))\n    labels = np.array(labels)\n    (segs, classes) = labels_to_segments(labels, mid_step)\n    for i in range(len(segs)):\n        print(segs[i], classes[i])\n    segs[-1] = len(signal) / float(sampling_rate)\n    (labels_gt, class_names_gt, accuracy, cm) = load_ground_truth(gt_file, labels, class_names, mid_step, plot_results)\n    return (labels, class_names, accuracy, cm)",
            "def mid_term_file_classification(input_file, model_name, model_type, plot_results=False, gt_file=''):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    This function performs mid-term classification of an audio stream.\\n    Towards this end, supervised knowledge is used,\\n    i.e. a pre-trained classifier.\\n    ARGUMENTS:\\n        - input_file:        path of the input WAV file\\n        - model_name:        name of the classification model\\n        - model_type:        svm or knn depending on the classifier type\\n        - plot_results:      True if results are to be plotted using\\n                             matplotlib along with a set of statistics\\n        - gt_file:           path to the ground truth file, if exists, \\n                             for calculating classification performance\\n    RETURNS:\\n    labels, class_names, accuracy, cm\\n          - labels:         a sequence of segment's labels: segs[i] is the label\\n                            of the i-th segment\\n          - class_names:    a string sequence of class_names used in classification:\\n                            class_names[i] is the name of classes[i]\\n          - accuracy:       the accuracy of the classification.\\n          - cm:             the confusion matrix of this classification\\n    \"\n    labels = []\n    accuracy = 0.0\n    class_names = []\n    cm = np.array([])\n    if not os.path.isfile(model_name):\n        print('mtFileClassificationError: input model_type not found!')\n        return (labels, class_names, accuracy, cm)\n    if model_type == 'knn':\n        (classifier, mean, std, class_names, mt_win, mid_step, st_win, st_step, compute_beat) = at.load_model_knn(model_name)\n    else:\n        (classifier, mean, std, class_names, mt_win, mid_step, st_win, st_step, compute_beat) = at.load_model(model_name)\n    if compute_beat:\n        print('Model ' + model_name + ' contains long-term music features (beat etc) and cannot be used in segmentation')\n        return (labels, class_names, accuracy, cm)\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(input_file)\n    if sampling_rate == 0:\n        return (labels, class_names, accuracy, cm)\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (mt_feats, _, _) = mtf.mid_feature_extraction(signal, sampling_rate, mt_win * sampling_rate, mid_step * sampling_rate, round(sampling_rate * st_win), round(sampling_rate * st_step))\n    posterior_matrix = []\n    for col_index in range(mt_feats.shape[1]):\n        feature_vector = (mt_feats[:, col_index] - mean) / std\n        (label_predicted, posterior) = at.classifier_wrapper(classifier, model_type, feature_vector)\n        labels.append(label_predicted)\n        posterior_matrix.append(np.max(posterior))\n    labels = np.array(labels)\n    (segs, classes) = labels_to_segments(labels, mid_step)\n    for i in range(len(segs)):\n        print(segs[i], classes[i])\n    segs[-1] = len(signal) / float(sampling_rate)\n    (labels_gt, class_names_gt, accuracy, cm) = load_ground_truth(gt_file, labels, class_names, mid_step, plot_results)\n    return (labels, class_names, accuracy, cm)"
        ]
    },
    {
        "func_name": "load_ground_truth",
        "original": "def load_ground_truth(gt_file, labels, class_names, mid_step, plot_results):\n    accuracy = 0\n    cm = np.array([])\n    labels_gt = np.array([])\n    if os.path.isfile(gt_file):\n        (labels_gt, class_names_gt) = load_ground_truth_segments(gt_file, mid_step)\n        labels_new = []\n        for (il, l) in enumerate(labels):\n            if class_names[int(l)] in class_names_gt:\n                labels_new.append(class_names_gt.index(class_names[int(l)]))\n            else:\n                labels_new.append(-1)\n        labels_new = np.array(labels_new)\n        cm = calculate_confusion_matrix(labels_new, labels_gt, class_names_gt)\n        accuracy = plot_segmentation_results(labels_new, labels_gt, class_names_gt, mid_step, not plot_results)\n        if accuracy >= 0:\n            print('Overall Accuracy: {0:.2f}'.format(accuracy))\n    return (labels_gt, class_names, accuracy, cm)",
        "mutated": [
            "def load_ground_truth(gt_file, labels, class_names, mid_step, plot_results):\n    if False:\n        i = 10\n    accuracy = 0\n    cm = np.array([])\n    labels_gt = np.array([])\n    if os.path.isfile(gt_file):\n        (labels_gt, class_names_gt) = load_ground_truth_segments(gt_file, mid_step)\n        labels_new = []\n        for (il, l) in enumerate(labels):\n            if class_names[int(l)] in class_names_gt:\n                labels_new.append(class_names_gt.index(class_names[int(l)]))\n            else:\n                labels_new.append(-1)\n        labels_new = np.array(labels_new)\n        cm = calculate_confusion_matrix(labels_new, labels_gt, class_names_gt)\n        accuracy = plot_segmentation_results(labels_new, labels_gt, class_names_gt, mid_step, not plot_results)\n        if accuracy >= 0:\n            print('Overall Accuracy: {0:.2f}'.format(accuracy))\n    return (labels_gt, class_names, accuracy, cm)",
            "def load_ground_truth(gt_file, labels, class_names, mid_step, plot_results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    accuracy = 0\n    cm = np.array([])\n    labels_gt = np.array([])\n    if os.path.isfile(gt_file):\n        (labels_gt, class_names_gt) = load_ground_truth_segments(gt_file, mid_step)\n        labels_new = []\n        for (il, l) in enumerate(labels):\n            if class_names[int(l)] in class_names_gt:\n                labels_new.append(class_names_gt.index(class_names[int(l)]))\n            else:\n                labels_new.append(-1)\n        labels_new = np.array(labels_new)\n        cm = calculate_confusion_matrix(labels_new, labels_gt, class_names_gt)\n        accuracy = plot_segmentation_results(labels_new, labels_gt, class_names_gt, mid_step, not plot_results)\n        if accuracy >= 0:\n            print('Overall Accuracy: {0:.2f}'.format(accuracy))\n    return (labels_gt, class_names, accuracy, cm)",
            "def load_ground_truth(gt_file, labels, class_names, mid_step, plot_results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    accuracy = 0\n    cm = np.array([])\n    labels_gt = np.array([])\n    if os.path.isfile(gt_file):\n        (labels_gt, class_names_gt) = load_ground_truth_segments(gt_file, mid_step)\n        labels_new = []\n        for (il, l) in enumerate(labels):\n            if class_names[int(l)] in class_names_gt:\n                labels_new.append(class_names_gt.index(class_names[int(l)]))\n            else:\n                labels_new.append(-1)\n        labels_new = np.array(labels_new)\n        cm = calculate_confusion_matrix(labels_new, labels_gt, class_names_gt)\n        accuracy = plot_segmentation_results(labels_new, labels_gt, class_names_gt, mid_step, not plot_results)\n        if accuracy >= 0:\n            print('Overall Accuracy: {0:.2f}'.format(accuracy))\n    return (labels_gt, class_names, accuracy, cm)",
            "def load_ground_truth(gt_file, labels, class_names, mid_step, plot_results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    accuracy = 0\n    cm = np.array([])\n    labels_gt = np.array([])\n    if os.path.isfile(gt_file):\n        (labels_gt, class_names_gt) = load_ground_truth_segments(gt_file, mid_step)\n        labels_new = []\n        for (il, l) in enumerate(labels):\n            if class_names[int(l)] in class_names_gt:\n                labels_new.append(class_names_gt.index(class_names[int(l)]))\n            else:\n                labels_new.append(-1)\n        labels_new = np.array(labels_new)\n        cm = calculate_confusion_matrix(labels_new, labels_gt, class_names_gt)\n        accuracy = plot_segmentation_results(labels_new, labels_gt, class_names_gt, mid_step, not plot_results)\n        if accuracy >= 0:\n            print('Overall Accuracy: {0:.2f}'.format(accuracy))\n    return (labels_gt, class_names, accuracy, cm)",
            "def load_ground_truth(gt_file, labels, class_names, mid_step, plot_results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    accuracy = 0\n    cm = np.array([])\n    labels_gt = np.array([])\n    if os.path.isfile(gt_file):\n        (labels_gt, class_names_gt) = load_ground_truth_segments(gt_file, mid_step)\n        labels_new = []\n        for (il, l) in enumerate(labels):\n            if class_names[int(l)] in class_names_gt:\n                labels_new.append(class_names_gt.index(class_names[int(l)]))\n            else:\n                labels_new.append(-1)\n        labels_new = np.array(labels_new)\n        cm = calculate_confusion_matrix(labels_new, labels_gt, class_names_gt)\n        accuracy = plot_segmentation_results(labels_new, labels_gt, class_names_gt, mid_step, not plot_results)\n        if accuracy >= 0:\n            print('Overall Accuracy: {0:.2f}'.format(accuracy))\n    return (labels_gt, class_names, accuracy, cm)"
        ]
    },
    {
        "func_name": "evaluate_segmentation_classification_dir",
        "original": "def evaluate_segmentation_classification_dir(dir_name, model_name, method_name):\n    accuracies = []\n    class_names = []\n    cm_total = np.array([])\n    for (index, wav_file) in enumerate(glob.glob(dir_name + os.sep + '*.wav')):\n        print(wav_file)\n        gt_file = wav_file.replace('.wav', '.segments')\n        if method_name.lower() in ['svm', 'svm_rbf', 'knn', 'randomforest', 'gradientboosting', 'extratrees']:\n            (flags_ind, class_names, accuracy, cm_temp) = mid_term_file_classification(wav_file, model_name, method_name, False, gt_file)\n        else:\n            (flags_ind, class_names, accuracy, cm_temp) = hmm_segmentation(wav_file, model_name, False, gt_file)\n        if accuracy > 0:\n            if not index:\n                cm_total = np.copy(cm_temp)\n            else:\n                cm_total = cm_total + cm_temp\n            accuracies.append(accuracy)\n            print(cm_temp, class_names)\n            print(cm_total)\n    if len(cm_total.shape) > 1:\n        cm_total = cm_total / np.sum(cm_total)\n        (rec, pre, f1) = compute_metrics(cm_total, class_names)\n        print(' - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n        print('Average Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).mean()))\n        print('Average recall: {0:.1f}'.format(100.0 * np.array(rec).mean()))\n        print('Average precision: {0:.1f}'.format(100.0 * np.array(pre).mean()))\n        print('Average f1: {0:.1f}'.format(100.0 * np.array(f1).mean()))\n        print('Median Accuracy: {0:.1f}'.format(100.0 * np.median(np.array(accuracies))))\n        print('Min Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).min()))\n        print('Max Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).max()))\n    else:\n        print('Confusion matrix was empty, accuracy for every file was 0')",
        "mutated": [
            "def evaluate_segmentation_classification_dir(dir_name, model_name, method_name):\n    if False:\n        i = 10\n    accuracies = []\n    class_names = []\n    cm_total = np.array([])\n    for (index, wav_file) in enumerate(glob.glob(dir_name + os.sep + '*.wav')):\n        print(wav_file)\n        gt_file = wav_file.replace('.wav', '.segments')\n        if method_name.lower() in ['svm', 'svm_rbf', 'knn', 'randomforest', 'gradientboosting', 'extratrees']:\n            (flags_ind, class_names, accuracy, cm_temp) = mid_term_file_classification(wav_file, model_name, method_name, False, gt_file)\n        else:\n            (flags_ind, class_names, accuracy, cm_temp) = hmm_segmentation(wav_file, model_name, False, gt_file)\n        if accuracy > 0:\n            if not index:\n                cm_total = np.copy(cm_temp)\n            else:\n                cm_total = cm_total + cm_temp\n            accuracies.append(accuracy)\n            print(cm_temp, class_names)\n            print(cm_total)\n    if len(cm_total.shape) > 1:\n        cm_total = cm_total / np.sum(cm_total)\n        (rec, pre, f1) = compute_metrics(cm_total, class_names)\n        print(' - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n        print('Average Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).mean()))\n        print('Average recall: {0:.1f}'.format(100.0 * np.array(rec).mean()))\n        print('Average precision: {0:.1f}'.format(100.0 * np.array(pre).mean()))\n        print('Average f1: {0:.1f}'.format(100.0 * np.array(f1).mean()))\n        print('Median Accuracy: {0:.1f}'.format(100.0 * np.median(np.array(accuracies))))\n        print('Min Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).min()))\n        print('Max Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).max()))\n    else:\n        print('Confusion matrix was empty, accuracy for every file was 0')",
            "def evaluate_segmentation_classification_dir(dir_name, model_name, method_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    accuracies = []\n    class_names = []\n    cm_total = np.array([])\n    for (index, wav_file) in enumerate(glob.glob(dir_name + os.sep + '*.wav')):\n        print(wav_file)\n        gt_file = wav_file.replace('.wav', '.segments')\n        if method_name.lower() in ['svm', 'svm_rbf', 'knn', 'randomforest', 'gradientboosting', 'extratrees']:\n            (flags_ind, class_names, accuracy, cm_temp) = mid_term_file_classification(wav_file, model_name, method_name, False, gt_file)\n        else:\n            (flags_ind, class_names, accuracy, cm_temp) = hmm_segmentation(wav_file, model_name, False, gt_file)\n        if accuracy > 0:\n            if not index:\n                cm_total = np.copy(cm_temp)\n            else:\n                cm_total = cm_total + cm_temp\n            accuracies.append(accuracy)\n            print(cm_temp, class_names)\n            print(cm_total)\n    if len(cm_total.shape) > 1:\n        cm_total = cm_total / np.sum(cm_total)\n        (rec, pre, f1) = compute_metrics(cm_total, class_names)\n        print(' - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n        print('Average Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).mean()))\n        print('Average recall: {0:.1f}'.format(100.0 * np.array(rec).mean()))\n        print('Average precision: {0:.1f}'.format(100.0 * np.array(pre).mean()))\n        print('Average f1: {0:.1f}'.format(100.0 * np.array(f1).mean()))\n        print('Median Accuracy: {0:.1f}'.format(100.0 * np.median(np.array(accuracies))))\n        print('Min Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).min()))\n        print('Max Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).max()))\n    else:\n        print('Confusion matrix was empty, accuracy for every file was 0')",
            "def evaluate_segmentation_classification_dir(dir_name, model_name, method_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    accuracies = []\n    class_names = []\n    cm_total = np.array([])\n    for (index, wav_file) in enumerate(glob.glob(dir_name + os.sep + '*.wav')):\n        print(wav_file)\n        gt_file = wav_file.replace('.wav', '.segments')\n        if method_name.lower() in ['svm', 'svm_rbf', 'knn', 'randomforest', 'gradientboosting', 'extratrees']:\n            (flags_ind, class_names, accuracy, cm_temp) = mid_term_file_classification(wav_file, model_name, method_name, False, gt_file)\n        else:\n            (flags_ind, class_names, accuracy, cm_temp) = hmm_segmentation(wav_file, model_name, False, gt_file)\n        if accuracy > 0:\n            if not index:\n                cm_total = np.copy(cm_temp)\n            else:\n                cm_total = cm_total + cm_temp\n            accuracies.append(accuracy)\n            print(cm_temp, class_names)\n            print(cm_total)\n    if len(cm_total.shape) > 1:\n        cm_total = cm_total / np.sum(cm_total)\n        (rec, pre, f1) = compute_metrics(cm_total, class_names)\n        print(' - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n        print('Average Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).mean()))\n        print('Average recall: {0:.1f}'.format(100.0 * np.array(rec).mean()))\n        print('Average precision: {0:.1f}'.format(100.0 * np.array(pre).mean()))\n        print('Average f1: {0:.1f}'.format(100.0 * np.array(f1).mean()))\n        print('Median Accuracy: {0:.1f}'.format(100.0 * np.median(np.array(accuracies))))\n        print('Min Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).min()))\n        print('Max Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).max()))\n    else:\n        print('Confusion matrix was empty, accuracy for every file was 0')",
            "def evaluate_segmentation_classification_dir(dir_name, model_name, method_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    accuracies = []\n    class_names = []\n    cm_total = np.array([])\n    for (index, wav_file) in enumerate(glob.glob(dir_name + os.sep + '*.wav')):\n        print(wav_file)\n        gt_file = wav_file.replace('.wav', '.segments')\n        if method_name.lower() in ['svm', 'svm_rbf', 'knn', 'randomforest', 'gradientboosting', 'extratrees']:\n            (flags_ind, class_names, accuracy, cm_temp) = mid_term_file_classification(wav_file, model_name, method_name, False, gt_file)\n        else:\n            (flags_ind, class_names, accuracy, cm_temp) = hmm_segmentation(wav_file, model_name, False, gt_file)\n        if accuracy > 0:\n            if not index:\n                cm_total = np.copy(cm_temp)\n            else:\n                cm_total = cm_total + cm_temp\n            accuracies.append(accuracy)\n            print(cm_temp, class_names)\n            print(cm_total)\n    if len(cm_total.shape) > 1:\n        cm_total = cm_total / np.sum(cm_total)\n        (rec, pre, f1) = compute_metrics(cm_total, class_names)\n        print(' - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n        print('Average Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).mean()))\n        print('Average recall: {0:.1f}'.format(100.0 * np.array(rec).mean()))\n        print('Average precision: {0:.1f}'.format(100.0 * np.array(pre).mean()))\n        print('Average f1: {0:.1f}'.format(100.0 * np.array(f1).mean()))\n        print('Median Accuracy: {0:.1f}'.format(100.0 * np.median(np.array(accuracies))))\n        print('Min Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).min()))\n        print('Max Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).max()))\n    else:\n        print('Confusion matrix was empty, accuracy for every file was 0')",
            "def evaluate_segmentation_classification_dir(dir_name, model_name, method_name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    accuracies = []\n    class_names = []\n    cm_total = np.array([])\n    for (index, wav_file) in enumerate(glob.glob(dir_name + os.sep + '*.wav')):\n        print(wav_file)\n        gt_file = wav_file.replace('.wav', '.segments')\n        if method_name.lower() in ['svm', 'svm_rbf', 'knn', 'randomforest', 'gradientboosting', 'extratrees']:\n            (flags_ind, class_names, accuracy, cm_temp) = mid_term_file_classification(wav_file, model_name, method_name, False, gt_file)\n        else:\n            (flags_ind, class_names, accuracy, cm_temp) = hmm_segmentation(wav_file, model_name, False, gt_file)\n        if accuracy > 0:\n            if not index:\n                cm_total = np.copy(cm_temp)\n            else:\n                cm_total = cm_total + cm_temp\n            accuracies.append(accuracy)\n            print(cm_temp, class_names)\n            print(cm_total)\n    if len(cm_total.shape) > 1:\n        cm_total = cm_total / np.sum(cm_total)\n        (rec, pre, f1) = compute_metrics(cm_total, class_names)\n        print(' - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n        print('Average Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).mean()))\n        print('Average recall: {0:.1f}'.format(100.0 * np.array(rec).mean()))\n        print('Average precision: {0:.1f}'.format(100.0 * np.array(pre).mean()))\n        print('Average f1: {0:.1f}'.format(100.0 * np.array(f1).mean()))\n        print('Median Accuracy: {0:.1f}'.format(100.0 * np.median(np.array(accuracies))))\n        print('Min Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).min()))\n        print('Max Accuracy: {0:.1f}'.format(100.0 * np.array(accuracies).max()))\n    else:\n        print('Confusion matrix was empty, accuracy for every file was 0')"
        ]
    },
    {
        "func_name": "silence_removal",
        "original": "def silence_removal(signal, sampling_rate, st_win, st_step, smooth_window=0.5, weight=0.5, plot=False):\n    \"\"\"\n    Event Detection (silence removal)\n    ARGUMENTS:\n         - signal:                the input audio signal\n         - sampling_rate:               sampling freq\n         - st_win, st_step:    window size and step in seconds\n         - smoothWindow:     (optinal) smooth window (in seconds)\n         - weight:           (optinal) weight factor (0 < weight < 1)\n                              the higher, the more strict\n         - plot:             (optinal) True if results are to be plotted\n    RETURNS:\n         - seg_limits:    list of segment limits in seconds (e.g [[0.1, 0.9],\n                          [1.4, 3.0]] means that\n                          the resulting segments are (0.1 - 0.9) seconds\n                          and (1.4, 3.0) seconds\n    \"\"\"\n    if weight >= 1:\n        weight = 0.99\n    if weight <= 0:\n        weight = 0.01\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (st_feats, _) = stf.feature_extraction(signal, sampling_rate, st_win * sampling_rate, st_step * sampling_rate)\n    st_energy = st_feats[1, :]\n    en = np.sort(st_energy)\n    st_windows_fraction = int(len(en) / 10)\n    low_threshold = np.mean(en[0:st_windows_fraction]) + 1e-15\n    high_threshold = np.mean(en[-st_windows_fraction:-1]) + 1e-15\n    low_energy = st_feats[:, np.where(st_energy <= low_threshold)[0]]\n    high_energy = st_feats[:, np.where(st_energy >= high_threshold)[0]]\n    features = [low_energy.T, high_energy.T]\n    (features, labels) = at.features_to_matrix(features)\n    scaler = StandardScaler()\n    features_norm = scaler.fit_transform(features)\n    mean = scaler.mean_\n    std = scaler.scale_\n    svm = at.train_svm(features_norm, labels, 1.0)\n    prob_on_set = []\n    for index in range(st_feats.shape[1]):\n        cur_fv = (st_feats[:, index] - mean) / std\n        prob_on_set.append(svm.predict_proba(cur_fv.reshape(1, -1))[0][1])\n    prob_on_set = np.array(prob_on_set)\n    prob_on_set = smooth_moving_avg(prob_on_set, smooth_window / st_step)\n    prog_on_set_sort = np.sort(prob_on_set)\n    nt = int(prog_on_set_sort.shape[0] / 10)\n    threshold = np.mean((1 - weight) * prog_on_set_sort[0:nt]) + weight * np.mean(prog_on_set_sort[-nt:])\n    max_indices = np.where(prob_on_set > threshold)[0]\n    index = 0\n    seg_limits = []\n    time_clusters = []\n    while index < len(max_indices):\n        cur_cluster = [max_indices[index]]\n        if index == len(max_indices) - 1:\n            break\n        while max_indices[index + 1] - cur_cluster[-1] <= 2:\n            cur_cluster.append(max_indices[index + 1])\n            index += 1\n            if index == len(max_indices) - 1:\n                break\n        index += 1\n        time_clusters.append(cur_cluster)\n        seg_limits.append([cur_cluster[0] * st_step, cur_cluster[-1] * st_step])\n    min_duration = 0.2\n    seg_limits_2 = []\n    for s_lim in seg_limits:\n        if s_lim[1] - s_lim[0] > min_duration:\n            seg_limits_2.append(s_lim)\n    seg_limits = seg_limits_2\n    if plot:\n        time_x = np.arange(0, signal.shape[0] / float(sampling_rate), 1.0 / sampling_rate)\n        plt.subplot(2, 1, 1)\n        plt.plot(time_x, signal)\n        for s_lim in seg_limits:\n            plt.axvline(x=s_lim[0], color='red')\n            plt.axvline(x=s_lim[1], color='red')\n        plt.subplot(2, 1, 2)\n        plt.plot(np.arange(0, prob_on_set.shape[0] * st_step, st_step), prob_on_set)\n        plt.title('Signal')\n        for s_lim in seg_limits:\n            plt.axvline(x=s_lim[0], color='red')\n            plt.axvline(x=s_lim[1], color='red')\n        plt.title('svm Probability')\n        plt.show()\n    return seg_limits",
        "mutated": [
            "def silence_removal(signal, sampling_rate, st_win, st_step, smooth_window=0.5, weight=0.5, plot=False):\n    if False:\n        i = 10\n    '\\n    Event Detection (silence removal)\\n    ARGUMENTS:\\n         - signal:                the input audio signal\\n         - sampling_rate:               sampling freq\\n         - st_win, st_step:    window size and step in seconds\\n         - smoothWindow:     (optinal) smooth window (in seconds)\\n         - weight:           (optinal) weight factor (0 < weight < 1)\\n                              the higher, the more strict\\n         - plot:             (optinal) True if results are to be plotted\\n    RETURNS:\\n         - seg_limits:    list of segment limits in seconds (e.g [[0.1, 0.9],\\n                          [1.4, 3.0]] means that\\n                          the resulting segments are (0.1 - 0.9) seconds\\n                          and (1.4, 3.0) seconds\\n    '\n    if weight >= 1:\n        weight = 0.99\n    if weight <= 0:\n        weight = 0.01\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (st_feats, _) = stf.feature_extraction(signal, sampling_rate, st_win * sampling_rate, st_step * sampling_rate)\n    st_energy = st_feats[1, :]\n    en = np.sort(st_energy)\n    st_windows_fraction = int(len(en) / 10)\n    low_threshold = np.mean(en[0:st_windows_fraction]) + 1e-15\n    high_threshold = np.mean(en[-st_windows_fraction:-1]) + 1e-15\n    low_energy = st_feats[:, np.where(st_energy <= low_threshold)[0]]\n    high_energy = st_feats[:, np.where(st_energy >= high_threshold)[0]]\n    features = [low_energy.T, high_energy.T]\n    (features, labels) = at.features_to_matrix(features)\n    scaler = StandardScaler()\n    features_norm = scaler.fit_transform(features)\n    mean = scaler.mean_\n    std = scaler.scale_\n    svm = at.train_svm(features_norm, labels, 1.0)\n    prob_on_set = []\n    for index in range(st_feats.shape[1]):\n        cur_fv = (st_feats[:, index] - mean) / std\n        prob_on_set.append(svm.predict_proba(cur_fv.reshape(1, -1))[0][1])\n    prob_on_set = np.array(prob_on_set)\n    prob_on_set = smooth_moving_avg(prob_on_set, smooth_window / st_step)\n    prog_on_set_sort = np.sort(prob_on_set)\n    nt = int(prog_on_set_sort.shape[0] / 10)\n    threshold = np.mean((1 - weight) * prog_on_set_sort[0:nt]) + weight * np.mean(prog_on_set_sort[-nt:])\n    max_indices = np.where(prob_on_set > threshold)[0]\n    index = 0\n    seg_limits = []\n    time_clusters = []\n    while index < len(max_indices):\n        cur_cluster = [max_indices[index]]\n        if index == len(max_indices) - 1:\n            break\n        while max_indices[index + 1] - cur_cluster[-1] <= 2:\n            cur_cluster.append(max_indices[index + 1])\n            index += 1\n            if index == len(max_indices) - 1:\n                break\n        index += 1\n        time_clusters.append(cur_cluster)\n        seg_limits.append([cur_cluster[0] * st_step, cur_cluster[-1] * st_step])\n    min_duration = 0.2\n    seg_limits_2 = []\n    for s_lim in seg_limits:\n        if s_lim[1] - s_lim[0] > min_duration:\n            seg_limits_2.append(s_lim)\n    seg_limits = seg_limits_2\n    if plot:\n        time_x = np.arange(0, signal.shape[0] / float(sampling_rate), 1.0 / sampling_rate)\n        plt.subplot(2, 1, 1)\n        plt.plot(time_x, signal)\n        for s_lim in seg_limits:\n            plt.axvline(x=s_lim[0], color='red')\n            plt.axvline(x=s_lim[1], color='red')\n        plt.subplot(2, 1, 2)\n        plt.plot(np.arange(0, prob_on_set.shape[0] * st_step, st_step), prob_on_set)\n        plt.title('Signal')\n        for s_lim in seg_limits:\n            plt.axvline(x=s_lim[0], color='red')\n            plt.axvline(x=s_lim[1], color='red')\n        plt.title('svm Probability')\n        plt.show()\n    return seg_limits",
            "def silence_removal(signal, sampling_rate, st_win, st_step, smooth_window=0.5, weight=0.5, plot=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    Event Detection (silence removal)\\n    ARGUMENTS:\\n         - signal:                the input audio signal\\n         - sampling_rate:               sampling freq\\n         - st_win, st_step:    window size and step in seconds\\n         - smoothWindow:     (optinal) smooth window (in seconds)\\n         - weight:           (optinal) weight factor (0 < weight < 1)\\n                              the higher, the more strict\\n         - plot:             (optinal) True if results are to be plotted\\n    RETURNS:\\n         - seg_limits:    list of segment limits in seconds (e.g [[0.1, 0.9],\\n                          [1.4, 3.0]] means that\\n                          the resulting segments are (0.1 - 0.9) seconds\\n                          and (1.4, 3.0) seconds\\n    '\n    if weight >= 1:\n        weight = 0.99\n    if weight <= 0:\n        weight = 0.01\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (st_feats, _) = stf.feature_extraction(signal, sampling_rate, st_win * sampling_rate, st_step * sampling_rate)\n    st_energy = st_feats[1, :]\n    en = np.sort(st_energy)\n    st_windows_fraction = int(len(en) / 10)\n    low_threshold = np.mean(en[0:st_windows_fraction]) + 1e-15\n    high_threshold = np.mean(en[-st_windows_fraction:-1]) + 1e-15\n    low_energy = st_feats[:, np.where(st_energy <= low_threshold)[0]]\n    high_energy = st_feats[:, np.where(st_energy >= high_threshold)[0]]\n    features = [low_energy.T, high_energy.T]\n    (features, labels) = at.features_to_matrix(features)\n    scaler = StandardScaler()\n    features_norm = scaler.fit_transform(features)\n    mean = scaler.mean_\n    std = scaler.scale_\n    svm = at.train_svm(features_norm, labels, 1.0)\n    prob_on_set = []\n    for index in range(st_feats.shape[1]):\n        cur_fv = (st_feats[:, index] - mean) / std\n        prob_on_set.append(svm.predict_proba(cur_fv.reshape(1, -1))[0][1])\n    prob_on_set = np.array(prob_on_set)\n    prob_on_set = smooth_moving_avg(prob_on_set, smooth_window / st_step)\n    prog_on_set_sort = np.sort(prob_on_set)\n    nt = int(prog_on_set_sort.shape[0] / 10)\n    threshold = np.mean((1 - weight) * prog_on_set_sort[0:nt]) + weight * np.mean(prog_on_set_sort[-nt:])\n    max_indices = np.where(prob_on_set > threshold)[0]\n    index = 0\n    seg_limits = []\n    time_clusters = []\n    while index < len(max_indices):\n        cur_cluster = [max_indices[index]]\n        if index == len(max_indices) - 1:\n            break\n        while max_indices[index + 1] - cur_cluster[-1] <= 2:\n            cur_cluster.append(max_indices[index + 1])\n            index += 1\n            if index == len(max_indices) - 1:\n                break\n        index += 1\n        time_clusters.append(cur_cluster)\n        seg_limits.append([cur_cluster[0] * st_step, cur_cluster[-1] * st_step])\n    min_duration = 0.2\n    seg_limits_2 = []\n    for s_lim in seg_limits:\n        if s_lim[1] - s_lim[0] > min_duration:\n            seg_limits_2.append(s_lim)\n    seg_limits = seg_limits_2\n    if plot:\n        time_x = np.arange(0, signal.shape[0] / float(sampling_rate), 1.0 / sampling_rate)\n        plt.subplot(2, 1, 1)\n        plt.plot(time_x, signal)\n        for s_lim in seg_limits:\n            plt.axvline(x=s_lim[0], color='red')\n            plt.axvline(x=s_lim[1], color='red')\n        plt.subplot(2, 1, 2)\n        plt.plot(np.arange(0, prob_on_set.shape[0] * st_step, st_step), prob_on_set)\n        plt.title('Signal')\n        for s_lim in seg_limits:\n            plt.axvline(x=s_lim[0], color='red')\n            plt.axvline(x=s_lim[1], color='red')\n        plt.title('svm Probability')\n        plt.show()\n    return seg_limits",
            "def silence_removal(signal, sampling_rate, st_win, st_step, smooth_window=0.5, weight=0.5, plot=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    Event Detection (silence removal)\\n    ARGUMENTS:\\n         - signal:                the input audio signal\\n         - sampling_rate:               sampling freq\\n         - st_win, st_step:    window size and step in seconds\\n         - smoothWindow:     (optinal) smooth window (in seconds)\\n         - weight:           (optinal) weight factor (0 < weight < 1)\\n                              the higher, the more strict\\n         - plot:             (optinal) True if results are to be plotted\\n    RETURNS:\\n         - seg_limits:    list of segment limits in seconds (e.g [[0.1, 0.9],\\n                          [1.4, 3.0]] means that\\n                          the resulting segments are (0.1 - 0.9) seconds\\n                          and (1.4, 3.0) seconds\\n    '\n    if weight >= 1:\n        weight = 0.99\n    if weight <= 0:\n        weight = 0.01\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (st_feats, _) = stf.feature_extraction(signal, sampling_rate, st_win * sampling_rate, st_step * sampling_rate)\n    st_energy = st_feats[1, :]\n    en = np.sort(st_energy)\n    st_windows_fraction = int(len(en) / 10)\n    low_threshold = np.mean(en[0:st_windows_fraction]) + 1e-15\n    high_threshold = np.mean(en[-st_windows_fraction:-1]) + 1e-15\n    low_energy = st_feats[:, np.where(st_energy <= low_threshold)[0]]\n    high_energy = st_feats[:, np.where(st_energy >= high_threshold)[0]]\n    features = [low_energy.T, high_energy.T]\n    (features, labels) = at.features_to_matrix(features)\n    scaler = StandardScaler()\n    features_norm = scaler.fit_transform(features)\n    mean = scaler.mean_\n    std = scaler.scale_\n    svm = at.train_svm(features_norm, labels, 1.0)\n    prob_on_set = []\n    for index in range(st_feats.shape[1]):\n        cur_fv = (st_feats[:, index] - mean) / std\n        prob_on_set.append(svm.predict_proba(cur_fv.reshape(1, -1))[0][1])\n    prob_on_set = np.array(prob_on_set)\n    prob_on_set = smooth_moving_avg(prob_on_set, smooth_window / st_step)\n    prog_on_set_sort = np.sort(prob_on_set)\n    nt = int(prog_on_set_sort.shape[0] / 10)\n    threshold = np.mean((1 - weight) * prog_on_set_sort[0:nt]) + weight * np.mean(prog_on_set_sort[-nt:])\n    max_indices = np.where(prob_on_set > threshold)[0]\n    index = 0\n    seg_limits = []\n    time_clusters = []\n    while index < len(max_indices):\n        cur_cluster = [max_indices[index]]\n        if index == len(max_indices) - 1:\n            break\n        while max_indices[index + 1] - cur_cluster[-1] <= 2:\n            cur_cluster.append(max_indices[index + 1])\n            index += 1\n            if index == len(max_indices) - 1:\n                break\n        index += 1\n        time_clusters.append(cur_cluster)\n        seg_limits.append([cur_cluster[0] * st_step, cur_cluster[-1] * st_step])\n    min_duration = 0.2\n    seg_limits_2 = []\n    for s_lim in seg_limits:\n        if s_lim[1] - s_lim[0] > min_duration:\n            seg_limits_2.append(s_lim)\n    seg_limits = seg_limits_2\n    if plot:\n        time_x = np.arange(0, signal.shape[0] / float(sampling_rate), 1.0 / sampling_rate)\n        plt.subplot(2, 1, 1)\n        plt.plot(time_x, signal)\n        for s_lim in seg_limits:\n            plt.axvline(x=s_lim[0], color='red')\n            plt.axvline(x=s_lim[1], color='red')\n        plt.subplot(2, 1, 2)\n        plt.plot(np.arange(0, prob_on_set.shape[0] * st_step, st_step), prob_on_set)\n        plt.title('Signal')\n        for s_lim in seg_limits:\n            plt.axvline(x=s_lim[0], color='red')\n            plt.axvline(x=s_lim[1], color='red')\n        plt.title('svm Probability')\n        plt.show()\n    return seg_limits",
            "def silence_removal(signal, sampling_rate, st_win, st_step, smooth_window=0.5, weight=0.5, plot=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    Event Detection (silence removal)\\n    ARGUMENTS:\\n         - signal:                the input audio signal\\n         - sampling_rate:               sampling freq\\n         - st_win, st_step:    window size and step in seconds\\n         - smoothWindow:     (optinal) smooth window (in seconds)\\n         - weight:           (optinal) weight factor (0 < weight < 1)\\n                              the higher, the more strict\\n         - plot:             (optinal) True if results are to be plotted\\n    RETURNS:\\n         - seg_limits:    list of segment limits in seconds (e.g [[0.1, 0.9],\\n                          [1.4, 3.0]] means that\\n                          the resulting segments are (0.1 - 0.9) seconds\\n                          and (1.4, 3.0) seconds\\n    '\n    if weight >= 1:\n        weight = 0.99\n    if weight <= 0:\n        weight = 0.01\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (st_feats, _) = stf.feature_extraction(signal, sampling_rate, st_win * sampling_rate, st_step * sampling_rate)\n    st_energy = st_feats[1, :]\n    en = np.sort(st_energy)\n    st_windows_fraction = int(len(en) / 10)\n    low_threshold = np.mean(en[0:st_windows_fraction]) + 1e-15\n    high_threshold = np.mean(en[-st_windows_fraction:-1]) + 1e-15\n    low_energy = st_feats[:, np.where(st_energy <= low_threshold)[0]]\n    high_energy = st_feats[:, np.where(st_energy >= high_threshold)[0]]\n    features = [low_energy.T, high_energy.T]\n    (features, labels) = at.features_to_matrix(features)\n    scaler = StandardScaler()\n    features_norm = scaler.fit_transform(features)\n    mean = scaler.mean_\n    std = scaler.scale_\n    svm = at.train_svm(features_norm, labels, 1.0)\n    prob_on_set = []\n    for index in range(st_feats.shape[1]):\n        cur_fv = (st_feats[:, index] - mean) / std\n        prob_on_set.append(svm.predict_proba(cur_fv.reshape(1, -1))[0][1])\n    prob_on_set = np.array(prob_on_set)\n    prob_on_set = smooth_moving_avg(prob_on_set, smooth_window / st_step)\n    prog_on_set_sort = np.sort(prob_on_set)\n    nt = int(prog_on_set_sort.shape[0] / 10)\n    threshold = np.mean((1 - weight) * prog_on_set_sort[0:nt]) + weight * np.mean(prog_on_set_sort[-nt:])\n    max_indices = np.where(prob_on_set > threshold)[0]\n    index = 0\n    seg_limits = []\n    time_clusters = []\n    while index < len(max_indices):\n        cur_cluster = [max_indices[index]]\n        if index == len(max_indices) - 1:\n            break\n        while max_indices[index + 1] - cur_cluster[-1] <= 2:\n            cur_cluster.append(max_indices[index + 1])\n            index += 1\n            if index == len(max_indices) - 1:\n                break\n        index += 1\n        time_clusters.append(cur_cluster)\n        seg_limits.append([cur_cluster[0] * st_step, cur_cluster[-1] * st_step])\n    min_duration = 0.2\n    seg_limits_2 = []\n    for s_lim in seg_limits:\n        if s_lim[1] - s_lim[0] > min_duration:\n            seg_limits_2.append(s_lim)\n    seg_limits = seg_limits_2\n    if plot:\n        time_x = np.arange(0, signal.shape[0] / float(sampling_rate), 1.0 / sampling_rate)\n        plt.subplot(2, 1, 1)\n        plt.plot(time_x, signal)\n        for s_lim in seg_limits:\n            plt.axvline(x=s_lim[0], color='red')\n            plt.axvline(x=s_lim[1], color='red')\n        plt.subplot(2, 1, 2)\n        plt.plot(np.arange(0, prob_on_set.shape[0] * st_step, st_step), prob_on_set)\n        plt.title('Signal')\n        for s_lim in seg_limits:\n            plt.axvline(x=s_lim[0], color='red')\n            plt.axvline(x=s_lim[1], color='red')\n        plt.title('svm Probability')\n        plt.show()\n    return seg_limits",
            "def silence_removal(signal, sampling_rate, st_win, st_step, smooth_window=0.5, weight=0.5, plot=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    Event Detection (silence removal)\\n    ARGUMENTS:\\n         - signal:                the input audio signal\\n         - sampling_rate:               sampling freq\\n         - st_win, st_step:    window size and step in seconds\\n         - smoothWindow:     (optinal) smooth window (in seconds)\\n         - weight:           (optinal) weight factor (0 < weight < 1)\\n                              the higher, the more strict\\n         - plot:             (optinal) True if results are to be plotted\\n    RETURNS:\\n         - seg_limits:    list of segment limits in seconds (e.g [[0.1, 0.9],\\n                          [1.4, 3.0]] means that\\n                          the resulting segments are (0.1 - 0.9) seconds\\n                          and (1.4, 3.0) seconds\\n    '\n    if weight >= 1:\n        weight = 0.99\n    if weight <= 0:\n        weight = 0.01\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (st_feats, _) = stf.feature_extraction(signal, sampling_rate, st_win * sampling_rate, st_step * sampling_rate)\n    st_energy = st_feats[1, :]\n    en = np.sort(st_energy)\n    st_windows_fraction = int(len(en) / 10)\n    low_threshold = np.mean(en[0:st_windows_fraction]) + 1e-15\n    high_threshold = np.mean(en[-st_windows_fraction:-1]) + 1e-15\n    low_energy = st_feats[:, np.where(st_energy <= low_threshold)[0]]\n    high_energy = st_feats[:, np.where(st_energy >= high_threshold)[0]]\n    features = [low_energy.T, high_energy.T]\n    (features, labels) = at.features_to_matrix(features)\n    scaler = StandardScaler()\n    features_norm = scaler.fit_transform(features)\n    mean = scaler.mean_\n    std = scaler.scale_\n    svm = at.train_svm(features_norm, labels, 1.0)\n    prob_on_set = []\n    for index in range(st_feats.shape[1]):\n        cur_fv = (st_feats[:, index] - mean) / std\n        prob_on_set.append(svm.predict_proba(cur_fv.reshape(1, -1))[0][1])\n    prob_on_set = np.array(prob_on_set)\n    prob_on_set = smooth_moving_avg(prob_on_set, smooth_window / st_step)\n    prog_on_set_sort = np.sort(prob_on_set)\n    nt = int(prog_on_set_sort.shape[0] / 10)\n    threshold = np.mean((1 - weight) * prog_on_set_sort[0:nt]) + weight * np.mean(prog_on_set_sort[-nt:])\n    max_indices = np.where(prob_on_set > threshold)[0]\n    index = 0\n    seg_limits = []\n    time_clusters = []\n    while index < len(max_indices):\n        cur_cluster = [max_indices[index]]\n        if index == len(max_indices) - 1:\n            break\n        while max_indices[index + 1] - cur_cluster[-1] <= 2:\n            cur_cluster.append(max_indices[index + 1])\n            index += 1\n            if index == len(max_indices) - 1:\n                break\n        index += 1\n        time_clusters.append(cur_cluster)\n        seg_limits.append([cur_cluster[0] * st_step, cur_cluster[-1] * st_step])\n    min_duration = 0.2\n    seg_limits_2 = []\n    for s_lim in seg_limits:\n        if s_lim[1] - s_lim[0] > min_duration:\n            seg_limits_2.append(s_lim)\n    seg_limits = seg_limits_2\n    if plot:\n        time_x = np.arange(0, signal.shape[0] / float(sampling_rate), 1.0 / sampling_rate)\n        plt.subplot(2, 1, 1)\n        plt.plot(time_x, signal)\n        for s_lim in seg_limits:\n            plt.axvline(x=s_lim[0], color='red')\n            plt.axvline(x=s_lim[1], color='red')\n        plt.subplot(2, 1, 2)\n        plt.plot(np.arange(0, prob_on_set.shape[0] * st_step, st_step), prob_on_set)\n        plt.title('Signal')\n        for s_lim in seg_limits:\n            plt.axvline(x=s_lim[0], color='red')\n            plt.axvline(x=s_lim[1], color='red')\n        plt.title('svm Probability')\n        plt.show()\n    return seg_limits"
        ]
    },
    {
        "func_name": "speaker_diarization",
        "original": "def speaker_diarization(filename, n_speakers, mid_window=1.0, mid_step=0.1, short_window=0.1, lda_dim=0, plot_res=False):\n    \"\"\"\n    ARGUMENTS:\n        - filename:        the name of the WAV file to be analyzed\n        - n_speakers       the number of speakers (clusters) in\n                           the recording (<=0 for unknown)\n        - mid_window (opt)    mid-term window size\n        - mid_step (opt)    mid-term window step\n        - short_window  (opt)    short-term window size\n        - lda_dim (opt     LDA dimension (0 for no LDA)\n        - plot_res         (opt)   0 for not plotting the results 1 for plotting\n    \"\"\"\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(filename)\n    signal = audioBasicIO.stereo_to_mono(signal)\n    duration = len(signal) / sampling_rate\n    base_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'data/models')\n    (classifier_all, mean_all, std_all, class_names_all, _, _, _, _, _) = at.load_model(os.path.join(base_dir, 'svm_rbf_speaker_10'))\n    (classifier_fm, mean_fm, std_fm, class_names_fm, _, _, _, _, _) = at.load_model(os.path.join(base_dir, 'svm_rbf_speaker_male_female'))\n    (mid_feats, st_feats, a) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    mid_term_features = np.zeros((mid_feats.shape[0] + len(class_names_all) + len(class_names_fm), mid_feats.shape[1]))\n    for index in range(mid_feats.shape[1]):\n        feature_norm_all = (mid_feats[:, index] - mean_all) / std_all\n        feature_norm_fm = (mid_feats[:, index] - mean_fm) / std_fm\n        (_, p1) = at.classifier_wrapper(classifier_all, 'svm_rbf', feature_norm_all)\n        (_, p2) = at.classifier_wrapper(classifier_fm, 'svm_rbf', feature_norm_fm)\n        start = mid_feats.shape[0]\n        end = mid_feats.shape[0] + len(class_names_all)\n        mid_term_features[0:mid_feats.shape[0], index] = mid_feats[:, index]\n        mid_term_features[start:end, index] = p1 + 0.0001\n        mid_term_features[end:, index] = p2 + 0.0001\n    scaler = StandardScaler()\n    mid_feats_norm = scaler.fit_transform(mid_term_features.T)\n    dist_all = np.sum(distance.squareform(distance.pdist(mid_feats_norm.T)), axis=0)\n    m_dist_all = np.mean(dist_all)\n    i_non_outliers = np.nonzero(dist_all < 1.1 * m_dist_all)[0]\n    mt_feats_norm_or = mid_feats_norm\n    mid_feats_norm = mid_feats_norm[:, i_non_outliers]\n    if lda_dim > 0:\n        window_ratio = int(round(mid_window / short_window))\n        step_ratio = int(round(short_window / short_window))\n        mt_feats_to_red = []\n        num_of_features = len(st_feats)\n        num_of_stats = 2\n        for index in range(num_of_stats * num_of_features):\n            mt_feats_to_red.append([])\n        for index in range(num_of_features):\n            cur_pos = 0\n            feat_len = len(st_feats[index])\n            while cur_pos < feat_len:\n                n1 = cur_pos\n                n2 = cur_pos + window_ratio\n                if n2 > feat_len:\n                    n2 = feat_len\n                short_features = st_feats[index][n1:n2]\n                mt_feats_to_red[index].append(np.mean(short_features))\n                mt_feats_to_red[index + num_of_features].append(np.std(short_features))\n                cur_pos += step_ratio\n        mt_feats_to_red = np.array(mt_feats_to_red)\n        mt_feats_to_red_2 = np.zeros((mt_feats_to_red.shape[0] + len(class_names_all) + len(class_names_fm), mt_feats_to_red.shape[1]))\n        limit = mt_feats_to_red.shape[0] + len(class_names_all)\n        for index in range(mt_feats_to_red.shape[1]):\n            feature_norm_all = (mt_feats_to_red[:, index] - mean_all) / std_all\n            feature_norm_fm = (mt_feats_to_red[:, index] - mean_fm) / std_fm\n            (_, p1) = at.classifier_wrapper(classifier_all, 'svm_rbf', feature_norm_all)\n            (_, p2) = at.classifier_wrapper(classifier_fm, 'svm_rbf', feature_norm_fm)\n            mt_feats_to_red_2[0:mt_feats_to_red.shape[0], index] = mt_feats_to_red[:, index]\n            mt_feats_to_red_2[mt_feats_to_red.shape[0]:limit, index] = p1 + 0.0001\n            mt_feats_to_red_2[limit:, index] = p2 + 0.0001\n        mt_feats_to_red = mt_feats_to_red_2\n        scaler = StandardScaler()\n        mt_feats_to_red = scaler.fit_transform(mt_feats_to_red.T).T\n        labels = np.zeros((mt_feats_to_red.shape[1],))\n        lda_step = 1.0\n        lda_step_ratio = lda_step / short_window\n        for index in range(labels.shape[0]):\n            labels[index] = int(index * short_window / lda_step_ratio)\n        clf = sklearn.discriminant_analysis.LinearDiscriminantAnalysis(n_components=lda_dim)\n        mid_feats_norm = clf.fit_transform(mt_feats_to_red.T, labels)\n    if n_speakers <= 0:\n        s_range = range(2, 10)\n    else:\n        s_range = [n_speakers]\n    cluster_labels = []\n    sil_all = []\n    cluster_centers = []\n    for speakers in s_range:\n        k_means = sklearn.cluster.KMeans(n_clusters=speakers)\n        k_means.fit(mid_feats_norm)\n        cls = k_means.labels_\n        cluster_labels.append(cls)\n        sil_1 = []\n        sil_2 = []\n        for c in range(speakers):\n            clust_per_cent = np.nonzero(cls == c)[0].shape[0] / float(len(cls))\n            if clust_per_cent < 0.02:\n                sil_1.append(0.0)\n                sil_2.append(0.0)\n            else:\n                mt_feats_norm_temp = mid_feats_norm[cls == c, :]\n                dist = distance.pdist(mt_feats_norm_temp.T)\n                sil_1.append(np.mean(dist) * clust_per_cent)\n                sil_temp = []\n                for c2 in range(speakers):\n                    if c2 != c:\n                        clust_per_cent_2 = np.nonzero(cls == c2)[0].shape[0] / float(len(cls))\n                        mid_features_temp = mid_feats_norm[cls == c2, :]\n                        dist = distance.cdist(mt_feats_norm_temp, mid_features_temp)\n                        sil_temp.append(np.mean(dist) * (clust_per_cent + clust_per_cent_2) / 2.0)\n                sil_temp = np.array(sil_temp)\n                sil_2.append(min(sil_temp))\n        sil_1 = np.array(sil_1)\n        sil_2 = np.array(sil_2)\n        sil = []\n        for c in range(speakers):\n            sil.append((sil_2[c] - sil_1[c]) / (max(sil_2[c], sil_1[c]) + 1e-05))\n        sil_all.append(np.mean(sil))\n    imax = int(np.argmax(sil_all))\n    num_speakers = s_range[imax]\n    if lda_dim <= 0:\n        for index in range(1):\n            (start_prob, transmat, means, cov) = train_hmm_compute_statistics(mt_feats_norm_or.T, cls)\n            hmm = hmmlearn.hmm.GaussianHMM(start_prob.shape[0], 'diag')\n            hmm.startprob_ = start_prob\n            hmm.transmat_ = transmat\n            hmm.means_ = means\n            hmm.covars_ = cov\n            cls = hmm.predict(mt_feats_norm_or)\n    cls = scipy.signal.medfilt(cls, 5)\n    class_names = ['speaker{0:d}'.format(c) for c in range(num_speakers)]\n    gt_file = filename.replace('.wav', '.segments')\n    if os.path.isfile(gt_file):\n        (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n        (flags_gt, class_names_gt) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n    if plot_res:\n        fig = plt.figure()\n        if n_speakers > 0:\n            ax1 = fig.add_subplot(111)\n        else:\n            ax1 = fig.add_subplot(211)\n        ax1.set_yticks(np.array(range(len(class_names))))\n        ax1.axis((0, duration, -1, len(class_names)))\n        ax1.set_yticklabels(class_names)\n        ax1.plot(np.array(range(len(cls))) * mid_step + mid_step / 2.0, cls)\n    (purity_cluster_m, purity_speaker_m) = (-1, -1)\n    if os.path.isfile(gt_file):\n        if plot_res:\n            ax1.plot(np.array(range(len(flags_gt))) * mid_step + mid_step / 2.0, flags_gt, 'r')\n        (purity_cluster_m, purity_speaker_m) = evaluate_speaker_diarization(cls, flags_gt)\n        print('{0:.1f}\\t{1:.1f}'.format(100 * purity_cluster_m, 100 * purity_speaker_m))\n        if plot_res:\n            plt.title('Cluster purity: {0:.1f}% - Speaker purity: {1:.1f}%'.format(100 * purity_cluster_m, 100 * purity_speaker_m))\n    if plot_res:\n        plt.xlabel('time (seconds)')\n        if n_speakers <= 0:\n            plt.subplot(212)\n            plt.plot(s_range, sil_all)\n            plt.xlabel('number of clusters')\n            plt.ylabel(\"average clustering's sillouette\")\n        plt.show()\n    return (cls, purity_cluster_m, purity_speaker_m)",
        "mutated": [
            "def speaker_diarization(filename, n_speakers, mid_window=1.0, mid_step=0.1, short_window=0.1, lda_dim=0, plot_res=False):\n    if False:\n        i = 10\n    '\\n    ARGUMENTS:\\n        - filename:        the name of the WAV file to be analyzed\\n        - n_speakers       the number of speakers (clusters) in\\n                           the recording (<=0 for unknown)\\n        - mid_window (opt)    mid-term window size\\n        - mid_step (opt)    mid-term window step\\n        - short_window  (opt)    short-term window size\\n        - lda_dim (opt     LDA dimension (0 for no LDA)\\n        - plot_res         (opt)   0 for not plotting the results 1 for plotting\\n    '\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(filename)\n    signal = audioBasicIO.stereo_to_mono(signal)\n    duration = len(signal) / sampling_rate\n    base_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'data/models')\n    (classifier_all, mean_all, std_all, class_names_all, _, _, _, _, _) = at.load_model(os.path.join(base_dir, 'svm_rbf_speaker_10'))\n    (classifier_fm, mean_fm, std_fm, class_names_fm, _, _, _, _, _) = at.load_model(os.path.join(base_dir, 'svm_rbf_speaker_male_female'))\n    (mid_feats, st_feats, a) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    mid_term_features = np.zeros((mid_feats.shape[0] + len(class_names_all) + len(class_names_fm), mid_feats.shape[1]))\n    for index in range(mid_feats.shape[1]):\n        feature_norm_all = (mid_feats[:, index] - mean_all) / std_all\n        feature_norm_fm = (mid_feats[:, index] - mean_fm) / std_fm\n        (_, p1) = at.classifier_wrapper(classifier_all, 'svm_rbf', feature_norm_all)\n        (_, p2) = at.classifier_wrapper(classifier_fm, 'svm_rbf', feature_norm_fm)\n        start = mid_feats.shape[0]\n        end = mid_feats.shape[0] + len(class_names_all)\n        mid_term_features[0:mid_feats.shape[0], index] = mid_feats[:, index]\n        mid_term_features[start:end, index] = p1 + 0.0001\n        mid_term_features[end:, index] = p2 + 0.0001\n    scaler = StandardScaler()\n    mid_feats_norm = scaler.fit_transform(mid_term_features.T)\n    dist_all = np.sum(distance.squareform(distance.pdist(mid_feats_norm.T)), axis=0)\n    m_dist_all = np.mean(dist_all)\n    i_non_outliers = np.nonzero(dist_all < 1.1 * m_dist_all)[0]\n    mt_feats_norm_or = mid_feats_norm\n    mid_feats_norm = mid_feats_norm[:, i_non_outliers]\n    if lda_dim > 0:\n        window_ratio = int(round(mid_window / short_window))\n        step_ratio = int(round(short_window / short_window))\n        mt_feats_to_red = []\n        num_of_features = len(st_feats)\n        num_of_stats = 2\n        for index in range(num_of_stats * num_of_features):\n            mt_feats_to_red.append([])\n        for index in range(num_of_features):\n            cur_pos = 0\n            feat_len = len(st_feats[index])\n            while cur_pos < feat_len:\n                n1 = cur_pos\n                n2 = cur_pos + window_ratio\n                if n2 > feat_len:\n                    n2 = feat_len\n                short_features = st_feats[index][n1:n2]\n                mt_feats_to_red[index].append(np.mean(short_features))\n                mt_feats_to_red[index + num_of_features].append(np.std(short_features))\n                cur_pos += step_ratio\n        mt_feats_to_red = np.array(mt_feats_to_red)\n        mt_feats_to_red_2 = np.zeros((mt_feats_to_red.shape[0] + len(class_names_all) + len(class_names_fm), mt_feats_to_red.shape[1]))\n        limit = mt_feats_to_red.shape[0] + len(class_names_all)\n        for index in range(mt_feats_to_red.shape[1]):\n            feature_norm_all = (mt_feats_to_red[:, index] - mean_all) / std_all\n            feature_norm_fm = (mt_feats_to_red[:, index] - mean_fm) / std_fm\n            (_, p1) = at.classifier_wrapper(classifier_all, 'svm_rbf', feature_norm_all)\n            (_, p2) = at.classifier_wrapper(classifier_fm, 'svm_rbf', feature_norm_fm)\n            mt_feats_to_red_2[0:mt_feats_to_red.shape[0], index] = mt_feats_to_red[:, index]\n            mt_feats_to_red_2[mt_feats_to_red.shape[0]:limit, index] = p1 + 0.0001\n            mt_feats_to_red_2[limit:, index] = p2 + 0.0001\n        mt_feats_to_red = mt_feats_to_red_2\n        scaler = StandardScaler()\n        mt_feats_to_red = scaler.fit_transform(mt_feats_to_red.T).T\n        labels = np.zeros((mt_feats_to_red.shape[1],))\n        lda_step = 1.0\n        lda_step_ratio = lda_step / short_window\n        for index in range(labels.shape[0]):\n            labels[index] = int(index * short_window / lda_step_ratio)\n        clf = sklearn.discriminant_analysis.LinearDiscriminantAnalysis(n_components=lda_dim)\n        mid_feats_norm = clf.fit_transform(mt_feats_to_red.T, labels)\n    if n_speakers <= 0:\n        s_range = range(2, 10)\n    else:\n        s_range = [n_speakers]\n    cluster_labels = []\n    sil_all = []\n    cluster_centers = []\n    for speakers in s_range:\n        k_means = sklearn.cluster.KMeans(n_clusters=speakers)\n        k_means.fit(mid_feats_norm)\n        cls = k_means.labels_\n        cluster_labels.append(cls)\n        sil_1 = []\n        sil_2 = []\n        for c in range(speakers):\n            clust_per_cent = np.nonzero(cls == c)[0].shape[0] / float(len(cls))\n            if clust_per_cent < 0.02:\n                sil_1.append(0.0)\n                sil_2.append(0.0)\n            else:\n                mt_feats_norm_temp = mid_feats_norm[cls == c, :]\n                dist = distance.pdist(mt_feats_norm_temp.T)\n                sil_1.append(np.mean(dist) * clust_per_cent)\n                sil_temp = []\n                for c2 in range(speakers):\n                    if c2 != c:\n                        clust_per_cent_2 = np.nonzero(cls == c2)[0].shape[0] / float(len(cls))\n                        mid_features_temp = mid_feats_norm[cls == c2, :]\n                        dist = distance.cdist(mt_feats_norm_temp, mid_features_temp)\n                        sil_temp.append(np.mean(dist) * (clust_per_cent + clust_per_cent_2) / 2.0)\n                sil_temp = np.array(sil_temp)\n                sil_2.append(min(sil_temp))\n        sil_1 = np.array(sil_1)\n        sil_2 = np.array(sil_2)\n        sil = []\n        for c in range(speakers):\n            sil.append((sil_2[c] - sil_1[c]) / (max(sil_2[c], sil_1[c]) + 1e-05))\n        sil_all.append(np.mean(sil))\n    imax = int(np.argmax(sil_all))\n    num_speakers = s_range[imax]\n    if lda_dim <= 0:\n        for index in range(1):\n            (start_prob, transmat, means, cov) = train_hmm_compute_statistics(mt_feats_norm_or.T, cls)\n            hmm = hmmlearn.hmm.GaussianHMM(start_prob.shape[0], 'diag')\n            hmm.startprob_ = start_prob\n            hmm.transmat_ = transmat\n            hmm.means_ = means\n            hmm.covars_ = cov\n            cls = hmm.predict(mt_feats_norm_or)\n    cls = scipy.signal.medfilt(cls, 5)\n    class_names = ['speaker{0:d}'.format(c) for c in range(num_speakers)]\n    gt_file = filename.replace('.wav', '.segments')\n    if os.path.isfile(gt_file):\n        (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n        (flags_gt, class_names_gt) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n    if plot_res:\n        fig = plt.figure()\n        if n_speakers > 0:\n            ax1 = fig.add_subplot(111)\n        else:\n            ax1 = fig.add_subplot(211)\n        ax1.set_yticks(np.array(range(len(class_names))))\n        ax1.axis((0, duration, -1, len(class_names)))\n        ax1.set_yticklabels(class_names)\n        ax1.plot(np.array(range(len(cls))) * mid_step + mid_step / 2.0, cls)\n    (purity_cluster_m, purity_speaker_m) = (-1, -1)\n    if os.path.isfile(gt_file):\n        if plot_res:\n            ax1.plot(np.array(range(len(flags_gt))) * mid_step + mid_step / 2.0, flags_gt, 'r')\n        (purity_cluster_m, purity_speaker_m) = evaluate_speaker_diarization(cls, flags_gt)\n        print('{0:.1f}\\t{1:.1f}'.format(100 * purity_cluster_m, 100 * purity_speaker_m))\n        if plot_res:\n            plt.title('Cluster purity: {0:.1f}% - Speaker purity: {1:.1f}%'.format(100 * purity_cluster_m, 100 * purity_speaker_m))\n    if plot_res:\n        plt.xlabel('time (seconds)')\n        if n_speakers <= 0:\n            plt.subplot(212)\n            plt.plot(s_range, sil_all)\n            plt.xlabel('number of clusters')\n            plt.ylabel(\"average clustering's sillouette\")\n        plt.show()\n    return (cls, purity_cluster_m, purity_speaker_m)",
            "def speaker_diarization(filename, n_speakers, mid_window=1.0, mid_step=0.1, short_window=0.1, lda_dim=0, plot_res=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    ARGUMENTS:\\n        - filename:        the name of the WAV file to be analyzed\\n        - n_speakers       the number of speakers (clusters) in\\n                           the recording (<=0 for unknown)\\n        - mid_window (opt)    mid-term window size\\n        - mid_step (opt)    mid-term window step\\n        - short_window  (opt)    short-term window size\\n        - lda_dim (opt     LDA dimension (0 for no LDA)\\n        - plot_res         (opt)   0 for not plotting the results 1 for plotting\\n    '\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(filename)\n    signal = audioBasicIO.stereo_to_mono(signal)\n    duration = len(signal) / sampling_rate\n    base_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'data/models')\n    (classifier_all, mean_all, std_all, class_names_all, _, _, _, _, _) = at.load_model(os.path.join(base_dir, 'svm_rbf_speaker_10'))\n    (classifier_fm, mean_fm, std_fm, class_names_fm, _, _, _, _, _) = at.load_model(os.path.join(base_dir, 'svm_rbf_speaker_male_female'))\n    (mid_feats, st_feats, a) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    mid_term_features = np.zeros((mid_feats.shape[0] + len(class_names_all) + len(class_names_fm), mid_feats.shape[1]))\n    for index in range(mid_feats.shape[1]):\n        feature_norm_all = (mid_feats[:, index] - mean_all) / std_all\n        feature_norm_fm = (mid_feats[:, index] - mean_fm) / std_fm\n        (_, p1) = at.classifier_wrapper(classifier_all, 'svm_rbf', feature_norm_all)\n        (_, p2) = at.classifier_wrapper(classifier_fm, 'svm_rbf', feature_norm_fm)\n        start = mid_feats.shape[0]\n        end = mid_feats.shape[0] + len(class_names_all)\n        mid_term_features[0:mid_feats.shape[0], index] = mid_feats[:, index]\n        mid_term_features[start:end, index] = p1 + 0.0001\n        mid_term_features[end:, index] = p2 + 0.0001\n    scaler = StandardScaler()\n    mid_feats_norm = scaler.fit_transform(mid_term_features.T)\n    dist_all = np.sum(distance.squareform(distance.pdist(mid_feats_norm.T)), axis=0)\n    m_dist_all = np.mean(dist_all)\n    i_non_outliers = np.nonzero(dist_all < 1.1 * m_dist_all)[0]\n    mt_feats_norm_or = mid_feats_norm\n    mid_feats_norm = mid_feats_norm[:, i_non_outliers]\n    if lda_dim > 0:\n        window_ratio = int(round(mid_window / short_window))\n        step_ratio = int(round(short_window / short_window))\n        mt_feats_to_red = []\n        num_of_features = len(st_feats)\n        num_of_stats = 2\n        for index in range(num_of_stats * num_of_features):\n            mt_feats_to_red.append([])\n        for index in range(num_of_features):\n            cur_pos = 0\n            feat_len = len(st_feats[index])\n            while cur_pos < feat_len:\n                n1 = cur_pos\n                n2 = cur_pos + window_ratio\n                if n2 > feat_len:\n                    n2 = feat_len\n                short_features = st_feats[index][n1:n2]\n                mt_feats_to_red[index].append(np.mean(short_features))\n                mt_feats_to_red[index + num_of_features].append(np.std(short_features))\n                cur_pos += step_ratio\n        mt_feats_to_red = np.array(mt_feats_to_red)\n        mt_feats_to_red_2 = np.zeros((mt_feats_to_red.shape[0] + len(class_names_all) + len(class_names_fm), mt_feats_to_red.shape[1]))\n        limit = mt_feats_to_red.shape[0] + len(class_names_all)\n        for index in range(mt_feats_to_red.shape[1]):\n            feature_norm_all = (mt_feats_to_red[:, index] - mean_all) / std_all\n            feature_norm_fm = (mt_feats_to_red[:, index] - mean_fm) / std_fm\n            (_, p1) = at.classifier_wrapper(classifier_all, 'svm_rbf', feature_norm_all)\n            (_, p2) = at.classifier_wrapper(classifier_fm, 'svm_rbf', feature_norm_fm)\n            mt_feats_to_red_2[0:mt_feats_to_red.shape[0], index] = mt_feats_to_red[:, index]\n            mt_feats_to_red_2[mt_feats_to_red.shape[0]:limit, index] = p1 + 0.0001\n            mt_feats_to_red_2[limit:, index] = p2 + 0.0001\n        mt_feats_to_red = mt_feats_to_red_2\n        scaler = StandardScaler()\n        mt_feats_to_red = scaler.fit_transform(mt_feats_to_red.T).T\n        labels = np.zeros((mt_feats_to_red.shape[1],))\n        lda_step = 1.0\n        lda_step_ratio = lda_step / short_window\n        for index in range(labels.shape[0]):\n            labels[index] = int(index * short_window / lda_step_ratio)\n        clf = sklearn.discriminant_analysis.LinearDiscriminantAnalysis(n_components=lda_dim)\n        mid_feats_norm = clf.fit_transform(mt_feats_to_red.T, labels)\n    if n_speakers <= 0:\n        s_range = range(2, 10)\n    else:\n        s_range = [n_speakers]\n    cluster_labels = []\n    sil_all = []\n    cluster_centers = []\n    for speakers in s_range:\n        k_means = sklearn.cluster.KMeans(n_clusters=speakers)\n        k_means.fit(mid_feats_norm)\n        cls = k_means.labels_\n        cluster_labels.append(cls)\n        sil_1 = []\n        sil_2 = []\n        for c in range(speakers):\n            clust_per_cent = np.nonzero(cls == c)[0].shape[0] / float(len(cls))\n            if clust_per_cent < 0.02:\n                sil_1.append(0.0)\n                sil_2.append(0.0)\n            else:\n                mt_feats_norm_temp = mid_feats_norm[cls == c, :]\n                dist = distance.pdist(mt_feats_norm_temp.T)\n                sil_1.append(np.mean(dist) * clust_per_cent)\n                sil_temp = []\n                for c2 in range(speakers):\n                    if c2 != c:\n                        clust_per_cent_2 = np.nonzero(cls == c2)[0].shape[0] / float(len(cls))\n                        mid_features_temp = mid_feats_norm[cls == c2, :]\n                        dist = distance.cdist(mt_feats_norm_temp, mid_features_temp)\n                        sil_temp.append(np.mean(dist) * (clust_per_cent + clust_per_cent_2) / 2.0)\n                sil_temp = np.array(sil_temp)\n                sil_2.append(min(sil_temp))\n        sil_1 = np.array(sil_1)\n        sil_2 = np.array(sil_2)\n        sil = []\n        for c in range(speakers):\n            sil.append((sil_2[c] - sil_1[c]) / (max(sil_2[c], sil_1[c]) + 1e-05))\n        sil_all.append(np.mean(sil))\n    imax = int(np.argmax(sil_all))\n    num_speakers = s_range[imax]\n    if lda_dim <= 0:\n        for index in range(1):\n            (start_prob, transmat, means, cov) = train_hmm_compute_statistics(mt_feats_norm_or.T, cls)\n            hmm = hmmlearn.hmm.GaussianHMM(start_prob.shape[0], 'diag')\n            hmm.startprob_ = start_prob\n            hmm.transmat_ = transmat\n            hmm.means_ = means\n            hmm.covars_ = cov\n            cls = hmm.predict(mt_feats_norm_or)\n    cls = scipy.signal.medfilt(cls, 5)\n    class_names = ['speaker{0:d}'.format(c) for c in range(num_speakers)]\n    gt_file = filename.replace('.wav', '.segments')\n    if os.path.isfile(gt_file):\n        (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n        (flags_gt, class_names_gt) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n    if plot_res:\n        fig = plt.figure()\n        if n_speakers > 0:\n            ax1 = fig.add_subplot(111)\n        else:\n            ax1 = fig.add_subplot(211)\n        ax1.set_yticks(np.array(range(len(class_names))))\n        ax1.axis((0, duration, -1, len(class_names)))\n        ax1.set_yticklabels(class_names)\n        ax1.plot(np.array(range(len(cls))) * mid_step + mid_step / 2.0, cls)\n    (purity_cluster_m, purity_speaker_m) = (-1, -1)\n    if os.path.isfile(gt_file):\n        if plot_res:\n            ax1.plot(np.array(range(len(flags_gt))) * mid_step + mid_step / 2.0, flags_gt, 'r')\n        (purity_cluster_m, purity_speaker_m) = evaluate_speaker_diarization(cls, flags_gt)\n        print('{0:.1f}\\t{1:.1f}'.format(100 * purity_cluster_m, 100 * purity_speaker_m))\n        if plot_res:\n            plt.title('Cluster purity: {0:.1f}% - Speaker purity: {1:.1f}%'.format(100 * purity_cluster_m, 100 * purity_speaker_m))\n    if plot_res:\n        plt.xlabel('time (seconds)')\n        if n_speakers <= 0:\n            plt.subplot(212)\n            plt.plot(s_range, sil_all)\n            plt.xlabel('number of clusters')\n            plt.ylabel(\"average clustering's sillouette\")\n        plt.show()\n    return (cls, purity_cluster_m, purity_speaker_m)",
            "def speaker_diarization(filename, n_speakers, mid_window=1.0, mid_step=0.1, short_window=0.1, lda_dim=0, plot_res=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    ARGUMENTS:\\n        - filename:        the name of the WAV file to be analyzed\\n        - n_speakers       the number of speakers (clusters) in\\n                           the recording (<=0 for unknown)\\n        - mid_window (opt)    mid-term window size\\n        - mid_step (opt)    mid-term window step\\n        - short_window  (opt)    short-term window size\\n        - lda_dim (opt     LDA dimension (0 for no LDA)\\n        - plot_res         (opt)   0 for not plotting the results 1 for plotting\\n    '\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(filename)\n    signal = audioBasicIO.stereo_to_mono(signal)\n    duration = len(signal) / sampling_rate\n    base_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'data/models')\n    (classifier_all, mean_all, std_all, class_names_all, _, _, _, _, _) = at.load_model(os.path.join(base_dir, 'svm_rbf_speaker_10'))\n    (classifier_fm, mean_fm, std_fm, class_names_fm, _, _, _, _, _) = at.load_model(os.path.join(base_dir, 'svm_rbf_speaker_male_female'))\n    (mid_feats, st_feats, a) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    mid_term_features = np.zeros((mid_feats.shape[0] + len(class_names_all) + len(class_names_fm), mid_feats.shape[1]))\n    for index in range(mid_feats.shape[1]):\n        feature_norm_all = (mid_feats[:, index] - mean_all) / std_all\n        feature_norm_fm = (mid_feats[:, index] - mean_fm) / std_fm\n        (_, p1) = at.classifier_wrapper(classifier_all, 'svm_rbf', feature_norm_all)\n        (_, p2) = at.classifier_wrapper(classifier_fm, 'svm_rbf', feature_norm_fm)\n        start = mid_feats.shape[0]\n        end = mid_feats.shape[0] + len(class_names_all)\n        mid_term_features[0:mid_feats.shape[0], index] = mid_feats[:, index]\n        mid_term_features[start:end, index] = p1 + 0.0001\n        mid_term_features[end:, index] = p2 + 0.0001\n    scaler = StandardScaler()\n    mid_feats_norm = scaler.fit_transform(mid_term_features.T)\n    dist_all = np.sum(distance.squareform(distance.pdist(mid_feats_norm.T)), axis=0)\n    m_dist_all = np.mean(dist_all)\n    i_non_outliers = np.nonzero(dist_all < 1.1 * m_dist_all)[0]\n    mt_feats_norm_or = mid_feats_norm\n    mid_feats_norm = mid_feats_norm[:, i_non_outliers]\n    if lda_dim > 0:\n        window_ratio = int(round(mid_window / short_window))\n        step_ratio = int(round(short_window / short_window))\n        mt_feats_to_red = []\n        num_of_features = len(st_feats)\n        num_of_stats = 2\n        for index in range(num_of_stats * num_of_features):\n            mt_feats_to_red.append([])\n        for index in range(num_of_features):\n            cur_pos = 0\n            feat_len = len(st_feats[index])\n            while cur_pos < feat_len:\n                n1 = cur_pos\n                n2 = cur_pos + window_ratio\n                if n2 > feat_len:\n                    n2 = feat_len\n                short_features = st_feats[index][n1:n2]\n                mt_feats_to_red[index].append(np.mean(short_features))\n                mt_feats_to_red[index + num_of_features].append(np.std(short_features))\n                cur_pos += step_ratio\n        mt_feats_to_red = np.array(mt_feats_to_red)\n        mt_feats_to_red_2 = np.zeros((mt_feats_to_red.shape[0] + len(class_names_all) + len(class_names_fm), mt_feats_to_red.shape[1]))\n        limit = mt_feats_to_red.shape[0] + len(class_names_all)\n        for index in range(mt_feats_to_red.shape[1]):\n            feature_norm_all = (mt_feats_to_red[:, index] - mean_all) / std_all\n            feature_norm_fm = (mt_feats_to_red[:, index] - mean_fm) / std_fm\n            (_, p1) = at.classifier_wrapper(classifier_all, 'svm_rbf', feature_norm_all)\n            (_, p2) = at.classifier_wrapper(classifier_fm, 'svm_rbf', feature_norm_fm)\n            mt_feats_to_red_2[0:mt_feats_to_red.shape[0], index] = mt_feats_to_red[:, index]\n            mt_feats_to_red_2[mt_feats_to_red.shape[0]:limit, index] = p1 + 0.0001\n            mt_feats_to_red_2[limit:, index] = p2 + 0.0001\n        mt_feats_to_red = mt_feats_to_red_2\n        scaler = StandardScaler()\n        mt_feats_to_red = scaler.fit_transform(mt_feats_to_red.T).T\n        labels = np.zeros((mt_feats_to_red.shape[1],))\n        lda_step = 1.0\n        lda_step_ratio = lda_step / short_window\n        for index in range(labels.shape[0]):\n            labels[index] = int(index * short_window / lda_step_ratio)\n        clf = sklearn.discriminant_analysis.LinearDiscriminantAnalysis(n_components=lda_dim)\n        mid_feats_norm = clf.fit_transform(mt_feats_to_red.T, labels)\n    if n_speakers <= 0:\n        s_range = range(2, 10)\n    else:\n        s_range = [n_speakers]\n    cluster_labels = []\n    sil_all = []\n    cluster_centers = []\n    for speakers in s_range:\n        k_means = sklearn.cluster.KMeans(n_clusters=speakers)\n        k_means.fit(mid_feats_norm)\n        cls = k_means.labels_\n        cluster_labels.append(cls)\n        sil_1 = []\n        sil_2 = []\n        for c in range(speakers):\n            clust_per_cent = np.nonzero(cls == c)[0].shape[0] / float(len(cls))\n            if clust_per_cent < 0.02:\n                sil_1.append(0.0)\n                sil_2.append(0.0)\n            else:\n                mt_feats_norm_temp = mid_feats_norm[cls == c, :]\n                dist = distance.pdist(mt_feats_norm_temp.T)\n                sil_1.append(np.mean(dist) * clust_per_cent)\n                sil_temp = []\n                for c2 in range(speakers):\n                    if c2 != c:\n                        clust_per_cent_2 = np.nonzero(cls == c2)[0].shape[0] / float(len(cls))\n                        mid_features_temp = mid_feats_norm[cls == c2, :]\n                        dist = distance.cdist(mt_feats_norm_temp, mid_features_temp)\n                        sil_temp.append(np.mean(dist) * (clust_per_cent + clust_per_cent_2) / 2.0)\n                sil_temp = np.array(sil_temp)\n                sil_2.append(min(sil_temp))\n        sil_1 = np.array(sil_1)\n        sil_2 = np.array(sil_2)\n        sil = []\n        for c in range(speakers):\n            sil.append((sil_2[c] - sil_1[c]) / (max(sil_2[c], sil_1[c]) + 1e-05))\n        sil_all.append(np.mean(sil))\n    imax = int(np.argmax(sil_all))\n    num_speakers = s_range[imax]\n    if lda_dim <= 0:\n        for index in range(1):\n            (start_prob, transmat, means, cov) = train_hmm_compute_statistics(mt_feats_norm_or.T, cls)\n            hmm = hmmlearn.hmm.GaussianHMM(start_prob.shape[0], 'diag')\n            hmm.startprob_ = start_prob\n            hmm.transmat_ = transmat\n            hmm.means_ = means\n            hmm.covars_ = cov\n            cls = hmm.predict(mt_feats_norm_or)\n    cls = scipy.signal.medfilt(cls, 5)\n    class_names = ['speaker{0:d}'.format(c) for c in range(num_speakers)]\n    gt_file = filename.replace('.wav', '.segments')\n    if os.path.isfile(gt_file):\n        (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n        (flags_gt, class_names_gt) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n    if plot_res:\n        fig = plt.figure()\n        if n_speakers > 0:\n            ax1 = fig.add_subplot(111)\n        else:\n            ax1 = fig.add_subplot(211)\n        ax1.set_yticks(np.array(range(len(class_names))))\n        ax1.axis((0, duration, -1, len(class_names)))\n        ax1.set_yticklabels(class_names)\n        ax1.plot(np.array(range(len(cls))) * mid_step + mid_step / 2.0, cls)\n    (purity_cluster_m, purity_speaker_m) = (-1, -1)\n    if os.path.isfile(gt_file):\n        if plot_res:\n            ax1.plot(np.array(range(len(flags_gt))) * mid_step + mid_step / 2.0, flags_gt, 'r')\n        (purity_cluster_m, purity_speaker_m) = evaluate_speaker_diarization(cls, flags_gt)\n        print('{0:.1f}\\t{1:.1f}'.format(100 * purity_cluster_m, 100 * purity_speaker_m))\n        if plot_res:\n            plt.title('Cluster purity: {0:.1f}% - Speaker purity: {1:.1f}%'.format(100 * purity_cluster_m, 100 * purity_speaker_m))\n    if plot_res:\n        plt.xlabel('time (seconds)')\n        if n_speakers <= 0:\n            plt.subplot(212)\n            plt.plot(s_range, sil_all)\n            plt.xlabel('number of clusters')\n            plt.ylabel(\"average clustering's sillouette\")\n        plt.show()\n    return (cls, purity_cluster_m, purity_speaker_m)",
            "def speaker_diarization(filename, n_speakers, mid_window=1.0, mid_step=0.1, short_window=0.1, lda_dim=0, plot_res=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    ARGUMENTS:\\n        - filename:        the name of the WAV file to be analyzed\\n        - n_speakers       the number of speakers (clusters) in\\n                           the recording (<=0 for unknown)\\n        - mid_window (opt)    mid-term window size\\n        - mid_step (opt)    mid-term window step\\n        - short_window  (opt)    short-term window size\\n        - lda_dim (opt     LDA dimension (0 for no LDA)\\n        - plot_res         (opt)   0 for not plotting the results 1 for plotting\\n    '\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(filename)\n    signal = audioBasicIO.stereo_to_mono(signal)\n    duration = len(signal) / sampling_rate\n    base_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'data/models')\n    (classifier_all, mean_all, std_all, class_names_all, _, _, _, _, _) = at.load_model(os.path.join(base_dir, 'svm_rbf_speaker_10'))\n    (classifier_fm, mean_fm, std_fm, class_names_fm, _, _, _, _, _) = at.load_model(os.path.join(base_dir, 'svm_rbf_speaker_male_female'))\n    (mid_feats, st_feats, a) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    mid_term_features = np.zeros((mid_feats.shape[0] + len(class_names_all) + len(class_names_fm), mid_feats.shape[1]))\n    for index in range(mid_feats.shape[1]):\n        feature_norm_all = (mid_feats[:, index] - mean_all) / std_all\n        feature_norm_fm = (mid_feats[:, index] - mean_fm) / std_fm\n        (_, p1) = at.classifier_wrapper(classifier_all, 'svm_rbf', feature_norm_all)\n        (_, p2) = at.classifier_wrapper(classifier_fm, 'svm_rbf', feature_norm_fm)\n        start = mid_feats.shape[0]\n        end = mid_feats.shape[0] + len(class_names_all)\n        mid_term_features[0:mid_feats.shape[0], index] = mid_feats[:, index]\n        mid_term_features[start:end, index] = p1 + 0.0001\n        mid_term_features[end:, index] = p2 + 0.0001\n    scaler = StandardScaler()\n    mid_feats_norm = scaler.fit_transform(mid_term_features.T)\n    dist_all = np.sum(distance.squareform(distance.pdist(mid_feats_norm.T)), axis=0)\n    m_dist_all = np.mean(dist_all)\n    i_non_outliers = np.nonzero(dist_all < 1.1 * m_dist_all)[0]\n    mt_feats_norm_or = mid_feats_norm\n    mid_feats_norm = mid_feats_norm[:, i_non_outliers]\n    if lda_dim > 0:\n        window_ratio = int(round(mid_window / short_window))\n        step_ratio = int(round(short_window / short_window))\n        mt_feats_to_red = []\n        num_of_features = len(st_feats)\n        num_of_stats = 2\n        for index in range(num_of_stats * num_of_features):\n            mt_feats_to_red.append([])\n        for index in range(num_of_features):\n            cur_pos = 0\n            feat_len = len(st_feats[index])\n            while cur_pos < feat_len:\n                n1 = cur_pos\n                n2 = cur_pos + window_ratio\n                if n2 > feat_len:\n                    n2 = feat_len\n                short_features = st_feats[index][n1:n2]\n                mt_feats_to_red[index].append(np.mean(short_features))\n                mt_feats_to_red[index + num_of_features].append(np.std(short_features))\n                cur_pos += step_ratio\n        mt_feats_to_red = np.array(mt_feats_to_red)\n        mt_feats_to_red_2 = np.zeros((mt_feats_to_red.shape[0] + len(class_names_all) + len(class_names_fm), mt_feats_to_red.shape[1]))\n        limit = mt_feats_to_red.shape[0] + len(class_names_all)\n        for index in range(mt_feats_to_red.shape[1]):\n            feature_norm_all = (mt_feats_to_red[:, index] - mean_all) / std_all\n            feature_norm_fm = (mt_feats_to_red[:, index] - mean_fm) / std_fm\n            (_, p1) = at.classifier_wrapper(classifier_all, 'svm_rbf', feature_norm_all)\n            (_, p2) = at.classifier_wrapper(classifier_fm, 'svm_rbf', feature_norm_fm)\n            mt_feats_to_red_2[0:mt_feats_to_red.shape[0], index] = mt_feats_to_red[:, index]\n            mt_feats_to_red_2[mt_feats_to_red.shape[0]:limit, index] = p1 + 0.0001\n            mt_feats_to_red_2[limit:, index] = p2 + 0.0001\n        mt_feats_to_red = mt_feats_to_red_2\n        scaler = StandardScaler()\n        mt_feats_to_red = scaler.fit_transform(mt_feats_to_red.T).T\n        labels = np.zeros((mt_feats_to_red.shape[1],))\n        lda_step = 1.0\n        lda_step_ratio = lda_step / short_window\n        for index in range(labels.shape[0]):\n            labels[index] = int(index * short_window / lda_step_ratio)\n        clf = sklearn.discriminant_analysis.LinearDiscriminantAnalysis(n_components=lda_dim)\n        mid_feats_norm = clf.fit_transform(mt_feats_to_red.T, labels)\n    if n_speakers <= 0:\n        s_range = range(2, 10)\n    else:\n        s_range = [n_speakers]\n    cluster_labels = []\n    sil_all = []\n    cluster_centers = []\n    for speakers in s_range:\n        k_means = sklearn.cluster.KMeans(n_clusters=speakers)\n        k_means.fit(mid_feats_norm)\n        cls = k_means.labels_\n        cluster_labels.append(cls)\n        sil_1 = []\n        sil_2 = []\n        for c in range(speakers):\n            clust_per_cent = np.nonzero(cls == c)[0].shape[0] / float(len(cls))\n            if clust_per_cent < 0.02:\n                sil_1.append(0.0)\n                sil_2.append(0.0)\n            else:\n                mt_feats_norm_temp = mid_feats_norm[cls == c, :]\n                dist = distance.pdist(mt_feats_norm_temp.T)\n                sil_1.append(np.mean(dist) * clust_per_cent)\n                sil_temp = []\n                for c2 in range(speakers):\n                    if c2 != c:\n                        clust_per_cent_2 = np.nonzero(cls == c2)[0].shape[0] / float(len(cls))\n                        mid_features_temp = mid_feats_norm[cls == c2, :]\n                        dist = distance.cdist(mt_feats_norm_temp, mid_features_temp)\n                        sil_temp.append(np.mean(dist) * (clust_per_cent + clust_per_cent_2) / 2.0)\n                sil_temp = np.array(sil_temp)\n                sil_2.append(min(sil_temp))\n        sil_1 = np.array(sil_1)\n        sil_2 = np.array(sil_2)\n        sil = []\n        for c in range(speakers):\n            sil.append((sil_2[c] - sil_1[c]) / (max(sil_2[c], sil_1[c]) + 1e-05))\n        sil_all.append(np.mean(sil))\n    imax = int(np.argmax(sil_all))\n    num_speakers = s_range[imax]\n    if lda_dim <= 0:\n        for index in range(1):\n            (start_prob, transmat, means, cov) = train_hmm_compute_statistics(mt_feats_norm_or.T, cls)\n            hmm = hmmlearn.hmm.GaussianHMM(start_prob.shape[0], 'diag')\n            hmm.startprob_ = start_prob\n            hmm.transmat_ = transmat\n            hmm.means_ = means\n            hmm.covars_ = cov\n            cls = hmm.predict(mt_feats_norm_or)\n    cls = scipy.signal.medfilt(cls, 5)\n    class_names = ['speaker{0:d}'.format(c) for c in range(num_speakers)]\n    gt_file = filename.replace('.wav', '.segments')\n    if os.path.isfile(gt_file):\n        (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n        (flags_gt, class_names_gt) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n    if plot_res:\n        fig = plt.figure()\n        if n_speakers > 0:\n            ax1 = fig.add_subplot(111)\n        else:\n            ax1 = fig.add_subplot(211)\n        ax1.set_yticks(np.array(range(len(class_names))))\n        ax1.axis((0, duration, -1, len(class_names)))\n        ax1.set_yticklabels(class_names)\n        ax1.plot(np.array(range(len(cls))) * mid_step + mid_step / 2.0, cls)\n    (purity_cluster_m, purity_speaker_m) = (-1, -1)\n    if os.path.isfile(gt_file):\n        if plot_res:\n            ax1.plot(np.array(range(len(flags_gt))) * mid_step + mid_step / 2.0, flags_gt, 'r')\n        (purity_cluster_m, purity_speaker_m) = evaluate_speaker_diarization(cls, flags_gt)\n        print('{0:.1f}\\t{1:.1f}'.format(100 * purity_cluster_m, 100 * purity_speaker_m))\n        if plot_res:\n            plt.title('Cluster purity: {0:.1f}% - Speaker purity: {1:.1f}%'.format(100 * purity_cluster_m, 100 * purity_speaker_m))\n    if plot_res:\n        plt.xlabel('time (seconds)')\n        if n_speakers <= 0:\n            plt.subplot(212)\n            plt.plot(s_range, sil_all)\n            plt.xlabel('number of clusters')\n            plt.ylabel(\"average clustering's sillouette\")\n        plt.show()\n    return (cls, purity_cluster_m, purity_speaker_m)",
            "def speaker_diarization(filename, n_speakers, mid_window=1.0, mid_step=0.1, short_window=0.1, lda_dim=0, plot_res=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    ARGUMENTS:\\n        - filename:        the name of the WAV file to be analyzed\\n        - n_speakers       the number of speakers (clusters) in\\n                           the recording (<=0 for unknown)\\n        - mid_window (opt)    mid-term window size\\n        - mid_step (opt)    mid-term window step\\n        - short_window  (opt)    short-term window size\\n        - lda_dim (opt     LDA dimension (0 for no LDA)\\n        - plot_res         (opt)   0 for not plotting the results 1 for plotting\\n    '\n    (sampling_rate, signal) = audioBasicIO.read_audio_file(filename)\n    signal = audioBasicIO.stereo_to_mono(signal)\n    duration = len(signal) / sampling_rate\n    base_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'data/models')\n    (classifier_all, mean_all, std_all, class_names_all, _, _, _, _, _) = at.load_model(os.path.join(base_dir, 'svm_rbf_speaker_10'))\n    (classifier_fm, mean_fm, std_fm, class_names_fm, _, _, _, _, _) = at.load_model(os.path.join(base_dir, 'svm_rbf_speaker_male_female'))\n    (mid_feats, st_feats, a) = mtf.mid_feature_extraction(signal, sampling_rate, mid_window * sampling_rate, mid_step * sampling_rate, round(sampling_rate * 0.05), round(sampling_rate * 0.05))\n    mid_term_features = np.zeros((mid_feats.shape[0] + len(class_names_all) + len(class_names_fm), mid_feats.shape[1]))\n    for index in range(mid_feats.shape[1]):\n        feature_norm_all = (mid_feats[:, index] - mean_all) / std_all\n        feature_norm_fm = (mid_feats[:, index] - mean_fm) / std_fm\n        (_, p1) = at.classifier_wrapper(classifier_all, 'svm_rbf', feature_norm_all)\n        (_, p2) = at.classifier_wrapper(classifier_fm, 'svm_rbf', feature_norm_fm)\n        start = mid_feats.shape[0]\n        end = mid_feats.shape[0] + len(class_names_all)\n        mid_term_features[0:mid_feats.shape[0], index] = mid_feats[:, index]\n        mid_term_features[start:end, index] = p1 + 0.0001\n        mid_term_features[end:, index] = p2 + 0.0001\n    scaler = StandardScaler()\n    mid_feats_norm = scaler.fit_transform(mid_term_features.T)\n    dist_all = np.sum(distance.squareform(distance.pdist(mid_feats_norm.T)), axis=0)\n    m_dist_all = np.mean(dist_all)\n    i_non_outliers = np.nonzero(dist_all < 1.1 * m_dist_all)[0]\n    mt_feats_norm_or = mid_feats_norm\n    mid_feats_norm = mid_feats_norm[:, i_non_outliers]\n    if lda_dim > 0:\n        window_ratio = int(round(mid_window / short_window))\n        step_ratio = int(round(short_window / short_window))\n        mt_feats_to_red = []\n        num_of_features = len(st_feats)\n        num_of_stats = 2\n        for index in range(num_of_stats * num_of_features):\n            mt_feats_to_red.append([])\n        for index in range(num_of_features):\n            cur_pos = 0\n            feat_len = len(st_feats[index])\n            while cur_pos < feat_len:\n                n1 = cur_pos\n                n2 = cur_pos + window_ratio\n                if n2 > feat_len:\n                    n2 = feat_len\n                short_features = st_feats[index][n1:n2]\n                mt_feats_to_red[index].append(np.mean(short_features))\n                mt_feats_to_red[index + num_of_features].append(np.std(short_features))\n                cur_pos += step_ratio\n        mt_feats_to_red = np.array(mt_feats_to_red)\n        mt_feats_to_red_2 = np.zeros((mt_feats_to_red.shape[0] + len(class_names_all) + len(class_names_fm), mt_feats_to_red.shape[1]))\n        limit = mt_feats_to_red.shape[0] + len(class_names_all)\n        for index in range(mt_feats_to_red.shape[1]):\n            feature_norm_all = (mt_feats_to_red[:, index] - mean_all) / std_all\n            feature_norm_fm = (mt_feats_to_red[:, index] - mean_fm) / std_fm\n            (_, p1) = at.classifier_wrapper(classifier_all, 'svm_rbf', feature_norm_all)\n            (_, p2) = at.classifier_wrapper(classifier_fm, 'svm_rbf', feature_norm_fm)\n            mt_feats_to_red_2[0:mt_feats_to_red.shape[0], index] = mt_feats_to_red[:, index]\n            mt_feats_to_red_2[mt_feats_to_red.shape[0]:limit, index] = p1 + 0.0001\n            mt_feats_to_red_2[limit:, index] = p2 + 0.0001\n        mt_feats_to_red = mt_feats_to_red_2\n        scaler = StandardScaler()\n        mt_feats_to_red = scaler.fit_transform(mt_feats_to_red.T).T\n        labels = np.zeros((mt_feats_to_red.shape[1],))\n        lda_step = 1.0\n        lda_step_ratio = lda_step / short_window\n        for index in range(labels.shape[0]):\n            labels[index] = int(index * short_window / lda_step_ratio)\n        clf = sklearn.discriminant_analysis.LinearDiscriminantAnalysis(n_components=lda_dim)\n        mid_feats_norm = clf.fit_transform(mt_feats_to_red.T, labels)\n    if n_speakers <= 0:\n        s_range = range(2, 10)\n    else:\n        s_range = [n_speakers]\n    cluster_labels = []\n    sil_all = []\n    cluster_centers = []\n    for speakers in s_range:\n        k_means = sklearn.cluster.KMeans(n_clusters=speakers)\n        k_means.fit(mid_feats_norm)\n        cls = k_means.labels_\n        cluster_labels.append(cls)\n        sil_1 = []\n        sil_2 = []\n        for c in range(speakers):\n            clust_per_cent = np.nonzero(cls == c)[0].shape[0] / float(len(cls))\n            if clust_per_cent < 0.02:\n                sil_1.append(0.0)\n                sil_2.append(0.0)\n            else:\n                mt_feats_norm_temp = mid_feats_norm[cls == c, :]\n                dist = distance.pdist(mt_feats_norm_temp.T)\n                sil_1.append(np.mean(dist) * clust_per_cent)\n                sil_temp = []\n                for c2 in range(speakers):\n                    if c2 != c:\n                        clust_per_cent_2 = np.nonzero(cls == c2)[0].shape[0] / float(len(cls))\n                        mid_features_temp = mid_feats_norm[cls == c2, :]\n                        dist = distance.cdist(mt_feats_norm_temp, mid_features_temp)\n                        sil_temp.append(np.mean(dist) * (clust_per_cent + clust_per_cent_2) / 2.0)\n                sil_temp = np.array(sil_temp)\n                sil_2.append(min(sil_temp))\n        sil_1 = np.array(sil_1)\n        sil_2 = np.array(sil_2)\n        sil = []\n        for c in range(speakers):\n            sil.append((sil_2[c] - sil_1[c]) / (max(sil_2[c], sil_1[c]) + 1e-05))\n        sil_all.append(np.mean(sil))\n    imax = int(np.argmax(sil_all))\n    num_speakers = s_range[imax]\n    if lda_dim <= 0:\n        for index in range(1):\n            (start_prob, transmat, means, cov) = train_hmm_compute_statistics(mt_feats_norm_or.T, cls)\n            hmm = hmmlearn.hmm.GaussianHMM(start_prob.shape[0], 'diag')\n            hmm.startprob_ = start_prob\n            hmm.transmat_ = transmat\n            hmm.means_ = means\n            hmm.covars_ = cov\n            cls = hmm.predict(mt_feats_norm_or)\n    cls = scipy.signal.medfilt(cls, 5)\n    class_names = ['speaker{0:d}'.format(c) for c in range(num_speakers)]\n    gt_file = filename.replace('.wav', '.segments')\n    if os.path.isfile(gt_file):\n        (seg_start, seg_end, seg_labs) = read_segmentation_gt(gt_file)\n        (flags_gt, class_names_gt) = segments_to_labels(seg_start, seg_end, seg_labs, mid_step)\n    if plot_res:\n        fig = plt.figure()\n        if n_speakers > 0:\n            ax1 = fig.add_subplot(111)\n        else:\n            ax1 = fig.add_subplot(211)\n        ax1.set_yticks(np.array(range(len(class_names))))\n        ax1.axis((0, duration, -1, len(class_names)))\n        ax1.set_yticklabels(class_names)\n        ax1.plot(np.array(range(len(cls))) * mid_step + mid_step / 2.0, cls)\n    (purity_cluster_m, purity_speaker_m) = (-1, -1)\n    if os.path.isfile(gt_file):\n        if plot_res:\n            ax1.plot(np.array(range(len(flags_gt))) * mid_step + mid_step / 2.0, flags_gt, 'r')\n        (purity_cluster_m, purity_speaker_m) = evaluate_speaker_diarization(cls, flags_gt)\n        print('{0:.1f}\\t{1:.1f}'.format(100 * purity_cluster_m, 100 * purity_speaker_m))\n        if plot_res:\n            plt.title('Cluster purity: {0:.1f}% - Speaker purity: {1:.1f}%'.format(100 * purity_cluster_m, 100 * purity_speaker_m))\n    if plot_res:\n        plt.xlabel('time (seconds)')\n        if n_speakers <= 0:\n            plt.subplot(212)\n            plt.plot(s_range, sil_all)\n            plt.xlabel('number of clusters')\n            plt.ylabel(\"average clustering's sillouette\")\n        plt.show()\n    return (cls, purity_cluster_m, purity_speaker_m)"
        ]
    },
    {
        "func_name": "speaker_diarization_evaluation",
        "original": "def speaker_diarization_evaluation(folder_name, lda_dimensions):\n    \"\"\"\n        This function prints the cluster purity and speaker purity for\n        each WAV file stored in a provided directory (.SEGMENT files\n         are needed as ground-truth)\n        ARGUMENTS:\n            - folder_name:     the full path of the folder where the WAV and\n                               segment (ground-truth) files are stored\n            - lda_dimensions:  a list of LDA dimensions (0 for no LDA)\n    \"\"\"\n    types = ('*.wav',)\n    wav_files = []\n    for files in types:\n        wav_files.extend(glob.glob(os.path.join(folder_name, files)))\n    wav_files = sorted(wav_files)\n    num_speakers = []\n    for wav_file in wav_files:\n        gt_file = wav_file.replace('.wav', '.segments')\n        if os.path.isfile(gt_file):\n            (_, _, seg_labs) = read_segmentation_gt(gt_file)\n            num_speakers.append(len(list(set(seg_labs))))\n        else:\n            num_speakers.append(-1)\n    for dim in lda_dimensions:\n        print('LDA = {0:d}'.format(dim))\n        for (i, wav_file) in enumerate(wav_files):\n            speaker_diarization(wav_file, num_speakers[i], 2.0, 0.2, 0.05, dim, plot_res=False)",
        "mutated": [
            "def speaker_diarization_evaluation(folder_name, lda_dimensions):\n    if False:\n        i = 10\n    '\\n        This function prints the cluster purity and speaker purity for\\n        each WAV file stored in a provided directory (.SEGMENT files\\n         are needed as ground-truth)\\n        ARGUMENTS:\\n            - folder_name:     the full path of the folder where the WAV and\\n                               segment (ground-truth) files are stored\\n            - lda_dimensions:  a list of LDA dimensions (0 for no LDA)\\n    '\n    types = ('*.wav',)\n    wav_files = []\n    for files in types:\n        wav_files.extend(glob.glob(os.path.join(folder_name, files)))\n    wav_files = sorted(wav_files)\n    num_speakers = []\n    for wav_file in wav_files:\n        gt_file = wav_file.replace('.wav', '.segments')\n        if os.path.isfile(gt_file):\n            (_, _, seg_labs) = read_segmentation_gt(gt_file)\n            num_speakers.append(len(list(set(seg_labs))))\n        else:\n            num_speakers.append(-1)\n    for dim in lda_dimensions:\n        print('LDA = {0:d}'.format(dim))\n        for (i, wav_file) in enumerate(wav_files):\n            speaker_diarization(wav_file, num_speakers[i], 2.0, 0.2, 0.05, dim, plot_res=False)",
            "def speaker_diarization_evaluation(folder_name, lda_dimensions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        This function prints the cluster purity and speaker purity for\\n        each WAV file stored in a provided directory (.SEGMENT files\\n         are needed as ground-truth)\\n        ARGUMENTS:\\n            - folder_name:     the full path of the folder where the WAV and\\n                               segment (ground-truth) files are stored\\n            - lda_dimensions:  a list of LDA dimensions (0 for no LDA)\\n    '\n    types = ('*.wav',)\n    wav_files = []\n    for files in types:\n        wav_files.extend(glob.glob(os.path.join(folder_name, files)))\n    wav_files = sorted(wav_files)\n    num_speakers = []\n    for wav_file in wav_files:\n        gt_file = wav_file.replace('.wav', '.segments')\n        if os.path.isfile(gt_file):\n            (_, _, seg_labs) = read_segmentation_gt(gt_file)\n            num_speakers.append(len(list(set(seg_labs))))\n        else:\n            num_speakers.append(-1)\n    for dim in lda_dimensions:\n        print('LDA = {0:d}'.format(dim))\n        for (i, wav_file) in enumerate(wav_files):\n            speaker_diarization(wav_file, num_speakers[i], 2.0, 0.2, 0.05, dim, plot_res=False)",
            "def speaker_diarization_evaluation(folder_name, lda_dimensions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        This function prints the cluster purity and speaker purity for\\n        each WAV file stored in a provided directory (.SEGMENT files\\n         are needed as ground-truth)\\n        ARGUMENTS:\\n            - folder_name:     the full path of the folder where the WAV and\\n                               segment (ground-truth) files are stored\\n            - lda_dimensions:  a list of LDA dimensions (0 for no LDA)\\n    '\n    types = ('*.wav',)\n    wav_files = []\n    for files in types:\n        wav_files.extend(glob.glob(os.path.join(folder_name, files)))\n    wav_files = sorted(wav_files)\n    num_speakers = []\n    for wav_file in wav_files:\n        gt_file = wav_file.replace('.wav', '.segments')\n        if os.path.isfile(gt_file):\n            (_, _, seg_labs) = read_segmentation_gt(gt_file)\n            num_speakers.append(len(list(set(seg_labs))))\n        else:\n            num_speakers.append(-1)\n    for dim in lda_dimensions:\n        print('LDA = {0:d}'.format(dim))\n        for (i, wav_file) in enumerate(wav_files):\n            speaker_diarization(wav_file, num_speakers[i], 2.0, 0.2, 0.05, dim, plot_res=False)",
            "def speaker_diarization_evaluation(folder_name, lda_dimensions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        This function prints the cluster purity and speaker purity for\\n        each WAV file stored in a provided directory (.SEGMENT files\\n         are needed as ground-truth)\\n        ARGUMENTS:\\n            - folder_name:     the full path of the folder where the WAV and\\n                               segment (ground-truth) files are stored\\n            - lda_dimensions:  a list of LDA dimensions (0 for no LDA)\\n    '\n    types = ('*.wav',)\n    wav_files = []\n    for files in types:\n        wav_files.extend(glob.glob(os.path.join(folder_name, files)))\n    wav_files = sorted(wav_files)\n    num_speakers = []\n    for wav_file in wav_files:\n        gt_file = wav_file.replace('.wav', '.segments')\n        if os.path.isfile(gt_file):\n            (_, _, seg_labs) = read_segmentation_gt(gt_file)\n            num_speakers.append(len(list(set(seg_labs))))\n        else:\n            num_speakers.append(-1)\n    for dim in lda_dimensions:\n        print('LDA = {0:d}'.format(dim))\n        for (i, wav_file) in enumerate(wav_files):\n            speaker_diarization(wav_file, num_speakers[i], 2.0, 0.2, 0.05, dim, plot_res=False)",
            "def speaker_diarization_evaluation(folder_name, lda_dimensions):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        This function prints the cluster purity and speaker purity for\\n        each WAV file stored in a provided directory (.SEGMENT files\\n         are needed as ground-truth)\\n        ARGUMENTS:\\n            - folder_name:     the full path of the folder where the WAV and\\n                               segment (ground-truth) files are stored\\n            - lda_dimensions:  a list of LDA dimensions (0 for no LDA)\\n    '\n    types = ('*.wav',)\n    wav_files = []\n    for files in types:\n        wav_files.extend(glob.glob(os.path.join(folder_name, files)))\n    wav_files = sorted(wav_files)\n    num_speakers = []\n    for wav_file in wav_files:\n        gt_file = wav_file.replace('.wav', '.segments')\n        if os.path.isfile(gt_file):\n            (_, _, seg_labs) = read_segmentation_gt(gt_file)\n            num_speakers.append(len(list(set(seg_labs))))\n        else:\n            num_speakers.append(-1)\n    for dim in lda_dimensions:\n        print('LDA = {0:d}'.format(dim))\n        for (i, wav_file) in enumerate(wav_files):\n            speaker_diarization(wav_file, num_speakers[i], 2.0, 0.2, 0.05, dim, plot_res=False)"
        ]
    },
    {
        "func_name": "music_thumbnailing",
        "original": "def music_thumbnailing(signal, sampling_rate, short_window=1.0, short_step=0.5, thumb_size=10.0, limit_1=0, limit_2=1):\n    \"\"\"\n    This function detects instances of the most representative part of a\n    music recording, also called \"music thumbnails\".\n    A technique similar to the one proposed in [1], however a wider set of\n    audio features is used instead of chroma features.\n    In particular the following steps are followed:\n     - Extract short-term audio features. Typical short-term window size: 1\n       second\n     - Compute the self-similarity matrix, i.e. all pairwise similarities\n       between feature vectors\n     - Apply a diagonal mask is as a moving average filter on the values of the\n       self-similarty matrix.\n       The size of the mask is equal to the desirable thumbnail length.\n     - Find the position of the maximum value of the new (filtered)\n       self-similarity matrix. The audio segments that correspond to the\n       diagonial around that position are the selected thumbnails\n    \n\n    ARGUMENTS:\n     - signal:            input signal\n     - sampling_rate:            sampling frequency\n     - short_window:     window size (in seconds)\n     - short_step:    window step (in seconds)\n     - thumb_size:    desider thumbnail size (in seconds)\n    \n    RETURNS:\n     - A1:            beginning of 1st thumbnail (in seconds)\n     - A2:            ending of 1st thumbnail (in seconds)\n     - B1:            beginning of 2nd thumbnail (in seconds)\n     - B2:            ending of 2nd thumbnail (in seconds)\n\n    USAGE EXAMPLE:\n       import audioFeatureExtraction as aF\n     [fs, x] = basicIO.readAudioFile(input_file)\n     [A1, A2, B1, B2] = musicThumbnailing(x, fs)\n\n    [1] Bartsch, M. A., & Wakefield, G. H. (2005). Audio thumbnailing\n    of popular music using chroma-based representations.\n    Multimedia, IEEE Transactions on, 7(1), 96-104.\n    \"\"\"\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (st_feats, _) = stf.feature_extraction(signal, sampling_rate, sampling_rate * short_window, sampling_rate * short_step)\n    sim_matrix = self_similarity_matrix(st_feats)\n    m_filter = int(round(thumb_size / short_step))\n    diagonal = np.eye(m_filter, m_filter)\n    sim_matrix = scipy.signal.convolve2d(sim_matrix, diagonal, 'valid')\n    min_sm = np.min(sim_matrix)\n    for i in range(sim_matrix.shape[0]):\n        for j in range(sim_matrix.shape[1]):\n            if abs(i - j) < 5.0 / short_step or i > j:\n                sim_matrix[i, j] = min_sm\n    sim_matrix[0:int(limit_1 * sim_matrix.shape[0]), :] = min_sm\n    sim_matrix[:, 0:int(limit_1 * sim_matrix.shape[0])] = min_sm\n    sim_matrix[int(limit_2 * sim_matrix.shape[0]):, :] = min_sm\n    sim_matrix[:, int(limit_2 * sim_matrix.shape[0]):] = min_sm\n    (rows, cols) = np.unravel_index(sim_matrix.argmax(), sim_matrix.shape)\n    i1 = rows\n    i2 = rows\n    j1 = cols\n    j2 = cols\n    while i2 - i1 < m_filter:\n        if i1 <= 0 or j1 <= 0 or i2 >= sim_matrix.shape[0] - 2 or (j2 >= sim_matrix.shape[1] - 2):\n            break\n        if sim_matrix[i1 - 1, j1 - 1] > sim_matrix[i2 + 1, j2 + 1]:\n            i1 -= 1\n            j1 -= 1\n        else:\n            i2 += 1\n            j2 += 1\n    return (short_step * i1, short_step * i2, short_step * j1, short_step * j2, sim_matrix)",
        "mutated": [
            "def music_thumbnailing(signal, sampling_rate, short_window=1.0, short_step=0.5, thumb_size=10.0, limit_1=0, limit_2=1):\n    if False:\n        i = 10\n    '\\n    This function detects instances of the most representative part of a\\n    music recording, also called \"music thumbnails\".\\n    A technique similar to the one proposed in [1], however a wider set of\\n    audio features is used instead of chroma features.\\n    In particular the following steps are followed:\\n     - Extract short-term audio features. Typical short-term window size: 1\\n       second\\n     - Compute the self-similarity matrix, i.e. all pairwise similarities\\n       between feature vectors\\n     - Apply a diagonal mask is as a moving average filter on the values of the\\n       self-similarty matrix.\\n       The size of the mask is equal to the desirable thumbnail length.\\n     - Find the position of the maximum value of the new (filtered)\\n       self-similarity matrix. The audio segments that correspond to the\\n       diagonial around that position are the selected thumbnails\\n    \\n\\n    ARGUMENTS:\\n     - signal:            input signal\\n     - sampling_rate:            sampling frequency\\n     - short_window:     window size (in seconds)\\n     - short_step:    window step (in seconds)\\n     - thumb_size:    desider thumbnail size (in seconds)\\n    \\n    RETURNS:\\n     - A1:            beginning of 1st thumbnail (in seconds)\\n     - A2:            ending of 1st thumbnail (in seconds)\\n     - B1:            beginning of 2nd thumbnail (in seconds)\\n     - B2:            ending of 2nd thumbnail (in seconds)\\n\\n    USAGE EXAMPLE:\\n       import audioFeatureExtraction as aF\\n     [fs, x] = basicIO.readAudioFile(input_file)\\n     [A1, A2, B1, B2] = musicThumbnailing(x, fs)\\n\\n    [1] Bartsch, M. A., & Wakefield, G. H. (2005). Audio thumbnailing\\n    of popular music using chroma-based representations.\\n    Multimedia, IEEE Transactions on, 7(1), 96-104.\\n    '\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (st_feats, _) = stf.feature_extraction(signal, sampling_rate, sampling_rate * short_window, sampling_rate * short_step)\n    sim_matrix = self_similarity_matrix(st_feats)\n    m_filter = int(round(thumb_size / short_step))\n    diagonal = np.eye(m_filter, m_filter)\n    sim_matrix = scipy.signal.convolve2d(sim_matrix, diagonal, 'valid')\n    min_sm = np.min(sim_matrix)\n    for i in range(sim_matrix.shape[0]):\n        for j in range(sim_matrix.shape[1]):\n            if abs(i - j) < 5.0 / short_step or i > j:\n                sim_matrix[i, j] = min_sm\n    sim_matrix[0:int(limit_1 * sim_matrix.shape[0]), :] = min_sm\n    sim_matrix[:, 0:int(limit_1 * sim_matrix.shape[0])] = min_sm\n    sim_matrix[int(limit_2 * sim_matrix.shape[0]):, :] = min_sm\n    sim_matrix[:, int(limit_2 * sim_matrix.shape[0]):] = min_sm\n    (rows, cols) = np.unravel_index(sim_matrix.argmax(), sim_matrix.shape)\n    i1 = rows\n    i2 = rows\n    j1 = cols\n    j2 = cols\n    while i2 - i1 < m_filter:\n        if i1 <= 0 or j1 <= 0 or i2 >= sim_matrix.shape[0] - 2 or (j2 >= sim_matrix.shape[1] - 2):\n            break\n        if sim_matrix[i1 - 1, j1 - 1] > sim_matrix[i2 + 1, j2 + 1]:\n            i1 -= 1\n            j1 -= 1\n        else:\n            i2 += 1\n            j2 += 1\n    return (short_step * i1, short_step * i2, short_step * j1, short_step * j2, sim_matrix)",
            "def music_thumbnailing(signal, sampling_rate, short_window=1.0, short_step=0.5, thumb_size=10.0, limit_1=0, limit_2=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n    This function detects instances of the most representative part of a\\n    music recording, also called \"music thumbnails\".\\n    A technique similar to the one proposed in [1], however a wider set of\\n    audio features is used instead of chroma features.\\n    In particular the following steps are followed:\\n     - Extract short-term audio features. Typical short-term window size: 1\\n       second\\n     - Compute the self-similarity matrix, i.e. all pairwise similarities\\n       between feature vectors\\n     - Apply a diagonal mask is as a moving average filter on the values of the\\n       self-similarty matrix.\\n       The size of the mask is equal to the desirable thumbnail length.\\n     - Find the position of the maximum value of the new (filtered)\\n       self-similarity matrix. The audio segments that correspond to the\\n       diagonial around that position are the selected thumbnails\\n    \\n\\n    ARGUMENTS:\\n     - signal:            input signal\\n     - sampling_rate:            sampling frequency\\n     - short_window:     window size (in seconds)\\n     - short_step:    window step (in seconds)\\n     - thumb_size:    desider thumbnail size (in seconds)\\n    \\n    RETURNS:\\n     - A1:            beginning of 1st thumbnail (in seconds)\\n     - A2:            ending of 1st thumbnail (in seconds)\\n     - B1:            beginning of 2nd thumbnail (in seconds)\\n     - B2:            ending of 2nd thumbnail (in seconds)\\n\\n    USAGE EXAMPLE:\\n       import audioFeatureExtraction as aF\\n     [fs, x] = basicIO.readAudioFile(input_file)\\n     [A1, A2, B1, B2] = musicThumbnailing(x, fs)\\n\\n    [1] Bartsch, M. A., & Wakefield, G. H. (2005). Audio thumbnailing\\n    of popular music using chroma-based representations.\\n    Multimedia, IEEE Transactions on, 7(1), 96-104.\\n    '\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (st_feats, _) = stf.feature_extraction(signal, sampling_rate, sampling_rate * short_window, sampling_rate * short_step)\n    sim_matrix = self_similarity_matrix(st_feats)\n    m_filter = int(round(thumb_size / short_step))\n    diagonal = np.eye(m_filter, m_filter)\n    sim_matrix = scipy.signal.convolve2d(sim_matrix, diagonal, 'valid')\n    min_sm = np.min(sim_matrix)\n    for i in range(sim_matrix.shape[0]):\n        for j in range(sim_matrix.shape[1]):\n            if abs(i - j) < 5.0 / short_step or i > j:\n                sim_matrix[i, j] = min_sm\n    sim_matrix[0:int(limit_1 * sim_matrix.shape[0]), :] = min_sm\n    sim_matrix[:, 0:int(limit_1 * sim_matrix.shape[0])] = min_sm\n    sim_matrix[int(limit_2 * sim_matrix.shape[0]):, :] = min_sm\n    sim_matrix[:, int(limit_2 * sim_matrix.shape[0]):] = min_sm\n    (rows, cols) = np.unravel_index(sim_matrix.argmax(), sim_matrix.shape)\n    i1 = rows\n    i2 = rows\n    j1 = cols\n    j2 = cols\n    while i2 - i1 < m_filter:\n        if i1 <= 0 or j1 <= 0 or i2 >= sim_matrix.shape[0] - 2 or (j2 >= sim_matrix.shape[1] - 2):\n            break\n        if sim_matrix[i1 - 1, j1 - 1] > sim_matrix[i2 + 1, j2 + 1]:\n            i1 -= 1\n            j1 -= 1\n        else:\n            i2 += 1\n            j2 += 1\n    return (short_step * i1, short_step * i2, short_step * j1, short_step * j2, sim_matrix)",
            "def music_thumbnailing(signal, sampling_rate, short_window=1.0, short_step=0.5, thumb_size=10.0, limit_1=0, limit_2=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n    This function detects instances of the most representative part of a\\n    music recording, also called \"music thumbnails\".\\n    A technique similar to the one proposed in [1], however a wider set of\\n    audio features is used instead of chroma features.\\n    In particular the following steps are followed:\\n     - Extract short-term audio features. Typical short-term window size: 1\\n       second\\n     - Compute the self-similarity matrix, i.e. all pairwise similarities\\n       between feature vectors\\n     - Apply a diagonal mask is as a moving average filter on the values of the\\n       self-similarty matrix.\\n       The size of the mask is equal to the desirable thumbnail length.\\n     - Find the position of the maximum value of the new (filtered)\\n       self-similarity matrix. The audio segments that correspond to the\\n       diagonial around that position are the selected thumbnails\\n    \\n\\n    ARGUMENTS:\\n     - signal:            input signal\\n     - sampling_rate:            sampling frequency\\n     - short_window:     window size (in seconds)\\n     - short_step:    window step (in seconds)\\n     - thumb_size:    desider thumbnail size (in seconds)\\n    \\n    RETURNS:\\n     - A1:            beginning of 1st thumbnail (in seconds)\\n     - A2:            ending of 1st thumbnail (in seconds)\\n     - B1:            beginning of 2nd thumbnail (in seconds)\\n     - B2:            ending of 2nd thumbnail (in seconds)\\n\\n    USAGE EXAMPLE:\\n       import audioFeatureExtraction as aF\\n     [fs, x] = basicIO.readAudioFile(input_file)\\n     [A1, A2, B1, B2] = musicThumbnailing(x, fs)\\n\\n    [1] Bartsch, M. A., & Wakefield, G. H. (2005). Audio thumbnailing\\n    of popular music using chroma-based representations.\\n    Multimedia, IEEE Transactions on, 7(1), 96-104.\\n    '\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (st_feats, _) = stf.feature_extraction(signal, sampling_rate, sampling_rate * short_window, sampling_rate * short_step)\n    sim_matrix = self_similarity_matrix(st_feats)\n    m_filter = int(round(thumb_size / short_step))\n    diagonal = np.eye(m_filter, m_filter)\n    sim_matrix = scipy.signal.convolve2d(sim_matrix, diagonal, 'valid')\n    min_sm = np.min(sim_matrix)\n    for i in range(sim_matrix.shape[0]):\n        for j in range(sim_matrix.shape[1]):\n            if abs(i - j) < 5.0 / short_step or i > j:\n                sim_matrix[i, j] = min_sm\n    sim_matrix[0:int(limit_1 * sim_matrix.shape[0]), :] = min_sm\n    sim_matrix[:, 0:int(limit_1 * sim_matrix.shape[0])] = min_sm\n    sim_matrix[int(limit_2 * sim_matrix.shape[0]):, :] = min_sm\n    sim_matrix[:, int(limit_2 * sim_matrix.shape[0]):] = min_sm\n    (rows, cols) = np.unravel_index(sim_matrix.argmax(), sim_matrix.shape)\n    i1 = rows\n    i2 = rows\n    j1 = cols\n    j2 = cols\n    while i2 - i1 < m_filter:\n        if i1 <= 0 or j1 <= 0 or i2 >= sim_matrix.shape[0] - 2 or (j2 >= sim_matrix.shape[1] - 2):\n            break\n        if sim_matrix[i1 - 1, j1 - 1] > sim_matrix[i2 + 1, j2 + 1]:\n            i1 -= 1\n            j1 -= 1\n        else:\n            i2 += 1\n            j2 += 1\n    return (short_step * i1, short_step * i2, short_step * j1, short_step * j2, sim_matrix)",
            "def music_thumbnailing(signal, sampling_rate, short_window=1.0, short_step=0.5, thumb_size=10.0, limit_1=0, limit_2=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n    This function detects instances of the most representative part of a\\n    music recording, also called \"music thumbnails\".\\n    A technique similar to the one proposed in [1], however a wider set of\\n    audio features is used instead of chroma features.\\n    In particular the following steps are followed:\\n     - Extract short-term audio features. Typical short-term window size: 1\\n       second\\n     - Compute the self-similarity matrix, i.e. all pairwise similarities\\n       between feature vectors\\n     - Apply a diagonal mask is as a moving average filter on the values of the\\n       self-similarty matrix.\\n       The size of the mask is equal to the desirable thumbnail length.\\n     - Find the position of the maximum value of the new (filtered)\\n       self-similarity matrix. The audio segments that correspond to the\\n       diagonial around that position are the selected thumbnails\\n    \\n\\n    ARGUMENTS:\\n     - signal:            input signal\\n     - sampling_rate:            sampling frequency\\n     - short_window:     window size (in seconds)\\n     - short_step:    window step (in seconds)\\n     - thumb_size:    desider thumbnail size (in seconds)\\n    \\n    RETURNS:\\n     - A1:            beginning of 1st thumbnail (in seconds)\\n     - A2:            ending of 1st thumbnail (in seconds)\\n     - B1:            beginning of 2nd thumbnail (in seconds)\\n     - B2:            ending of 2nd thumbnail (in seconds)\\n\\n    USAGE EXAMPLE:\\n       import audioFeatureExtraction as aF\\n     [fs, x] = basicIO.readAudioFile(input_file)\\n     [A1, A2, B1, B2] = musicThumbnailing(x, fs)\\n\\n    [1] Bartsch, M. A., & Wakefield, G. H. (2005). Audio thumbnailing\\n    of popular music using chroma-based representations.\\n    Multimedia, IEEE Transactions on, 7(1), 96-104.\\n    '\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (st_feats, _) = stf.feature_extraction(signal, sampling_rate, sampling_rate * short_window, sampling_rate * short_step)\n    sim_matrix = self_similarity_matrix(st_feats)\n    m_filter = int(round(thumb_size / short_step))\n    diagonal = np.eye(m_filter, m_filter)\n    sim_matrix = scipy.signal.convolve2d(sim_matrix, diagonal, 'valid')\n    min_sm = np.min(sim_matrix)\n    for i in range(sim_matrix.shape[0]):\n        for j in range(sim_matrix.shape[1]):\n            if abs(i - j) < 5.0 / short_step or i > j:\n                sim_matrix[i, j] = min_sm\n    sim_matrix[0:int(limit_1 * sim_matrix.shape[0]), :] = min_sm\n    sim_matrix[:, 0:int(limit_1 * sim_matrix.shape[0])] = min_sm\n    sim_matrix[int(limit_2 * sim_matrix.shape[0]):, :] = min_sm\n    sim_matrix[:, int(limit_2 * sim_matrix.shape[0]):] = min_sm\n    (rows, cols) = np.unravel_index(sim_matrix.argmax(), sim_matrix.shape)\n    i1 = rows\n    i2 = rows\n    j1 = cols\n    j2 = cols\n    while i2 - i1 < m_filter:\n        if i1 <= 0 or j1 <= 0 or i2 >= sim_matrix.shape[0] - 2 or (j2 >= sim_matrix.shape[1] - 2):\n            break\n        if sim_matrix[i1 - 1, j1 - 1] > sim_matrix[i2 + 1, j2 + 1]:\n            i1 -= 1\n            j1 -= 1\n        else:\n            i2 += 1\n            j2 += 1\n    return (short_step * i1, short_step * i2, short_step * j1, short_step * j2, sim_matrix)",
            "def music_thumbnailing(signal, sampling_rate, short_window=1.0, short_step=0.5, thumb_size=10.0, limit_1=0, limit_2=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n    This function detects instances of the most representative part of a\\n    music recording, also called \"music thumbnails\".\\n    A technique similar to the one proposed in [1], however a wider set of\\n    audio features is used instead of chroma features.\\n    In particular the following steps are followed:\\n     - Extract short-term audio features. Typical short-term window size: 1\\n       second\\n     - Compute the self-similarity matrix, i.e. all pairwise similarities\\n       between feature vectors\\n     - Apply a diagonal mask is as a moving average filter on the values of the\\n       self-similarty matrix.\\n       The size of the mask is equal to the desirable thumbnail length.\\n     - Find the position of the maximum value of the new (filtered)\\n       self-similarity matrix. The audio segments that correspond to the\\n       diagonial around that position are the selected thumbnails\\n    \\n\\n    ARGUMENTS:\\n     - signal:            input signal\\n     - sampling_rate:            sampling frequency\\n     - short_window:     window size (in seconds)\\n     - short_step:    window step (in seconds)\\n     - thumb_size:    desider thumbnail size (in seconds)\\n    \\n    RETURNS:\\n     - A1:            beginning of 1st thumbnail (in seconds)\\n     - A2:            ending of 1st thumbnail (in seconds)\\n     - B1:            beginning of 2nd thumbnail (in seconds)\\n     - B2:            ending of 2nd thumbnail (in seconds)\\n\\n    USAGE EXAMPLE:\\n       import audioFeatureExtraction as aF\\n     [fs, x] = basicIO.readAudioFile(input_file)\\n     [A1, A2, B1, B2] = musicThumbnailing(x, fs)\\n\\n    [1] Bartsch, M. A., & Wakefield, G. H. (2005). Audio thumbnailing\\n    of popular music using chroma-based representations.\\n    Multimedia, IEEE Transactions on, 7(1), 96-104.\\n    '\n    signal = audioBasicIO.stereo_to_mono(signal)\n    (st_feats, _) = stf.feature_extraction(signal, sampling_rate, sampling_rate * short_window, sampling_rate * short_step)\n    sim_matrix = self_similarity_matrix(st_feats)\n    m_filter = int(round(thumb_size / short_step))\n    diagonal = np.eye(m_filter, m_filter)\n    sim_matrix = scipy.signal.convolve2d(sim_matrix, diagonal, 'valid')\n    min_sm = np.min(sim_matrix)\n    for i in range(sim_matrix.shape[0]):\n        for j in range(sim_matrix.shape[1]):\n            if abs(i - j) < 5.0 / short_step or i > j:\n                sim_matrix[i, j] = min_sm\n    sim_matrix[0:int(limit_1 * sim_matrix.shape[0]), :] = min_sm\n    sim_matrix[:, 0:int(limit_1 * sim_matrix.shape[0])] = min_sm\n    sim_matrix[int(limit_2 * sim_matrix.shape[0]):, :] = min_sm\n    sim_matrix[:, int(limit_2 * sim_matrix.shape[0]):] = min_sm\n    (rows, cols) = np.unravel_index(sim_matrix.argmax(), sim_matrix.shape)\n    i1 = rows\n    i2 = rows\n    j1 = cols\n    j2 = cols\n    while i2 - i1 < m_filter:\n        if i1 <= 0 or j1 <= 0 or i2 >= sim_matrix.shape[0] - 2 or (j2 >= sim_matrix.shape[1] - 2):\n            break\n        if sim_matrix[i1 - 1, j1 - 1] > sim_matrix[i2 + 1, j2 + 1]:\n            i1 -= 1\n            j1 -= 1\n        else:\n            i2 += 1\n            j2 += 1\n    return (short_step * i1, short_step * i2, short_step * j1, short_step * j2, sim_matrix)"
        ]
    }
]