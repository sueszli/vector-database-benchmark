[
    {
        "func_name": "pos2posemb3d",
        "original": "def pos2posemb3d(pos, num_pos_feats=128, temperature=10000):\n    scale = 2 * math.pi\n    pos = pos * scale\n    dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=pos.device)\n    dim_t = temperature ** (2 * torch.div(dim_t, 2, rounding_mode='floor') / num_pos_feats)\n    pos_x = pos[..., 0, None] / dim_t\n    pos_y = pos[..., 1, None] / dim_t\n    pos_z = pos[..., 2, None] / dim_t\n    pos_x = torch.stack((pos_x[..., 0::2].sin(), pos_x[..., 1::2].cos()), dim=-1).flatten(-2)\n    pos_y = torch.stack((pos_y[..., 0::2].sin(), pos_y[..., 1::2].cos()), dim=-1).flatten(-2)\n    pos_z = torch.stack((pos_z[..., 0::2].sin(), pos_z[..., 1::2].cos()), dim=-1).flatten(-2)\n    posemb = torch.cat((pos_y, pos_x, pos_z), dim=-1)\n    return posemb",
        "mutated": [
            "def pos2posemb3d(pos, num_pos_feats=128, temperature=10000):\n    if False:\n        i = 10\n    scale = 2 * math.pi\n    pos = pos * scale\n    dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=pos.device)\n    dim_t = temperature ** (2 * torch.div(dim_t, 2, rounding_mode='floor') / num_pos_feats)\n    pos_x = pos[..., 0, None] / dim_t\n    pos_y = pos[..., 1, None] / dim_t\n    pos_z = pos[..., 2, None] / dim_t\n    pos_x = torch.stack((pos_x[..., 0::2].sin(), pos_x[..., 1::2].cos()), dim=-1).flatten(-2)\n    pos_y = torch.stack((pos_y[..., 0::2].sin(), pos_y[..., 1::2].cos()), dim=-1).flatten(-2)\n    pos_z = torch.stack((pos_z[..., 0::2].sin(), pos_z[..., 1::2].cos()), dim=-1).flatten(-2)\n    posemb = torch.cat((pos_y, pos_x, pos_z), dim=-1)\n    return posemb",
            "def pos2posemb3d(pos, num_pos_feats=128, temperature=10000):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    scale = 2 * math.pi\n    pos = pos * scale\n    dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=pos.device)\n    dim_t = temperature ** (2 * torch.div(dim_t, 2, rounding_mode='floor') / num_pos_feats)\n    pos_x = pos[..., 0, None] / dim_t\n    pos_y = pos[..., 1, None] / dim_t\n    pos_z = pos[..., 2, None] / dim_t\n    pos_x = torch.stack((pos_x[..., 0::2].sin(), pos_x[..., 1::2].cos()), dim=-1).flatten(-2)\n    pos_y = torch.stack((pos_y[..., 0::2].sin(), pos_y[..., 1::2].cos()), dim=-1).flatten(-2)\n    pos_z = torch.stack((pos_z[..., 0::2].sin(), pos_z[..., 1::2].cos()), dim=-1).flatten(-2)\n    posemb = torch.cat((pos_y, pos_x, pos_z), dim=-1)\n    return posemb",
            "def pos2posemb3d(pos, num_pos_feats=128, temperature=10000):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    scale = 2 * math.pi\n    pos = pos * scale\n    dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=pos.device)\n    dim_t = temperature ** (2 * torch.div(dim_t, 2, rounding_mode='floor') / num_pos_feats)\n    pos_x = pos[..., 0, None] / dim_t\n    pos_y = pos[..., 1, None] / dim_t\n    pos_z = pos[..., 2, None] / dim_t\n    pos_x = torch.stack((pos_x[..., 0::2].sin(), pos_x[..., 1::2].cos()), dim=-1).flatten(-2)\n    pos_y = torch.stack((pos_y[..., 0::2].sin(), pos_y[..., 1::2].cos()), dim=-1).flatten(-2)\n    pos_z = torch.stack((pos_z[..., 0::2].sin(), pos_z[..., 1::2].cos()), dim=-1).flatten(-2)\n    posemb = torch.cat((pos_y, pos_x, pos_z), dim=-1)\n    return posemb",
            "def pos2posemb3d(pos, num_pos_feats=128, temperature=10000):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    scale = 2 * math.pi\n    pos = pos * scale\n    dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=pos.device)\n    dim_t = temperature ** (2 * torch.div(dim_t, 2, rounding_mode='floor') / num_pos_feats)\n    pos_x = pos[..., 0, None] / dim_t\n    pos_y = pos[..., 1, None] / dim_t\n    pos_z = pos[..., 2, None] / dim_t\n    pos_x = torch.stack((pos_x[..., 0::2].sin(), pos_x[..., 1::2].cos()), dim=-1).flatten(-2)\n    pos_y = torch.stack((pos_y[..., 0::2].sin(), pos_y[..., 1::2].cos()), dim=-1).flatten(-2)\n    pos_z = torch.stack((pos_z[..., 0::2].sin(), pos_z[..., 1::2].cos()), dim=-1).flatten(-2)\n    posemb = torch.cat((pos_y, pos_x, pos_z), dim=-1)\n    return posemb",
            "def pos2posemb3d(pos, num_pos_feats=128, temperature=10000):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    scale = 2 * math.pi\n    pos = pos * scale\n    dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=pos.device)\n    dim_t = temperature ** (2 * torch.div(dim_t, 2, rounding_mode='floor') / num_pos_feats)\n    pos_x = pos[..., 0, None] / dim_t\n    pos_y = pos[..., 1, None] / dim_t\n    pos_z = pos[..., 2, None] / dim_t\n    pos_x = torch.stack((pos_x[..., 0::2].sin(), pos_x[..., 1::2].cos()), dim=-1).flatten(-2)\n    pos_y = torch.stack((pos_y[..., 0::2].sin(), pos_y[..., 1::2].cos()), dim=-1).flatten(-2)\n    pos_z = torch.stack((pos_z[..., 0::2].sin(), pos_z[..., 1::2].cos()), dim=-1).flatten(-2)\n    posemb = torch.cat((pos_y, pos_x, pos_z), dim=-1)\n    return posemb"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, se_channels, x_channels, act_layer=nn.ReLU, gate_layer=nn.Sigmoid):\n    super().__init__()\n    self.conv_reduce = nn.Conv2d(se_channels, x_channels, 1, bias=True)\n    self.act1 = act_layer()\n    self.conv_expand = nn.Conv2d(x_channels, x_channels, 1, bias=True)\n    self.gate = gate_layer()",
        "mutated": [
            "def __init__(self, se_channels, x_channels, act_layer=nn.ReLU, gate_layer=nn.Sigmoid):\n    if False:\n        i = 10\n    super().__init__()\n    self.conv_reduce = nn.Conv2d(se_channels, x_channels, 1, bias=True)\n    self.act1 = act_layer()\n    self.conv_expand = nn.Conv2d(x_channels, x_channels, 1, bias=True)\n    self.gate = gate_layer()",
            "def __init__(self, se_channels, x_channels, act_layer=nn.ReLU, gate_layer=nn.Sigmoid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.conv_reduce = nn.Conv2d(se_channels, x_channels, 1, bias=True)\n    self.act1 = act_layer()\n    self.conv_expand = nn.Conv2d(x_channels, x_channels, 1, bias=True)\n    self.gate = gate_layer()",
            "def __init__(self, se_channels, x_channels, act_layer=nn.ReLU, gate_layer=nn.Sigmoid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.conv_reduce = nn.Conv2d(se_channels, x_channels, 1, bias=True)\n    self.act1 = act_layer()\n    self.conv_expand = nn.Conv2d(x_channels, x_channels, 1, bias=True)\n    self.gate = gate_layer()",
            "def __init__(self, se_channels, x_channels, act_layer=nn.ReLU, gate_layer=nn.Sigmoid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.conv_reduce = nn.Conv2d(se_channels, x_channels, 1, bias=True)\n    self.act1 = act_layer()\n    self.conv_expand = nn.Conv2d(x_channels, x_channels, 1, bias=True)\n    self.gate = gate_layer()",
            "def __init__(self, se_channels, x_channels, act_layer=nn.ReLU, gate_layer=nn.Sigmoid):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.conv_reduce = nn.Conv2d(se_channels, x_channels, 1, bias=True)\n    self.act1 = act_layer()\n    self.conv_expand = nn.Conv2d(x_channels, x_channels, 1, bias=True)\n    self.gate = gate_layer()"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, x, x_se):\n    x_se = self.conv_reduce(x_se)\n    x_se = self.act1(x_se)\n    x_se = self.conv_expand(x_se)\n    return x * self.gate(x_se)",
        "mutated": [
            "def forward(self, x, x_se):\n    if False:\n        i = 10\n    x_se = self.conv_reduce(x_se)\n    x_se = self.act1(x_se)\n    x_se = self.conv_expand(x_se)\n    return x * self.gate(x_se)",
            "def forward(self, x, x_se):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    x_se = self.conv_reduce(x_se)\n    x_se = self.act1(x_se)\n    x_se = self.conv_expand(x_se)\n    return x * self.gate(x_se)",
            "def forward(self, x, x_se):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    x_se = self.conv_reduce(x_se)\n    x_se = self.act1(x_se)\n    x_se = self.conv_expand(x_se)\n    return x * self.gate(x_se)",
            "def forward(self, x, x_se):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    x_se = self.conv_reduce(x_se)\n    x_se = self.act1(x_se)\n    x_se = self.conv_expand(x_se)\n    return x * self.gate(x_se)",
            "def forward(self, x, x_se):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    x_se = self.conv_reduce(x_se)\n    x_se = self.act1(x_se)\n    x_se = self.conv_expand(x_se)\n    return x * self.gate(x_se)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, embed_dims=256, shared_reg_fcs=2, group_reg_dims=(2, 2, 2, 2, 2), act_layer=nn.ReLU, drop=0.0):\n    super().__init__()\n    reg_branch = []\n    for _ in range(shared_reg_fcs):\n        reg_branch.append(Linear(embed_dims, embed_dims))\n        reg_branch.append(act_layer())\n        reg_branch.append(nn.Dropout(drop))\n    self.reg_branch = nn.Sequential(*reg_branch)\n    self.task_heads = nn.ModuleList()\n    for reg_dim in group_reg_dims:\n        task_head = nn.Sequential(Linear(embed_dims, embed_dims), act_layer(), Linear(embed_dims, reg_dim))\n        self.task_heads.append(task_head)",
        "mutated": [
            "def __init__(self, embed_dims=256, shared_reg_fcs=2, group_reg_dims=(2, 2, 2, 2, 2), act_layer=nn.ReLU, drop=0.0):\n    if False:\n        i = 10\n    super().__init__()\n    reg_branch = []\n    for _ in range(shared_reg_fcs):\n        reg_branch.append(Linear(embed_dims, embed_dims))\n        reg_branch.append(act_layer())\n        reg_branch.append(nn.Dropout(drop))\n    self.reg_branch = nn.Sequential(*reg_branch)\n    self.task_heads = nn.ModuleList()\n    for reg_dim in group_reg_dims:\n        task_head = nn.Sequential(Linear(embed_dims, embed_dims), act_layer(), Linear(embed_dims, reg_dim))\n        self.task_heads.append(task_head)",
            "def __init__(self, embed_dims=256, shared_reg_fcs=2, group_reg_dims=(2, 2, 2, 2, 2), act_layer=nn.ReLU, drop=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    reg_branch = []\n    for _ in range(shared_reg_fcs):\n        reg_branch.append(Linear(embed_dims, embed_dims))\n        reg_branch.append(act_layer())\n        reg_branch.append(nn.Dropout(drop))\n    self.reg_branch = nn.Sequential(*reg_branch)\n    self.task_heads = nn.ModuleList()\n    for reg_dim in group_reg_dims:\n        task_head = nn.Sequential(Linear(embed_dims, embed_dims), act_layer(), Linear(embed_dims, reg_dim))\n        self.task_heads.append(task_head)",
            "def __init__(self, embed_dims=256, shared_reg_fcs=2, group_reg_dims=(2, 2, 2, 2, 2), act_layer=nn.ReLU, drop=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    reg_branch = []\n    for _ in range(shared_reg_fcs):\n        reg_branch.append(Linear(embed_dims, embed_dims))\n        reg_branch.append(act_layer())\n        reg_branch.append(nn.Dropout(drop))\n    self.reg_branch = nn.Sequential(*reg_branch)\n    self.task_heads = nn.ModuleList()\n    for reg_dim in group_reg_dims:\n        task_head = nn.Sequential(Linear(embed_dims, embed_dims), act_layer(), Linear(embed_dims, reg_dim))\n        self.task_heads.append(task_head)",
            "def __init__(self, embed_dims=256, shared_reg_fcs=2, group_reg_dims=(2, 2, 2, 2, 2), act_layer=nn.ReLU, drop=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    reg_branch = []\n    for _ in range(shared_reg_fcs):\n        reg_branch.append(Linear(embed_dims, embed_dims))\n        reg_branch.append(act_layer())\n        reg_branch.append(nn.Dropout(drop))\n    self.reg_branch = nn.Sequential(*reg_branch)\n    self.task_heads = nn.ModuleList()\n    for reg_dim in group_reg_dims:\n        task_head = nn.Sequential(Linear(embed_dims, embed_dims), act_layer(), Linear(embed_dims, reg_dim))\n        self.task_heads.append(task_head)",
            "def __init__(self, embed_dims=256, shared_reg_fcs=2, group_reg_dims=(2, 2, 2, 2, 2), act_layer=nn.ReLU, drop=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    reg_branch = []\n    for _ in range(shared_reg_fcs):\n        reg_branch.append(Linear(embed_dims, embed_dims))\n        reg_branch.append(act_layer())\n        reg_branch.append(nn.Dropout(drop))\n    self.reg_branch = nn.Sequential(*reg_branch)\n    self.task_heads = nn.ModuleList()\n    for reg_dim in group_reg_dims:\n        task_head = nn.Sequential(Linear(embed_dims, embed_dims), act_layer(), Linear(embed_dims, reg_dim))\n        self.task_heads.append(task_head)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, x):\n    reg_feat = self.reg_branch(x)\n    outs = []\n    for task_head in self.task_heads:\n        out = task_head(reg_feat.clone())\n        outs.append(out)\n    outs = torch.cat(outs, -1)\n    return outs",
        "mutated": [
            "def forward(self, x):\n    if False:\n        i = 10\n    reg_feat = self.reg_branch(x)\n    outs = []\n    for task_head in self.task_heads:\n        out = task_head(reg_feat.clone())\n        outs.append(out)\n    outs = torch.cat(outs, -1)\n    return outs",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    reg_feat = self.reg_branch(x)\n    outs = []\n    for task_head in self.task_heads:\n        out = task_head(reg_feat.clone())\n        outs.append(out)\n    outs = torch.cat(outs, -1)\n    return outs",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    reg_feat = self.reg_branch(x)\n    outs = []\n    for task_head in self.task_heads:\n        out = task_head(reg_feat.clone())\n        outs.append(out)\n    outs = torch.cat(outs, -1)\n    return outs",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    reg_feat = self.reg_branch(x)\n    outs = []\n    for task_head in self.task_heads:\n        out = task_head(reg_feat.clone())\n        outs.append(out)\n    outs = torch.cat(outs, -1)\n    return outs",
            "def forward(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    reg_feat = self.reg_branch(x)\n    outs = []\n    for task_head in self.task_heads:\n        out = task_head(reg_feat.clone())\n        outs.append(out)\n    outs = torch.cat(outs, -1)\n    return outs"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, num_classes, in_channels, num_query=100, num_reg_fcs=2, transformer=None, sync_cls_avg_factor=False, positional_encoding=dict(type='SinePositionalEncoding', num_feats=128, normalize=True), code_weights=None, bbox_coder=None, loss_cls=dict(type='CrossEntropyLoss', bg_cls_weight=0.1, use_sigmoid=False, loss_weight=1.0, class_weight=1.0), loss_bbox=dict(type='L1Loss', loss_weight=5.0), loss_iou=dict(type='GIoULoss', loss_weight=2.0), train_cfg=dict(assigner=dict(type='HungarianAssigner', cls_cost=dict(type='ClassificationCost', weight=1.0), reg_cost=dict(type='BBoxL1Cost', weight=5.0), iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0))), test_cfg=dict(max_per_img=100), with_position=True, with_multiview=False, depth_step=0.8, depth_num=64, LID=False, depth_start=1, position_level=0, depth_level=0, position_range=[-65, -65, -8.0, 65, 65, 8.0], group_reg_dims=(2, 2, 2, 2, 2), scalar=5, noise_scale=0.4, noise_trans=0.0, dn_weight=1.0, split=0.5, init_cfg=None, normedlinear=False, with_fpe=False, with_time=False, with_multi=False, **kwargs):\n    if 'code_size' in kwargs:\n        self.code_size = kwargs['code_size']\n    else:\n        self.code_size = 10\n    if code_weights is not None:\n        self.code_weights = code_weights\n    else:\n        self.code_weights = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]\n    self.code_weights = self.code_weights[:self.code_size]\n    self.bg_cls_weight = 0\n    self.sync_cls_avg_factor = sync_cls_avg_factor\n    class_weight = loss_cls.get('class_weight', None)\n    if class_weight is not None and self.__class__ is PETRv2DEDNHead:\n        assert isinstance(class_weight, float), f'Expected class_weight to have type float. Found {type(class_weight)}.'\n        bg_cls_weight = loss_cls.get('bg_cls_weight', class_weight)\n        assert isinstance(bg_cls_weight, float), f'Expected bg_cls_weight to have type float. Found {type(bg_cls_weight)}.'\n        class_weight = torch.ones(num_classes + 1) * class_weight\n        class_weight[num_classes] = bg_cls_weight\n        loss_cls.update({'class_weight': class_weight})\n        if 'bg_cls_weight' in loss_cls:\n            loss_cls.pop('bg_cls_weight')\n        self.bg_cls_weight = bg_cls_weight\n    if train_cfg:\n        assert 'assigner' in train_cfg, 'assigner should be provided when train_cfg is set.'\n        assigner = train_cfg['assigner']\n        self.assigner = build_assigner(assigner)\n        sampler_cfg = dict(type='PseudoSampler')\n        self.sampler = build_sampler(sampler_cfg, context=self)\n    self.num_query = num_query\n    self.num_classes = num_classes\n    self.in_channels = in_channels\n    self.num_reg_fcs = num_reg_fcs\n    self.train_cfg = train_cfg\n    self.test_cfg = test_cfg\n    self.fp16_enabled = False\n    self.embed_dims = 256\n    self.depth_step = depth_step\n    self.depth_num = depth_num\n    self.position_range = position_range\n    self.LID = LID\n    self.with_depthnet = kwargs.get('with_depthnet', False)\n    self.with_fde = kwargs.get('with_fde', False)\n    self.depth_pe = kwargs.get('depth_pe', False)\n    self.sweep_lidar = kwargs.get('sweep_lidar', False)\n    self.depth_LID = kwargs.get('depth_LID', False)\n    self.depth_LID_num = kwargs.get('depth_LID_num', 64)\n    self.depth_thresh = kwargs.get('depth_thresh', 0.0)\n    if not self.with_depthnet:\n        self.with_fde = False\n        self.depth_pe = False\n        self.sweep_lidar = False\n    self.depth_start = depth_start\n    self.de_intv = 0.5\n    if self.with_depthnet:\n        if self.depth_LID:\n            self.D = self.depth_LID_num\n        else:\n            coords_d = np.arange(self.depth_start, self.position_range[3], self.de_intv)\n            self.D = len(coords_d)\n    self.position_dim = 3 * self.depth_num\n    self.position_level = position_level\n    self.depth_level = depth_level\n    self.with_position = with_position\n    self.with_multiview = with_multiview\n    self.scalar = scalar\n    self.bbox_noise_scale = noise_scale\n    self.bbox_noise_trans = noise_trans\n    self.dn_weight = dn_weight\n    self.split = split\n    assert 'num_feats' in positional_encoding\n    num_feats = positional_encoding['num_feats']\n    assert num_feats * 2 == self.embed_dims, f'embed_dims should be exactly 2 times of num_feats. Found {self.embed_dims} and {num_feats}.'\n    self.act_cfg = transformer.get('act_cfg', dict(type='ReLU', inplace=True))\n    self.num_pred = 6\n    self.normedlinear = normedlinear\n    self.with_fpe = with_fpe\n    self.with_time = with_time\n    self.with_multi = with_multi\n    self.group_reg_dims = group_reg_dims\n    super(PETRv2DEDNHead, self).__init__(num_classes, in_channels, init_cfg=init_cfg)\n    self.loss_cls = build_loss(loss_cls)\n    self.loss_bbox = build_loss(loss_bbox)\n    self.loss_iou = build_loss(loss_iou)\n    if self.loss_cls.use_sigmoid:\n        self.cls_out_channels = num_classes\n    else:\n        self.cls_out_channels = num_classes + 1\n    self.positional_encoding = build_positional_encoding(positional_encoding)\n    self.transformer = build_transformer(transformer)\n    self.code_weights = nn.Parameter(torch.tensor(self.code_weights, requires_grad=False), requires_grad=False)\n    self.bbox_coder = build_bbox_coder(bbox_coder)\n    self.pc_range = self.bbox_coder.pc_range\n    if 'depthnet_cfg' in kwargs:\n        depthnet_cfg = kwargs['depthnet_cfg']\n    else:\n        depthnet_cfg = {}\n    self.depthnet_cfg = depthnet_cfg\n    self.loss_depth_weight = kwargs.get('loss_depth_weight', 3.0)\n    if self.with_depthnet:\n        self.depth_net = DepthNet(self.in_channels, self.in_channels, self.in_channels, self.D, **depthnet_cfg)\n    self._init_layers()",
        "mutated": [
            "def __init__(self, num_classes, in_channels, num_query=100, num_reg_fcs=2, transformer=None, sync_cls_avg_factor=False, positional_encoding=dict(type='SinePositionalEncoding', num_feats=128, normalize=True), code_weights=None, bbox_coder=None, loss_cls=dict(type='CrossEntropyLoss', bg_cls_weight=0.1, use_sigmoid=False, loss_weight=1.0, class_weight=1.0), loss_bbox=dict(type='L1Loss', loss_weight=5.0), loss_iou=dict(type='GIoULoss', loss_weight=2.0), train_cfg=dict(assigner=dict(type='HungarianAssigner', cls_cost=dict(type='ClassificationCost', weight=1.0), reg_cost=dict(type='BBoxL1Cost', weight=5.0), iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0))), test_cfg=dict(max_per_img=100), with_position=True, with_multiview=False, depth_step=0.8, depth_num=64, LID=False, depth_start=1, position_level=0, depth_level=0, position_range=[-65, -65, -8.0, 65, 65, 8.0], group_reg_dims=(2, 2, 2, 2, 2), scalar=5, noise_scale=0.4, noise_trans=0.0, dn_weight=1.0, split=0.5, init_cfg=None, normedlinear=False, with_fpe=False, with_time=False, with_multi=False, **kwargs):\n    if False:\n        i = 10\n    if 'code_size' in kwargs:\n        self.code_size = kwargs['code_size']\n    else:\n        self.code_size = 10\n    if code_weights is not None:\n        self.code_weights = code_weights\n    else:\n        self.code_weights = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]\n    self.code_weights = self.code_weights[:self.code_size]\n    self.bg_cls_weight = 0\n    self.sync_cls_avg_factor = sync_cls_avg_factor\n    class_weight = loss_cls.get('class_weight', None)\n    if class_weight is not None and self.__class__ is PETRv2DEDNHead:\n        assert isinstance(class_weight, float), f'Expected class_weight to have type float. Found {type(class_weight)}.'\n        bg_cls_weight = loss_cls.get('bg_cls_weight', class_weight)\n        assert isinstance(bg_cls_weight, float), f'Expected bg_cls_weight to have type float. Found {type(bg_cls_weight)}.'\n        class_weight = torch.ones(num_classes + 1) * class_weight\n        class_weight[num_classes] = bg_cls_weight\n        loss_cls.update({'class_weight': class_weight})\n        if 'bg_cls_weight' in loss_cls:\n            loss_cls.pop('bg_cls_weight')\n        self.bg_cls_weight = bg_cls_weight\n    if train_cfg:\n        assert 'assigner' in train_cfg, 'assigner should be provided when train_cfg is set.'\n        assigner = train_cfg['assigner']\n        self.assigner = build_assigner(assigner)\n        sampler_cfg = dict(type='PseudoSampler')\n        self.sampler = build_sampler(sampler_cfg, context=self)\n    self.num_query = num_query\n    self.num_classes = num_classes\n    self.in_channels = in_channels\n    self.num_reg_fcs = num_reg_fcs\n    self.train_cfg = train_cfg\n    self.test_cfg = test_cfg\n    self.fp16_enabled = False\n    self.embed_dims = 256\n    self.depth_step = depth_step\n    self.depth_num = depth_num\n    self.position_range = position_range\n    self.LID = LID\n    self.with_depthnet = kwargs.get('with_depthnet', False)\n    self.with_fde = kwargs.get('with_fde', False)\n    self.depth_pe = kwargs.get('depth_pe', False)\n    self.sweep_lidar = kwargs.get('sweep_lidar', False)\n    self.depth_LID = kwargs.get('depth_LID', False)\n    self.depth_LID_num = kwargs.get('depth_LID_num', 64)\n    self.depth_thresh = kwargs.get('depth_thresh', 0.0)\n    if not self.with_depthnet:\n        self.with_fde = False\n        self.depth_pe = False\n        self.sweep_lidar = False\n    self.depth_start = depth_start\n    self.de_intv = 0.5\n    if self.with_depthnet:\n        if self.depth_LID:\n            self.D = self.depth_LID_num\n        else:\n            coords_d = np.arange(self.depth_start, self.position_range[3], self.de_intv)\n            self.D = len(coords_d)\n    self.position_dim = 3 * self.depth_num\n    self.position_level = position_level\n    self.depth_level = depth_level\n    self.with_position = with_position\n    self.with_multiview = with_multiview\n    self.scalar = scalar\n    self.bbox_noise_scale = noise_scale\n    self.bbox_noise_trans = noise_trans\n    self.dn_weight = dn_weight\n    self.split = split\n    assert 'num_feats' in positional_encoding\n    num_feats = positional_encoding['num_feats']\n    assert num_feats * 2 == self.embed_dims, f'embed_dims should be exactly 2 times of num_feats. Found {self.embed_dims} and {num_feats}.'\n    self.act_cfg = transformer.get('act_cfg', dict(type='ReLU', inplace=True))\n    self.num_pred = 6\n    self.normedlinear = normedlinear\n    self.with_fpe = with_fpe\n    self.with_time = with_time\n    self.with_multi = with_multi\n    self.group_reg_dims = group_reg_dims\n    super(PETRv2DEDNHead, self).__init__(num_classes, in_channels, init_cfg=init_cfg)\n    self.loss_cls = build_loss(loss_cls)\n    self.loss_bbox = build_loss(loss_bbox)\n    self.loss_iou = build_loss(loss_iou)\n    if self.loss_cls.use_sigmoid:\n        self.cls_out_channels = num_classes\n    else:\n        self.cls_out_channels = num_classes + 1\n    self.positional_encoding = build_positional_encoding(positional_encoding)\n    self.transformer = build_transformer(transformer)\n    self.code_weights = nn.Parameter(torch.tensor(self.code_weights, requires_grad=False), requires_grad=False)\n    self.bbox_coder = build_bbox_coder(bbox_coder)\n    self.pc_range = self.bbox_coder.pc_range\n    if 'depthnet_cfg' in kwargs:\n        depthnet_cfg = kwargs['depthnet_cfg']\n    else:\n        depthnet_cfg = {}\n    self.depthnet_cfg = depthnet_cfg\n    self.loss_depth_weight = kwargs.get('loss_depth_weight', 3.0)\n    if self.with_depthnet:\n        self.depth_net = DepthNet(self.in_channels, self.in_channels, self.in_channels, self.D, **depthnet_cfg)\n    self._init_layers()",
            "def __init__(self, num_classes, in_channels, num_query=100, num_reg_fcs=2, transformer=None, sync_cls_avg_factor=False, positional_encoding=dict(type='SinePositionalEncoding', num_feats=128, normalize=True), code_weights=None, bbox_coder=None, loss_cls=dict(type='CrossEntropyLoss', bg_cls_weight=0.1, use_sigmoid=False, loss_weight=1.0, class_weight=1.0), loss_bbox=dict(type='L1Loss', loss_weight=5.0), loss_iou=dict(type='GIoULoss', loss_weight=2.0), train_cfg=dict(assigner=dict(type='HungarianAssigner', cls_cost=dict(type='ClassificationCost', weight=1.0), reg_cost=dict(type='BBoxL1Cost', weight=5.0), iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0))), test_cfg=dict(max_per_img=100), with_position=True, with_multiview=False, depth_step=0.8, depth_num=64, LID=False, depth_start=1, position_level=0, depth_level=0, position_range=[-65, -65, -8.0, 65, 65, 8.0], group_reg_dims=(2, 2, 2, 2, 2), scalar=5, noise_scale=0.4, noise_trans=0.0, dn_weight=1.0, split=0.5, init_cfg=None, normedlinear=False, with_fpe=False, with_time=False, with_multi=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if 'code_size' in kwargs:\n        self.code_size = kwargs['code_size']\n    else:\n        self.code_size = 10\n    if code_weights is not None:\n        self.code_weights = code_weights\n    else:\n        self.code_weights = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]\n    self.code_weights = self.code_weights[:self.code_size]\n    self.bg_cls_weight = 0\n    self.sync_cls_avg_factor = sync_cls_avg_factor\n    class_weight = loss_cls.get('class_weight', None)\n    if class_weight is not None and self.__class__ is PETRv2DEDNHead:\n        assert isinstance(class_weight, float), f'Expected class_weight to have type float. Found {type(class_weight)}.'\n        bg_cls_weight = loss_cls.get('bg_cls_weight', class_weight)\n        assert isinstance(bg_cls_weight, float), f'Expected bg_cls_weight to have type float. Found {type(bg_cls_weight)}.'\n        class_weight = torch.ones(num_classes + 1) * class_weight\n        class_weight[num_classes] = bg_cls_weight\n        loss_cls.update({'class_weight': class_weight})\n        if 'bg_cls_weight' in loss_cls:\n            loss_cls.pop('bg_cls_weight')\n        self.bg_cls_weight = bg_cls_weight\n    if train_cfg:\n        assert 'assigner' in train_cfg, 'assigner should be provided when train_cfg is set.'\n        assigner = train_cfg['assigner']\n        self.assigner = build_assigner(assigner)\n        sampler_cfg = dict(type='PseudoSampler')\n        self.sampler = build_sampler(sampler_cfg, context=self)\n    self.num_query = num_query\n    self.num_classes = num_classes\n    self.in_channels = in_channels\n    self.num_reg_fcs = num_reg_fcs\n    self.train_cfg = train_cfg\n    self.test_cfg = test_cfg\n    self.fp16_enabled = False\n    self.embed_dims = 256\n    self.depth_step = depth_step\n    self.depth_num = depth_num\n    self.position_range = position_range\n    self.LID = LID\n    self.with_depthnet = kwargs.get('with_depthnet', False)\n    self.with_fde = kwargs.get('with_fde', False)\n    self.depth_pe = kwargs.get('depth_pe', False)\n    self.sweep_lidar = kwargs.get('sweep_lidar', False)\n    self.depth_LID = kwargs.get('depth_LID', False)\n    self.depth_LID_num = kwargs.get('depth_LID_num', 64)\n    self.depth_thresh = kwargs.get('depth_thresh', 0.0)\n    if not self.with_depthnet:\n        self.with_fde = False\n        self.depth_pe = False\n        self.sweep_lidar = False\n    self.depth_start = depth_start\n    self.de_intv = 0.5\n    if self.with_depthnet:\n        if self.depth_LID:\n            self.D = self.depth_LID_num\n        else:\n            coords_d = np.arange(self.depth_start, self.position_range[3], self.de_intv)\n            self.D = len(coords_d)\n    self.position_dim = 3 * self.depth_num\n    self.position_level = position_level\n    self.depth_level = depth_level\n    self.with_position = with_position\n    self.with_multiview = with_multiview\n    self.scalar = scalar\n    self.bbox_noise_scale = noise_scale\n    self.bbox_noise_trans = noise_trans\n    self.dn_weight = dn_weight\n    self.split = split\n    assert 'num_feats' in positional_encoding\n    num_feats = positional_encoding['num_feats']\n    assert num_feats * 2 == self.embed_dims, f'embed_dims should be exactly 2 times of num_feats. Found {self.embed_dims} and {num_feats}.'\n    self.act_cfg = transformer.get('act_cfg', dict(type='ReLU', inplace=True))\n    self.num_pred = 6\n    self.normedlinear = normedlinear\n    self.with_fpe = with_fpe\n    self.with_time = with_time\n    self.with_multi = with_multi\n    self.group_reg_dims = group_reg_dims\n    super(PETRv2DEDNHead, self).__init__(num_classes, in_channels, init_cfg=init_cfg)\n    self.loss_cls = build_loss(loss_cls)\n    self.loss_bbox = build_loss(loss_bbox)\n    self.loss_iou = build_loss(loss_iou)\n    if self.loss_cls.use_sigmoid:\n        self.cls_out_channels = num_classes\n    else:\n        self.cls_out_channels = num_classes + 1\n    self.positional_encoding = build_positional_encoding(positional_encoding)\n    self.transformer = build_transformer(transformer)\n    self.code_weights = nn.Parameter(torch.tensor(self.code_weights, requires_grad=False), requires_grad=False)\n    self.bbox_coder = build_bbox_coder(bbox_coder)\n    self.pc_range = self.bbox_coder.pc_range\n    if 'depthnet_cfg' in kwargs:\n        depthnet_cfg = kwargs['depthnet_cfg']\n    else:\n        depthnet_cfg = {}\n    self.depthnet_cfg = depthnet_cfg\n    self.loss_depth_weight = kwargs.get('loss_depth_weight', 3.0)\n    if self.with_depthnet:\n        self.depth_net = DepthNet(self.in_channels, self.in_channels, self.in_channels, self.D, **depthnet_cfg)\n    self._init_layers()",
            "def __init__(self, num_classes, in_channels, num_query=100, num_reg_fcs=2, transformer=None, sync_cls_avg_factor=False, positional_encoding=dict(type='SinePositionalEncoding', num_feats=128, normalize=True), code_weights=None, bbox_coder=None, loss_cls=dict(type='CrossEntropyLoss', bg_cls_weight=0.1, use_sigmoid=False, loss_weight=1.0, class_weight=1.0), loss_bbox=dict(type='L1Loss', loss_weight=5.0), loss_iou=dict(type='GIoULoss', loss_weight=2.0), train_cfg=dict(assigner=dict(type='HungarianAssigner', cls_cost=dict(type='ClassificationCost', weight=1.0), reg_cost=dict(type='BBoxL1Cost', weight=5.0), iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0))), test_cfg=dict(max_per_img=100), with_position=True, with_multiview=False, depth_step=0.8, depth_num=64, LID=False, depth_start=1, position_level=0, depth_level=0, position_range=[-65, -65, -8.0, 65, 65, 8.0], group_reg_dims=(2, 2, 2, 2, 2), scalar=5, noise_scale=0.4, noise_trans=0.0, dn_weight=1.0, split=0.5, init_cfg=None, normedlinear=False, with_fpe=False, with_time=False, with_multi=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if 'code_size' in kwargs:\n        self.code_size = kwargs['code_size']\n    else:\n        self.code_size = 10\n    if code_weights is not None:\n        self.code_weights = code_weights\n    else:\n        self.code_weights = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]\n    self.code_weights = self.code_weights[:self.code_size]\n    self.bg_cls_weight = 0\n    self.sync_cls_avg_factor = sync_cls_avg_factor\n    class_weight = loss_cls.get('class_weight', None)\n    if class_weight is not None and self.__class__ is PETRv2DEDNHead:\n        assert isinstance(class_weight, float), f'Expected class_weight to have type float. Found {type(class_weight)}.'\n        bg_cls_weight = loss_cls.get('bg_cls_weight', class_weight)\n        assert isinstance(bg_cls_weight, float), f'Expected bg_cls_weight to have type float. Found {type(bg_cls_weight)}.'\n        class_weight = torch.ones(num_classes + 1) * class_weight\n        class_weight[num_classes] = bg_cls_weight\n        loss_cls.update({'class_weight': class_weight})\n        if 'bg_cls_weight' in loss_cls:\n            loss_cls.pop('bg_cls_weight')\n        self.bg_cls_weight = bg_cls_weight\n    if train_cfg:\n        assert 'assigner' in train_cfg, 'assigner should be provided when train_cfg is set.'\n        assigner = train_cfg['assigner']\n        self.assigner = build_assigner(assigner)\n        sampler_cfg = dict(type='PseudoSampler')\n        self.sampler = build_sampler(sampler_cfg, context=self)\n    self.num_query = num_query\n    self.num_classes = num_classes\n    self.in_channels = in_channels\n    self.num_reg_fcs = num_reg_fcs\n    self.train_cfg = train_cfg\n    self.test_cfg = test_cfg\n    self.fp16_enabled = False\n    self.embed_dims = 256\n    self.depth_step = depth_step\n    self.depth_num = depth_num\n    self.position_range = position_range\n    self.LID = LID\n    self.with_depthnet = kwargs.get('with_depthnet', False)\n    self.with_fde = kwargs.get('with_fde', False)\n    self.depth_pe = kwargs.get('depth_pe', False)\n    self.sweep_lidar = kwargs.get('sweep_lidar', False)\n    self.depth_LID = kwargs.get('depth_LID', False)\n    self.depth_LID_num = kwargs.get('depth_LID_num', 64)\n    self.depth_thresh = kwargs.get('depth_thresh', 0.0)\n    if not self.with_depthnet:\n        self.with_fde = False\n        self.depth_pe = False\n        self.sweep_lidar = False\n    self.depth_start = depth_start\n    self.de_intv = 0.5\n    if self.with_depthnet:\n        if self.depth_LID:\n            self.D = self.depth_LID_num\n        else:\n            coords_d = np.arange(self.depth_start, self.position_range[3], self.de_intv)\n            self.D = len(coords_d)\n    self.position_dim = 3 * self.depth_num\n    self.position_level = position_level\n    self.depth_level = depth_level\n    self.with_position = with_position\n    self.with_multiview = with_multiview\n    self.scalar = scalar\n    self.bbox_noise_scale = noise_scale\n    self.bbox_noise_trans = noise_trans\n    self.dn_weight = dn_weight\n    self.split = split\n    assert 'num_feats' in positional_encoding\n    num_feats = positional_encoding['num_feats']\n    assert num_feats * 2 == self.embed_dims, f'embed_dims should be exactly 2 times of num_feats. Found {self.embed_dims} and {num_feats}.'\n    self.act_cfg = transformer.get('act_cfg', dict(type='ReLU', inplace=True))\n    self.num_pred = 6\n    self.normedlinear = normedlinear\n    self.with_fpe = with_fpe\n    self.with_time = with_time\n    self.with_multi = with_multi\n    self.group_reg_dims = group_reg_dims\n    super(PETRv2DEDNHead, self).__init__(num_classes, in_channels, init_cfg=init_cfg)\n    self.loss_cls = build_loss(loss_cls)\n    self.loss_bbox = build_loss(loss_bbox)\n    self.loss_iou = build_loss(loss_iou)\n    if self.loss_cls.use_sigmoid:\n        self.cls_out_channels = num_classes\n    else:\n        self.cls_out_channels = num_classes + 1\n    self.positional_encoding = build_positional_encoding(positional_encoding)\n    self.transformer = build_transformer(transformer)\n    self.code_weights = nn.Parameter(torch.tensor(self.code_weights, requires_grad=False), requires_grad=False)\n    self.bbox_coder = build_bbox_coder(bbox_coder)\n    self.pc_range = self.bbox_coder.pc_range\n    if 'depthnet_cfg' in kwargs:\n        depthnet_cfg = kwargs['depthnet_cfg']\n    else:\n        depthnet_cfg = {}\n    self.depthnet_cfg = depthnet_cfg\n    self.loss_depth_weight = kwargs.get('loss_depth_weight', 3.0)\n    if self.with_depthnet:\n        self.depth_net = DepthNet(self.in_channels, self.in_channels, self.in_channels, self.D, **depthnet_cfg)\n    self._init_layers()",
            "def __init__(self, num_classes, in_channels, num_query=100, num_reg_fcs=2, transformer=None, sync_cls_avg_factor=False, positional_encoding=dict(type='SinePositionalEncoding', num_feats=128, normalize=True), code_weights=None, bbox_coder=None, loss_cls=dict(type='CrossEntropyLoss', bg_cls_weight=0.1, use_sigmoid=False, loss_weight=1.0, class_weight=1.0), loss_bbox=dict(type='L1Loss', loss_weight=5.0), loss_iou=dict(type='GIoULoss', loss_weight=2.0), train_cfg=dict(assigner=dict(type='HungarianAssigner', cls_cost=dict(type='ClassificationCost', weight=1.0), reg_cost=dict(type='BBoxL1Cost', weight=5.0), iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0))), test_cfg=dict(max_per_img=100), with_position=True, with_multiview=False, depth_step=0.8, depth_num=64, LID=False, depth_start=1, position_level=0, depth_level=0, position_range=[-65, -65, -8.0, 65, 65, 8.0], group_reg_dims=(2, 2, 2, 2, 2), scalar=5, noise_scale=0.4, noise_trans=0.0, dn_weight=1.0, split=0.5, init_cfg=None, normedlinear=False, with_fpe=False, with_time=False, with_multi=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if 'code_size' in kwargs:\n        self.code_size = kwargs['code_size']\n    else:\n        self.code_size = 10\n    if code_weights is not None:\n        self.code_weights = code_weights\n    else:\n        self.code_weights = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]\n    self.code_weights = self.code_weights[:self.code_size]\n    self.bg_cls_weight = 0\n    self.sync_cls_avg_factor = sync_cls_avg_factor\n    class_weight = loss_cls.get('class_weight', None)\n    if class_weight is not None and self.__class__ is PETRv2DEDNHead:\n        assert isinstance(class_weight, float), f'Expected class_weight to have type float. Found {type(class_weight)}.'\n        bg_cls_weight = loss_cls.get('bg_cls_weight', class_weight)\n        assert isinstance(bg_cls_weight, float), f'Expected bg_cls_weight to have type float. Found {type(bg_cls_weight)}.'\n        class_weight = torch.ones(num_classes + 1) * class_weight\n        class_weight[num_classes] = bg_cls_weight\n        loss_cls.update({'class_weight': class_weight})\n        if 'bg_cls_weight' in loss_cls:\n            loss_cls.pop('bg_cls_weight')\n        self.bg_cls_weight = bg_cls_weight\n    if train_cfg:\n        assert 'assigner' in train_cfg, 'assigner should be provided when train_cfg is set.'\n        assigner = train_cfg['assigner']\n        self.assigner = build_assigner(assigner)\n        sampler_cfg = dict(type='PseudoSampler')\n        self.sampler = build_sampler(sampler_cfg, context=self)\n    self.num_query = num_query\n    self.num_classes = num_classes\n    self.in_channels = in_channels\n    self.num_reg_fcs = num_reg_fcs\n    self.train_cfg = train_cfg\n    self.test_cfg = test_cfg\n    self.fp16_enabled = False\n    self.embed_dims = 256\n    self.depth_step = depth_step\n    self.depth_num = depth_num\n    self.position_range = position_range\n    self.LID = LID\n    self.with_depthnet = kwargs.get('with_depthnet', False)\n    self.with_fde = kwargs.get('with_fde', False)\n    self.depth_pe = kwargs.get('depth_pe', False)\n    self.sweep_lidar = kwargs.get('sweep_lidar', False)\n    self.depth_LID = kwargs.get('depth_LID', False)\n    self.depth_LID_num = kwargs.get('depth_LID_num', 64)\n    self.depth_thresh = kwargs.get('depth_thresh', 0.0)\n    if not self.with_depthnet:\n        self.with_fde = False\n        self.depth_pe = False\n        self.sweep_lidar = False\n    self.depth_start = depth_start\n    self.de_intv = 0.5\n    if self.with_depthnet:\n        if self.depth_LID:\n            self.D = self.depth_LID_num\n        else:\n            coords_d = np.arange(self.depth_start, self.position_range[3], self.de_intv)\n            self.D = len(coords_d)\n    self.position_dim = 3 * self.depth_num\n    self.position_level = position_level\n    self.depth_level = depth_level\n    self.with_position = with_position\n    self.with_multiview = with_multiview\n    self.scalar = scalar\n    self.bbox_noise_scale = noise_scale\n    self.bbox_noise_trans = noise_trans\n    self.dn_weight = dn_weight\n    self.split = split\n    assert 'num_feats' in positional_encoding\n    num_feats = positional_encoding['num_feats']\n    assert num_feats * 2 == self.embed_dims, f'embed_dims should be exactly 2 times of num_feats. Found {self.embed_dims} and {num_feats}.'\n    self.act_cfg = transformer.get('act_cfg', dict(type='ReLU', inplace=True))\n    self.num_pred = 6\n    self.normedlinear = normedlinear\n    self.with_fpe = with_fpe\n    self.with_time = with_time\n    self.with_multi = with_multi\n    self.group_reg_dims = group_reg_dims\n    super(PETRv2DEDNHead, self).__init__(num_classes, in_channels, init_cfg=init_cfg)\n    self.loss_cls = build_loss(loss_cls)\n    self.loss_bbox = build_loss(loss_bbox)\n    self.loss_iou = build_loss(loss_iou)\n    if self.loss_cls.use_sigmoid:\n        self.cls_out_channels = num_classes\n    else:\n        self.cls_out_channels = num_classes + 1\n    self.positional_encoding = build_positional_encoding(positional_encoding)\n    self.transformer = build_transformer(transformer)\n    self.code_weights = nn.Parameter(torch.tensor(self.code_weights, requires_grad=False), requires_grad=False)\n    self.bbox_coder = build_bbox_coder(bbox_coder)\n    self.pc_range = self.bbox_coder.pc_range\n    if 'depthnet_cfg' in kwargs:\n        depthnet_cfg = kwargs['depthnet_cfg']\n    else:\n        depthnet_cfg = {}\n    self.depthnet_cfg = depthnet_cfg\n    self.loss_depth_weight = kwargs.get('loss_depth_weight', 3.0)\n    if self.with_depthnet:\n        self.depth_net = DepthNet(self.in_channels, self.in_channels, self.in_channels, self.D, **depthnet_cfg)\n    self._init_layers()",
            "def __init__(self, num_classes, in_channels, num_query=100, num_reg_fcs=2, transformer=None, sync_cls_avg_factor=False, positional_encoding=dict(type='SinePositionalEncoding', num_feats=128, normalize=True), code_weights=None, bbox_coder=None, loss_cls=dict(type='CrossEntropyLoss', bg_cls_weight=0.1, use_sigmoid=False, loss_weight=1.0, class_weight=1.0), loss_bbox=dict(type='L1Loss', loss_weight=5.0), loss_iou=dict(type='GIoULoss', loss_weight=2.0), train_cfg=dict(assigner=dict(type='HungarianAssigner', cls_cost=dict(type='ClassificationCost', weight=1.0), reg_cost=dict(type='BBoxL1Cost', weight=5.0), iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0))), test_cfg=dict(max_per_img=100), with_position=True, with_multiview=False, depth_step=0.8, depth_num=64, LID=False, depth_start=1, position_level=0, depth_level=0, position_range=[-65, -65, -8.0, 65, 65, 8.0], group_reg_dims=(2, 2, 2, 2, 2), scalar=5, noise_scale=0.4, noise_trans=0.0, dn_weight=1.0, split=0.5, init_cfg=None, normedlinear=False, with_fpe=False, with_time=False, with_multi=False, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if 'code_size' in kwargs:\n        self.code_size = kwargs['code_size']\n    else:\n        self.code_size = 10\n    if code_weights is not None:\n        self.code_weights = code_weights\n    else:\n        self.code_weights = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]\n    self.code_weights = self.code_weights[:self.code_size]\n    self.bg_cls_weight = 0\n    self.sync_cls_avg_factor = sync_cls_avg_factor\n    class_weight = loss_cls.get('class_weight', None)\n    if class_weight is not None and self.__class__ is PETRv2DEDNHead:\n        assert isinstance(class_weight, float), f'Expected class_weight to have type float. Found {type(class_weight)}.'\n        bg_cls_weight = loss_cls.get('bg_cls_weight', class_weight)\n        assert isinstance(bg_cls_weight, float), f'Expected bg_cls_weight to have type float. Found {type(bg_cls_weight)}.'\n        class_weight = torch.ones(num_classes + 1) * class_weight\n        class_weight[num_classes] = bg_cls_weight\n        loss_cls.update({'class_weight': class_weight})\n        if 'bg_cls_weight' in loss_cls:\n            loss_cls.pop('bg_cls_weight')\n        self.bg_cls_weight = bg_cls_weight\n    if train_cfg:\n        assert 'assigner' in train_cfg, 'assigner should be provided when train_cfg is set.'\n        assigner = train_cfg['assigner']\n        self.assigner = build_assigner(assigner)\n        sampler_cfg = dict(type='PseudoSampler')\n        self.sampler = build_sampler(sampler_cfg, context=self)\n    self.num_query = num_query\n    self.num_classes = num_classes\n    self.in_channels = in_channels\n    self.num_reg_fcs = num_reg_fcs\n    self.train_cfg = train_cfg\n    self.test_cfg = test_cfg\n    self.fp16_enabled = False\n    self.embed_dims = 256\n    self.depth_step = depth_step\n    self.depth_num = depth_num\n    self.position_range = position_range\n    self.LID = LID\n    self.with_depthnet = kwargs.get('with_depthnet', False)\n    self.with_fde = kwargs.get('with_fde', False)\n    self.depth_pe = kwargs.get('depth_pe', False)\n    self.sweep_lidar = kwargs.get('sweep_lidar', False)\n    self.depth_LID = kwargs.get('depth_LID', False)\n    self.depth_LID_num = kwargs.get('depth_LID_num', 64)\n    self.depth_thresh = kwargs.get('depth_thresh', 0.0)\n    if not self.with_depthnet:\n        self.with_fde = False\n        self.depth_pe = False\n        self.sweep_lidar = False\n    self.depth_start = depth_start\n    self.de_intv = 0.5\n    if self.with_depthnet:\n        if self.depth_LID:\n            self.D = self.depth_LID_num\n        else:\n            coords_d = np.arange(self.depth_start, self.position_range[3], self.de_intv)\n            self.D = len(coords_d)\n    self.position_dim = 3 * self.depth_num\n    self.position_level = position_level\n    self.depth_level = depth_level\n    self.with_position = with_position\n    self.with_multiview = with_multiview\n    self.scalar = scalar\n    self.bbox_noise_scale = noise_scale\n    self.bbox_noise_trans = noise_trans\n    self.dn_weight = dn_weight\n    self.split = split\n    assert 'num_feats' in positional_encoding\n    num_feats = positional_encoding['num_feats']\n    assert num_feats * 2 == self.embed_dims, f'embed_dims should be exactly 2 times of num_feats. Found {self.embed_dims} and {num_feats}.'\n    self.act_cfg = transformer.get('act_cfg', dict(type='ReLU', inplace=True))\n    self.num_pred = 6\n    self.normedlinear = normedlinear\n    self.with_fpe = with_fpe\n    self.with_time = with_time\n    self.with_multi = with_multi\n    self.group_reg_dims = group_reg_dims\n    super(PETRv2DEDNHead, self).__init__(num_classes, in_channels, init_cfg=init_cfg)\n    self.loss_cls = build_loss(loss_cls)\n    self.loss_bbox = build_loss(loss_bbox)\n    self.loss_iou = build_loss(loss_iou)\n    if self.loss_cls.use_sigmoid:\n        self.cls_out_channels = num_classes\n    else:\n        self.cls_out_channels = num_classes + 1\n    self.positional_encoding = build_positional_encoding(positional_encoding)\n    self.transformer = build_transformer(transformer)\n    self.code_weights = nn.Parameter(torch.tensor(self.code_weights, requires_grad=False), requires_grad=False)\n    self.bbox_coder = build_bbox_coder(bbox_coder)\n    self.pc_range = self.bbox_coder.pc_range\n    if 'depthnet_cfg' in kwargs:\n        depthnet_cfg = kwargs['depthnet_cfg']\n    else:\n        depthnet_cfg = {}\n    self.depthnet_cfg = depthnet_cfg\n    self.loss_depth_weight = kwargs.get('loss_depth_weight', 3.0)\n    if self.with_depthnet:\n        self.depth_net = DepthNet(self.in_channels, self.in_channels, self.in_channels, self.D, **depthnet_cfg)\n    self._init_layers()"
        ]
    },
    {
        "func_name": "_init_layers",
        "original": "def _init_layers(self):\n    \"\"\"Initialize layers of the transformer head.\"\"\"\n    if not self.with_depthnet:\n        self.input_proj = Conv2d(self.in_channels, self.embed_dims, kernel_size=1)\n    cls_branch = []\n    for _ in range(self.num_reg_fcs):\n        cls_branch.append(Linear(self.embed_dims, self.embed_dims))\n        cls_branch.append(nn.LayerNorm(self.embed_dims))\n        cls_branch.append(nn.ReLU(inplace=True))\n    if self.normedlinear:\n        cls_branch.append(NormedLinear(self.embed_dims, self.cls_out_channels))\n    else:\n        cls_branch.append(Linear(self.embed_dims, self.cls_out_channels))\n    fc_cls = nn.Sequential(*cls_branch)\n    if self.with_multi:\n        reg_branch = RegLayer(self.embed_dims, self.num_reg_fcs, self.group_reg_dims)\n    else:\n        reg_branch = []\n        for _ in range(self.num_reg_fcs):\n            reg_branch.append(Linear(self.embed_dims, self.embed_dims))\n            reg_branch.append(nn.ReLU())\n        reg_branch.append(Linear(self.embed_dims, self.code_size))\n        reg_branch = nn.Sequential(*reg_branch)\n    self.cls_branches = nn.ModuleList([copy.deepcopy(fc_cls) for _ in range(self.num_pred)])\n    self.reg_branches = nn.ModuleList([copy.deepcopy(reg_branch) for _ in range(self.num_pred)])\n    if self.with_multiview:\n        self.adapt_pos3d = nn.Sequential(nn.Conv2d(self.embed_dims * 3 // 2, self.embed_dims * 4, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(self.embed_dims * 4, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    else:\n        self.adapt_pos3d = nn.Sequential(nn.Conv2d(self.embed_dims, self.embed_dims, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(self.embed_dims, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    if self.with_position:\n        chan_in = self.position_dim\n        chan_mid = self.embed_dims * 4\n        self.position_encoder = nn.Sequential(nn.Conv2d(chan_in, chan_mid, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(chan_mid, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    if self.depth_pe:\n        self.depth_encoder = nn.Conv2d(self.D, self.embed_dims, kernel_size=1, stride=1, padding=0)\n    self.reference_points = nn.Embedding(self.num_query, 3)\n    self.query_embedding = nn.Sequential(nn.Linear(self.embed_dims * 3 // 2, self.embed_dims), nn.ReLU(), nn.Linear(self.embed_dims, self.embed_dims))\n    if self.with_fpe:\n        self.fpe = SELayer(self.embed_dims, self.embed_dims)\n    if self.with_fde:\n        self.fde = SELayer(self.embed_dims, self.D)",
        "mutated": [
            "def _init_layers(self):\n    if False:\n        i = 10\n    'Initialize layers of the transformer head.'\n    if not self.with_depthnet:\n        self.input_proj = Conv2d(self.in_channels, self.embed_dims, kernel_size=1)\n    cls_branch = []\n    for _ in range(self.num_reg_fcs):\n        cls_branch.append(Linear(self.embed_dims, self.embed_dims))\n        cls_branch.append(nn.LayerNorm(self.embed_dims))\n        cls_branch.append(nn.ReLU(inplace=True))\n    if self.normedlinear:\n        cls_branch.append(NormedLinear(self.embed_dims, self.cls_out_channels))\n    else:\n        cls_branch.append(Linear(self.embed_dims, self.cls_out_channels))\n    fc_cls = nn.Sequential(*cls_branch)\n    if self.with_multi:\n        reg_branch = RegLayer(self.embed_dims, self.num_reg_fcs, self.group_reg_dims)\n    else:\n        reg_branch = []\n        for _ in range(self.num_reg_fcs):\n            reg_branch.append(Linear(self.embed_dims, self.embed_dims))\n            reg_branch.append(nn.ReLU())\n        reg_branch.append(Linear(self.embed_dims, self.code_size))\n        reg_branch = nn.Sequential(*reg_branch)\n    self.cls_branches = nn.ModuleList([copy.deepcopy(fc_cls) for _ in range(self.num_pred)])\n    self.reg_branches = nn.ModuleList([copy.deepcopy(reg_branch) for _ in range(self.num_pred)])\n    if self.with_multiview:\n        self.adapt_pos3d = nn.Sequential(nn.Conv2d(self.embed_dims * 3 // 2, self.embed_dims * 4, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(self.embed_dims * 4, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    else:\n        self.adapt_pos3d = nn.Sequential(nn.Conv2d(self.embed_dims, self.embed_dims, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(self.embed_dims, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    if self.with_position:\n        chan_in = self.position_dim\n        chan_mid = self.embed_dims * 4\n        self.position_encoder = nn.Sequential(nn.Conv2d(chan_in, chan_mid, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(chan_mid, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    if self.depth_pe:\n        self.depth_encoder = nn.Conv2d(self.D, self.embed_dims, kernel_size=1, stride=1, padding=0)\n    self.reference_points = nn.Embedding(self.num_query, 3)\n    self.query_embedding = nn.Sequential(nn.Linear(self.embed_dims * 3 // 2, self.embed_dims), nn.ReLU(), nn.Linear(self.embed_dims, self.embed_dims))\n    if self.with_fpe:\n        self.fpe = SELayer(self.embed_dims, self.embed_dims)\n    if self.with_fde:\n        self.fde = SELayer(self.embed_dims, self.D)",
            "def _init_layers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize layers of the transformer head.'\n    if not self.with_depthnet:\n        self.input_proj = Conv2d(self.in_channels, self.embed_dims, kernel_size=1)\n    cls_branch = []\n    for _ in range(self.num_reg_fcs):\n        cls_branch.append(Linear(self.embed_dims, self.embed_dims))\n        cls_branch.append(nn.LayerNorm(self.embed_dims))\n        cls_branch.append(nn.ReLU(inplace=True))\n    if self.normedlinear:\n        cls_branch.append(NormedLinear(self.embed_dims, self.cls_out_channels))\n    else:\n        cls_branch.append(Linear(self.embed_dims, self.cls_out_channels))\n    fc_cls = nn.Sequential(*cls_branch)\n    if self.with_multi:\n        reg_branch = RegLayer(self.embed_dims, self.num_reg_fcs, self.group_reg_dims)\n    else:\n        reg_branch = []\n        for _ in range(self.num_reg_fcs):\n            reg_branch.append(Linear(self.embed_dims, self.embed_dims))\n            reg_branch.append(nn.ReLU())\n        reg_branch.append(Linear(self.embed_dims, self.code_size))\n        reg_branch = nn.Sequential(*reg_branch)\n    self.cls_branches = nn.ModuleList([copy.deepcopy(fc_cls) for _ in range(self.num_pred)])\n    self.reg_branches = nn.ModuleList([copy.deepcopy(reg_branch) for _ in range(self.num_pred)])\n    if self.with_multiview:\n        self.adapt_pos3d = nn.Sequential(nn.Conv2d(self.embed_dims * 3 // 2, self.embed_dims * 4, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(self.embed_dims * 4, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    else:\n        self.adapt_pos3d = nn.Sequential(nn.Conv2d(self.embed_dims, self.embed_dims, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(self.embed_dims, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    if self.with_position:\n        chan_in = self.position_dim\n        chan_mid = self.embed_dims * 4\n        self.position_encoder = nn.Sequential(nn.Conv2d(chan_in, chan_mid, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(chan_mid, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    if self.depth_pe:\n        self.depth_encoder = nn.Conv2d(self.D, self.embed_dims, kernel_size=1, stride=1, padding=0)\n    self.reference_points = nn.Embedding(self.num_query, 3)\n    self.query_embedding = nn.Sequential(nn.Linear(self.embed_dims * 3 // 2, self.embed_dims), nn.ReLU(), nn.Linear(self.embed_dims, self.embed_dims))\n    if self.with_fpe:\n        self.fpe = SELayer(self.embed_dims, self.embed_dims)\n    if self.with_fde:\n        self.fde = SELayer(self.embed_dims, self.D)",
            "def _init_layers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize layers of the transformer head.'\n    if not self.with_depthnet:\n        self.input_proj = Conv2d(self.in_channels, self.embed_dims, kernel_size=1)\n    cls_branch = []\n    for _ in range(self.num_reg_fcs):\n        cls_branch.append(Linear(self.embed_dims, self.embed_dims))\n        cls_branch.append(nn.LayerNorm(self.embed_dims))\n        cls_branch.append(nn.ReLU(inplace=True))\n    if self.normedlinear:\n        cls_branch.append(NormedLinear(self.embed_dims, self.cls_out_channels))\n    else:\n        cls_branch.append(Linear(self.embed_dims, self.cls_out_channels))\n    fc_cls = nn.Sequential(*cls_branch)\n    if self.with_multi:\n        reg_branch = RegLayer(self.embed_dims, self.num_reg_fcs, self.group_reg_dims)\n    else:\n        reg_branch = []\n        for _ in range(self.num_reg_fcs):\n            reg_branch.append(Linear(self.embed_dims, self.embed_dims))\n            reg_branch.append(nn.ReLU())\n        reg_branch.append(Linear(self.embed_dims, self.code_size))\n        reg_branch = nn.Sequential(*reg_branch)\n    self.cls_branches = nn.ModuleList([copy.deepcopy(fc_cls) for _ in range(self.num_pred)])\n    self.reg_branches = nn.ModuleList([copy.deepcopy(reg_branch) for _ in range(self.num_pred)])\n    if self.with_multiview:\n        self.adapt_pos3d = nn.Sequential(nn.Conv2d(self.embed_dims * 3 // 2, self.embed_dims * 4, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(self.embed_dims * 4, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    else:\n        self.adapt_pos3d = nn.Sequential(nn.Conv2d(self.embed_dims, self.embed_dims, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(self.embed_dims, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    if self.with_position:\n        chan_in = self.position_dim\n        chan_mid = self.embed_dims * 4\n        self.position_encoder = nn.Sequential(nn.Conv2d(chan_in, chan_mid, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(chan_mid, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    if self.depth_pe:\n        self.depth_encoder = nn.Conv2d(self.D, self.embed_dims, kernel_size=1, stride=1, padding=0)\n    self.reference_points = nn.Embedding(self.num_query, 3)\n    self.query_embedding = nn.Sequential(nn.Linear(self.embed_dims * 3 // 2, self.embed_dims), nn.ReLU(), nn.Linear(self.embed_dims, self.embed_dims))\n    if self.with_fpe:\n        self.fpe = SELayer(self.embed_dims, self.embed_dims)\n    if self.with_fde:\n        self.fde = SELayer(self.embed_dims, self.D)",
            "def _init_layers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize layers of the transformer head.'\n    if not self.with_depthnet:\n        self.input_proj = Conv2d(self.in_channels, self.embed_dims, kernel_size=1)\n    cls_branch = []\n    for _ in range(self.num_reg_fcs):\n        cls_branch.append(Linear(self.embed_dims, self.embed_dims))\n        cls_branch.append(nn.LayerNorm(self.embed_dims))\n        cls_branch.append(nn.ReLU(inplace=True))\n    if self.normedlinear:\n        cls_branch.append(NormedLinear(self.embed_dims, self.cls_out_channels))\n    else:\n        cls_branch.append(Linear(self.embed_dims, self.cls_out_channels))\n    fc_cls = nn.Sequential(*cls_branch)\n    if self.with_multi:\n        reg_branch = RegLayer(self.embed_dims, self.num_reg_fcs, self.group_reg_dims)\n    else:\n        reg_branch = []\n        for _ in range(self.num_reg_fcs):\n            reg_branch.append(Linear(self.embed_dims, self.embed_dims))\n            reg_branch.append(nn.ReLU())\n        reg_branch.append(Linear(self.embed_dims, self.code_size))\n        reg_branch = nn.Sequential(*reg_branch)\n    self.cls_branches = nn.ModuleList([copy.deepcopy(fc_cls) for _ in range(self.num_pred)])\n    self.reg_branches = nn.ModuleList([copy.deepcopy(reg_branch) for _ in range(self.num_pred)])\n    if self.with_multiview:\n        self.adapt_pos3d = nn.Sequential(nn.Conv2d(self.embed_dims * 3 // 2, self.embed_dims * 4, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(self.embed_dims * 4, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    else:\n        self.adapt_pos3d = nn.Sequential(nn.Conv2d(self.embed_dims, self.embed_dims, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(self.embed_dims, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    if self.with_position:\n        chan_in = self.position_dim\n        chan_mid = self.embed_dims * 4\n        self.position_encoder = nn.Sequential(nn.Conv2d(chan_in, chan_mid, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(chan_mid, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    if self.depth_pe:\n        self.depth_encoder = nn.Conv2d(self.D, self.embed_dims, kernel_size=1, stride=1, padding=0)\n    self.reference_points = nn.Embedding(self.num_query, 3)\n    self.query_embedding = nn.Sequential(nn.Linear(self.embed_dims * 3 // 2, self.embed_dims), nn.ReLU(), nn.Linear(self.embed_dims, self.embed_dims))\n    if self.with_fpe:\n        self.fpe = SELayer(self.embed_dims, self.embed_dims)\n    if self.with_fde:\n        self.fde = SELayer(self.embed_dims, self.D)",
            "def _init_layers(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize layers of the transformer head.'\n    if not self.with_depthnet:\n        self.input_proj = Conv2d(self.in_channels, self.embed_dims, kernel_size=1)\n    cls_branch = []\n    for _ in range(self.num_reg_fcs):\n        cls_branch.append(Linear(self.embed_dims, self.embed_dims))\n        cls_branch.append(nn.LayerNorm(self.embed_dims))\n        cls_branch.append(nn.ReLU(inplace=True))\n    if self.normedlinear:\n        cls_branch.append(NormedLinear(self.embed_dims, self.cls_out_channels))\n    else:\n        cls_branch.append(Linear(self.embed_dims, self.cls_out_channels))\n    fc_cls = nn.Sequential(*cls_branch)\n    if self.with_multi:\n        reg_branch = RegLayer(self.embed_dims, self.num_reg_fcs, self.group_reg_dims)\n    else:\n        reg_branch = []\n        for _ in range(self.num_reg_fcs):\n            reg_branch.append(Linear(self.embed_dims, self.embed_dims))\n            reg_branch.append(nn.ReLU())\n        reg_branch.append(Linear(self.embed_dims, self.code_size))\n        reg_branch = nn.Sequential(*reg_branch)\n    self.cls_branches = nn.ModuleList([copy.deepcopy(fc_cls) for _ in range(self.num_pred)])\n    self.reg_branches = nn.ModuleList([copy.deepcopy(reg_branch) for _ in range(self.num_pred)])\n    if self.with_multiview:\n        self.adapt_pos3d = nn.Sequential(nn.Conv2d(self.embed_dims * 3 // 2, self.embed_dims * 4, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(self.embed_dims * 4, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    else:\n        self.adapt_pos3d = nn.Sequential(nn.Conv2d(self.embed_dims, self.embed_dims, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(self.embed_dims, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    if self.with_position:\n        chan_in = self.position_dim\n        chan_mid = self.embed_dims * 4\n        self.position_encoder = nn.Sequential(nn.Conv2d(chan_in, chan_mid, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(chan_mid, self.embed_dims, kernel_size=1, stride=1, padding=0))\n    if self.depth_pe:\n        self.depth_encoder = nn.Conv2d(self.D, self.embed_dims, kernel_size=1, stride=1, padding=0)\n    self.reference_points = nn.Embedding(self.num_query, 3)\n    self.query_embedding = nn.Sequential(nn.Linear(self.embed_dims * 3 // 2, self.embed_dims), nn.ReLU(), nn.Linear(self.embed_dims, self.embed_dims))\n    if self.with_fpe:\n        self.fpe = SELayer(self.embed_dims, self.embed_dims)\n    if self.with_fde:\n        self.fde = SELayer(self.embed_dims, self.D)"
        ]
    },
    {
        "func_name": "init_weights",
        "original": "def init_weights(self):\n    \"\"\"Initialize weights of the transformer head.\"\"\"\n    self.transformer.init_weights()\n    nn.init.uniform_(self.reference_points.weight.data, 0, 1)\n    if self.loss_cls.use_sigmoid:\n        bias_init = bias_init_with_prob(0.01)\n        for m in self.cls_branches:\n            nn.init.constant_(m[-1].bias, bias_init)",
        "mutated": [
            "def init_weights(self):\n    if False:\n        i = 10\n    'Initialize weights of the transformer head.'\n    self.transformer.init_weights()\n    nn.init.uniform_(self.reference_points.weight.data, 0, 1)\n    if self.loss_cls.use_sigmoid:\n        bias_init = bias_init_with_prob(0.01)\n        for m in self.cls_branches:\n            nn.init.constant_(m[-1].bias, bias_init)",
            "def init_weights(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initialize weights of the transformer head.'\n    self.transformer.init_weights()\n    nn.init.uniform_(self.reference_points.weight.data, 0, 1)\n    if self.loss_cls.use_sigmoid:\n        bias_init = bias_init_with_prob(0.01)\n        for m in self.cls_branches:\n            nn.init.constant_(m[-1].bias, bias_init)",
            "def init_weights(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initialize weights of the transformer head.'\n    self.transformer.init_weights()\n    nn.init.uniform_(self.reference_points.weight.data, 0, 1)\n    if self.loss_cls.use_sigmoid:\n        bias_init = bias_init_with_prob(0.01)\n        for m in self.cls_branches:\n            nn.init.constant_(m[-1].bias, bias_init)",
            "def init_weights(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initialize weights of the transformer head.'\n    self.transformer.init_weights()\n    nn.init.uniform_(self.reference_points.weight.data, 0, 1)\n    if self.loss_cls.use_sigmoid:\n        bias_init = bias_init_with_prob(0.01)\n        for m in self.cls_branches:\n            nn.init.constant_(m[-1].bias, bias_init)",
            "def init_weights(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initialize weights of the transformer head.'\n    self.transformer.init_weights()\n    nn.init.uniform_(self.reference_points.weight.data, 0, 1)\n    if self.loss_cls.use_sigmoid:\n        bias_init = bias_init_with_prob(0.01)\n        for m in self.cls_branches:\n            nn.init.constant_(m[-1].bias, bias_init)"
        ]
    },
    {
        "func_name": "get_mlp_input",
        "original": "def get_mlp_input(self, img_metas):\n    num_view = 6\n    num_img = len(img_metas[0]['lidar2img'])\n    lidar2img = torch.tensor([img_meta['lidar2img'][:num_view] for img_meta in img_metas])\n    lidar2img = lidar2img.repeat(1, num_img // num_view, 1, 1)\n    lidar2img = lidar2img[..., :3, :]\n    (B, N, _, _) = lidar2img.shape\n    mlp_input = lidar2img.reshape(B, N, -1)\n    return mlp_input",
        "mutated": [
            "def get_mlp_input(self, img_metas):\n    if False:\n        i = 10\n    num_view = 6\n    num_img = len(img_metas[0]['lidar2img'])\n    lidar2img = torch.tensor([img_meta['lidar2img'][:num_view] for img_meta in img_metas])\n    lidar2img = lidar2img.repeat(1, num_img // num_view, 1, 1)\n    lidar2img = lidar2img[..., :3, :]\n    (B, N, _, _) = lidar2img.shape\n    mlp_input = lidar2img.reshape(B, N, -1)\n    return mlp_input",
            "def get_mlp_input(self, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    num_view = 6\n    num_img = len(img_metas[0]['lidar2img'])\n    lidar2img = torch.tensor([img_meta['lidar2img'][:num_view] for img_meta in img_metas])\n    lidar2img = lidar2img.repeat(1, num_img // num_view, 1, 1)\n    lidar2img = lidar2img[..., :3, :]\n    (B, N, _, _) = lidar2img.shape\n    mlp_input = lidar2img.reshape(B, N, -1)\n    return mlp_input",
            "def get_mlp_input(self, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    num_view = 6\n    num_img = len(img_metas[0]['lidar2img'])\n    lidar2img = torch.tensor([img_meta['lidar2img'][:num_view] for img_meta in img_metas])\n    lidar2img = lidar2img.repeat(1, num_img // num_view, 1, 1)\n    lidar2img = lidar2img[..., :3, :]\n    (B, N, _, _) = lidar2img.shape\n    mlp_input = lidar2img.reshape(B, N, -1)\n    return mlp_input",
            "def get_mlp_input(self, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    num_view = 6\n    num_img = len(img_metas[0]['lidar2img'])\n    lidar2img = torch.tensor([img_meta['lidar2img'][:num_view] for img_meta in img_metas])\n    lidar2img = lidar2img.repeat(1, num_img // num_view, 1, 1)\n    lidar2img = lidar2img[..., :3, :]\n    (B, N, _, _) = lidar2img.shape\n    mlp_input = lidar2img.reshape(B, N, -1)\n    return mlp_input",
            "def get_mlp_input(self, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    num_view = 6\n    num_img = len(img_metas[0]['lidar2img'])\n    lidar2img = torch.tensor([img_meta['lidar2img'][:num_view] for img_meta in img_metas])\n    lidar2img = lidar2img.repeat(1, num_img // num_view, 1, 1)\n    lidar2img = lidar2img[..., :3, :]\n    (B, N, _, _) = lidar2img.shape\n    mlp_input = lidar2img.reshape(B, N, -1)\n    return mlp_input"
        ]
    },
    {
        "func_name": "position_embeding",
        "original": "def position_embeding(self, img_feats, img_metas, masks=None):\n    eps = 1e-05\n    (pad_h, pad_w, _) = img_metas[0]['pad_shape'][0]\n    (B, N, C, H, W) = img_feats[self.position_level].shape\n    coords_h = torch.arange(H, device=img_feats[0].device).float() * pad_h / H\n    coords_w = torch.arange(W, device=img_feats[0].device).float() * pad_w / W\n    coords_h += pad_h / H / 2.0\n    coords_w += pad_w / W / 2.0\n    if self.LID:\n        index = torch.arange(start=0, end=self.depth_num, step=1, device=img_feats[0].device).float()\n        index_1 = index + 1\n        bin_size = (self.position_range[3] - self.depth_start) / (self.depth_num * (1 + self.depth_num))\n        coords_d = self.depth_start + bin_size * index * index_1\n    else:\n        index = torch.arange(start=0, end=self.depth_num, step=1, device=img_feats[0].device).float()\n        bin_size = (self.position_range[3] - self.depth_start) / self.depth_num\n        coords_d = self.depth_start + bin_size * index\n    self.coords_d = coords_d\n    D = coords_d.shape[0]\n    coords = torch.stack(torch.meshgrid([coords_w, coords_h, coords_d])).permute(1, 2, 3, 0)\n    coords = torch.cat((coords, torch.ones_like(coords[..., :1])), -1)\n    coords[..., :2] = coords[..., :2] * torch.maximum(coords[..., 2:3], torch.ones_like(coords[..., 2:3]) * eps)\n    img2lidars = []\n    for img_meta in img_metas:\n        img2lidar = []\n        for i in range(len(img_meta['lidar2img'])):\n            img2lidar.append(np.linalg.inv(img_meta['lidar2img'][i]))\n        img2lidars.append(np.asarray(img2lidar))\n    img2lidars = np.asarray(img2lidars)\n    img2lidars = coords.new_tensor(img2lidars)\n    coords = coords.view(1, 1, W, H, D, 4, 1).repeat(B, N, 1, 1, 1, 1, 1)\n    img2lidars = img2lidars.view(B, N, 1, 1, 1, 4, 4).repeat(1, 1, W, H, D, 1, 1)\n    coords3d = torch.matmul(img2lidars, coords).squeeze(-1)[..., :3]\n    coords3d[..., 0:1] = (coords3d[..., 0:1] - self.position_range[0]) / (self.position_range[3] - self.position_range[0])\n    coords3d[..., 1:2] = (coords3d[..., 1:2] - self.position_range[1]) / (self.position_range[4] - self.position_range[1])\n    coords3d[..., 2:3] = (coords3d[..., 2:3] - self.position_range[2]) / (self.position_range[5] - self.position_range[2])\n    coords_mask = (coords3d > 1.0) | (coords3d < 0.0)\n    coords_mask = coords_mask.flatten(-2).sum(-1) > D * 0.5\n    coords_mask = masks | coords_mask.permute(0, 1, 3, 2)\n    coords3d = coords3d.permute(0, 1, 4, 5, 3, 2).contiguous().view(B * N, -1, H, W)\n    coords3d = inverse_sigmoid(coords3d)\n    coords_position_embeding = self.position_encoder(coords3d)\n    return (coords_position_embeding.view(B, N, self.embed_dims, H, W), coords_mask)",
        "mutated": [
            "def position_embeding(self, img_feats, img_metas, masks=None):\n    if False:\n        i = 10\n    eps = 1e-05\n    (pad_h, pad_w, _) = img_metas[0]['pad_shape'][0]\n    (B, N, C, H, W) = img_feats[self.position_level].shape\n    coords_h = torch.arange(H, device=img_feats[0].device).float() * pad_h / H\n    coords_w = torch.arange(W, device=img_feats[0].device).float() * pad_w / W\n    coords_h += pad_h / H / 2.0\n    coords_w += pad_w / W / 2.0\n    if self.LID:\n        index = torch.arange(start=0, end=self.depth_num, step=1, device=img_feats[0].device).float()\n        index_1 = index + 1\n        bin_size = (self.position_range[3] - self.depth_start) / (self.depth_num * (1 + self.depth_num))\n        coords_d = self.depth_start + bin_size * index * index_1\n    else:\n        index = torch.arange(start=0, end=self.depth_num, step=1, device=img_feats[0].device).float()\n        bin_size = (self.position_range[3] - self.depth_start) / self.depth_num\n        coords_d = self.depth_start + bin_size * index\n    self.coords_d = coords_d\n    D = coords_d.shape[0]\n    coords = torch.stack(torch.meshgrid([coords_w, coords_h, coords_d])).permute(1, 2, 3, 0)\n    coords = torch.cat((coords, torch.ones_like(coords[..., :1])), -1)\n    coords[..., :2] = coords[..., :2] * torch.maximum(coords[..., 2:3], torch.ones_like(coords[..., 2:3]) * eps)\n    img2lidars = []\n    for img_meta in img_metas:\n        img2lidar = []\n        for i in range(len(img_meta['lidar2img'])):\n            img2lidar.append(np.linalg.inv(img_meta['lidar2img'][i]))\n        img2lidars.append(np.asarray(img2lidar))\n    img2lidars = np.asarray(img2lidars)\n    img2lidars = coords.new_tensor(img2lidars)\n    coords = coords.view(1, 1, W, H, D, 4, 1).repeat(B, N, 1, 1, 1, 1, 1)\n    img2lidars = img2lidars.view(B, N, 1, 1, 1, 4, 4).repeat(1, 1, W, H, D, 1, 1)\n    coords3d = torch.matmul(img2lidars, coords).squeeze(-1)[..., :3]\n    coords3d[..., 0:1] = (coords3d[..., 0:1] - self.position_range[0]) / (self.position_range[3] - self.position_range[0])\n    coords3d[..., 1:2] = (coords3d[..., 1:2] - self.position_range[1]) / (self.position_range[4] - self.position_range[1])\n    coords3d[..., 2:3] = (coords3d[..., 2:3] - self.position_range[2]) / (self.position_range[5] - self.position_range[2])\n    coords_mask = (coords3d > 1.0) | (coords3d < 0.0)\n    coords_mask = coords_mask.flatten(-2).sum(-1) > D * 0.5\n    coords_mask = masks | coords_mask.permute(0, 1, 3, 2)\n    coords3d = coords3d.permute(0, 1, 4, 5, 3, 2).contiguous().view(B * N, -1, H, W)\n    coords3d = inverse_sigmoid(coords3d)\n    coords_position_embeding = self.position_encoder(coords3d)\n    return (coords_position_embeding.view(B, N, self.embed_dims, H, W), coords_mask)",
            "def position_embeding(self, img_feats, img_metas, masks=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    eps = 1e-05\n    (pad_h, pad_w, _) = img_metas[0]['pad_shape'][0]\n    (B, N, C, H, W) = img_feats[self.position_level].shape\n    coords_h = torch.arange(H, device=img_feats[0].device).float() * pad_h / H\n    coords_w = torch.arange(W, device=img_feats[0].device).float() * pad_w / W\n    coords_h += pad_h / H / 2.0\n    coords_w += pad_w / W / 2.0\n    if self.LID:\n        index = torch.arange(start=0, end=self.depth_num, step=1, device=img_feats[0].device).float()\n        index_1 = index + 1\n        bin_size = (self.position_range[3] - self.depth_start) / (self.depth_num * (1 + self.depth_num))\n        coords_d = self.depth_start + bin_size * index * index_1\n    else:\n        index = torch.arange(start=0, end=self.depth_num, step=1, device=img_feats[0].device).float()\n        bin_size = (self.position_range[3] - self.depth_start) / self.depth_num\n        coords_d = self.depth_start + bin_size * index\n    self.coords_d = coords_d\n    D = coords_d.shape[0]\n    coords = torch.stack(torch.meshgrid([coords_w, coords_h, coords_d])).permute(1, 2, 3, 0)\n    coords = torch.cat((coords, torch.ones_like(coords[..., :1])), -1)\n    coords[..., :2] = coords[..., :2] * torch.maximum(coords[..., 2:3], torch.ones_like(coords[..., 2:3]) * eps)\n    img2lidars = []\n    for img_meta in img_metas:\n        img2lidar = []\n        for i in range(len(img_meta['lidar2img'])):\n            img2lidar.append(np.linalg.inv(img_meta['lidar2img'][i]))\n        img2lidars.append(np.asarray(img2lidar))\n    img2lidars = np.asarray(img2lidars)\n    img2lidars = coords.new_tensor(img2lidars)\n    coords = coords.view(1, 1, W, H, D, 4, 1).repeat(B, N, 1, 1, 1, 1, 1)\n    img2lidars = img2lidars.view(B, N, 1, 1, 1, 4, 4).repeat(1, 1, W, H, D, 1, 1)\n    coords3d = torch.matmul(img2lidars, coords).squeeze(-1)[..., :3]\n    coords3d[..., 0:1] = (coords3d[..., 0:1] - self.position_range[0]) / (self.position_range[3] - self.position_range[0])\n    coords3d[..., 1:2] = (coords3d[..., 1:2] - self.position_range[1]) / (self.position_range[4] - self.position_range[1])\n    coords3d[..., 2:3] = (coords3d[..., 2:3] - self.position_range[2]) / (self.position_range[5] - self.position_range[2])\n    coords_mask = (coords3d > 1.0) | (coords3d < 0.0)\n    coords_mask = coords_mask.flatten(-2).sum(-1) > D * 0.5\n    coords_mask = masks | coords_mask.permute(0, 1, 3, 2)\n    coords3d = coords3d.permute(0, 1, 4, 5, 3, 2).contiguous().view(B * N, -1, H, W)\n    coords3d = inverse_sigmoid(coords3d)\n    coords_position_embeding = self.position_encoder(coords3d)\n    return (coords_position_embeding.view(B, N, self.embed_dims, H, W), coords_mask)",
            "def position_embeding(self, img_feats, img_metas, masks=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    eps = 1e-05\n    (pad_h, pad_w, _) = img_metas[0]['pad_shape'][0]\n    (B, N, C, H, W) = img_feats[self.position_level].shape\n    coords_h = torch.arange(H, device=img_feats[0].device).float() * pad_h / H\n    coords_w = torch.arange(W, device=img_feats[0].device).float() * pad_w / W\n    coords_h += pad_h / H / 2.0\n    coords_w += pad_w / W / 2.0\n    if self.LID:\n        index = torch.arange(start=0, end=self.depth_num, step=1, device=img_feats[0].device).float()\n        index_1 = index + 1\n        bin_size = (self.position_range[3] - self.depth_start) / (self.depth_num * (1 + self.depth_num))\n        coords_d = self.depth_start + bin_size * index * index_1\n    else:\n        index = torch.arange(start=0, end=self.depth_num, step=1, device=img_feats[0].device).float()\n        bin_size = (self.position_range[3] - self.depth_start) / self.depth_num\n        coords_d = self.depth_start + bin_size * index\n    self.coords_d = coords_d\n    D = coords_d.shape[0]\n    coords = torch.stack(torch.meshgrid([coords_w, coords_h, coords_d])).permute(1, 2, 3, 0)\n    coords = torch.cat((coords, torch.ones_like(coords[..., :1])), -1)\n    coords[..., :2] = coords[..., :2] * torch.maximum(coords[..., 2:3], torch.ones_like(coords[..., 2:3]) * eps)\n    img2lidars = []\n    for img_meta in img_metas:\n        img2lidar = []\n        for i in range(len(img_meta['lidar2img'])):\n            img2lidar.append(np.linalg.inv(img_meta['lidar2img'][i]))\n        img2lidars.append(np.asarray(img2lidar))\n    img2lidars = np.asarray(img2lidars)\n    img2lidars = coords.new_tensor(img2lidars)\n    coords = coords.view(1, 1, W, H, D, 4, 1).repeat(B, N, 1, 1, 1, 1, 1)\n    img2lidars = img2lidars.view(B, N, 1, 1, 1, 4, 4).repeat(1, 1, W, H, D, 1, 1)\n    coords3d = torch.matmul(img2lidars, coords).squeeze(-1)[..., :3]\n    coords3d[..., 0:1] = (coords3d[..., 0:1] - self.position_range[0]) / (self.position_range[3] - self.position_range[0])\n    coords3d[..., 1:2] = (coords3d[..., 1:2] - self.position_range[1]) / (self.position_range[4] - self.position_range[1])\n    coords3d[..., 2:3] = (coords3d[..., 2:3] - self.position_range[2]) / (self.position_range[5] - self.position_range[2])\n    coords_mask = (coords3d > 1.0) | (coords3d < 0.0)\n    coords_mask = coords_mask.flatten(-2).sum(-1) > D * 0.5\n    coords_mask = masks | coords_mask.permute(0, 1, 3, 2)\n    coords3d = coords3d.permute(0, 1, 4, 5, 3, 2).contiguous().view(B * N, -1, H, W)\n    coords3d = inverse_sigmoid(coords3d)\n    coords_position_embeding = self.position_encoder(coords3d)\n    return (coords_position_embeding.view(B, N, self.embed_dims, H, W), coords_mask)",
            "def position_embeding(self, img_feats, img_metas, masks=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    eps = 1e-05\n    (pad_h, pad_w, _) = img_metas[0]['pad_shape'][0]\n    (B, N, C, H, W) = img_feats[self.position_level].shape\n    coords_h = torch.arange(H, device=img_feats[0].device).float() * pad_h / H\n    coords_w = torch.arange(W, device=img_feats[0].device).float() * pad_w / W\n    coords_h += pad_h / H / 2.0\n    coords_w += pad_w / W / 2.0\n    if self.LID:\n        index = torch.arange(start=0, end=self.depth_num, step=1, device=img_feats[0].device).float()\n        index_1 = index + 1\n        bin_size = (self.position_range[3] - self.depth_start) / (self.depth_num * (1 + self.depth_num))\n        coords_d = self.depth_start + bin_size * index * index_1\n    else:\n        index = torch.arange(start=0, end=self.depth_num, step=1, device=img_feats[0].device).float()\n        bin_size = (self.position_range[3] - self.depth_start) / self.depth_num\n        coords_d = self.depth_start + bin_size * index\n    self.coords_d = coords_d\n    D = coords_d.shape[0]\n    coords = torch.stack(torch.meshgrid([coords_w, coords_h, coords_d])).permute(1, 2, 3, 0)\n    coords = torch.cat((coords, torch.ones_like(coords[..., :1])), -1)\n    coords[..., :2] = coords[..., :2] * torch.maximum(coords[..., 2:3], torch.ones_like(coords[..., 2:3]) * eps)\n    img2lidars = []\n    for img_meta in img_metas:\n        img2lidar = []\n        for i in range(len(img_meta['lidar2img'])):\n            img2lidar.append(np.linalg.inv(img_meta['lidar2img'][i]))\n        img2lidars.append(np.asarray(img2lidar))\n    img2lidars = np.asarray(img2lidars)\n    img2lidars = coords.new_tensor(img2lidars)\n    coords = coords.view(1, 1, W, H, D, 4, 1).repeat(B, N, 1, 1, 1, 1, 1)\n    img2lidars = img2lidars.view(B, N, 1, 1, 1, 4, 4).repeat(1, 1, W, H, D, 1, 1)\n    coords3d = torch.matmul(img2lidars, coords).squeeze(-1)[..., :3]\n    coords3d[..., 0:1] = (coords3d[..., 0:1] - self.position_range[0]) / (self.position_range[3] - self.position_range[0])\n    coords3d[..., 1:2] = (coords3d[..., 1:2] - self.position_range[1]) / (self.position_range[4] - self.position_range[1])\n    coords3d[..., 2:3] = (coords3d[..., 2:3] - self.position_range[2]) / (self.position_range[5] - self.position_range[2])\n    coords_mask = (coords3d > 1.0) | (coords3d < 0.0)\n    coords_mask = coords_mask.flatten(-2).sum(-1) > D * 0.5\n    coords_mask = masks | coords_mask.permute(0, 1, 3, 2)\n    coords3d = coords3d.permute(0, 1, 4, 5, 3, 2).contiguous().view(B * N, -1, H, W)\n    coords3d = inverse_sigmoid(coords3d)\n    coords_position_embeding = self.position_encoder(coords3d)\n    return (coords_position_embeding.view(B, N, self.embed_dims, H, W), coords_mask)",
            "def position_embeding(self, img_feats, img_metas, masks=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    eps = 1e-05\n    (pad_h, pad_w, _) = img_metas[0]['pad_shape'][0]\n    (B, N, C, H, W) = img_feats[self.position_level].shape\n    coords_h = torch.arange(H, device=img_feats[0].device).float() * pad_h / H\n    coords_w = torch.arange(W, device=img_feats[0].device).float() * pad_w / W\n    coords_h += pad_h / H / 2.0\n    coords_w += pad_w / W / 2.0\n    if self.LID:\n        index = torch.arange(start=0, end=self.depth_num, step=1, device=img_feats[0].device).float()\n        index_1 = index + 1\n        bin_size = (self.position_range[3] - self.depth_start) / (self.depth_num * (1 + self.depth_num))\n        coords_d = self.depth_start + bin_size * index * index_1\n    else:\n        index = torch.arange(start=0, end=self.depth_num, step=1, device=img_feats[0].device).float()\n        bin_size = (self.position_range[3] - self.depth_start) / self.depth_num\n        coords_d = self.depth_start + bin_size * index\n    self.coords_d = coords_d\n    D = coords_d.shape[0]\n    coords = torch.stack(torch.meshgrid([coords_w, coords_h, coords_d])).permute(1, 2, 3, 0)\n    coords = torch.cat((coords, torch.ones_like(coords[..., :1])), -1)\n    coords[..., :2] = coords[..., :2] * torch.maximum(coords[..., 2:3], torch.ones_like(coords[..., 2:3]) * eps)\n    img2lidars = []\n    for img_meta in img_metas:\n        img2lidar = []\n        for i in range(len(img_meta['lidar2img'])):\n            img2lidar.append(np.linalg.inv(img_meta['lidar2img'][i]))\n        img2lidars.append(np.asarray(img2lidar))\n    img2lidars = np.asarray(img2lidars)\n    img2lidars = coords.new_tensor(img2lidars)\n    coords = coords.view(1, 1, W, H, D, 4, 1).repeat(B, N, 1, 1, 1, 1, 1)\n    img2lidars = img2lidars.view(B, N, 1, 1, 1, 4, 4).repeat(1, 1, W, H, D, 1, 1)\n    coords3d = torch.matmul(img2lidars, coords).squeeze(-1)[..., :3]\n    coords3d[..., 0:1] = (coords3d[..., 0:1] - self.position_range[0]) / (self.position_range[3] - self.position_range[0])\n    coords3d[..., 1:2] = (coords3d[..., 1:2] - self.position_range[1]) / (self.position_range[4] - self.position_range[1])\n    coords3d[..., 2:3] = (coords3d[..., 2:3] - self.position_range[2]) / (self.position_range[5] - self.position_range[2])\n    coords_mask = (coords3d > 1.0) | (coords3d < 0.0)\n    coords_mask = coords_mask.flatten(-2).sum(-1) > D * 0.5\n    coords_mask = masks | coords_mask.permute(0, 1, 3, 2)\n    coords3d = coords3d.permute(0, 1, 4, 5, 3, 2).contiguous().view(B * N, -1, H, W)\n    coords3d = inverse_sigmoid(coords3d)\n    coords_position_embeding = self.position_encoder(coords3d)\n    return (coords_position_embeding.view(B, N, self.embed_dims, H, W), coords_mask)"
        ]
    },
    {
        "func_name": "prepare_for_dn",
        "original": "def prepare_for_dn(self, batch_size, reference_points, img_metas):\n    if self.training:\n        targets = [torch.cat((img_meta['gt_bboxes_3d']._data.gravity_center, img_meta['gt_bboxes_3d']._data.tensor[:, 3:]), dim=1) for img_meta in img_metas]\n        labels = [img_meta['gt_labels_3d']._data for img_meta in img_metas]\n        known = [torch.ones_like(t).cuda() for t in labels]\n        know_idx = known\n        unmask_bbox = unmask_label = torch.cat(known)\n        known_num = [t.size(0) for t in targets]\n        labels = torch.cat([t for t in labels])\n        boxes = torch.cat([t for t in targets])\n        batch_idx = torch.cat([torch.full((t.size(0),), i) for (i, t) in enumerate(targets)])\n        known_indice = torch.nonzero(unmask_label + unmask_bbox)\n        known_indice = known_indice.view(-1)\n        known_indice = known_indice.repeat(self.scalar, 1).view(-1)\n        known_labels = labels.repeat(self.scalar, 1).view(-1).long().to(reference_points.device)\n        known_bid = batch_idx.repeat(self.scalar, 1).view(-1)\n        known_bboxs = boxes.repeat(self.scalar, 1).to(reference_points.device)\n        known_bbox_center = known_bboxs[:, :3].clone()\n        known_bbox_scale = known_bboxs[:, 3:6].clone()\n        if self.bbox_noise_scale > 0:\n            diff = known_bbox_scale / 2 + self.bbox_noise_trans\n            rand_prob = torch.rand_like(known_bbox_center) * 2 - 1.0\n            known_bbox_center += torch.mul(rand_prob, diff) * self.bbox_noise_scale\n            known_bbox_center[..., 0:1] = (known_bbox_center[..., 0:1] - self.pc_range[0]) / (self.pc_range[3] - self.pc_range[0])\n            known_bbox_center[..., 1:2] = (known_bbox_center[..., 1:2] - self.pc_range[1]) / (self.pc_range[4] - self.pc_range[1])\n            known_bbox_center[..., 2:3] = (known_bbox_center[..., 2:3] - self.pc_range[2]) / (self.pc_range[5] - self.pc_range[2])\n            known_bbox_center = known_bbox_center.clamp(min=0.0, max=1.0)\n            mask = torch.norm(rand_prob, 2, 1) > self.split\n            known_labels[mask] = self.num_classes\n        single_pad = int(max(known_num))\n        pad_size = int(single_pad * self.scalar)\n        padding_bbox = torch.zeros(pad_size, 3).to(reference_points.device)\n        padded_reference_points = torch.cat([padding_bbox, reference_points], dim=0).unsqueeze(0).repeat(batch_size, 1, 1)\n        if len(known_num):\n            map_known_indice = torch.cat([torch.tensor(range(num)) for num in known_num])\n            map_known_indice = torch.cat([map_known_indice + single_pad * i for i in range(self.scalar)]).long()\n        if len(known_bid):\n            padded_reference_points[known_bid.long(), map_known_indice] = known_bbox_center.to(reference_points.device)\n        tgt_size = pad_size + self.num_query\n        attn_mask = torch.ones(tgt_size, tgt_size).to(reference_points.device) < 0\n        attn_mask[pad_size:, :pad_size] = True\n        for i in range(self.scalar):\n            if i == 0:\n                attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n            if i == self.scalar - 1:\n                attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n            else:\n                attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n                attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n        mask_dict = {'known_indice': torch.as_tensor(known_indice).long(), 'batch_idx': torch.as_tensor(batch_idx).long(), 'map_known_indice': torch.as_tensor(map_known_indice).long(), 'known_lbs_bboxes': (known_labels, known_bboxs), 'know_idx': know_idx, 'pad_size': pad_size}\n    else:\n        padded_reference_points = reference_points.unsqueeze(0).repeat(batch_size, 1, 1)\n        attn_mask = None\n        mask_dict = None\n    return (padded_reference_points, attn_mask, mask_dict)",
        "mutated": [
            "def prepare_for_dn(self, batch_size, reference_points, img_metas):\n    if False:\n        i = 10\n    if self.training:\n        targets = [torch.cat((img_meta['gt_bboxes_3d']._data.gravity_center, img_meta['gt_bboxes_3d']._data.tensor[:, 3:]), dim=1) for img_meta in img_metas]\n        labels = [img_meta['gt_labels_3d']._data for img_meta in img_metas]\n        known = [torch.ones_like(t).cuda() for t in labels]\n        know_idx = known\n        unmask_bbox = unmask_label = torch.cat(known)\n        known_num = [t.size(0) for t in targets]\n        labels = torch.cat([t for t in labels])\n        boxes = torch.cat([t for t in targets])\n        batch_idx = torch.cat([torch.full((t.size(0),), i) for (i, t) in enumerate(targets)])\n        known_indice = torch.nonzero(unmask_label + unmask_bbox)\n        known_indice = known_indice.view(-1)\n        known_indice = known_indice.repeat(self.scalar, 1).view(-1)\n        known_labels = labels.repeat(self.scalar, 1).view(-1).long().to(reference_points.device)\n        known_bid = batch_idx.repeat(self.scalar, 1).view(-1)\n        known_bboxs = boxes.repeat(self.scalar, 1).to(reference_points.device)\n        known_bbox_center = known_bboxs[:, :3].clone()\n        known_bbox_scale = known_bboxs[:, 3:6].clone()\n        if self.bbox_noise_scale > 0:\n            diff = known_bbox_scale / 2 + self.bbox_noise_trans\n            rand_prob = torch.rand_like(known_bbox_center) * 2 - 1.0\n            known_bbox_center += torch.mul(rand_prob, diff) * self.bbox_noise_scale\n            known_bbox_center[..., 0:1] = (known_bbox_center[..., 0:1] - self.pc_range[0]) / (self.pc_range[3] - self.pc_range[0])\n            known_bbox_center[..., 1:2] = (known_bbox_center[..., 1:2] - self.pc_range[1]) / (self.pc_range[4] - self.pc_range[1])\n            known_bbox_center[..., 2:3] = (known_bbox_center[..., 2:3] - self.pc_range[2]) / (self.pc_range[5] - self.pc_range[2])\n            known_bbox_center = known_bbox_center.clamp(min=0.0, max=1.0)\n            mask = torch.norm(rand_prob, 2, 1) > self.split\n            known_labels[mask] = self.num_classes\n        single_pad = int(max(known_num))\n        pad_size = int(single_pad * self.scalar)\n        padding_bbox = torch.zeros(pad_size, 3).to(reference_points.device)\n        padded_reference_points = torch.cat([padding_bbox, reference_points], dim=0).unsqueeze(0).repeat(batch_size, 1, 1)\n        if len(known_num):\n            map_known_indice = torch.cat([torch.tensor(range(num)) for num in known_num])\n            map_known_indice = torch.cat([map_known_indice + single_pad * i for i in range(self.scalar)]).long()\n        if len(known_bid):\n            padded_reference_points[known_bid.long(), map_known_indice] = known_bbox_center.to(reference_points.device)\n        tgt_size = pad_size + self.num_query\n        attn_mask = torch.ones(tgt_size, tgt_size).to(reference_points.device) < 0\n        attn_mask[pad_size:, :pad_size] = True\n        for i in range(self.scalar):\n            if i == 0:\n                attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n            if i == self.scalar - 1:\n                attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n            else:\n                attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n                attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n        mask_dict = {'known_indice': torch.as_tensor(known_indice).long(), 'batch_idx': torch.as_tensor(batch_idx).long(), 'map_known_indice': torch.as_tensor(map_known_indice).long(), 'known_lbs_bboxes': (known_labels, known_bboxs), 'know_idx': know_idx, 'pad_size': pad_size}\n    else:\n        padded_reference_points = reference_points.unsqueeze(0).repeat(batch_size, 1, 1)\n        attn_mask = None\n        mask_dict = None\n    return (padded_reference_points, attn_mask, mask_dict)",
            "def prepare_for_dn(self, batch_size, reference_points, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if self.training:\n        targets = [torch.cat((img_meta['gt_bboxes_3d']._data.gravity_center, img_meta['gt_bboxes_3d']._data.tensor[:, 3:]), dim=1) for img_meta in img_metas]\n        labels = [img_meta['gt_labels_3d']._data for img_meta in img_metas]\n        known = [torch.ones_like(t).cuda() for t in labels]\n        know_idx = known\n        unmask_bbox = unmask_label = torch.cat(known)\n        known_num = [t.size(0) for t in targets]\n        labels = torch.cat([t for t in labels])\n        boxes = torch.cat([t for t in targets])\n        batch_idx = torch.cat([torch.full((t.size(0),), i) for (i, t) in enumerate(targets)])\n        known_indice = torch.nonzero(unmask_label + unmask_bbox)\n        known_indice = known_indice.view(-1)\n        known_indice = known_indice.repeat(self.scalar, 1).view(-1)\n        known_labels = labels.repeat(self.scalar, 1).view(-1).long().to(reference_points.device)\n        known_bid = batch_idx.repeat(self.scalar, 1).view(-1)\n        known_bboxs = boxes.repeat(self.scalar, 1).to(reference_points.device)\n        known_bbox_center = known_bboxs[:, :3].clone()\n        known_bbox_scale = known_bboxs[:, 3:6].clone()\n        if self.bbox_noise_scale > 0:\n            diff = known_bbox_scale / 2 + self.bbox_noise_trans\n            rand_prob = torch.rand_like(known_bbox_center) * 2 - 1.0\n            known_bbox_center += torch.mul(rand_prob, diff) * self.bbox_noise_scale\n            known_bbox_center[..., 0:1] = (known_bbox_center[..., 0:1] - self.pc_range[0]) / (self.pc_range[3] - self.pc_range[0])\n            known_bbox_center[..., 1:2] = (known_bbox_center[..., 1:2] - self.pc_range[1]) / (self.pc_range[4] - self.pc_range[1])\n            known_bbox_center[..., 2:3] = (known_bbox_center[..., 2:3] - self.pc_range[2]) / (self.pc_range[5] - self.pc_range[2])\n            known_bbox_center = known_bbox_center.clamp(min=0.0, max=1.0)\n            mask = torch.norm(rand_prob, 2, 1) > self.split\n            known_labels[mask] = self.num_classes\n        single_pad = int(max(known_num))\n        pad_size = int(single_pad * self.scalar)\n        padding_bbox = torch.zeros(pad_size, 3).to(reference_points.device)\n        padded_reference_points = torch.cat([padding_bbox, reference_points], dim=0).unsqueeze(0).repeat(batch_size, 1, 1)\n        if len(known_num):\n            map_known_indice = torch.cat([torch.tensor(range(num)) for num in known_num])\n            map_known_indice = torch.cat([map_known_indice + single_pad * i for i in range(self.scalar)]).long()\n        if len(known_bid):\n            padded_reference_points[known_bid.long(), map_known_indice] = known_bbox_center.to(reference_points.device)\n        tgt_size = pad_size + self.num_query\n        attn_mask = torch.ones(tgt_size, tgt_size).to(reference_points.device) < 0\n        attn_mask[pad_size:, :pad_size] = True\n        for i in range(self.scalar):\n            if i == 0:\n                attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n            if i == self.scalar - 1:\n                attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n            else:\n                attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n                attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n        mask_dict = {'known_indice': torch.as_tensor(known_indice).long(), 'batch_idx': torch.as_tensor(batch_idx).long(), 'map_known_indice': torch.as_tensor(map_known_indice).long(), 'known_lbs_bboxes': (known_labels, known_bboxs), 'know_idx': know_idx, 'pad_size': pad_size}\n    else:\n        padded_reference_points = reference_points.unsqueeze(0).repeat(batch_size, 1, 1)\n        attn_mask = None\n        mask_dict = None\n    return (padded_reference_points, attn_mask, mask_dict)",
            "def prepare_for_dn(self, batch_size, reference_points, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if self.training:\n        targets = [torch.cat((img_meta['gt_bboxes_3d']._data.gravity_center, img_meta['gt_bboxes_3d']._data.tensor[:, 3:]), dim=1) for img_meta in img_metas]\n        labels = [img_meta['gt_labels_3d']._data for img_meta in img_metas]\n        known = [torch.ones_like(t).cuda() for t in labels]\n        know_idx = known\n        unmask_bbox = unmask_label = torch.cat(known)\n        known_num = [t.size(0) for t in targets]\n        labels = torch.cat([t for t in labels])\n        boxes = torch.cat([t for t in targets])\n        batch_idx = torch.cat([torch.full((t.size(0),), i) for (i, t) in enumerate(targets)])\n        known_indice = torch.nonzero(unmask_label + unmask_bbox)\n        known_indice = known_indice.view(-1)\n        known_indice = known_indice.repeat(self.scalar, 1).view(-1)\n        known_labels = labels.repeat(self.scalar, 1).view(-1).long().to(reference_points.device)\n        known_bid = batch_idx.repeat(self.scalar, 1).view(-1)\n        known_bboxs = boxes.repeat(self.scalar, 1).to(reference_points.device)\n        known_bbox_center = known_bboxs[:, :3].clone()\n        known_bbox_scale = known_bboxs[:, 3:6].clone()\n        if self.bbox_noise_scale > 0:\n            diff = known_bbox_scale / 2 + self.bbox_noise_trans\n            rand_prob = torch.rand_like(known_bbox_center) * 2 - 1.0\n            known_bbox_center += torch.mul(rand_prob, diff) * self.bbox_noise_scale\n            known_bbox_center[..., 0:1] = (known_bbox_center[..., 0:1] - self.pc_range[0]) / (self.pc_range[3] - self.pc_range[0])\n            known_bbox_center[..., 1:2] = (known_bbox_center[..., 1:2] - self.pc_range[1]) / (self.pc_range[4] - self.pc_range[1])\n            known_bbox_center[..., 2:3] = (known_bbox_center[..., 2:3] - self.pc_range[2]) / (self.pc_range[5] - self.pc_range[2])\n            known_bbox_center = known_bbox_center.clamp(min=0.0, max=1.0)\n            mask = torch.norm(rand_prob, 2, 1) > self.split\n            known_labels[mask] = self.num_classes\n        single_pad = int(max(known_num))\n        pad_size = int(single_pad * self.scalar)\n        padding_bbox = torch.zeros(pad_size, 3).to(reference_points.device)\n        padded_reference_points = torch.cat([padding_bbox, reference_points], dim=0).unsqueeze(0).repeat(batch_size, 1, 1)\n        if len(known_num):\n            map_known_indice = torch.cat([torch.tensor(range(num)) for num in known_num])\n            map_known_indice = torch.cat([map_known_indice + single_pad * i for i in range(self.scalar)]).long()\n        if len(known_bid):\n            padded_reference_points[known_bid.long(), map_known_indice] = known_bbox_center.to(reference_points.device)\n        tgt_size = pad_size + self.num_query\n        attn_mask = torch.ones(tgt_size, tgt_size).to(reference_points.device) < 0\n        attn_mask[pad_size:, :pad_size] = True\n        for i in range(self.scalar):\n            if i == 0:\n                attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n            if i == self.scalar - 1:\n                attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n            else:\n                attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n                attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n        mask_dict = {'known_indice': torch.as_tensor(known_indice).long(), 'batch_idx': torch.as_tensor(batch_idx).long(), 'map_known_indice': torch.as_tensor(map_known_indice).long(), 'known_lbs_bboxes': (known_labels, known_bboxs), 'know_idx': know_idx, 'pad_size': pad_size}\n    else:\n        padded_reference_points = reference_points.unsqueeze(0).repeat(batch_size, 1, 1)\n        attn_mask = None\n        mask_dict = None\n    return (padded_reference_points, attn_mask, mask_dict)",
            "def prepare_for_dn(self, batch_size, reference_points, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if self.training:\n        targets = [torch.cat((img_meta['gt_bboxes_3d']._data.gravity_center, img_meta['gt_bboxes_3d']._data.tensor[:, 3:]), dim=1) for img_meta in img_metas]\n        labels = [img_meta['gt_labels_3d']._data for img_meta in img_metas]\n        known = [torch.ones_like(t).cuda() for t in labels]\n        know_idx = known\n        unmask_bbox = unmask_label = torch.cat(known)\n        known_num = [t.size(0) for t in targets]\n        labels = torch.cat([t for t in labels])\n        boxes = torch.cat([t for t in targets])\n        batch_idx = torch.cat([torch.full((t.size(0),), i) for (i, t) in enumerate(targets)])\n        known_indice = torch.nonzero(unmask_label + unmask_bbox)\n        known_indice = known_indice.view(-1)\n        known_indice = known_indice.repeat(self.scalar, 1).view(-1)\n        known_labels = labels.repeat(self.scalar, 1).view(-1).long().to(reference_points.device)\n        known_bid = batch_idx.repeat(self.scalar, 1).view(-1)\n        known_bboxs = boxes.repeat(self.scalar, 1).to(reference_points.device)\n        known_bbox_center = known_bboxs[:, :3].clone()\n        known_bbox_scale = known_bboxs[:, 3:6].clone()\n        if self.bbox_noise_scale > 0:\n            diff = known_bbox_scale / 2 + self.bbox_noise_trans\n            rand_prob = torch.rand_like(known_bbox_center) * 2 - 1.0\n            known_bbox_center += torch.mul(rand_prob, diff) * self.bbox_noise_scale\n            known_bbox_center[..., 0:1] = (known_bbox_center[..., 0:1] - self.pc_range[0]) / (self.pc_range[3] - self.pc_range[0])\n            known_bbox_center[..., 1:2] = (known_bbox_center[..., 1:2] - self.pc_range[1]) / (self.pc_range[4] - self.pc_range[1])\n            known_bbox_center[..., 2:3] = (known_bbox_center[..., 2:3] - self.pc_range[2]) / (self.pc_range[5] - self.pc_range[2])\n            known_bbox_center = known_bbox_center.clamp(min=0.0, max=1.0)\n            mask = torch.norm(rand_prob, 2, 1) > self.split\n            known_labels[mask] = self.num_classes\n        single_pad = int(max(known_num))\n        pad_size = int(single_pad * self.scalar)\n        padding_bbox = torch.zeros(pad_size, 3).to(reference_points.device)\n        padded_reference_points = torch.cat([padding_bbox, reference_points], dim=0).unsqueeze(0).repeat(batch_size, 1, 1)\n        if len(known_num):\n            map_known_indice = torch.cat([torch.tensor(range(num)) for num in known_num])\n            map_known_indice = torch.cat([map_known_indice + single_pad * i for i in range(self.scalar)]).long()\n        if len(known_bid):\n            padded_reference_points[known_bid.long(), map_known_indice] = known_bbox_center.to(reference_points.device)\n        tgt_size = pad_size + self.num_query\n        attn_mask = torch.ones(tgt_size, tgt_size).to(reference_points.device) < 0\n        attn_mask[pad_size:, :pad_size] = True\n        for i in range(self.scalar):\n            if i == 0:\n                attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n            if i == self.scalar - 1:\n                attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n            else:\n                attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n                attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n        mask_dict = {'known_indice': torch.as_tensor(known_indice).long(), 'batch_idx': torch.as_tensor(batch_idx).long(), 'map_known_indice': torch.as_tensor(map_known_indice).long(), 'known_lbs_bboxes': (known_labels, known_bboxs), 'know_idx': know_idx, 'pad_size': pad_size}\n    else:\n        padded_reference_points = reference_points.unsqueeze(0).repeat(batch_size, 1, 1)\n        attn_mask = None\n        mask_dict = None\n    return (padded_reference_points, attn_mask, mask_dict)",
            "def prepare_for_dn(self, batch_size, reference_points, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if self.training:\n        targets = [torch.cat((img_meta['gt_bboxes_3d']._data.gravity_center, img_meta['gt_bboxes_3d']._data.tensor[:, 3:]), dim=1) for img_meta in img_metas]\n        labels = [img_meta['gt_labels_3d']._data for img_meta in img_metas]\n        known = [torch.ones_like(t).cuda() for t in labels]\n        know_idx = known\n        unmask_bbox = unmask_label = torch.cat(known)\n        known_num = [t.size(0) for t in targets]\n        labels = torch.cat([t for t in labels])\n        boxes = torch.cat([t for t in targets])\n        batch_idx = torch.cat([torch.full((t.size(0),), i) for (i, t) in enumerate(targets)])\n        known_indice = torch.nonzero(unmask_label + unmask_bbox)\n        known_indice = known_indice.view(-1)\n        known_indice = known_indice.repeat(self.scalar, 1).view(-1)\n        known_labels = labels.repeat(self.scalar, 1).view(-1).long().to(reference_points.device)\n        known_bid = batch_idx.repeat(self.scalar, 1).view(-1)\n        known_bboxs = boxes.repeat(self.scalar, 1).to(reference_points.device)\n        known_bbox_center = known_bboxs[:, :3].clone()\n        known_bbox_scale = known_bboxs[:, 3:6].clone()\n        if self.bbox_noise_scale > 0:\n            diff = known_bbox_scale / 2 + self.bbox_noise_trans\n            rand_prob = torch.rand_like(known_bbox_center) * 2 - 1.0\n            known_bbox_center += torch.mul(rand_prob, diff) * self.bbox_noise_scale\n            known_bbox_center[..., 0:1] = (known_bbox_center[..., 0:1] - self.pc_range[0]) / (self.pc_range[3] - self.pc_range[0])\n            known_bbox_center[..., 1:2] = (known_bbox_center[..., 1:2] - self.pc_range[1]) / (self.pc_range[4] - self.pc_range[1])\n            known_bbox_center[..., 2:3] = (known_bbox_center[..., 2:3] - self.pc_range[2]) / (self.pc_range[5] - self.pc_range[2])\n            known_bbox_center = known_bbox_center.clamp(min=0.0, max=1.0)\n            mask = torch.norm(rand_prob, 2, 1) > self.split\n            known_labels[mask] = self.num_classes\n        single_pad = int(max(known_num))\n        pad_size = int(single_pad * self.scalar)\n        padding_bbox = torch.zeros(pad_size, 3).to(reference_points.device)\n        padded_reference_points = torch.cat([padding_bbox, reference_points], dim=0).unsqueeze(0).repeat(batch_size, 1, 1)\n        if len(known_num):\n            map_known_indice = torch.cat([torch.tensor(range(num)) for num in known_num])\n            map_known_indice = torch.cat([map_known_indice + single_pad * i for i in range(self.scalar)]).long()\n        if len(known_bid):\n            padded_reference_points[known_bid.long(), map_known_indice] = known_bbox_center.to(reference_points.device)\n        tgt_size = pad_size + self.num_query\n        attn_mask = torch.ones(tgt_size, tgt_size).to(reference_points.device) < 0\n        attn_mask[pad_size:, :pad_size] = True\n        for i in range(self.scalar):\n            if i == 0:\n                attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n            if i == self.scalar - 1:\n                attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n            else:\n                attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n                attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n        mask_dict = {'known_indice': torch.as_tensor(known_indice).long(), 'batch_idx': torch.as_tensor(batch_idx).long(), 'map_known_indice': torch.as_tensor(map_known_indice).long(), 'known_lbs_bboxes': (known_labels, known_bboxs), 'know_idx': know_idx, 'pad_size': pad_size}\n    else:\n        padded_reference_points = reference_points.unsqueeze(0).repeat(batch_size, 1, 1)\n        attn_mask = None\n        mask_dict = None\n    return (padded_reference_points, attn_mask, mask_dict)"
        ]
    },
    {
        "func_name": "_load_from_state_dict",
        "original": "def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n    \"\"\"load checkpoints.\"\"\"\n    version = local_metadata.get('version', None)\n    if (version is None or version < 2) and self.__class__ is PETRv2DEDNHead:\n        convert_dict = {'.self_attn.': '.attentions.0.', '.multihead_attn.': '.attentions.1.', '.decoder.norm.': '.decoder.post_norm.'}\n        state_dict_keys = list(state_dict.keys())\n        for k in state_dict_keys:\n            for (ori_key, convert_key) in convert_dict.items():\n                if ori_key in k:\n                    convert_key = k.replace(ori_key, convert_key)\n                    state_dict[convert_key] = state_dict[k]\n                    del state_dict[k]\n    super(AnchorFreeHead, self)._load_from_state_dict(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)",
        "mutated": [
            "def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n    if False:\n        i = 10\n    'load checkpoints.'\n    version = local_metadata.get('version', None)\n    if (version is None or version < 2) and self.__class__ is PETRv2DEDNHead:\n        convert_dict = {'.self_attn.': '.attentions.0.', '.multihead_attn.': '.attentions.1.', '.decoder.norm.': '.decoder.post_norm.'}\n        state_dict_keys = list(state_dict.keys())\n        for k in state_dict_keys:\n            for (ori_key, convert_key) in convert_dict.items():\n                if ori_key in k:\n                    convert_key = k.replace(ori_key, convert_key)\n                    state_dict[convert_key] = state_dict[k]\n                    del state_dict[k]\n    super(AnchorFreeHead, self)._load_from_state_dict(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)",
            "def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'load checkpoints.'\n    version = local_metadata.get('version', None)\n    if (version is None or version < 2) and self.__class__ is PETRv2DEDNHead:\n        convert_dict = {'.self_attn.': '.attentions.0.', '.multihead_attn.': '.attentions.1.', '.decoder.norm.': '.decoder.post_norm.'}\n        state_dict_keys = list(state_dict.keys())\n        for k in state_dict_keys:\n            for (ori_key, convert_key) in convert_dict.items():\n                if ori_key in k:\n                    convert_key = k.replace(ori_key, convert_key)\n                    state_dict[convert_key] = state_dict[k]\n                    del state_dict[k]\n    super(AnchorFreeHead, self)._load_from_state_dict(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)",
            "def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'load checkpoints.'\n    version = local_metadata.get('version', None)\n    if (version is None or version < 2) and self.__class__ is PETRv2DEDNHead:\n        convert_dict = {'.self_attn.': '.attentions.0.', '.multihead_attn.': '.attentions.1.', '.decoder.norm.': '.decoder.post_norm.'}\n        state_dict_keys = list(state_dict.keys())\n        for k in state_dict_keys:\n            for (ori_key, convert_key) in convert_dict.items():\n                if ori_key in k:\n                    convert_key = k.replace(ori_key, convert_key)\n                    state_dict[convert_key] = state_dict[k]\n                    del state_dict[k]\n    super(AnchorFreeHead, self)._load_from_state_dict(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)",
            "def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'load checkpoints.'\n    version = local_metadata.get('version', None)\n    if (version is None or version < 2) and self.__class__ is PETRv2DEDNHead:\n        convert_dict = {'.self_attn.': '.attentions.0.', '.multihead_attn.': '.attentions.1.', '.decoder.norm.': '.decoder.post_norm.'}\n        state_dict_keys = list(state_dict.keys())\n        for k in state_dict_keys:\n            for (ori_key, convert_key) in convert_dict.items():\n                if ori_key in k:\n                    convert_key = k.replace(ori_key, convert_key)\n                    state_dict[convert_key] = state_dict[k]\n                    del state_dict[k]\n    super(AnchorFreeHead, self)._load_from_state_dict(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)",
            "def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'load checkpoints.'\n    version = local_metadata.get('version', None)\n    if (version is None or version < 2) and self.__class__ is PETRv2DEDNHead:\n        convert_dict = {'.self_attn.': '.attentions.0.', '.multihead_attn.': '.attentions.1.', '.decoder.norm.': '.decoder.post_norm.'}\n        state_dict_keys = list(state_dict.keys())\n        for k in state_dict_keys:\n            for (ori_key, convert_key) in convert_dict.items():\n                if ori_key in k:\n                    convert_key = k.replace(ori_key, convert_key)\n                    state_dict[convert_key] = state_dict[k]\n                    del state_dict[k]\n    super(AnchorFreeHead, self)._load_from_state_dict(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, mlvl_feats, img_metas):\n    \"\"\"Forward function.\n        Args:\n            mlvl_feats (tuple[Tensor]): Features from the upstream\n                network, each is a 5D-tensor with shape\n                (B, N, C, H, W).\n        Returns:\n            all_cls_scores (Tensor): Outputs from the classification head,                 shape [nb_dec, bs, num_query, cls_out_channels]. Note                 cls_out_channels should includes background.\n            all_bbox_preds (Tensor): Sigmoid outputs from the regression                 head with normalized coordinate format (cx, cy, w, l, cz, h, theta, vx, vy).                 Shape [nb_dec, bs, num_query, 9].\n        \"\"\"\n    x = mlvl_feats[self.position_level]\n    (batch_size, num_cams) = (x.size(0), x.size(1))\n    (input_img_h, input_img_w, _) = img_metas[0]['pad_shape'][0]\n    masks = x.new_ones((batch_size, num_cams, input_img_h, input_img_w))\n    for img_id in range(batch_size):\n        for cam_id in range(num_cams):\n            (img_h, img_w, _) = img_metas[img_id]['img_shape'][cam_id]\n            masks[img_id, cam_id, :img_h, :img_w] = 0\n    if not self.with_depthnet:\n        feat = self.input_proj(x.flatten(0, 1))\n        feat = feat.view(batch_size, num_cams, *feat.shape[-3:])\n        depth = None\n        depth_all = None\n        depth_enc = None\n    else:\n        feat = x\n        use_mlp = self.depthnet_cfg['use_mlp']\n        if use_mlp:\n            mlp_input_all = self.get_mlp_input(img_metas).to(x.device).float()\n        x_de = mlvl_feats[self.depth_level]\n        self.depth_downsample = int(input_img_w / x_de.shape[-1])\n        num_views = 6\n        depth_list = []\n        feat_list = []\n        keyFrame = True\n        for frame in range(num_cams // num_views):\n            input_x = x_de[:, frame * num_views:(frame + 1) * num_views, ...]\n            mlp_input = mlp_input_all[:, frame * num_views:(frame + 1) * num_views, ...] if use_mlp else None\n            if keyFrame:\n                output_x = self.depth_net(input_x.flatten(0, 1), mlp_input)\n                output_x = output_x.view(batch_size, num_views, *output_x.shape[-3:])\n                depth_digit = output_x[:, :, :self.D, ...]\n                depth = depth_digit.softmax(dim=2)\n                feat = output_x[:, :, self.D:self.D + self.in_channels, ...]\n            else:\n                with torch.no_grad():\n                    output_x = self.depth_net(input_x.flatten(0, 1), mlp_input)\n                    output_x = output_x.view(batch_size, num_views, *output_x.shape[-3:])\n                    depth_digit = output_x[:, :, :self.D, ...]\n                    depth = depth_digit.softmax(dim=2)\n                    feat = output_x[:, :, self.D:self.D + self.in_channels, ...]\n            if not self.sweep_lidar:\n                keyFrame = False\n            depth_list += [depth]\n            feat_list += [feat]\n        depth = torch.cat(depth_list, dim=1)\n        feat = torch.cat(feat_list, dim=1)\n        depth_all = depth\n        if self.with_fde:\n            depth_all = self.fde(depth_all, feat.flatten(0, 1)).view(depth_all.size())\n        if self.depth_thresh > 0.0:\n            d_mask = depth_all.max(dim=2, keepdim=True).values\n            d_mask = torch.where(d_mask > self.depth_thresh, torch.ones_like(d_mask), torch.zeros_like(d_mask))\n            d_mask = d_mask.repeat(1, 1, depth_all.shape[2], 1, 1)\n            depth_all = torch.mul(depth_all, d_mask)\n        (de_d, de_h, de_w) = depth_all.shape[-3:]\n        (ft_h, ft_w) = feat.shape[-2:]\n        if de_h != ft_h or de_w != ft_w:\n            depth_all = F.interpolate(depth_all, size=(de_d, ft_h, ft_w))\n        if self.depth_pe:\n            depth_enc = self.depth_encoder(depth_all.flatten(0, 1))\n    masks = F.interpolate(masks, size=x.shape[-2:]).to(torch.bool)\n    if self.with_position:\n        (coords_position_embeding, _) = self.position_embeding(mlvl_feats, img_metas, masks)\n        if self.with_fpe:\n            coords_position_embeding = self.fpe(coords_position_embeding.flatten(0, 1), feat.flatten(0, 1)).view(feat.size())\n        pos_embed = coords_position_embeding\n        if self.depth_pe:\n            pos_embed += depth_enc.view(feat.size())\n        if self.with_multiview:\n            sin_embed = self.positional_encoding(masks)\n            sin_embed = self.adapt_pos3d(sin_embed.flatten(0, 1)).view(feat.size())\n            pos_embed = pos_embed + sin_embed\n        else:\n            pos_embeds = []\n            for i in range(num_cams):\n                xy_embed = self.positional_encoding(masks[:, i, :, :])\n                pos_embeds.append(xy_embed.unsqueeze(1))\n            sin_embed = torch.cat(pos_embeds, 1)\n            sin_embed = self.adapt_pos3d(sin_embed.flatten(0, 1)).view(feat.size())\n            pos_embed = pos_embed + sin_embed\n    elif self.with_multiview:\n        pos_embed = self.positional_encoding(masks)\n        pos_embed = self.adapt_pos3d(pos_embed.flatten(0, 1)).view(feat.size())\n    else:\n        pos_embeds = []\n        for i in range(num_cams):\n            pos_embed = self.positional_encoding(masks[:, i, :, :])\n            pos_embeds.append(pos_embed.unsqueeze(1))\n        pos_embed = torch.cat(pos_embeds, 1)\n    reference_points = self.reference_points.weight\n    (reference_points, attn_mask, mask_dict) = self.prepare_for_dn(batch_size, reference_points, img_metas)\n    query_embeds = self.query_embedding(pos2posemb3d(reference_points))\n    (outs_dec, _) = self.transformer(feat, masks, query_embeds, pos_embed, attn_mask, self.reg_branches)\n    if self.with_time:\n        time_stamps = []\n        for img_meta in img_metas:\n            time_stamps.append(np.asarray(img_meta['timestamp'][:12]))\n        time_stamp = x.new_tensor(np.array(time_stamps))\n        time_stamp = time_stamp.view(batch_size, -1, 6)\n        mean_time_stamp = (time_stamp[:, 1, :] - time_stamp[:, 0, :]).mean(-1)\n    outputs_classes = []\n    outputs_coords = []\n    for lvl in range(outs_dec.shape[0]):\n        reference = inverse_sigmoid(reference_points.clone())\n        assert reference.shape[-1] == 3\n        outputs_class = self.cls_branches[lvl](outs_dec[lvl])\n        tmp = self.reg_branches[lvl](outs_dec[lvl])\n        tmp[..., 0:2] += reference[..., 0:2]\n        tmp[..., 0:2] = tmp[..., 0:2].sigmoid()\n        tmp[..., 4:5] += reference[..., 2:3]\n        tmp[..., 4:5] = tmp[..., 4:5].sigmoid()\n        if self.with_time:\n            tmp[..., 8:] = tmp[..., 8:] / mean_time_stamp[:, None, None]\n        outputs_coord = tmp\n        outputs_classes.append(outputs_class)\n        outputs_coords.append(outputs_coord)\n    all_cls_scores = torch.stack(outputs_classes)\n    all_bbox_preds = torch.stack(outputs_coords)\n    tmp = all_bbox_preds[..., 0:1] * (self.pc_range[3] - self.pc_range[0])\n    all_bbox_preds[..., 0:1] = tmp + self.pc_range[0]\n    tmp = all_bbox_preds[..., 1:2] * (self.pc_range[4] - self.pc_range[1])\n    all_bbox_preds[..., 1:2] = tmp + self.pc_range[1]\n    tmp = all_bbox_preds[..., 4:5] * (self.pc_range[5] - self.pc_range[2])\n    all_bbox_preds[..., 4:5] = tmp + self.pc_range[2]\n    if mask_dict and mask_dict['pad_size'] > 0:\n        output_known_class = all_cls_scores[:, :, :mask_dict['pad_size'], :]\n        output_known_coord = all_bbox_preds[:, :, :mask_dict['pad_size'], :]\n        outputs_class = all_cls_scores[:, :, mask_dict['pad_size']:, :]\n        outputs_coord = all_bbox_preds[:, :, mask_dict['pad_size']:, :]\n        mask_dict['output_known_lbs_bboxes'] = (output_known_class, output_known_coord)\n        outs = {'all_cls_scores': outputs_class, 'all_bbox_preds': outputs_coord, 'enc_cls_scores': None, 'enc_bbox_preds': None, 'dn_mask_dict': mask_dict, 'depth_pred': depth}\n    else:\n        outs = {'all_cls_scores': all_cls_scores, 'all_bbox_preds': all_bbox_preds, 'enc_cls_scores': None, 'enc_bbox_preds': None, 'dn_mask_dict': None, 'depth_pred': depth}\n    return outs",
        "mutated": [
            "def forward(self, mlvl_feats, img_metas):\n    if False:\n        i = 10\n    'Forward function.\\n        Args:\\n            mlvl_feats (tuple[Tensor]): Features from the upstream\\n                network, each is a 5D-tensor with shape\\n                (B, N, C, H, W).\\n        Returns:\\n            all_cls_scores (Tensor): Outputs from the classification head,                 shape [nb_dec, bs, num_query, cls_out_channels]. Note                 cls_out_channels should includes background.\\n            all_bbox_preds (Tensor): Sigmoid outputs from the regression                 head with normalized coordinate format (cx, cy, w, l, cz, h, theta, vx, vy).                 Shape [nb_dec, bs, num_query, 9].\\n        '\n    x = mlvl_feats[self.position_level]\n    (batch_size, num_cams) = (x.size(0), x.size(1))\n    (input_img_h, input_img_w, _) = img_metas[0]['pad_shape'][0]\n    masks = x.new_ones((batch_size, num_cams, input_img_h, input_img_w))\n    for img_id in range(batch_size):\n        for cam_id in range(num_cams):\n            (img_h, img_w, _) = img_metas[img_id]['img_shape'][cam_id]\n            masks[img_id, cam_id, :img_h, :img_w] = 0\n    if not self.with_depthnet:\n        feat = self.input_proj(x.flatten(0, 1))\n        feat = feat.view(batch_size, num_cams, *feat.shape[-3:])\n        depth = None\n        depth_all = None\n        depth_enc = None\n    else:\n        feat = x\n        use_mlp = self.depthnet_cfg['use_mlp']\n        if use_mlp:\n            mlp_input_all = self.get_mlp_input(img_metas).to(x.device).float()\n        x_de = mlvl_feats[self.depth_level]\n        self.depth_downsample = int(input_img_w / x_de.shape[-1])\n        num_views = 6\n        depth_list = []\n        feat_list = []\n        keyFrame = True\n        for frame in range(num_cams // num_views):\n            input_x = x_de[:, frame * num_views:(frame + 1) * num_views, ...]\n            mlp_input = mlp_input_all[:, frame * num_views:(frame + 1) * num_views, ...] if use_mlp else None\n            if keyFrame:\n                output_x = self.depth_net(input_x.flatten(0, 1), mlp_input)\n                output_x = output_x.view(batch_size, num_views, *output_x.shape[-3:])\n                depth_digit = output_x[:, :, :self.D, ...]\n                depth = depth_digit.softmax(dim=2)\n                feat = output_x[:, :, self.D:self.D + self.in_channels, ...]\n            else:\n                with torch.no_grad():\n                    output_x = self.depth_net(input_x.flatten(0, 1), mlp_input)\n                    output_x = output_x.view(batch_size, num_views, *output_x.shape[-3:])\n                    depth_digit = output_x[:, :, :self.D, ...]\n                    depth = depth_digit.softmax(dim=2)\n                    feat = output_x[:, :, self.D:self.D + self.in_channels, ...]\n            if not self.sweep_lidar:\n                keyFrame = False\n            depth_list += [depth]\n            feat_list += [feat]\n        depth = torch.cat(depth_list, dim=1)\n        feat = torch.cat(feat_list, dim=1)\n        depth_all = depth\n        if self.with_fde:\n            depth_all = self.fde(depth_all, feat.flatten(0, 1)).view(depth_all.size())\n        if self.depth_thresh > 0.0:\n            d_mask = depth_all.max(dim=2, keepdim=True).values\n            d_mask = torch.where(d_mask > self.depth_thresh, torch.ones_like(d_mask), torch.zeros_like(d_mask))\n            d_mask = d_mask.repeat(1, 1, depth_all.shape[2], 1, 1)\n            depth_all = torch.mul(depth_all, d_mask)\n        (de_d, de_h, de_w) = depth_all.shape[-3:]\n        (ft_h, ft_w) = feat.shape[-2:]\n        if de_h != ft_h or de_w != ft_w:\n            depth_all = F.interpolate(depth_all, size=(de_d, ft_h, ft_w))\n        if self.depth_pe:\n            depth_enc = self.depth_encoder(depth_all.flatten(0, 1))\n    masks = F.interpolate(masks, size=x.shape[-2:]).to(torch.bool)\n    if self.with_position:\n        (coords_position_embeding, _) = self.position_embeding(mlvl_feats, img_metas, masks)\n        if self.with_fpe:\n            coords_position_embeding = self.fpe(coords_position_embeding.flatten(0, 1), feat.flatten(0, 1)).view(feat.size())\n        pos_embed = coords_position_embeding\n        if self.depth_pe:\n            pos_embed += depth_enc.view(feat.size())\n        if self.with_multiview:\n            sin_embed = self.positional_encoding(masks)\n            sin_embed = self.adapt_pos3d(sin_embed.flatten(0, 1)).view(feat.size())\n            pos_embed = pos_embed + sin_embed\n        else:\n            pos_embeds = []\n            for i in range(num_cams):\n                xy_embed = self.positional_encoding(masks[:, i, :, :])\n                pos_embeds.append(xy_embed.unsqueeze(1))\n            sin_embed = torch.cat(pos_embeds, 1)\n            sin_embed = self.adapt_pos3d(sin_embed.flatten(0, 1)).view(feat.size())\n            pos_embed = pos_embed + sin_embed\n    elif self.with_multiview:\n        pos_embed = self.positional_encoding(masks)\n        pos_embed = self.adapt_pos3d(pos_embed.flatten(0, 1)).view(feat.size())\n    else:\n        pos_embeds = []\n        for i in range(num_cams):\n            pos_embed = self.positional_encoding(masks[:, i, :, :])\n            pos_embeds.append(pos_embed.unsqueeze(1))\n        pos_embed = torch.cat(pos_embeds, 1)\n    reference_points = self.reference_points.weight\n    (reference_points, attn_mask, mask_dict) = self.prepare_for_dn(batch_size, reference_points, img_metas)\n    query_embeds = self.query_embedding(pos2posemb3d(reference_points))\n    (outs_dec, _) = self.transformer(feat, masks, query_embeds, pos_embed, attn_mask, self.reg_branches)\n    if self.with_time:\n        time_stamps = []\n        for img_meta in img_metas:\n            time_stamps.append(np.asarray(img_meta['timestamp'][:12]))\n        time_stamp = x.new_tensor(np.array(time_stamps))\n        time_stamp = time_stamp.view(batch_size, -1, 6)\n        mean_time_stamp = (time_stamp[:, 1, :] - time_stamp[:, 0, :]).mean(-1)\n    outputs_classes = []\n    outputs_coords = []\n    for lvl in range(outs_dec.shape[0]):\n        reference = inverse_sigmoid(reference_points.clone())\n        assert reference.shape[-1] == 3\n        outputs_class = self.cls_branches[lvl](outs_dec[lvl])\n        tmp = self.reg_branches[lvl](outs_dec[lvl])\n        tmp[..., 0:2] += reference[..., 0:2]\n        tmp[..., 0:2] = tmp[..., 0:2].sigmoid()\n        tmp[..., 4:5] += reference[..., 2:3]\n        tmp[..., 4:5] = tmp[..., 4:5].sigmoid()\n        if self.with_time:\n            tmp[..., 8:] = tmp[..., 8:] / mean_time_stamp[:, None, None]\n        outputs_coord = tmp\n        outputs_classes.append(outputs_class)\n        outputs_coords.append(outputs_coord)\n    all_cls_scores = torch.stack(outputs_classes)\n    all_bbox_preds = torch.stack(outputs_coords)\n    tmp = all_bbox_preds[..., 0:1] * (self.pc_range[3] - self.pc_range[0])\n    all_bbox_preds[..., 0:1] = tmp + self.pc_range[0]\n    tmp = all_bbox_preds[..., 1:2] * (self.pc_range[4] - self.pc_range[1])\n    all_bbox_preds[..., 1:2] = tmp + self.pc_range[1]\n    tmp = all_bbox_preds[..., 4:5] * (self.pc_range[5] - self.pc_range[2])\n    all_bbox_preds[..., 4:5] = tmp + self.pc_range[2]\n    if mask_dict and mask_dict['pad_size'] > 0:\n        output_known_class = all_cls_scores[:, :, :mask_dict['pad_size'], :]\n        output_known_coord = all_bbox_preds[:, :, :mask_dict['pad_size'], :]\n        outputs_class = all_cls_scores[:, :, mask_dict['pad_size']:, :]\n        outputs_coord = all_bbox_preds[:, :, mask_dict['pad_size']:, :]\n        mask_dict['output_known_lbs_bboxes'] = (output_known_class, output_known_coord)\n        outs = {'all_cls_scores': outputs_class, 'all_bbox_preds': outputs_coord, 'enc_cls_scores': None, 'enc_bbox_preds': None, 'dn_mask_dict': mask_dict, 'depth_pred': depth}\n    else:\n        outs = {'all_cls_scores': all_cls_scores, 'all_bbox_preds': all_bbox_preds, 'enc_cls_scores': None, 'enc_bbox_preds': None, 'dn_mask_dict': None, 'depth_pred': depth}\n    return outs",
            "def forward(self, mlvl_feats, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Forward function.\\n        Args:\\n            mlvl_feats (tuple[Tensor]): Features from the upstream\\n                network, each is a 5D-tensor with shape\\n                (B, N, C, H, W).\\n        Returns:\\n            all_cls_scores (Tensor): Outputs from the classification head,                 shape [nb_dec, bs, num_query, cls_out_channels]. Note                 cls_out_channels should includes background.\\n            all_bbox_preds (Tensor): Sigmoid outputs from the regression                 head with normalized coordinate format (cx, cy, w, l, cz, h, theta, vx, vy).                 Shape [nb_dec, bs, num_query, 9].\\n        '\n    x = mlvl_feats[self.position_level]\n    (batch_size, num_cams) = (x.size(0), x.size(1))\n    (input_img_h, input_img_w, _) = img_metas[0]['pad_shape'][0]\n    masks = x.new_ones((batch_size, num_cams, input_img_h, input_img_w))\n    for img_id in range(batch_size):\n        for cam_id in range(num_cams):\n            (img_h, img_w, _) = img_metas[img_id]['img_shape'][cam_id]\n            masks[img_id, cam_id, :img_h, :img_w] = 0\n    if not self.with_depthnet:\n        feat = self.input_proj(x.flatten(0, 1))\n        feat = feat.view(batch_size, num_cams, *feat.shape[-3:])\n        depth = None\n        depth_all = None\n        depth_enc = None\n    else:\n        feat = x\n        use_mlp = self.depthnet_cfg['use_mlp']\n        if use_mlp:\n            mlp_input_all = self.get_mlp_input(img_metas).to(x.device).float()\n        x_de = mlvl_feats[self.depth_level]\n        self.depth_downsample = int(input_img_w / x_de.shape[-1])\n        num_views = 6\n        depth_list = []\n        feat_list = []\n        keyFrame = True\n        for frame in range(num_cams // num_views):\n            input_x = x_de[:, frame * num_views:(frame + 1) * num_views, ...]\n            mlp_input = mlp_input_all[:, frame * num_views:(frame + 1) * num_views, ...] if use_mlp else None\n            if keyFrame:\n                output_x = self.depth_net(input_x.flatten(0, 1), mlp_input)\n                output_x = output_x.view(batch_size, num_views, *output_x.shape[-3:])\n                depth_digit = output_x[:, :, :self.D, ...]\n                depth = depth_digit.softmax(dim=2)\n                feat = output_x[:, :, self.D:self.D + self.in_channels, ...]\n            else:\n                with torch.no_grad():\n                    output_x = self.depth_net(input_x.flatten(0, 1), mlp_input)\n                    output_x = output_x.view(batch_size, num_views, *output_x.shape[-3:])\n                    depth_digit = output_x[:, :, :self.D, ...]\n                    depth = depth_digit.softmax(dim=2)\n                    feat = output_x[:, :, self.D:self.D + self.in_channels, ...]\n            if not self.sweep_lidar:\n                keyFrame = False\n            depth_list += [depth]\n            feat_list += [feat]\n        depth = torch.cat(depth_list, dim=1)\n        feat = torch.cat(feat_list, dim=1)\n        depth_all = depth\n        if self.with_fde:\n            depth_all = self.fde(depth_all, feat.flatten(0, 1)).view(depth_all.size())\n        if self.depth_thresh > 0.0:\n            d_mask = depth_all.max(dim=2, keepdim=True).values\n            d_mask = torch.where(d_mask > self.depth_thresh, torch.ones_like(d_mask), torch.zeros_like(d_mask))\n            d_mask = d_mask.repeat(1, 1, depth_all.shape[2], 1, 1)\n            depth_all = torch.mul(depth_all, d_mask)\n        (de_d, de_h, de_w) = depth_all.shape[-3:]\n        (ft_h, ft_w) = feat.shape[-2:]\n        if de_h != ft_h or de_w != ft_w:\n            depth_all = F.interpolate(depth_all, size=(de_d, ft_h, ft_w))\n        if self.depth_pe:\n            depth_enc = self.depth_encoder(depth_all.flatten(0, 1))\n    masks = F.interpolate(masks, size=x.shape[-2:]).to(torch.bool)\n    if self.with_position:\n        (coords_position_embeding, _) = self.position_embeding(mlvl_feats, img_metas, masks)\n        if self.with_fpe:\n            coords_position_embeding = self.fpe(coords_position_embeding.flatten(0, 1), feat.flatten(0, 1)).view(feat.size())\n        pos_embed = coords_position_embeding\n        if self.depth_pe:\n            pos_embed += depth_enc.view(feat.size())\n        if self.with_multiview:\n            sin_embed = self.positional_encoding(masks)\n            sin_embed = self.adapt_pos3d(sin_embed.flatten(0, 1)).view(feat.size())\n            pos_embed = pos_embed + sin_embed\n        else:\n            pos_embeds = []\n            for i in range(num_cams):\n                xy_embed = self.positional_encoding(masks[:, i, :, :])\n                pos_embeds.append(xy_embed.unsqueeze(1))\n            sin_embed = torch.cat(pos_embeds, 1)\n            sin_embed = self.adapt_pos3d(sin_embed.flatten(0, 1)).view(feat.size())\n            pos_embed = pos_embed + sin_embed\n    elif self.with_multiview:\n        pos_embed = self.positional_encoding(masks)\n        pos_embed = self.adapt_pos3d(pos_embed.flatten(0, 1)).view(feat.size())\n    else:\n        pos_embeds = []\n        for i in range(num_cams):\n            pos_embed = self.positional_encoding(masks[:, i, :, :])\n            pos_embeds.append(pos_embed.unsqueeze(1))\n        pos_embed = torch.cat(pos_embeds, 1)\n    reference_points = self.reference_points.weight\n    (reference_points, attn_mask, mask_dict) = self.prepare_for_dn(batch_size, reference_points, img_metas)\n    query_embeds = self.query_embedding(pos2posemb3d(reference_points))\n    (outs_dec, _) = self.transformer(feat, masks, query_embeds, pos_embed, attn_mask, self.reg_branches)\n    if self.with_time:\n        time_stamps = []\n        for img_meta in img_metas:\n            time_stamps.append(np.asarray(img_meta['timestamp'][:12]))\n        time_stamp = x.new_tensor(np.array(time_stamps))\n        time_stamp = time_stamp.view(batch_size, -1, 6)\n        mean_time_stamp = (time_stamp[:, 1, :] - time_stamp[:, 0, :]).mean(-1)\n    outputs_classes = []\n    outputs_coords = []\n    for lvl in range(outs_dec.shape[0]):\n        reference = inverse_sigmoid(reference_points.clone())\n        assert reference.shape[-1] == 3\n        outputs_class = self.cls_branches[lvl](outs_dec[lvl])\n        tmp = self.reg_branches[lvl](outs_dec[lvl])\n        tmp[..., 0:2] += reference[..., 0:2]\n        tmp[..., 0:2] = tmp[..., 0:2].sigmoid()\n        tmp[..., 4:5] += reference[..., 2:3]\n        tmp[..., 4:5] = tmp[..., 4:5].sigmoid()\n        if self.with_time:\n            tmp[..., 8:] = tmp[..., 8:] / mean_time_stamp[:, None, None]\n        outputs_coord = tmp\n        outputs_classes.append(outputs_class)\n        outputs_coords.append(outputs_coord)\n    all_cls_scores = torch.stack(outputs_classes)\n    all_bbox_preds = torch.stack(outputs_coords)\n    tmp = all_bbox_preds[..., 0:1] * (self.pc_range[3] - self.pc_range[0])\n    all_bbox_preds[..., 0:1] = tmp + self.pc_range[0]\n    tmp = all_bbox_preds[..., 1:2] * (self.pc_range[4] - self.pc_range[1])\n    all_bbox_preds[..., 1:2] = tmp + self.pc_range[1]\n    tmp = all_bbox_preds[..., 4:5] * (self.pc_range[5] - self.pc_range[2])\n    all_bbox_preds[..., 4:5] = tmp + self.pc_range[2]\n    if mask_dict and mask_dict['pad_size'] > 0:\n        output_known_class = all_cls_scores[:, :, :mask_dict['pad_size'], :]\n        output_known_coord = all_bbox_preds[:, :, :mask_dict['pad_size'], :]\n        outputs_class = all_cls_scores[:, :, mask_dict['pad_size']:, :]\n        outputs_coord = all_bbox_preds[:, :, mask_dict['pad_size']:, :]\n        mask_dict['output_known_lbs_bboxes'] = (output_known_class, output_known_coord)\n        outs = {'all_cls_scores': outputs_class, 'all_bbox_preds': outputs_coord, 'enc_cls_scores': None, 'enc_bbox_preds': None, 'dn_mask_dict': mask_dict, 'depth_pred': depth}\n    else:\n        outs = {'all_cls_scores': all_cls_scores, 'all_bbox_preds': all_bbox_preds, 'enc_cls_scores': None, 'enc_bbox_preds': None, 'dn_mask_dict': None, 'depth_pred': depth}\n    return outs",
            "def forward(self, mlvl_feats, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Forward function.\\n        Args:\\n            mlvl_feats (tuple[Tensor]): Features from the upstream\\n                network, each is a 5D-tensor with shape\\n                (B, N, C, H, W).\\n        Returns:\\n            all_cls_scores (Tensor): Outputs from the classification head,                 shape [nb_dec, bs, num_query, cls_out_channels]. Note                 cls_out_channels should includes background.\\n            all_bbox_preds (Tensor): Sigmoid outputs from the regression                 head with normalized coordinate format (cx, cy, w, l, cz, h, theta, vx, vy).                 Shape [nb_dec, bs, num_query, 9].\\n        '\n    x = mlvl_feats[self.position_level]\n    (batch_size, num_cams) = (x.size(0), x.size(1))\n    (input_img_h, input_img_w, _) = img_metas[0]['pad_shape'][0]\n    masks = x.new_ones((batch_size, num_cams, input_img_h, input_img_w))\n    for img_id in range(batch_size):\n        for cam_id in range(num_cams):\n            (img_h, img_w, _) = img_metas[img_id]['img_shape'][cam_id]\n            masks[img_id, cam_id, :img_h, :img_w] = 0\n    if not self.with_depthnet:\n        feat = self.input_proj(x.flatten(0, 1))\n        feat = feat.view(batch_size, num_cams, *feat.shape[-3:])\n        depth = None\n        depth_all = None\n        depth_enc = None\n    else:\n        feat = x\n        use_mlp = self.depthnet_cfg['use_mlp']\n        if use_mlp:\n            mlp_input_all = self.get_mlp_input(img_metas).to(x.device).float()\n        x_de = mlvl_feats[self.depth_level]\n        self.depth_downsample = int(input_img_w / x_de.shape[-1])\n        num_views = 6\n        depth_list = []\n        feat_list = []\n        keyFrame = True\n        for frame in range(num_cams // num_views):\n            input_x = x_de[:, frame * num_views:(frame + 1) * num_views, ...]\n            mlp_input = mlp_input_all[:, frame * num_views:(frame + 1) * num_views, ...] if use_mlp else None\n            if keyFrame:\n                output_x = self.depth_net(input_x.flatten(0, 1), mlp_input)\n                output_x = output_x.view(batch_size, num_views, *output_x.shape[-3:])\n                depth_digit = output_x[:, :, :self.D, ...]\n                depth = depth_digit.softmax(dim=2)\n                feat = output_x[:, :, self.D:self.D + self.in_channels, ...]\n            else:\n                with torch.no_grad():\n                    output_x = self.depth_net(input_x.flatten(0, 1), mlp_input)\n                    output_x = output_x.view(batch_size, num_views, *output_x.shape[-3:])\n                    depth_digit = output_x[:, :, :self.D, ...]\n                    depth = depth_digit.softmax(dim=2)\n                    feat = output_x[:, :, self.D:self.D + self.in_channels, ...]\n            if not self.sweep_lidar:\n                keyFrame = False\n            depth_list += [depth]\n            feat_list += [feat]\n        depth = torch.cat(depth_list, dim=1)\n        feat = torch.cat(feat_list, dim=1)\n        depth_all = depth\n        if self.with_fde:\n            depth_all = self.fde(depth_all, feat.flatten(0, 1)).view(depth_all.size())\n        if self.depth_thresh > 0.0:\n            d_mask = depth_all.max(dim=2, keepdim=True).values\n            d_mask = torch.where(d_mask > self.depth_thresh, torch.ones_like(d_mask), torch.zeros_like(d_mask))\n            d_mask = d_mask.repeat(1, 1, depth_all.shape[2], 1, 1)\n            depth_all = torch.mul(depth_all, d_mask)\n        (de_d, de_h, de_w) = depth_all.shape[-3:]\n        (ft_h, ft_w) = feat.shape[-2:]\n        if de_h != ft_h or de_w != ft_w:\n            depth_all = F.interpolate(depth_all, size=(de_d, ft_h, ft_w))\n        if self.depth_pe:\n            depth_enc = self.depth_encoder(depth_all.flatten(0, 1))\n    masks = F.interpolate(masks, size=x.shape[-2:]).to(torch.bool)\n    if self.with_position:\n        (coords_position_embeding, _) = self.position_embeding(mlvl_feats, img_metas, masks)\n        if self.with_fpe:\n            coords_position_embeding = self.fpe(coords_position_embeding.flatten(0, 1), feat.flatten(0, 1)).view(feat.size())\n        pos_embed = coords_position_embeding\n        if self.depth_pe:\n            pos_embed += depth_enc.view(feat.size())\n        if self.with_multiview:\n            sin_embed = self.positional_encoding(masks)\n            sin_embed = self.adapt_pos3d(sin_embed.flatten(0, 1)).view(feat.size())\n            pos_embed = pos_embed + sin_embed\n        else:\n            pos_embeds = []\n            for i in range(num_cams):\n                xy_embed = self.positional_encoding(masks[:, i, :, :])\n                pos_embeds.append(xy_embed.unsqueeze(1))\n            sin_embed = torch.cat(pos_embeds, 1)\n            sin_embed = self.adapt_pos3d(sin_embed.flatten(0, 1)).view(feat.size())\n            pos_embed = pos_embed + sin_embed\n    elif self.with_multiview:\n        pos_embed = self.positional_encoding(masks)\n        pos_embed = self.adapt_pos3d(pos_embed.flatten(0, 1)).view(feat.size())\n    else:\n        pos_embeds = []\n        for i in range(num_cams):\n            pos_embed = self.positional_encoding(masks[:, i, :, :])\n            pos_embeds.append(pos_embed.unsqueeze(1))\n        pos_embed = torch.cat(pos_embeds, 1)\n    reference_points = self.reference_points.weight\n    (reference_points, attn_mask, mask_dict) = self.prepare_for_dn(batch_size, reference_points, img_metas)\n    query_embeds = self.query_embedding(pos2posemb3d(reference_points))\n    (outs_dec, _) = self.transformer(feat, masks, query_embeds, pos_embed, attn_mask, self.reg_branches)\n    if self.with_time:\n        time_stamps = []\n        for img_meta in img_metas:\n            time_stamps.append(np.asarray(img_meta['timestamp'][:12]))\n        time_stamp = x.new_tensor(np.array(time_stamps))\n        time_stamp = time_stamp.view(batch_size, -1, 6)\n        mean_time_stamp = (time_stamp[:, 1, :] - time_stamp[:, 0, :]).mean(-1)\n    outputs_classes = []\n    outputs_coords = []\n    for lvl in range(outs_dec.shape[0]):\n        reference = inverse_sigmoid(reference_points.clone())\n        assert reference.shape[-1] == 3\n        outputs_class = self.cls_branches[lvl](outs_dec[lvl])\n        tmp = self.reg_branches[lvl](outs_dec[lvl])\n        tmp[..., 0:2] += reference[..., 0:2]\n        tmp[..., 0:2] = tmp[..., 0:2].sigmoid()\n        tmp[..., 4:5] += reference[..., 2:3]\n        tmp[..., 4:5] = tmp[..., 4:5].sigmoid()\n        if self.with_time:\n            tmp[..., 8:] = tmp[..., 8:] / mean_time_stamp[:, None, None]\n        outputs_coord = tmp\n        outputs_classes.append(outputs_class)\n        outputs_coords.append(outputs_coord)\n    all_cls_scores = torch.stack(outputs_classes)\n    all_bbox_preds = torch.stack(outputs_coords)\n    tmp = all_bbox_preds[..., 0:1] * (self.pc_range[3] - self.pc_range[0])\n    all_bbox_preds[..., 0:1] = tmp + self.pc_range[0]\n    tmp = all_bbox_preds[..., 1:2] * (self.pc_range[4] - self.pc_range[1])\n    all_bbox_preds[..., 1:2] = tmp + self.pc_range[1]\n    tmp = all_bbox_preds[..., 4:5] * (self.pc_range[5] - self.pc_range[2])\n    all_bbox_preds[..., 4:5] = tmp + self.pc_range[2]\n    if mask_dict and mask_dict['pad_size'] > 0:\n        output_known_class = all_cls_scores[:, :, :mask_dict['pad_size'], :]\n        output_known_coord = all_bbox_preds[:, :, :mask_dict['pad_size'], :]\n        outputs_class = all_cls_scores[:, :, mask_dict['pad_size']:, :]\n        outputs_coord = all_bbox_preds[:, :, mask_dict['pad_size']:, :]\n        mask_dict['output_known_lbs_bboxes'] = (output_known_class, output_known_coord)\n        outs = {'all_cls_scores': outputs_class, 'all_bbox_preds': outputs_coord, 'enc_cls_scores': None, 'enc_bbox_preds': None, 'dn_mask_dict': mask_dict, 'depth_pred': depth}\n    else:\n        outs = {'all_cls_scores': all_cls_scores, 'all_bbox_preds': all_bbox_preds, 'enc_cls_scores': None, 'enc_bbox_preds': None, 'dn_mask_dict': None, 'depth_pred': depth}\n    return outs",
            "def forward(self, mlvl_feats, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Forward function.\\n        Args:\\n            mlvl_feats (tuple[Tensor]): Features from the upstream\\n                network, each is a 5D-tensor with shape\\n                (B, N, C, H, W).\\n        Returns:\\n            all_cls_scores (Tensor): Outputs from the classification head,                 shape [nb_dec, bs, num_query, cls_out_channels]. Note                 cls_out_channels should includes background.\\n            all_bbox_preds (Tensor): Sigmoid outputs from the regression                 head with normalized coordinate format (cx, cy, w, l, cz, h, theta, vx, vy).                 Shape [nb_dec, bs, num_query, 9].\\n        '\n    x = mlvl_feats[self.position_level]\n    (batch_size, num_cams) = (x.size(0), x.size(1))\n    (input_img_h, input_img_w, _) = img_metas[0]['pad_shape'][0]\n    masks = x.new_ones((batch_size, num_cams, input_img_h, input_img_w))\n    for img_id in range(batch_size):\n        for cam_id in range(num_cams):\n            (img_h, img_w, _) = img_metas[img_id]['img_shape'][cam_id]\n            masks[img_id, cam_id, :img_h, :img_w] = 0\n    if not self.with_depthnet:\n        feat = self.input_proj(x.flatten(0, 1))\n        feat = feat.view(batch_size, num_cams, *feat.shape[-3:])\n        depth = None\n        depth_all = None\n        depth_enc = None\n    else:\n        feat = x\n        use_mlp = self.depthnet_cfg['use_mlp']\n        if use_mlp:\n            mlp_input_all = self.get_mlp_input(img_metas).to(x.device).float()\n        x_de = mlvl_feats[self.depth_level]\n        self.depth_downsample = int(input_img_w / x_de.shape[-1])\n        num_views = 6\n        depth_list = []\n        feat_list = []\n        keyFrame = True\n        for frame in range(num_cams // num_views):\n            input_x = x_de[:, frame * num_views:(frame + 1) * num_views, ...]\n            mlp_input = mlp_input_all[:, frame * num_views:(frame + 1) * num_views, ...] if use_mlp else None\n            if keyFrame:\n                output_x = self.depth_net(input_x.flatten(0, 1), mlp_input)\n                output_x = output_x.view(batch_size, num_views, *output_x.shape[-3:])\n                depth_digit = output_x[:, :, :self.D, ...]\n                depth = depth_digit.softmax(dim=2)\n                feat = output_x[:, :, self.D:self.D + self.in_channels, ...]\n            else:\n                with torch.no_grad():\n                    output_x = self.depth_net(input_x.flatten(0, 1), mlp_input)\n                    output_x = output_x.view(batch_size, num_views, *output_x.shape[-3:])\n                    depth_digit = output_x[:, :, :self.D, ...]\n                    depth = depth_digit.softmax(dim=2)\n                    feat = output_x[:, :, self.D:self.D + self.in_channels, ...]\n            if not self.sweep_lidar:\n                keyFrame = False\n            depth_list += [depth]\n            feat_list += [feat]\n        depth = torch.cat(depth_list, dim=1)\n        feat = torch.cat(feat_list, dim=1)\n        depth_all = depth\n        if self.with_fde:\n            depth_all = self.fde(depth_all, feat.flatten(0, 1)).view(depth_all.size())\n        if self.depth_thresh > 0.0:\n            d_mask = depth_all.max(dim=2, keepdim=True).values\n            d_mask = torch.where(d_mask > self.depth_thresh, torch.ones_like(d_mask), torch.zeros_like(d_mask))\n            d_mask = d_mask.repeat(1, 1, depth_all.shape[2], 1, 1)\n            depth_all = torch.mul(depth_all, d_mask)\n        (de_d, de_h, de_w) = depth_all.shape[-3:]\n        (ft_h, ft_w) = feat.shape[-2:]\n        if de_h != ft_h or de_w != ft_w:\n            depth_all = F.interpolate(depth_all, size=(de_d, ft_h, ft_w))\n        if self.depth_pe:\n            depth_enc = self.depth_encoder(depth_all.flatten(0, 1))\n    masks = F.interpolate(masks, size=x.shape[-2:]).to(torch.bool)\n    if self.with_position:\n        (coords_position_embeding, _) = self.position_embeding(mlvl_feats, img_metas, masks)\n        if self.with_fpe:\n            coords_position_embeding = self.fpe(coords_position_embeding.flatten(0, 1), feat.flatten(0, 1)).view(feat.size())\n        pos_embed = coords_position_embeding\n        if self.depth_pe:\n            pos_embed += depth_enc.view(feat.size())\n        if self.with_multiview:\n            sin_embed = self.positional_encoding(masks)\n            sin_embed = self.adapt_pos3d(sin_embed.flatten(0, 1)).view(feat.size())\n            pos_embed = pos_embed + sin_embed\n        else:\n            pos_embeds = []\n            for i in range(num_cams):\n                xy_embed = self.positional_encoding(masks[:, i, :, :])\n                pos_embeds.append(xy_embed.unsqueeze(1))\n            sin_embed = torch.cat(pos_embeds, 1)\n            sin_embed = self.adapt_pos3d(sin_embed.flatten(0, 1)).view(feat.size())\n            pos_embed = pos_embed + sin_embed\n    elif self.with_multiview:\n        pos_embed = self.positional_encoding(masks)\n        pos_embed = self.adapt_pos3d(pos_embed.flatten(0, 1)).view(feat.size())\n    else:\n        pos_embeds = []\n        for i in range(num_cams):\n            pos_embed = self.positional_encoding(masks[:, i, :, :])\n            pos_embeds.append(pos_embed.unsqueeze(1))\n        pos_embed = torch.cat(pos_embeds, 1)\n    reference_points = self.reference_points.weight\n    (reference_points, attn_mask, mask_dict) = self.prepare_for_dn(batch_size, reference_points, img_metas)\n    query_embeds = self.query_embedding(pos2posemb3d(reference_points))\n    (outs_dec, _) = self.transformer(feat, masks, query_embeds, pos_embed, attn_mask, self.reg_branches)\n    if self.with_time:\n        time_stamps = []\n        for img_meta in img_metas:\n            time_stamps.append(np.asarray(img_meta['timestamp'][:12]))\n        time_stamp = x.new_tensor(np.array(time_stamps))\n        time_stamp = time_stamp.view(batch_size, -1, 6)\n        mean_time_stamp = (time_stamp[:, 1, :] - time_stamp[:, 0, :]).mean(-1)\n    outputs_classes = []\n    outputs_coords = []\n    for lvl in range(outs_dec.shape[0]):\n        reference = inverse_sigmoid(reference_points.clone())\n        assert reference.shape[-1] == 3\n        outputs_class = self.cls_branches[lvl](outs_dec[lvl])\n        tmp = self.reg_branches[lvl](outs_dec[lvl])\n        tmp[..., 0:2] += reference[..., 0:2]\n        tmp[..., 0:2] = tmp[..., 0:2].sigmoid()\n        tmp[..., 4:5] += reference[..., 2:3]\n        tmp[..., 4:5] = tmp[..., 4:5].sigmoid()\n        if self.with_time:\n            tmp[..., 8:] = tmp[..., 8:] / mean_time_stamp[:, None, None]\n        outputs_coord = tmp\n        outputs_classes.append(outputs_class)\n        outputs_coords.append(outputs_coord)\n    all_cls_scores = torch.stack(outputs_classes)\n    all_bbox_preds = torch.stack(outputs_coords)\n    tmp = all_bbox_preds[..., 0:1] * (self.pc_range[3] - self.pc_range[0])\n    all_bbox_preds[..., 0:1] = tmp + self.pc_range[0]\n    tmp = all_bbox_preds[..., 1:2] * (self.pc_range[4] - self.pc_range[1])\n    all_bbox_preds[..., 1:2] = tmp + self.pc_range[1]\n    tmp = all_bbox_preds[..., 4:5] * (self.pc_range[5] - self.pc_range[2])\n    all_bbox_preds[..., 4:5] = tmp + self.pc_range[2]\n    if mask_dict and mask_dict['pad_size'] > 0:\n        output_known_class = all_cls_scores[:, :, :mask_dict['pad_size'], :]\n        output_known_coord = all_bbox_preds[:, :, :mask_dict['pad_size'], :]\n        outputs_class = all_cls_scores[:, :, mask_dict['pad_size']:, :]\n        outputs_coord = all_bbox_preds[:, :, mask_dict['pad_size']:, :]\n        mask_dict['output_known_lbs_bboxes'] = (output_known_class, output_known_coord)\n        outs = {'all_cls_scores': outputs_class, 'all_bbox_preds': outputs_coord, 'enc_cls_scores': None, 'enc_bbox_preds': None, 'dn_mask_dict': mask_dict, 'depth_pred': depth}\n    else:\n        outs = {'all_cls_scores': all_cls_scores, 'all_bbox_preds': all_bbox_preds, 'enc_cls_scores': None, 'enc_bbox_preds': None, 'dn_mask_dict': None, 'depth_pred': depth}\n    return outs",
            "def forward(self, mlvl_feats, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Forward function.\\n        Args:\\n            mlvl_feats (tuple[Tensor]): Features from the upstream\\n                network, each is a 5D-tensor with shape\\n                (B, N, C, H, W).\\n        Returns:\\n            all_cls_scores (Tensor): Outputs from the classification head,                 shape [nb_dec, bs, num_query, cls_out_channels]. Note                 cls_out_channels should includes background.\\n            all_bbox_preds (Tensor): Sigmoid outputs from the regression                 head with normalized coordinate format (cx, cy, w, l, cz, h, theta, vx, vy).                 Shape [nb_dec, bs, num_query, 9].\\n        '\n    x = mlvl_feats[self.position_level]\n    (batch_size, num_cams) = (x.size(0), x.size(1))\n    (input_img_h, input_img_w, _) = img_metas[0]['pad_shape'][0]\n    masks = x.new_ones((batch_size, num_cams, input_img_h, input_img_w))\n    for img_id in range(batch_size):\n        for cam_id in range(num_cams):\n            (img_h, img_w, _) = img_metas[img_id]['img_shape'][cam_id]\n            masks[img_id, cam_id, :img_h, :img_w] = 0\n    if not self.with_depthnet:\n        feat = self.input_proj(x.flatten(0, 1))\n        feat = feat.view(batch_size, num_cams, *feat.shape[-3:])\n        depth = None\n        depth_all = None\n        depth_enc = None\n    else:\n        feat = x\n        use_mlp = self.depthnet_cfg['use_mlp']\n        if use_mlp:\n            mlp_input_all = self.get_mlp_input(img_metas).to(x.device).float()\n        x_de = mlvl_feats[self.depth_level]\n        self.depth_downsample = int(input_img_w / x_de.shape[-1])\n        num_views = 6\n        depth_list = []\n        feat_list = []\n        keyFrame = True\n        for frame in range(num_cams // num_views):\n            input_x = x_de[:, frame * num_views:(frame + 1) * num_views, ...]\n            mlp_input = mlp_input_all[:, frame * num_views:(frame + 1) * num_views, ...] if use_mlp else None\n            if keyFrame:\n                output_x = self.depth_net(input_x.flatten(0, 1), mlp_input)\n                output_x = output_x.view(batch_size, num_views, *output_x.shape[-3:])\n                depth_digit = output_x[:, :, :self.D, ...]\n                depth = depth_digit.softmax(dim=2)\n                feat = output_x[:, :, self.D:self.D + self.in_channels, ...]\n            else:\n                with torch.no_grad():\n                    output_x = self.depth_net(input_x.flatten(0, 1), mlp_input)\n                    output_x = output_x.view(batch_size, num_views, *output_x.shape[-3:])\n                    depth_digit = output_x[:, :, :self.D, ...]\n                    depth = depth_digit.softmax(dim=2)\n                    feat = output_x[:, :, self.D:self.D + self.in_channels, ...]\n            if not self.sweep_lidar:\n                keyFrame = False\n            depth_list += [depth]\n            feat_list += [feat]\n        depth = torch.cat(depth_list, dim=1)\n        feat = torch.cat(feat_list, dim=1)\n        depth_all = depth\n        if self.with_fde:\n            depth_all = self.fde(depth_all, feat.flatten(0, 1)).view(depth_all.size())\n        if self.depth_thresh > 0.0:\n            d_mask = depth_all.max(dim=2, keepdim=True).values\n            d_mask = torch.where(d_mask > self.depth_thresh, torch.ones_like(d_mask), torch.zeros_like(d_mask))\n            d_mask = d_mask.repeat(1, 1, depth_all.shape[2], 1, 1)\n            depth_all = torch.mul(depth_all, d_mask)\n        (de_d, de_h, de_w) = depth_all.shape[-3:]\n        (ft_h, ft_w) = feat.shape[-2:]\n        if de_h != ft_h or de_w != ft_w:\n            depth_all = F.interpolate(depth_all, size=(de_d, ft_h, ft_w))\n        if self.depth_pe:\n            depth_enc = self.depth_encoder(depth_all.flatten(0, 1))\n    masks = F.interpolate(masks, size=x.shape[-2:]).to(torch.bool)\n    if self.with_position:\n        (coords_position_embeding, _) = self.position_embeding(mlvl_feats, img_metas, masks)\n        if self.with_fpe:\n            coords_position_embeding = self.fpe(coords_position_embeding.flatten(0, 1), feat.flatten(0, 1)).view(feat.size())\n        pos_embed = coords_position_embeding\n        if self.depth_pe:\n            pos_embed += depth_enc.view(feat.size())\n        if self.with_multiview:\n            sin_embed = self.positional_encoding(masks)\n            sin_embed = self.adapt_pos3d(sin_embed.flatten(0, 1)).view(feat.size())\n            pos_embed = pos_embed + sin_embed\n        else:\n            pos_embeds = []\n            for i in range(num_cams):\n                xy_embed = self.positional_encoding(masks[:, i, :, :])\n                pos_embeds.append(xy_embed.unsqueeze(1))\n            sin_embed = torch.cat(pos_embeds, 1)\n            sin_embed = self.adapt_pos3d(sin_embed.flatten(0, 1)).view(feat.size())\n            pos_embed = pos_embed + sin_embed\n    elif self.with_multiview:\n        pos_embed = self.positional_encoding(masks)\n        pos_embed = self.adapt_pos3d(pos_embed.flatten(0, 1)).view(feat.size())\n    else:\n        pos_embeds = []\n        for i in range(num_cams):\n            pos_embed = self.positional_encoding(masks[:, i, :, :])\n            pos_embeds.append(pos_embed.unsqueeze(1))\n        pos_embed = torch.cat(pos_embeds, 1)\n    reference_points = self.reference_points.weight\n    (reference_points, attn_mask, mask_dict) = self.prepare_for_dn(batch_size, reference_points, img_metas)\n    query_embeds = self.query_embedding(pos2posemb3d(reference_points))\n    (outs_dec, _) = self.transformer(feat, masks, query_embeds, pos_embed, attn_mask, self.reg_branches)\n    if self.with_time:\n        time_stamps = []\n        for img_meta in img_metas:\n            time_stamps.append(np.asarray(img_meta['timestamp'][:12]))\n        time_stamp = x.new_tensor(np.array(time_stamps))\n        time_stamp = time_stamp.view(batch_size, -1, 6)\n        mean_time_stamp = (time_stamp[:, 1, :] - time_stamp[:, 0, :]).mean(-1)\n    outputs_classes = []\n    outputs_coords = []\n    for lvl in range(outs_dec.shape[0]):\n        reference = inverse_sigmoid(reference_points.clone())\n        assert reference.shape[-1] == 3\n        outputs_class = self.cls_branches[lvl](outs_dec[lvl])\n        tmp = self.reg_branches[lvl](outs_dec[lvl])\n        tmp[..., 0:2] += reference[..., 0:2]\n        tmp[..., 0:2] = tmp[..., 0:2].sigmoid()\n        tmp[..., 4:5] += reference[..., 2:3]\n        tmp[..., 4:5] = tmp[..., 4:5].sigmoid()\n        if self.with_time:\n            tmp[..., 8:] = tmp[..., 8:] / mean_time_stamp[:, None, None]\n        outputs_coord = tmp\n        outputs_classes.append(outputs_class)\n        outputs_coords.append(outputs_coord)\n    all_cls_scores = torch.stack(outputs_classes)\n    all_bbox_preds = torch.stack(outputs_coords)\n    tmp = all_bbox_preds[..., 0:1] * (self.pc_range[3] - self.pc_range[0])\n    all_bbox_preds[..., 0:1] = tmp + self.pc_range[0]\n    tmp = all_bbox_preds[..., 1:2] * (self.pc_range[4] - self.pc_range[1])\n    all_bbox_preds[..., 1:2] = tmp + self.pc_range[1]\n    tmp = all_bbox_preds[..., 4:5] * (self.pc_range[5] - self.pc_range[2])\n    all_bbox_preds[..., 4:5] = tmp + self.pc_range[2]\n    if mask_dict and mask_dict['pad_size'] > 0:\n        output_known_class = all_cls_scores[:, :, :mask_dict['pad_size'], :]\n        output_known_coord = all_bbox_preds[:, :, :mask_dict['pad_size'], :]\n        outputs_class = all_cls_scores[:, :, mask_dict['pad_size']:, :]\n        outputs_coord = all_bbox_preds[:, :, mask_dict['pad_size']:, :]\n        mask_dict['output_known_lbs_bboxes'] = (output_known_class, output_known_coord)\n        outs = {'all_cls_scores': outputs_class, 'all_bbox_preds': outputs_coord, 'enc_cls_scores': None, 'enc_bbox_preds': None, 'dn_mask_dict': mask_dict, 'depth_pred': depth}\n    else:\n        outs = {'all_cls_scores': all_cls_scores, 'all_bbox_preds': all_bbox_preds, 'enc_cls_scores': None, 'enc_bbox_preds': None, 'dn_mask_dict': None, 'depth_pred': depth}\n    return outs"
        ]
    },
    {
        "func_name": "get_downsampled_gt_depth",
        "original": "def get_downsampled_gt_depth(self, gt_depths):\n    \"\"\"\n        Input:\n            gt_depths: [B, N, H, W]\n        Output:\n            gt_depths: [B*N*h*w, d]\n        \"\"\"\n    downsample = self.depth_downsample\n    depth_config = [self.depth_start, self.position_range[3], self.de_intv]\n    (B, N, H, W) = gt_depths.shape\n    gt_depths = gt_depths.view(B * N, H // downsample, downsample, W // downsample, downsample, 1)\n    gt_depths = gt_depths.permute(0, 1, 3, 5, 2, 4).contiguous()\n    gt_depths = gt_depths.view(-1, downsample * downsample)\n    gt_depths_tmp = torch.where(gt_depths == 0.0, 100000.0 * torch.ones_like(gt_depths), gt_depths)\n    gt_depths = torch.min(gt_depths_tmp, dim=-1).values\n    gt_depths = gt_depths.view(B * N, H // downsample, W // downsample)\n    if self.depth_LID:\n        bin_size = (self.position_range[3] - self.depth_start) / (self.D * (1 + self.D))\n        t = (gt_depths - self.depth_start) / bin_size\n        gt_depths = (torch.sqrt(1 + 4 * t) - 1) / 2.0 + 1\n    else:\n        tmp = gt_depths - (depth_config[0] - depth_config[2])\n        gt_depths = tmp / depth_config[2]\n    gt_depths = torch.where((gt_depths < self.D + 1) & (gt_depths >= 0.0), gt_depths, torch.zeros_like(gt_depths))\n    gt_depths = F.one_hot(gt_depths.long(), num_classes=self.D + 1).view(-1, self.D + 1)[:, 1:]\n    return gt_depths.float()",
        "mutated": [
            "def get_downsampled_gt_depth(self, gt_depths):\n    if False:\n        i = 10\n    '\\n        Input:\\n            gt_depths: [B, N, H, W]\\n        Output:\\n            gt_depths: [B*N*h*w, d]\\n        '\n    downsample = self.depth_downsample\n    depth_config = [self.depth_start, self.position_range[3], self.de_intv]\n    (B, N, H, W) = gt_depths.shape\n    gt_depths = gt_depths.view(B * N, H // downsample, downsample, W // downsample, downsample, 1)\n    gt_depths = gt_depths.permute(0, 1, 3, 5, 2, 4).contiguous()\n    gt_depths = gt_depths.view(-1, downsample * downsample)\n    gt_depths_tmp = torch.where(gt_depths == 0.0, 100000.0 * torch.ones_like(gt_depths), gt_depths)\n    gt_depths = torch.min(gt_depths_tmp, dim=-1).values\n    gt_depths = gt_depths.view(B * N, H // downsample, W // downsample)\n    if self.depth_LID:\n        bin_size = (self.position_range[3] - self.depth_start) / (self.D * (1 + self.D))\n        t = (gt_depths - self.depth_start) / bin_size\n        gt_depths = (torch.sqrt(1 + 4 * t) - 1) / 2.0 + 1\n    else:\n        tmp = gt_depths - (depth_config[0] - depth_config[2])\n        gt_depths = tmp / depth_config[2]\n    gt_depths = torch.where((gt_depths < self.D + 1) & (gt_depths >= 0.0), gt_depths, torch.zeros_like(gt_depths))\n    gt_depths = F.one_hot(gt_depths.long(), num_classes=self.D + 1).view(-1, self.D + 1)[:, 1:]\n    return gt_depths.float()",
            "def get_downsampled_gt_depth(self, gt_depths):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Input:\\n            gt_depths: [B, N, H, W]\\n        Output:\\n            gt_depths: [B*N*h*w, d]\\n        '\n    downsample = self.depth_downsample\n    depth_config = [self.depth_start, self.position_range[3], self.de_intv]\n    (B, N, H, W) = gt_depths.shape\n    gt_depths = gt_depths.view(B * N, H // downsample, downsample, W // downsample, downsample, 1)\n    gt_depths = gt_depths.permute(0, 1, 3, 5, 2, 4).contiguous()\n    gt_depths = gt_depths.view(-1, downsample * downsample)\n    gt_depths_tmp = torch.where(gt_depths == 0.0, 100000.0 * torch.ones_like(gt_depths), gt_depths)\n    gt_depths = torch.min(gt_depths_tmp, dim=-1).values\n    gt_depths = gt_depths.view(B * N, H // downsample, W // downsample)\n    if self.depth_LID:\n        bin_size = (self.position_range[3] - self.depth_start) / (self.D * (1 + self.D))\n        t = (gt_depths - self.depth_start) / bin_size\n        gt_depths = (torch.sqrt(1 + 4 * t) - 1) / 2.0 + 1\n    else:\n        tmp = gt_depths - (depth_config[0] - depth_config[2])\n        gt_depths = tmp / depth_config[2]\n    gt_depths = torch.where((gt_depths < self.D + 1) & (gt_depths >= 0.0), gt_depths, torch.zeros_like(gt_depths))\n    gt_depths = F.one_hot(gt_depths.long(), num_classes=self.D + 1).view(-1, self.D + 1)[:, 1:]\n    return gt_depths.float()",
            "def get_downsampled_gt_depth(self, gt_depths):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Input:\\n            gt_depths: [B, N, H, W]\\n        Output:\\n            gt_depths: [B*N*h*w, d]\\n        '\n    downsample = self.depth_downsample\n    depth_config = [self.depth_start, self.position_range[3], self.de_intv]\n    (B, N, H, W) = gt_depths.shape\n    gt_depths = gt_depths.view(B * N, H // downsample, downsample, W // downsample, downsample, 1)\n    gt_depths = gt_depths.permute(0, 1, 3, 5, 2, 4).contiguous()\n    gt_depths = gt_depths.view(-1, downsample * downsample)\n    gt_depths_tmp = torch.where(gt_depths == 0.0, 100000.0 * torch.ones_like(gt_depths), gt_depths)\n    gt_depths = torch.min(gt_depths_tmp, dim=-1).values\n    gt_depths = gt_depths.view(B * N, H // downsample, W // downsample)\n    if self.depth_LID:\n        bin_size = (self.position_range[3] - self.depth_start) / (self.D * (1 + self.D))\n        t = (gt_depths - self.depth_start) / bin_size\n        gt_depths = (torch.sqrt(1 + 4 * t) - 1) / 2.0 + 1\n    else:\n        tmp = gt_depths - (depth_config[0] - depth_config[2])\n        gt_depths = tmp / depth_config[2]\n    gt_depths = torch.where((gt_depths < self.D + 1) & (gt_depths >= 0.0), gt_depths, torch.zeros_like(gt_depths))\n    gt_depths = F.one_hot(gt_depths.long(), num_classes=self.D + 1).view(-1, self.D + 1)[:, 1:]\n    return gt_depths.float()",
            "def get_downsampled_gt_depth(self, gt_depths):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Input:\\n            gt_depths: [B, N, H, W]\\n        Output:\\n            gt_depths: [B*N*h*w, d]\\n        '\n    downsample = self.depth_downsample\n    depth_config = [self.depth_start, self.position_range[3], self.de_intv]\n    (B, N, H, W) = gt_depths.shape\n    gt_depths = gt_depths.view(B * N, H // downsample, downsample, W // downsample, downsample, 1)\n    gt_depths = gt_depths.permute(0, 1, 3, 5, 2, 4).contiguous()\n    gt_depths = gt_depths.view(-1, downsample * downsample)\n    gt_depths_tmp = torch.where(gt_depths == 0.0, 100000.0 * torch.ones_like(gt_depths), gt_depths)\n    gt_depths = torch.min(gt_depths_tmp, dim=-1).values\n    gt_depths = gt_depths.view(B * N, H // downsample, W // downsample)\n    if self.depth_LID:\n        bin_size = (self.position_range[3] - self.depth_start) / (self.D * (1 + self.D))\n        t = (gt_depths - self.depth_start) / bin_size\n        gt_depths = (torch.sqrt(1 + 4 * t) - 1) / 2.0 + 1\n    else:\n        tmp = gt_depths - (depth_config[0] - depth_config[2])\n        gt_depths = tmp / depth_config[2]\n    gt_depths = torch.where((gt_depths < self.D + 1) & (gt_depths >= 0.0), gt_depths, torch.zeros_like(gt_depths))\n    gt_depths = F.one_hot(gt_depths.long(), num_classes=self.D + 1).view(-1, self.D + 1)[:, 1:]\n    return gt_depths.float()",
            "def get_downsampled_gt_depth(self, gt_depths):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Input:\\n            gt_depths: [B, N, H, W]\\n        Output:\\n            gt_depths: [B*N*h*w, d]\\n        '\n    downsample = self.depth_downsample\n    depth_config = [self.depth_start, self.position_range[3], self.de_intv]\n    (B, N, H, W) = gt_depths.shape\n    gt_depths = gt_depths.view(B * N, H // downsample, downsample, W // downsample, downsample, 1)\n    gt_depths = gt_depths.permute(0, 1, 3, 5, 2, 4).contiguous()\n    gt_depths = gt_depths.view(-1, downsample * downsample)\n    gt_depths_tmp = torch.where(gt_depths == 0.0, 100000.0 * torch.ones_like(gt_depths), gt_depths)\n    gt_depths = torch.min(gt_depths_tmp, dim=-1).values\n    gt_depths = gt_depths.view(B * N, H // downsample, W // downsample)\n    if self.depth_LID:\n        bin_size = (self.position_range[3] - self.depth_start) / (self.D * (1 + self.D))\n        t = (gt_depths - self.depth_start) / bin_size\n        gt_depths = (torch.sqrt(1 + 4 * t) - 1) / 2.0 + 1\n    else:\n        tmp = gt_depths - (depth_config[0] - depth_config[2])\n        gt_depths = tmp / depth_config[2]\n    gt_depths = torch.where((gt_depths < self.D + 1) & (gt_depths >= 0.0), gt_depths, torch.zeros_like(gt_depths))\n    gt_depths = F.one_hot(gt_depths.long(), num_classes=self.D + 1).view(-1, self.D + 1)[:, 1:]\n    return gt_depths.float()"
        ]
    },
    {
        "func_name": "get_depth_loss",
        "original": "@force_fp32()\ndef get_depth_loss(self, gt_depth, pred_depth):\n    depth_labels = self.get_downsampled_gt_depth(gt_depth)\n    depth_preds = pred_depth.permute(0, 2, 3, 1).contiguous().view(-1, self.D)\n    fg_mask = torch.max(depth_labels, dim=1).values > 0.0\n    depth_labels = depth_labels[fg_mask]\n    depth_preds = depth_preds[fg_mask]\n    with autocast(enabled=False):\n        depth_loss = F.binary_cross_entropy(depth_preds, depth_labels, reduction='none').sum() / max(1.0, fg_mask.sum())\n    return self.loss_depth_weight * depth_loss",
        "mutated": [
            "@force_fp32()\ndef get_depth_loss(self, gt_depth, pred_depth):\n    if False:\n        i = 10\n    depth_labels = self.get_downsampled_gt_depth(gt_depth)\n    depth_preds = pred_depth.permute(0, 2, 3, 1).contiguous().view(-1, self.D)\n    fg_mask = torch.max(depth_labels, dim=1).values > 0.0\n    depth_labels = depth_labels[fg_mask]\n    depth_preds = depth_preds[fg_mask]\n    with autocast(enabled=False):\n        depth_loss = F.binary_cross_entropy(depth_preds, depth_labels, reduction='none').sum() / max(1.0, fg_mask.sum())\n    return self.loss_depth_weight * depth_loss",
            "@force_fp32()\ndef get_depth_loss(self, gt_depth, pred_depth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    depth_labels = self.get_downsampled_gt_depth(gt_depth)\n    depth_preds = pred_depth.permute(0, 2, 3, 1).contiguous().view(-1, self.D)\n    fg_mask = torch.max(depth_labels, dim=1).values > 0.0\n    depth_labels = depth_labels[fg_mask]\n    depth_preds = depth_preds[fg_mask]\n    with autocast(enabled=False):\n        depth_loss = F.binary_cross_entropy(depth_preds, depth_labels, reduction='none').sum() / max(1.0, fg_mask.sum())\n    return self.loss_depth_weight * depth_loss",
            "@force_fp32()\ndef get_depth_loss(self, gt_depth, pred_depth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    depth_labels = self.get_downsampled_gt_depth(gt_depth)\n    depth_preds = pred_depth.permute(0, 2, 3, 1).contiguous().view(-1, self.D)\n    fg_mask = torch.max(depth_labels, dim=1).values > 0.0\n    depth_labels = depth_labels[fg_mask]\n    depth_preds = depth_preds[fg_mask]\n    with autocast(enabled=False):\n        depth_loss = F.binary_cross_entropy(depth_preds, depth_labels, reduction='none').sum() / max(1.0, fg_mask.sum())\n    return self.loss_depth_weight * depth_loss",
            "@force_fp32()\ndef get_depth_loss(self, gt_depth, pred_depth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    depth_labels = self.get_downsampled_gt_depth(gt_depth)\n    depth_preds = pred_depth.permute(0, 2, 3, 1).contiguous().view(-1, self.D)\n    fg_mask = torch.max(depth_labels, dim=1).values > 0.0\n    depth_labels = depth_labels[fg_mask]\n    depth_preds = depth_preds[fg_mask]\n    with autocast(enabled=False):\n        depth_loss = F.binary_cross_entropy(depth_preds, depth_labels, reduction='none').sum() / max(1.0, fg_mask.sum())\n    return self.loss_depth_weight * depth_loss",
            "@force_fp32()\ndef get_depth_loss(self, gt_depth, pred_depth):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    depth_labels = self.get_downsampled_gt_depth(gt_depth)\n    depth_preds = pred_depth.permute(0, 2, 3, 1).contiguous().view(-1, self.D)\n    fg_mask = torch.max(depth_labels, dim=1).values > 0.0\n    depth_labels = depth_labels[fg_mask]\n    depth_preds = depth_preds[fg_mask]\n    with autocast(enabled=False):\n        depth_loss = F.binary_cross_entropy(depth_preds, depth_labels, reduction='none').sum() / max(1.0, fg_mask.sum())\n    return self.loss_depth_weight * depth_loss"
        ]
    },
    {
        "func_name": "prepare_for_loss",
        "original": "def prepare_for_loss(self, mask_dict):\n    \"\"\"\n        prepare dn components to calculate loss\n        Args:\n            mask_dict: a dict that contains dn information\n        \"\"\"\n    (output_known_class, output_known_coord) = mask_dict['output_known_lbs_bboxes']\n    (known_labels, known_bboxs) = mask_dict['known_lbs_bboxes']\n    map_known_indice = mask_dict['map_known_indice'].long()\n    known_indice = mask_dict['known_indice'].long()\n    batch_idx = mask_dict['batch_idx'].long()\n    bid = batch_idx[known_indice]\n    if len(output_known_class) > 0:\n        output_known_class = output_known_class.permute(1, 2, 0, 3)[bid, map_known_indice].permute(1, 0, 2)\n        output_known_coord = output_known_coord.permute(1, 2, 0, 3)[bid, map_known_indice].permute(1, 0, 2)\n    num_tgt = known_indice.numel()\n    return (known_labels, known_bboxs, output_known_class, output_known_coord, num_tgt)",
        "mutated": [
            "def prepare_for_loss(self, mask_dict):\n    if False:\n        i = 10\n    '\\n        prepare dn components to calculate loss\\n        Args:\\n            mask_dict: a dict that contains dn information\\n        '\n    (output_known_class, output_known_coord) = mask_dict['output_known_lbs_bboxes']\n    (known_labels, known_bboxs) = mask_dict['known_lbs_bboxes']\n    map_known_indice = mask_dict['map_known_indice'].long()\n    known_indice = mask_dict['known_indice'].long()\n    batch_idx = mask_dict['batch_idx'].long()\n    bid = batch_idx[known_indice]\n    if len(output_known_class) > 0:\n        output_known_class = output_known_class.permute(1, 2, 0, 3)[bid, map_known_indice].permute(1, 0, 2)\n        output_known_coord = output_known_coord.permute(1, 2, 0, 3)[bid, map_known_indice].permute(1, 0, 2)\n    num_tgt = known_indice.numel()\n    return (known_labels, known_bboxs, output_known_class, output_known_coord, num_tgt)",
            "def prepare_for_loss(self, mask_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        prepare dn components to calculate loss\\n        Args:\\n            mask_dict: a dict that contains dn information\\n        '\n    (output_known_class, output_known_coord) = mask_dict['output_known_lbs_bboxes']\n    (known_labels, known_bboxs) = mask_dict['known_lbs_bboxes']\n    map_known_indice = mask_dict['map_known_indice'].long()\n    known_indice = mask_dict['known_indice'].long()\n    batch_idx = mask_dict['batch_idx'].long()\n    bid = batch_idx[known_indice]\n    if len(output_known_class) > 0:\n        output_known_class = output_known_class.permute(1, 2, 0, 3)[bid, map_known_indice].permute(1, 0, 2)\n        output_known_coord = output_known_coord.permute(1, 2, 0, 3)[bid, map_known_indice].permute(1, 0, 2)\n    num_tgt = known_indice.numel()\n    return (known_labels, known_bboxs, output_known_class, output_known_coord, num_tgt)",
            "def prepare_for_loss(self, mask_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        prepare dn components to calculate loss\\n        Args:\\n            mask_dict: a dict that contains dn information\\n        '\n    (output_known_class, output_known_coord) = mask_dict['output_known_lbs_bboxes']\n    (known_labels, known_bboxs) = mask_dict['known_lbs_bboxes']\n    map_known_indice = mask_dict['map_known_indice'].long()\n    known_indice = mask_dict['known_indice'].long()\n    batch_idx = mask_dict['batch_idx'].long()\n    bid = batch_idx[known_indice]\n    if len(output_known_class) > 0:\n        output_known_class = output_known_class.permute(1, 2, 0, 3)[bid, map_known_indice].permute(1, 0, 2)\n        output_known_coord = output_known_coord.permute(1, 2, 0, 3)[bid, map_known_indice].permute(1, 0, 2)\n    num_tgt = known_indice.numel()\n    return (known_labels, known_bboxs, output_known_class, output_known_coord, num_tgt)",
            "def prepare_for_loss(self, mask_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        prepare dn components to calculate loss\\n        Args:\\n            mask_dict: a dict that contains dn information\\n        '\n    (output_known_class, output_known_coord) = mask_dict['output_known_lbs_bboxes']\n    (known_labels, known_bboxs) = mask_dict['known_lbs_bboxes']\n    map_known_indice = mask_dict['map_known_indice'].long()\n    known_indice = mask_dict['known_indice'].long()\n    batch_idx = mask_dict['batch_idx'].long()\n    bid = batch_idx[known_indice]\n    if len(output_known_class) > 0:\n        output_known_class = output_known_class.permute(1, 2, 0, 3)[bid, map_known_indice].permute(1, 0, 2)\n        output_known_coord = output_known_coord.permute(1, 2, 0, 3)[bid, map_known_indice].permute(1, 0, 2)\n    num_tgt = known_indice.numel()\n    return (known_labels, known_bboxs, output_known_class, output_known_coord, num_tgt)",
            "def prepare_for_loss(self, mask_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        prepare dn components to calculate loss\\n        Args:\\n            mask_dict: a dict that contains dn information\\n        '\n    (output_known_class, output_known_coord) = mask_dict['output_known_lbs_bboxes']\n    (known_labels, known_bboxs) = mask_dict['known_lbs_bboxes']\n    map_known_indice = mask_dict['map_known_indice'].long()\n    known_indice = mask_dict['known_indice'].long()\n    batch_idx = mask_dict['batch_idx'].long()\n    bid = batch_idx[known_indice]\n    if len(output_known_class) > 0:\n        output_known_class = output_known_class.permute(1, 2, 0, 3)[bid, map_known_indice].permute(1, 0, 2)\n        output_known_coord = output_known_coord.permute(1, 2, 0, 3)[bid, map_known_indice].permute(1, 0, 2)\n    num_tgt = known_indice.numel()\n    return (known_labels, known_bboxs, output_known_class, output_known_coord, num_tgt)"
        ]
    },
    {
        "func_name": "_get_target_single",
        "original": "def _get_target_single(self, cls_score, bbox_pred, gt_labels, gt_bboxes, gt_bboxes_ignore=None):\n    \"\"\"\"Compute regression and classification targets for one image.\n        Outputs from a single decoder layer of a single feature level are used.\n        Args:\n            cls_score (Tensor): Box score logits from a single decoder layer\n                for one image. Shape [num_query, cls_out_channels].\n            bbox_pred (Tensor): Sigmoid outputs from a single decoder layer\n                for one image, with normalized coordinate (cx, cy, w, h) and\n                shape [num_query, 4].\n            gt_bboxes (Tensor): Ground truth bboxes for one image with\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels (Tensor): Ground truth class indices for one image\n                with shape (num_gts, ).\n            gt_bboxes_ignore (Tensor, optional): Bounding boxes\n                which can be ignored. Default None.\n        Returns:\n            tuple[Tensor]: a tuple containing the following for one image.\n                - labels (Tensor): Labels of each image.\n                - label_weights (Tensor]): Label weights of each image.\n                - bbox_targets (Tensor): BBox targets of each image.\n                - bbox_weights (Tensor): BBox weights of each image.\n                - pos_inds (Tensor): Sampled positive indices for each image.\n                - neg_inds (Tensor): Sampled negative indices for each image.\n        \"\"\"\n    num_bboxes = bbox_pred.size(0)\n    assign_result = self.assigner.assign(bbox_pred, cls_score, gt_bboxes, gt_labels, gt_bboxes_ignore)\n    sampling_result = self.sampler.sample(assign_result, bbox_pred, gt_bboxes)\n    pos_inds = sampling_result.pos_inds\n    neg_inds = sampling_result.neg_inds\n    labels = gt_bboxes.new_full((num_bboxes,), self.num_classes, dtype=torch.long)\n    labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]\n    label_weights = gt_bboxes.new_ones(num_bboxes)\n    code_size = gt_bboxes.size(1)\n    bbox_targets = torch.zeros_like(bbox_pred)[..., :code_size]\n    bbox_weights = torch.zeros_like(bbox_pred)\n    bbox_weights[pos_inds] = 1.0\n    if sampling_result.pos_gt_bboxes.shape[1] == 4:\n        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes.reshape(sampling_result.pos_gt_bboxes.shape[0], self.code_size - 1)\n    else:\n        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes\n    return (labels, label_weights, bbox_targets, bbox_weights, pos_inds, neg_inds)",
        "mutated": [
            "def _get_target_single(self, cls_score, bbox_pred, gt_labels, gt_bboxes, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n    '\"Compute regression and classification targets for one image.\\n        Outputs from a single decoder layer of a single feature level are used.\\n        Args:\\n            cls_score (Tensor): Box score logits from a single decoder layer\\n                for one image. Shape [num_query, cls_out_channels].\\n            bbox_pred (Tensor): Sigmoid outputs from a single decoder layer\\n                for one image, with normalized coordinate (cx, cy, w, h) and\\n                shape [num_query, 4].\\n            gt_bboxes (Tensor): Ground truth bboxes for one image with\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (Tensor): Ground truth class indices for one image\\n                with shape (num_gts, ).\\n            gt_bboxes_ignore (Tensor, optional): Bounding boxes\\n                which can be ignored. Default None.\\n        Returns:\\n            tuple[Tensor]: a tuple containing the following for one image.\\n                - labels (Tensor): Labels of each image.\\n                - label_weights (Tensor]): Label weights of each image.\\n                - bbox_targets (Tensor): BBox targets of each image.\\n                - bbox_weights (Tensor): BBox weights of each image.\\n                - pos_inds (Tensor): Sampled positive indices for each image.\\n                - neg_inds (Tensor): Sampled negative indices for each image.\\n        '\n    num_bboxes = bbox_pred.size(0)\n    assign_result = self.assigner.assign(bbox_pred, cls_score, gt_bboxes, gt_labels, gt_bboxes_ignore)\n    sampling_result = self.sampler.sample(assign_result, bbox_pred, gt_bboxes)\n    pos_inds = sampling_result.pos_inds\n    neg_inds = sampling_result.neg_inds\n    labels = gt_bboxes.new_full((num_bboxes,), self.num_classes, dtype=torch.long)\n    labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]\n    label_weights = gt_bboxes.new_ones(num_bboxes)\n    code_size = gt_bboxes.size(1)\n    bbox_targets = torch.zeros_like(bbox_pred)[..., :code_size]\n    bbox_weights = torch.zeros_like(bbox_pred)\n    bbox_weights[pos_inds] = 1.0\n    if sampling_result.pos_gt_bboxes.shape[1] == 4:\n        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes.reshape(sampling_result.pos_gt_bboxes.shape[0], self.code_size - 1)\n    else:\n        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes\n    return (labels, label_weights, bbox_targets, bbox_weights, pos_inds, neg_inds)",
            "def _get_target_single(self, cls_score, bbox_pred, gt_labels, gt_bboxes, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\"Compute regression and classification targets for one image.\\n        Outputs from a single decoder layer of a single feature level are used.\\n        Args:\\n            cls_score (Tensor): Box score logits from a single decoder layer\\n                for one image. Shape [num_query, cls_out_channels].\\n            bbox_pred (Tensor): Sigmoid outputs from a single decoder layer\\n                for one image, with normalized coordinate (cx, cy, w, h) and\\n                shape [num_query, 4].\\n            gt_bboxes (Tensor): Ground truth bboxes for one image with\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (Tensor): Ground truth class indices for one image\\n                with shape (num_gts, ).\\n            gt_bboxes_ignore (Tensor, optional): Bounding boxes\\n                which can be ignored. Default None.\\n        Returns:\\n            tuple[Tensor]: a tuple containing the following for one image.\\n                - labels (Tensor): Labels of each image.\\n                - label_weights (Tensor]): Label weights of each image.\\n                - bbox_targets (Tensor): BBox targets of each image.\\n                - bbox_weights (Tensor): BBox weights of each image.\\n                - pos_inds (Tensor): Sampled positive indices for each image.\\n                - neg_inds (Tensor): Sampled negative indices for each image.\\n        '\n    num_bboxes = bbox_pred.size(0)\n    assign_result = self.assigner.assign(bbox_pred, cls_score, gt_bboxes, gt_labels, gt_bboxes_ignore)\n    sampling_result = self.sampler.sample(assign_result, bbox_pred, gt_bboxes)\n    pos_inds = sampling_result.pos_inds\n    neg_inds = sampling_result.neg_inds\n    labels = gt_bboxes.new_full((num_bboxes,), self.num_classes, dtype=torch.long)\n    labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]\n    label_weights = gt_bboxes.new_ones(num_bboxes)\n    code_size = gt_bboxes.size(1)\n    bbox_targets = torch.zeros_like(bbox_pred)[..., :code_size]\n    bbox_weights = torch.zeros_like(bbox_pred)\n    bbox_weights[pos_inds] = 1.0\n    if sampling_result.pos_gt_bboxes.shape[1] == 4:\n        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes.reshape(sampling_result.pos_gt_bboxes.shape[0], self.code_size - 1)\n    else:\n        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes\n    return (labels, label_weights, bbox_targets, bbox_weights, pos_inds, neg_inds)",
            "def _get_target_single(self, cls_score, bbox_pred, gt_labels, gt_bboxes, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\"Compute regression and classification targets for one image.\\n        Outputs from a single decoder layer of a single feature level are used.\\n        Args:\\n            cls_score (Tensor): Box score logits from a single decoder layer\\n                for one image. Shape [num_query, cls_out_channels].\\n            bbox_pred (Tensor): Sigmoid outputs from a single decoder layer\\n                for one image, with normalized coordinate (cx, cy, w, h) and\\n                shape [num_query, 4].\\n            gt_bboxes (Tensor): Ground truth bboxes for one image with\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (Tensor): Ground truth class indices for one image\\n                with shape (num_gts, ).\\n            gt_bboxes_ignore (Tensor, optional): Bounding boxes\\n                which can be ignored. Default None.\\n        Returns:\\n            tuple[Tensor]: a tuple containing the following for one image.\\n                - labels (Tensor): Labels of each image.\\n                - label_weights (Tensor]): Label weights of each image.\\n                - bbox_targets (Tensor): BBox targets of each image.\\n                - bbox_weights (Tensor): BBox weights of each image.\\n                - pos_inds (Tensor): Sampled positive indices for each image.\\n                - neg_inds (Tensor): Sampled negative indices for each image.\\n        '\n    num_bboxes = bbox_pred.size(0)\n    assign_result = self.assigner.assign(bbox_pred, cls_score, gt_bboxes, gt_labels, gt_bboxes_ignore)\n    sampling_result = self.sampler.sample(assign_result, bbox_pred, gt_bboxes)\n    pos_inds = sampling_result.pos_inds\n    neg_inds = sampling_result.neg_inds\n    labels = gt_bboxes.new_full((num_bboxes,), self.num_classes, dtype=torch.long)\n    labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]\n    label_weights = gt_bboxes.new_ones(num_bboxes)\n    code_size = gt_bboxes.size(1)\n    bbox_targets = torch.zeros_like(bbox_pred)[..., :code_size]\n    bbox_weights = torch.zeros_like(bbox_pred)\n    bbox_weights[pos_inds] = 1.0\n    if sampling_result.pos_gt_bboxes.shape[1] == 4:\n        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes.reshape(sampling_result.pos_gt_bboxes.shape[0], self.code_size - 1)\n    else:\n        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes\n    return (labels, label_weights, bbox_targets, bbox_weights, pos_inds, neg_inds)",
            "def _get_target_single(self, cls_score, bbox_pred, gt_labels, gt_bboxes, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\"Compute regression and classification targets for one image.\\n        Outputs from a single decoder layer of a single feature level are used.\\n        Args:\\n            cls_score (Tensor): Box score logits from a single decoder layer\\n                for one image. Shape [num_query, cls_out_channels].\\n            bbox_pred (Tensor): Sigmoid outputs from a single decoder layer\\n                for one image, with normalized coordinate (cx, cy, w, h) and\\n                shape [num_query, 4].\\n            gt_bboxes (Tensor): Ground truth bboxes for one image with\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (Tensor): Ground truth class indices for one image\\n                with shape (num_gts, ).\\n            gt_bboxes_ignore (Tensor, optional): Bounding boxes\\n                which can be ignored. Default None.\\n        Returns:\\n            tuple[Tensor]: a tuple containing the following for one image.\\n                - labels (Tensor): Labels of each image.\\n                - label_weights (Tensor]): Label weights of each image.\\n                - bbox_targets (Tensor): BBox targets of each image.\\n                - bbox_weights (Tensor): BBox weights of each image.\\n                - pos_inds (Tensor): Sampled positive indices for each image.\\n                - neg_inds (Tensor): Sampled negative indices for each image.\\n        '\n    num_bboxes = bbox_pred.size(0)\n    assign_result = self.assigner.assign(bbox_pred, cls_score, gt_bboxes, gt_labels, gt_bboxes_ignore)\n    sampling_result = self.sampler.sample(assign_result, bbox_pred, gt_bboxes)\n    pos_inds = sampling_result.pos_inds\n    neg_inds = sampling_result.neg_inds\n    labels = gt_bboxes.new_full((num_bboxes,), self.num_classes, dtype=torch.long)\n    labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]\n    label_weights = gt_bboxes.new_ones(num_bboxes)\n    code_size = gt_bboxes.size(1)\n    bbox_targets = torch.zeros_like(bbox_pred)[..., :code_size]\n    bbox_weights = torch.zeros_like(bbox_pred)\n    bbox_weights[pos_inds] = 1.0\n    if sampling_result.pos_gt_bboxes.shape[1] == 4:\n        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes.reshape(sampling_result.pos_gt_bboxes.shape[0], self.code_size - 1)\n    else:\n        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes\n    return (labels, label_weights, bbox_targets, bbox_weights, pos_inds, neg_inds)",
            "def _get_target_single(self, cls_score, bbox_pred, gt_labels, gt_bboxes, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\"Compute regression and classification targets for one image.\\n        Outputs from a single decoder layer of a single feature level are used.\\n        Args:\\n            cls_score (Tensor): Box score logits from a single decoder layer\\n                for one image. Shape [num_query, cls_out_channels].\\n            bbox_pred (Tensor): Sigmoid outputs from a single decoder layer\\n                for one image, with normalized coordinate (cx, cy, w, h) and\\n                shape [num_query, 4].\\n            gt_bboxes (Tensor): Ground truth bboxes for one image with\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (Tensor): Ground truth class indices for one image\\n                with shape (num_gts, ).\\n            gt_bboxes_ignore (Tensor, optional): Bounding boxes\\n                which can be ignored. Default None.\\n        Returns:\\n            tuple[Tensor]: a tuple containing the following for one image.\\n                - labels (Tensor): Labels of each image.\\n                - label_weights (Tensor]): Label weights of each image.\\n                - bbox_targets (Tensor): BBox targets of each image.\\n                - bbox_weights (Tensor): BBox weights of each image.\\n                - pos_inds (Tensor): Sampled positive indices for each image.\\n                - neg_inds (Tensor): Sampled negative indices for each image.\\n        '\n    num_bboxes = bbox_pred.size(0)\n    assign_result = self.assigner.assign(bbox_pred, cls_score, gt_bboxes, gt_labels, gt_bboxes_ignore)\n    sampling_result = self.sampler.sample(assign_result, bbox_pred, gt_bboxes)\n    pos_inds = sampling_result.pos_inds\n    neg_inds = sampling_result.neg_inds\n    labels = gt_bboxes.new_full((num_bboxes,), self.num_classes, dtype=torch.long)\n    labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]\n    label_weights = gt_bboxes.new_ones(num_bboxes)\n    code_size = gt_bboxes.size(1)\n    bbox_targets = torch.zeros_like(bbox_pred)[..., :code_size]\n    bbox_weights = torch.zeros_like(bbox_pred)\n    bbox_weights[pos_inds] = 1.0\n    if sampling_result.pos_gt_bboxes.shape[1] == 4:\n        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes.reshape(sampling_result.pos_gt_bboxes.shape[0], self.code_size - 1)\n    else:\n        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes\n    return (labels, label_weights, bbox_targets, bbox_weights, pos_inds, neg_inds)"
        ]
    },
    {
        "func_name": "get_targets",
        "original": "def get_targets(self, cls_scores_list, bbox_preds_list, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list=None):\n    \"\"\"\"Compute regression and classification targets for a batch image.\n        Outputs from a single decoder layer of a single feature level are used.\n        Args:\n            cls_scores_list (list[Tensor]): Box score logits from a single\n                decoder layer for each image with shape [num_query,\n                cls_out_channels].\n            bbox_preds_list (list[Tensor]): Sigmoid outputs from a single\n                decoder layer for each image, with normalized coordinate\n                (cx, cy, w, h) and shape [num_query, 4].\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\n                image with shape (num_gts, ).\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\n                boxes which can be ignored for each image. Default None.\n        Returns:\n            tuple: a tuple containing the following targets.\n                - labels_list (list[Tensor]): Labels for all images.\n                - label_weights_list (list[Tensor]): Label weights for all                     images.\n                - bbox_targets_list (list[Tensor]): BBox targets for all                     images.\n                - bbox_weights_list (list[Tensor]): BBox weights for all                     images.\n                - num_total_pos (int): Number of positive samples in all                     images.\n                - num_total_neg (int): Number of negative samples in all                     images.\n        \"\"\"\n    assert gt_bboxes_ignore_list is None, 'Only supports for gt_bboxes_ignore setting to None.'\n    num_imgs = len(cls_scores_list)\n    gt_bboxes_ignore_list = [gt_bboxes_ignore_list for _ in range(num_imgs)]\n    (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, pos_inds_list, neg_inds_list) = multi_apply(self._get_target_single, cls_scores_list, bbox_preds_list, gt_labels_list, gt_bboxes_list, gt_bboxes_ignore_list)\n    num_total_pos = sum((inds.numel() for inds in pos_inds_list))\n    num_total_neg = sum((inds.numel() for inds in neg_inds_list))\n    return (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, num_total_pos, num_total_neg)",
        "mutated": [
            "def get_targets(self, cls_scores_list, bbox_preds_list, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list=None):\n    if False:\n        i = 10\n    '\"Compute regression and classification targets for a batch image.\\n        Outputs from a single decoder layer of a single feature level are used.\\n        Args:\\n            cls_scores_list (list[Tensor]): Box score logits from a single\\n                decoder layer for each image with shape [num_query,\\n                cls_out_channels].\\n            bbox_preds_list (list[Tensor]): Sigmoid outputs from a single\\n                decoder layer for each image, with normalized coordinate\\n                (cx, cy, w, h) and shape [num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            tuple: a tuple containing the following targets.\\n                - labels_list (list[Tensor]): Labels for all images.\\n                - label_weights_list (list[Tensor]): Label weights for all                     images.\\n                - bbox_targets_list (list[Tensor]): BBox targets for all                     images.\\n                - bbox_weights_list (list[Tensor]): BBox weights for all                     images.\\n                - num_total_pos (int): Number of positive samples in all                     images.\\n                - num_total_neg (int): Number of negative samples in all                     images.\\n        '\n    assert gt_bboxes_ignore_list is None, 'Only supports for gt_bboxes_ignore setting to None.'\n    num_imgs = len(cls_scores_list)\n    gt_bboxes_ignore_list = [gt_bboxes_ignore_list for _ in range(num_imgs)]\n    (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, pos_inds_list, neg_inds_list) = multi_apply(self._get_target_single, cls_scores_list, bbox_preds_list, gt_labels_list, gt_bboxes_list, gt_bboxes_ignore_list)\n    num_total_pos = sum((inds.numel() for inds in pos_inds_list))\n    num_total_neg = sum((inds.numel() for inds in neg_inds_list))\n    return (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, num_total_pos, num_total_neg)",
            "def get_targets(self, cls_scores_list, bbox_preds_list, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\"Compute regression and classification targets for a batch image.\\n        Outputs from a single decoder layer of a single feature level are used.\\n        Args:\\n            cls_scores_list (list[Tensor]): Box score logits from a single\\n                decoder layer for each image with shape [num_query,\\n                cls_out_channels].\\n            bbox_preds_list (list[Tensor]): Sigmoid outputs from a single\\n                decoder layer for each image, with normalized coordinate\\n                (cx, cy, w, h) and shape [num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            tuple: a tuple containing the following targets.\\n                - labels_list (list[Tensor]): Labels for all images.\\n                - label_weights_list (list[Tensor]): Label weights for all                     images.\\n                - bbox_targets_list (list[Tensor]): BBox targets for all                     images.\\n                - bbox_weights_list (list[Tensor]): BBox weights for all                     images.\\n                - num_total_pos (int): Number of positive samples in all                     images.\\n                - num_total_neg (int): Number of negative samples in all                     images.\\n        '\n    assert gt_bboxes_ignore_list is None, 'Only supports for gt_bboxes_ignore setting to None.'\n    num_imgs = len(cls_scores_list)\n    gt_bboxes_ignore_list = [gt_bboxes_ignore_list for _ in range(num_imgs)]\n    (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, pos_inds_list, neg_inds_list) = multi_apply(self._get_target_single, cls_scores_list, bbox_preds_list, gt_labels_list, gt_bboxes_list, gt_bboxes_ignore_list)\n    num_total_pos = sum((inds.numel() for inds in pos_inds_list))\n    num_total_neg = sum((inds.numel() for inds in neg_inds_list))\n    return (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, num_total_pos, num_total_neg)",
            "def get_targets(self, cls_scores_list, bbox_preds_list, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\"Compute regression and classification targets for a batch image.\\n        Outputs from a single decoder layer of a single feature level are used.\\n        Args:\\n            cls_scores_list (list[Tensor]): Box score logits from a single\\n                decoder layer for each image with shape [num_query,\\n                cls_out_channels].\\n            bbox_preds_list (list[Tensor]): Sigmoid outputs from a single\\n                decoder layer for each image, with normalized coordinate\\n                (cx, cy, w, h) and shape [num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            tuple: a tuple containing the following targets.\\n                - labels_list (list[Tensor]): Labels for all images.\\n                - label_weights_list (list[Tensor]): Label weights for all                     images.\\n                - bbox_targets_list (list[Tensor]): BBox targets for all                     images.\\n                - bbox_weights_list (list[Tensor]): BBox weights for all                     images.\\n                - num_total_pos (int): Number of positive samples in all                     images.\\n                - num_total_neg (int): Number of negative samples in all                     images.\\n        '\n    assert gt_bboxes_ignore_list is None, 'Only supports for gt_bboxes_ignore setting to None.'\n    num_imgs = len(cls_scores_list)\n    gt_bboxes_ignore_list = [gt_bboxes_ignore_list for _ in range(num_imgs)]\n    (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, pos_inds_list, neg_inds_list) = multi_apply(self._get_target_single, cls_scores_list, bbox_preds_list, gt_labels_list, gt_bboxes_list, gt_bboxes_ignore_list)\n    num_total_pos = sum((inds.numel() for inds in pos_inds_list))\n    num_total_neg = sum((inds.numel() for inds in neg_inds_list))\n    return (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, num_total_pos, num_total_neg)",
            "def get_targets(self, cls_scores_list, bbox_preds_list, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\"Compute regression and classification targets for a batch image.\\n        Outputs from a single decoder layer of a single feature level are used.\\n        Args:\\n            cls_scores_list (list[Tensor]): Box score logits from a single\\n                decoder layer for each image with shape [num_query,\\n                cls_out_channels].\\n            bbox_preds_list (list[Tensor]): Sigmoid outputs from a single\\n                decoder layer for each image, with normalized coordinate\\n                (cx, cy, w, h) and shape [num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            tuple: a tuple containing the following targets.\\n                - labels_list (list[Tensor]): Labels for all images.\\n                - label_weights_list (list[Tensor]): Label weights for all                     images.\\n                - bbox_targets_list (list[Tensor]): BBox targets for all                     images.\\n                - bbox_weights_list (list[Tensor]): BBox weights for all                     images.\\n                - num_total_pos (int): Number of positive samples in all                     images.\\n                - num_total_neg (int): Number of negative samples in all                     images.\\n        '\n    assert gt_bboxes_ignore_list is None, 'Only supports for gt_bboxes_ignore setting to None.'\n    num_imgs = len(cls_scores_list)\n    gt_bboxes_ignore_list = [gt_bboxes_ignore_list for _ in range(num_imgs)]\n    (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, pos_inds_list, neg_inds_list) = multi_apply(self._get_target_single, cls_scores_list, bbox_preds_list, gt_labels_list, gt_bboxes_list, gt_bboxes_ignore_list)\n    num_total_pos = sum((inds.numel() for inds in pos_inds_list))\n    num_total_neg = sum((inds.numel() for inds in neg_inds_list))\n    return (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, num_total_pos, num_total_neg)",
            "def get_targets(self, cls_scores_list, bbox_preds_list, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\"Compute regression and classification targets for a batch image.\\n        Outputs from a single decoder layer of a single feature level are used.\\n        Args:\\n            cls_scores_list (list[Tensor]): Box score logits from a single\\n                decoder layer for each image with shape [num_query,\\n                cls_out_channels].\\n            bbox_preds_list (list[Tensor]): Sigmoid outputs from a single\\n                decoder layer for each image, with normalized coordinate\\n                (cx, cy, w, h) and shape [num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            tuple: a tuple containing the following targets.\\n                - labels_list (list[Tensor]): Labels for all images.\\n                - label_weights_list (list[Tensor]): Label weights for all                     images.\\n                - bbox_targets_list (list[Tensor]): BBox targets for all                     images.\\n                - bbox_weights_list (list[Tensor]): BBox weights for all                     images.\\n                - num_total_pos (int): Number of positive samples in all                     images.\\n                - num_total_neg (int): Number of negative samples in all                     images.\\n        '\n    assert gt_bboxes_ignore_list is None, 'Only supports for gt_bboxes_ignore setting to None.'\n    num_imgs = len(cls_scores_list)\n    gt_bboxes_ignore_list = [gt_bboxes_ignore_list for _ in range(num_imgs)]\n    (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, pos_inds_list, neg_inds_list) = multi_apply(self._get_target_single, cls_scores_list, bbox_preds_list, gt_labels_list, gt_bboxes_list, gt_bboxes_ignore_list)\n    num_total_pos = sum((inds.numel() for inds in pos_inds_list))\n    num_total_neg = sum((inds.numel() for inds in neg_inds_list))\n    return (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, num_total_pos, num_total_neg)"
        ]
    },
    {
        "func_name": "loss_single",
        "original": "def loss_single(self, cls_scores, bbox_preds, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list=None):\n    \"\"\"\"Loss function for outputs from a single decoder layer of a single\n        feature level.\n        Args:\n            cls_scores (Tensor): Box score logits from a single decoder layer\n                for all images. Shape [bs, num_query, cls_out_channels].\n            bbox_preds (Tensor): Sigmoid outputs from a single decoder layer\n                for all images, with normalized coordinate (cx, cy, w, h) and\n                shape [bs, num_query, 4].\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\n                image with shape (num_gts, ).\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\n                boxes which can be ignored for each image. Default None.\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components for outputs from\n                a single decoder layer.\n        \"\"\"\n    num_imgs = cls_scores.size(0)\n    cls_scores_list = [cls_scores[i] for i in range(num_imgs)]\n    bbox_preds_list = [bbox_preds[i] for i in range(num_imgs)]\n    cls_reg_targets = self.get_targets(cls_scores_list, bbox_preds_list, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list)\n    (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, num_total_pos, num_total_neg) = cls_reg_targets\n    labels = torch.cat(labels_list, 0)\n    label_weights = torch.cat(label_weights_list, 0)\n    bbox_targets = torch.cat(bbox_targets_list, 0)\n    bbox_weights = torch.cat(bbox_weights_list, 0)\n    cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n    cls_avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_cls_weight\n    if self.sync_cls_avg_factor:\n        cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n    cls_avg_factor = max(cls_avg_factor, 1)\n    loss_cls = self.loss_cls(cls_scores, labels, label_weights, avg_factor=cls_avg_factor)\n    num_total_pos = loss_cls.new_tensor([num_total_pos])\n    num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n    bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n    normalized_bbox_targets = normalize_bbox(bbox_targets, self.pc_range)\n    isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n    bbox_weights = bbox_weights * self.code_weights\n    loss_bbox = self.loss_bbox(bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan, :10], bbox_weights[isnotnan, :10], avg_factor=num_total_pos)\n    loss_cls = torch.nan_to_num(loss_cls)\n    loss_bbox = torch.nan_to_num(loss_bbox)\n    return (loss_cls, loss_bbox)",
        "mutated": [
            "def loss_single(self, cls_scores, bbox_preds, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list=None):\n    if False:\n        i = 10\n    '\"Loss function for outputs from a single decoder layer of a single\\n        feature level.\\n        Args:\\n            cls_scores (Tensor): Box score logits from a single decoder layer\\n                for all images. Shape [bs, num_query, cls_out_channels].\\n            bbox_preds (Tensor): Sigmoid outputs from a single decoder layer\\n                for all images, with normalized coordinate (cx, cy, w, h) and\\n                shape [bs, num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components for outputs from\\n                a single decoder layer.\\n        '\n    num_imgs = cls_scores.size(0)\n    cls_scores_list = [cls_scores[i] for i in range(num_imgs)]\n    bbox_preds_list = [bbox_preds[i] for i in range(num_imgs)]\n    cls_reg_targets = self.get_targets(cls_scores_list, bbox_preds_list, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list)\n    (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, num_total_pos, num_total_neg) = cls_reg_targets\n    labels = torch.cat(labels_list, 0)\n    label_weights = torch.cat(label_weights_list, 0)\n    bbox_targets = torch.cat(bbox_targets_list, 0)\n    bbox_weights = torch.cat(bbox_weights_list, 0)\n    cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n    cls_avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_cls_weight\n    if self.sync_cls_avg_factor:\n        cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n    cls_avg_factor = max(cls_avg_factor, 1)\n    loss_cls = self.loss_cls(cls_scores, labels, label_weights, avg_factor=cls_avg_factor)\n    num_total_pos = loss_cls.new_tensor([num_total_pos])\n    num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n    bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n    normalized_bbox_targets = normalize_bbox(bbox_targets, self.pc_range)\n    isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n    bbox_weights = bbox_weights * self.code_weights\n    loss_bbox = self.loss_bbox(bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan, :10], bbox_weights[isnotnan, :10], avg_factor=num_total_pos)\n    loss_cls = torch.nan_to_num(loss_cls)\n    loss_bbox = torch.nan_to_num(loss_bbox)\n    return (loss_cls, loss_bbox)",
            "def loss_single(self, cls_scores, bbox_preds, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\"Loss function for outputs from a single decoder layer of a single\\n        feature level.\\n        Args:\\n            cls_scores (Tensor): Box score logits from a single decoder layer\\n                for all images. Shape [bs, num_query, cls_out_channels].\\n            bbox_preds (Tensor): Sigmoid outputs from a single decoder layer\\n                for all images, with normalized coordinate (cx, cy, w, h) and\\n                shape [bs, num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components for outputs from\\n                a single decoder layer.\\n        '\n    num_imgs = cls_scores.size(0)\n    cls_scores_list = [cls_scores[i] for i in range(num_imgs)]\n    bbox_preds_list = [bbox_preds[i] for i in range(num_imgs)]\n    cls_reg_targets = self.get_targets(cls_scores_list, bbox_preds_list, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list)\n    (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, num_total_pos, num_total_neg) = cls_reg_targets\n    labels = torch.cat(labels_list, 0)\n    label_weights = torch.cat(label_weights_list, 0)\n    bbox_targets = torch.cat(bbox_targets_list, 0)\n    bbox_weights = torch.cat(bbox_weights_list, 0)\n    cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n    cls_avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_cls_weight\n    if self.sync_cls_avg_factor:\n        cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n    cls_avg_factor = max(cls_avg_factor, 1)\n    loss_cls = self.loss_cls(cls_scores, labels, label_weights, avg_factor=cls_avg_factor)\n    num_total_pos = loss_cls.new_tensor([num_total_pos])\n    num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n    bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n    normalized_bbox_targets = normalize_bbox(bbox_targets, self.pc_range)\n    isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n    bbox_weights = bbox_weights * self.code_weights\n    loss_bbox = self.loss_bbox(bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan, :10], bbox_weights[isnotnan, :10], avg_factor=num_total_pos)\n    loss_cls = torch.nan_to_num(loss_cls)\n    loss_bbox = torch.nan_to_num(loss_bbox)\n    return (loss_cls, loss_bbox)",
            "def loss_single(self, cls_scores, bbox_preds, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\"Loss function for outputs from a single decoder layer of a single\\n        feature level.\\n        Args:\\n            cls_scores (Tensor): Box score logits from a single decoder layer\\n                for all images. Shape [bs, num_query, cls_out_channels].\\n            bbox_preds (Tensor): Sigmoid outputs from a single decoder layer\\n                for all images, with normalized coordinate (cx, cy, w, h) and\\n                shape [bs, num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components for outputs from\\n                a single decoder layer.\\n        '\n    num_imgs = cls_scores.size(0)\n    cls_scores_list = [cls_scores[i] for i in range(num_imgs)]\n    bbox_preds_list = [bbox_preds[i] for i in range(num_imgs)]\n    cls_reg_targets = self.get_targets(cls_scores_list, bbox_preds_list, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list)\n    (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, num_total_pos, num_total_neg) = cls_reg_targets\n    labels = torch.cat(labels_list, 0)\n    label_weights = torch.cat(label_weights_list, 0)\n    bbox_targets = torch.cat(bbox_targets_list, 0)\n    bbox_weights = torch.cat(bbox_weights_list, 0)\n    cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n    cls_avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_cls_weight\n    if self.sync_cls_avg_factor:\n        cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n    cls_avg_factor = max(cls_avg_factor, 1)\n    loss_cls = self.loss_cls(cls_scores, labels, label_weights, avg_factor=cls_avg_factor)\n    num_total_pos = loss_cls.new_tensor([num_total_pos])\n    num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n    bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n    normalized_bbox_targets = normalize_bbox(bbox_targets, self.pc_range)\n    isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n    bbox_weights = bbox_weights * self.code_weights\n    loss_bbox = self.loss_bbox(bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan, :10], bbox_weights[isnotnan, :10], avg_factor=num_total_pos)\n    loss_cls = torch.nan_to_num(loss_cls)\n    loss_bbox = torch.nan_to_num(loss_bbox)\n    return (loss_cls, loss_bbox)",
            "def loss_single(self, cls_scores, bbox_preds, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\"Loss function for outputs from a single decoder layer of a single\\n        feature level.\\n        Args:\\n            cls_scores (Tensor): Box score logits from a single decoder layer\\n                for all images. Shape [bs, num_query, cls_out_channels].\\n            bbox_preds (Tensor): Sigmoid outputs from a single decoder layer\\n                for all images, with normalized coordinate (cx, cy, w, h) and\\n                shape [bs, num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components for outputs from\\n                a single decoder layer.\\n        '\n    num_imgs = cls_scores.size(0)\n    cls_scores_list = [cls_scores[i] for i in range(num_imgs)]\n    bbox_preds_list = [bbox_preds[i] for i in range(num_imgs)]\n    cls_reg_targets = self.get_targets(cls_scores_list, bbox_preds_list, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list)\n    (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, num_total_pos, num_total_neg) = cls_reg_targets\n    labels = torch.cat(labels_list, 0)\n    label_weights = torch.cat(label_weights_list, 0)\n    bbox_targets = torch.cat(bbox_targets_list, 0)\n    bbox_weights = torch.cat(bbox_weights_list, 0)\n    cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n    cls_avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_cls_weight\n    if self.sync_cls_avg_factor:\n        cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n    cls_avg_factor = max(cls_avg_factor, 1)\n    loss_cls = self.loss_cls(cls_scores, labels, label_weights, avg_factor=cls_avg_factor)\n    num_total_pos = loss_cls.new_tensor([num_total_pos])\n    num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n    bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n    normalized_bbox_targets = normalize_bbox(bbox_targets, self.pc_range)\n    isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n    bbox_weights = bbox_weights * self.code_weights\n    loss_bbox = self.loss_bbox(bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan, :10], bbox_weights[isnotnan, :10], avg_factor=num_total_pos)\n    loss_cls = torch.nan_to_num(loss_cls)\n    loss_bbox = torch.nan_to_num(loss_bbox)\n    return (loss_cls, loss_bbox)",
            "def loss_single(self, cls_scores, bbox_preds, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\"Loss function for outputs from a single decoder layer of a single\\n        feature level.\\n        Args:\\n            cls_scores (Tensor): Box score logits from a single decoder layer\\n                for all images. Shape [bs, num_query, cls_out_channels].\\n            bbox_preds (Tensor): Sigmoid outputs from a single decoder layer\\n                for all images, with normalized coordinate (cx, cy, w, h) and\\n                shape [bs, num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components for outputs from\\n                a single decoder layer.\\n        '\n    num_imgs = cls_scores.size(0)\n    cls_scores_list = [cls_scores[i] for i in range(num_imgs)]\n    bbox_preds_list = [bbox_preds[i] for i in range(num_imgs)]\n    cls_reg_targets = self.get_targets(cls_scores_list, bbox_preds_list, gt_bboxes_list, gt_labels_list, gt_bboxes_ignore_list)\n    (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list, num_total_pos, num_total_neg) = cls_reg_targets\n    labels = torch.cat(labels_list, 0)\n    label_weights = torch.cat(label_weights_list, 0)\n    bbox_targets = torch.cat(bbox_targets_list, 0)\n    bbox_weights = torch.cat(bbox_weights_list, 0)\n    cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n    cls_avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_cls_weight\n    if self.sync_cls_avg_factor:\n        cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n    cls_avg_factor = max(cls_avg_factor, 1)\n    loss_cls = self.loss_cls(cls_scores, labels, label_weights, avg_factor=cls_avg_factor)\n    num_total_pos = loss_cls.new_tensor([num_total_pos])\n    num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n    bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n    normalized_bbox_targets = normalize_bbox(bbox_targets, self.pc_range)\n    isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n    bbox_weights = bbox_weights * self.code_weights\n    loss_bbox = self.loss_bbox(bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan, :10], bbox_weights[isnotnan, :10], avg_factor=num_total_pos)\n    loss_cls = torch.nan_to_num(loss_cls)\n    loss_bbox = torch.nan_to_num(loss_bbox)\n    return (loss_cls, loss_bbox)"
        ]
    },
    {
        "func_name": "dn_loss_single",
        "original": "def dn_loss_single(self, cls_scores, bbox_preds, known_bboxs, known_labels, num_total_pos=None):\n    \"\"\"\"Loss function for outputs from a single decoder layer of a single\n        feature level.\n        Args:\n            cls_scores (Tensor): Box score logits from a single decoder layer\n                for all images. Shape [bs, num_query, cls_out_channels].\n            bbox_preds (Tensor): Sigmoid outputs from a single decoder layer\n                for all images, with normalized coordinate (cx, cy, w, h) and\n                shape [bs, num_query, 4].\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\n                image with shape (num_gts, ).\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\n                boxes which can be ignored for each image. Default None.\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components for outputs from\n                a single decoder layer.\n        \"\"\"\n    cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n    cls_avg_factor = num_total_pos * 3.14159 / 6 * self.split * self.split * self.split\n    if self.sync_cls_avg_factor:\n        cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n    bbox_weights = torch.ones_like(bbox_preds)\n    label_weights = torch.ones_like(known_labels)\n    cls_avg_factor = max(cls_avg_factor, 1)\n    loss_cls = self.loss_cls(cls_scores, known_labels.long(), label_weights, avg_factor=cls_avg_factor)\n    num_total_pos = loss_cls.new_tensor([num_total_pos])\n    num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n    bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n    normalized_bbox_targets = normalize_bbox(known_bboxs, self.pc_range)\n    isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n    bbox_weights = bbox_weights * self.code_weights\n    bbox_weights[:, 6:8] = 0\n    loss_bbox = self.loss_bbox(bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan, :10], bbox_weights[isnotnan, :10], avg_factor=num_total_pos)\n    loss_cls = torch.nan_to_num(loss_cls)\n    loss_bbox = torch.nan_to_num(loss_bbox)\n    return (self.dn_weight * loss_cls, self.dn_weight * loss_bbox)",
        "mutated": [
            "def dn_loss_single(self, cls_scores, bbox_preds, known_bboxs, known_labels, num_total_pos=None):\n    if False:\n        i = 10\n    '\"Loss function for outputs from a single decoder layer of a single\\n        feature level.\\n        Args:\\n            cls_scores (Tensor): Box score logits from a single decoder layer\\n                for all images. Shape [bs, num_query, cls_out_channels].\\n            bbox_preds (Tensor): Sigmoid outputs from a single decoder layer\\n                for all images, with normalized coordinate (cx, cy, w, h) and\\n                shape [bs, num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components for outputs from\\n                a single decoder layer.\\n        '\n    cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n    cls_avg_factor = num_total_pos * 3.14159 / 6 * self.split * self.split * self.split\n    if self.sync_cls_avg_factor:\n        cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n    bbox_weights = torch.ones_like(bbox_preds)\n    label_weights = torch.ones_like(known_labels)\n    cls_avg_factor = max(cls_avg_factor, 1)\n    loss_cls = self.loss_cls(cls_scores, known_labels.long(), label_weights, avg_factor=cls_avg_factor)\n    num_total_pos = loss_cls.new_tensor([num_total_pos])\n    num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n    bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n    normalized_bbox_targets = normalize_bbox(known_bboxs, self.pc_range)\n    isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n    bbox_weights = bbox_weights * self.code_weights\n    bbox_weights[:, 6:8] = 0\n    loss_bbox = self.loss_bbox(bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan, :10], bbox_weights[isnotnan, :10], avg_factor=num_total_pos)\n    loss_cls = torch.nan_to_num(loss_cls)\n    loss_bbox = torch.nan_to_num(loss_bbox)\n    return (self.dn_weight * loss_cls, self.dn_weight * loss_bbox)",
            "def dn_loss_single(self, cls_scores, bbox_preds, known_bboxs, known_labels, num_total_pos=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\"Loss function for outputs from a single decoder layer of a single\\n        feature level.\\n        Args:\\n            cls_scores (Tensor): Box score logits from a single decoder layer\\n                for all images. Shape [bs, num_query, cls_out_channels].\\n            bbox_preds (Tensor): Sigmoid outputs from a single decoder layer\\n                for all images, with normalized coordinate (cx, cy, w, h) and\\n                shape [bs, num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components for outputs from\\n                a single decoder layer.\\n        '\n    cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n    cls_avg_factor = num_total_pos * 3.14159 / 6 * self.split * self.split * self.split\n    if self.sync_cls_avg_factor:\n        cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n    bbox_weights = torch.ones_like(bbox_preds)\n    label_weights = torch.ones_like(known_labels)\n    cls_avg_factor = max(cls_avg_factor, 1)\n    loss_cls = self.loss_cls(cls_scores, known_labels.long(), label_weights, avg_factor=cls_avg_factor)\n    num_total_pos = loss_cls.new_tensor([num_total_pos])\n    num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n    bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n    normalized_bbox_targets = normalize_bbox(known_bboxs, self.pc_range)\n    isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n    bbox_weights = bbox_weights * self.code_weights\n    bbox_weights[:, 6:8] = 0\n    loss_bbox = self.loss_bbox(bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan, :10], bbox_weights[isnotnan, :10], avg_factor=num_total_pos)\n    loss_cls = torch.nan_to_num(loss_cls)\n    loss_bbox = torch.nan_to_num(loss_bbox)\n    return (self.dn_weight * loss_cls, self.dn_weight * loss_bbox)",
            "def dn_loss_single(self, cls_scores, bbox_preds, known_bboxs, known_labels, num_total_pos=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\"Loss function for outputs from a single decoder layer of a single\\n        feature level.\\n        Args:\\n            cls_scores (Tensor): Box score logits from a single decoder layer\\n                for all images. Shape [bs, num_query, cls_out_channels].\\n            bbox_preds (Tensor): Sigmoid outputs from a single decoder layer\\n                for all images, with normalized coordinate (cx, cy, w, h) and\\n                shape [bs, num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components for outputs from\\n                a single decoder layer.\\n        '\n    cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n    cls_avg_factor = num_total_pos * 3.14159 / 6 * self.split * self.split * self.split\n    if self.sync_cls_avg_factor:\n        cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n    bbox_weights = torch.ones_like(bbox_preds)\n    label_weights = torch.ones_like(known_labels)\n    cls_avg_factor = max(cls_avg_factor, 1)\n    loss_cls = self.loss_cls(cls_scores, known_labels.long(), label_weights, avg_factor=cls_avg_factor)\n    num_total_pos = loss_cls.new_tensor([num_total_pos])\n    num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n    bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n    normalized_bbox_targets = normalize_bbox(known_bboxs, self.pc_range)\n    isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n    bbox_weights = bbox_weights * self.code_weights\n    bbox_weights[:, 6:8] = 0\n    loss_bbox = self.loss_bbox(bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan, :10], bbox_weights[isnotnan, :10], avg_factor=num_total_pos)\n    loss_cls = torch.nan_to_num(loss_cls)\n    loss_bbox = torch.nan_to_num(loss_bbox)\n    return (self.dn_weight * loss_cls, self.dn_weight * loss_bbox)",
            "def dn_loss_single(self, cls_scores, bbox_preds, known_bboxs, known_labels, num_total_pos=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\"Loss function for outputs from a single decoder layer of a single\\n        feature level.\\n        Args:\\n            cls_scores (Tensor): Box score logits from a single decoder layer\\n                for all images. Shape [bs, num_query, cls_out_channels].\\n            bbox_preds (Tensor): Sigmoid outputs from a single decoder layer\\n                for all images, with normalized coordinate (cx, cy, w, h) and\\n                shape [bs, num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components for outputs from\\n                a single decoder layer.\\n        '\n    cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n    cls_avg_factor = num_total_pos * 3.14159 / 6 * self.split * self.split * self.split\n    if self.sync_cls_avg_factor:\n        cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n    bbox_weights = torch.ones_like(bbox_preds)\n    label_weights = torch.ones_like(known_labels)\n    cls_avg_factor = max(cls_avg_factor, 1)\n    loss_cls = self.loss_cls(cls_scores, known_labels.long(), label_weights, avg_factor=cls_avg_factor)\n    num_total_pos = loss_cls.new_tensor([num_total_pos])\n    num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n    bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n    normalized_bbox_targets = normalize_bbox(known_bboxs, self.pc_range)\n    isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n    bbox_weights = bbox_weights * self.code_weights\n    bbox_weights[:, 6:8] = 0\n    loss_bbox = self.loss_bbox(bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan, :10], bbox_weights[isnotnan, :10], avg_factor=num_total_pos)\n    loss_cls = torch.nan_to_num(loss_cls)\n    loss_bbox = torch.nan_to_num(loss_bbox)\n    return (self.dn_weight * loss_cls, self.dn_weight * loss_bbox)",
            "def dn_loss_single(self, cls_scores, bbox_preds, known_bboxs, known_labels, num_total_pos=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\"Loss function for outputs from a single decoder layer of a single\\n        feature level.\\n        Args:\\n            cls_scores (Tensor): Box score logits from a single decoder layer\\n                for all images. Shape [bs, num_query, cls_out_channels].\\n            bbox_preds (Tensor): Sigmoid outputs from a single decoder layer\\n                for all images, with normalized coordinate (cx, cy, w, h) and\\n                shape [bs, num_query, 4].\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            gt_bboxes_ignore_list (list[Tensor], optional): Bounding\\n                boxes which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components for outputs from\\n                a single decoder layer.\\n        '\n    cls_scores = cls_scores.reshape(-1, self.cls_out_channels)\n    cls_avg_factor = num_total_pos * 3.14159 / 6 * self.split * self.split * self.split\n    if self.sync_cls_avg_factor:\n        cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))\n    bbox_weights = torch.ones_like(bbox_preds)\n    label_weights = torch.ones_like(known_labels)\n    cls_avg_factor = max(cls_avg_factor, 1)\n    loss_cls = self.loss_cls(cls_scores, known_labels.long(), label_weights, avg_factor=cls_avg_factor)\n    num_total_pos = loss_cls.new_tensor([num_total_pos])\n    num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()\n    bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))\n    normalized_bbox_targets = normalize_bbox(known_bboxs, self.pc_range)\n    isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)\n    bbox_weights = bbox_weights * self.code_weights\n    bbox_weights[:, 6:8] = 0\n    loss_bbox = self.loss_bbox(bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan, :10], bbox_weights[isnotnan, :10], avg_factor=num_total_pos)\n    loss_cls = torch.nan_to_num(loss_cls)\n    loss_bbox = torch.nan_to_num(loss_bbox)\n    return (self.dn_weight * loss_cls, self.dn_weight * loss_bbox)"
        ]
    },
    {
        "func_name": "loss",
        "original": "@force_fp32(apply_to='preds_dicts')\ndef loss(self, gt_bboxes_list, gt_labels_list, preds_dicts, gt_depth, gt_bboxes_ignore=None):\n    \"\"\"\"Loss function.\n        Args:\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\n                image with shape (num_gts, ).\n            preds_dicts:\n                all_cls_scores (Tensor): Classification score of all\n                    decoder layers, has shape\n                    [nb_dec, bs, num_query, cls_out_channels].\n                all_bbox_preds (Tensor): Sigmoid regression\n                    outputs of all decode layers. Each is a 4D-tensor with\n                    normalized coordinate format (cx, cy, w, h) and shape\n                    [nb_dec, bs, num_query, 4].\n                enc_cls_scores (Tensor): Classification scores of\n                    points on encode feature map , has shape\n                    (N, h*w, num_classes). Only be passed when as_two_stage is\n                    True, otherwise is None.\n                enc_bbox_preds (Tensor): Regression results of each points\n                    on the encode feature map, has shape (N, h*w, 4). Only be\n                    passed when as_two_stage is True, otherwise is None.\n            gt_bboxes_ignore (list[Tensor], optional): Bounding boxes\n                which can be ignored for each image. Default None.\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components.\n        \"\"\"\n    assert gt_bboxes_ignore is None, f'{self.__class__.__name__} only supports for gt_bboxes_ignore setting to None.'\n    loss_dict = dict()\n    if self.with_depthnet:\n        depth_pred = preds_dicts['depth_pred']\n        if not self.sweep_lidar:\n            depth_pred = depth_pred[:, :6, ...]\n            gt_depth = gt_depth[:, :6, ...]\n        (B, N, C, H, W) = depth_pred.shape\n        depth_pred = depth_pred.view(B * N, C, H, W)\n        loss_depth = self.get_depth_loss(gt_depth.to(depth_pred.device), depth_pred)\n        loss_dict['loss_depth'] = loss_depth\n    all_cls_scores = preds_dicts['all_cls_scores']\n    all_bbox_preds = preds_dicts['all_bbox_preds']\n    enc_cls_scores = preds_dicts['enc_cls_scores']\n    enc_bbox_preds = preds_dicts['enc_bbox_preds']\n    num_dec_layers = len(all_cls_scores)\n    device = gt_labels_list[0].device\n    gt_bboxes_list = [torch.cat((gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]), dim=1).to(device) for gt_bboxes in gt_bboxes_list]\n    all_gt_bboxes_list = [gt_bboxes_list for _ in range(num_dec_layers)]\n    all_gt_labels_list = [gt_labels_list for _ in range(num_dec_layers)]\n    all_gt_bboxes_ignore_list = [gt_bboxes_ignore for _ in range(num_dec_layers)]\n    (losses_cls, losses_bbox) = multi_apply(self.loss_single, all_cls_scores, all_bbox_preds, all_gt_bboxes_list, all_gt_labels_list, all_gt_bboxes_ignore_list)\n    if enc_cls_scores is not None:\n        binary_labels_list = [torch.zeros_like(gt_labels_list[i]) for i in range(len(all_gt_labels_list))]\n        (enc_loss_cls, enc_losses_bbox) = self.loss_single(enc_cls_scores, enc_bbox_preds, gt_bboxes_list, binary_labels_list, gt_bboxes_ignore)\n        loss_dict['enc_loss_cls'] = enc_loss_cls\n        loss_dict['enc_loss_bbox'] = enc_losses_bbox\n    if preds_dicts['dn_mask_dict'] is not None:\n        (known_labels, known_bboxs, output_known_class, output_known_coord, num_tgt) = self.prepare_for_loss(preds_dicts['dn_mask_dict'])\n        all_known_bboxs_list = [known_bboxs for _ in range(num_dec_layers)]\n        all_known_labels_list = [known_labels for _ in range(num_dec_layers)]\n        all_num_tgts_list = [num_tgt for _ in range(num_dec_layers)]\n        (dn_losses_cls, dn_losses_bbox) = multi_apply(self.dn_loss_single, output_known_class, output_known_coord, all_known_bboxs_list, all_known_labels_list, all_num_tgts_list)\n        loss_dict['dn_loss_cls'] = dn_losses_cls[-1]\n        loss_dict['dn_loss_bbox'] = dn_losses_bbox[-1]\n        num_dec_layer = 0\n        for (loss_cls_i, loss_bbox_i) in zip(dn_losses_cls[:-1], dn_losses_bbox[:-1]):\n            loss_dict[f'd{num_dec_layer}.dn_loss_cls'] = loss_cls_i\n            loss_dict[f'd{num_dec_layer}.dn_loss_bbox'] = loss_bbox_i\n            num_dec_layer += 1\n    loss_dict['loss_cls'] = losses_cls[-1]\n    loss_dict['loss_bbox'] = losses_bbox[-1]\n    num_dec_layer = 0\n    for (loss_cls_i, loss_bbox_i) in zip(losses_cls[:-1], losses_bbox[:-1]):\n        loss_dict[f'd{num_dec_layer}.loss_cls'] = loss_cls_i\n        loss_dict[f'd{num_dec_layer}.loss_bbox'] = loss_bbox_i\n        num_dec_layer += 1\n    return loss_dict",
        "mutated": [
            "@force_fp32(apply_to='preds_dicts')\ndef loss(self, gt_bboxes_list, gt_labels_list, preds_dicts, gt_depth, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n    '\"Loss function.\\n        Args:\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            preds_dicts:\\n                all_cls_scores (Tensor): Classification score of all\\n                    decoder layers, has shape\\n                    [nb_dec, bs, num_query, cls_out_channels].\\n                all_bbox_preds (Tensor): Sigmoid regression\\n                    outputs of all decode layers. Each is a 4D-tensor with\\n                    normalized coordinate format (cx, cy, w, h) and shape\\n                    [nb_dec, bs, num_query, 4].\\n                enc_cls_scores (Tensor): Classification scores of\\n                    points on encode feature map , has shape\\n                    (N, h*w, num_classes). Only be passed when as_two_stage is\\n                    True, otherwise is None.\\n                enc_bbox_preds (Tensor): Regression results of each points\\n                    on the encode feature map, has shape (N, h*w, 4). Only be\\n                    passed when as_two_stage is True, otherwise is None.\\n            gt_bboxes_ignore (list[Tensor], optional): Bounding boxes\\n                which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        '\n    assert gt_bboxes_ignore is None, f'{self.__class__.__name__} only supports for gt_bboxes_ignore setting to None.'\n    loss_dict = dict()\n    if self.with_depthnet:\n        depth_pred = preds_dicts['depth_pred']\n        if not self.sweep_lidar:\n            depth_pred = depth_pred[:, :6, ...]\n            gt_depth = gt_depth[:, :6, ...]\n        (B, N, C, H, W) = depth_pred.shape\n        depth_pred = depth_pred.view(B * N, C, H, W)\n        loss_depth = self.get_depth_loss(gt_depth.to(depth_pred.device), depth_pred)\n        loss_dict['loss_depth'] = loss_depth\n    all_cls_scores = preds_dicts['all_cls_scores']\n    all_bbox_preds = preds_dicts['all_bbox_preds']\n    enc_cls_scores = preds_dicts['enc_cls_scores']\n    enc_bbox_preds = preds_dicts['enc_bbox_preds']\n    num_dec_layers = len(all_cls_scores)\n    device = gt_labels_list[0].device\n    gt_bboxes_list = [torch.cat((gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]), dim=1).to(device) for gt_bboxes in gt_bboxes_list]\n    all_gt_bboxes_list = [gt_bboxes_list for _ in range(num_dec_layers)]\n    all_gt_labels_list = [gt_labels_list for _ in range(num_dec_layers)]\n    all_gt_bboxes_ignore_list = [gt_bboxes_ignore for _ in range(num_dec_layers)]\n    (losses_cls, losses_bbox) = multi_apply(self.loss_single, all_cls_scores, all_bbox_preds, all_gt_bboxes_list, all_gt_labels_list, all_gt_bboxes_ignore_list)\n    if enc_cls_scores is not None:\n        binary_labels_list = [torch.zeros_like(gt_labels_list[i]) for i in range(len(all_gt_labels_list))]\n        (enc_loss_cls, enc_losses_bbox) = self.loss_single(enc_cls_scores, enc_bbox_preds, gt_bboxes_list, binary_labels_list, gt_bboxes_ignore)\n        loss_dict['enc_loss_cls'] = enc_loss_cls\n        loss_dict['enc_loss_bbox'] = enc_losses_bbox\n    if preds_dicts['dn_mask_dict'] is not None:\n        (known_labels, known_bboxs, output_known_class, output_known_coord, num_tgt) = self.prepare_for_loss(preds_dicts['dn_mask_dict'])\n        all_known_bboxs_list = [known_bboxs for _ in range(num_dec_layers)]\n        all_known_labels_list = [known_labels for _ in range(num_dec_layers)]\n        all_num_tgts_list = [num_tgt for _ in range(num_dec_layers)]\n        (dn_losses_cls, dn_losses_bbox) = multi_apply(self.dn_loss_single, output_known_class, output_known_coord, all_known_bboxs_list, all_known_labels_list, all_num_tgts_list)\n        loss_dict['dn_loss_cls'] = dn_losses_cls[-1]\n        loss_dict['dn_loss_bbox'] = dn_losses_bbox[-1]\n        num_dec_layer = 0\n        for (loss_cls_i, loss_bbox_i) in zip(dn_losses_cls[:-1], dn_losses_bbox[:-1]):\n            loss_dict[f'd{num_dec_layer}.dn_loss_cls'] = loss_cls_i\n            loss_dict[f'd{num_dec_layer}.dn_loss_bbox'] = loss_bbox_i\n            num_dec_layer += 1\n    loss_dict['loss_cls'] = losses_cls[-1]\n    loss_dict['loss_bbox'] = losses_bbox[-1]\n    num_dec_layer = 0\n    for (loss_cls_i, loss_bbox_i) in zip(losses_cls[:-1], losses_bbox[:-1]):\n        loss_dict[f'd{num_dec_layer}.loss_cls'] = loss_cls_i\n        loss_dict[f'd{num_dec_layer}.loss_bbox'] = loss_bbox_i\n        num_dec_layer += 1\n    return loss_dict",
            "@force_fp32(apply_to='preds_dicts')\ndef loss(self, gt_bboxes_list, gt_labels_list, preds_dicts, gt_depth, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\"Loss function.\\n        Args:\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            preds_dicts:\\n                all_cls_scores (Tensor): Classification score of all\\n                    decoder layers, has shape\\n                    [nb_dec, bs, num_query, cls_out_channels].\\n                all_bbox_preds (Tensor): Sigmoid regression\\n                    outputs of all decode layers. Each is a 4D-tensor with\\n                    normalized coordinate format (cx, cy, w, h) and shape\\n                    [nb_dec, bs, num_query, 4].\\n                enc_cls_scores (Tensor): Classification scores of\\n                    points on encode feature map , has shape\\n                    (N, h*w, num_classes). Only be passed when as_two_stage is\\n                    True, otherwise is None.\\n                enc_bbox_preds (Tensor): Regression results of each points\\n                    on the encode feature map, has shape (N, h*w, 4). Only be\\n                    passed when as_two_stage is True, otherwise is None.\\n            gt_bboxes_ignore (list[Tensor], optional): Bounding boxes\\n                which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        '\n    assert gt_bboxes_ignore is None, f'{self.__class__.__name__} only supports for gt_bboxes_ignore setting to None.'\n    loss_dict = dict()\n    if self.with_depthnet:\n        depth_pred = preds_dicts['depth_pred']\n        if not self.sweep_lidar:\n            depth_pred = depth_pred[:, :6, ...]\n            gt_depth = gt_depth[:, :6, ...]\n        (B, N, C, H, W) = depth_pred.shape\n        depth_pred = depth_pred.view(B * N, C, H, W)\n        loss_depth = self.get_depth_loss(gt_depth.to(depth_pred.device), depth_pred)\n        loss_dict['loss_depth'] = loss_depth\n    all_cls_scores = preds_dicts['all_cls_scores']\n    all_bbox_preds = preds_dicts['all_bbox_preds']\n    enc_cls_scores = preds_dicts['enc_cls_scores']\n    enc_bbox_preds = preds_dicts['enc_bbox_preds']\n    num_dec_layers = len(all_cls_scores)\n    device = gt_labels_list[0].device\n    gt_bboxes_list = [torch.cat((gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]), dim=1).to(device) for gt_bboxes in gt_bboxes_list]\n    all_gt_bboxes_list = [gt_bboxes_list for _ in range(num_dec_layers)]\n    all_gt_labels_list = [gt_labels_list for _ in range(num_dec_layers)]\n    all_gt_bboxes_ignore_list = [gt_bboxes_ignore for _ in range(num_dec_layers)]\n    (losses_cls, losses_bbox) = multi_apply(self.loss_single, all_cls_scores, all_bbox_preds, all_gt_bboxes_list, all_gt_labels_list, all_gt_bboxes_ignore_list)\n    if enc_cls_scores is not None:\n        binary_labels_list = [torch.zeros_like(gt_labels_list[i]) for i in range(len(all_gt_labels_list))]\n        (enc_loss_cls, enc_losses_bbox) = self.loss_single(enc_cls_scores, enc_bbox_preds, gt_bboxes_list, binary_labels_list, gt_bboxes_ignore)\n        loss_dict['enc_loss_cls'] = enc_loss_cls\n        loss_dict['enc_loss_bbox'] = enc_losses_bbox\n    if preds_dicts['dn_mask_dict'] is not None:\n        (known_labels, known_bboxs, output_known_class, output_known_coord, num_tgt) = self.prepare_for_loss(preds_dicts['dn_mask_dict'])\n        all_known_bboxs_list = [known_bboxs for _ in range(num_dec_layers)]\n        all_known_labels_list = [known_labels for _ in range(num_dec_layers)]\n        all_num_tgts_list = [num_tgt for _ in range(num_dec_layers)]\n        (dn_losses_cls, dn_losses_bbox) = multi_apply(self.dn_loss_single, output_known_class, output_known_coord, all_known_bboxs_list, all_known_labels_list, all_num_tgts_list)\n        loss_dict['dn_loss_cls'] = dn_losses_cls[-1]\n        loss_dict['dn_loss_bbox'] = dn_losses_bbox[-1]\n        num_dec_layer = 0\n        for (loss_cls_i, loss_bbox_i) in zip(dn_losses_cls[:-1], dn_losses_bbox[:-1]):\n            loss_dict[f'd{num_dec_layer}.dn_loss_cls'] = loss_cls_i\n            loss_dict[f'd{num_dec_layer}.dn_loss_bbox'] = loss_bbox_i\n            num_dec_layer += 1\n    loss_dict['loss_cls'] = losses_cls[-1]\n    loss_dict['loss_bbox'] = losses_bbox[-1]\n    num_dec_layer = 0\n    for (loss_cls_i, loss_bbox_i) in zip(losses_cls[:-1], losses_bbox[:-1]):\n        loss_dict[f'd{num_dec_layer}.loss_cls'] = loss_cls_i\n        loss_dict[f'd{num_dec_layer}.loss_bbox'] = loss_bbox_i\n        num_dec_layer += 1\n    return loss_dict",
            "@force_fp32(apply_to='preds_dicts')\ndef loss(self, gt_bboxes_list, gt_labels_list, preds_dicts, gt_depth, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\"Loss function.\\n        Args:\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            preds_dicts:\\n                all_cls_scores (Tensor): Classification score of all\\n                    decoder layers, has shape\\n                    [nb_dec, bs, num_query, cls_out_channels].\\n                all_bbox_preds (Tensor): Sigmoid regression\\n                    outputs of all decode layers. Each is a 4D-tensor with\\n                    normalized coordinate format (cx, cy, w, h) and shape\\n                    [nb_dec, bs, num_query, 4].\\n                enc_cls_scores (Tensor): Classification scores of\\n                    points on encode feature map , has shape\\n                    (N, h*w, num_classes). Only be passed when as_two_stage is\\n                    True, otherwise is None.\\n                enc_bbox_preds (Tensor): Regression results of each points\\n                    on the encode feature map, has shape (N, h*w, 4). Only be\\n                    passed when as_two_stage is True, otherwise is None.\\n            gt_bboxes_ignore (list[Tensor], optional): Bounding boxes\\n                which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        '\n    assert gt_bboxes_ignore is None, f'{self.__class__.__name__} only supports for gt_bboxes_ignore setting to None.'\n    loss_dict = dict()\n    if self.with_depthnet:\n        depth_pred = preds_dicts['depth_pred']\n        if not self.sweep_lidar:\n            depth_pred = depth_pred[:, :6, ...]\n            gt_depth = gt_depth[:, :6, ...]\n        (B, N, C, H, W) = depth_pred.shape\n        depth_pred = depth_pred.view(B * N, C, H, W)\n        loss_depth = self.get_depth_loss(gt_depth.to(depth_pred.device), depth_pred)\n        loss_dict['loss_depth'] = loss_depth\n    all_cls_scores = preds_dicts['all_cls_scores']\n    all_bbox_preds = preds_dicts['all_bbox_preds']\n    enc_cls_scores = preds_dicts['enc_cls_scores']\n    enc_bbox_preds = preds_dicts['enc_bbox_preds']\n    num_dec_layers = len(all_cls_scores)\n    device = gt_labels_list[0].device\n    gt_bboxes_list = [torch.cat((gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]), dim=1).to(device) for gt_bboxes in gt_bboxes_list]\n    all_gt_bboxes_list = [gt_bboxes_list for _ in range(num_dec_layers)]\n    all_gt_labels_list = [gt_labels_list for _ in range(num_dec_layers)]\n    all_gt_bboxes_ignore_list = [gt_bboxes_ignore for _ in range(num_dec_layers)]\n    (losses_cls, losses_bbox) = multi_apply(self.loss_single, all_cls_scores, all_bbox_preds, all_gt_bboxes_list, all_gt_labels_list, all_gt_bboxes_ignore_list)\n    if enc_cls_scores is not None:\n        binary_labels_list = [torch.zeros_like(gt_labels_list[i]) for i in range(len(all_gt_labels_list))]\n        (enc_loss_cls, enc_losses_bbox) = self.loss_single(enc_cls_scores, enc_bbox_preds, gt_bboxes_list, binary_labels_list, gt_bboxes_ignore)\n        loss_dict['enc_loss_cls'] = enc_loss_cls\n        loss_dict['enc_loss_bbox'] = enc_losses_bbox\n    if preds_dicts['dn_mask_dict'] is not None:\n        (known_labels, known_bboxs, output_known_class, output_known_coord, num_tgt) = self.prepare_for_loss(preds_dicts['dn_mask_dict'])\n        all_known_bboxs_list = [known_bboxs for _ in range(num_dec_layers)]\n        all_known_labels_list = [known_labels for _ in range(num_dec_layers)]\n        all_num_tgts_list = [num_tgt for _ in range(num_dec_layers)]\n        (dn_losses_cls, dn_losses_bbox) = multi_apply(self.dn_loss_single, output_known_class, output_known_coord, all_known_bboxs_list, all_known_labels_list, all_num_tgts_list)\n        loss_dict['dn_loss_cls'] = dn_losses_cls[-1]\n        loss_dict['dn_loss_bbox'] = dn_losses_bbox[-1]\n        num_dec_layer = 0\n        for (loss_cls_i, loss_bbox_i) in zip(dn_losses_cls[:-1], dn_losses_bbox[:-1]):\n            loss_dict[f'd{num_dec_layer}.dn_loss_cls'] = loss_cls_i\n            loss_dict[f'd{num_dec_layer}.dn_loss_bbox'] = loss_bbox_i\n            num_dec_layer += 1\n    loss_dict['loss_cls'] = losses_cls[-1]\n    loss_dict['loss_bbox'] = losses_bbox[-1]\n    num_dec_layer = 0\n    for (loss_cls_i, loss_bbox_i) in zip(losses_cls[:-1], losses_bbox[:-1]):\n        loss_dict[f'd{num_dec_layer}.loss_cls'] = loss_cls_i\n        loss_dict[f'd{num_dec_layer}.loss_bbox'] = loss_bbox_i\n        num_dec_layer += 1\n    return loss_dict",
            "@force_fp32(apply_to='preds_dicts')\ndef loss(self, gt_bboxes_list, gt_labels_list, preds_dicts, gt_depth, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\"Loss function.\\n        Args:\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            preds_dicts:\\n                all_cls_scores (Tensor): Classification score of all\\n                    decoder layers, has shape\\n                    [nb_dec, bs, num_query, cls_out_channels].\\n                all_bbox_preds (Tensor): Sigmoid regression\\n                    outputs of all decode layers. Each is a 4D-tensor with\\n                    normalized coordinate format (cx, cy, w, h) and shape\\n                    [nb_dec, bs, num_query, 4].\\n                enc_cls_scores (Tensor): Classification scores of\\n                    points on encode feature map , has shape\\n                    (N, h*w, num_classes). Only be passed when as_two_stage is\\n                    True, otherwise is None.\\n                enc_bbox_preds (Tensor): Regression results of each points\\n                    on the encode feature map, has shape (N, h*w, 4). Only be\\n                    passed when as_two_stage is True, otherwise is None.\\n            gt_bboxes_ignore (list[Tensor], optional): Bounding boxes\\n                which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        '\n    assert gt_bboxes_ignore is None, f'{self.__class__.__name__} only supports for gt_bboxes_ignore setting to None.'\n    loss_dict = dict()\n    if self.with_depthnet:\n        depth_pred = preds_dicts['depth_pred']\n        if not self.sweep_lidar:\n            depth_pred = depth_pred[:, :6, ...]\n            gt_depth = gt_depth[:, :6, ...]\n        (B, N, C, H, W) = depth_pred.shape\n        depth_pred = depth_pred.view(B * N, C, H, W)\n        loss_depth = self.get_depth_loss(gt_depth.to(depth_pred.device), depth_pred)\n        loss_dict['loss_depth'] = loss_depth\n    all_cls_scores = preds_dicts['all_cls_scores']\n    all_bbox_preds = preds_dicts['all_bbox_preds']\n    enc_cls_scores = preds_dicts['enc_cls_scores']\n    enc_bbox_preds = preds_dicts['enc_bbox_preds']\n    num_dec_layers = len(all_cls_scores)\n    device = gt_labels_list[0].device\n    gt_bboxes_list = [torch.cat((gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]), dim=1).to(device) for gt_bboxes in gt_bboxes_list]\n    all_gt_bboxes_list = [gt_bboxes_list for _ in range(num_dec_layers)]\n    all_gt_labels_list = [gt_labels_list for _ in range(num_dec_layers)]\n    all_gt_bboxes_ignore_list = [gt_bboxes_ignore for _ in range(num_dec_layers)]\n    (losses_cls, losses_bbox) = multi_apply(self.loss_single, all_cls_scores, all_bbox_preds, all_gt_bboxes_list, all_gt_labels_list, all_gt_bboxes_ignore_list)\n    if enc_cls_scores is not None:\n        binary_labels_list = [torch.zeros_like(gt_labels_list[i]) for i in range(len(all_gt_labels_list))]\n        (enc_loss_cls, enc_losses_bbox) = self.loss_single(enc_cls_scores, enc_bbox_preds, gt_bboxes_list, binary_labels_list, gt_bboxes_ignore)\n        loss_dict['enc_loss_cls'] = enc_loss_cls\n        loss_dict['enc_loss_bbox'] = enc_losses_bbox\n    if preds_dicts['dn_mask_dict'] is not None:\n        (known_labels, known_bboxs, output_known_class, output_known_coord, num_tgt) = self.prepare_for_loss(preds_dicts['dn_mask_dict'])\n        all_known_bboxs_list = [known_bboxs for _ in range(num_dec_layers)]\n        all_known_labels_list = [known_labels for _ in range(num_dec_layers)]\n        all_num_tgts_list = [num_tgt for _ in range(num_dec_layers)]\n        (dn_losses_cls, dn_losses_bbox) = multi_apply(self.dn_loss_single, output_known_class, output_known_coord, all_known_bboxs_list, all_known_labels_list, all_num_tgts_list)\n        loss_dict['dn_loss_cls'] = dn_losses_cls[-1]\n        loss_dict['dn_loss_bbox'] = dn_losses_bbox[-1]\n        num_dec_layer = 0\n        for (loss_cls_i, loss_bbox_i) in zip(dn_losses_cls[:-1], dn_losses_bbox[:-1]):\n            loss_dict[f'd{num_dec_layer}.dn_loss_cls'] = loss_cls_i\n            loss_dict[f'd{num_dec_layer}.dn_loss_bbox'] = loss_bbox_i\n            num_dec_layer += 1\n    loss_dict['loss_cls'] = losses_cls[-1]\n    loss_dict['loss_bbox'] = losses_bbox[-1]\n    num_dec_layer = 0\n    for (loss_cls_i, loss_bbox_i) in zip(losses_cls[:-1], losses_bbox[:-1]):\n        loss_dict[f'd{num_dec_layer}.loss_cls'] = loss_cls_i\n        loss_dict[f'd{num_dec_layer}.loss_bbox'] = loss_bbox_i\n        num_dec_layer += 1\n    return loss_dict",
            "@force_fp32(apply_to='preds_dicts')\ndef loss(self, gt_bboxes_list, gt_labels_list, preds_dicts, gt_depth, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\"Loss function.\\n        Args:\\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image\\n                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels_list (list[Tensor]): Ground truth class indices for each\\n                image with shape (num_gts, ).\\n            preds_dicts:\\n                all_cls_scores (Tensor): Classification score of all\\n                    decoder layers, has shape\\n                    [nb_dec, bs, num_query, cls_out_channels].\\n                all_bbox_preds (Tensor): Sigmoid regression\\n                    outputs of all decode layers. Each is a 4D-tensor with\\n                    normalized coordinate format (cx, cy, w, h) and shape\\n                    [nb_dec, bs, num_query, 4].\\n                enc_cls_scores (Tensor): Classification scores of\\n                    points on encode feature map , has shape\\n                    (N, h*w, num_classes). Only be passed when as_two_stage is\\n                    True, otherwise is None.\\n                enc_bbox_preds (Tensor): Regression results of each points\\n                    on the encode feature map, has shape (N, h*w, 4). Only be\\n                    passed when as_two_stage is True, otherwise is None.\\n            gt_bboxes_ignore (list[Tensor], optional): Bounding boxes\\n                which can be ignored for each image. Default None.\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        '\n    assert gt_bboxes_ignore is None, f'{self.__class__.__name__} only supports for gt_bboxes_ignore setting to None.'\n    loss_dict = dict()\n    if self.with_depthnet:\n        depth_pred = preds_dicts['depth_pred']\n        if not self.sweep_lidar:\n            depth_pred = depth_pred[:, :6, ...]\n            gt_depth = gt_depth[:, :6, ...]\n        (B, N, C, H, W) = depth_pred.shape\n        depth_pred = depth_pred.view(B * N, C, H, W)\n        loss_depth = self.get_depth_loss(gt_depth.to(depth_pred.device), depth_pred)\n        loss_dict['loss_depth'] = loss_depth\n    all_cls_scores = preds_dicts['all_cls_scores']\n    all_bbox_preds = preds_dicts['all_bbox_preds']\n    enc_cls_scores = preds_dicts['enc_cls_scores']\n    enc_bbox_preds = preds_dicts['enc_bbox_preds']\n    num_dec_layers = len(all_cls_scores)\n    device = gt_labels_list[0].device\n    gt_bboxes_list = [torch.cat((gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]), dim=1).to(device) for gt_bboxes in gt_bboxes_list]\n    all_gt_bboxes_list = [gt_bboxes_list for _ in range(num_dec_layers)]\n    all_gt_labels_list = [gt_labels_list for _ in range(num_dec_layers)]\n    all_gt_bboxes_ignore_list = [gt_bboxes_ignore for _ in range(num_dec_layers)]\n    (losses_cls, losses_bbox) = multi_apply(self.loss_single, all_cls_scores, all_bbox_preds, all_gt_bboxes_list, all_gt_labels_list, all_gt_bboxes_ignore_list)\n    if enc_cls_scores is not None:\n        binary_labels_list = [torch.zeros_like(gt_labels_list[i]) for i in range(len(all_gt_labels_list))]\n        (enc_loss_cls, enc_losses_bbox) = self.loss_single(enc_cls_scores, enc_bbox_preds, gt_bboxes_list, binary_labels_list, gt_bboxes_ignore)\n        loss_dict['enc_loss_cls'] = enc_loss_cls\n        loss_dict['enc_loss_bbox'] = enc_losses_bbox\n    if preds_dicts['dn_mask_dict'] is not None:\n        (known_labels, known_bboxs, output_known_class, output_known_coord, num_tgt) = self.prepare_for_loss(preds_dicts['dn_mask_dict'])\n        all_known_bboxs_list = [known_bboxs for _ in range(num_dec_layers)]\n        all_known_labels_list = [known_labels for _ in range(num_dec_layers)]\n        all_num_tgts_list = [num_tgt for _ in range(num_dec_layers)]\n        (dn_losses_cls, dn_losses_bbox) = multi_apply(self.dn_loss_single, output_known_class, output_known_coord, all_known_bboxs_list, all_known_labels_list, all_num_tgts_list)\n        loss_dict['dn_loss_cls'] = dn_losses_cls[-1]\n        loss_dict['dn_loss_bbox'] = dn_losses_bbox[-1]\n        num_dec_layer = 0\n        for (loss_cls_i, loss_bbox_i) in zip(dn_losses_cls[:-1], dn_losses_bbox[:-1]):\n            loss_dict[f'd{num_dec_layer}.dn_loss_cls'] = loss_cls_i\n            loss_dict[f'd{num_dec_layer}.dn_loss_bbox'] = loss_bbox_i\n            num_dec_layer += 1\n    loss_dict['loss_cls'] = losses_cls[-1]\n    loss_dict['loss_bbox'] = losses_bbox[-1]\n    num_dec_layer = 0\n    for (loss_cls_i, loss_bbox_i) in zip(losses_cls[:-1], losses_bbox[:-1]):\n        loss_dict[f'd{num_dec_layer}.loss_cls'] = loss_cls_i\n        loss_dict[f'd{num_dec_layer}.loss_bbox'] = loss_bbox_i\n        num_dec_layer += 1\n    return loss_dict"
        ]
    },
    {
        "func_name": "get_bboxes",
        "original": "@force_fp32(apply_to='preds_dicts')\ndef get_bboxes(self, preds_dicts, img_metas, rescale=False):\n    \"\"\"Generate bboxes from bbox head predictions.\n        Args:\n            preds_dicts (tuple[list[dict]]): Prediction results.\n            img_metas (list[dict]): Point cloud and image's meta info.\n        Returns:\n            list[dict]: Decoded bbox, scores and labels after nms.\n        \"\"\"\n    preds_dicts = self.bbox_coder.decode(preds_dicts)\n    num_samples = len(preds_dicts)\n    ret_list = []\n    for i in range(num_samples):\n        preds = preds_dicts[i]\n        bboxes = preds['bboxes']\n        bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 5] * 0.5\n        bboxes = img_metas[i]['box_type_3d'](bboxes, bboxes.size(-1))\n        scores = preds['scores']\n        labels = preds['labels']\n        ret_list.append([bboxes, scores, labels])\n    return ret_list",
        "mutated": [
            "@force_fp32(apply_to='preds_dicts')\ndef get_bboxes(self, preds_dicts, img_metas, rescale=False):\n    if False:\n        i = 10\n    \"Generate bboxes from bbox head predictions.\\n        Args:\\n            preds_dicts (tuple[list[dict]]): Prediction results.\\n            img_metas (list[dict]): Point cloud and image's meta info.\\n        Returns:\\n            list[dict]: Decoded bbox, scores and labels after nms.\\n        \"\n    preds_dicts = self.bbox_coder.decode(preds_dicts)\n    num_samples = len(preds_dicts)\n    ret_list = []\n    for i in range(num_samples):\n        preds = preds_dicts[i]\n        bboxes = preds['bboxes']\n        bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 5] * 0.5\n        bboxes = img_metas[i]['box_type_3d'](bboxes, bboxes.size(-1))\n        scores = preds['scores']\n        labels = preds['labels']\n        ret_list.append([bboxes, scores, labels])\n    return ret_list",
            "@force_fp32(apply_to='preds_dicts')\ndef get_bboxes(self, preds_dicts, img_metas, rescale=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Generate bboxes from bbox head predictions.\\n        Args:\\n            preds_dicts (tuple[list[dict]]): Prediction results.\\n            img_metas (list[dict]): Point cloud and image's meta info.\\n        Returns:\\n            list[dict]: Decoded bbox, scores and labels after nms.\\n        \"\n    preds_dicts = self.bbox_coder.decode(preds_dicts)\n    num_samples = len(preds_dicts)\n    ret_list = []\n    for i in range(num_samples):\n        preds = preds_dicts[i]\n        bboxes = preds['bboxes']\n        bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 5] * 0.5\n        bboxes = img_metas[i]['box_type_3d'](bboxes, bboxes.size(-1))\n        scores = preds['scores']\n        labels = preds['labels']\n        ret_list.append([bboxes, scores, labels])\n    return ret_list",
            "@force_fp32(apply_to='preds_dicts')\ndef get_bboxes(self, preds_dicts, img_metas, rescale=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Generate bboxes from bbox head predictions.\\n        Args:\\n            preds_dicts (tuple[list[dict]]): Prediction results.\\n            img_metas (list[dict]): Point cloud and image's meta info.\\n        Returns:\\n            list[dict]: Decoded bbox, scores and labels after nms.\\n        \"\n    preds_dicts = self.bbox_coder.decode(preds_dicts)\n    num_samples = len(preds_dicts)\n    ret_list = []\n    for i in range(num_samples):\n        preds = preds_dicts[i]\n        bboxes = preds['bboxes']\n        bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 5] * 0.5\n        bboxes = img_metas[i]['box_type_3d'](bboxes, bboxes.size(-1))\n        scores = preds['scores']\n        labels = preds['labels']\n        ret_list.append([bboxes, scores, labels])\n    return ret_list",
            "@force_fp32(apply_to='preds_dicts')\ndef get_bboxes(self, preds_dicts, img_metas, rescale=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Generate bboxes from bbox head predictions.\\n        Args:\\n            preds_dicts (tuple[list[dict]]): Prediction results.\\n            img_metas (list[dict]): Point cloud and image's meta info.\\n        Returns:\\n            list[dict]: Decoded bbox, scores and labels after nms.\\n        \"\n    preds_dicts = self.bbox_coder.decode(preds_dicts)\n    num_samples = len(preds_dicts)\n    ret_list = []\n    for i in range(num_samples):\n        preds = preds_dicts[i]\n        bboxes = preds['bboxes']\n        bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 5] * 0.5\n        bboxes = img_metas[i]['box_type_3d'](bboxes, bboxes.size(-1))\n        scores = preds['scores']\n        labels = preds['labels']\n        ret_list.append([bboxes, scores, labels])\n    return ret_list",
            "@force_fp32(apply_to='preds_dicts')\ndef get_bboxes(self, preds_dicts, img_metas, rescale=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Generate bboxes from bbox head predictions.\\n        Args:\\n            preds_dicts (tuple[list[dict]]): Prediction results.\\n            img_metas (list[dict]): Point cloud and image's meta info.\\n        Returns:\\n            list[dict]: Decoded bbox, scores and labels after nms.\\n        \"\n    preds_dicts = self.bbox_coder.decode(preds_dicts)\n    num_samples = len(preds_dicts)\n    ret_list = []\n    for i in range(num_samples):\n        preds = preds_dicts[i]\n        bboxes = preds['bboxes']\n        bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 5] * 0.5\n        bboxes = img_metas[i]['box_type_3d'](bboxes, bboxes.size(-1))\n        scores = preds['scores']\n        labels = preds['labels']\n        ret_list.append([bboxes, scores, labels])\n    return ret_list"
        ]
    }
]