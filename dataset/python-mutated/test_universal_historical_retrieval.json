[
    {
        "func_name": "test_historical_features",
        "original": "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: f'full:{v}')\ndef test_historical_features(environment, universal_data_sources, full_feature_names):\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    entity_df_with_request_data = datasets.entity_df.copy(deep=True)\n    entity_df_with_request_data['val_to_add'] = [i for i in range(len(entity_df_with_request_data))]\n    entity_df_with_request_data['driver_age'] = [i + 100 for i in range(len(entity_df_with_request_data))]\n    feature_service = FeatureService(name='convrate_plus100', features=[feature_views.driver[['conv_rate']], feature_views.driver_odfv])\n    feature_service_entity_mapping = FeatureService(name='entity_mapping', features=[feature_views.location.with_name('origin').with_join_key_map({'location_id': 'origin_id'}), feature_views.location.with_name('destination').with_join_key_map({'location_id': 'destination_id'})])\n    store.apply([driver(), customer(), location(), feature_service, feature_service_entity_mapping, *feature_views.values()])\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL if DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL in datasets.orders_df.columns else 'e_ts'\n    full_expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df_with_request_data, event_timestamp, full_feature_names)\n    expected_df = full_expected_df.drop(columns=['origin__temperature', 'destination__temperature'])\n    job_from_df = store.get_historical_features(entity_df=entity_df_with_request_data, features=['driver_stats:conv_rate', 'driver_stats:avg_daily_trips', 'customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'conv_rate_plus_100:conv_rate_plus_100', 'conv_rate_plus_100:conv_rate_plus_100_rounded', 'conv_rate_plus_100:conv_rate_plus_val_to_add', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    if job_from_df.supports_remote_storage_export():\n        files = job_from_df.to_remote_storage()\n        print(files)\n        assert len(files) > 0\n    start_time = datetime.utcnow()\n    actual_df_from_df_entities = job_from_df.to_df()\n    print(f'actual_df_from_df_entities shape: {actual_df_from_df_entities.shape}')\n    end_time = datetime.utcnow()\n    print(str(f\"Time to execute job_from_df.to_df() = '{end_time - start_time}'\\n\"))\n    assert sorted(expected_df.columns) == sorted(actual_df_from_df_entities.columns)\n    validate_dataframes(expected_df, actual_df_from_df_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    assert_feature_service_correctness(store, feature_service, full_feature_names, entity_df_with_request_data, expected_df, event_timestamp)\n    assert_feature_service_entity_mapping_correctness(store, feature_service_entity_mapping, full_feature_names, entity_df_with_request_data, full_expected_df, event_timestamp)\n    table_from_df_entities: pd.DataFrame = job_from_df.to_arrow().to_pandas()\n    validate_dataframes(expected_df, table_from_df_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
        "mutated": [
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: f'full:{v}')\ndef test_historical_features(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    entity_df_with_request_data = datasets.entity_df.copy(deep=True)\n    entity_df_with_request_data['val_to_add'] = [i for i in range(len(entity_df_with_request_data))]\n    entity_df_with_request_data['driver_age'] = [i + 100 for i in range(len(entity_df_with_request_data))]\n    feature_service = FeatureService(name='convrate_plus100', features=[feature_views.driver[['conv_rate']], feature_views.driver_odfv])\n    feature_service_entity_mapping = FeatureService(name='entity_mapping', features=[feature_views.location.with_name('origin').with_join_key_map({'location_id': 'origin_id'}), feature_views.location.with_name('destination').with_join_key_map({'location_id': 'destination_id'})])\n    store.apply([driver(), customer(), location(), feature_service, feature_service_entity_mapping, *feature_views.values()])\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL if DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL in datasets.orders_df.columns else 'e_ts'\n    full_expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df_with_request_data, event_timestamp, full_feature_names)\n    expected_df = full_expected_df.drop(columns=['origin__temperature', 'destination__temperature'])\n    job_from_df = store.get_historical_features(entity_df=entity_df_with_request_data, features=['driver_stats:conv_rate', 'driver_stats:avg_daily_trips', 'customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'conv_rate_plus_100:conv_rate_plus_100', 'conv_rate_plus_100:conv_rate_plus_100_rounded', 'conv_rate_plus_100:conv_rate_plus_val_to_add', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    if job_from_df.supports_remote_storage_export():\n        files = job_from_df.to_remote_storage()\n        print(files)\n        assert len(files) > 0\n    start_time = datetime.utcnow()\n    actual_df_from_df_entities = job_from_df.to_df()\n    print(f'actual_df_from_df_entities shape: {actual_df_from_df_entities.shape}')\n    end_time = datetime.utcnow()\n    print(str(f\"Time to execute job_from_df.to_df() = '{end_time - start_time}'\\n\"))\n    assert sorted(expected_df.columns) == sorted(actual_df_from_df_entities.columns)\n    validate_dataframes(expected_df, actual_df_from_df_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    assert_feature_service_correctness(store, feature_service, full_feature_names, entity_df_with_request_data, expected_df, event_timestamp)\n    assert_feature_service_entity_mapping_correctness(store, feature_service_entity_mapping, full_feature_names, entity_df_with_request_data, full_expected_df, event_timestamp)\n    table_from_df_entities: pd.DataFrame = job_from_df.to_arrow().to_pandas()\n    validate_dataframes(expected_df, table_from_df_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: f'full:{v}')\ndef test_historical_features(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    entity_df_with_request_data = datasets.entity_df.copy(deep=True)\n    entity_df_with_request_data['val_to_add'] = [i for i in range(len(entity_df_with_request_data))]\n    entity_df_with_request_data['driver_age'] = [i + 100 for i in range(len(entity_df_with_request_data))]\n    feature_service = FeatureService(name='convrate_plus100', features=[feature_views.driver[['conv_rate']], feature_views.driver_odfv])\n    feature_service_entity_mapping = FeatureService(name='entity_mapping', features=[feature_views.location.with_name('origin').with_join_key_map({'location_id': 'origin_id'}), feature_views.location.with_name('destination').with_join_key_map({'location_id': 'destination_id'})])\n    store.apply([driver(), customer(), location(), feature_service, feature_service_entity_mapping, *feature_views.values()])\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL if DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL in datasets.orders_df.columns else 'e_ts'\n    full_expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df_with_request_data, event_timestamp, full_feature_names)\n    expected_df = full_expected_df.drop(columns=['origin__temperature', 'destination__temperature'])\n    job_from_df = store.get_historical_features(entity_df=entity_df_with_request_data, features=['driver_stats:conv_rate', 'driver_stats:avg_daily_trips', 'customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'conv_rate_plus_100:conv_rate_plus_100', 'conv_rate_plus_100:conv_rate_plus_100_rounded', 'conv_rate_plus_100:conv_rate_plus_val_to_add', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    if job_from_df.supports_remote_storage_export():\n        files = job_from_df.to_remote_storage()\n        print(files)\n        assert len(files) > 0\n    start_time = datetime.utcnow()\n    actual_df_from_df_entities = job_from_df.to_df()\n    print(f'actual_df_from_df_entities shape: {actual_df_from_df_entities.shape}')\n    end_time = datetime.utcnow()\n    print(str(f\"Time to execute job_from_df.to_df() = '{end_time - start_time}'\\n\"))\n    assert sorted(expected_df.columns) == sorted(actual_df_from_df_entities.columns)\n    validate_dataframes(expected_df, actual_df_from_df_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    assert_feature_service_correctness(store, feature_service, full_feature_names, entity_df_with_request_data, expected_df, event_timestamp)\n    assert_feature_service_entity_mapping_correctness(store, feature_service_entity_mapping, full_feature_names, entity_df_with_request_data, full_expected_df, event_timestamp)\n    table_from_df_entities: pd.DataFrame = job_from_df.to_arrow().to_pandas()\n    validate_dataframes(expected_df, table_from_df_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: f'full:{v}')\ndef test_historical_features(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    entity_df_with_request_data = datasets.entity_df.copy(deep=True)\n    entity_df_with_request_data['val_to_add'] = [i for i in range(len(entity_df_with_request_data))]\n    entity_df_with_request_data['driver_age'] = [i + 100 for i in range(len(entity_df_with_request_data))]\n    feature_service = FeatureService(name='convrate_plus100', features=[feature_views.driver[['conv_rate']], feature_views.driver_odfv])\n    feature_service_entity_mapping = FeatureService(name='entity_mapping', features=[feature_views.location.with_name('origin').with_join_key_map({'location_id': 'origin_id'}), feature_views.location.with_name('destination').with_join_key_map({'location_id': 'destination_id'})])\n    store.apply([driver(), customer(), location(), feature_service, feature_service_entity_mapping, *feature_views.values()])\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL if DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL in datasets.orders_df.columns else 'e_ts'\n    full_expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df_with_request_data, event_timestamp, full_feature_names)\n    expected_df = full_expected_df.drop(columns=['origin__temperature', 'destination__temperature'])\n    job_from_df = store.get_historical_features(entity_df=entity_df_with_request_data, features=['driver_stats:conv_rate', 'driver_stats:avg_daily_trips', 'customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'conv_rate_plus_100:conv_rate_plus_100', 'conv_rate_plus_100:conv_rate_plus_100_rounded', 'conv_rate_plus_100:conv_rate_plus_val_to_add', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    if job_from_df.supports_remote_storage_export():\n        files = job_from_df.to_remote_storage()\n        print(files)\n        assert len(files) > 0\n    start_time = datetime.utcnow()\n    actual_df_from_df_entities = job_from_df.to_df()\n    print(f'actual_df_from_df_entities shape: {actual_df_from_df_entities.shape}')\n    end_time = datetime.utcnow()\n    print(str(f\"Time to execute job_from_df.to_df() = '{end_time - start_time}'\\n\"))\n    assert sorted(expected_df.columns) == sorted(actual_df_from_df_entities.columns)\n    validate_dataframes(expected_df, actual_df_from_df_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    assert_feature_service_correctness(store, feature_service, full_feature_names, entity_df_with_request_data, expected_df, event_timestamp)\n    assert_feature_service_entity_mapping_correctness(store, feature_service_entity_mapping, full_feature_names, entity_df_with_request_data, full_expected_df, event_timestamp)\n    table_from_df_entities: pd.DataFrame = job_from_df.to_arrow().to_pandas()\n    validate_dataframes(expected_df, table_from_df_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: f'full:{v}')\ndef test_historical_features(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    entity_df_with_request_data = datasets.entity_df.copy(deep=True)\n    entity_df_with_request_data['val_to_add'] = [i for i in range(len(entity_df_with_request_data))]\n    entity_df_with_request_data['driver_age'] = [i + 100 for i in range(len(entity_df_with_request_data))]\n    feature_service = FeatureService(name='convrate_plus100', features=[feature_views.driver[['conv_rate']], feature_views.driver_odfv])\n    feature_service_entity_mapping = FeatureService(name='entity_mapping', features=[feature_views.location.with_name('origin').with_join_key_map({'location_id': 'origin_id'}), feature_views.location.with_name('destination').with_join_key_map({'location_id': 'destination_id'})])\n    store.apply([driver(), customer(), location(), feature_service, feature_service_entity_mapping, *feature_views.values()])\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL if DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL in datasets.orders_df.columns else 'e_ts'\n    full_expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df_with_request_data, event_timestamp, full_feature_names)\n    expected_df = full_expected_df.drop(columns=['origin__temperature', 'destination__temperature'])\n    job_from_df = store.get_historical_features(entity_df=entity_df_with_request_data, features=['driver_stats:conv_rate', 'driver_stats:avg_daily_trips', 'customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'conv_rate_plus_100:conv_rate_plus_100', 'conv_rate_plus_100:conv_rate_plus_100_rounded', 'conv_rate_plus_100:conv_rate_plus_val_to_add', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    if job_from_df.supports_remote_storage_export():\n        files = job_from_df.to_remote_storage()\n        print(files)\n        assert len(files) > 0\n    start_time = datetime.utcnow()\n    actual_df_from_df_entities = job_from_df.to_df()\n    print(f'actual_df_from_df_entities shape: {actual_df_from_df_entities.shape}')\n    end_time = datetime.utcnow()\n    print(str(f\"Time to execute job_from_df.to_df() = '{end_time - start_time}'\\n\"))\n    assert sorted(expected_df.columns) == sorted(actual_df_from_df_entities.columns)\n    validate_dataframes(expected_df, actual_df_from_df_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    assert_feature_service_correctness(store, feature_service, full_feature_names, entity_df_with_request_data, expected_df, event_timestamp)\n    assert_feature_service_entity_mapping_correctness(store, feature_service_entity_mapping, full_feature_names, entity_df_with_request_data, full_expected_df, event_timestamp)\n    table_from_df_entities: pd.DataFrame = job_from_df.to_arrow().to_pandas()\n    validate_dataframes(expected_df, table_from_df_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: f'full:{v}')\ndef test_historical_features(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    entity_df_with_request_data = datasets.entity_df.copy(deep=True)\n    entity_df_with_request_data['val_to_add'] = [i for i in range(len(entity_df_with_request_data))]\n    entity_df_with_request_data['driver_age'] = [i + 100 for i in range(len(entity_df_with_request_data))]\n    feature_service = FeatureService(name='convrate_plus100', features=[feature_views.driver[['conv_rate']], feature_views.driver_odfv])\n    feature_service_entity_mapping = FeatureService(name='entity_mapping', features=[feature_views.location.with_name('origin').with_join_key_map({'location_id': 'origin_id'}), feature_views.location.with_name('destination').with_join_key_map({'location_id': 'destination_id'})])\n    store.apply([driver(), customer(), location(), feature_service, feature_service_entity_mapping, *feature_views.values()])\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL if DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL in datasets.orders_df.columns else 'e_ts'\n    full_expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df_with_request_data, event_timestamp, full_feature_names)\n    expected_df = full_expected_df.drop(columns=['origin__temperature', 'destination__temperature'])\n    job_from_df = store.get_historical_features(entity_df=entity_df_with_request_data, features=['driver_stats:conv_rate', 'driver_stats:avg_daily_trips', 'customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'conv_rate_plus_100:conv_rate_plus_100', 'conv_rate_plus_100:conv_rate_plus_100_rounded', 'conv_rate_plus_100:conv_rate_plus_val_to_add', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    if job_from_df.supports_remote_storage_export():\n        files = job_from_df.to_remote_storage()\n        print(files)\n        assert len(files) > 0\n    start_time = datetime.utcnow()\n    actual_df_from_df_entities = job_from_df.to_df()\n    print(f'actual_df_from_df_entities shape: {actual_df_from_df_entities.shape}')\n    end_time = datetime.utcnow()\n    print(str(f\"Time to execute job_from_df.to_df() = '{end_time - start_time}'\\n\"))\n    assert sorted(expected_df.columns) == sorted(actual_df_from_df_entities.columns)\n    validate_dataframes(expected_df, actual_df_from_df_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    assert_feature_service_correctness(store, feature_service, full_feature_names, entity_df_with_request_data, expected_df, event_timestamp)\n    assert_feature_service_entity_mapping_correctness(store, feature_service_entity_mapping, full_feature_names, entity_df_with_request_data, full_expected_df, event_timestamp)\n    table_from_df_entities: pd.DataFrame = job_from_df.to_arrow().to_pandas()\n    validate_dataframes(expected_df, table_from_df_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))"
        ]
    },
    {
        "func_name": "test_historical_features_with_shared_batch_source",
        "original": "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_shared_batch_source(environment, universal_data_sources, full_feature_names):\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    driver_entity = driver()\n    driver_stats_v1 = FeatureView(name='driver_stats_v1', entities=[driver_entity], schema=[Field(name='avg_daily_trips', dtype=Int32)], source=data_sources.driver)\n    driver_stats_v2 = FeatureView(name='driver_stats_v2', entities=[driver_entity], schema=[Field(name='avg_daily_trips', dtype=Int32), Field(name='conv_rate', dtype=Float32)], source=data_sources.driver)\n    store.apply([driver_entity, driver_stats_v1, driver_stats_v2])\n    with pytest.raises(KeyError):\n        store.get_historical_features(entity_df=datasets.entity_df, features=['driver_stats_v1:conv_rate'], full_feature_names=full_feature_names).to_df()",
        "mutated": [
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_shared_batch_source(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    driver_entity = driver()\n    driver_stats_v1 = FeatureView(name='driver_stats_v1', entities=[driver_entity], schema=[Field(name='avg_daily_trips', dtype=Int32)], source=data_sources.driver)\n    driver_stats_v2 = FeatureView(name='driver_stats_v2', entities=[driver_entity], schema=[Field(name='avg_daily_trips', dtype=Int32), Field(name='conv_rate', dtype=Float32)], source=data_sources.driver)\n    store.apply([driver_entity, driver_stats_v1, driver_stats_v2])\n    with pytest.raises(KeyError):\n        store.get_historical_features(entity_df=datasets.entity_df, features=['driver_stats_v1:conv_rate'], full_feature_names=full_feature_names).to_df()",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_shared_batch_source(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    driver_entity = driver()\n    driver_stats_v1 = FeatureView(name='driver_stats_v1', entities=[driver_entity], schema=[Field(name='avg_daily_trips', dtype=Int32)], source=data_sources.driver)\n    driver_stats_v2 = FeatureView(name='driver_stats_v2', entities=[driver_entity], schema=[Field(name='avg_daily_trips', dtype=Int32), Field(name='conv_rate', dtype=Float32)], source=data_sources.driver)\n    store.apply([driver_entity, driver_stats_v1, driver_stats_v2])\n    with pytest.raises(KeyError):\n        store.get_historical_features(entity_df=datasets.entity_df, features=['driver_stats_v1:conv_rate'], full_feature_names=full_feature_names).to_df()",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_shared_batch_source(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    driver_entity = driver()\n    driver_stats_v1 = FeatureView(name='driver_stats_v1', entities=[driver_entity], schema=[Field(name='avg_daily_trips', dtype=Int32)], source=data_sources.driver)\n    driver_stats_v2 = FeatureView(name='driver_stats_v2', entities=[driver_entity], schema=[Field(name='avg_daily_trips', dtype=Int32), Field(name='conv_rate', dtype=Float32)], source=data_sources.driver)\n    store.apply([driver_entity, driver_stats_v1, driver_stats_v2])\n    with pytest.raises(KeyError):\n        store.get_historical_features(entity_df=datasets.entity_df, features=['driver_stats_v1:conv_rate'], full_feature_names=full_feature_names).to_df()",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_shared_batch_source(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    driver_entity = driver()\n    driver_stats_v1 = FeatureView(name='driver_stats_v1', entities=[driver_entity], schema=[Field(name='avg_daily_trips', dtype=Int32)], source=data_sources.driver)\n    driver_stats_v2 = FeatureView(name='driver_stats_v2', entities=[driver_entity], schema=[Field(name='avg_daily_trips', dtype=Int32), Field(name='conv_rate', dtype=Float32)], source=data_sources.driver)\n    store.apply([driver_entity, driver_stats_v1, driver_stats_v2])\n    with pytest.raises(KeyError):\n        store.get_historical_features(entity_df=datasets.entity_df, features=['driver_stats_v1:conv_rate'], full_feature_names=full_feature_names).to_df()",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_shared_batch_source(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    driver_entity = driver()\n    driver_stats_v1 = FeatureView(name='driver_stats_v1', entities=[driver_entity], schema=[Field(name='avg_daily_trips', dtype=Int32)], source=data_sources.driver)\n    driver_stats_v2 = FeatureView(name='driver_stats_v2', entities=[driver_entity], schema=[Field(name='avg_daily_trips', dtype=Int32), Field(name='conv_rate', dtype=Float32)], source=data_sources.driver)\n    store.apply([driver_entity, driver_stats_v1, driver_stats_v2])\n    with pytest.raises(KeyError):\n        store.get_historical_features(entity_df=datasets.entity_df, features=['driver_stats_v1:conv_rate'], full_feature_names=full_feature_names).to_df()"
        ]
    },
    {
        "func_name": "test_historical_features_with_missing_request_data",
        "original": "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\ndef test_historical_features_with_missing_request_data(environment, universal_data_sources):\n    store = environment.feature_store\n    (_, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    with pytest.raises(RequestDataNotFoundInEntityDfException):\n        store.get_historical_features(entity_df=datasets.entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'conv_rate_plus_100:conv_rate_plus_100', 'conv_rate_plus_100:conv_rate_plus_val_to_add', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=True)",
        "mutated": [
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\ndef test_historical_features_with_missing_request_data(environment, universal_data_sources):\n    if False:\n        i = 10\n    store = environment.feature_store\n    (_, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    with pytest.raises(RequestDataNotFoundInEntityDfException):\n        store.get_historical_features(entity_df=datasets.entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'conv_rate_plus_100:conv_rate_plus_100', 'conv_rate_plus_100:conv_rate_plus_val_to_add', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=True)",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\ndef test_historical_features_with_missing_request_data(environment, universal_data_sources):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    store = environment.feature_store\n    (_, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    with pytest.raises(RequestDataNotFoundInEntityDfException):\n        store.get_historical_features(entity_df=datasets.entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'conv_rate_plus_100:conv_rate_plus_100', 'conv_rate_plus_100:conv_rate_plus_val_to_add', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=True)",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\ndef test_historical_features_with_missing_request_data(environment, universal_data_sources):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    store = environment.feature_store\n    (_, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    with pytest.raises(RequestDataNotFoundInEntityDfException):\n        store.get_historical_features(entity_df=datasets.entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'conv_rate_plus_100:conv_rate_plus_100', 'conv_rate_plus_100:conv_rate_plus_val_to_add', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=True)",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\ndef test_historical_features_with_missing_request_data(environment, universal_data_sources):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    store = environment.feature_store\n    (_, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    with pytest.raises(RequestDataNotFoundInEntityDfException):\n        store.get_historical_features(entity_df=datasets.entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'conv_rate_plus_100:conv_rate_plus_100', 'conv_rate_plus_100:conv_rate_plus_val_to_add', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=True)",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\ndef test_historical_features_with_missing_request_data(environment, universal_data_sources):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    store = environment.feature_store\n    (_, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    with pytest.raises(RequestDataNotFoundInEntityDfException):\n        store.get_historical_features(entity_df=datasets.entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'conv_rate_plus_100:conv_rate_plus_100', 'conv_rate_plus_100:conv_rate_plus_val_to_add', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=True)"
        ]
    },
    {
        "func_name": "test_historical_features_with_entities_from_query",
        "original": "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_entities_from_query(environment, universal_data_sources, full_feature_names):\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    orders_table = table_name_from_data_source(data_sources.orders)\n    if not orders_table:\n        raise pytest.skip('Offline source is not sql-based')\n    data_source_creator = environment.test_repo_config.offline_store_creator\n    if data_source_creator.__name__ == SnowflakeDataSourceCreator.__name__:\n        entity_df_query = f'\\n        SELECT \"customer_id\", \"driver_id\", \"order_id\", \"origin_id\", \"destination_id\", \"event_timestamp\"\\n        FROM \"{orders_table}\"\\n        '\n    else:\n        entity_df_query = f'\\n        SELECT customer_id, driver_id, order_id, origin_id, destination_id, event_timestamp\\n        FROM {orders_table}\\n        '\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    job_from_sql = store.get_historical_features(entity_df=entity_df_query, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    start_time = datetime.utcnow()\n    actual_df_from_sql_entities = job_from_sql.to_df()\n    end_time = datetime.utcnow()\n    print(str(f\"\\nTime to execute job_from_sql.to_df() = '{end_time - start_time}'\"))\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL if DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL in datasets.orders_df.columns else 'e_ts'\n    full_expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, datasets.entity_df, event_timestamp, full_feature_names)\n    expected_df_query = full_expected_df.drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df_query, actual_df_from_sql_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    table_from_sql_entities = job_from_sql.to_arrow().to_pandas()\n    for col in table_from_sql_entities.columns:\n        expected_df_query[col] = expected_df_query[col].astype(table_from_sql_entities[col].dtype)\n    validate_dataframes(expected_df_query, table_from_sql_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
        "mutated": [
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_entities_from_query(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    orders_table = table_name_from_data_source(data_sources.orders)\n    if not orders_table:\n        raise pytest.skip('Offline source is not sql-based')\n    data_source_creator = environment.test_repo_config.offline_store_creator\n    if data_source_creator.__name__ == SnowflakeDataSourceCreator.__name__:\n        entity_df_query = f'\\n        SELECT \"customer_id\", \"driver_id\", \"order_id\", \"origin_id\", \"destination_id\", \"event_timestamp\"\\n        FROM \"{orders_table}\"\\n        '\n    else:\n        entity_df_query = f'\\n        SELECT customer_id, driver_id, order_id, origin_id, destination_id, event_timestamp\\n        FROM {orders_table}\\n        '\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    job_from_sql = store.get_historical_features(entity_df=entity_df_query, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    start_time = datetime.utcnow()\n    actual_df_from_sql_entities = job_from_sql.to_df()\n    end_time = datetime.utcnow()\n    print(str(f\"\\nTime to execute job_from_sql.to_df() = '{end_time - start_time}'\"))\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL if DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL in datasets.orders_df.columns else 'e_ts'\n    full_expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, datasets.entity_df, event_timestamp, full_feature_names)\n    expected_df_query = full_expected_df.drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df_query, actual_df_from_sql_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    table_from_sql_entities = job_from_sql.to_arrow().to_pandas()\n    for col in table_from_sql_entities.columns:\n        expected_df_query[col] = expected_df_query[col].astype(table_from_sql_entities[col].dtype)\n    validate_dataframes(expected_df_query, table_from_sql_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_entities_from_query(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    orders_table = table_name_from_data_source(data_sources.orders)\n    if not orders_table:\n        raise pytest.skip('Offline source is not sql-based')\n    data_source_creator = environment.test_repo_config.offline_store_creator\n    if data_source_creator.__name__ == SnowflakeDataSourceCreator.__name__:\n        entity_df_query = f'\\n        SELECT \"customer_id\", \"driver_id\", \"order_id\", \"origin_id\", \"destination_id\", \"event_timestamp\"\\n        FROM \"{orders_table}\"\\n        '\n    else:\n        entity_df_query = f'\\n        SELECT customer_id, driver_id, order_id, origin_id, destination_id, event_timestamp\\n        FROM {orders_table}\\n        '\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    job_from_sql = store.get_historical_features(entity_df=entity_df_query, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    start_time = datetime.utcnow()\n    actual_df_from_sql_entities = job_from_sql.to_df()\n    end_time = datetime.utcnow()\n    print(str(f\"\\nTime to execute job_from_sql.to_df() = '{end_time - start_time}'\"))\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL if DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL in datasets.orders_df.columns else 'e_ts'\n    full_expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, datasets.entity_df, event_timestamp, full_feature_names)\n    expected_df_query = full_expected_df.drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df_query, actual_df_from_sql_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    table_from_sql_entities = job_from_sql.to_arrow().to_pandas()\n    for col in table_from_sql_entities.columns:\n        expected_df_query[col] = expected_df_query[col].astype(table_from_sql_entities[col].dtype)\n    validate_dataframes(expected_df_query, table_from_sql_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_entities_from_query(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    orders_table = table_name_from_data_source(data_sources.orders)\n    if not orders_table:\n        raise pytest.skip('Offline source is not sql-based')\n    data_source_creator = environment.test_repo_config.offline_store_creator\n    if data_source_creator.__name__ == SnowflakeDataSourceCreator.__name__:\n        entity_df_query = f'\\n        SELECT \"customer_id\", \"driver_id\", \"order_id\", \"origin_id\", \"destination_id\", \"event_timestamp\"\\n        FROM \"{orders_table}\"\\n        '\n    else:\n        entity_df_query = f'\\n        SELECT customer_id, driver_id, order_id, origin_id, destination_id, event_timestamp\\n        FROM {orders_table}\\n        '\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    job_from_sql = store.get_historical_features(entity_df=entity_df_query, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    start_time = datetime.utcnow()\n    actual_df_from_sql_entities = job_from_sql.to_df()\n    end_time = datetime.utcnow()\n    print(str(f\"\\nTime to execute job_from_sql.to_df() = '{end_time - start_time}'\"))\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL if DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL in datasets.orders_df.columns else 'e_ts'\n    full_expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, datasets.entity_df, event_timestamp, full_feature_names)\n    expected_df_query = full_expected_df.drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df_query, actual_df_from_sql_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    table_from_sql_entities = job_from_sql.to_arrow().to_pandas()\n    for col in table_from_sql_entities.columns:\n        expected_df_query[col] = expected_df_query[col].astype(table_from_sql_entities[col].dtype)\n    validate_dataframes(expected_df_query, table_from_sql_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_entities_from_query(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    orders_table = table_name_from_data_source(data_sources.orders)\n    if not orders_table:\n        raise pytest.skip('Offline source is not sql-based')\n    data_source_creator = environment.test_repo_config.offline_store_creator\n    if data_source_creator.__name__ == SnowflakeDataSourceCreator.__name__:\n        entity_df_query = f'\\n        SELECT \"customer_id\", \"driver_id\", \"order_id\", \"origin_id\", \"destination_id\", \"event_timestamp\"\\n        FROM \"{orders_table}\"\\n        '\n    else:\n        entity_df_query = f'\\n        SELECT customer_id, driver_id, order_id, origin_id, destination_id, event_timestamp\\n        FROM {orders_table}\\n        '\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    job_from_sql = store.get_historical_features(entity_df=entity_df_query, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    start_time = datetime.utcnow()\n    actual_df_from_sql_entities = job_from_sql.to_df()\n    end_time = datetime.utcnow()\n    print(str(f\"\\nTime to execute job_from_sql.to_df() = '{end_time - start_time}'\"))\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL if DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL in datasets.orders_df.columns else 'e_ts'\n    full_expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, datasets.entity_df, event_timestamp, full_feature_names)\n    expected_df_query = full_expected_df.drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df_query, actual_df_from_sql_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    table_from_sql_entities = job_from_sql.to_arrow().to_pandas()\n    for col in table_from_sql_entities.columns:\n        expected_df_query[col] = expected_df_query[col].astype(table_from_sql_entities[col].dtype)\n    validate_dataframes(expected_df_query, table_from_sql_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_entities_from_query(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    orders_table = table_name_from_data_source(data_sources.orders)\n    if not orders_table:\n        raise pytest.skip('Offline source is not sql-based')\n    data_source_creator = environment.test_repo_config.offline_store_creator\n    if data_source_creator.__name__ == SnowflakeDataSourceCreator.__name__:\n        entity_df_query = f'\\n        SELECT \"customer_id\", \"driver_id\", \"order_id\", \"origin_id\", \"destination_id\", \"event_timestamp\"\\n        FROM \"{orders_table}\"\\n        '\n    else:\n        entity_df_query = f'\\n        SELECT customer_id, driver_id, order_id, origin_id, destination_id, event_timestamp\\n        FROM {orders_table}\\n        '\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    job_from_sql = store.get_historical_features(entity_df=entity_df_query, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    start_time = datetime.utcnow()\n    actual_df_from_sql_entities = job_from_sql.to_df()\n    end_time = datetime.utcnow()\n    print(str(f\"\\nTime to execute job_from_sql.to_df() = '{end_time - start_time}'\"))\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL if DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL in datasets.orders_df.columns else 'e_ts'\n    full_expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, datasets.entity_df, event_timestamp, full_feature_names)\n    expected_df_query = full_expected_df.drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df_query, actual_df_from_sql_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    table_from_sql_entities = job_from_sql.to_arrow().to_pandas()\n    for col in table_from_sql_entities.columns:\n        expected_df_query[col] = expected_df_query[col].astype(table_from_sql_entities[col].dtype)\n    validate_dataframes(expected_df_query, table_from_sql_entities, sort_by=[event_timestamp, 'order_id', 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))"
        ]
    },
    {
        "func_name": "test_historical_features_persisting",
        "original": "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_persisting(environment, universal_data_sources, full_feature_names):\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    entity_df = datasets.entity_df.drop(columns=['order_id', 'origin_id', 'destination_id'])\n    job = store.get_historical_features(entity_df=entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    saved_dataset = store.create_saved_dataset(from_=job, name='saved_dataset', storage=environment.data_source_creator.create_saved_dataset_destination(), tags={'env': 'test'}, allow_overwrite=True)\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL\n    expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df, event_timestamp, full_feature_names).drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df, saved_dataset.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    validate_dataframes(job.to_df(), saved_dataset.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
        "mutated": [
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_persisting(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    entity_df = datasets.entity_df.drop(columns=['order_id', 'origin_id', 'destination_id'])\n    job = store.get_historical_features(entity_df=entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    saved_dataset = store.create_saved_dataset(from_=job, name='saved_dataset', storage=environment.data_source_creator.create_saved_dataset_destination(), tags={'env': 'test'}, allow_overwrite=True)\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL\n    expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df, event_timestamp, full_feature_names).drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df, saved_dataset.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    validate_dataframes(job.to_df(), saved_dataset.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_persisting(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    entity_df = datasets.entity_df.drop(columns=['order_id', 'origin_id', 'destination_id'])\n    job = store.get_historical_features(entity_df=entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    saved_dataset = store.create_saved_dataset(from_=job, name='saved_dataset', storage=environment.data_source_creator.create_saved_dataset_destination(), tags={'env': 'test'}, allow_overwrite=True)\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL\n    expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df, event_timestamp, full_feature_names).drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df, saved_dataset.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    validate_dataframes(job.to_df(), saved_dataset.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_persisting(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    entity_df = datasets.entity_df.drop(columns=['order_id', 'origin_id', 'destination_id'])\n    job = store.get_historical_features(entity_df=entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    saved_dataset = store.create_saved_dataset(from_=job, name='saved_dataset', storage=environment.data_source_creator.create_saved_dataset_destination(), tags={'env': 'test'}, allow_overwrite=True)\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL\n    expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df, event_timestamp, full_feature_names).drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df, saved_dataset.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    validate_dataframes(job.to_df(), saved_dataset.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_persisting(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    entity_df = datasets.entity_df.drop(columns=['order_id', 'origin_id', 'destination_id'])\n    job = store.get_historical_features(entity_df=entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    saved_dataset = store.create_saved_dataset(from_=job, name='saved_dataset', storage=environment.data_source_creator.create_saved_dataset_destination(), tags={'env': 'test'}, allow_overwrite=True)\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL\n    expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df, event_timestamp, full_feature_names).drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df, saved_dataset.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    validate_dataframes(job.to_df(), saved_dataset.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_persisting(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    entity_df = datasets.entity_df.drop(columns=['order_id', 'origin_id', 'destination_id'])\n    job = store.get_historical_features(entity_df=entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    saved_dataset = store.create_saved_dataset(from_=job, name='saved_dataset', storage=environment.data_source_creator.create_saved_dataset_destination(), tags={'env': 'test'}, allow_overwrite=True)\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL\n    expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df, event_timestamp, full_feature_names).drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df, saved_dataset.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))\n    validate_dataframes(job.to_df(), saved_dataset.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))"
        ]
    },
    {
        "func_name": "test_historical_features_with_no_ttl",
        "original": "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_no_ttl(environment, universal_data_sources, full_feature_names):\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    feature_views.customer.ttl = timedelta(seconds=0)\n    feature_views.order.ttl = timedelta(seconds=0)\n    feature_views.global_fv.ttl = timedelta(seconds=0)\n    feature_views.field_mapping.ttl = timedelta(seconds=0)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    entity_df = datasets.entity_df.drop(columns=['order_id', 'origin_id', 'destination_id'])\n    job = store.get_historical_features(entity_df=entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL\n    expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df, event_timestamp, full_feature_names).drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df, job.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
        "mutated": [
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_no_ttl(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    feature_views.customer.ttl = timedelta(seconds=0)\n    feature_views.order.ttl = timedelta(seconds=0)\n    feature_views.global_fv.ttl = timedelta(seconds=0)\n    feature_views.field_mapping.ttl = timedelta(seconds=0)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    entity_df = datasets.entity_df.drop(columns=['order_id', 'origin_id', 'destination_id'])\n    job = store.get_historical_features(entity_df=entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL\n    expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df, event_timestamp, full_feature_names).drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df, job.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_no_ttl(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    feature_views.customer.ttl = timedelta(seconds=0)\n    feature_views.order.ttl = timedelta(seconds=0)\n    feature_views.global_fv.ttl = timedelta(seconds=0)\n    feature_views.field_mapping.ttl = timedelta(seconds=0)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    entity_df = datasets.entity_df.drop(columns=['order_id', 'origin_id', 'destination_id'])\n    job = store.get_historical_features(entity_df=entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL\n    expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df, event_timestamp, full_feature_names).drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df, job.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_no_ttl(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    feature_views.customer.ttl = timedelta(seconds=0)\n    feature_views.order.ttl = timedelta(seconds=0)\n    feature_views.global_fv.ttl = timedelta(seconds=0)\n    feature_views.field_mapping.ttl = timedelta(seconds=0)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    entity_df = datasets.entity_df.drop(columns=['order_id', 'origin_id', 'destination_id'])\n    job = store.get_historical_features(entity_df=entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL\n    expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df, event_timestamp, full_feature_names).drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df, job.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_no_ttl(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    feature_views.customer.ttl = timedelta(seconds=0)\n    feature_views.order.ttl = timedelta(seconds=0)\n    feature_views.global_fv.ttl = timedelta(seconds=0)\n    feature_views.field_mapping.ttl = timedelta(seconds=0)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    entity_df = datasets.entity_df.drop(columns=['order_id', 'origin_id', 'destination_id'])\n    job = store.get_historical_features(entity_df=entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL\n    expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df, event_timestamp, full_feature_names).drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df, job.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\n@pytest.mark.parametrize('full_feature_names', [True, False], ids=lambda v: str(v))\ndef test_historical_features_with_no_ttl(environment, universal_data_sources, full_feature_names):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    store = environment.feature_store\n    (entities, datasets, data_sources) = universal_data_sources\n    feature_views = construct_universal_feature_views(data_sources)\n    feature_views.customer.ttl = timedelta(seconds=0)\n    feature_views.order.ttl = timedelta(seconds=0)\n    feature_views.global_fv.ttl = timedelta(seconds=0)\n    feature_views.field_mapping.ttl = timedelta(seconds=0)\n    store.apply([driver(), customer(), location(), *feature_views.values()])\n    entity_df = datasets.entity_df.drop(columns=['order_id', 'origin_id', 'destination_id'])\n    job = store.get_historical_features(entity_df=entity_df, features=['customer_profile:current_balance', 'customer_profile:avg_passenger_count', 'customer_profile:lifetime_trip_count', 'order:order_is_success', 'global_stats:num_rides', 'global_stats:avg_ride_length', 'field_mapping:feature_name'], full_feature_names=full_feature_names)\n    event_timestamp = DEFAULT_ENTITY_DF_EVENT_TIMESTAMP_COL\n    expected_df = get_expected_training_df(datasets.customer_df, feature_views.customer, datasets.driver_df, feature_views.driver, datasets.orders_df, feature_views.order, datasets.location_df, feature_views.location, datasets.global_df, feature_views.global_fv, datasets.field_mapping_df, feature_views.field_mapping, entity_df, event_timestamp, full_feature_names).drop(columns=[get_response_feature_name('conv_rate_plus_100', full_feature_names), get_response_feature_name('conv_rate_plus_100_rounded', full_feature_names), get_response_feature_name('avg_daily_trips', full_feature_names), get_response_feature_name('conv_rate', full_feature_names), 'origin__temperature', 'destination__temperature'])\n    validate_dataframes(expected_df, job.to_df(), sort_by=[event_timestamp, 'driver_id', 'customer_id'], event_timestamp_column=event_timestamp, timestamp_precision=timedelta(milliseconds=1))"
        ]
    },
    {
        "func_name": "test_historical_features_from_bigquery_sources_containing_backfills",
        "original": "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\ndef test_historical_features_from_bigquery_sources_containing_backfills(environment):\n    store = environment.feature_store\n    now = datetime.now().replace(microsecond=0, second=0, minute=0)\n    tomorrow = now + timedelta(days=1)\n    day_after_tomorrow = now + timedelta(days=2)\n    entity_df = pd.DataFrame(data=[{'driver_id': 1001, 'event_timestamp': day_after_tomorrow}, {'driver_id': 1002, 'event_timestamp': day_after_tomorrow}])\n    driver_stats_df = pd.DataFrame(data=[{'driver_id': 1001, 'avg_daily_trips': 10, 'event_timestamp': now, 'created': now}, {'driver_id': 1001, 'avg_daily_trips': 20, 'event_timestamp': now, 'created': tomorrow}, {'driver_id': 1002, 'avg_daily_trips': 30, 'event_timestamp': now, 'created': tomorrow}, {'driver_id': 1002, 'avg_daily_trips': 40, 'event_timestamp': tomorrow, 'created': now}])\n    expected_df = pd.DataFrame(data=[{'driver_id': 1001, 'event_timestamp': day_after_tomorrow, 'avg_daily_trips': 20}, {'driver_id': 1002, 'event_timestamp': day_after_tomorrow, 'avg_daily_trips': 40}])\n    driver_stats_data_source = environment.data_source_creator.create_data_source(df=driver_stats_df, destination_name=f'test_driver_stats_{int(time.time_ns())}_{random.randint(1000, 9999)}', timestamp_field='event_timestamp', created_timestamp_column='created')\n    driver = Entity(name='driver', join_keys=['driver_id'])\n    driver_fv = FeatureView(name='driver_stats', entities=[driver], schema=[Field(name='avg_daily_trips', dtype=Int32)], source=driver_stats_data_source)\n    store.apply([driver, driver_fv])\n    offline_job = store.get_historical_features(entity_df=entity_df, features=['driver_stats:avg_daily_trips'], full_feature_names=False)\n    start_time = datetime.utcnow()\n    actual_df = offline_job.to_df()\n    print(f'actual_df shape: {actual_df.shape}')\n    end_time = datetime.utcnow()\n    print(str(f\"Time to execute job_from_df.to_df() = '{end_time - start_time}'\\n\"))\n    assert sorted(expected_df.columns) == sorted(actual_df.columns)\n    validate_dataframes(expected_df, actual_df, sort_by=['driver_id'])",
        "mutated": [
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\ndef test_historical_features_from_bigquery_sources_containing_backfills(environment):\n    if False:\n        i = 10\n    store = environment.feature_store\n    now = datetime.now().replace(microsecond=0, second=0, minute=0)\n    tomorrow = now + timedelta(days=1)\n    day_after_tomorrow = now + timedelta(days=2)\n    entity_df = pd.DataFrame(data=[{'driver_id': 1001, 'event_timestamp': day_after_tomorrow}, {'driver_id': 1002, 'event_timestamp': day_after_tomorrow}])\n    driver_stats_df = pd.DataFrame(data=[{'driver_id': 1001, 'avg_daily_trips': 10, 'event_timestamp': now, 'created': now}, {'driver_id': 1001, 'avg_daily_trips': 20, 'event_timestamp': now, 'created': tomorrow}, {'driver_id': 1002, 'avg_daily_trips': 30, 'event_timestamp': now, 'created': tomorrow}, {'driver_id': 1002, 'avg_daily_trips': 40, 'event_timestamp': tomorrow, 'created': now}])\n    expected_df = pd.DataFrame(data=[{'driver_id': 1001, 'event_timestamp': day_after_tomorrow, 'avg_daily_trips': 20}, {'driver_id': 1002, 'event_timestamp': day_after_tomorrow, 'avg_daily_trips': 40}])\n    driver_stats_data_source = environment.data_source_creator.create_data_source(df=driver_stats_df, destination_name=f'test_driver_stats_{int(time.time_ns())}_{random.randint(1000, 9999)}', timestamp_field='event_timestamp', created_timestamp_column='created')\n    driver = Entity(name='driver', join_keys=['driver_id'])\n    driver_fv = FeatureView(name='driver_stats', entities=[driver], schema=[Field(name='avg_daily_trips', dtype=Int32)], source=driver_stats_data_source)\n    store.apply([driver, driver_fv])\n    offline_job = store.get_historical_features(entity_df=entity_df, features=['driver_stats:avg_daily_trips'], full_feature_names=False)\n    start_time = datetime.utcnow()\n    actual_df = offline_job.to_df()\n    print(f'actual_df shape: {actual_df.shape}')\n    end_time = datetime.utcnow()\n    print(str(f\"Time to execute job_from_df.to_df() = '{end_time - start_time}'\\n\"))\n    assert sorted(expected_df.columns) == sorted(actual_df.columns)\n    validate_dataframes(expected_df, actual_df, sort_by=['driver_id'])",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\ndef test_historical_features_from_bigquery_sources_containing_backfills(environment):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    store = environment.feature_store\n    now = datetime.now().replace(microsecond=0, second=0, minute=0)\n    tomorrow = now + timedelta(days=1)\n    day_after_tomorrow = now + timedelta(days=2)\n    entity_df = pd.DataFrame(data=[{'driver_id': 1001, 'event_timestamp': day_after_tomorrow}, {'driver_id': 1002, 'event_timestamp': day_after_tomorrow}])\n    driver_stats_df = pd.DataFrame(data=[{'driver_id': 1001, 'avg_daily_trips': 10, 'event_timestamp': now, 'created': now}, {'driver_id': 1001, 'avg_daily_trips': 20, 'event_timestamp': now, 'created': tomorrow}, {'driver_id': 1002, 'avg_daily_trips': 30, 'event_timestamp': now, 'created': tomorrow}, {'driver_id': 1002, 'avg_daily_trips': 40, 'event_timestamp': tomorrow, 'created': now}])\n    expected_df = pd.DataFrame(data=[{'driver_id': 1001, 'event_timestamp': day_after_tomorrow, 'avg_daily_trips': 20}, {'driver_id': 1002, 'event_timestamp': day_after_tomorrow, 'avg_daily_trips': 40}])\n    driver_stats_data_source = environment.data_source_creator.create_data_source(df=driver_stats_df, destination_name=f'test_driver_stats_{int(time.time_ns())}_{random.randint(1000, 9999)}', timestamp_field='event_timestamp', created_timestamp_column='created')\n    driver = Entity(name='driver', join_keys=['driver_id'])\n    driver_fv = FeatureView(name='driver_stats', entities=[driver], schema=[Field(name='avg_daily_trips', dtype=Int32)], source=driver_stats_data_source)\n    store.apply([driver, driver_fv])\n    offline_job = store.get_historical_features(entity_df=entity_df, features=['driver_stats:avg_daily_trips'], full_feature_names=False)\n    start_time = datetime.utcnow()\n    actual_df = offline_job.to_df()\n    print(f'actual_df shape: {actual_df.shape}')\n    end_time = datetime.utcnow()\n    print(str(f\"Time to execute job_from_df.to_df() = '{end_time - start_time}'\\n\"))\n    assert sorted(expected_df.columns) == sorted(actual_df.columns)\n    validate_dataframes(expected_df, actual_df, sort_by=['driver_id'])",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\ndef test_historical_features_from_bigquery_sources_containing_backfills(environment):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    store = environment.feature_store\n    now = datetime.now().replace(microsecond=0, second=0, minute=0)\n    tomorrow = now + timedelta(days=1)\n    day_after_tomorrow = now + timedelta(days=2)\n    entity_df = pd.DataFrame(data=[{'driver_id': 1001, 'event_timestamp': day_after_tomorrow}, {'driver_id': 1002, 'event_timestamp': day_after_tomorrow}])\n    driver_stats_df = pd.DataFrame(data=[{'driver_id': 1001, 'avg_daily_trips': 10, 'event_timestamp': now, 'created': now}, {'driver_id': 1001, 'avg_daily_trips': 20, 'event_timestamp': now, 'created': tomorrow}, {'driver_id': 1002, 'avg_daily_trips': 30, 'event_timestamp': now, 'created': tomorrow}, {'driver_id': 1002, 'avg_daily_trips': 40, 'event_timestamp': tomorrow, 'created': now}])\n    expected_df = pd.DataFrame(data=[{'driver_id': 1001, 'event_timestamp': day_after_tomorrow, 'avg_daily_trips': 20}, {'driver_id': 1002, 'event_timestamp': day_after_tomorrow, 'avg_daily_trips': 40}])\n    driver_stats_data_source = environment.data_source_creator.create_data_source(df=driver_stats_df, destination_name=f'test_driver_stats_{int(time.time_ns())}_{random.randint(1000, 9999)}', timestamp_field='event_timestamp', created_timestamp_column='created')\n    driver = Entity(name='driver', join_keys=['driver_id'])\n    driver_fv = FeatureView(name='driver_stats', entities=[driver], schema=[Field(name='avg_daily_trips', dtype=Int32)], source=driver_stats_data_source)\n    store.apply([driver, driver_fv])\n    offline_job = store.get_historical_features(entity_df=entity_df, features=['driver_stats:avg_daily_trips'], full_feature_names=False)\n    start_time = datetime.utcnow()\n    actual_df = offline_job.to_df()\n    print(f'actual_df shape: {actual_df.shape}')\n    end_time = datetime.utcnow()\n    print(str(f\"Time to execute job_from_df.to_df() = '{end_time - start_time}'\\n\"))\n    assert sorted(expected_df.columns) == sorted(actual_df.columns)\n    validate_dataframes(expected_df, actual_df, sort_by=['driver_id'])",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\ndef test_historical_features_from_bigquery_sources_containing_backfills(environment):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    store = environment.feature_store\n    now = datetime.now().replace(microsecond=0, second=0, minute=0)\n    tomorrow = now + timedelta(days=1)\n    day_after_tomorrow = now + timedelta(days=2)\n    entity_df = pd.DataFrame(data=[{'driver_id': 1001, 'event_timestamp': day_after_tomorrow}, {'driver_id': 1002, 'event_timestamp': day_after_tomorrow}])\n    driver_stats_df = pd.DataFrame(data=[{'driver_id': 1001, 'avg_daily_trips': 10, 'event_timestamp': now, 'created': now}, {'driver_id': 1001, 'avg_daily_trips': 20, 'event_timestamp': now, 'created': tomorrow}, {'driver_id': 1002, 'avg_daily_trips': 30, 'event_timestamp': now, 'created': tomorrow}, {'driver_id': 1002, 'avg_daily_trips': 40, 'event_timestamp': tomorrow, 'created': now}])\n    expected_df = pd.DataFrame(data=[{'driver_id': 1001, 'event_timestamp': day_after_tomorrow, 'avg_daily_trips': 20}, {'driver_id': 1002, 'event_timestamp': day_after_tomorrow, 'avg_daily_trips': 40}])\n    driver_stats_data_source = environment.data_source_creator.create_data_source(df=driver_stats_df, destination_name=f'test_driver_stats_{int(time.time_ns())}_{random.randint(1000, 9999)}', timestamp_field='event_timestamp', created_timestamp_column='created')\n    driver = Entity(name='driver', join_keys=['driver_id'])\n    driver_fv = FeatureView(name='driver_stats', entities=[driver], schema=[Field(name='avg_daily_trips', dtype=Int32)], source=driver_stats_data_source)\n    store.apply([driver, driver_fv])\n    offline_job = store.get_historical_features(entity_df=entity_df, features=['driver_stats:avg_daily_trips'], full_feature_names=False)\n    start_time = datetime.utcnow()\n    actual_df = offline_job.to_df()\n    print(f'actual_df shape: {actual_df.shape}')\n    end_time = datetime.utcnow()\n    print(str(f\"Time to execute job_from_df.to_df() = '{end_time - start_time}'\\n\"))\n    assert sorted(expected_df.columns) == sorted(actual_df.columns)\n    validate_dataframes(expected_df, actual_df, sort_by=['driver_id'])",
            "@pytest.mark.integration\n@pytest.mark.universal_offline_stores\ndef test_historical_features_from_bigquery_sources_containing_backfills(environment):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    store = environment.feature_store\n    now = datetime.now().replace(microsecond=0, second=0, minute=0)\n    tomorrow = now + timedelta(days=1)\n    day_after_tomorrow = now + timedelta(days=2)\n    entity_df = pd.DataFrame(data=[{'driver_id': 1001, 'event_timestamp': day_after_tomorrow}, {'driver_id': 1002, 'event_timestamp': day_after_tomorrow}])\n    driver_stats_df = pd.DataFrame(data=[{'driver_id': 1001, 'avg_daily_trips': 10, 'event_timestamp': now, 'created': now}, {'driver_id': 1001, 'avg_daily_trips': 20, 'event_timestamp': now, 'created': tomorrow}, {'driver_id': 1002, 'avg_daily_trips': 30, 'event_timestamp': now, 'created': tomorrow}, {'driver_id': 1002, 'avg_daily_trips': 40, 'event_timestamp': tomorrow, 'created': now}])\n    expected_df = pd.DataFrame(data=[{'driver_id': 1001, 'event_timestamp': day_after_tomorrow, 'avg_daily_trips': 20}, {'driver_id': 1002, 'event_timestamp': day_after_tomorrow, 'avg_daily_trips': 40}])\n    driver_stats_data_source = environment.data_source_creator.create_data_source(df=driver_stats_df, destination_name=f'test_driver_stats_{int(time.time_ns())}_{random.randint(1000, 9999)}', timestamp_field='event_timestamp', created_timestamp_column='created')\n    driver = Entity(name='driver', join_keys=['driver_id'])\n    driver_fv = FeatureView(name='driver_stats', entities=[driver], schema=[Field(name='avg_daily_trips', dtype=Int32)], source=driver_stats_data_source)\n    store.apply([driver, driver_fv])\n    offline_job = store.get_historical_features(entity_df=entity_df, features=['driver_stats:avg_daily_trips'], full_feature_names=False)\n    start_time = datetime.utcnow()\n    actual_df = offline_job.to_df()\n    print(f'actual_df shape: {actual_df.shape}')\n    end_time = datetime.utcnow()\n    print(str(f\"Time to execute job_from_df.to_df() = '{end_time - start_time}'\\n\"))\n    assert sorted(expected_df.columns) == sorted(actual_df.columns)\n    validate_dataframes(expected_df, actual_df, sort_by=['driver_id'])"
        ]
    }
]