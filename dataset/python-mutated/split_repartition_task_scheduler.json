[
    {
        "func_name": "execute",
        "original": "def execute(self, refs: List[RefBundle], output_num_blocks: int, ctx: TaskContext, map_ray_remote_args: Optional[Dict[str, Any]]=None, reduce_ray_remote_args: Optional[Dict[str, Any]]=None) -> Tuple[List[RefBundle], StatsDict]:\n    input_num_rows = 0\n    input_owned_by_consumer = True\n    for ref_bundle in refs:\n        block_num_rows = ref_bundle.num_rows()\n        if block_num_rows is None:\n            raise ValueError('Cannot split partition on blocks with unknown number of rows.')\n        input_num_rows += block_num_rows\n        if not ref_bundle.owns_blocks:\n            input_owned_by_consumer = False\n    indices = []\n    if output_num_blocks == 1:\n        indices = [input_num_rows]\n    else:\n        cur_idx = 0\n        for _ in range(output_num_blocks - 1):\n            cur_idx += input_num_rows / output_num_blocks\n            indices.append(int(cur_idx))\n    assert len(indices) <= output_num_blocks, (indices, output_num_blocks)\n    if map_ray_remote_args is None:\n        map_ray_remote_args = {}\n    if reduce_ray_remote_args is None:\n        reduce_ray_remote_args = {}\n    if 'scheduling_strategy' not in reduce_ray_remote_args:\n        reduce_ray_remote_args = reduce_ray_remote_args.copy()\n        reduce_ray_remote_args['scheduling_strategy'] = 'SPREAD'\n    blocks_with_metadata: List[Tuple[ObjectRef[Block], BlockMetadata]] = []\n    for ref_bundle in refs:\n        blocks_with_metadata.extend(ref_bundle.blocks)\n    split_return = _split_at_indices(blocks_with_metadata, indices, input_owned_by_consumer)\n    (split_block_refs, split_metadata) = ([], [])\n    for (b, m) in zip(*split_return):\n        split_block_refs.append(b)\n        split_metadata.extend(m)\n    sub_progress_bar_dict = ctx.sub_progress_bar_dict\n    bar_name = ShuffleTaskSpec.SPLIT_REPARTITION_SUB_PROGRESS_BAR_NAME\n    assert bar_name in sub_progress_bar_dict, sub_progress_bar_dict\n    reduce_bar = sub_progress_bar_dict[bar_name]\n    reduce_task = cached_remote_fn(self._exchange_spec.reduce)\n    reduce_return = [reduce_task.options(**reduce_ray_remote_args, num_returns=2).remote(*self._exchange_spec._reduce_args, *split_block_refs[j]) for j in range(output_num_blocks) if len(split_block_refs[j]) > 0]\n    (reduce_block_refs, reduce_metadata) = zip(*reduce_return)\n    reduce_metadata = reduce_bar.fetch_until_complete(list(reduce_metadata))\n    (reduce_block_refs, reduce_metadata) = (list(reduce_block_refs), list(reduce_metadata))\n    if len(reduce_block_refs) < output_num_blocks:\n        import pyarrow as pa\n        from ray.data._internal.arrow_block import ArrowBlockBuilder\n        from ray.data._internal.pandas_block import PandasBlockBuilder, PandasBlockSchema\n        num_empty_blocks = output_num_blocks - len(reduce_block_refs)\n        first_block_schema = reduce_metadata[0].schema\n        if first_block_schema is None:\n            raise ValueError('Cannot split partition on blocks with unknown block format.')\n        elif isinstance(first_block_schema, pa.Schema):\n            builder = ArrowBlockBuilder()\n        elif isinstance(first_block_schema, PandasBlockSchema):\n            builder = PandasBlockBuilder()\n        empty_block = builder.build()\n        empty_meta = BlockAccessor.for_block(empty_block).get_metadata(input_files=None, exec_stats=None)\n        (empty_block_refs, empty_metadata) = zip(*[(ray.put(empty_block), empty_meta) for _ in range(num_empty_blocks)])\n        reduce_block_refs.extend(empty_block_refs)\n        reduce_metadata.extend(empty_metadata)\n    output = []\n    for (block, meta) in zip(reduce_block_refs, reduce_metadata):\n        output.append(RefBundle([(block, meta)], owns_blocks=input_owned_by_consumer))\n    stats = {'split': split_metadata, 'reduce': reduce_metadata}\n    return (output, stats)",
        "mutated": [
            "def execute(self, refs: List[RefBundle], output_num_blocks: int, ctx: TaskContext, map_ray_remote_args: Optional[Dict[str, Any]]=None, reduce_ray_remote_args: Optional[Dict[str, Any]]=None) -> Tuple[List[RefBundle], StatsDict]:\n    if False:\n        i = 10\n    input_num_rows = 0\n    input_owned_by_consumer = True\n    for ref_bundle in refs:\n        block_num_rows = ref_bundle.num_rows()\n        if block_num_rows is None:\n            raise ValueError('Cannot split partition on blocks with unknown number of rows.')\n        input_num_rows += block_num_rows\n        if not ref_bundle.owns_blocks:\n            input_owned_by_consumer = False\n    indices = []\n    if output_num_blocks == 1:\n        indices = [input_num_rows]\n    else:\n        cur_idx = 0\n        for _ in range(output_num_blocks - 1):\n            cur_idx += input_num_rows / output_num_blocks\n            indices.append(int(cur_idx))\n    assert len(indices) <= output_num_blocks, (indices, output_num_blocks)\n    if map_ray_remote_args is None:\n        map_ray_remote_args = {}\n    if reduce_ray_remote_args is None:\n        reduce_ray_remote_args = {}\n    if 'scheduling_strategy' not in reduce_ray_remote_args:\n        reduce_ray_remote_args = reduce_ray_remote_args.copy()\n        reduce_ray_remote_args['scheduling_strategy'] = 'SPREAD'\n    blocks_with_metadata: List[Tuple[ObjectRef[Block], BlockMetadata]] = []\n    for ref_bundle in refs:\n        blocks_with_metadata.extend(ref_bundle.blocks)\n    split_return = _split_at_indices(blocks_with_metadata, indices, input_owned_by_consumer)\n    (split_block_refs, split_metadata) = ([], [])\n    for (b, m) in zip(*split_return):\n        split_block_refs.append(b)\n        split_metadata.extend(m)\n    sub_progress_bar_dict = ctx.sub_progress_bar_dict\n    bar_name = ShuffleTaskSpec.SPLIT_REPARTITION_SUB_PROGRESS_BAR_NAME\n    assert bar_name in sub_progress_bar_dict, sub_progress_bar_dict\n    reduce_bar = sub_progress_bar_dict[bar_name]\n    reduce_task = cached_remote_fn(self._exchange_spec.reduce)\n    reduce_return = [reduce_task.options(**reduce_ray_remote_args, num_returns=2).remote(*self._exchange_spec._reduce_args, *split_block_refs[j]) for j in range(output_num_blocks) if len(split_block_refs[j]) > 0]\n    (reduce_block_refs, reduce_metadata) = zip(*reduce_return)\n    reduce_metadata = reduce_bar.fetch_until_complete(list(reduce_metadata))\n    (reduce_block_refs, reduce_metadata) = (list(reduce_block_refs), list(reduce_metadata))\n    if len(reduce_block_refs) < output_num_blocks:\n        import pyarrow as pa\n        from ray.data._internal.arrow_block import ArrowBlockBuilder\n        from ray.data._internal.pandas_block import PandasBlockBuilder, PandasBlockSchema\n        num_empty_blocks = output_num_blocks - len(reduce_block_refs)\n        first_block_schema = reduce_metadata[0].schema\n        if first_block_schema is None:\n            raise ValueError('Cannot split partition on blocks with unknown block format.')\n        elif isinstance(first_block_schema, pa.Schema):\n            builder = ArrowBlockBuilder()\n        elif isinstance(first_block_schema, PandasBlockSchema):\n            builder = PandasBlockBuilder()\n        empty_block = builder.build()\n        empty_meta = BlockAccessor.for_block(empty_block).get_metadata(input_files=None, exec_stats=None)\n        (empty_block_refs, empty_metadata) = zip(*[(ray.put(empty_block), empty_meta) for _ in range(num_empty_blocks)])\n        reduce_block_refs.extend(empty_block_refs)\n        reduce_metadata.extend(empty_metadata)\n    output = []\n    for (block, meta) in zip(reduce_block_refs, reduce_metadata):\n        output.append(RefBundle([(block, meta)], owns_blocks=input_owned_by_consumer))\n    stats = {'split': split_metadata, 'reduce': reduce_metadata}\n    return (output, stats)",
            "def execute(self, refs: List[RefBundle], output_num_blocks: int, ctx: TaskContext, map_ray_remote_args: Optional[Dict[str, Any]]=None, reduce_ray_remote_args: Optional[Dict[str, Any]]=None) -> Tuple[List[RefBundle], StatsDict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    input_num_rows = 0\n    input_owned_by_consumer = True\n    for ref_bundle in refs:\n        block_num_rows = ref_bundle.num_rows()\n        if block_num_rows is None:\n            raise ValueError('Cannot split partition on blocks with unknown number of rows.')\n        input_num_rows += block_num_rows\n        if not ref_bundle.owns_blocks:\n            input_owned_by_consumer = False\n    indices = []\n    if output_num_blocks == 1:\n        indices = [input_num_rows]\n    else:\n        cur_idx = 0\n        for _ in range(output_num_blocks - 1):\n            cur_idx += input_num_rows / output_num_blocks\n            indices.append(int(cur_idx))\n    assert len(indices) <= output_num_blocks, (indices, output_num_blocks)\n    if map_ray_remote_args is None:\n        map_ray_remote_args = {}\n    if reduce_ray_remote_args is None:\n        reduce_ray_remote_args = {}\n    if 'scheduling_strategy' not in reduce_ray_remote_args:\n        reduce_ray_remote_args = reduce_ray_remote_args.copy()\n        reduce_ray_remote_args['scheduling_strategy'] = 'SPREAD'\n    blocks_with_metadata: List[Tuple[ObjectRef[Block], BlockMetadata]] = []\n    for ref_bundle in refs:\n        blocks_with_metadata.extend(ref_bundle.blocks)\n    split_return = _split_at_indices(blocks_with_metadata, indices, input_owned_by_consumer)\n    (split_block_refs, split_metadata) = ([], [])\n    for (b, m) in zip(*split_return):\n        split_block_refs.append(b)\n        split_metadata.extend(m)\n    sub_progress_bar_dict = ctx.sub_progress_bar_dict\n    bar_name = ShuffleTaskSpec.SPLIT_REPARTITION_SUB_PROGRESS_BAR_NAME\n    assert bar_name in sub_progress_bar_dict, sub_progress_bar_dict\n    reduce_bar = sub_progress_bar_dict[bar_name]\n    reduce_task = cached_remote_fn(self._exchange_spec.reduce)\n    reduce_return = [reduce_task.options(**reduce_ray_remote_args, num_returns=2).remote(*self._exchange_spec._reduce_args, *split_block_refs[j]) for j in range(output_num_blocks) if len(split_block_refs[j]) > 0]\n    (reduce_block_refs, reduce_metadata) = zip(*reduce_return)\n    reduce_metadata = reduce_bar.fetch_until_complete(list(reduce_metadata))\n    (reduce_block_refs, reduce_metadata) = (list(reduce_block_refs), list(reduce_metadata))\n    if len(reduce_block_refs) < output_num_blocks:\n        import pyarrow as pa\n        from ray.data._internal.arrow_block import ArrowBlockBuilder\n        from ray.data._internal.pandas_block import PandasBlockBuilder, PandasBlockSchema\n        num_empty_blocks = output_num_blocks - len(reduce_block_refs)\n        first_block_schema = reduce_metadata[0].schema\n        if first_block_schema is None:\n            raise ValueError('Cannot split partition on blocks with unknown block format.')\n        elif isinstance(first_block_schema, pa.Schema):\n            builder = ArrowBlockBuilder()\n        elif isinstance(first_block_schema, PandasBlockSchema):\n            builder = PandasBlockBuilder()\n        empty_block = builder.build()\n        empty_meta = BlockAccessor.for_block(empty_block).get_metadata(input_files=None, exec_stats=None)\n        (empty_block_refs, empty_metadata) = zip(*[(ray.put(empty_block), empty_meta) for _ in range(num_empty_blocks)])\n        reduce_block_refs.extend(empty_block_refs)\n        reduce_metadata.extend(empty_metadata)\n    output = []\n    for (block, meta) in zip(reduce_block_refs, reduce_metadata):\n        output.append(RefBundle([(block, meta)], owns_blocks=input_owned_by_consumer))\n    stats = {'split': split_metadata, 'reduce': reduce_metadata}\n    return (output, stats)",
            "def execute(self, refs: List[RefBundle], output_num_blocks: int, ctx: TaskContext, map_ray_remote_args: Optional[Dict[str, Any]]=None, reduce_ray_remote_args: Optional[Dict[str, Any]]=None) -> Tuple[List[RefBundle], StatsDict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    input_num_rows = 0\n    input_owned_by_consumer = True\n    for ref_bundle in refs:\n        block_num_rows = ref_bundle.num_rows()\n        if block_num_rows is None:\n            raise ValueError('Cannot split partition on blocks with unknown number of rows.')\n        input_num_rows += block_num_rows\n        if not ref_bundle.owns_blocks:\n            input_owned_by_consumer = False\n    indices = []\n    if output_num_blocks == 1:\n        indices = [input_num_rows]\n    else:\n        cur_idx = 0\n        for _ in range(output_num_blocks - 1):\n            cur_idx += input_num_rows / output_num_blocks\n            indices.append(int(cur_idx))\n    assert len(indices) <= output_num_blocks, (indices, output_num_blocks)\n    if map_ray_remote_args is None:\n        map_ray_remote_args = {}\n    if reduce_ray_remote_args is None:\n        reduce_ray_remote_args = {}\n    if 'scheduling_strategy' not in reduce_ray_remote_args:\n        reduce_ray_remote_args = reduce_ray_remote_args.copy()\n        reduce_ray_remote_args['scheduling_strategy'] = 'SPREAD'\n    blocks_with_metadata: List[Tuple[ObjectRef[Block], BlockMetadata]] = []\n    for ref_bundle in refs:\n        blocks_with_metadata.extend(ref_bundle.blocks)\n    split_return = _split_at_indices(blocks_with_metadata, indices, input_owned_by_consumer)\n    (split_block_refs, split_metadata) = ([], [])\n    for (b, m) in zip(*split_return):\n        split_block_refs.append(b)\n        split_metadata.extend(m)\n    sub_progress_bar_dict = ctx.sub_progress_bar_dict\n    bar_name = ShuffleTaskSpec.SPLIT_REPARTITION_SUB_PROGRESS_BAR_NAME\n    assert bar_name in sub_progress_bar_dict, sub_progress_bar_dict\n    reduce_bar = sub_progress_bar_dict[bar_name]\n    reduce_task = cached_remote_fn(self._exchange_spec.reduce)\n    reduce_return = [reduce_task.options(**reduce_ray_remote_args, num_returns=2).remote(*self._exchange_spec._reduce_args, *split_block_refs[j]) for j in range(output_num_blocks) if len(split_block_refs[j]) > 0]\n    (reduce_block_refs, reduce_metadata) = zip(*reduce_return)\n    reduce_metadata = reduce_bar.fetch_until_complete(list(reduce_metadata))\n    (reduce_block_refs, reduce_metadata) = (list(reduce_block_refs), list(reduce_metadata))\n    if len(reduce_block_refs) < output_num_blocks:\n        import pyarrow as pa\n        from ray.data._internal.arrow_block import ArrowBlockBuilder\n        from ray.data._internal.pandas_block import PandasBlockBuilder, PandasBlockSchema\n        num_empty_blocks = output_num_blocks - len(reduce_block_refs)\n        first_block_schema = reduce_metadata[0].schema\n        if first_block_schema is None:\n            raise ValueError('Cannot split partition on blocks with unknown block format.')\n        elif isinstance(first_block_schema, pa.Schema):\n            builder = ArrowBlockBuilder()\n        elif isinstance(first_block_schema, PandasBlockSchema):\n            builder = PandasBlockBuilder()\n        empty_block = builder.build()\n        empty_meta = BlockAccessor.for_block(empty_block).get_metadata(input_files=None, exec_stats=None)\n        (empty_block_refs, empty_metadata) = zip(*[(ray.put(empty_block), empty_meta) for _ in range(num_empty_blocks)])\n        reduce_block_refs.extend(empty_block_refs)\n        reduce_metadata.extend(empty_metadata)\n    output = []\n    for (block, meta) in zip(reduce_block_refs, reduce_metadata):\n        output.append(RefBundle([(block, meta)], owns_blocks=input_owned_by_consumer))\n    stats = {'split': split_metadata, 'reduce': reduce_metadata}\n    return (output, stats)",
            "def execute(self, refs: List[RefBundle], output_num_blocks: int, ctx: TaskContext, map_ray_remote_args: Optional[Dict[str, Any]]=None, reduce_ray_remote_args: Optional[Dict[str, Any]]=None) -> Tuple[List[RefBundle], StatsDict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    input_num_rows = 0\n    input_owned_by_consumer = True\n    for ref_bundle in refs:\n        block_num_rows = ref_bundle.num_rows()\n        if block_num_rows is None:\n            raise ValueError('Cannot split partition on blocks with unknown number of rows.')\n        input_num_rows += block_num_rows\n        if not ref_bundle.owns_blocks:\n            input_owned_by_consumer = False\n    indices = []\n    if output_num_blocks == 1:\n        indices = [input_num_rows]\n    else:\n        cur_idx = 0\n        for _ in range(output_num_blocks - 1):\n            cur_idx += input_num_rows / output_num_blocks\n            indices.append(int(cur_idx))\n    assert len(indices) <= output_num_blocks, (indices, output_num_blocks)\n    if map_ray_remote_args is None:\n        map_ray_remote_args = {}\n    if reduce_ray_remote_args is None:\n        reduce_ray_remote_args = {}\n    if 'scheduling_strategy' not in reduce_ray_remote_args:\n        reduce_ray_remote_args = reduce_ray_remote_args.copy()\n        reduce_ray_remote_args['scheduling_strategy'] = 'SPREAD'\n    blocks_with_metadata: List[Tuple[ObjectRef[Block], BlockMetadata]] = []\n    for ref_bundle in refs:\n        blocks_with_metadata.extend(ref_bundle.blocks)\n    split_return = _split_at_indices(blocks_with_metadata, indices, input_owned_by_consumer)\n    (split_block_refs, split_metadata) = ([], [])\n    for (b, m) in zip(*split_return):\n        split_block_refs.append(b)\n        split_metadata.extend(m)\n    sub_progress_bar_dict = ctx.sub_progress_bar_dict\n    bar_name = ShuffleTaskSpec.SPLIT_REPARTITION_SUB_PROGRESS_BAR_NAME\n    assert bar_name in sub_progress_bar_dict, sub_progress_bar_dict\n    reduce_bar = sub_progress_bar_dict[bar_name]\n    reduce_task = cached_remote_fn(self._exchange_spec.reduce)\n    reduce_return = [reduce_task.options(**reduce_ray_remote_args, num_returns=2).remote(*self._exchange_spec._reduce_args, *split_block_refs[j]) for j in range(output_num_blocks) if len(split_block_refs[j]) > 0]\n    (reduce_block_refs, reduce_metadata) = zip(*reduce_return)\n    reduce_metadata = reduce_bar.fetch_until_complete(list(reduce_metadata))\n    (reduce_block_refs, reduce_metadata) = (list(reduce_block_refs), list(reduce_metadata))\n    if len(reduce_block_refs) < output_num_blocks:\n        import pyarrow as pa\n        from ray.data._internal.arrow_block import ArrowBlockBuilder\n        from ray.data._internal.pandas_block import PandasBlockBuilder, PandasBlockSchema\n        num_empty_blocks = output_num_blocks - len(reduce_block_refs)\n        first_block_schema = reduce_metadata[0].schema\n        if first_block_schema is None:\n            raise ValueError('Cannot split partition on blocks with unknown block format.')\n        elif isinstance(first_block_schema, pa.Schema):\n            builder = ArrowBlockBuilder()\n        elif isinstance(first_block_schema, PandasBlockSchema):\n            builder = PandasBlockBuilder()\n        empty_block = builder.build()\n        empty_meta = BlockAccessor.for_block(empty_block).get_metadata(input_files=None, exec_stats=None)\n        (empty_block_refs, empty_metadata) = zip(*[(ray.put(empty_block), empty_meta) for _ in range(num_empty_blocks)])\n        reduce_block_refs.extend(empty_block_refs)\n        reduce_metadata.extend(empty_metadata)\n    output = []\n    for (block, meta) in zip(reduce_block_refs, reduce_metadata):\n        output.append(RefBundle([(block, meta)], owns_blocks=input_owned_by_consumer))\n    stats = {'split': split_metadata, 'reduce': reduce_metadata}\n    return (output, stats)",
            "def execute(self, refs: List[RefBundle], output_num_blocks: int, ctx: TaskContext, map_ray_remote_args: Optional[Dict[str, Any]]=None, reduce_ray_remote_args: Optional[Dict[str, Any]]=None) -> Tuple[List[RefBundle], StatsDict]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    input_num_rows = 0\n    input_owned_by_consumer = True\n    for ref_bundle in refs:\n        block_num_rows = ref_bundle.num_rows()\n        if block_num_rows is None:\n            raise ValueError('Cannot split partition on blocks with unknown number of rows.')\n        input_num_rows += block_num_rows\n        if not ref_bundle.owns_blocks:\n            input_owned_by_consumer = False\n    indices = []\n    if output_num_blocks == 1:\n        indices = [input_num_rows]\n    else:\n        cur_idx = 0\n        for _ in range(output_num_blocks - 1):\n            cur_idx += input_num_rows / output_num_blocks\n            indices.append(int(cur_idx))\n    assert len(indices) <= output_num_blocks, (indices, output_num_blocks)\n    if map_ray_remote_args is None:\n        map_ray_remote_args = {}\n    if reduce_ray_remote_args is None:\n        reduce_ray_remote_args = {}\n    if 'scheduling_strategy' not in reduce_ray_remote_args:\n        reduce_ray_remote_args = reduce_ray_remote_args.copy()\n        reduce_ray_remote_args['scheduling_strategy'] = 'SPREAD'\n    blocks_with_metadata: List[Tuple[ObjectRef[Block], BlockMetadata]] = []\n    for ref_bundle in refs:\n        blocks_with_metadata.extend(ref_bundle.blocks)\n    split_return = _split_at_indices(blocks_with_metadata, indices, input_owned_by_consumer)\n    (split_block_refs, split_metadata) = ([], [])\n    for (b, m) in zip(*split_return):\n        split_block_refs.append(b)\n        split_metadata.extend(m)\n    sub_progress_bar_dict = ctx.sub_progress_bar_dict\n    bar_name = ShuffleTaskSpec.SPLIT_REPARTITION_SUB_PROGRESS_BAR_NAME\n    assert bar_name in sub_progress_bar_dict, sub_progress_bar_dict\n    reduce_bar = sub_progress_bar_dict[bar_name]\n    reduce_task = cached_remote_fn(self._exchange_spec.reduce)\n    reduce_return = [reduce_task.options(**reduce_ray_remote_args, num_returns=2).remote(*self._exchange_spec._reduce_args, *split_block_refs[j]) for j in range(output_num_blocks) if len(split_block_refs[j]) > 0]\n    (reduce_block_refs, reduce_metadata) = zip(*reduce_return)\n    reduce_metadata = reduce_bar.fetch_until_complete(list(reduce_metadata))\n    (reduce_block_refs, reduce_metadata) = (list(reduce_block_refs), list(reduce_metadata))\n    if len(reduce_block_refs) < output_num_blocks:\n        import pyarrow as pa\n        from ray.data._internal.arrow_block import ArrowBlockBuilder\n        from ray.data._internal.pandas_block import PandasBlockBuilder, PandasBlockSchema\n        num_empty_blocks = output_num_blocks - len(reduce_block_refs)\n        first_block_schema = reduce_metadata[0].schema\n        if first_block_schema is None:\n            raise ValueError('Cannot split partition on blocks with unknown block format.')\n        elif isinstance(first_block_schema, pa.Schema):\n            builder = ArrowBlockBuilder()\n        elif isinstance(first_block_schema, PandasBlockSchema):\n            builder = PandasBlockBuilder()\n        empty_block = builder.build()\n        empty_meta = BlockAccessor.for_block(empty_block).get_metadata(input_files=None, exec_stats=None)\n        (empty_block_refs, empty_metadata) = zip(*[(ray.put(empty_block), empty_meta) for _ in range(num_empty_blocks)])\n        reduce_block_refs.extend(empty_block_refs)\n        reduce_metadata.extend(empty_metadata)\n    output = []\n    for (block, meta) in zip(reduce_block_refs, reduce_metadata):\n        output.append(RefBundle([(block, meta)], owns_blocks=input_owned_by_consumer))\n    stats = {'split': split_metadata, 'reduce': reduce_metadata}\n    return (output, stats)"
        ]
    }
]