[
    {
        "func_name": "init_process_group",
        "original": "def init_process_group(strategy=None):\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = dist.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    pg_group = dist.init_parallel_env()\n    return pg_group.process_group",
        "mutated": [
            "def init_process_group(strategy=None):\n    if False:\n        i = 10\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = dist.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    pg_group = dist.init_parallel_env()\n    return pg_group.process_group",
            "def init_process_group(strategy=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = dist.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    pg_group = dist.init_parallel_env()\n    return pg_group.process_group",
            "def init_process_group(strategy=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = dist.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    pg_group = dist.init_parallel_env()\n    return pg_group.process_group",
            "def init_process_group(strategy=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = dist.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    pg_group = dist.init_parallel_env()\n    return pg_group.process_group",
            "def init_process_group(strategy=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    nranks = paddle.distributed.ParallelEnv().nranks\n    rank = dist.ParallelEnv().local_rank\n    is_master = True if rank == 0 else False\n    pg_group = dist.init_parallel_env()\n    return pg_group.process_group"
        ]
    },
    {
        "func_name": "setUp",
        "original": "def setUp(self):\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
        "mutated": [
            "def setUp(self):\n    if False:\n        i = 10\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()"
        ]
    },
    {
        "func_name": "config",
        "original": "def config(self):\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
        "mutated": [
            "def config(self):\n    if False:\n        i = 10\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.dtype = 'float32'\n    self.shape = (2, 10, 5)"
        ]
    },
    {
        "func_name": "test_create_process_group_nccl",
        "original": "def test_create_process_group_nccl(self):\n    device_id = paddle.distributed.ParallelEnv().dev_id\n    paddle.set_device('gpu:%d' % device_id)\n    assert paddle.distributed.is_available()\n    pg = init_process_group()\n    print('rank:', pg.rank(), 'size:', pg.size(), 'name:', pg.name())\n    print('test new group api ok')\n    assert paddle.distributed.get_backend() == 'NCCL'\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x)\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    else:\n        task = dist.all_reduce(tensor_y)\n        np.testing.assert_array_equal(tensor_y, sum_result)\n    print('test allreduce sum api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x)\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    else:\n        task = dist.all_reduce(tensor_y)\n        np.testing.assert_array_equal(tensor_y, sum_result)\n    print('test allreduce sum api with = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api with shape = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, min_result)\n    print('test allreduce min api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, min_result)\n    print('test allreduce min api with shape [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, prod_result)\n    print('test allreduce prod api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, prod_result)\n    print('test allreduce prod api with shape = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = dist.broadcast(tensor_x, 0, sync_op=False)\n        task.synchronize()\n        paddle.device.cuda.synchronize()\n        assert task.is_completed()\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = dist.broadcast(tensor_y, 0)\n        paddle.device.cuda.synchronize()\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    print('test broadcast api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = dist.broadcast(tensor_x, 0, sync_op=False)\n        task.synchronize()\n        paddle.device.cuda.synchronize()\n        assert task.is_completed()\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = dist.broadcast(tensor_y, 0)\n        paddle.device.cuda.synchronize()\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    assert tensor_y.shape == []\n    print('test broadcast api with shape=[] ok')\n    if pg.rank() == 0:\n        pg.barrier(device_id)\n    else:\n        task = pg.barrier(device_id)\n        task.wait()\n    print('test barrier api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        tensor_out_list = [paddle.empty_like(tensor_x), paddle.empty_like(tensor_x)]\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n        tensor_out = paddle.concat(tensor_out_list)\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api ok\\n')\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        tensor_out_list = []\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n        tensor_out = paddle.concat(tensor_out_list)\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api2 ok\\n')\n    x = np.random.random([]).astype(self.dtype)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out_list = []\n    if pg.rank() == 0:\n        task = dist.all_gather(tensor_out_list, tensor_x)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n    out_1 = tensor_out_list[0]\n    out_2 = tensor_out_list[1]\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api with shape [] ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        (in_1, in_2) = paddle.split(tensor_y, 2)\n        (out_1, out_2) = paddle.split(tensor_out2, 2)\n        out_tensor_list = [out_1, out_2]\n        task = dist.alltoall([in_1, in_2], out_tensor_list)\n        paddle.device.cuda.synchronize()\n        tensor_out2 = paddle.concat(out_tensor_list)\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(out1_2.numpy(), raw_tensor_y_1.numpy())\n    else:\n        np.testing.assert_array_equal(out2_1, raw_tensor_x_2)\n    print('test alltoall api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        (in_1, in_2) = paddle.split(tensor_y, 2)\n        (out_1, out_2) = paddle.split(tensor_out2, 2)\n        out_tensor_list = []\n        task = dist.alltoall([in_1, in_2], out_tensor_list)\n        paddle.device.cuda.synchronize()\n        tensor_out2 = paddle.concat(out_tensor_list)\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(out1_2.numpy(), raw_tensor_y_1.numpy())\n    else:\n        np.testing.assert_array_equal(out2_1, raw_tensor_x_2)\n    print('test alltoall api2 ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.reduce(tensor_y, 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    print('test reduce sum api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n    print('test reduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n    print('test reduce min api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n    print('test reduce prod api ok')\n    test_reduce_with_zero_dim([], self.dtype, pg)\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        (in_1, in_2) = paddle.split(tensor_x, 2)\n        task = dist.scatter(tensor_y, [in_1, in_2], 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.scatter(tensor_y, [], 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    print('test scatter api ok\\n')\n    x = np.random.random([]).astype(self.dtype)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        (in_1, in_2) = (tensor_x, tensor_x + 1)\n        task = dist.scatter(tensor_y, [in_1, in_2], 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.scatter(tensor_y, [], 0, sync_op=True)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    out1 = paddle.assign(tensor_x)\n    out2 = paddle.assign(tensor_x + 1)\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    assert tensor_y.shape == []\n    print('test scatter api with shape=[] ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=False)\n        task.wait()\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, tensor_x)\n    print('test send api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=True)\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=True)\n        np.testing.assert_array_equal(tensor_y, tensor_x)\n    print('test send api ok')\n    x = np.random.uniform(-1, 1, []).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.array(0.2022).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=True)\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=True)\n        assert np.array_equal(tensor_y, tensor_x) and tensor_y.shape == []\n    print('test send & recv 0-d tensor ok')",
        "mutated": [
            "def test_create_process_group_nccl(self):\n    if False:\n        i = 10\n    device_id = paddle.distributed.ParallelEnv().dev_id\n    paddle.set_device('gpu:%d' % device_id)\n    assert paddle.distributed.is_available()\n    pg = init_process_group()\n    print('rank:', pg.rank(), 'size:', pg.size(), 'name:', pg.name())\n    print('test new group api ok')\n    assert paddle.distributed.get_backend() == 'NCCL'\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x)\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    else:\n        task = dist.all_reduce(tensor_y)\n        np.testing.assert_array_equal(tensor_y, sum_result)\n    print('test allreduce sum api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x)\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    else:\n        task = dist.all_reduce(tensor_y)\n        np.testing.assert_array_equal(tensor_y, sum_result)\n    print('test allreduce sum api with = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api with shape = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, min_result)\n    print('test allreduce min api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, min_result)\n    print('test allreduce min api with shape [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, prod_result)\n    print('test allreduce prod api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, prod_result)\n    print('test allreduce prod api with shape = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = dist.broadcast(tensor_x, 0, sync_op=False)\n        task.synchronize()\n        paddle.device.cuda.synchronize()\n        assert task.is_completed()\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = dist.broadcast(tensor_y, 0)\n        paddle.device.cuda.synchronize()\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    print('test broadcast api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = dist.broadcast(tensor_x, 0, sync_op=False)\n        task.synchronize()\n        paddle.device.cuda.synchronize()\n        assert task.is_completed()\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = dist.broadcast(tensor_y, 0)\n        paddle.device.cuda.synchronize()\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    assert tensor_y.shape == []\n    print('test broadcast api with shape=[] ok')\n    if pg.rank() == 0:\n        pg.barrier(device_id)\n    else:\n        task = pg.barrier(device_id)\n        task.wait()\n    print('test barrier api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        tensor_out_list = [paddle.empty_like(tensor_x), paddle.empty_like(tensor_x)]\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n        tensor_out = paddle.concat(tensor_out_list)\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api ok\\n')\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        tensor_out_list = []\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n        tensor_out = paddle.concat(tensor_out_list)\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api2 ok\\n')\n    x = np.random.random([]).astype(self.dtype)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out_list = []\n    if pg.rank() == 0:\n        task = dist.all_gather(tensor_out_list, tensor_x)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n    out_1 = tensor_out_list[0]\n    out_2 = tensor_out_list[1]\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api with shape [] ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        (in_1, in_2) = paddle.split(tensor_y, 2)\n        (out_1, out_2) = paddle.split(tensor_out2, 2)\n        out_tensor_list = [out_1, out_2]\n        task = dist.alltoall([in_1, in_2], out_tensor_list)\n        paddle.device.cuda.synchronize()\n        tensor_out2 = paddle.concat(out_tensor_list)\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(out1_2.numpy(), raw_tensor_y_1.numpy())\n    else:\n        np.testing.assert_array_equal(out2_1, raw_tensor_x_2)\n    print('test alltoall api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        (in_1, in_2) = paddle.split(tensor_y, 2)\n        (out_1, out_2) = paddle.split(tensor_out2, 2)\n        out_tensor_list = []\n        task = dist.alltoall([in_1, in_2], out_tensor_list)\n        paddle.device.cuda.synchronize()\n        tensor_out2 = paddle.concat(out_tensor_list)\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(out1_2.numpy(), raw_tensor_y_1.numpy())\n    else:\n        np.testing.assert_array_equal(out2_1, raw_tensor_x_2)\n    print('test alltoall api2 ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.reduce(tensor_y, 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    print('test reduce sum api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n    print('test reduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n    print('test reduce min api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n    print('test reduce prod api ok')\n    test_reduce_with_zero_dim([], self.dtype, pg)\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        (in_1, in_2) = paddle.split(tensor_x, 2)\n        task = dist.scatter(tensor_y, [in_1, in_2], 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.scatter(tensor_y, [], 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    print('test scatter api ok\\n')\n    x = np.random.random([]).astype(self.dtype)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        (in_1, in_2) = (tensor_x, tensor_x + 1)\n        task = dist.scatter(tensor_y, [in_1, in_2], 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.scatter(tensor_y, [], 0, sync_op=True)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    out1 = paddle.assign(tensor_x)\n    out2 = paddle.assign(tensor_x + 1)\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    assert tensor_y.shape == []\n    print('test scatter api with shape=[] ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=False)\n        task.wait()\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, tensor_x)\n    print('test send api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=True)\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=True)\n        np.testing.assert_array_equal(tensor_y, tensor_x)\n    print('test send api ok')\n    x = np.random.uniform(-1, 1, []).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.array(0.2022).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=True)\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=True)\n        assert np.array_equal(tensor_y, tensor_x) and tensor_y.shape == []\n    print('test send & recv 0-d tensor ok')",
            "def test_create_process_group_nccl(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    device_id = paddle.distributed.ParallelEnv().dev_id\n    paddle.set_device('gpu:%d' % device_id)\n    assert paddle.distributed.is_available()\n    pg = init_process_group()\n    print('rank:', pg.rank(), 'size:', pg.size(), 'name:', pg.name())\n    print('test new group api ok')\n    assert paddle.distributed.get_backend() == 'NCCL'\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x)\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    else:\n        task = dist.all_reduce(tensor_y)\n        np.testing.assert_array_equal(tensor_y, sum_result)\n    print('test allreduce sum api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x)\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    else:\n        task = dist.all_reduce(tensor_y)\n        np.testing.assert_array_equal(tensor_y, sum_result)\n    print('test allreduce sum api with = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api with shape = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, min_result)\n    print('test allreduce min api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, min_result)\n    print('test allreduce min api with shape [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, prod_result)\n    print('test allreduce prod api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, prod_result)\n    print('test allreduce prod api with shape = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = dist.broadcast(tensor_x, 0, sync_op=False)\n        task.synchronize()\n        paddle.device.cuda.synchronize()\n        assert task.is_completed()\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = dist.broadcast(tensor_y, 0)\n        paddle.device.cuda.synchronize()\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    print('test broadcast api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = dist.broadcast(tensor_x, 0, sync_op=False)\n        task.synchronize()\n        paddle.device.cuda.synchronize()\n        assert task.is_completed()\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = dist.broadcast(tensor_y, 0)\n        paddle.device.cuda.synchronize()\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    assert tensor_y.shape == []\n    print('test broadcast api with shape=[] ok')\n    if pg.rank() == 0:\n        pg.barrier(device_id)\n    else:\n        task = pg.barrier(device_id)\n        task.wait()\n    print('test barrier api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        tensor_out_list = [paddle.empty_like(tensor_x), paddle.empty_like(tensor_x)]\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n        tensor_out = paddle.concat(tensor_out_list)\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api ok\\n')\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        tensor_out_list = []\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n        tensor_out = paddle.concat(tensor_out_list)\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api2 ok\\n')\n    x = np.random.random([]).astype(self.dtype)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out_list = []\n    if pg.rank() == 0:\n        task = dist.all_gather(tensor_out_list, tensor_x)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n    out_1 = tensor_out_list[0]\n    out_2 = tensor_out_list[1]\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api with shape [] ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        (in_1, in_2) = paddle.split(tensor_y, 2)\n        (out_1, out_2) = paddle.split(tensor_out2, 2)\n        out_tensor_list = [out_1, out_2]\n        task = dist.alltoall([in_1, in_2], out_tensor_list)\n        paddle.device.cuda.synchronize()\n        tensor_out2 = paddle.concat(out_tensor_list)\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(out1_2.numpy(), raw_tensor_y_1.numpy())\n    else:\n        np.testing.assert_array_equal(out2_1, raw_tensor_x_2)\n    print('test alltoall api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        (in_1, in_2) = paddle.split(tensor_y, 2)\n        (out_1, out_2) = paddle.split(tensor_out2, 2)\n        out_tensor_list = []\n        task = dist.alltoall([in_1, in_2], out_tensor_list)\n        paddle.device.cuda.synchronize()\n        tensor_out2 = paddle.concat(out_tensor_list)\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(out1_2.numpy(), raw_tensor_y_1.numpy())\n    else:\n        np.testing.assert_array_equal(out2_1, raw_tensor_x_2)\n    print('test alltoall api2 ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.reduce(tensor_y, 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    print('test reduce sum api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n    print('test reduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n    print('test reduce min api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n    print('test reduce prod api ok')\n    test_reduce_with_zero_dim([], self.dtype, pg)\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        (in_1, in_2) = paddle.split(tensor_x, 2)\n        task = dist.scatter(tensor_y, [in_1, in_2], 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.scatter(tensor_y, [], 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    print('test scatter api ok\\n')\n    x = np.random.random([]).astype(self.dtype)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        (in_1, in_2) = (tensor_x, tensor_x + 1)\n        task = dist.scatter(tensor_y, [in_1, in_2], 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.scatter(tensor_y, [], 0, sync_op=True)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    out1 = paddle.assign(tensor_x)\n    out2 = paddle.assign(tensor_x + 1)\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    assert tensor_y.shape == []\n    print('test scatter api with shape=[] ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=False)\n        task.wait()\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, tensor_x)\n    print('test send api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=True)\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=True)\n        np.testing.assert_array_equal(tensor_y, tensor_x)\n    print('test send api ok')\n    x = np.random.uniform(-1, 1, []).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.array(0.2022).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=True)\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=True)\n        assert np.array_equal(tensor_y, tensor_x) and tensor_y.shape == []\n    print('test send & recv 0-d tensor ok')",
            "def test_create_process_group_nccl(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    device_id = paddle.distributed.ParallelEnv().dev_id\n    paddle.set_device('gpu:%d' % device_id)\n    assert paddle.distributed.is_available()\n    pg = init_process_group()\n    print('rank:', pg.rank(), 'size:', pg.size(), 'name:', pg.name())\n    print('test new group api ok')\n    assert paddle.distributed.get_backend() == 'NCCL'\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x)\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    else:\n        task = dist.all_reduce(tensor_y)\n        np.testing.assert_array_equal(tensor_y, sum_result)\n    print('test allreduce sum api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x)\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    else:\n        task = dist.all_reduce(tensor_y)\n        np.testing.assert_array_equal(tensor_y, sum_result)\n    print('test allreduce sum api with = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api with shape = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, min_result)\n    print('test allreduce min api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, min_result)\n    print('test allreduce min api with shape [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, prod_result)\n    print('test allreduce prod api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, prod_result)\n    print('test allreduce prod api with shape = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = dist.broadcast(tensor_x, 0, sync_op=False)\n        task.synchronize()\n        paddle.device.cuda.synchronize()\n        assert task.is_completed()\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = dist.broadcast(tensor_y, 0)\n        paddle.device.cuda.synchronize()\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    print('test broadcast api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = dist.broadcast(tensor_x, 0, sync_op=False)\n        task.synchronize()\n        paddle.device.cuda.synchronize()\n        assert task.is_completed()\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = dist.broadcast(tensor_y, 0)\n        paddle.device.cuda.synchronize()\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    assert tensor_y.shape == []\n    print('test broadcast api with shape=[] ok')\n    if pg.rank() == 0:\n        pg.barrier(device_id)\n    else:\n        task = pg.barrier(device_id)\n        task.wait()\n    print('test barrier api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        tensor_out_list = [paddle.empty_like(tensor_x), paddle.empty_like(tensor_x)]\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n        tensor_out = paddle.concat(tensor_out_list)\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api ok\\n')\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        tensor_out_list = []\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n        tensor_out = paddle.concat(tensor_out_list)\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api2 ok\\n')\n    x = np.random.random([]).astype(self.dtype)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out_list = []\n    if pg.rank() == 0:\n        task = dist.all_gather(tensor_out_list, tensor_x)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n    out_1 = tensor_out_list[0]\n    out_2 = tensor_out_list[1]\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api with shape [] ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        (in_1, in_2) = paddle.split(tensor_y, 2)\n        (out_1, out_2) = paddle.split(tensor_out2, 2)\n        out_tensor_list = [out_1, out_2]\n        task = dist.alltoall([in_1, in_2], out_tensor_list)\n        paddle.device.cuda.synchronize()\n        tensor_out2 = paddle.concat(out_tensor_list)\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(out1_2.numpy(), raw_tensor_y_1.numpy())\n    else:\n        np.testing.assert_array_equal(out2_1, raw_tensor_x_2)\n    print('test alltoall api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        (in_1, in_2) = paddle.split(tensor_y, 2)\n        (out_1, out_2) = paddle.split(tensor_out2, 2)\n        out_tensor_list = []\n        task = dist.alltoall([in_1, in_2], out_tensor_list)\n        paddle.device.cuda.synchronize()\n        tensor_out2 = paddle.concat(out_tensor_list)\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(out1_2.numpy(), raw_tensor_y_1.numpy())\n    else:\n        np.testing.assert_array_equal(out2_1, raw_tensor_x_2)\n    print('test alltoall api2 ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.reduce(tensor_y, 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    print('test reduce sum api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n    print('test reduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n    print('test reduce min api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n    print('test reduce prod api ok')\n    test_reduce_with_zero_dim([], self.dtype, pg)\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        (in_1, in_2) = paddle.split(tensor_x, 2)\n        task = dist.scatter(tensor_y, [in_1, in_2], 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.scatter(tensor_y, [], 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    print('test scatter api ok\\n')\n    x = np.random.random([]).astype(self.dtype)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        (in_1, in_2) = (tensor_x, tensor_x + 1)\n        task = dist.scatter(tensor_y, [in_1, in_2], 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.scatter(tensor_y, [], 0, sync_op=True)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    out1 = paddle.assign(tensor_x)\n    out2 = paddle.assign(tensor_x + 1)\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    assert tensor_y.shape == []\n    print('test scatter api with shape=[] ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=False)\n        task.wait()\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, tensor_x)\n    print('test send api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=True)\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=True)\n        np.testing.assert_array_equal(tensor_y, tensor_x)\n    print('test send api ok')\n    x = np.random.uniform(-1, 1, []).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.array(0.2022).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=True)\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=True)\n        assert np.array_equal(tensor_y, tensor_x) and tensor_y.shape == []\n    print('test send & recv 0-d tensor ok')",
            "def test_create_process_group_nccl(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    device_id = paddle.distributed.ParallelEnv().dev_id\n    paddle.set_device('gpu:%d' % device_id)\n    assert paddle.distributed.is_available()\n    pg = init_process_group()\n    print('rank:', pg.rank(), 'size:', pg.size(), 'name:', pg.name())\n    print('test new group api ok')\n    assert paddle.distributed.get_backend() == 'NCCL'\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x)\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    else:\n        task = dist.all_reduce(tensor_y)\n        np.testing.assert_array_equal(tensor_y, sum_result)\n    print('test allreduce sum api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x)\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    else:\n        task = dist.all_reduce(tensor_y)\n        np.testing.assert_array_equal(tensor_y, sum_result)\n    print('test allreduce sum api with = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api with shape = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, min_result)\n    print('test allreduce min api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, min_result)\n    print('test allreduce min api with shape [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, prod_result)\n    print('test allreduce prod api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, prod_result)\n    print('test allreduce prod api with shape = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = dist.broadcast(tensor_x, 0, sync_op=False)\n        task.synchronize()\n        paddle.device.cuda.synchronize()\n        assert task.is_completed()\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = dist.broadcast(tensor_y, 0)\n        paddle.device.cuda.synchronize()\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    print('test broadcast api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = dist.broadcast(tensor_x, 0, sync_op=False)\n        task.synchronize()\n        paddle.device.cuda.synchronize()\n        assert task.is_completed()\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = dist.broadcast(tensor_y, 0)\n        paddle.device.cuda.synchronize()\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    assert tensor_y.shape == []\n    print('test broadcast api with shape=[] ok')\n    if pg.rank() == 0:\n        pg.barrier(device_id)\n    else:\n        task = pg.barrier(device_id)\n        task.wait()\n    print('test barrier api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        tensor_out_list = [paddle.empty_like(tensor_x), paddle.empty_like(tensor_x)]\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n        tensor_out = paddle.concat(tensor_out_list)\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api ok\\n')\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        tensor_out_list = []\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n        tensor_out = paddle.concat(tensor_out_list)\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api2 ok\\n')\n    x = np.random.random([]).astype(self.dtype)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out_list = []\n    if pg.rank() == 0:\n        task = dist.all_gather(tensor_out_list, tensor_x)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n    out_1 = tensor_out_list[0]\n    out_2 = tensor_out_list[1]\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api with shape [] ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        (in_1, in_2) = paddle.split(tensor_y, 2)\n        (out_1, out_2) = paddle.split(tensor_out2, 2)\n        out_tensor_list = [out_1, out_2]\n        task = dist.alltoall([in_1, in_2], out_tensor_list)\n        paddle.device.cuda.synchronize()\n        tensor_out2 = paddle.concat(out_tensor_list)\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(out1_2.numpy(), raw_tensor_y_1.numpy())\n    else:\n        np.testing.assert_array_equal(out2_1, raw_tensor_x_2)\n    print('test alltoall api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        (in_1, in_2) = paddle.split(tensor_y, 2)\n        (out_1, out_2) = paddle.split(tensor_out2, 2)\n        out_tensor_list = []\n        task = dist.alltoall([in_1, in_2], out_tensor_list)\n        paddle.device.cuda.synchronize()\n        tensor_out2 = paddle.concat(out_tensor_list)\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(out1_2.numpy(), raw_tensor_y_1.numpy())\n    else:\n        np.testing.assert_array_equal(out2_1, raw_tensor_x_2)\n    print('test alltoall api2 ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.reduce(tensor_y, 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    print('test reduce sum api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n    print('test reduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n    print('test reduce min api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n    print('test reduce prod api ok')\n    test_reduce_with_zero_dim([], self.dtype, pg)\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        (in_1, in_2) = paddle.split(tensor_x, 2)\n        task = dist.scatter(tensor_y, [in_1, in_2], 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.scatter(tensor_y, [], 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    print('test scatter api ok\\n')\n    x = np.random.random([]).astype(self.dtype)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        (in_1, in_2) = (tensor_x, tensor_x + 1)\n        task = dist.scatter(tensor_y, [in_1, in_2], 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.scatter(tensor_y, [], 0, sync_op=True)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    out1 = paddle.assign(tensor_x)\n    out2 = paddle.assign(tensor_x + 1)\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    assert tensor_y.shape == []\n    print('test scatter api with shape=[] ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=False)\n        task.wait()\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, tensor_x)\n    print('test send api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=True)\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=True)\n        np.testing.assert_array_equal(tensor_y, tensor_x)\n    print('test send api ok')\n    x = np.random.uniform(-1, 1, []).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.array(0.2022).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=True)\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=True)\n        assert np.array_equal(tensor_y, tensor_x) and tensor_y.shape == []\n    print('test send & recv 0-d tensor ok')",
            "def test_create_process_group_nccl(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    device_id = paddle.distributed.ParallelEnv().dev_id\n    paddle.set_device('gpu:%d' % device_id)\n    assert paddle.distributed.is_available()\n    pg = init_process_group()\n    print('rank:', pg.rank(), 'size:', pg.size(), 'name:', pg.name())\n    print('test new group api ok')\n    assert paddle.distributed.get_backend() == 'NCCL'\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x)\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    else:\n        task = dist.all_reduce(tensor_y)\n        np.testing.assert_array_equal(tensor_y, sum_result)\n    print('test allreduce sum api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x)\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    else:\n        task = dist.all_reduce(tensor_y)\n        np.testing.assert_array_equal(tensor_y, sum_result)\n    print('test allreduce sum api with = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, max_result)\n    print('test allreduce max api with shape = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, min_result)\n    print('test allreduce min api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, min_result)\n    print('test allreduce min api with shape [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, prod_result)\n    print('test allreduce prod api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.all_reduce(tensor_x, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.all_reduce(tensor_y, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, prod_result)\n    print('test allreduce prod api with shape = [] ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = dist.broadcast(tensor_x, 0, sync_op=False)\n        task.synchronize()\n        paddle.device.cuda.synchronize()\n        assert task.is_completed()\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = dist.broadcast(tensor_y, 0)\n        paddle.device.cuda.synchronize()\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    print('test broadcast api ok')\n    x = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    broadcast_result = paddle.assign(tensor_x)\n    if pg.rank() == 0:\n        task = dist.broadcast(tensor_x, 0, sync_op=False)\n        task.synchronize()\n        paddle.device.cuda.synchronize()\n        assert task.is_completed()\n        np.testing.assert_array_equal(broadcast_result, tensor_x)\n    else:\n        task = dist.broadcast(tensor_y, 0)\n        paddle.device.cuda.synchronize()\n        np.testing.assert_array_equal(broadcast_result, tensor_y)\n    assert tensor_y.shape == []\n    print('test broadcast api with shape=[] ok')\n    if pg.rank() == 0:\n        pg.barrier(device_id)\n    else:\n        task = pg.barrier(device_id)\n        task.wait()\n    print('test barrier api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    out_shape = list(self.shape)\n    out_shape[0] *= 2\n    out = np.random.random(out_shape).astype(self.dtype)\n    tensor_out = paddle.to_tensor(out)\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        tensor_out_list = [paddle.empty_like(tensor_x), paddle.empty_like(tensor_x)]\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n        tensor_out = paddle.concat(tensor_out_list)\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api ok\\n')\n    if pg.rank() == 0:\n        task = pg.all_gather(tensor_x, tensor_out)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        tensor_out_list = []\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n        tensor_out = paddle.concat(tensor_out_list)\n    out_1 = paddle.slice(tensor_out, [0], [0], [out_shape[0] // 2])\n    out_2 = paddle.slice(tensor_out, [0], [out_shape[0] // 2], [out_shape[0]])\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api2 ok\\n')\n    x = np.random.random([]).astype(self.dtype)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out_list = []\n    if pg.rank() == 0:\n        task = dist.all_gather(tensor_out_list, tensor_x)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.all_gather(tensor_out_list, tensor_y, sync_op=False)\n        paddle.device.cuda.synchronize()\n    out_1 = tensor_out_list[0]\n    out_2 = tensor_out_list[1]\n    np.testing.assert_array_equal(tensor_x, out_1)\n    np.testing.assert_array_equal(tensor_y, out_2)\n    print('test allgather api with shape [] ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        (in_1, in_2) = paddle.split(tensor_y, 2)\n        (out_1, out_2) = paddle.split(tensor_out2, 2)\n        out_tensor_list = [out_1, out_2]\n        task = dist.alltoall([in_1, in_2], out_tensor_list)\n        paddle.device.cuda.synchronize()\n        tensor_out2 = paddle.concat(out_tensor_list)\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(out1_2.numpy(), raw_tensor_y_1.numpy())\n    else:\n        np.testing.assert_array_equal(out2_1, raw_tensor_x_2)\n    print('test alltoall api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    out1 = np.random.random(self.shape).astype(self.dtype)\n    out2 = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    tensor_out1 = paddle.to_tensor(out1)\n    tensor_out2 = paddle.to_tensor(out2)\n    raw_tensor_x_2 = paddle.slice(tensor_x, [0], [self.shape[0] // 2], [self.shape[0]])\n    raw_tensor_y_1 = paddle.slice(tensor_y, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        task = pg.alltoall(tensor_x, tensor_out1)\n        task.wait()\n    else:\n        (in_1, in_2) = paddle.split(tensor_y, 2)\n        (out_1, out_2) = paddle.split(tensor_out2, 2)\n        out_tensor_list = []\n        task = dist.alltoall([in_1, in_2], out_tensor_list)\n        paddle.device.cuda.synchronize()\n        tensor_out2 = paddle.concat(out_tensor_list)\n    out1_2 = paddle.slice(tensor_out1, [0], [self.shape[0] // 2], [self.shape[0]])\n    out2_1 = paddle.slice(tensor_out2, [0], [0], [self.shape[0] // 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(out1_2.numpy(), raw_tensor_y_1.numpy())\n    else:\n        np.testing.assert_array_equal(out2_1, raw_tensor_x_2)\n    print('test alltoall api2 ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.reduce(tensor_y, 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_x, sum_result)\n    print('test reduce sum api ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, max_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n    print('test reduce max api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, min_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n    print('test reduce min api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_x, prod_result)\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n    print('test reduce prod api ok')\n    test_reduce_with_zero_dim([], self.dtype, pg)\n    in_shape = list(self.shape)\n    in_shape[0] *= 2\n    x = np.random.random(in_shape).astype(self.dtype)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        (in_1, in_2) = paddle.split(tensor_x, 2)\n        task = dist.scatter(tensor_y, [in_1, in_2], 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.scatter(tensor_y, [], 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    out1 = paddle.slice(tensor_x, [0], [0], [self.shape[0]])\n    out2 = paddle.slice(tensor_x, [0], [self.shape[0]], [self.shape[0] * 2])\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    print('test scatter api ok\\n')\n    x = np.random.random([]).astype(self.dtype)\n    y = np.random.random([]).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        (in_1, in_2) = (tensor_x, tensor_x + 1)\n        task = dist.scatter(tensor_y, [in_1, in_2], 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.scatter(tensor_y, [], 0, sync_op=True)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    out1 = paddle.assign(tensor_x)\n    out2 = paddle.assign(tensor_x + 1)\n    if pg.rank() == 0:\n        np.testing.assert_array_equal(tensor_y, out1)\n    else:\n        np.testing.assert_array_equal(tensor_y, out2)\n    assert tensor_y.shape == []\n    print('test scatter api with shape=[] ok\\n')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=False)\n        task.wait()\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=False)\n        task.wait()\n        np.testing.assert_array_equal(tensor_y, tensor_x)\n    print('test send api ok')\n    x = np.random.random(self.shape).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(self.shape).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=True)\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=True)\n        np.testing.assert_array_equal(tensor_y, tensor_x)\n    print('test send api ok')\n    x = np.random.uniform(-1, 1, []).astype(self.dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.array(0.2022).astype(self.dtype)\n    tensor_y = paddle.to_tensor(y)\n    if pg.rank() == 0:\n        task = dist.send(tensor_x, 1, sync_op=True)\n    else:\n        task = dist.recv(tensor_y, 0, sync_op=True)\n        assert np.array_equal(tensor_y, tensor_x) and tensor_y.shape == []\n    print('test send & recv 0-d tensor ok')"
        ]
    },
    {
        "func_name": "setUp",
        "original": "def setUp(self):\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
        "mutated": [
            "def setUp(self):\n    if False:\n        i = 10\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()",
            "def setUp(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    paddle.seed(2022)\n    random.seed(2022)\n    np.random.seed(2022)\n    self.config()"
        ]
    },
    {
        "func_name": "config",
        "original": "def config(self):\n    self.dtype = 'float16'\n    self.shape = (4, 20, 20)",
        "mutated": [
            "def config(self):\n    if False:\n        i = 10\n    self.dtype = 'float16'\n    self.shape = (4, 20, 20)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.dtype = 'float16'\n    self.shape = (4, 20, 20)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.dtype = 'float16'\n    self.shape = (4, 20, 20)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.dtype = 'float16'\n    self.shape = (4, 20, 20)",
            "def config(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.dtype = 'float16'\n    self.shape = (4, 20, 20)"
        ]
    },
    {
        "func_name": "test_reduce_with_zero_dim",
        "original": "def test_reduce_with_zero_dim(shape, dtype, pg):\n    x = np.random.random(shape).astype(dtype)\n    y = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.reduce(tensor_y, 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    if pg.rank() == 0:\n        assert np.array_equal(tensor_x, sum_result) and len(tensor_x.shape) == 0\n    print('test reduce with zero dim sum api ok\\n')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, max_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim max api ok')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, min_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim min api ok')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, prod_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim prod api ok')",
        "mutated": [
            "def test_reduce_with_zero_dim(shape, dtype, pg):\n    if False:\n        i = 10\n    x = np.random.random(shape).astype(dtype)\n    y = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.reduce(tensor_y, 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    if pg.rank() == 0:\n        assert np.array_equal(tensor_x, sum_result) and len(tensor_x.shape) == 0\n    print('test reduce with zero dim sum api ok\\n')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, max_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim max api ok')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, min_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim min api ok')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, prod_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim prod api ok')",
            "def test_reduce_with_zero_dim(shape, dtype, pg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    x = np.random.random(shape).astype(dtype)\n    y = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.reduce(tensor_y, 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    if pg.rank() == 0:\n        assert np.array_equal(tensor_x, sum_result) and len(tensor_x.shape) == 0\n    print('test reduce with zero dim sum api ok\\n')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, max_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim max api ok')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, min_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim min api ok')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, prod_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim prod api ok')",
            "def test_reduce_with_zero_dim(shape, dtype, pg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    x = np.random.random(shape).astype(dtype)\n    y = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.reduce(tensor_y, 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    if pg.rank() == 0:\n        assert np.array_equal(tensor_x, sum_result) and len(tensor_x.shape) == 0\n    print('test reduce with zero dim sum api ok\\n')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, max_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim max api ok')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, min_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim min api ok')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, prod_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim prod api ok')",
            "def test_reduce_with_zero_dim(shape, dtype, pg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    x = np.random.random(shape).astype(dtype)\n    y = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.reduce(tensor_y, 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    if pg.rank() == 0:\n        assert np.array_equal(tensor_x, sum_result) and len(tensor_x.shape) == 0\n    print('test reduce with zero dim sum api ok\\n')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, max_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim max api ok')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, min_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim min api ok')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, prod_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim prod api ok')",
            "def test_reduce_with_zero_dim(shape, dtype, pg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    x = np.random.random(shape).astype(dtype)\n    y = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    tensor_y = paddle.to_tensor(y)\n    sum_result = tensor_x + tensor_y\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, sync_op=True)\n        paddle.device.cuda.synchronize()\n    else:\n        task = dist.reduce(tensor_y, 0, sync_op=False)\n        task.wait()\n        paddle.device.cuda.synchronize()\n    if pg.rank() == 0:\n        assert np.array_equal(tensor_x, sum_result) and len(tensor_x.shape) == 0\n    print('test reduce with zero dim sum api ok\\n')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    max_result = paddle.maximum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, max_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MAX, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim max api ok')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    min_result = paddle.minimum(tensor_x, tensor_y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, min_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.MIN, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim min api ok')\n    x = np.random.random(shape).astype(dtype)\n    tensor_x = paddle.to_tensor(x)\n    y = np.random.random(shape).astype(dtype)\n    tensor_y = paddle.to_tensor(y)\n    prod_result = np.multiply(x, y)\n    if pg.rank() == 0:\n        task = dist.reduce(tensor_x, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n        assert np.array_equal(tensor_x, prod_result) and len(tensor_x.shape) == 0\n    else:\n        task = dist.reduce(tensor_y, 0, dist.ReduceOp.PROD, sync_op=False)\n        task.wait()\n    print('test reduce with zero dim prod api ok')"
        ]
    }
]