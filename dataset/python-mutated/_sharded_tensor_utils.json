[
    {
        "func_name": "rewrite_dict",
        "original": "def rewrite_dict(path: OBJ_PATH, value: STATE_DICT_ITEM) -> None:\n    if not isinstance(value, ShardedTensor):\n        set_element(new_state_dict, path, value)\n        return\n    shards = value.local_shards()\n    if len(shards) == 0:\n        return\n    if len(shards) != 1:\n        set_element(new_state_dict, path, value)\n        return\n    outer_shard = shards[0]\n    inner_st = outer_shard.tensor\n    if not isinstance(inner_st, ShardedTensor):\n        set_element(new_state_dict, path, value)\n        return\n    if len(inner_st.local_shards()) != 1:\n        raise ValueError('Cannot handle inner tensor with more than 1 shard')\n    inner_shard = inner_st.local_shards()[0]\n    local_shards = [Shard(tensor=inner_shard.tensor, metadata=ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_shard.metadata.shard_offsets), shard_sizes=inner_shard.metadata.shard_sizes, placement=f'rank:{dist.get_rank()}/{inner_shard.tensor.device}'))]\n    st_meta: ShardedTensorMetadata = copy.deepcopy(value.metadata())\n    other_rank = 0 if dist.get_rank() > 0 else 1\n    device_info = _normalize_device_info(inner_shard.tensor.device.type, 0)\n    for (i, shard_md) in enumerate(st_meta.shards_metadata):\n        if shard_md.shard_offsets == outer_shard.metadata.shard_offsets:\n            st_meta.shards_metadata.pop(i)\n            break\n    for shard_md in st_meta.shards_metadata:\n        shard_md.placement = _remote_device(f'rank:{other_rank}/{device_info}')\n    for inner_md in inner_st.metadata().shards_metadata:\n        if inner_md.shard_offsets != inner_shard.metadata.shard_offsets:\n            st_meta.shards_metadata.append(ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_md.shard_offsets), shard_sizes=inner_md.shard_sizes, placement=f'rank:{other_rank}/{device_info}'))\n    st_meta.shards_metadata.append(local_shards[0].metadata)\n    st = ShardedTensor._init_from_local_shards_and_global_metadata(local_shards=local_shards, sharded_tensor_metadata=st_meta)\n    set_element(new_state_dict, path, st)",
        "mutated": [
            "def rewrite_dict(path: OBJ_PATH, value: STATE_DICT_ITEM) -> None:\n    if False:\n        i = 10\n    if not isinstance(value, ShardedTensor):\n        set_element(new_state_dict, path, value)\n        return\n    shards = value.local_shards()\n    if len(shards) == 0:\n        return\n    if len(shards) != 1:\n        set_element(new_state_dict, path, value)\n        return\n    outer_shard = shards[0]\n    inner_st = outer_shard.tensor\n    if not isinstance(inner_st, ShardedTensor):\n        set_element(new_state_dict, path, value)\n        return\n    if len(inner_st.local_shards()) != 1:\n        raise ValueError('Cannot handle inner tensor with more than 1 shard')\n    inner_shard = inner_st.local_shards()[0]\n    local_shards = [Shard(tensor=inner_shard.tensor, metadata=ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_shard.metadata.shard_offsets), shard_sizes=inner_shard.metadata.shard_sizes, placement=f'rank:{dist.get_rank()}/{inner_shard.tensor.device}'))]\n    st_meta: ShardedTensorMetadata = copy.deepcopy(value.metadata())\n    other_rank = 0 if dist.get_rank() > 0 else 1\n    device_info = _normalize_device_info(inner_shard.tensor.device.type, 0)\n    for (i, shard_md) in enumerate(st_meta.shards_metadata):\n        if shard_md.shard_offsets == outer_shard.metadata.shard_offsets:\n            st_meta.shards_metadata.pop(i)\n            break\n    for shard_md in st_meta.shards_metadata:\n        shard_md.placement = _remote_device(f'rank:{other_rank}/{device_info}')\n    for inner_md in inner_st.metadata().shards_metadata:\n        if inner_md.shard_offsets != inner_shard.metadata.shard_offsets:\n            st_meta.shards_metadata.append(ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_md.shard_offsets), shard_sizes=inner_md.shard_sizes, placement=f'rank:{other_rank}/{device_info}'))\n    st_meta.shards_metadata.append(local_shards[0].metadata)\n    st = ShardedTensor._init_from_local_shards_and_global_metadata(local_shards=local_shards, sharded_tensor_metadata=st_meta)\n    set_element(new_state_dict, path, st)",
            "def rewrite_dict(path: OBJ_PATH, value: STATE_DICT_ITEM) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not isinstance(value, ShardedTensor):\n        set_element(new_state_dict, path, value)\n        return\n    shards = value.local_shards()\n    if len(shards) == 0:\n        return\n    if len(shards) != 1:\n        set_element(new_state_dict, path, value)\n        return\n    outer_shard = shards[0]\n    inner_st = outer_shard.tensor\n    if not isinstance(inner_st, ShardedTensor):\n        set_element(new_state_dict, path, value)\n        return\n    if len(inner_st.local_shards()) != 1:\n        raise ValueError('Cannot handle inner tensor with more than 1 shard')\n    inner_shard = inner_st.local_shards()[0]\n    local_shards = [Shard(tensor=inner_shard.tensor, metadata=ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_shard.metadata.shard_offsets), shard_sizes=inner_shard.metadata.shard_sizes, placement=f'rank:{dist.get_rank()}/{inner_shard.tensor.device}'))]\n    st_meta: ShardedTensorMetadata = copy.deepcopy(value.metadata())\n    other_rank = 0 if dist.get_rank() > 0 else 1\n    device_info = _normalize_device_info(inner_shard.tensor.device.type, 0)\n    for (i, shard_md) in enumerate(st_meta.shards_metadata):\n        if shard_md.shard_offsets == outer_shard.metadata.shard_offsets:\n            st_meta.shards_metadata.pop(i)\n            break\n    for shard_md in st_meta.shards_metadata:\n        shard_md.placement = _remote_device(f'rank:{other_rank}/{device_info}')\n    for inner_md in inner_st.metadata().shards_metadata:\n        if inner_md.shard_offsets != inner_shard.metadata.shard_offsets:\n            st_meta.shards_metadata.append(ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_md.shard_offsets), shard_sizes=inner_md.shard_sizes, placement=f'rank:{other_rank}/{device_info}'))\n    st_meta.shards_metadata.append(local_shards[0].metadata)\n    st = ShardedTensor._init_from_local_shards_and_global_metadata(local_shards=local_shards, sharded_tensor_metadata=st_meta)\n    set_element(new_state_dict, path, st)",
            "def rewrite_dict(path: OBJ_PATH, value: STATE_DICT_ITEM) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not isinstance(value, ShardedTensor):\n        set_element(new_state_dict, path, value)\n        return\n    shards = value.local_shards()\n    if len(shards) == 0:\n        return\n    if len(shards) != 1:\n        set_element(new_state_dict, path, value)\n        return\n    outer_shard = shards[0]\n    inner_st = outer_shard.tensor\n    if not isinstance(inner_st, ShardedTensor):\n        set_element(new_state_dict, path, value)\n        return\n    if len(inner_st.local_shards()) != 1:\n        raise ValueError('Cannot handle inner tensor with more than 1 shard')\n    inner_shard = inner_st.local_shards()[0]\n    local_shards = [Shard(tensor=inner_shard.tensor, metadata=ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_shard.metadata.shard_offsets), shard_sizes=inner_shard.metadata.shard_sizes, placement=f'rank:{dist.get_rank()}/{inner_shard.tensor.device}'))]\n    st_meta: ShardedTensorMetadata = copy.deepcopy(value.metadata())\n    other_rank = 0 if dist.get_rank() > 0 else 1\n    device_info = _normalize_device_info(inner_shard.tensor.device.type, 0)\n    for (i, shard_md) in enumerate(st_meta.shards_metadata):\n        if shard_md.shard_offsets == outer_shard.metadata.shard_offsets:\n            st_meta.shards_metadata.pop(i)\n            break\n    for shard_md in st_meta.shards_metadata:\n        shard_md.placement = _remote_device(f'rank:{other_rank}/{device_info}')\n    for inner_md in inner_st.metadata().shards_metadata:\n        if inner_md.shard_offsets != inner_shard.metadata.shard_offsets:\n            st_meta.shards_metadata.append(ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_md.shard_offsets), shard_sizes=inner_md.shard_sizes, placement=f'rank:{other_rank}/{device_info}'))\n    st_meta.shards_metadata.append(local_shards[0].metadata)\n    st = ShardedTensor._init_from_local_shards_and_global_metadata(local_shards=local_shards, sharded_tensor_metadata=st_meta)\n    set_element(new_state_dict, path, st)",
            "def rewrite_dict(path: OBJ_PATH, value: STATE_DICT_ITEM) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not isinstance(value, ShardedTensor):\n        set_element(new_state_dict, path, value)\n        return\n    shards = value.local_shards()\n    if len(shards) == 0:\n        return\n    if len(shards) != 1:\n        set_element(new_state_dict, path, value)\n        return\n    outer_shard = shards[0]\n    inner_st = outer_shard.tensor\n    if not isinstance(inner_st, ShardedTensor):\n        set_element(new_state_dict, path, value)\n        return\n    if len(inner_st.local_shards()) != 1:\n        raise ValueError('Cannot handle inner tensor with more than 1 shard')\n    inner_shard = inner_st.local_shards()[0]\n    local_shards = [Shard(tensor=inner_shard.tensor, metadata=ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_shard.metadata.shard_offsets), shard_sizes=inner_shard.metadata.shard_sizes, placement=f'rank:{dist.get_rank()}/{inner_shard.tensor.device}'))]\n    st_meta: ShardedTensorMetadata = copy.deepcopy(value.metadata())\n    other_rank = 0 if dist.get_rank() > 0 else 1\n    device_info = _normalize_device_info(inner_shard.tensor.device.type, 0)\n    for (i, shard_md) in enumerate(st_meta.shards_metadata):\n        if shard_md.shard_offsets == outer_shard.metadata.shard_offsets:\n            st_meta.shards_metadata.pop(i)\n            break\n    for shard_md in st_meta.shards_metadata:\n        shard_md.placement = _remote_device(f'rank:{other_rank}/{device_info}')\n    for inner_md in inner_st.metadata().shards_metadata:\n        if inner_md.shard_offsets != inner_shard.metadata.shard_offsets:\n            st_meta.shards_metadata.append(ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_md.shard_offsets), shard_sizes=inner_md.shard_sizes, placement=f'rank:{other_rank}/{device_info}'))\n    st_meta.shards_metadata.append(local_shards[0].metadata)\n    st = ShardedTensor._init_from_local_shards_and_global_metadata(local_shards=local_shards, sharded_tensor_metadata=st_meta)\n    set_element(new_state_dict, path, st)",
            "def rewrite_dict(path: OBJ_PATH, value: STATE_DICT_ITEM) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not isinstance(value, ShardedTensor):\n        set_element(new_state_dict, path, value)\n        return\n    shards = value.local_shards()\n    if len(shards) == 0:\n        return\n    if len(shards) != 1:\n        set_element(new_state_dict, path, value)\n        return\n    outer_shard = shards[0]\n    inner_st = outer_shard.tensor\n    if not isinstance(inner_st, ShardedTensor):\n        set_element(new_state_dict, path, value)\n        return\n    if len(inner_st.local_shards()) != 1:\n        raise ValueError('Cannot handle inner tensor with more than 1 shard')\n    inner_shard = inner_st.local_shards()[0]\n    local_shards = [Shard(tensor=inner_shard.tensor, metadata=ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_shard.metadata.shard_offsets), shard_sizes=inner_shard.metadata.shard_sizes, placement=f'rank:{dist.get_rank()}/{inner_shard.tensor.device}'))]\n    st_meta: ShardedTensorMetadata = copy.deepcopy(value.metadata())\n    other_rank = 0 if dist.get_rank() > 0 else 1\n    device_info = _normalize_device_info(inner_shard.tensor.device.type, 0)\n    for (i, shard_md) in enumerate(st_meta.shards_metadata):\n        if shard_md.shard_offsets == outer_shard.metadata.shard_offsets:\n            st_meta.shards_metadata.pop(i)\n            break\n    for shard_md in st_meta.shards_metadata:\n        shard_md.placement = _remote_device(f'rank:{other_rank}/{device_info}')\n    for inner_md in inner_st.metadata().shards_metadata:\n        if inner_md.shard_offsets != inner_shard.metadata.shard_offsets:\n            st_meta.shards_metadata.append(ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_md.shard_offsets), shard_sizes=inner_md.shard_sizes, placement=f'rank:{other_rank}/{device_info}'))\n    st_meta.shards_metadata.append(local_shards[0].metadata)\n    st = ShardedTensor._init_from_local_shards_and_global_metadata(local_shards=local_shards, sharded_tensor_metadata=st_meta)\n    set_element(new_state_dict, path, st)"
        ]
    },
    {
        "func_name": "_flatten_sharded_tensors",
        "original": "def _flatten_sharded_tensors(state_dict: STATE_DICT_TYPE) -> STATE_DICT_TYPE:\n    \"\"\"\n    Transform ``state_dict`` by flattening all nested ShardedTensor instances found.\n\n    The resulting ShardedTensor instances are only correct regarding the local shard and\n    MUST not be used for any other purpose but checkpointing, as no operator will work with them.\n\n    This function should be used in conjunction with a state_dict produced by FSDP's\n    StateDictType.SHARDED_STATE_DICT methods.\n    \"\"\"\n    new_state_dict: STATE_DICT_TYPE = {}\n\n    def rewrite_dict(path: OBJ_PATH, value: STATE_DICT_ITEM) -> None:\n        if not isinstance(value, ShardedTensor):\n            set_element(new_state_dict, path, value)\n            return\n        shards = value.local_shards()\n        if len(shards) == 0:\n            return\n        if len(shards) != 1:\n            set_element(new_state_dict, path, value)\n            return\n        outer_shard = shards[0]\n        inner_st = outer_shard.tensor\n        if not isinstance(inner_st, ShardedTensor):\n            set_element(new_state_dict, path, value)\n            return\n        if len(inner_st.local_shards()) != 1:\n            raise ValueError('Cannot handle inner tensor with more than 1 shard')\n        inner_shard = inner_st.local_shards()[0]\n        local_shards = [Shard(tensor=inner_shard.tensor, metadata=ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_shard.metadata.shard_offsets), shard_sizes=inner_shard.metadata.shard_sizes, placement=f'rank:{dist.get_rank()}/{inner_shard.tensor.device}'))]\n        st_meta: ShardedTensorMetadata = copy.deepcopy(value.metadata())\n        other_rank = 0 if dist.get_rank() > 0 else 1\n        device_info = _normalize_device_info(inner_shard.tensor.device.type, 0)\n        for (i, shard_md) in enumerate(st_meta.shards_metadata):\n            if shard_md.shard_offsets == outer_shard.metadata.shard_offsets:\n                st_meta.shards_metadata.pop(i)\n                break\n        for shard_md in st_meta.shards_metadata:\n            shard_md.placement = _remote_device(f'rank:{other_rank}/{device_info}')\n        for inner_md in inner_st.metadata().shards_metadata:\n            if inner_md.shard_offsets != inner_shard.metadata.shard_offsets:\n                st_meta.shards_metadata.append(ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_md.shard_offsets), shard_sizes=inner_md.shard_sizes, placement=f'rank:{other_rank}/{device_info}'))\n        st_meta.shards_metadata.append(local_shards[0].metadata)\n        st = ShardedTensor._init_from_local_shards_and_global_metadata(local_shards=local_shards, sharded_tensor_metadata=st_meta)\n        set_element(new_state_dict, path, st)\n    traverse_state_dict(state_dict, rewrite_dict)\n    return new_state_dict",
        "mutated": [
            "def _flatten_sharded_tensors(state_dict: STATE_DICT_TYPE) -> STATE_DICT_TYPE:\n    if False:\n        i = 10\n    \"\\n    Transform ``state_dict`` by flattening all nested ShardedTensor instances found.\\n\\n    The resulting ShardedTensor instances are only correct regarding the local shard and\\n    MUST not be used for any other purpose but checkpointing, as no operator will work with them.\\n\\n    This function should be used in conjunction with a state_dict produced by FSDP's\\n    StateDictType.SHARDED_STATE_DICT methods.\\n    \"\n    new_state_dict: STATE_DICT_TYPE = {}\n\n    def rewrite_dict(path: OBJ_PATH, value: STATE_DICT_ITEM) -> None:\n        if not isinstance(value, ShardedTensor):\n            set_element(new_state_dict, path, value)\n            return\n        shards = value.local_shards()\n        if len(shards) == 0:\n            return\n        if len(shards) != 1:\n            set_element(new_state_dict, path, value)\n            return\n        outer_shard = shards[0]\n        inner_st = outer_shard.tensor\n        if not isinstance(inner_st, ShardedTensor):\n            set_element(new_state_dict, path, value)\n            return\n        if len(inner_st.local_shards()) != 1:\n            raise ValueError('Cannot handle inner tensor with more than 1 shard')\n        inner_shard = inner_st.local_shards()[0]\n        local_shards = [Shard(tensor=inner_shard.tensor, metadata=ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_shard.metadata.shard_offsets), shard_sizes=inner_shard.metadata.shard_sizes, placement=f'rank:{dist.get_rank()}/{inner_shard.tensor.device}'))]\n        st_meta: ShardedTensorMetadata = copy.deepcopy(value.metadata())\n        other_rank = 0 if dist.get_rank() > 0 else 1\n        device_info = _normalize_device_info(inner_shard.tensor.device.type, 0)\n        for (i, shard_md) in enumerate(st_meta.shards_metadata):\n            if shard_md.shard_offsets == outer_shard.metadata.shard_offsets:\n                st_meta.shards_metadata.pop(i)\n                break\n        for shard_md in st_meta.shards_metadata:\n            shard_md.placement = _remote_device(f'rank:{other_rank}/{device_info}')\n        for inner_md in inner_st.metadata().shards_metadata:\n            if inner_md.shard_offsets != inner_shard.metadata.shard_offsets:\n                st_meta.shards_metadata.append(ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_md.shard_offsets), shard_sizes=inner_md.shard_sizes, placement=f'rank:{other_rank}/{device_info}'))\n        st_meta.shards_metadata.append(local_shards[0].metadata)\n        st = ShardedTensor._init_from_local_shards_and_global_metadata(local_shards=local_shards, sharded_tensor_metadata=st_meta)\n        set_element(new_state_dict, path, st)\n    traverse_state_dict(state_dict, rewrite_dict)\n    return new_state_dict",
            "def _flatten_sharded_tensors(state_dict: STATE_DICT_TYPE) -> STATE_DICT_TYPE:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Transform ``state_dict`` by flattening all nested ShardedTensor instances found.\\n\\n    The resulting ShardedTensor instances are only correct regarding the local shard and\\n    MUST not be used for any other purpose but checkpointing, as no operator will work with them.\\n\\n    This function should be used in conjunction with a state_dict produced by FSDP's\\n    StateDictType.SHARDED_STATE_DICT methods.\\n    \"\n    new_state_dict: STATE_DICT_TYPE = {}\n\n    def rewrite_dict(path: OBJ_PATH, value: STATE_DICT_ITEM) -> None:\n        if not isinstance(value, ShardedTensor):\n            set_element(new_state_dict, path, value)\n            return\n        shards = value.local_shards()\n        if len(shards) == 0:\n            return\n        if len(shards) != 1:\n            set_element(new_state_dict, path, value)\n            return\n        outer_shard = shards[0]\n        inner_st = outer_shard.tensor\n        if not isinstance(inner_st, ShardedTensor):\n            set_element(new_state_dict, path, value)\n            return\n        if len(inner_st.local_shards()) != 1:\n            raise ValueError('Cannot handle inner tensor with more than 1 shard')\n        inner_shard = inner_st.local_shards()[0]\n        local_shards = [Shard(tensor=inner_shard.tensor, metadata=ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_shard.metadata.shard_offsets), shard_sizes=inner_shard.metadata.shard_sizes, placement=f'rank:{dist.get_rank()}/{inner_shard.tensor.device}'))]\n        st_meta: ShardedTensorMetadata = copy.deepcopy(value.metadata())\n        other_rank = 0 if dist.get_rank() > 0 else 1\n        device_info = _normalize_device_info(inner_shard.tensor.device.type, 0)\n        for (i, shard_md) in enumerate(st_meta.shards_metadata):\n            if shard_md.shard_offsets == outer_shard.metadata.shard_offsets:\n                st_meta.shards_metadata.pop(i)\n                break\n        for shard_md in st_meta.shards_metadata:\n            shard_md.placement = _remote_device(f'rank:{other_rank}/{device_info}')\n        for inner_md in inner_st.metadata().shards_metadata:\n            if inner_md.shard_offsets != inner_shard.metadata.shard_offsets:\n                st_meta.shards_metadata.append(ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_md.shard_offsets), shard_sizes=inner_md.shard_sizes, placement=f'rank:{other_rank}/{device_info}'))\n        st_meta.shards_metadata.append(local_shards[0].metadata)\n        st = ShardedTensor._init_from_local_shards_and_global_metadata(local_shards=local_shards, sharded_tensor_metadata=st_meta)\n        set_element(new_state_dict, path, st)\n    traverse_state_dict(state_dict, rewrite_dict)\n    return new_state_dict",
            "def _flatten_sharded_tensors(state_dict: STATE_DICT_TYPE) -> STATE_DICT_TYPE:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Transform ``state_dict`` by flattening all nested ShardedTensor instances found.\\n\\n    The resulting ShardedTensor instances are only correct regarding the local shard and\\n    MUST not be used for any other purpose but checkpointing, as no operator will work with them.\\n\\n    This function should be used in conjunction with a state_dict produced by FSDP's\\n    StateDictType.SHARDED_STATE_DICT methods.\\n    \"\n    new_state_dict: STATE_DICT_TYPE = {}\n\n    def rewrite_dict(path: OBJ_PATH, value: STATE_DICT_ITEM) -> None:\n        if not isinstance(value, ShardedTensor):\n            set_element(new_state_dict, path, value)\n            return\n        shards = value.local_shards()\n        if len(shards) == 0:\n            return\n        if len(shards) != 1:\n            set_element(new_state_dict, path, value)\n            return\n        outer_shard = shards[0]\n        inner_st = outer_shard.tensor\n        if not isinstance(inner_st, ShardedTensor):\n            set_element(new_state_dict, path, value)\n            return\n        if len(inner_st.local_shards()) != 1:\n            raise ValueError('Cannot handle inner tensor with more than 1 shard')\n        inner_shard = inner_st.local_shards()[0]\n        local_shards = [Shard(tensor=inner_shard.tensor, metadata=ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_shard.metadata.shard_offsets), shard_sizes=inner_shard.metadata.shard_sizes, placement=f'rank:{dist.get_rank()}/{inner_shard.tensor.device}'))]\n        st_meta: ShardedTensorMetadata = copy.deepcopy(value.metadata())\n        other_rank = 0 if dist.get_rank() > 0 else 1\n        device_info = _normalize_device_info(inner_shard.tensor.device.type, 0)\n        for (i, shard_md) in enumerate(st_meta.shards_metadata):\n            if shard_md.shard_offsets == outer_shard.metadata.shard_offsets:\n                st_meta.shards_metadata.pop(i)\n                break\n        for shard_md in st_meta.shards_metadata:\n            shard_md.placement = _remote_device(f'rank:{other_rank}/{device_info}')\n        for inner_md in inner_st.metadata().shards_metadata:\n            if inner_md.shard_offsets != inner_shard.metadata.shard_offsets:\n                st_meta.shards_metadata.append(ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_md.shard_offsets), shard_sizes=inner_md.shard_sizes, placement=f'rank:{other_rank}/{device_info}'))\n        st_meta.shards_metadata.append(local_shards[0].metadata)\n        st = ShardedTensor._init_from_local_shards_and_global_metadata(local_shards=local_shards, sharded_tensor_metadata=st_meta)\n        set_element(new_state_dict, path, st)\n    traverse_state_dict(state_dict, rewrite_dict)\n    return new_state_dict",
            "def _flatten_sharded_tensors(state_dict: STATE_DICT_TYPE) -> STATE_DICT_TYPE:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Transform ``state_dict`` by flattening all nested ShardedTensor instances found.\\n\\n    The resulting ShardedTensor instances are only correct regarding the local shard and\\n    MUST not be used for any other purpose but checkpointing, as no operator will work with them.\\n\\n    This function should be used in conjunction with a state_dict produced by FSDP's\\n    StateDictType.SHARDED_STATE_DICT methods.\\n    \"\n    new_state_dict: STATE_DICT_TYPE = {}\n\n    def rewrite_dict(path: OBJ_PATH, value: STATE_DICT_ITEM) -> None:\n        if not isinstance(value, ShardedTensor):\n            set_element(new_state_dict, path, value)\n            return\n        shards = value.local_shards()\n        if len(shards) == 0:\n            return\n        if len(shards) != 1:\n            set_element(new_state_dict, path, value)\n            return\n        outer_shard = shards[0]\n        inner_st = outer_shard.tensor\n        if not isinstance(inner_st, ShardedTensor):\n            set_element(new_state_dict, path, value)\n            return\n        if len(inner_st.local_shards()) != 1:\n            raise ValueError('Cannot handle inner tensor with more than 1 shard')\n        inner_shard = inner_st.local_shards()[0]\n        local_shards = [Shard(tensor=inner_shard.tensor, metadata=ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_shard.metadata.shard_offsets), shard_sizes=inner_shard.metadata.shard_sizes, placement=f'rank:{dist.get_rank()}/{inner_shard.tensor.device}'))]\n        st_meta: ShardedTensorMetadata = copy.deepcopy(value.metadata())\n        other_rank = 0 if dist.get_rank() > 0 else 1\n        device_info = _normalize_device_info(inner_shard.tensor.device.type, 0)\n        for (i, shard_md) in enumerate(st_meta.shards_metadata):\n            if shard_md.shard_offsets == outer_shard.metadata.shard_offsets:\n                st_meta.shards_metadata.pop(i)\n                break\n        for shard_md in st_meta.shards_metadata:\n            shard_md.placement = _remote_device(f'rank:{other_rank}/{device_info}')\n        for inner_md in inner_st.metadata().shards_metadata:\n            if inner_md.shard_offsets != inner_shard.metadata.shard_offsets:\n                st_meta.shards_metadata.append(ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_md.shard_offsets), shard_sizes=inner_md.shard_sizes, placement=f'rank:{other_rank}/{device_info}'))\n        st_meta.shards_metadata.append(local_shards[0].metadata)\n        st = ShardedTensor._init_from_local_shards_and_global_metadata(local_shards=local_shards, sharded_tensor_metadata=st_meta)\n        set_element(new_state_dict, path, st)\n    traverse_state_dict(state_dict, rewrite_dict)\n    return new_state_dict",
            "def _flatten_sharded_tensors(state_dict: STATE_DICT_TYPE) -> STATE_DICT_TYPE:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Transform ``state_dict`` by flattening all nested ShardedTensor instances found.\\n\\n    The resulting ShardedTensor instances are only correct regarding the local shard and\\n    MUST not be used for any other purpose but checkpointing, as no operator will work with them.\\n\\n    This function should be used in conjunction with a state_dict produced by FSDP's\\n    StateDictType.SHARDED_STATE_DICT methods.\\n    \"\n    new_state_dict: STATE_DICT_TYPE = {}\n\n    def rewrite_dict(path: OBJ_PATH, value: STATE_DICT_ITEM) -> None:\n        if not isinstance(value, ShardedTensor):\n            set_element(new_state_dict, path, value)\n            return\n        shards = value.local_shards()\n        if len(shards) == 0:\n            return\n        if len(shards) != 1:\n            set_element(new_state_dict, path, value)\n            return\n        outer_shard = shards[0]\n        inner_st = outer_shard.tensor\n        if not isinstance(inner_st, ShardedTensor):\n            set_element(new_state_dict, path, value)\n            return\n        if len(inner_st.local_shards()) != 1:\n            raise ValueError('Cannot handle inner tensor with more than 1 shard')\n        inner_shard = inner_st.local_shards()[0]\n        local_shards = [Shard(tensor=inner_shard.tensor, metadata=ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_shard.metadata.shard_offsets), shard_sizes=inner_shard.metadata.shard_sizes, placement=f'rank:{dist.get_rank()}/{inner_shard.tensor.device}'))]\n        st_meta: ShardedTensorMetadata = copy.deepcopy(value.metadata())\n        other_rank = 0 if dist.get_rank() > 0 else 1\n        device_info = _normalize_device_info(inner_shard.tensor.device.type, 0)\n        for (i, shard_md) in enumerate(st_meta.shards_metadata):\n            if shard_md.shard_offsets == outer_shard.metadata.shard_offsets:\n                st_meta.shards_metadata.pop(i)\n                break\n        for shard_md in st_meta.shards_metadata:\n            shard_md.placement = _remote_device(f'rank:{other_rank}/{device_info}')\n        for inner_md in inner_st.metadata().shards_metadata:\n            if inner_md.shard_offsets != inner_shard.metadata.shard_offsets:\n                st_meta.shards_metadata.append(ShardMetadata(shard_offsets=_element_wise_add(outer_shard.metadata.shard_offsets, inner_md.shard_offsets), shard_sizes=inner_md.shard_sizes, placement=f'rank:{other_rank}/{device_info}'))\n        st_meta.shards_metadata.append(local_shards[0].metadata)\n        st = ShardedTensor._init_from_local_shards_and_global_metadata(local_shards=local_shards, sharded_tensor_metadata=st_meta)\n        set_element(new_state_dict, path, st)\n    traverse_state_dict(state_dict, rewrite_dict)\n    return new_state_dict"
        ]
    }
]