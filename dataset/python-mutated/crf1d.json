[
    {
        "func_name": "crf1d",
        "original": "def crf1d(cost, xs, ys, reduce='mean'):\n    \"\"\"Calculates negative log-likelihood of linear-chain CRF.\n\n    It takes a transition cost matrix, a sequence of costs, and a sequence of\n    labels. Let :math:`c_{st}` be a transition cost from a label :math:`s` to\n    a label :math:`t`, :math:`x_{it}` be a cost of a label :math:`t` at\n    position :math:`i`, and :math:`y_i` be an expected label at position\n    :math:`i`. The negative log-likelihood of linear-chain CRF is defined as\n\n    .. math::\n        L = -\\\\left( \\\\sum_{i=1}^l x_{iy_i} + \\\\\n             \\\\sum_{i=1}^{l-1} c_{y_i y_{i+1}} - {\\\\log(Z)} \\\\right) ,\n\n    where :math:`l` is the length of the input sequence and :math:`Z` is the\n    normalizing constant called partition function.\n\n    .. note::\n\n       When you want to calculate the negative log-likelihood of sequences\n       which have different lengths, sort the sequences in descending order of\n       lengths and transpose the sequences.\n       For example, you have three input sequences:\n\n       >>> a1 = a2 = a3 = a4 = np.random.uniform(-1, 1, 3).astype(np.float32)\n       >>> b1 = b2 = b3 = np.random.uniform(-1, 1, 3).astype(np.float32)\n       >>> c1 = c2 = np.random.uniform(-1, 1, 3).astype(np.float32)\n\n       >>> a = [a1, a2, a3, a4]\n       >>> b = [b1, b2, b3]\n       >>> c = [c1, c2]\n\n       where ``a1`` and all other variables are arrays with ``(K,)`` shape.\n       Make a transpose of the sequences:\n\n       >>> x1 = np.stack([a1, b1, c1])\n       >>> x2 = np.stack([a2, b2, c2])\n       >>> x3 = np.stack([a3, b3])\n       >>> x4 = np.stack([a4])\n\n       and make a list of the arrays:\n\n       >>> xs = [x1, x2, x3, x4]\n\n       You need to make label sequences in the same fashion.\n       And then, call the function:\n\n       >>> cost = chainer.Variable(\n       ...     np.random.uniform(-1, 1, (3, 3)).astype(np.float32))\n       >>> ys = [np.zeros(x.shape[0:1], dtype=np.int32) for x in xs]\n       >>> loss = F.crf1d(cost, xs, ys)\n\n       It calculates mean of the negative log-likelihood of the three\n       sequences.\n\n       The output is a variable whose value depends on the value of\n       the option ``reduce``. If it is ``'no'``, it holds the elementwise\n       loss values. If it is ``'mean'``, it holds mean of the loss values.\n\n\n    Args:\n        cost (:class:`~chainer.Variable` or :ref:`ndarray`):\n            A :math:`K \\\\times K` matrix which holds transition\n            cost between two labels, where :math:`K` is the number of labels.\n        xs (list of Variable): Input vector for each label.\n            ``len(xs)`` denotes the length of the sequence,\n            and each :class:`~chainer.Variable` holds a :math:`B \\\\times K`\n            matrix, where :math:`B` is mini-batch size, :math:`K` is the number\n            of labels.\n            Note that :math:`B`\\\\ s in all the variables are not necessary\n            the same, i.e., it accepts the input sequences with different\n            lengths.\n        ys (list of Variable): Expected output labels. It needs to have the\n            same length as ``xs``. Each :class:`~chainer.Variable` holds a\n            :math:`B` integer vector.\n            When ``x`` in ``xs`` has the different :math:`B`, correspoding\n            ``y`` has the same :math:`B`. In other words, ``ys`` must satisfy\n            ``ys[i].shape == xs[i].shape[0:1]`` for all ``i``.\n        reduce (str): Reduction option. Its value must be either\n            ``'mean'`` or ``'no'``. Otherwise, :class:`ValueError` is raised.\n\n    Returns:\n        ~chainer.Variable: A variable holding the average negative\n        log-likelihood of the input sequences.\n\n    .. note::\n\n        See detail in the original paper: `Conditional Random Fields:\n        Probabilistic Models for Segmenting and Labeling Sequence Data\n        <https://repository.upenn.edu/cis_papers/159/>`_.\n\n    \"\"\"\n    if reduce not in ('mean', 'no'):\n        raise ValueError(\"only 'mean' and 'no' are valid for 'reduce', but '%s' is given\" % reduce)\n    assert xs[0].shape[1] == cost.shape[0]\n    n_label = cost.shape[0]\n    n_batch = xs[0].shape[0]\n    alpha = xs[0]\n    alphas = []\n    for x in xs[1:]:\n        batch = x.shape[0]\n        if alpha.shape[0] > batch:\n            (alpha, alpha_rest) = split_axis.split_axis(alpha, [batch], axis=0)\n            alphas.append(alpha_rest)\n        (b_alpha, b_cost) = broadcast.broadcast(alpha[..., None], cost)\n        alpha = logsumexp.logsumexp(b_alpha + b_cost, axis=1) + x\n    if alphas:\n        alphas.append(alpha)\n        alpha = concat.concat(alphas[::-1], axis=0)\n    logz = logsumexp.logsumexp(alpha, axis=1)\n    cost = reshape.reshape(cost, (cost.size, 1))\n    score = select_item.select_item(xs[0], ys[0])\n    scores = []\n    for (x, y, y_prev) in zip(xs[1:], ys[1:], ys[:-1]):\n        batch = x.shape[0]\n        if score.shape[0] > batch:\n            (y_prev, _) = split_axis.split_axis(y_prev, [batch], axis=0)\n            (score, score_rest) = split_axis.split_axis(score, [batch], axis=0)\n            scores.append(score_rest)\n        score += select_item.select_item(x, y) + reshape.reshape(embed_id.embed_id(y_prev * n_label + y, cost), (batch,))\n    if scores:\n        scores.append(score)\n        score = concat.concat(scores[::-1], axis=0)\n    loss = logz - score\n    if reduce == 'mean':\n        return _sum.sum(loss) / n_batch\n    else:\n        return loss",
        "mutated": [
            "def crf1d(cost, xs, ys, reduce='mean'):\n    if False:\n        i = 10\n    \"Calculates negative log-likelihood of linear-chain CRF.\\n\\n    It takes a transition cost matrix, a sequence of costs, and a sequence of\\n    labels. Let :math:`c_{st}` be a transition cost from a label :math:`s` to\\n    a label :math:`t`, :math:`x_{it}` be a cost of a label :math:`t` at\\n    position :math:`i`, and :math:`y_i` be an expected label at position\\n    :math:`i`. The negative log-likelihood of linear-chain CRF is defined as\\n\\n    .. math::\\n        L = -\\\\left( \\\\sum_{i=1}^l x_{iy_i} + \\\\\\n             \\\\sum_{i=1}^{l-1} c_{y_i y_{i+1}} - {\\\\log(Z)} \\\\right) ,\\n\\n    where :math:`l` is the length of the input sequence and :math:`Z` is the\\n    normalizing constant called partition function.\\n\\n    .. note::\\n\\n       When you want to calculate the negative log-likelihood of sequences\\n       which have different lengths, sort the sequences in descending order of\\n       lengths and transpose the sequences.\\n       For example, you have three input sequences:\\n\\n       >>> a1 = a2 = a3 = a4 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n       >>> b1 = b2 = b3 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n       >>> c1 = c2 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n\\n       >>> a = [a1, a2, a3, a4]\\n       >>> b = [b1, b2, b3]\\n       >>> c = [c1, c2]\\n\\n       where ``a1`` and all other variables are arrays with ``(K,)`` shape.\\n       Make a transpose of the sequences:\\n\\n       >>> x1 = np.stack([a1, b1, c1])\\n       >>> x2 = np.stack([a2, b2, c2])\\n       >>> x3 = np.stack([a3, b3])\\n       >>> x4 = np.stack([a4])\\n\\n       and make a list of the arrays:\\n\\n       >>> xs = [x1, x2, x3, x4]\\n\\n       You need to make label sequences in the same fashion.\\n       And then, call the function:\\n\\n       >>> cost = chainer.Variable(\\n       ...     np.random.uniform(-1, 1, (3, 3)).astype(np.float32))\\n       >>> ys = [np.zeros(x.shape[0:1], dtype=np.int32) for x in xs]\\n       >>> loss = F.crf1d(cost, xs, ys)\\n\\n       It calculates mean of the negative log-likelihood of the three\\n       sequences.\\n\\n       The output is a variable whose value depends on the value of\\n       the option ``reduce``. If it is ``'no'``, it holds the elementwise\\n       loss values. If it is ``'mean'``, it holds mean of the loss values.\\n\\n\\n    Args:\\n        cost (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            A :math:`K \\\\times K` matrix which holds transition\\n            cost between two labels, where :math:`K` is the number of labels.\\n        xs (list of Variable): Input vector for each label.\\n            ``len(xs)`` denotes the length of the sequence,\\n            and each :class:`~chainer.Variable` holds a :math:`B \\\\times K`\\n            matrix, where :math:`B` is mini-batch size, :math:`K` is the number\\n            of labels.\\n            Note that :math:`B`\\\\ s in all the variables are not necessary\\n            the same, i.e., it accepts the input sequences with different\\n            lengths.\\n        ys (list of Variable): Expected output labels. It needs to have the\\n            same length as ``xs``. Each :class:`~chainer.Variable` holds a\\n            :math:`B` integer vector.\\n            When ``x`` in ``xs`` has the different :math:`B`, correspoding\\n            ``y`` has the same :math:`B`. In other words, ``ys`` must satisfy\\n            ``ys[i].shape == xs[i].shape[0:1]`` for all ``i``.\\n        reduce (str): Reduction option. Its value must be either\\n            ``'mean'`` or ``'no'``. Otherwise, :class:`ValueError` is raised.\\n\\n    Returns:\\n        ~chainer.Variable: A variable holding the average negative\\n        log-likelihood of the input sequences.\\n\\n    .. note::\\n\\n        See detail in the original paper: `Conditional Random Fields:\\n        Probabilistic Models for Segmenting and Labeling Sequence Data\\n        <https://repository.upenn.edu/cis_papers/159/>`_.\\n\\n    \"\n    if reduce not in ('mean', 'no'):\n        raise ValueError(\"only 'mean' and 'no' are valid for 'reduce', but '%s' is given\" % reduce)\n    assert xs[0].shape[1] == cost.shape[0]\n    n_label = cost.shape[0]\n    n_batch = xs[0].shape[0]\n    alpha = xs[0]\n    alphas = []\n    for x in xs[1:]:\n        batch = x.shape[0]\n        if alpha.shape[0] > batch:\n            (alpha, alpha_rest) = split_axis.split_axis(alpha, [batch], axis=0)\n            alphas.append(alpha_rest)\n        (b_alpha, b_cost) = broadcast.broadcast(alpha[..., None], cost)\n        alpha = logsumexp.logsumexp(b_alpha + b_cost, axis=1) + x\n    if alphas:\n        alphas.append(alpha)\n        alpha = concat.concat(alphas[::-1], axis=0)\n    logz = logsumexp.logsumexp(alpha, axis=1)\n    cost = reshape.reshape(cost, (cost.size, 1))\n    score = select_item.select_item(xs[0], ys[0])\n    scores = []\n    for (x, y, y_prev) in zip(xs[1:], ys[1:], ys[:-1]):\n        batch = x.shape[0]\n        if score.shape[0] > batch:\n            (y_prev, _) = split_axis.split_axis(y_prev, [batch], axis=0)\n            (score, score_rest) = split_axis.split_axis(score, [batch], axis=0)\n            scores.append(score_rest)\n        score += select_item.select_item(x, y) + reshape.reshape(embed_id.embed_id(y_prev * n_label + y, cost), (batch,))\n    if scores:\n        scores.append(score)\n        score = concat.concat(scores[::-1], axis=0)\n    loss = logz - score\n    if reduce == 'mean':\n        return _sum.sum(loss) / n_batch\n    else:\n        return loss",
            "def crf1d(cost, xs, ys, reduce='mean'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Calculates negative log-likelihood of linear-chain CRF.\\n\\n    It takes a transition cost matrix, a sequence of costs, and a sequence of\\n    labels. Let :math:`c_{st}` be a transition cost from a label :math:`s` to\\n    a label :math:`t`, :math:`x_{it}` be a cost of a label :math:`t` at\\n    position :math:`i`, and :math:`y_i` be an expected label at position\\n    :math:`i`. The negative log-likelihood of linear-chain CRF is defined as\\n\\n    .. math::\\n        L = -\\\\left( \\\\sum_{i=1}^l x_{iy_i} + \\\\\\n             \\\\sum_{i=1}^{l-1} c_{y_i y_{i+1}} - {\\\\log(Z)} \\\\right) ,\\n\\n    where :math:`l` is the length of the input sequence and :math:`Z` is the\\n    normalizing constant called partition function.\\n\\n    .. note::\\n\\n       When you want to calculate the negative log-likelihood of sequences\\n       which have different lengths, sort the sequences in descending order of\\n       lengths and transpose the sequences.\\n       For example, you have three input sequences:\\n\\n       >>> a1 = a2 = a3 = a4 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n       >>> b1 = b2 = b3 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n       >>> c1 = c2 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n\\n       >>> a = [a1, a2, a3, a4]\\n       >>> b = [b1, b2, b3]\\n       >>> c = [c1, c2]\\n\\n       where ``a1`` and all other variables are arrays with ``(K,)`` shape.\\n       Make a transpose of the sequences:\\n\\n       >>> x1 = np.stack([a1, b1, c1])\\n       >>> x2 = np.stack([a2, b2, c2])\\n       >>> x3 = np.stack([a3, b3])\\n       >>> x4 = np.stack([a4])\\n\\n       and make a list of the arrays:\\n\\n       >>> xs = [x1, x2, x3, x4]\\n\\n       You need to make label sequences in the same fashion.\\n       And then, call the function:\\n\\n       >>> cost = chainer.Variable(\\n       ...     np.random.uniform(-1, 1, (3, 3)).astype(np.float32))\\n       >>> ys = [np.zeros(x.shape[0:1], dtype=np.int32) for x in xs]\\n       >>> loss = F.crf1d(cost, xs, ys)\\n\\n       It calculates mean of the negative log-likelihood of the three\\n       sequences.\\n\\n       The output is a variable whose value depends on the value of\\n       the option ``reduce``. If it is ``'no'``, it holds the elementwise\\n       loss values. If it is ``'mean'``, it holds mean of the loss values.\\n\\n\\n    Args:\\n        cost (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            A :math:`K \\\\times K` matrix which holds transition\\n            cost between two labels, where :math:`K` is the number of labels.\\n        xs (list of Variable): Input vector for each label.\\n            ``len(xs)`` denotes the length of the sequence,\\n            and each :class:`~chainer.Variable` holds a :math:`B \\\\times K`\\n            matrix, where :math:`B` is mini-batch size, :math:`K` is the number\\n            of labels.\\n            Note that :math:`B`\\\\ s in all the variables are not necessary\\n            the same, i.e., it accepts the input sequences with different\\n            lengths.\\n        ys (list of Variable): Expected output labels. It needs to have the\\n            same length as ``xs``. Each :class:`~chainer.Variable` holds a\\n            :math:`B` integer vector.\\n            When ``x`` in ``xs`` has the different :math:`B`, correspoding\\n            ``y`` has the same :math:`B`. In other words, ``ys`` must satisfy\\n            ``ys[i].shape == xs[i].shape[0:1]`` for all ``i``.\\n        reduce (str): Reduction option. Its value must be either\\n            ``'mean'`` or ``'no'``. Otherwise, :class:`ValueError` is raised.\\n\\n    Returns:\\n        ~chainer.Variable: A variable holding the average negative\\n        log-likelihood of the input sequences.\\n\\n    .. note::\\n\\n        See detail in the original paper: `Conditional Random Fields:\\n        Probabilistic Models for Segmenting and Labeling Sequence Data\\n        <https://repository.upenn.edu/cis_papers/159/>`_.\\n\\n    \"\n    if reduce not in ('mean', 'no'):\n        raise ValueError(\"only 'mean' and 'no' are valid for 'reduce', but '%s' is given\" % reduce)\n    assert xs[0].shape[1] == cost.shape[0]\n    n_label = cost.shape[0]\n    n_batch = xs[0].shape[0]\n    alpha = xs[0]\n    alphas = []\n    for x in xs[1:]:\n        batch = x.shape[0]\n        if alpha.shape[0] > batch:\n            (alpha, alpha_rest) = split_axis.split_axis(alpha, [batch], axis=0)\n            alphas.append(alpha_rest)\n        (b_alpha, b_cost) = broadcast.broadcast(alpha[..., None], cost)\n        alpha = logsumexp.logsumexp(b_alpha + b_cost, axis=1) + x\n    if alphas:\n        alphas.append(alpha)\n        alpha = concat.concat(alphas[::-1], axis=0)\n    logz = logsumexp.logsumexp(alpha, axis=1)\n    cost = reshape.reshape(cost, (cost.size, 1))\n    score = select_item.select_item(xs[0], ys[0])\n    scores = []\n    for (x, y, y_prev) in zip(xs[1:], ys[1:], ys[:-1]):\n        batch = x.shape[0]\n        if score.shape[0] > batch:\n            (y_prev, _) = split_axis.split_axis(y_prev, [batch], axis=0)\n            (score, score_rest) = split_axis.split_axis(score, [batch], axis=0)\n            scores.append(score_rest)\n        score += select_item.select_item(x, y) + reshape.reshape(embed_id.embed_id(y_prev * n_label + y, cost), (batch,))\n    if scores:\n        scores.append(score)\n        score = concat.concat(scores[::-1], axis=0)\n    loss = logz - score\n    if reduce == 'mean':\n        return _sum.sum(loss) / n_batch\n    else:\n        return loss",
            "def crf1d(cost, xs, ys, reduce='mean'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Calculates negative log-likelihood of linear-chain CRF.\\n\\n    It takes a transition cost matrix, a sequence of costs, and a sequence of\\n    labels. Let :math:`c_{st}` be a transition cost from a label :math:`s` to\\n    a label :math:`t`, :math:`x_{it}` be a cost of a label :math:`t` at\\n    position :math:`i`, and :math:`y_i` be an expected label at position\\n    :math:`i`. The negative log-likelihood of linear-chain CRF is defined as\\n\\n    .. math::\\n        L = -\\\\left( \\\\sum_{i=1}^l x_{iy_i} + \\\\\\n             \\\\sum_{i=1}^{l-1} c_{y_i y_{i+1}} - {\\\\log(Z)} \\\\right) ,\\n\\n    where :math:`l` is the length of the input sequence and :math:`Z` is the\\n    normalizing constant called partition function.\\n\\n    .. note::\\n\\n       When you want to calculate the negative log-likelihood of sequences\\n       which have different lengths, sort the sequences in descending order of\\n       lengths and transpose the sequences.\\n       For example, you have three input sequences:\\n\\n       >>> a1 = a2 = a3 = a4 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n       >>> b1 = b2 = b3 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n       >>> c1 = c2 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n\\n       >>> a = [a1, a2, a3, a4]\\n       >>> b = [b1, b2, b3]\\n       >>> c = [c1, c2]\\n\\n       where ``a1`` and all other variables are arrays with ``(K,)`` shape.\\n       Make a transpose of the sequences:\\n\\n       >>> x1 = np.stack([a1, b1, c1])\\n       >>> x2 = np.stack([a2, b2, c2])\\n       >>> x3 = np.stack([a3, b3])\\n       >>> x4 = np.stack([a4])\\n\\n       and make a list of the arrays:\\n\\n       >>> xs = [x1, x2, x3, x4]\\n\\n       You need to make label sequences in the same fashion.\\n       And then, call the function:\\n\\n       >>> cost = chainer.Variable(\\n       ...     np.random.uniform(-1, 1, (3, 3)).astype(np.float32))\\n       >>> ys = [np.zeros(x.shape[0:1], dtype=np.int32) for x in xs]\\n       >>> loss = F.crf1d(cost, xs, ys)\\n\\n       It calculates mean of the negative log-likelihood of the three\\n       sequences.\\n\\n       The output is a variable whose value depends on the value of\\n       the option ``reduce``. If it is ``'no'``, it holds the elementwise\\n       loss values. If it is ``'mean'``, it holds mean of the loss values.\\n\\n\\n    Args:\\n        cost (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            A :math:`K \\\\times K` matrix which holds transition\\n            cost between two labels, where :math:`K` is the number of labels.\\n        xs (list of Variable): Input vector for each label.\\n            ``len(xs)`` denotes the length of the sequence,\\n            and each :class:`~chainer.Variable` holds a :math:`B \\\\times K`\\n            matrix, where :math:`B` is mini-batch size, :math:`K` is the number\\n            of labels.\\n            Note that :math:`B`\\\\ s in all the variables are not necessary\\n            the same, i.e., it accepts the input sequences with different\\n            lengths.\\n        ys (list of Variable): Expected output labels. It needs to have the\\n            same length as ``xs``. Each :class:`~chainer.Variable` holds a\\n            :math:`B` integer vector.\\n            When ``x`` in ``xs`` has the different :math:`B`, correspoding\\n            ``y`` has the same :math:`B`. In other words, ``ys`` must satisfy\\n            ``ys[i].shape == xs[i].shape[0:1]`` for all ``i``.\\n        reduce (str): Reduction option. Its value must be either\\n            ``'mean'`` or ``'no'``. Otherwise, :class:`ValueError` is raised.\\n\\n    Returns:\\n        ~chainer.Variable: A variable holding the average negative\\n        log-likelihood of the input sequences.\\n\\n    .. note::\\n\\n        See detail in the original paper: `Conditional Random Fields:\\n        Probabilistic Models for Segmenting and Labeling Sequence Data\\n        <https://repository.upenn.edu/cis_papers/159/>`_.\\n\\n    \"\n    if reduce not in ('mean', 'no'):\n        raise ValueError(\"only 'mean' and 'no' are valid for 'reduce', but '%s' is given\" % reduce)\n    assert xs[0].shape[1] == cost.shape[0]\n    n_label = cost.shape[0]\n    n_batch = xs[0].shape[0]\n    alpha = xs[0]\n    alphas = []\n    for x in xs[1:]:\n        batch = x.shape[0]\n        if alpha.shape[0] > batch:\n            (alpha, alpha_rest) = split_axis.split_axis(alpha, [batch], axis=0)\n            alphas.append(alpha_rest)\n        (b_alpha, b_cost) = broadcast.broadcast(alpha[..., None], cost)\n        alpha = logsumexp.logsumexp(b_alpha + b_cost, axis=1) + x\n    if alphas:\n        alphas.append(alpha)\n        alpha = concat.concat(alphas[::-1], axis=0)\n    logz = logsumexp.logsumexp(alpha, axis=1)\n    cost = reshape.reshape(cost, (cost.size, 1))\n    score = select_item.select_item(xs[0], ys[0])\n    scores = []\n    for (x, y, y_prev) in zip(xs[1:], ys[1:], ys[:-1]):\n        batch = x.shape[0]\n        if score.shape[0] > batch:\n            (y_prev, _) = split_axis.split_axis(y_prev, [batch], axis=0)\n            (score, score_rest) = split_axis.split_axis(score, [batch], axis=0)\n            scores.append(score_rest)\n        score += select_item.select_item(x, y) + reshape.reshape(embed_id.embed_id(y_prev * n_label + y, cost), (batch,))\n    if scores:\n        scores.append(score)\n        score = concat.concat(scores[::-1], axis=0)\n    loss = logz - score\n    if reduce == 'mean':\n        return _sum.sum(loss) / n_batch\n    else:\n        return loss",
            "def crf1d(cost, xs, ys, reduce='mean'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Calculates negative log-likelihood of linear-chain CRF.\\n\\n    It takes a transition cost matrix, a sequence of costs, and a sequence of\\n    labels. Let :math:`c_{st}` be a transition cost from a label :math:`s` to\\n    a label :math:`t`, :math:`x_{it}` be a cost of a label :math:`t` at\\n    position :math:`i`, and :math:`y_i` be an expected label at position\\n    :math:`i`. The negative log-likelihood of linear-chain CRF is defined as\\n\\n    .. math::\\n        L = -\\\\left( \\\\sum_{i=1}^l x_{iy_i} + \\\\\\n             \\\\sum_{i=1}^{l-1} c_{y_i y_{i+1}} - {\\\\log(Z)} \\\\right) ,\\n\\n    where :math:`l` is the length of the input sequence and :math:`Z` is the\\n    normalizing constant called partition function.\\n\\n    .. note::\\n\\n       When you want to calculate the negative log-likelihood of sequences\\n       which have different lengths, sort the sequences in descending order of\\n       lengths and transpose the sequences.\\n       For example, you have three input sequences:\\n\\n       >>> a1 = a2 = a3 = a4 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n       >>> b1 = b2 = b3 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n       >>> c1 = c2 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n\\n       >>> a = [a1, a2, a3, a4]\\n       >>> b = [b1, b2, b3]\\n       >>> c = [c1, c2]\\n\\n       where ``a1`` and all other variables are arrays with ``(K,)`` shape.\\n       Make a transpose of the sequences:\\n\\n       >>> x1 = np.stack([a1, b1, c1])\\n       >>> x2 = np.stack([a2, b2, c2])\\n       >>> x3 = np.stack([a3, b3])\\n       >>> x4 = np.stack([a4])\\n\\n       and make a list of the arrays:\\n\\n       >>> xs = [x1, x2, x3, x4]\\n\\n       You need to make label sequences in the same fashion.\\n       And then, call the function:\\n\\n       >>> cost = chainer.Variable(\\n       ...     np.random.uniform(-1, 1, (3, 3)).astype(np.float32))\\n       >>> ys = [np.zeros(x.shape[0:1], dtype=np.int32) for x in xs]\\n       >>> loss = F.crf1d(cost, xs, ys)\\n\\n       It calculates mean of the negative log-likelihood of the three\\n       sequences.\\n\\n       The output is a variable whose value depends on the value of\\n       the option ``reduce``. If it is ``'no'``, it holds the elementwise\\n       loss values. If it is ``'mean'``, it holds mean of the loss values.\\n\\n\\n    Args:\\n        cost (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            A :math:`K \\\\times K` matrix which holds transition\\n            cost between two labels, where :math:`K` is the number of labels.\\n        xs (list of Variable): Input vector for each label.\\n            ``len(xs)`` denotes the length of the sequence,\\n            and each :class:`~chainer.Variable` holds a :math:`B \\\\times K`\\n            matrix, where :math:`B` is mini-batch size, :math:`K` is the number\\n            of labels.\\n            Note that :math:`B`\\\\ s in all the variables are not necessary\\n            the same, i.e., it accepts the input sequences with different\\n            lengths.\\n        ys (list of Variable): Expected output labels. It needs to have the\\n            same length as ``xs``. Each :class:`~chainer.Variable` holds a\\n            :math:`B` integer vector.\\n            When ``x`` in ``xs`` has the different :math:`B`, correspoding\\n            ``y`` has the same :math:`B`. In other words, ``ys`` must satisfy\\n            ``ys[i].shape == xs[i].shape[0:1]`` for all ``i``.\\n        reduce (str): Reduction option. Its value must be either\\n            ``'mean'`` or ``'no'``. Otherwise, :class:`ValueError` is raised.\\n\\n    Returns:\\n        ~chainer.Variable: A variable holding the average negative\\n        log-likelihood of the input sequences.\\n\\n    .. note::\\n\\n        See detail in the original paper: `Conditional Random Fields:\\n        Probabilistic Models for Segmenting and Labeling Sequence Data\\n        <https://repository.upenn.edu/cis_papers/159/>`_.\\n\\n    \"\n    if reduce not in ('mean', 'no'):\n        raise ValueError(\"only 'mean' and 'no' are valid for 'reduce', but '%s' is given\" % reduce)\n    assert xs[0].shape[1] == cost.shape[0]\n    n_label = cost.shape[0]\n    n_batch = xs[0].shape[0]\n    alpha = xs[0]\n    alphas = []\n    for x in xs[1:]:\n        batch = x.shape[0]\n        if alpha.shape[0] > batch:\n            (alpha, alpha_rest) = split_axis.split_axis(alpha, [batch], axis=0)\n            alphas.append(alpha_rest)\n        (b_alpha, b_cost) = broadcast.broadcast(alpha[..., None], cost)\n        alpha = logsumexp.logsumexp(b_alpha + b_cost, axis=1) + x\n    if alphas:\n        alphas.append(alpha)\n        alpha = concat.concat(alphas[::-1], axis=0)\n    logz = logsumexp.logsumexp(alpha, axis=1)\n    cost = reshape.reshape(cost, (cost.size, 1))\n    score = select_item.select_item(xs[0], ys[0])\n    scores = []\n    for (x, y, y_prev) in zip(xs[1:], ys[1:], ys[:-1]):\n        batch = x.shape[0]\n        if score.shape[0] > batch:\n            (y_prev, _) = split_axis.split_axis(y_prev, [batch], axis=0)\n            (score, score_rest) = split_axis.split_axis(score, [batch], axis=0)\n            scores.append(score_rest)\n        score += select_item.select_item(x, y) + reshape.reshape(embed_id.embed_id(y_prev * n_label + y, cost), (batch,))\n    if scores:\n        scores.append(score)\n        score = concat.concat(scores[::-1], axis=0)\n    loss = logz - score\n    if reduce == 'mean':\n        return _sum.sum(loss) / n_batch\n    else:\n        return loss",
            "def crf1d(cost, xs, ys, reduce='mean'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Calculates negative log-likelihood of linear-chain CRF.\\n\\n    It takes a transition cost matrix, a sequence of costs, and a sequence of\\n    labels. Let :math:`c_{st}` be a transition cost from a label :math:`s` to\\n    a label :math:`t`, :math:`x_{it}` be a cost of a label :math:`t` at\\n    position :math:`i`, and :math:`y_i` be an expected label at position\\n    :math:`i`. The negative log-likelihood of linear-chain CRF is defined as\\n\\n    .. math::\\n        L = -\\\\left( \\\\sum_{i=1}^l x_{iy_i} + \\\\\\n             \\\\sum_{i=1}^{l-1} c_{y_i y_{i+1}} - {\\\\log(Z)} \\\\right) ,\\n\\n    where :math:`l` is the length of the input sequence and :math:`Z` is the\\n    normalizing constant called partition function.\\n\\n    .. note::\\n\\n       When you want to calculate the negative log-likelihood of sequences\\n       which have different lengths, sort the sequences in descending order of\\n       lengths and transpose the sequences.\\n       For example, you have three input sequences:\\n\\n       >>> a1 = a2 = a3 = a4 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n       >>> b1 = b2 = b3 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n       >>> c1 = c2 = np.random.uniform(-1, 1, 3).astype(np.float32)\\n\\n       >>> a = [a1, a2, a3, a4]\\n       >>> b = [b1, b2, b3]\\n       >>> c = [c1, c2]\\n\\n       where ``a1`` and all other variables are arrays with ``(K,)`` shape.\\n       Make a transpose of the sequences:\\n\\n       >>> x1 = np.stack([a1, b1, c1])\\n       >>> x2 = np.stack([a2, b2, c2])\\n       >>> x3 = np.stack([a3, b3])\\n       >>> x4 = np.stack([a4])\\n\\n       and make a list of the arrays:\\n\\n       >>> xs = [x1, x2, x3, x4]\\n\\n       You need to make label sequences in the same fashion.\\n       And then, call the function:\\n\\n       >>> cost = chainer.Variable(\\n       ...     np.random.uniform(-1, 1, (3, 3)).astype(np.float32))\\n       >>> ys = [np.zeros(x.shape[0:1], dtype=np.int32) for x in xs]\\n       >>> loss = F.crf1d(cost, xs, ys)\\n\\n       It calculates mean of the negative log-likelihood of the three\\n       sequences.\\n\\n       The output is a variable whose value depends on the value of\\n       the option ``reduce``. If it is ``'no'``, it holds the elementwise\\n       loss values. If it is ``'mean'``, it holds mean of the loss values.\\n\\n\\n    Args:\\n        cost (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            A :math:`K \\\\times K` matrix which holds transition\\n            cost between two labels, where :math:`K` is the number of labels.\\n        xs (list of Variable): Input vector for each label.\\n            ``len(xs)`` denotes the length of the sequence,\\n            and each :class:`~chainer.Variable` holds a :math:`B \\\\times K`\\n            matrix, where :math:`B` is mini-batch size, :math:`K` is the number\\n            of labels.\\n            Note that :math:`B`\\\\ s in all the variables are not necessary\\n            the same, i.e., it accepts the input sequences with different\\n            lengths.\\n        ys (list of Variable): Expected output labels. It needs to have the\\n            same length as ``xs``. Each :class:`~chainer.Variable` holds a\\n            :math:`B` integer vector.\\n            When ``x`` in ``xs`` has the different :math:`B`, correspoding\\n            ``y`` has the same :math:`B`. In other words, ``ys`` must satisfy\\n            ``ys[i].shape == xs[i].shape[0:1]`` for all ``i``.\\n        reduce (str): Reduction option. Its value must be either\\n            ``'mean'`` or ``'no'``. Otherwise, :class:`ValueError` is raised.\\n\\n    Returns:\\n        ~chainer.Variable: A variable holding the average negative\\n        log-likelihood of the input sequences.\\n\\n    .. note::\\n\\n        See detail in the original paper: `Conditional Random Fields:\\n        Probabilistic Models for Segmenting and Labeling Sequence Data\\n        <https://repository.upenn.edu/cis_papers/159/>`_.\\n\\n    \"\n    if reduce not in ('mean', 'no'):\n        raise ValueError(\"only 'mean' and 'no' are valid for 'reduce', but '%s' is given\" % reduce)\n    assert xs[0].shape[1] == cost.shape[0]\n    n_label = cost.shape[0]\n    n_batch = xs[0].shape[0]\n    alpha = xs[0]\n    alphas = []\n    for x in xs[1:]:\n        batch = x.shape[0]\n        if alpha.shape[0] > batch:\n            (alpha, alpha_rest) = split_axis.split_axis(alpha, [batch], axis=0)\n            alphas.append(alpha_rest)\n        (b_alpha, b_cost) = broadcast.broadcast(alpha[..., None], cost)\n        alpha = logsumexp.logsumexp(b_alpha + b_cost, axis=1) + x\n    if alphas:\n        alphas.append(alpha)\n        alpha = concat.concat(alphas[::-1], axis=0)\n    logz = logsumexp.logsumexp(alpha, axis=1)\n    cost = reshape.reshape(cost, (cost.size, 1))\n    score = select_item.select_item(xs[0], ys[0])\n    scores = []\n    for (x, y, y_prev) in zip(xs[1:], ys[1:], ys[:-1]):\n        batch = x.shape[0]\n        if score.shape[0] > batch:\n            (y_prev, _) = split_axis.split_axis(y_prev, [batch], axis=0)\n            (score, score_rest) = split_axis.split_axis(score, [batch], axis=0)\n            scores.append(score_rest)\n        score += select_item.select_item(x, y) + reshape.reshape(embed_id.embed_id(y_prev * n_label + y, cost), (batch,))\n    if scores:\n        scores.append(score)\n        score = concat.concat(scores[::-1], axis=0)\n    loss = logz - score\n    if reduce == 'mean':\n        return _sum.sum(loss) / n_batch\n    else:\n        return loss"
        ]
    },
    {
        "func_name": "argmax_crf1d",
        "original": "def argmax_crf1d(cost, xs):\n    \"\"\"Computes a state that maximizes a joint probability of the given CRF.\n\n    Args:\n        cost (:class:`~chainer.Variable` or :ref:`ndarray`):\n            A :math:`K \\\\times K` matrix which holds transition\n            cost between two labels, where :math:`K` is the number of labels.\n        xs (list of Variable): Input vector for each label.\n            ``len(xs)`` denotes the length of the sequence,\n            and each :class:`~chainer.Variable` holds a :math:`B \\\\times K`\n            matrix, where :math:`B` is mini-batch size, :math:`K` is the number\n            of labels.\n            Note that :math:`B`\\\\ s in all the variables are not necessary\n            the same, i.e., it accepts the input sequences with different\n            lengths.\n\n    Returns:\n        tuple: A tuple of :class:`~chainer.Variable` object ``s`` and a\n        :class:`list` ``ps``.\n        The shape of ``s`` is ``(B,)``, where ``B`` is the mini-batch size.\n        i-th element of ``s``, ``s[i]``, represents log-likelihood of i-th\n        data.\n        ``ps`` is a list of :ref:`ndarray`, and denotes the state that\n        maximizes the point probability.\n        ``len(ps)`` is equal to ``len(xs)``, and shape of each ``ps[i]`` is\n        the mini-batch size of the corresponding ``xs[i]``. That means,\n        ``ps[i].shape == xs[i].shape[0:1]``.\n    \"\"\"\n    alpha = xs[0]\n    alphas = []\n    max_inds = []\n    for x in xs[1:]:\n        batch = x.shape[0]\n        if alpha.shape[0] > batch:\n            (alpha, alpha_rest) = split_axis.split_axis(alpha, [batch], axis=0)\n            alphas.append(alpha_rest)\n        else:\n            alphas.append(None)\n        (b_alpha, b_cost) = broadcast.broadcast(alpha[..., None], cost)\n        scores = b_alpha + b_cost\n        max_ind = minmax.argmax(scores, axis=1)\n        max_inds.append(max_ind)\n        alpha = minmax.max(scores, axis=1) + x\n    inds = minmax.argmax(alpha, axis=1)\n    path = [inds.data]\n    for (m, a) in zip(max_inds[::-1], alphas[::-1]):\n        inds = select_item.select_item(m, inds)\n        if a is not None:\n            inds = concat.concat([inds, minmax.argmax(a, axis=1)], axis=0)\n        path.append(inds.data)\n    path.reverse()\n    score = minmax.max(alpha, axis=1)\n    for a in alphas[::-1]:\n        if a is None:\n            continue\n        score = concat.concat([score, minmax.max(a, axis=1)], axis=0)\n    return (score, path)",
        "mutated": [
            "def argmax_crf1d(cost, xs):\n    if False:\n        i = 10\n    'Computes a state that maximizes a joint probability of the given CRF.\\n\\n    Args:\\n        cost (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            A :math:`K \\\\times K` matrix which holds transition\\n            cost between two labels, where :math:`K` is the number of labels.\\n        xs (list of Variable): Input vector for each label.\\n            ``len(xs)`` denotes the length of the sequence,\\n            and each :class:`~chainer.Variable` holds a :math:`B \\\\times K`\\n            matrix, where :math:`B` is mini-batch size, :math:`K` is the number\\n            of labels.\\n            Note that :math:`B`\\\\ s in all the variables are not necessary\\n            the same, i.e., it accepts the input sequences with different\\n            lengths.\\n\\n    Returns:\\n        tuple: A tuple of :class:`~chainer.Variable` object ``s`` and a\\n        :class:`list` ``ps``.\\n        The shape of ``s`` is ``(B,)``, where ``B`` is the mini-batch size.\\n        i-th element of ``s``, ``s[i]``, represents log-likelihood of i-th\\n        data.\\n        ``ps`` is a list of :ref:`ndarray`, and denotes the state that\\n        maximizes the point probability.\\n        ``len(ps)`` is equal to ``len(xs)``, and shape of each ``ps[i]`` is\\n        the mini-batch size of the corresponding ``xs[i]``. That means,\\n        ``ps[i].shape == xs[i].shape[0:1]``.\\n    '\n    alpha = xs[0]\n    alphas = []\n    max_inds = []\n    for x in xs[1:]:\n        batch = x.shape[0]\n        if alpha.shape[0] > batch:\n            (alpha, alpha_rest) = split_axis.split_axis(alpha, [batch], axis=0)\n            alphas.append(alpha_rest)\n        else:\n            alphas.append(None)\n        (b_alpha, b_cost) = broadcast.broadcast(alpha[..., None], cost)\n        scores = b_alpha + b_cost\n        max_ind = minmax.argmax(scores, axis=1)\n        max_inds.append(max_ind)\n        alpha = minmax.max(scores, axis=1) + x\n    inds = minmax.argmax(alpha, axis=1)\n    path = [inds.data]\n    for (m, a) in zip(max_inds[::-1], alphas[::-1]):\n        inds = select_item.select_item(m, inds)\n        if a is not None:\n            inds = concat.concat([inds, minmax.argmax(a, axis=1)], axis=0)\n        path.append(inds.data)\n    path.reverse()\n    score = minmax.max(alpha, axis=1)\n    for a in alphas[::-1]:\n        if a is None:\n            continue\n        score = concat.concat([score, minmax.max(a, axis=1)], axis=0)\n    return (score, path)",
            "def argmax_crf1d(cost, xs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Computes a state that maximizes a joint probability of the given CRF.\\n\\n    Args:\\n        cost (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            A :math:`K \\\\times K` matrix which holds transition\\n            cost between two labels, where :math:`K` is the number of labels.\\n        xs (list of Variable): Input vector for each label.\\n            ``len(xs)`` denotes the length of the sequence,\\n            and each :class:`~chainer.Variable` holds a :math:`B \\\\times K`\\n            matrix, where :math:`B` is mini-batch size, :math:`K` is the number\\n            of labels.\\n            Note that :math:`B`\\\\ s in all the variables are not necessary\\n            the same, i.e., it accepts the input sequences with different\\n            lengths.\\n\\n    Returns:\\n        tuple: A tuple of :class:`~chainer.Variable` object ``s`` and a\\n        :class:`list` ``ps``.\\n        The shape of ``s`` is ``(B,)``, where ``B`` is the mini-batch size.\\n        i-th element of ``s``, ``s[i]``, represents log-likelihood of i-th\\n        data.\\n        ``ps`` is a list of :ref:`ndarray`, and denotes the state that\\n        maximizes the point probability.\\n        ``len(ps)`` is equal to ``len(xs)``, and shape of each ``ps[i]`` is\\n        the mini-batch size of the corresponding ``xs[i]``. That means,\\n        ``ps[i].shape == xs[i].shape[0:1]``.\\n    '\n    alpha = xs[0]\n    alphas = []\n    max_inds = []\n    for x in xs[1:]:\n        batch = x.shape[0]\n        if alpha.shape[0] > batch:\n            (alpha, alpha_rest) = split_axis.split_axis(alpha, [batch], axis=0)\n            alphas.append(alpha_rest)\n        else:\n            alphas.append(None)\n        (b_alpha, b_cost) = broadcast.broadcast(alpha[..., None], cost)\n        scores = b_alpha + b_cost\n        max_ind = minmax.argmax(scores, axis=1)\n        max_inds.append(max_ind)\n        alpha = minmax.max(scores, axis=1) + x\n    inds = minmax.argmax(alpha, axis=1)\n    path = [inds.data]\n    for (m, a) in zip(max_inds[::-1], alphas[::-1]):\n        inds = select_item.select_item(m, inds)\n        if a is not None:\n            inds = concat.concat([inds, minmax.argmax(a, axis=1)], axis=0)\n        path.append(inds.data)\n    path.reverse()\n    score = minmax.max(alpha, axis=1)\n    for a in alphas[::-1]:\n        if a is None:\n            continue\n        score = concat.concat([score, minmax.max(a, axis=1)], axis=0)\n    return (score, path)",
            "def argmax_crf1d(cost, xs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Computes a state that maximizes a joint probability of the given CRF.\\n\\n    Args:\\n        cost (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            A :math:`K \\\\times K` matrix which holds transition\\n            cost between two labels, where :math:`K` is the number of labels.\\n        xs (list of Variable): Input vector for each label.\\n            ``len(xs)`` denotes the length of the sequence,\\n            and each :class:`~chainer.Variable` holds a :math:`B \\\\times K`\\n            matrix, where :math:`B` is mini-batch size, :math:`K` is the number\\n            of labels.\\n            Note that :math:`B`\\\\ s in all the variables are not necessary\\n            the same, i.e., it accepts the input sequences with different\\n            lengths.\\n\\n    Returns:\\n        tuple: A tuple of :class:`~chainer.Variable` object ``s`` and a\\n        :class:`list` ``ps``.\\n        The shape of ``s`` is ``(B,)``, where ``B`` is the mini-batch size.\\n        i-th element of ``s``, ``s[i]``, represents log-likelihood of i-th\\n        data.\\n        ``ps`` is a list of :ref:`ndarray`, and denotes the state that\\n        maximizes the point probability.\\n        ``len(ps)`` is equal to ``len(xs)``, and shape of each ``ps[i]`` is\\n        the mini-batch size of the corresponding ``xs[i]``. That means,\\n        ``ps[i].shape == xs[i].shape[0:1]``.\\n    '\n    alpha = xs[0]\n    alphas = []\n    max_inds = []\n    for x in xs[1:]:\n        batch = x.shape[0]\n        if alpha.shape[0] > batch:\n            (alpha, alpha_rest) = split_axis.split_axis(alpha, [batch], axis=0)\n            alphas.append(alpha_rest)\n        else:\n            alphas.append(None)\n        (b_alpha, b_cost) = broadcast.broadcast(alpha[..., None], cost)\n        scores = b_alpha + b_cost\n        max_ind = minmax.argmax(scores, axis=1)\n        max_inds.append(max_ind)\n        alpha = minmax.max(scores, axis=1) + x\n    inds = minmax.argmax(alpha, axis=1)\n    path = [inds.data]\n    for (m, a) in zip(max_inds[::-1], alphas[::-1]):\n        inds = select_item.select_item(m, inds)\n        if a is not None:\n            inds = concat.concat([inds, minmax.argmax(a, axis=1)], axis=0)\n        path.append(inds.data)\n    path.reverse()\n    score = minmax.max(alpha, axis=1)\n    for a in alphas[::-1]:\n        if a is None:\n            continue\n        score = concat.concat([score, minmax.max(a, axis=1)], axis=0)\n    return (score, path)",
            "def argmax_crf1d(cost, xs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Computes a state that maximizes a joint probability of the given CRF.\\n\\n    Args:\\n        cost (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            A :math:`K \\\\times K` matrix which holds transition\\n            cost between two labels, where :math:`K` is the number of labels.\\n        xs (list of Variable): Input vector for each label.\\n            ``len(xs)`` denotes the length of the sequence,\\n            and each :class:`~chainer.Variable` holds a :math:`B \\\\times K`\\n            matrix, where :math:`B` is mini-batch size, :math:`K` is the number\\n            of labels.\\n            Note that :math:`B`\\\\ s in all the variables are not necessary\\n            the same, i.e., it accepts the input sequences with different\\n            lengths.\\n\\n    Returns:\\n        tuple: A tuple of :class:`~chainer.Variable` object ``s`` and a\\n        :class:`list` ``ps``.\\n        The shape of ``s`` is ``(B,)``, where ``B`` is the mini-batch size.\\n        i-th element of ``s``, ``s[i]``, represents log-likelihood of i-th\\n        data.\\n        ``ps`` is a list of :ref:`ndarray`, and denotes the state that\\n        maximizes the point probability.\\n        ``len(ps)`` is equal to ``len(xs)``, and shape of each ``ps[i]`` is\\n        the mini-batch size of the corresponding ``xs[i]``. That means,\\n        ``ps[i].shape == xs[i].shape[0:1]``.\\n    '\n    alpha = xs[0]\n    alphas = []\n    max_inds = []\n    for x in xs[1:]:\n        batch = x.shape[0]\n        if alpha.shape[0] > batch:\n            (alpha, alpha_rest) = split_axis.split_axis(alpha, [batch], axis=0)\n            alphas.append(alpha_rest)\n        else:\n            alphas.append(None)\n        (b_alpha, b_cost) = broadcast.broadcast(alpha[..., None], cost)\n        scores = b_alpha + b_cost\n        max_ind = minmax.argmax(scores, axis=1)\n        max_inds.append(max_ind)\n        alpha = minmax.max(scores, axis=1) + x\n    inds = minmax.argmax(alpha, axis=1)\n    path = [inds.data]\n    for (m, a) in zip(max_inds[::-1], alphas[::-1]):\n        inds = select_item.select_item(m, inds)\n        if a is not None:\n            inds = concat.concat([inds, minmax.argmax(a, axis=1)], axis=0)\n        path.append(inds.data)\n    path.reverse()\n    score = minmax.max(alpha, axis=1)\n    for a in alphas[::-1]:\n        if a is None:\n            continue\n        score = concat.concat([score, minmax.max(a, axis=1)], axis=0)\n    return (score, path)",
            "def argmax_crf1d(cost, xs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Computes a state that maximizes a joint probability of the given CRF.\\n\\n    Args:\\n        cost (:class:`~chainer.Variable` or :ref:`ndarray`):\\n            A :math:`K \\\\times K` matrix which holds transition\\n            cost between two labels, where :math:`K` is the number of labels.\\n        xs (list of Variable): Input vector for each label.\\n            ``len(xs)`` denotes the length of the sequence,\\n            and each :class:`~chainer.Variable` holds a :math:`B \\\\times K`\\n            matrix, where :math:`B` is mini-batch size, :math:`K` is the number\\n            of labels.\\n            Note that :math:`B`\\\\ s in all the variables are not necessary\\n            the same, i.e., it accepts the input sequences with different\\n            lengths.\\n\\n    Returns:\\n        tuple: A tuple of :class:`~chainer.Variable` object ``s`` and a\\n        :class:`list` ``ps``.\\n        The shape of ``s`` is ``(B,)``, where ``B`` is the mini-batch size.\\n        i-th element of ``s``, ``s[i]``, represents log-likelihood of i-th\\n        data.\\n        ``ps`` is a list of :ref:`ndarray`, and denotes the state that\\n        maximizes the point probability.\\n        ``len(ps)`` is equal to ``len(xs)``, and shape of each ``ps[i]`` is\\n        the mini-batch size of the corresponding ``xs[i]``. That means,\\n        ``ps[i].shape == xs[i].shape[0:1]``.\\n    '\n    alpha = xs[0]\n    alphas = []\n    max_inds = []\n    for x in xs[1:]:\n        batch = x.shape[0]\n        if alpha.shape[0] > batch:\n            (alpha, alpha_rest) = split_axis.split_axis(alpha, [batch], axis=0)\n            alphas.append(alpha_rest)\n        else:\n            alphas.append(None)\n        (b_alpha, b_cost) = broadcast.broadcast(alpha[..., None], cost)\n        scores = b_alpha + b_cost\n        max_ind = minmax.argmax(scores, axis=1)\n        max_inds.append(max_ind)\n        alpha = minmax.max(scores, axis=1) + x\n    inds = minmax.argmax(alpha, axis=1)\n    path = [inds.data]\n    for (m, a) in zip(max_inds[::-1], alphas[::-1]):\n        inds = select_item.select_item(m, inds)\n        if a is not None:\n            inds = concat.concat([inds, minmax.argmax(a, axis=1)], axis=0)\n        path.append(inds.data)\n    path.reverse()\n    score = minmax.max(alpha, axis=1)\n    for a in alphas[::-1]:\n        if a is None:\n            continue\n        score = concat.concat([score, minmax.max(a, axis=1)], axis=0)\n    return (score, path)"
        ]
    }
]