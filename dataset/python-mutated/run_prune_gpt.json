[
    {
        "func_name": "save_model",
        "original": "def save_model(model, dirpath):\n    if os.path.exists(dirpath):\n        if os.path.exists(os.path.join(dirpath, 'config.json')) and os.path.isfile(os.path.join(dirpath, 'config.json')):\n            os.remove(os.path.join(dirpath, 'config.json'))\n        if os.path.exists(os.path.join(dirpath, 'pytorch_model.bin')) and os.path.isfile(os.path.join(dirpath, 'pytorch_model.bin')):\n            os.remove(os.path.join(dirpath, 'pytorch_model.bin'))\n    else:\n        os.makedirs(dirpath)\n    model.save_pretrained(dirpath)",
        "mutated": [
            "def save_model(model, dirpath):\n    if False:\n        i = 10\n    if os.path.exists(dirpath):\n        if os.path.exists(os.path.join(dirpath, 'config.json')) and os.path.isfile(os.path.join(dirpath, 'config.json')):\n            os.remove(os.path.join(dirpath, 'config.json'))\n        if os.path.exists(os.path.join(dirpath, 'pytorch_model.bin')) and os.path.isfile(os.path.join(dirpath, 'pytorch_model.bin')):\n            os.remove(os.path.join(dirpath, 'pytorch_model.bin'))\n    else:\n        os.makedirs(dirpath)\n    model.save_pretrained(dirpath)",
            "def save_model(model, dirpath):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if os.path.exists(dirpath):\n        if os.path.exists(os.path.join(dirpath, 'config.json')) and os.path.isfile(os.path.join(dirpath, 'config.json')):\n            os.remove(os.path.join(dirpath, 'config.json'))\n        if os.path.exists(os.path.join(dirpath, 'pytorch_model.bin')) and os.path.isfile(os.path.join(dirpath, 'pytorch_model.bin')):\n            os.remove(os.path.join(dirpath, 'pytorch_model.bin'))\n    else:\n        os.makedirs(dirpath)\n    model.save_pretrained(dirpath)",
            "def save_model(model, dirpath):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if os.path.exists(dirpath):\n        if os.path.exists(os.path.join(dirpath, 'config.json')) and os.path.isfile(os.path.join(dirpath, 'config.json')):\n            os.remove(os.path.join(dirpath, 'config.json'))\n        if os.path.exists(os.path.join(dirpath, 'pytorch_model.bin')) and os.path.isfile(os.path.join(dirpath, 'pytorch_model.bin')):\n            os.remove(os.path.join(dirpath, 'pytorch_model.bin'))\n    else:\n        os.makedirs(dirpath)\n    model.save_pretrained(dirpath)",
            "def save_model(model, dirpath):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if os.path.exists(dirpath):\n        if os.path.exists(os.path.join(dirpath, 'config.json')) and os.path.isfile(os.path.join(dirpath, 'config.json')):\n            os.remove(os.path.join(dirpath, 'config.json'))\n        if os.path.exists(os.path.join(dirpath, 'pytorch_model.bin')) and os.path.isfile(os.path.join(dirpath, 'pytorch_model.bin')):\n            os.remove(os.path.join(dirpath, 'pytorch_model.bin'))\n    else:\n        os.makedirs(dirpath)\n    model.save_pretrained(dirpath)",
            "def save_model(model, dirpath):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if os.path.exists(dirpath):\n        if os.path.exists(os.path.join(dirpath, 'config.json')) and os.path.isfile(os.path.join(dirpath, 'config.json')):\n            os.remove(os.path.join(dirpath, 'config.json'))\n        if os.path.exists(os.path.join(dirpath, 'pytorch_model.bin')) and os.path.isfile(os.path.join(dirpath, 'pytorch_model.bin')):\n            os.remove(os.path.join(dirpath, 'pytorch_model.bin'))\n    else:\n        os.makedirs(dirpath)\n    model.save_pretrained(dirpath)"
        ]
    },
    {
        "func_name": "entropy",
        "original": "def entropy(p, unlogit=False):\n    \"\"\"Compute the entropy of a probability distribution\"\"\"\n    exponent = 2\n    if unlogit:\n        p = torch.pow(p, exponent)\n    plogp = p * torch.log(p)\n    plogp[p == 0] = 0\n    return -plogp.sum(dim=-1)",
        "mutated": [
            "def entropy(p, unlogit=False):\n    if False:\n        i = 10\n    'Compute the entropy of a probability distribution'\n    exponent = 2\n    if unlogit:\n        p = torch.pow(p, exponent)\n    plogp = p * torch.log(p)\n    plogp[p == 0] = 0\n    return -plogp.sum(dim=-1)",
            "def entropy(p, unlogit=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute the entropy of a probability distribution'\n    exponent = 2\n    if unlogit:\n        p = torch.pow(p, exponent)\n    plogp = p * torch.log(p)\n    plogp[p == 0] = 0\n    return -plogp.sum(dim=-1)",
            "def entropy(p, unlogit=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute the entropy of a probability distribution'\n    exponent = 2\n    if unlogit:\n        p = torch.pow(p, exponent)\n    plogp = p * torch.log(p)\n    plogp[p == 0] = 0\n    return -plogp.sum(dim=-1)",
            "def entropy(p, unlogit=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute the entropy of a probability distribution'\n    exponent = 2\n    if unlogit:\n        p = torch.pow(p, exponent)\n    plogp = p * torch.log(p)\n    plogp[p == 0] = 0\n    return -plogp.sum(dim=-1)",
            "def entropy(p, unlogit=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute the entropy of a probability distribution'\n    exponent = 2\n    if unlogit:\n        p = torch.pow(p, exponent)\n    plogp = p * torch.log(p)\n    plogp[p == 0] = 0\n    return -plogp.sum(dim=-1)"
        ]
    },
    {
        "func_name": "print_2d_tensor",
        "original": "def print_2d_tensor(tensor):\n    \"\"\"Print a 2D tensor\"\"\"\n    logger.info('lv, h >\\t' + '\\t'.join((f'{x + 1}' for x in range(len(tensor)))))\n    for row in range(len(tensor)):\n        if tensor.dtype != torch.long:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:.5f}' for x in tensor[row].cpu().data)))\n        else:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:d}' for x in tensor[row].cpu().data)))",
        "mutated": [
            "def print_2d_tensor(tensor):\n    if False:\n        i = 10\n    'Print a 2D tensor'\n    logger.info('lv, h >\\t' + '\\t'.join((f'{x + 1}' for x in range(len(tensor)))))\n    for row in range(len(tensor)):\n        if tensor.dtype != torch.long:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:.5f}' for x in tensor[row].cpu().data)))\n        else:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:d}' for x in tensor[row].cpu().data)))",
            "def print_2d_tensor(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Print a 2D tensor'\n    logger.info('lv, h >\\t' + '\\t'.join((f'{x + 1}' for x in range(len(tensor)))))\n    for row in range(len(tensor)):\n        if tensor.dtype != torch.long:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:.5f}' for x in tensor[row].cpu().data)))\n        else:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:d}' for x in tensor[row].cpu().data)))",
            "def print_2d_tensor(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Print a 2D tensor'\n    logger.info('lv, h >\\t' + '\\t'.join((f'{x + 1}' for x in range(len(tensor)))))\n    for row in range(len(tensor)):\n        if tensor.dtype != torch.long:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:.5f}' for x in tensor[row].cpu().data)))\n        else:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:d}' for x in tensor[row].cpu().data)))",
            "def print_2d_tensor(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Print a 2D tensor'\n    logger.info('lv, h >\\t' + '\\t'.join((f'{x + 1}' for x in range(len(tensor)))))\n    for row in range(len(tensor)):\n        if tensor.dtype != torch.long:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:.5f}' for x in tensor[row].cpu().data)))\n        else:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:d}' for x in tensor[row].cpu().data)))",
            "def print_2d_tensor(tensor):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Print a 2D tensor'\n    logger.info('lv, h >\\t' + '\\t'.join((f'{x + 1}' for x in range(len(tensor)))))\n    for row in range(len(tensor)):\n        if tensor.dtype != torch.long:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:.5f}' for x in tensor[row].cpu().data)))\n        else:\n            logger.info(f'layer {row + 1}:\\t' + '\\t'.join((f'{x:d}' for x in tensor[row].cpu().data)))"
        ]
    },
    {
        "func_name": "compute_heads_importance",
        "original": "def compute_heads_importance(args, model, eval_dataloader, compute_entropy=True, compute_importance=True, head_mask=None, actually_pruned=False):\n    \"\"\"This method shows how to compute:\n    - head attention entropy\n    - head importance scores according to http://arxiv.org/abs/1905.10650\n    \"\"\"\n    (n_layers, n_heads) = (model.config.num_hidden_layers, model.config.num_attention_heads)\n    head_importance = torch.zeros(n_layers, n_heads).to(args.device)\n    attn_entropy = torch.zeros(n_layers, n_heads).to(args.device)\n    if head_mask is None:\n        head_mask = torch.ones(n_layers, n_heads).to(args.device)\n    head_mask.requires_grad_(requires_grad=True)\n    if actually_pruned:\n        head_mask = None\n    tot_tokens = 0.0\n    total_loss = 0.0\n    for (step, inputs) in enumerate(tqdm(eval_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])):\n        inputs = tuple((t.to(args.device) for t in inputs))\n        (input_ids,) = inputs\n        outputs = model(input_ids, labels=input_ids, head_mask=head_mask)\n        (loss, _, all_attentions) = (outputs[0], outputs[1], outputs[-1])\n        loss.backward()\n        total_loss += loss.detach().cpu().numpy()\n        if compute_entropy:\n            for (layer, attn) in enumerate(all_attentions):\n                masked_entropy = entropy(attn.detach(), True)\n                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).sum(0).detach()\n        if compute_importance:\n            head_importance += head_mask.grad.abs().detach()\n        tot_tokens += torch.ones_like(input_ids).float().detach().sum().data\n    attn_entropy /= tot_tokens\n    head_importance /= tot_tokens\n    if not args.dont_normalize_importance_by_layer:\n        exponent = 2\n        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n    if not args.dont_normalize_global_importance:\n        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n    if compute_entropy:\n        logger.info('Attention entropies')\n        print_2d_tensor(attn_entropy)\n    if compute_importance:\n        logger.info('Head importance scores')\n        print_2d_tensor(head_importance)\n    logger.info('Head ranked by importance scores')\n    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=args.device)\n    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(head_importance.numel(), device=args.device)\n    head_ranks = head_ranks.view_as(head_importance)\n    print_2d_tensor(head_ranks)\n    return (attn_entropy, head_importance, total_loss)",
        "mutated": [
            "def compute_heads_importance(args, model, eval_dataloader, compute_entropy=True, compute_importance=True, head_mask=None, actually_pruned=False):\n    if False:\n        i = 10\n    'This method shows how to compute:\\n    - head attention entropy\\n    - head importance scores according to http://arxiv.org/abs/1905.10650\\n    '\n    (n_layers, n_heads) = (model.config.num_hidden_layers, model.config.num_attention_heads)\n    head_importance = torch.zeros(n_layers, n_heads).to(args.device)\n    attn_entropy = torch.zeros(n_layers, n_heads).to(args.device)\n    if head_mask is None:\n        head_mask = torch.ones(n_layers, n_heads).to(args.device)\n    head_mask.requires_grad_(requires_grad=True)\n    if actually_pruned:\n        head_mask = None\n    tot_tokens = 0.0\n    total_loss = 0.0\n    for (step, inputs) in enumerate(tqdm(eval_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])):\n        inputs = tuple((t.to(args.device) for t in inputs))\n        (input_ids,) = inputs\n        outputs = model(input_ids, labels=input_ids, head_mask=head_mask)\n        (loss, _, all_attentions) = (outputs[0], outputs[1], outputs[-1])\n        loss.backward()\n        total_loss += loss.detach().cpu().numpy()\n        if compute_entropy:\n            for (layer, attn) in enumerate(all_attentions):\n                masked_entropy = entropy(attn.detach(), True)\n                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).sum(0).detach()\n        if compute_importance:\n            head_importance += head_mask.grad.abs().detach()\n        tot_tokens += torch.ones_like(input_ids).float().detach().sum().data\n    attn_entropy /= tot_tokens\n    head_importance /= tot_tokens\n    if not args.dont_normalize_importance_by_layer:\n        exponent = 2\n        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n    if not args.dont_normalize_global_importance:\n        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n    if compute_entropy:\n        logger.info('Attention entropies')\n        print_2d_tensor(attn_entropy)\n    if compute_importance:\n        logger.info('Head importance scores')\n        print_2d_tensor(head_importance)\n    logger.info('Head ranked by importance scores')\n    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=args.device)\n    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(head_importance.numel(), device=args.device)\n    head_ranks = head_ranks.view_as(head_importance)\n    print_2d_tensor(head_ranks)\n    return (attn_entropy, head_importance, total_loss)",
            "def compute_heads_importance(args, model, eval_dataloader, compute_entropy=True, compute_importance=True, head_mask=None, actually_pruned=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'This method shows how to compute:\\n    - head attention entropy\\n    - head importance scores according to http://arxiv.org/abs/1905.10650\\n    '\n    (n_layers, n_heads) = (model.config.num_hidden_layers, model.config.num_attention_heads)\n    head_importance = torch.zeros(n_layers, n_heads).to(args.device)\n    attn_entropy = torch.zeros(n_layers, n_heads).to(args.device)\n    if head_mask is None:\n        head_mask = torch.ones(n_layers, n_heads).to(args.device)\n    head_mask.requires_grad_(requires_grad=True)\n    if actually_pruned:\n        head_mask = None\n    tot_tokens = 0.0\n    total_loss = 0.0\n    for (step, inputs) in enumerate(tqdm(eval_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])):\n        inputs = tuple((t.to(args.device) for t in inputs))\n        (input_ids,) = inputs\n        outputs = model(input_ids, labels=input_ids, head_mask=head_mask)\n        (loss, _, all_attentions) = (outputs[0], outputs[1], outputs[-1])\n        loss.backward()\n        total_loss += loss.detach().cpu().numpy()\n        if compute_entropy:\n            for (layer, attn) in enumerate(all_attentions):\n                masked_entropy = entropy(attn.detach(), True)\n                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).sum(0).detach()\n        if compute_importance:\n            head_importance += head_mask.grad.abs().detach()\n        tot_tokens += torch.ones_like(input_ids).float().detach().sum().data\n    attn_entropy /= tot_tokens\n    head_importance /= tot_tokens\n    if not args.dont_normalize_importance_by_layer:\n        exponent = 2\n        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n    if not args.dont_normalize_global_importance:\n        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n    if compute_entropy:\n        logger.info('Attention entropies')\n        print_2d_tensor(attn_entropy)\n    if compute_importance:\n        logger.info('Head importance scores')\n        print_2d_tensor(head_importance)\n    logger.info('Head ranked by importance scores')\n    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=args.device)\n    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(head_importance.numel(), device=args.device)\n    head_ranks = head_ranks.view_as(head_importance)\n    print_2d_tensor(head_ranks)\n    return (attn_entropy, head_importance, total_loss)",
            "def compute_heads_importance(args, model, eval_dataloader, compute_entropy=True, compute_importance=True, head_mask=None, actually_pruned=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'This method shows how to compute:\\n    - head attention entropy\\n    - head importance scores according to http://arxiv.org/abs/1905.10650\\n    '\n    (n_layers, n_heads) = (model.config.num_hidden_layers, model.config.num_attention_heads)\n    head_importance = torch.zeros(n_layers, n_heads).to(args.device)\n    attn_entropy = torch.zeros(n_layers, n_heads).to(args.device)\n    if head_mask is None:\n        head_mask = torch.ones(n_layers, n_heads).to(args.device)\n    head_mask.requires_grad_(requires_grad=True)\n    if actually_pruned:\n        head_mask = None\n    tot_tokens = 0.0\n    total_loss = 0.0\n    for (step, inputs) in enumerate(tqdm(eval_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])):\n        inputs = tuple((t.to(args.device) for t in inputs))\n        (input_ids,) = inputs\n        outputs = model(input_ids, labels=input_ids, head_mask=head_mask)\n        (loss, _, all_attentions) = (outputs[0], outputs[1], outputs[-1])\n        loss.backward()\n        total_loss += loss.detach().cpu().numpy()\n        if compute_entropy:\n            for (layer, attn) in enumerate(all_attentions):\n                masked_entropy = entropy(attn.detach(), True)\n                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).sum(0).detach()\n        if compute_importance:\n            head_importance += head_mask.grad.abs().detach()\n        tot_tokens += torch.ones_like(input_ids).float().detach().sum().data\n    attn_entropy /= tot_tokens\n    head_importance /= tot_tokens\n    if not args.dont_normalize_importance_by_layer:\n        exponent = 2\n        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n    if not args.dont_normalize_global_importance:\n        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n    if compute_entropy:\n        logger.info('Attention entropies')\n        print_2d_tensor(attn_entropy)\n    if compute_importance:\n        logger.info('Head importance scores')\n        print_2d_tensor(head_importance)\n    logger.info('Head ranked by importance scores')\n    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=args.device)\n    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(head_importance.numel(), device=args.device)\n    head_ranks = head_ranks.view_as(head_importance)\n    print_2d_tensor(head_ranks)\n    return (attn_entropy, head_importance, total_loss)",
            "def compute_heads_importance(args, model, eval_dataloader, compute_entropy=True, compute_importance=True, head_mask=None, actually_pruned=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'This method shows how to compute:\\n    - head attention entropy\\n    - head importance scores according to http://arxiv.org/abs/1905.10650\\n    '\n    (n_layers, n_heads) = (model.config.num_hidden_layers, model.config.num_attention_heads)\n    head_importance = torch.zeros(n_layers, n_heads).to(args.device)\n    attn_entropy = torch.zeros(n_layers, n_heads).to(args.device)\n    if head_mask is None:\n        head_mask = torch.ones(n_layers, n_heads).to(args.device)\n    head_mask.requires_grad_(requires_grad=True)\n    if actually_pruned:\n        head_mask = None\n    tot_tokens = 0.0\n    total_loss = 0.0\n    for (step, inputs) in enumerate(tqdm(eval_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])):\n        inputs = tuple((t.to(args.device) for t in inputs))\n        (input_ids,) = inputs\n        outputs = model(input_ids, labels=input_ids, head_mask=head_mask)\n        (loss, _, all_attentions) = (outputs[0], outputs[1], outputs[-1])\n        loss.backward()\n        total_loss += loss.detach().cpu().numpy()\n        if compute_entropy:\n            for (layer, attn) in enumerate(all_attentions):\n                masked_entropy = entropy(attn.detach(), True)\n                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).sum(0).detach()\n        if compute_importance:\n            head_importance += head_mask.grad.abs().detach()\n        tot_tokens += torch.ones_like(input_ids).float().detach().sum().data\n    attn_entropy /= tot_tokens\n    head_importance /= tot_tokens\n    if not args.dont_normalize_importance_by_layer:\n        exponent = 2\n        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n    if not args.dont_normalize_global_importance:\n        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n    if compute_entropy:\n        logger.info('Attention entropies')\n        print_2d_tensor(attn_entropy)\n    if compute_importance:\n        logger.info('Head importance scores')\n        print_2d_tensor(head_importance)\n    logger.info('Head ranked by importance scores')\n    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=args.device)\n    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(head_importance.numel(), device=args.device)\n    head_ranks = head_ranks.view_as(head_importance)\n    print_2d_tensor(head_ranks)\n    return (attn_entropy, head_importance, total_loss)",
            "def compute_heads_importance(args, model, eval_dataloader, compute_entropy=True, compute_importance=True, head_mask=None, actually_pruned=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'This method shows how to compute:\\n    - head attention entropy\\n    - head importance scores according to http://arxiv.org/abs/1905.10650\\n    '\n    (n_layers, n_heads) = (model.config.num_hidden_layers, model.config.num_attention_heads)\n    head_importance = torch.zeros(n_layers, n_heads).to(args.device)\n    attn_entropy = torch.zeros(n_layers, n_heads).to(args.device)\n    if head_mask is None:\n        head_mask = torch.ones(n_layers, n_heads).to(args.device)\n    head_mask.requires_grad_(requires_grad=True)\n    if actually_pruned:\n        head_mask = None\n    tot_tokens = 0.0\n    total_loss = 0.0\n    for (step, inputs) in enumerate(tqdm(eval_dataloader, desc='Iteration', disable=args.local_rank not in [-1, 0])):\n        inputs = tuple((t.to(args.device) for t in inputs))\n        (input_ids,) = inputs\n        outputs = model(input_ids, labels=input_ids, head_mask=head_mask)\n        (loss, _, all_attentions) = (outputs[0], outputs[1], outputs[-1])\n        loss.backward()\n        total_loss += loss.detach().cpu().numpy()\n        if compute_entropy:\n            for (layer, attn) in enumerate(all_attentions):\n                masked_entropy = entropy(attn.detach(), True)\n                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).sum(0).detach()\n        if compute_importance:\n            head_importance += head_mask.grad.abs().detach()\n        tot_tokens += torch.ones_like(input_ids).float().detach().sum().data\n    attn_entropy /= tot_tokens\n    head_importance /= tot_tokens\n    if not args.dont_normalize_importance_by_layer:\n        exponent = 2\n        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n    if not args.dont_normalize_global_importance:\n        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n    if compute_entropy:\n        logger.info('Attention entropies')\n        print_2d_tensor(attn_entropy)\n    if compute_importance:\n        logger.info('Head importance scores')\n        print_2d_tensor(head_importance)\n    logger.info('Head ranked by importance scores')\n    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=args.device)\n    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(head_importance.numel(), device=args.device)\n    head_ranks = head_ranks.view_as(head_importance)\n    print_2d_tensor(head_ranks)\n    return (attn_entropy, head_importance, total_loss)"
        ]
    },
    {
        "func_name": "mask_heads",
        "original": "def mask_heads(args, model, eval_dataloader):\n    \"\"\"This method shows how to mask head (set some heads to zero), to test the effect on the network,\n    based on the head importance scores, as described in Michel et al. (http://arxiv.org/abs/1905.10650)\n    \"\"\"\n    (_, head_importance, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False)\n    original_score = 1 / loss\n    logger.info('Pruning: original score: %f, threshold: %f', original_score, original_score * args.masking_threshold)\n    new_head_mask = torch.ones_like(head_importance)\n    num_to_mask = max(1, int(new_head_mask.numel() * args.masking_amount))\n    current_score = original_score\n    while current_score >= original_score * args.masking_threshold:\n        head_mask = new_head_mask.clone().detach()\n        head_importance[head_mask == 0.0] = float('Inf')\n        current_heads_to_mask = head_importance.view(-1).sort()[1]\n        if len(current_heads_to_mask) <= num_to_mask:\n            print('BREAK BY num_to_mask')\n            break\n        current_heads_to_mask = current_heads_to_mask[:num_to_mask]\n        logger.info('Heads to mask: %s', str(current_heads_to_mask.tolist()))\n        new_head_mask = new_head_mask.view(-1)\n        new_head_mask[current_heads_to_mask] = 0.0\n        new_head_mask = new_head_mask.view_as(head_mask)\n        new_head_mask = new_head_mask.clone().detach()\n        print_2d_tensor(new_head_mask)\n        (_, head_importance, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, head_mask=new_head_mask)\n        current_score = 1 / loss\n        logger.info('Masking: current score: %f, remaining heads %d (%.1f percents)', current_score, new_head_mask.sum(), new_head_mask.sum() / new_head_mask.numel() * 100)\n    logger.info('Final head mask')\n    print_2d_tensor(head_mask)\n    np.save(os.path.join(args.output_dir, 'head_mask.npy'), head_mask.detach().cpu().numpy())\n    return head_mask",
        "mutated": [
            "def mask_heads(args, model, eval_dataloader):\n    if False:\n        i = 10\n    'This method shows how to mask head (set some heads to zero), to test the effect on the network,\\n    based on the head importance scores, as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    (_, head_importance, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False)\n    original_score = 1 / loss\n    logger.info('Pruning: original score: %f, threshold: %f', original_score, original_score * args.masking_threshold)\n    new_head_mask = torch.ones_like(head_importance)\n    num_to_mask = max(1, int(new_head_mask.numel() * args.masking_amount))\n    current_score = original_score\n    while current_score >= original_score * args.masking_threshold:\n        head_mask = new_head_mask.clone().detach()\n        head_importance[head_mask == 0.0] = float('Inf')\n        current_heads_to_mask = head_importance.view(-1).sort()[1]\n        if len(current_heads_to_mask) <= num_to_mask:\n            print('BREAK BY num_to_mask')\n            break\n        current_heads_to_mask = current_heads_to_mask[:num_to_mask]\n        logger.info('Heads to mask: %s', str(current_heads_to_mask.tolist()))\n        new_head_mask = new_head_mask.view(-1)\n        new_head_mask[current_heads_to_mask] = 0.0\n        new_head_mask = new_head_mask.view_as(head_mask)\n        new_head_mask = new_head_mask.clone().detach()\n        print_2d_tensor(new_head_mask)\n        (_, head_importance, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, head_mask=new_head_mask)\n        current_score = 1 / loss\n        logger.info('Masking: current score: %f, remaining heads %d (%.1f percents)', current_score, new_head_mask.sum(), new_head_mask.sum() / new_head_mask.numel() * 100)\n    logger.info('Final head mask')\n    print_2d_tensor(head_mask)\n    np.save(os.path.join(args.output_dir, 'head_mask.npy'), head_mask.detach().cpu().numpy())\n    return head_mask",
            "def mask_heads(args, model, eval_dataloader):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'This method shows how to mask head (set some heads to zero), to test the effect on the network,\\n    based on the head importance scores, as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    (_, head_importance, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False)\n    original_score = 1 / loss\n    logger.info('Pruning: original score: %f, threshold: %f', original_score, original_score * args.masking_threshold)\n    new_head_mask = torch.ones_like(head_importance)\n    num_to_mask = max(1, int(new_head_mask.numel() * args.masking_amount))\n    current_score = original_score\n    while current_score >= original_score * args.masking_threshold:\n        head_mask = new_head_mask.clone().detach()\n        head_importance[head_mask == 0.0] = float('Inf')\n        current_heads_to_mask = head_importance.view(-1).sort()[1]\n        if len(current_heads_to_mask) <= num_to_mask:\n            print('BREAK BY num_to_mask')\n            break\n        current_heads_to_mask = current_heads_to_mask[:num_to_mask]\n        logger.info('Heads to mask: %s', str(current_heads_to_mask.tolist()))\n        new_head_mask = new_head_mask.view(-1)\n        new_head_mask[current_heads_to_mask] = 0.0\n        new_head_mask = new_head_mask.view_as(head_mask)\n        new_head_mask = new_head_mask.clone().detach()\n        print_2d_tensor(new_head_mask)\n        (_, head_importance, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, head_mask=new_head_mask)\n        current_score = 1 / loss\n        logger.info('Masking: current score: %f, remaining heads %d (%.1f percents)', current_score, new_head_mask.sum(), new_head_mask.sum() / new_head_mask.numel() * 100)\n    logger.info('Final head mask')\n    print_2d_tensor(head_mask)\n    np.save(os.path.join(args.output_dir, 'head_mask.npy'), head_mask.detach().cpu().numpy())\n    return head_mask",
            "def mask_heads(args, model, eval_dataloader):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'This method shows how to mask head (set some heads to zero), to test the effect on the network,\\n    based on the head importance scores, as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    (_, head_importance, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False)\n    original_score = 1 / loss\n    logger.info('Pruning: original score: %f, threshold: %f', original_score, original_score * args.masking_threshold)\n    new_head_mask = torch.ones_like(head_importance)\n    num_to_mask = max(1, int(new_head_mask.numel() * args.masking_amount))\n    current_score = original_score\n    while current_score >= original_score * args.masking_threshold:\n        head_mask = new_head_mask.clone().detach()\n        head_importance[head_mask == 0.0] = float('Inf')\n        current_heads_to_mask = head_importance.view(-1).sort()[1]\n        if len(current_heads_to_mask) <= num_to_mask:\n            print('BREAK BY num_to_mask')\n            break\n        current_heads_to_mask = current_heads_to_mask[:num_to_mask]\n        logger.info('Heads to mask: %s', str(current_heads_to_mask.tolist()))\n        new_head_mask = new_head_mask.view(-1)\n        new_head_mask[current_heads_to_mask] = 0.0\n        new_head_mask = new_head_mask.view_as(head_mask)\n        new_head_mask = new_head_mask.clone().detach()\n        print_2d_tensor(new_head_mask)\n        (_, head_importance, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, head_mask=new_head_mask)\n        current_score = 1 / loss\n        logger.info('Masking: current score: %f, remaining heads %d (%.1f percents)', current_score, new_head_mask.sum(), new_head_mask.sum() / new_head_mask.numel() * 100)\n    logger.info('Final head mask')\n    print_2d_tensor(head_mask)\n    np.save(os.path.join(args.output_dir, 'head_mask.npy'), head_mask.detach().cpu().numpy())\n    return head_mask",
            "def mask_heads(args, model, eval_dataloader):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'This method shows how to mask head (set some heads to zero), to test the effect on the network,\\n    based on the head importance scores, as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    (_, head_importance, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False)\n    original_score = 1 / loss\n    logger.info('Pruning: original score: %f, threshold: %f', original_score, original_score * args.masking_threshold)\n    new_head_mask = torch.ones_like(head_importance)\n    num_to_mask = max(1, int(new_head_mask.numel() * args.masking_amount))\n    current_score = original_score\n    while current_score >= original_score * args.masking_threshold:\n        head_mask = new_head_mask.clone().detach()\n        head_importance[head_mask == 0.0] = float('Inf')\n        current_heads_to_mask = head_importance.view(-1).sort()[1]\n        if len(current_heads_to_mask) <= num_to_mask:\n            print('BREAK BY num_to_mask')\n            break\n        current_heads_to_mask = current_heads_to_mask[:num_to_mask]\n        logger.info('Heads to mask: %s', str(current_heads_to_mask.tolist()))\n        new_head_mask = new_head_mask.view(-1)\n        new_head_mask[current_heads_to_mask] = 0.0\n        new_head_mask = new_head_mask.view_as(head_mask)\n        new_head_mask = new_head_mask.clone().detach()\n        print_2d_tensor(new_head_mask)\n        (_, head_importance, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, head_mask=new_head_mask)\n        current_score = 1 / loss\n        logger.info('Masking: current score: %f, remaining heads %d (%.1f percents)', current_score, new_head_mask.sum(), new_head_mask.sum() / new_head_mask.numel() * 100)\n    logger.info('Final head mask')\n    print_2d_tensor(head_mask)\n    np.save(os.path.join(args.output_dir, 'head_mask.npy'), head_mask.detach().cpu().numpy())\n    return head_mask",
            "def mask_heads(args, model, eval_dataloader):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'This method shows how to mask head (set some heads to zero), to test the effect on the network,\\n    based on the head importance scores, as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    (_, head_importance, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False)\n    original_score = 1 / loss\n    logger.info('Pruning: original score: %f, threshold: %f', original_score, original_score * args.masking_threshold)\n    new_head_mask = torch.ones_like(head_importance)\n    num_to_mask = max(1, int(new_head_mask.numel() * args.masking_amount))\n    current_score = original_score\n    while current_score >= original_score * args.masking_threshold:\n        head_mask = new_head_mask.clone().detach()\n        head_importance[head_mask == 0.0] = float('Inf')\n        current_heads_to_mask = head_importance.view(-1).sort()[1]\n        if len(current_heads_to_mask) <= num_to_mask:\n            print('BREAK BY num_to_mask')\n            break\n        current_heads_to_mask = current_heads_to_mask[:num_to_mask]\n        logger.info('Heads to mask: %s', str(current_heads_to_mask.tolist()))\n        new_head_mask = new_head_mask.view(-1)\n        new_head_mask[current_heads_to_mask] = 0.0\n        new_head_mask = new_head_mask.view_as(head_mask)\n        new_head_mask = new_head_mask.clone().detach()\n        print_2d_tensor(new_head_mask)\n        (_, head_importance, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, head_mask=new_head_mask)\n        current_score = 1 / loss\n        logger.info('Masking: current score: %f, remaining heads %d (%.1f percents)', current_score, new_head_mask.sum(), new_head_mask.sum() / new_head_mask.numel() * 100)\n    logger.info('Final head mask')\n    print_2d_tensor(head_mask)\n    np.save(os.path.join(args.output_dir, 'head_mask.npy'), head_mask.detach().cpu().numpy())\n    return head_mask"
        ]
    },
    {
        "func_name": "prune_heads",
        "original": "def prune_heads(args, model, eval_dataloader, head_mask):\n    \"\"\"This method shows how to prune head (remove heads weights) based on\n    the head importance scores as described in Michel et al. (http://arxiv.org/abs/1905.10650)\n    \"\"\"\n    before_time = datetime.now()\n    (_, _, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=head_mask)\n    score_masking = 1 / loss\n    original_time = datetime.now() - before_time\n    original_num_params = sum((p.numel() for p in model.parameters()))\n    heads_to_prune = {layer: (1 - head_mask[layer].long()).nonzero().squeeze().tolist() for layer in range(len(head_mask))}\n    for (k, v) in heads_to_prune.items():\n        if isinstance(v, int):\n            heads_to_prune[k] = [v]\n    assert sum((len(h) for h in heads_to_prune.values())) == (1 - head_mask.long()).sum().item()\n    model.prune_heads(heads_to_prune)\n    pruned_num_params = sum((p.numel() for p in model.parameters()))\n    before_time = datetime.now()\n    (_, _, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=None, actually_pruned=True)\n    score_pruning = 1 / loss\n    new_time = datetime.now() - before_time\n    logger.info('Pruning: original num of params: %.2e, after pruning %.2e (%.1f percents)', original_num_params, pruned_num_params, pruned_num_params / original_num_params * 100)\n    logger.info('Pruning: score with masking: %f score with pruning: %f', score_masking, score_pruning)\n    logger.info('Pruning: speed ratio (original timing / new timing): %f percents', original_time / new_time * 100)\n    save_model(model, args.output_dir)",
        "mutated": [
            "def prune_heads(args, model, eval_dataloader, head_mask):\n    if False:\n        i = 10\n    'This method shows how to prune head (remove heads weights) based on\\n    the head importance scores as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    before_time = datetime.now()\n    (_, _, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=head_mask)\n    score_masking = 1 / loss\n    original_time = datetime.now() - before_time\n    original_num_params = sum((p.numel() for p in model.parameters()))\n    heads_to_prune = {layer: (1 - head_mask[layer].long()).nonzero().squeeze().tolist() for layer in range(len(head_mask))}\n    for (k, v) in heads_to_prune.items():\n        if isinstance(v, int):\n            heads_to_prune[k] = [v]\n    assert sum((len(h) for h in heads_to_prune.values())) == (1 - head_mask.long()).sum().item()\n    model.prune_heads(heads_to_prune)\n    pruned_num_params = sum((p.numel() for p in model.parameters()))\n    before_time = datetime.now()\n    (_, _, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=None, actually_pruned=True)\n    score_pruning = 1 / loss\n    new_time = datetime.now() - before_time\n    logger.info('Pruning: original num of params: %.2e, after pruning %.2e (%.1f percents)', original_num_params, pruned_num_params, pruned_num_params / original_num_params * 100)\n    logger.info('Pruning: score with masking: %f score with pruning: %f', score_masking, score_pruning)\n    logger.info('Pruning: speed ratio (original timing / new timing): %f percents', original_time / new_time * 100)\n    save_model(model, args.output_dir)",
            "def prune_heads(args, model, eval_dataloader, head_mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'This method shows how to prune head (remove heads weights) based on\\n    the head importance scores as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    before_time = datetime.now()\n    (_, _, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=head_mask)\n    score_masking = 1 / loss\n    original_time = datetime.now() - before_time\n    original_num_params = sum((p.numel() for p in model.parameters()))\n    heads_to_prune = {layer: (1 - head_mask[layer].long()).nonzero().squeeze().tolist() for layer in range(len(head_mask))}\n    for (k, v) in heads_to_prune.items():\n        if isinstance(v, int):\n            heads_to_prune[k] = [v]\n    assert sum((len(h) for h in heads_to_prune.values())) == (1 - head_mask.long()).sum().item()\n    model.prune_heads(heads_to_prune)\n    pruned_num_params = sum((p.numel() for p in model.parameters()))\n    before_time = datetime.now()\n    (_, _, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=None, actually_pruned=True)\n    score_pruning = 1 / loss\n    new_time = datetime.now() - before_time\n    logger.info('Pruning: original num of params: %.2e, after pruning %.2e (%.1f percents)', original_num_params, pruned_num_params, pruned_num_params / original_num_params * 100)\n    logger.info('Pruning: score with masking: %f score with pruning: %f', score_masking, score_pruning)\n    logger.info('Pruning: speed ratio (original timing / new timing): %f percents', original_time / new_time * 100)\n    save_model(model, args.output_dir)",
            "def prune_heads(args, model, eval_dataloader, head_mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'This method shows how to prune head (remove heads weights) based on\\n    the head importance scores as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    before_time = datetime.now()\n    (_, _, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=head_mask)\n    score_masking = 1 / loss\n    original_time = datetime.now() - before_time\n    original_num_params = sum((p.numel() for p in model.parameters()))\n    heads_to_prune = {layer: (1 - head_mask[layer].long()).nonzero().squeeze().tolist() for layer in range(len(head_mask))}\n    for (k, v) in heads_to_prune.items():\n        if isinstance(v, int):\n            heads_to_prune[k] = [v]\n    assert sum((len(h) for h in heads_to_prune.values())) == (1 - head_mask.long()).sum().item()\n    model.prune_heads(heads_to_prune)\n    pruned_num_params = sum((p.numel() for p in model.parameters()))\n    before_time = datetime.now()\n    (_, _, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=None, actually_pruned=True)\n    score_pruning = 1 / loss\n    new_time = datetime.now() - before_time\n    logger.info('Pruning: original num of params: %.2e, after pruning %.2e (%.1f percents)', original_num_params, pruned_num_params, pruned_num_params / original_num_params * 100)\n    logger.info('Pruning: score with masking: %f score with pruning: %f', score_masking, score_pruning)\n    logger.info('Pruning: speed ratio (original timing / new timing): %f percents', original_time / new_time * 100)\n    save_model(model, args.output_dir)",
            "def prune_heads(args, model, eval_dataloader, head_mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'This method shows how to prune head (remove heads weights) based on\\n    the head importance scores as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    before_time = datetime.now()\n    (_, _, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=head_mask)\n    score_masking = 1 / loss\n    original_time = datetime.now() - before_time\n    original_num_params = sum((p.numel() for p in model.parameters()))\n    heads_to_prune = {layer: (1 - head_mask[layer].long()).nonzero().squeeze().tolist() for layer in range(len(head_mask))}\n    for (k, v) in heads_to_prune.items():\n        if isinstance(v, int):\n            heads_to_prune[k] = [v]\n    assert sum((len(h) for h in heads_to_prune.values())) == (1 - head_mask.long()).sum().item()\n    model.prune_heads(heads_to_prune)\n    pruned_num_params = sum((p.numel() for p in model.parameters()))\n    before_time = datetime.now()\n    (_, _, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=None, actually_pruned=True)\n    score_pruning = 1 / loss\n    new_time = datetime.now() - before_time\n    logger.info('Pruning: original num of params: %.2e, after pruning %.2e (%.1f percents)', original_num_params, pruned_num_params, pruned_num_params / original_num_params * 100)\n    logger.info('Pruning: score with masking: %f score with pruning: %f', score_masking, score_pruning)\n    logger.info('Pruning: speed ratio (original timing / new timing): %f percents', original_time / new_time * 100)\n    save_model(model, args.output_dir)",
            "def prune_heads(args, model, eval_dataloader, head_mask):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'This method shows how to prune head (remove heads weights) based on\\n    the head importance scores as described in Michel et al. (http://arxiv.org/abs/1905.10650)\\n    '\n    before_time = datetime.now()\n    (_, _, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=head_mask)\n    score_masking = 1 / loss\n    original_time = datetime.now() - before_time\n    original_num_params = sum((p.numel() for p in model.parameters()))\n    heads_to_prune = {layer: (1 - head_mask[layer].long()).nonzero().squeeze().tolist() for layer in range(len(head_mask))}\n    for (k, v) in heads_to_prune.items():\n        if isinstance(v, int):\n            heads_to_prune[k] = [v]\n    assert sum((len(h) for h in heads_to_prune.values())) == (1 - head_mask.long()).sum().item()\n    model.prune_heads(heads_to_prune)\n    pruned_num_params = sum((p.numel() for p in model.parameters()))\n    before_time = datetime.now()\n    (_, _, loss) = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=None, actually_pruned=True)\n    score_pruning = 1 / loss\n    new_time = datetime.now() - before_time\n    logger.info('Pruning: original num of params: %.2e, after pruning %.2e (%.1f percents)', original_num_params, pruned_num_params, pruned_num_params / original_num_params * 100)\n    logger.info('Pruning: score with masking: %f score with pruning: %f', score_masking, score_pruning)\n    logger.info('Pruning: speed ratio (original timing / new timing): %f percents', original_time / new_time * 100)\n    save_model(model, args.output_dir)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name_or_path')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name_or_path')\n    parser.add_argument('--cache_dir', default=None, type=str, help='Where do you want to store the pre-trained models downloaded from s3')\n    parser.add_argument('--data_subset', type=int, default=-1, help='If > 0: limit the data to a subset of data_subset instances.')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Whether to overwrite data in output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--dont_normalize_importance_by_layer', action='store_true', help=\"Don't normalize importance score by layers\")\n    parser.add_argument('--dont_normalize_global_importance', action='store_true', help=\"Don't normalize all importance scores between 0 and 1\")\n    parser.add_argument('--try_masking', action='store_true', help='Whether to try to mask head until a threshold of accuracy.')\n    parser.add_argument('--masking_threshold', default=0.9, type=float, help='masking threshold in term of metrics (stop masking when metric < threshold * original metric value).')\n    parser.add_argument('--masking_amount', default=0.1, type=float, help='Amount to heads to masking at each masking step.')\n    parser.add_argument('--metric_name', default='acc', type=str, help='Metric to use for head masking.')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after WordPiece tokenization. \\nSequences longer than this will be truncated, sequences shorter padded.')\n    parser.add_argument('--batch_size', default=1, type=int, help='Batch size.')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    args = parser.parse_args()\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        args.device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        args.device = torch.device('cuda', args.local_rank)\n        args.n_gpu = 1\n        torch.distributed.init_process_group(backend='nccl')\n    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.info('device: {} n_gpu: {}, distributed: {}'.format(args.device, args.n_gpu, bool(args.local_rank != -1)))\n    model = GPT2LMHeadModel.from_pretrained(args.model_name_or_path)\n    model.to(args.device)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    elif args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    os.makedirs(args.output_dir, exist_ok=True)\n    torch.save(args, os.path.join(args.output_dir, 'run_args.bin'))\n    logger.info('Training/evaluation parameters %s', args)\n    numpy_data = np.concatenate([np.loadtxt(args.data_dir, dtype=np.int64)])\n    train_tensor_dataset = (torch.from_numpy(numpy_data),)\n    train_data = TensorDataset(*train_tensor_dataset)\n    train_sampler = RandomSampler(train_data)\n    eval_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.batch_size)\n    compute_heads_importance(args, model, eval_dataloader)\n    if args.try_masking and args.masking_threshold > 0.0 and (args.masking_threshold < 1.0):\n        head_mask = mask_heads(args, model, eval_dataloader)\n        prune_heads(args, model, eval_dataloader, head_mask)",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name_or_path')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name_or_path')\n    parser.add_argument('--cache_dir', default=None, type=str, help='Where do you want to store the pre-trained models downloaded from s3')\n    parser.add_argument('--data_subset', type=int, default=-1, help='If > 0: limit the data to a subset of data_subset instances.')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Whether to overwrite data in output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--dont_normalize_importance_by_layer', action='store_true', help=\"Don't normalize importance score by layers\")\n    parser.add_argument('--dont_normalize_global_importance', action='store_true', help=\"Don't normalize all importance scores between 0 and 1\")\n    parser.add_argument('--try_masking', action='store_true', help='Whether to try to mask head until a threshold of accuracy.')\n    parser.add_argument('--masking_threshold', default=0.9, type=float, help='masking threshold in term of metrics (stop masking when metric < threshold * original metric value).')\n    parser.add_argument('--masking_amount', default=0.1, type=float, help='Amount to heads to masking at each masking step.')\n    parser.add_argument('--metric_name', default='acc', type=str, help='Metric to use for head masking.')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after WordPiece tokenization. \\nSequences longer than this will be truncated, sequences shorter padded.')\n    parser.add_argument('--batch_size', default=1, type=int, help='Batch size.')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    args = parser.parse_args()\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        args.device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        args.device = torch.device('cuda', args.local_rank)\n        args.n_gpu = 1\n        torch.distributed.init_process_group(backend='nccl')\n    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.info('device: {} n_gpu: {}, distributed: {}'.format(args.device, args.n_gpu, bool(args.local_rank != -1)))\n    model = GPT2LMHeadModel.from_pretrained(args.model_name_or_path)\n    model.to(args.device)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    elif args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    os.makedirs(args.output_dir, exist_ok=True)\n    torch.save(args, os.path.join(args.output_dir, 'run_args.bin'))\n    logger.info('Training/evaluation parameters %s', args)\n    numpy_data = np.concatenate([np.loadtxt(args.data_dir, dtype=np.int64)])\n    train_tensor_dataset = (torch.from_numpy(numpy_data),)\n    train_data = TensorDataset(*train_tensor_dataset)\n    train_sampler = RandomSampler(train_data)\n    eval_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.batch_size)\n    compute_heads_importance(args, model, eval_dataloader)\n    if args.try_masking and args.masking_threshold > 0.0 and (args.masking_threshold < 1.0):\n        head_mask = mask_heads(args, model, eval_dataloader)\n        prune_heads(args, model, eval_dataloader, head_mask)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name_or_path')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name_or_path')\n    parser.add_argument('--cache_dir', default=None, type=str, help='Where do you want to store the pre-trained models downloaded from s3')\n    parser.add_argument('--data_subset', type=int, default=-1, help='If > 0: limit the data to a subset of data_subset instances.')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Whether to overwrite data in output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--dont_normalize_importance_by_layer', action='store_true', help=\"Don't normalize importance score by layers\")\n    parser.add_argument('--dont_normalize_global_importance', action='store_true', help=\"Don't normalize all importance scores between 0 and 1\")\n    parser.add_argument('--try_masking', action='store_true', help='Whether to try to mask head until a threshold of accuracy.')\n    parser.add_argument('--masking_threshold', default=0.9, type=float, help='masking threshold in term of metrics (stop masking when metric < threshold * original metric value).')\n    parser.add_argument('--masking_amount', default=0.1, type=float, help='Amount to heads to masking at each masking step.')\n    parser.add_argument('--metric_name', default='acc', type=str, help='Metric to use for head masking.')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after WordPiece tokenization. \\nSequences longer than this will be truncated, sequences shorter padded.')\n    parser.add_argument('--batch_size', default=1, type=int, help='Batch size.')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    args = parser.parse_args()\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        args.device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        args.device = torch.device('cuda', args.local_rank)\n        args.n_gpu = 1\n        torch.distributed.init_process_group(backend='nccl')\n    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.info('device: {} n_gpu: {}, distributed: {}'.format(args.device, args.n_gpu, bool(args.local_rank != -1)))\n    model = GPT2LMHeadModel.from_pretrained(args.model_name_or_path)\n    model.to(args.device)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    elif args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    os.makedirs(args.output_dir, exist_ok=True)\n    torch.save(args, os.path.join(args.output_dir, 'run_args.bin'))\n    logger.info('Training/evaluation parameters %s', args)\n    numpy_data = np.concatenate([np.loadtxt(args.data_dir, dtype=np.int64)])\n    train_tensor_dataset = (torch.from_numpy(numpy_data),)\n    train_data = TensorDataset(*train_tensor_dataset)\n    train_sampler = RandomSampler(train_data)\n    eval_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.batch_size)\n    compute_heads_importance(args, model, eval_dataloader)\n    if args.try_masking and args.masking_threshold > 0.0 and (args.masking_threshold < 1.0):\n        head_mask = mask_heads(args, model, eval_dataloader)\n        prune_heads(args, model, eval_dataloader, head_mask)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name_or_path')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name_or_path')\n    parser.add_argument('--cache_dir', default=None, type=str, help='Where do you want to store the pre-trained models downloaded from s3')\n    parser.add_argument('--data_subset', type=int, default=-1, help='If > 0: limit the data to a subset of data_subset instances.')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Whether to overwrite data in output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--dont_normalize_importance_by_layer', action='store_true', help=\"Don't normalize importance score by layers\")\n    parser.add_argument('--dont_normalize_global_importance', action='store_true', help=\"Don't normalize all importance scores between 0 and 1\")\n    parser.add_argument('--try_masking', action='store_true', help='Whether to try to mask head until a threshold of accuracy.')\n    parser.add_argument('--masking_threshold', default=0.9, type=float, help='masking threshold in term of metrics (stop masking when metric < threshold * original metric value).')\n    parser.add_argument('--masking_amount', default=0.1, type=float, help='Amount to heads to masking at each masking step.')\n    parser.add_argument('--metric_name', default='acc', type=str, help='Metric to use for head masking.')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after WordPiece tokenization. \\nSequences longer than this will be truncated, sequences shorter padded.')\n    parser.add_argument('--batch_size', default=1, type=int, help='Batch size.')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    args = parser.parse_args()\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        args.device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        args.device = torch.device('cuda', args.local_rank)\n        args.n_gpu = 1\n        torch.distributed.init_process_group(backend='nccl')\n    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.info('device: {} n_gpu: {}, distributed: {}'.format(args.device, args.n_gpu, bool(args.local_rank != -1)))\n    model = GPT2LMHeadModel.from_pretrained(args.model_name_or_path)\n    model.to(args.device)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    elif args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    os.makedirs(args.output_dir, exist_ok=True)\n    torch.save(args, os.path.join(args.output_dir, 'run_args.bin'))\n    logger.info('Training/evaluation parameters %s', args)\n    numpy_data = np.concatenate([np.loadtxt(args.data_dir, dtype=np.int64)])\n    train_tensor_dataset = (torch.from_numpy(numpy_data),)\n    train_data = TensorDataset(*train_tensor_dataset)\n    train_sampler = RandomSampler(train_data)\n    eval_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.batch_size)\n    compute_heads_importance(args, model, eval_dataloader)\n    if args.try_masking and args.masking_threshold > 0.0 and (args.masking_threshold < 1.0):\n        head_mask = mask_heads(args, model, eval_dataloader)\n        prune_heads(args, model, eval_dataloader, head_mask)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name_or_path')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name_or_path')\n    parser.add_argument('--cache_dir', default=None, type=str, help='Where do you want to store the pre-trained models downloaded from s3')\n    parser.add_argument('--data_subset', type=int, default=-1, help='If > 0: limit the data to a subset of data_subset instances.')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Whether to overwrite data in output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--dont_normalize_importance_by_layer', action='store_true', help=\"Don't normalize importance score by layers\")\n    parser.add_argument('--dont_normalize_global_importance', action='store_true', help=\"Don't normalize all importance scores between 0 and 1\")\n    parser.add_argument('--try_masking', action='store_true', help='Whether to try to mask head until a threshold of accuracy.')\n    parser.add_argument('--masking_threshold', default=0.9, type=float, help='masking threshold in term of metrics (stop masking when metric < threshold * original metric value).')\n    parser.add_argument('--masking_amount', default=0.1, type=float, help='Amount to heads to masking at each masking step.')\n    parser.add_argument('--metric_name', default='acc', type=str, help='Metric to use for head masking.')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after WordPiece tokenization. \\nSequences longer than this will be truncated, sequences shorter padded.')\n    parser.add_argument('--batch_size', default=1, type=int, help='Batch size.')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    args = parser.parse_args()\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        args.device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        args.device = torch.device('cuda', args.local_rank)\n        args.n_gpu = 1\n        torch.distributed.init_process_group(backend='nccl')\n    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.info('device: {} n_gpu: {}, distributed: {}'.format(args.device, args.n_gpu, bool(args.local_rank != -1)))\n    model = GPT2LMHeadModel.from_pretrained(args.model_name_or_path)\n    model.to(args.device)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    elif args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    os.makedirs(args.output_dir, exist_ok=True)\n    torch.save(args, os.path.join(args.output_dir, 'run_args.bin'))\n    logger.info('Training/evaluation parameters %s', args)\n    numpy_data = np.concatenate([np.loadtxt(args.data_dir, dtype=np.int64)])\n    train_tensor_dataset = (torch.from_numpy(numpy_data),)\n    train_data = TensorDataset(*train_tensor_dataset)\n    train_sampler = RandomSampler(train_data)\n    eval_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.batch_size)\n    compute_heads_importance(args, model, eval_dataloader)\n    if args.try_masking and args.masking_threshold > 0.0 and (args.masking_threshold < 1.0):\n        head_mask = mask_heads(args, model, eval_dataloader)\n        prune_heads(args, model, eval_dataloader, head_mask)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_dir', default=None, type=str, required=True, help='The input data dir. Should contain the .tsv files (or other data files) for the task.')\n    parser.add_argument('--model_name_or_path', default=None, type=str, required=True, help='Path to pretrained model or model identifier from huggingface.co/models')\n    parser.add_argument('--output_dir', default=None, type=str, required=True, help='The output directory where the model predictions and checkpoints will be written.')\n    parser.add_argument('--config_name', default='', type=str, help='Pretrained config name or path if not the same as model_name_or_path')\n    parser.add_argument('--tokenizer_name', default='', type=str, help='Pretrained tokenizer name or path if not the same as model_name_or_path')\n    parser.add_argument('--cache_dir', default=None, type=str, help='Where do you want to store the pre-trained models downloaded from s3')\n    parser.add_argument('--data_subset', type=int, default=-1, help='If > 0: limit the data to a subset of data_subset instances.')\n    parser.add_argument('--overwrite_output_dir', action='store_true', help='Whether to overwrite data in output directory')\n    parser.add_argument('--overwrite_cache', action='store_true', help='Overwrite the cached training and evaluation sets')\n    parser.add_argument('--dont_normalize_importance_by_layer', action='store_true', help=\"Don't normalize importance score by layers\")\n    parser.add_argument('--dont_normalize_global_importance', action='store_true', help=\"Don't normalize all importance scores between 0 and 1\")\n    parser.add_argument('--try_masking', action='store_true', help='Whether to try to mask head until a threshold of accuracy.')\n    parser.add_argument('--masking_threshold', default=0.9, type=float, help='masking threshold in term of metrics (stop masking when metric < threshold * original metric value).')\n    parser.add_argument('--masking_amount', default=0.1, type=float, help='Amount to heads to masking at each masking step.')\n    parser.add_argument('--metric_name', default='acc', type=str, help='Metric to use for head masking.')\n    parser.add_argument('--max_seq_length', default=128, type=int, help='The maximum total input sequence length after WordPiece tokenization. \\nSequences longer than this will be truncated, sequences shorter padded.')\n    parser.add_argument('--batch_size', default=1, type=int, help='Batch size.')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--local_rank', type=int, default=-1, help='local_rank for distributed training on gpus')\n    parser.add_argument('--no_cuda', action='store_true', help='Whether not to use CUDA when available')\n    parser.add_argument('--server_ip', type=str, default='', help='Can be used for distant debugging.')\n    parser.add_argument('--server_port', type=str, default='', help='Can be used for distant debugging.')\n    args = parser.parse_args()\n    if args.server_ip and args.server_port:\n        import ptvsd\n        print('Waiting for debugger attach')\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    if args.local_rank == -1 or args.no_cuda:\n        args.device = torch.device('cuda' if torch.cuda.is_available() and (not args.no_cuda) else 'cpu')\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        args.device = torch.device('cuda', args.local_rank)\n        args.n_gpu = 1\n        torch.distributed.init_process_group(backend='nccl')\n    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.info('device: {} n_gpu: {}, distributed: {}'.format(args.device, args.n_gpu, bool(args.local_rank != -1)))\n    model = GPT2LMHeadModel.from_pretrained(args.model_name_or_path)\n    model.to(args.device)\n    if args.local_rank != -1:\n        model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n    elif args.n_gpu > 1:\n        model = nn.DataParallel(model)\n    os.makedirs(args.output_dir, exist_ok=True)\n    torch.save(args, os.path.join(args.output_dir, 'run_args.bin'))\n    logger.info('Training/evaluation parameters %s', args)\n    numpy_data = np.concatenate([np.loadtxt(args.data_dir, dtype=np.int64)])\n    train_tensor_dataset = (torch.from_numpy(numpy_data),)\n    train_data = TensorDataset(*train_tensor_dataset)\n    train_sampler = RandomSampler(train_data)\n    eval_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.batch_size)\n    compute_heads_importance(args, model, eval_dataloader)\n    if args.try_masking and args.masking_threshold > 0.0 and (args.masking_threshold < 1.0):\n        head_mask = mask_heads(args, model, eval_dataloader)\n        prune_heads(args, model, eval_dataloader, head_mask)"
        ]
    }
]