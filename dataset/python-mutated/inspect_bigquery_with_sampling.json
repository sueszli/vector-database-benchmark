[
    {
        "func_name": "callback",
        "original": "def callback(message: google.cloud.pubsub_v1.subscriber.message.Message) -> None:\n    try:\n        if message.attributes['DlpJobName'] == operation.name:\n            message.ack()\n            job = dlp.get_dlp_job(request={'name': operation.name})\n            print(f'Job name: {job.name}')\n            if job.inspect_details.result.info_type_stats:\n                for finding in job.inspect_details.result.info_type_stats:\n                    print(f'Info type: {finding.info_type.name}; Count: {finding.count}')\n            else:\n                print('No findings.')\n            job_done.set()\n        else:\n            message.drop()\n    except Exception as e:\n        print(e)\n        raise",
        "mutated": [
            "def callback(message: google.cloud.pubsub_v1.subscriber.message.Message) -> None:\n    if False:\n        i = 10\n    try:\n        if message.attributes['DlpJobName'] == operation.name:\n            message.ack()\n            job = dlp.get_dlp_job(request={'name': operation.name})\n            print(f'Job name: {job.name}')\n            if job.inspect_details.result.info_type_stats:\n                for finding in job.inspect_details.result.info_type_stats:\n                    print(f'Info type: {finding.info_type.name}; Count: {finding.count}')\n            else:\n                print('No findings.')\n            job_done.set()\n        else:\n            message.drop()\n    except Exception as e:\n        print(e)\n        raise",
            "def callback(message: google.cloud.pubsub_v1.subscriber.message.Message) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    try:\n        if message.attributes['DlpJobName'] == operation.name:\n            message.ack()\n            job = dlp.get_dlp_job(request={'name': operation.name})\n            print(f'Job name: {job.name}')\n            if job.inspect_details.result.info_type_stats:\n                for finding in job.inspect_details.result.info_type_stats:\n                    print(f'Info type: {finding.info_type.name}; Count: {finding.count}')\n            else:\n                print('No findings.')\n            job_done.set()\n        else:\n            message.drop()\n    except Exception as e:\n        print(e)\n        raise",
            "def callback(message: google.cloud.pubsub_v1.subscriber.message.Message) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    try:\n        if message.attributes['DlpJobName'] == operation.name:\n            message.ack()\n            job = dlp.get_dlp_job(request={'name': operation.name})\n            print(f'Job name: {job.name}')\n            if job.inspect_details.result.info_type_stats:\n                for finding in job.inspect_details.result.info_type_stats:\n                    print(f'Info type: {finding.info_type.name}; Count: {finding.count}')\n            else:\n                print('No findings.')\n            job_done.set()\n        else:\n            message.drop()\n    except Exception as e:\n        print(e)\n        raise",
            "def callback(message: google.cloud.pubsub_v1.subscriber.message.Message) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    try:\n        if message.attributes['DlpJobName'] == operation.name:\n            message.ack()\n            job = dlp.get_dlp_job(request={'name': operation.name})\n            print(f'Job name: {job.name}')\n            if job.inspect_details.result.info_type_stats:\n                for finding in job.inspect_details.result.info_type_stats:\n                    print(f'Info type: {finding.info_type.name}; Count: {finding.count}')\n            else:\n                print('No findings.')\n            job_done.set()\n        else:\n            message.drop()\n    except Exception as e:\n        print(e)\n        raise",
            "def callback(message: google.cloud.pubsub_v1.subscriber.message.Message) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    try:\n        if message.attributes['DlpJobName'] == operation.name:\n            message.ack()\n            job = dlp.get_dlp_job(request={'name': operation.name})\n            print(f'Job name: {job.name}')\n            if job.inspect_details.result.info_type_stats:\n                for finding in job.inspect_details.result.info_type_stats:\n                    print(f'Info type: {finding.info_type.name}; Count: {finding.count}')\n            else:\n                print('No findings.')\n            job_done.set()\n        else:\n            message.drop()\n    except Exception as e:\n        print(e)\n        raise"
        ]
    },
    {
        "func_name": "inspect_bigquery_table_with_sampling",
        "original": "def inspect_bigquery_table_with_sampling(project: str, topic_id: str, subscription_id: str, min_likelihood: str=None, max_findings: str=None, timeout: int=300) -> None:\n    \"\"\"Uses the Data Loss Prevention API to analyze BigQuery data by limiting\n    the amount of data to be scanned.\n    Args:\n        project: The Google Cloud project id to use as a parent resource.\n        topic_id: The id of the Cloud Pub/Sub topic to which the API will\n            broadcast job completion. The topic must already exist.\n        subscription_id: The id of the Cloud Pub/Sub subscription to listen on\n            while waiting for job completion. The subscription must already\n            exist and be subscribed to the topic.\n        min_likelihood: A string representing the minimum likelihood threshold\n            that constitutes a match. One of: 'LIKELIHOOD_UNSPECIFIED',\n            'VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE', 'LIKELY', 'VERY_LIKELY'.\n        max_findings: The maximum number of findings to report; 0 = no maximum.\n        timeout: The number of seconds to wait for a response from the API.\n    \"\"\"\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    inspect_config = {'info_types': [{'name': 'PERSON_NAME'}], 'min_likelihood': min_likelihood, 'limits': {'max_findings_per_request': max_findings}, 'include_quote': True}\n    table_reference = {'project_id': 'bigquery-public-data', 'dataset_id': 'usa_names', 'table_id': 'usa_1910_current'}\n    storage_config = {'big_query_options': {'table_reference': table_reference, 'rows_limit': 1000, 'sample_method': 'RANDOM_START', 'identifying_fields': [{'name': 'name'}]}}\n    topic = google.cloud.pubsub.PublisherClient.topic_path(project, topic_id)\n    actions = [{'pub_sub': {'topic': topic}}]\n    inspect_job = {'inspect_config': inspect_config, 'storage_config': storage_config, 'actions': actions}\n    parent = f'projects/{project}/locations/global'\n    operation = dlp.create_dlp_job(request={'parent': parent, 'inspect_job': inspect_job})\n    print(f'Inspection operation started: {operation.name}')\n    subscriber = google.cloud.pubsub.SubscriberClient()\n    subscription_path = subscriber.subscription_path(project, subscription_id)\n    job_done = threading.Event()\n\n    def callback(message: google.cloud.pubsub_v1.subscriber.message.Message) -> None:\n        try:\n            if message.attributes['DlpJobName'] == operation.name:\n                message.ack()\n                job = dlp.get_dlp_job(request={'name': operation.name})\n                print(f'Job name: {job.name}')\n                if job.inspect_details.result.info_type_stats:\n                    for finding in job.inspect_details.result.info_type_stats:\n                        print(f'Info type: {finding.info_type.name}; Count: {finding.count}')\n                else:\n                    print('No findings.')\n                job_done.set()\n            else:\n                message.drop()\n        except Exception as e:\n            print(e)\n            raise\n    subscriber.subscribe(subscription_path, callback=callback)\n    finished = job_done.wait(timeout=timeout)\n    if not finished:\n        print('No event received before the timeout. Please verify that the subscription provided is subscribed to the topic provided.')",
        "mutated": [
            "def inspect_bigquery_table_with_sampling(project: str, topic_id: str, subscription_id: str, min_likelihood: str=None, max_findings: str=None, timeout: int=300) -> None:\n    if False:\n        i = 10\n    \"Uses the Data Loss Prevention API to analyze BigQuery data by limiting\\n    the amount of data to be scanned.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        topic_id: The id of the Cloud Pub/Sub topic to which the API will\\n            broadcast job completion. The topic must already exist.\\n        subscription_id: The id of the Cloud Pub/Sub subscription to listen on\\n            while waiting for job completion. The subscription must already\\n            exist and be subscribed to the topic.\\n        min_likelihood: A string representing the minimum likelihood threshold\\n            that constitutes a match. One of: 'LIKELIHOOD_UNSPECIFIED',\\n            'VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE', 'LIKELY', 'VERY_LIKELY'.\\n        max_findings: The maximum number of findings to report; 0 = no maximum.\\n        timeout: The number of seconds to wait for a response from the API.\\n    \"\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    inspect_config = {'info_types': [{'name': 'PERSON_NAME'}], 'min_likelihood': min_likelihood, 'limits': {'max_findings_per_request': max_findings}, 'include_quote': True}\n    table_reference = {'project_id': 'bigquery-public-data', 'dataset_id': 'usa_names', 'table_id': 'usa_1910_current'}\n    storage_config = {'big_query_options': {'table_reference': table_reference, 'rows_limit': 1000, 'sample_method': 'RANDOM_START', 'identifying_fields': [{'name': 'name'}]}}\n    topic = google.cloud.pubsub.PublisherClient.topic_path(project, topic_id)\n    actions = [{'pub_sub': {'topic': topic}}]\n    inspect_job = {'inspect_config': inspect_config, 'storage_config': storage_config, 'actions': actions}\n    parent = f'projects/{project}/locations/global'\n    operation = dlp.create_dlp_job(request={'parent': parent, 'inspect_job': inspect_job})\n    print(f'Inspection operation started: {operation.name}')\n    subscriber = google.cloud.pubsub.SubscriberClient()\n    subscription_path = subscriber.subscription_path(project, subscription_id)\n    job_done = threading.Event()\n\n    def callback(message: google.cloud.pubsub_v1.subscriber.message.Message) -> None:\n        try:\n            if message.attributes['DlpJobName'] == operation.name:\n                message.ack()\n                job = dlp.get_dlp_job(request={'name': operation.name})\n                print(f'Job name: {job.name}')\n                if job.inspect_details.result.info_type_stats:\n                    for finding in job.inspect_details.result.info_type_stats:\n                        print(f'Info type: {finding.info_type.name}; Count: {finding.count}')\n                else:\n                    print('No findings.')\n                job_done.set()\n            else:\n                message.drop()\n        except Exception as e:\n            print(e)\n            raise\n    subscriber.subscribe(subscription_path, callback=callback)\n    finished = job_done.wait(timeout=timeout)\n    if not finished:\n        print('No event received before the timeout. Please verify that the subscription provided is subscribed to the topic provided.')",
            "def inspect_bigquery_table_with_sampling(project: str, topic_id: str, subscription_id: str, min_likelihood: str=None, max_findings: str=None, timeout: int=300) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Uses the Data Loss Prevention API to analyze BigQuery data by limiting\\n    the amount of data to be scanned.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        topic_id: The id of the Cloud Pub/Sub topic to which the API will\\n            broadcast job completion. The topic must already exist.\\n        subscription_id: The id of the Cloud Pub/Sub subscription to listen on\\n            while waiting for job completion. The subscription must already\\n            exist and be subscribed to the topic.\\n        min_likelihood: A string representing the minimum likelihood threshold\\n            that constitutes a match. One of: 'LIKELIHOOD_UNSPECIFIED',\\n            'VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE', 'LIKELY', 'VERY_LIKELY'.\\n        max_findings: The maximum number of findings to report; 0 = no maximum.\\n        timeout: The number of seconds to wait for a response from the API.\\n    \"\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    inspect_config = {'info_types': [{'name': 'PERSON_NAME'}], 'min_likelihood': min_likelihood, 'limits': {'max_findings_per_request': max_findings}, 'include_quote': True}\n    table_reference = {'project_id': 'bigquery-public-data', 'dataset_id': 'usa_names', 'table_id': 'usa_1910_current'}\n    storage_config = {'big_query_options': {'table_reference': table_reference, 'rows_limit': 1000, 'sample_method': 'RANDOM_START', 'identifying_fields': [{'name': 'name'}]}}\n    topic = google.cloud.pubsub.PublisherClient.topic_path(project, topic_id)\n    actions = [{'pub_sub': {'topic': topic}}]\n    inspect_job = {'inspect_config': inspect_config, 'storage_config': storage_config, 'actions': actions}\n    parent = f'projects/{project}/locations/global'\n    operation = dlp.create_dlp_job(request={'parent': parent, 'inspect_job': inspect_job})\n    print(f'Inspection operation started: {operation.name}')\n    subscriber = google.cloud.pubsub.SubscriberClient()\n    subscription_path = subscriber.subscription_path(project, subscription_id)\n    job_done = threading.Event()\n\n    def callback(message: google.cloud.pubsub_v1.subscriber.message.Message) -> None:\n        try:\n            if message.attributes['DlpJobName'] == operation.name:\n                message.ack()\n                job = dlp.get_dlp_job(request={'name': operation.name})\n                print(f'Job name: {job.name}')\n                if job.inspect_details.result.info_type_stats:\n                    for finding in job.inspect_details.result.info_type_stats:\n                        print(f'Info type: {finding.info_type.name}; Count: {finding.count}')\n                else:\n                    print('No findings.')\n                job_done.set()\n            else:\n                message.drop()\n        except Exception as e:\n            print(e)\n            raise\n    subscriber.subscribe(subscription_path, callback=callback)\n    finished = job_done.wait(timeout=timeout)\n    if not finished:\n        print('No event received before the timeout. Please verify that the subscription provided is subscribed to the topic provided.')",
            "def inspect_bigquery_table_with_sampling(project: str, topic_id: str, subscription_id: str, min_likelihood: str=None, max_findings: str=None, timeout: int=300) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Uses the Data Loss Prevention API to analyze BigQuery data by limiting\\n    the amount of data to be scanned.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        topic_id: The id of the Cloud Pub/Sub topic to which the API will\\n            broadcast job completion. The topic must already exist.\\n        subscription_id: The id of the Cloud Pub/Sub subscription to listen on\\n            while waiting for job completion. The subscription must already\\n            exist and be subscribed to the topic.\\n        min_likelihood: A string representing the minimum likelihood threshold\\n            that constitutes a match. One of: 'LIKELIHOOD_UNSPECIFIED',\\n            'VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE', 'LIKELY', 'VERY_LIKELY'.\\n        max_findings: The maximum number of findings to report; 0 = no maximum.\\n        timeout: The number of seconds to wait for a response from the API.\\n    \"\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    inspect_config = {'info_types': [{'name': 'PERSON_NAME'}], 'min_likelihood': min_likelihood, 'limits': {'max_findings_per_request': max_findings}, 'include_quote': True}\n    table_reference = {'project_id': 'bigquery-public-data', 'dataset_id': 'usa_names', 'table_id': 'usa_1910_current'}\n    storage_config = {'big_query_options': {'table_reference': table_reference, 'rows_limit': 1000, 'sample_method': 'RANDOM_START', 'identifying_fields': [{'name': 'name'}]}}\n    topic = google.cloud.pubsub.PublisherClient.topic_path(project, topic_id)\n    actions = [{'pub_sub': {'topic': topic}}]\n    inspect_job = {'inspect_config': inspect_config, 'storage_config': storage_config, 'actions': actions}\n    parent = f'projects/{project}/locations/global'\n    operation = dlp.create_dlp_job(request={'parent': parent, 'inspect_job': inspect_job})\n    print(f'Inspection operation started: {operation.name}')\n    subscriber = google.cloud.pubsub.SubscriberClient()\n    subscription_path = subscriber.subscription_path(project, subscription_id)\n    job_done = threading.Event()\n\n    def callback(message: google.cloud.pubsub_v1.subscriber.message.Message) -> None:\n        try:\n            if message.attributes['DlpJobName'] == operation.name:\n                message.ack()\n                job = dlp.get_dlp_job(request={'name': operation.name})\n                print(f'Job name: {job.name}')\n                if job.inspect_details.result.info_type_stats:\n                    for finding in job.inspect_details.result.info_type_stats:\n                        print(f'Info type: {finding.info_type.name}; Count: {finding.count}')\n                else:\n                    print('No findings.')\n                job_done.set()\n            else:\n                message.drop()\n        except Exception as e:\n            print(e)\n            raise\n    subscriber.subscribe(subscription_path, callback=callback)\n    finished = job_done.wait(timeout=timeout)\n    if not finished:\n        print('No event received before the timeout. Please verify that the subscription provided is subscribed to the topic provided.')",
            "def inspect_bigquery_table_with_sampling(project: str, topic_id: str, subscription_id: str, min_likelihood: str=None, max_findings: str=None, timeout: int=300) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Uses the Data Loss Prevention API to analyze BigQuery data by limiting\\n    the amount of data to be scanned.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        topic_id: The id of the Cloud Pub/Sub topic to which the API will\\n            broadcast job completion. The topic must already exist.\\n        subscription_id: The id of the Cloud Pub/Sub subscription to listen on\\n            while waiting for job completion. The subscription must already\\n            exist and be subscribed to the topic.\\n        min_likelihood: A string representing the minimum likelihood threshold\\n            that constitutes a match. One of: 'LIKELIHOOD_UNSPECIFIED',\\n            'VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE', 'LIKELY', 'VERY_LIKELY'.\\n        max_findings: The maximum number of findings to report; 0 = no maximum.\\n        timeout: The number of seconds to wait for a response from the API.\\n    \"\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    inspect_config = {'info_types': [{'name': 'PERSON_NAME'}], 'min_likelihood': min_likelihood, 'limits': {'max_findings_per_request': max_findings}, 'include_quote': True}\n    table_reference = {'project_id': 'bigquery-public-data', 'dataset_id': 'usa_names', 'table_id': 'usa_1910_current'}\n    storage_config = {'big_query_options': {'table_reference': table_reference, 'rows_limit': 1000, 'sample_method': 'RANDOM_START', 'identifying_fields': [{'name': 'name'}]}}\n    topic = google.cloud.pubsub.PublisherClient.topic_path(project, topic_id)\n    actions = [{'pub_sub': {'topic': topic}}]\n    inspect_job = {'inspect_config': inspect_config, 'storage_config': storage_config, 'actions': actions}\n    parent = f'projects/{project}/locations/global'\n    operation = dlp.create_dlp_job(request={'parent': parent, 'inspect_job': inspect_job})\n    print(f'Inspection operation started: {operation.name}')\n    subscriber = google.cloud.pubsub.SubscriberClient()\n    subscription_path = subscriber.subscription_path(project, subscription_id)\n    job_done = threading.Event()\n\n    def callback(message: google.cloud.pubsub_v1.subscriber.message.Message) -> None:\n        try:\n            if message.attributes['DlpJobName'] == operation.name:\n                message.ack()\n                job = dlp.get_dlp_job(request={'name': operation.name})\n                print(f'Job name: {job.name}')\n                if job.inspect_details.result.info_type_stats:\n                    for finding in job.inspect_details.result.info_type_stats:\n                        print(f'Info type: {finding.info_type.name}; Count: {finding.count}')\n                else:\n                    print('No findings.')\n                job_done.set()\n            else:\n                message.drop()\n        except Exception as e:\n            print(e)\n            raise\n    subscriber.subscribe(subscription_path, callback=callback)\n    finished = job_done.wait(timeout=timeout)\n    if not finished:\n        print('No event received before the timeout. Please verify that the subscription provided is subscribed to the topic provided.')",
            "def inspect_bigquery_table_with_sampling(project: str, topic_id: str, subscription_id: str, min_likelihood: str=None, max_findings: str=None, timeout: int=300) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Uses the Data Loss Prevention API to analyze BigQuery data by limiting\\n    the amount of data to be scanned.\\n    Args:\\n        project: The Google Cloud project id to use as a parent resource.\\n        topic_id: The id of the Cloud Pub/Sub topic to which the API will\\n            broadcast job completion. The topic must already exist.\\n        subscription_id: The id of the Cloud Pub/Sub subscription to listen on\\n            while waiting for job completion. The subscription must already\\n            exist and be subscribed to the topic.\\n        min_likelihood: A string representing the minimum likelihood threshold\\n            that constitutes a match. One of: 'LIKELIHOOD_UNSPECIFIED',\\n            'VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE', 'LIKELY', 'VERY_LIKELY'.\\n        max_findings: The maximum number of findings to report; 0 = no maximum.\\n        timeout: The number of seconds to wait for a response from the API.\\n    \"\n    dlp = google.cloud.dlp_v2.DlpServiceClient()\n    inspect_config = {'info_types': [{'name': 'PERSON_NAME'}], 'min_likelihood': min_likelihood, 'limits': {'max_findings_per_request': max_findings}, 'include_quote': True}\n    table_reference = {'project_id': 'bigquery-public-data', 'dataset_id': 'usa_names', 'table_id': 'usa_1910_current'}\n    storage_config = {'big_query_options': {'table_reference': table_reference, 'rows_limit': 1000, 'sample_method': 'RANDOM_START', 'identifying_fields': [{'name': 'name'}]}}\n    topic = google.cloud.pubsub.PublisherClient.topic_path(project, topic_id)\n    actions = [{'pub_sub': {'topic': topic}}]\n    inspect_job = {'inspect_config': inspect_config, 'storage_config': storage_config, 'actions': actions}\n    parent = f'projects/{project}/locations/global'\n    operation = dlp.create_dlp_job(request={'parent': parent, 'inspect_job': inspect_job})\n    print(f'Inspection operation started: {operation.name}')\n    subscriber = google.cloud.pubsub.SubscriberClient()\n    subscription_path = subscriber.subscription_path(project, subscription_id)\n    job_done = threading.Event()\n\n    def callback(message: google.cloud.pubsub_v1.subscriber.message.Message) -> None:\n        try:\n            if message.attributes['DlpJobName'] == operation.name:\n                message.ack()\n                job = dlp.get_dlp_job(request={'name': operation.name})\n                print(f'Job name: {job.name}')\n                if job.inspect_details.result.info_type_stats:\n                    for finding in job.inspect_details.result.info_type_stats:\n                        print(f'Info type: {finding.info_type.name}; Count: {finding.count}')\n                else:\n                    print('No findings.')\n                job_done.set()\n            else:\n                message.drop()\n        except Exception as e:\n            print(e)\n            raise\n    subscriber.subscribe(subscription_path, callback=callback)\n    finished = job_done.wait(timeout=timeout)\n    if not finished:\n        print('No event received before the timeout. Please verify that the subscription provided is subscribed to the topic provided.')"
        ]
    }
]