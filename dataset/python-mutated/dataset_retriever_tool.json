[
    {
        "func_name": "from_dataset",
        "original": "@classmethod\ndef from_dataset(cls, dataset: Dataset, **kwargs):\n    description = dataset.description\n    if not description:\n        description = 'useful for when you want to answer queries about the ' + dataset.name\n    description = description.replace('\\n', '').replace('\\r', '')\n    return cls(name=f'dataset-{dataset.id}', tenant_id=dataset.tenant_id, dataset_id=dataset.id, description=description, **kwargs)",
        "mutated": [
            "@classmethod\ndef from_dataset(cls, dataset: Dataset, **kwargs):\n    if False:\n        i = 10\n    description = dataset.description\n    if not description:\n        description = 'useful for when you want to answer queries about the ' + dataset.name\n    description = description.replace('\\n', '').replace('\\r', '')\n    return cls(name=f'dataset-{dataset.id}', tenant_id=dataset.tenant_id, dataset_id=dataset.id, description=description, **kwargs)",
            "@classmethod\ndef from_dataset(cls, dataset: Dataset, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    description = dataset.description\n    if not description:\n        description = 'useful for when you want to answer queries about the ' + dataset.name\n    description = description.replace('\\n', '').replace('\\r', '')\n    return cls(name=f'dataset-{dataset.id}', tenant_id=dataset.tenant_id, dataset_id=dataset.id, description=description, **kwargs)",
            "@classmethod\ndef from_dataset(cls, dataset: Dataset, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    description = dataset.description\n    if not description:\n        description = 'useful for when you want to answer queries about the ' + dataset.name\n    description = description.replace('\\n', '').replace('\\r', '')\n    return cls(name=f'dataset-{dataset.id}', tenant_id=dataset.tenant_id, dataset_id=dataset.id, description=description, **kwargs)",
            "@classmethod\ndef from_dataset(cls, dataset: Dataset, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    description = dataset.description\n    if not description:\n        description = 'useful for when you want to answer queries about the ' + dataset.name\n    description = description.replace('\\n', '').replace('\\r', '')\n    return cls(name=f'dataset-{dataset.id}', tenant_id=dataset.tenant_id, dataset_id=dataset.id, description=description, **kwargs)",
            "@classmethod\ndef from_dataset(cls, dataset: Dataset, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    description = dataset.description\n    if not description:\n        description = 'useful for when you want to answer queries about the ' + dataset.name\n    description = description.replace('\\n', '').replace('\\r', '')\n    return cls(name=f'dataset-{dataset.id}', tenant_id=dataset.tenant_id, dataset_id=dataset.id, description=description, **kwargs)"
        ]
    },
    {
        "func_name": "_run",
        "original": "def _run(self, query: str) -> str:\n    dataset = db.session.query(Dataset).filter(Dataset.tenant_id == self.tenant_id, Dataset.id == self.dataset_id).first()\n    if not dataset:\n        return f'[{self.name} failed to find dataset with id {self.dataset_id}.]'\n    if dataset.indexing_technique == 'economy':\n        kw_table_index = KeywordTableIndex(dataset=dataset, config=KeywordTableConfig(max_keywords_per_chunk=5))\n        documents = kw_table_index.search(query, search_kwargs={'k': self.top_k})\n        return str('\\n'.join([document.page_content for document in documents]))\n    else:\n        try:\n            embedding_model = ModelFactory.get_embedding_model(tenant_id=dataset.tenant_id, model_provider_name=dataset.embedding_model_provider, model_name=dataset.embedding_model)\n        except LLMBadRequestError:\n            return ''\n        except ProviderTokenNotInitError:\n            return ''\n        embeddings = CacheEmbedding(embedding_model)\n        vector_index = VectorIndex(dataset=dataset, config=current_app.config, embeddings=embeddings)\n        if self.top_k > 0:\n            documents = vector_index.search(query, search_type='similarity_score_threshold', search_kwargs={'k': self.top_k, 'score_threshold': self.score_threshold, 'filter': {'group_id': [dataset.id]}})\n        else:\n            documents = []\n        hit_callback = DatasetIndexToolCallbackHandler(dataset.id, self.conversation_message_task)\n        hit_callback.on_tool_end(documents)\n        document_score_list = {}\n        if dataset.indexing_technique != 'economy':\n            for item in documents:\n                document_score_list[item.metadata['doc_id']] = item.metadata['score']\n        document_context_list = []\n        index_node_ids = [document.metadata['doc_id'] for document in documents]\n        segments = DocumentSegment.query.filter(DocumentSegment.dataset_id == self.dataset_id, DocumentSegment.completed_at.isnot(None), DocumentSegment.status == 'completed', DocumentSegment.enabled == True, DocumentSegment.index_node_id.in_(index_node_ids)).all()\n        if segments:\n            index_node_id_to_position = {id: position for (position, id) in enumerate(index_node_ids)}\n            sorted_segments = sorted(segments, key=lambda segment: index_node_id_to_position.get(segment.index_node_id, float('inf')))\n            for segment in sorted_segments:\n                if segment.answer:\n                    document_context_list.append(f'question:{segment.content} answer:{segment.answer}')\n                else:\n                    document_context_list.append(segment.content)\n            if self.return_resource:\n                context_list = []\n                resource_number = 1\n                for segment in sorted_segments:\n                    context = {}\n                    document = Document.query.filter(Document.id == segment.document_id, Document.enabled == True, Document.archived == False).first()\n                    if dataset and document:\n                        source = {'position': resource_number, 'dataset_id': dataset.id, 'dataset_name': dataset.name, 'document_id': document.id, 'document_name': document.name, 'data_source_type': document.data_source_type, 'segment_id': segment.id, 'retriever_from': self.retriever_from}\n                        if dataset.indexing_technique != 'economy':\n                            source['score'] = document_score_list.get(segment.index_node_id)\n                        if self.retriever_from == 'dev':\n                            source['hit_count'] = segment.hit_count\n                            source['word_count'] = segment.word_count\n                            source['segment_position'] = segment.position\n                            source['index_node_hash'] = segment.index_node_hash\n                        if segment.answer:\n                            source['content'] = f'question:{segment.content} \\nanswer:{segment.answer}'\n                        else:\n                            source['content'] = segment.content\n                        context_list.append(source)\n                    resource_number += 1\n                hit_callback.return_retriever_resource_info(context_list)\n        return str('\\n'.join(document_context_list))",
        "mutated": [
            "def _run(self, query: str) -> str:\n    if False:\n        i = 10\n    dataset = db.session.query(Dataset).filter(Dataset.tenant_id == self.tenant_id, Dataset.id == self.dataset_id).first()\n    if not dataset:\n        return f'[{self.name} failed to find dataset with id {self.dataset_id}.]'\n    if dataset.indexing_technique == 'economy':\n        kw_table_index = KeywordTableIndex(dataset=dataset, config=KeywordTableConfig(max_keywords_per_chunk=5))\n        documents = kw_table_index.search(query, search_kwargs={'k': self.top_k})\n        return str('\\n'.join([document.page_content for document in documents]))\n    else:\n        try:\n            embedding_model = ModelFactory.get_embedding_model(tenant_id=dataset.tenant_id, model_provider_name=dataset.embedding_model_provider, model_name=dataset.embedding_model)\n        except LLMBadRequestError:\n            return ''\n        except ProviderTokenNotInitError:\n            return ''\n        embeddings = CacheEmbedding(embedding_model)\n        vector_index = VectorIndex(dataset=dataset, config=current_app.config, embeddings=embeddings)\n        if self.top_k > 0:\n            documents = vector_index.search(query, search_type='similarity_score_threshold', search_kwargs={'k': self.top_k, 'score_threshold': self.score_threshold, 'filter': {'group_id': [dataset.id]}})\n        else:\n            documents = []\n        hit_callback = DatasetIndexToolCallbackHandler(dataset.id, self.conversation_message_task)\n        hit_callback.on_tool_end(documents)\n        document_score_list = {}\n        if dataset.indexing_technique != 'economy':\n            for item in documents:\n                document_score_list[item.metadata['doc_id']] = item.metadata['score']\n        document_context_list = []\n        index_node_ids = [document.metadata['doc_id'] for document in documents]\n        segments = DocumentSegment.query.filter(DocumentSegment.dataset_id == self.dataset_id, DocumentSegment.completed_at.isnot(None), DocumentSegment.status == 'completed', DocumentSegment.enabled == True, DocumentSegment.index_node_id.in_(index_node_ids)).all()\n        if segments:\n            index_node_id_to_position = {id: position for (position, id) in enumerate(index_node_ids)}\n            sorted_segments = sorted(segments, key=lambda segment: index_node_id_to_position.get(segment.index_node_id, float('inf')))\n            for segment in sorted_segments:\n                if segment.answer:\n                    document_context_list.append(f'question:{segment.content} answer:{segment.answer}')\n                else:\n                    document_context_list.append(segment.content)\n            if self.return_resource:\n                context_list = []\n                resource_number = 1\n                for segment in sorted_segments:\n                    context = {}\n                    document = Document.query.filter(Document.id == segment.document_id, Document.enabled == True, Document.archived == False).first()\n                    if dataset and document:\n                        source = {'position': resource_number, 'dataset_id': dataset.id, 'dataset_name': dataset.name, 'document_id': document.id, 'document_name': document.name, 'data_source_type': document.data_source_type, 'segment_id': segment.id, 'retriever_from': self.retriever_from}\n                        if dataset.indexing_technique != 'economy':\n                            source['score'] = document_score_list.get(segment.index_node_id)\n                        if self.retriever_from == 'dev':\n                            source['hit_count'] = segment.hit_count\n                            source['word_count'] = segment.word_count\n                            source['segment_position'] = segment.position\n                            source['index_node_hash'] = segment.index_node_hash\n                        if segment.answer:\n                            source['content'] = f'question:{segment.content} \\nanswer:{segment.answer}'\n                        else:\n                            source['content'] = segment.content\n                        context_list.append(source)\n                    resource_number += 1\n                hit_callback.return_retriever_resource_info(context_list)\n        return str('\\n'.join(document_context_list))",
            "def _run(self, query: str) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    dataset = db.session.query(Dataset).filter(Dataset.tenant_id == self.tenant_id, Dataset.id == self.dataset_id).first()\n    if not dataset:\n        return f'[{self.name} failed to find dataset with id {self.dataset_id}.]'\n    if dataset.indexing_technique == 'economy':\n        kw_table_index = KeywordTableIndex(dataset=dataset, config=KeywordTableConfig(max_keywords_per_chunk=5))\n        documents = kw_table_index.search(query, search_kwargs={'k': self.top_k})\n        return str('\\n'.join([document.page_content for document in documents]))\n    else:\n        try:\n            embedding_model = ModelFactory.get_embedding_model(tenant_id=dataset.tenant_id, model_provider_name=dataset.embedding_model_provider, model_name=dataset.embedding_model)\n        except LLMBadRequestError:\n            return ''\n        except ProviderTokenNotInitError:\n            return ''\n        embeddings = CacheEmbedding(embedding_model)\n        vector_index = VectorIndex(dataset=dataset, config=current_app.config, embeddings=embeddings)\n        if self.top_k > 0:\n            documents = vector_index.search(query, search_type='similarity_score_threshold', search_kwargs={'k': self.top_k, 'score_threshold': self.score_threshold, 'filter': {'group_id': [dataset.id]}})\n        else:\n            documents = []\n        hit_callback = DatasetIndexToolCallbackHandler(dataset.id, self.conversation_message_task)\n        hit_callback.on_tool_end(documents)\n        document_score_list = {}\n        if dataset.indexing_technique != 'economy':\n            for item in documents:\n                document_score_list[item.metadata['doc_id']] = item.metadata['score']\n        document_context_list = []\n        index_node_ids = [document.metadata['doc_id'] for document in documents]\n        segments = DocumentSegment.query.filter(DocumentSegment.dataset_id == self.dataset_id, DocumentSegment.completed_at.isnot(None), DocumentSegment.status == 'completed', DocumentSegment.enabled == True, DocumentSegment.index_node_id.in_(index_node_ids)).all()\n        if segments:\n            index_node_id_to_position = {id: position for (position, id) in enumerate(index_node_ids)}\n            sorted_segments = sorted(segments, key=lambda segment: index_node_id_to_position.get(segment.index_node_id, float('inf')))\n            for segment in sorted_segments:\n                if segment.answer:\n                    document_context_list.append(f'question:{segment.content} answer:{segment.answer}')\n                else:\n                    document_context_list.append(segment.content)\n            if self.return_resource:\n                context_list = []\n                resource_number = 1\n                for segment in sorted_segments:\n                    context = {}\n                    document = Document.query.filter(Document.id == segment.document_id, Document.enabled == True, Document.archived == False).first()\n                    if dataset and document:\n                        source = {'position': resource_number, 'dataset_id': dataset.id, 'dataset_name': dataset.name, 'document_id': document.id, 'document_name': document.name, 'data_source_type': document.data_source_type, 'segment_id': segment.id, 'retriever_from': self.retriever_from}\n                        if dataset.indexing_technique != 'economy':\n                            source['score'] = document_score_list.get(segment.index_node_id)\n                        if self.retriever_from == 'dev':\n                            source['hit_count'] = segment.hit_count\n                            source['word_count'] = segment.word_count\n                            source['segment_position'] = segment.position\n                            source['index_node_hash'] = segment.index_node_hash\n                        if segment.answer:\n                            source['content'] = f'question:{segment.content} \\nanswer:{segment.answer}'\n                        else:\n                            source['content'] = segment.content\n                        context_list.append(source)\n                    resource_number += 1\n                hit_callback.return_retriever_resource_info(context_list)\n        return str('\\n'.join(document_context_list))",
            "def _run(self, query: str) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    dataset = db.session.query(Dataset).filter(Dataset.tenant_id == self.tenant_id, Dataset.id == self.dataset_id).first()\n    if not dataset:\n        return f'[{self.name} failed to find dataset with id {self.dataset_id}.]'\n    if dataset.indexing_technique == 'economy':\n        kw_table_index = KeywordTableIndex(dataset=dataset, config=KeywordTableConfig(max_keywords_per_chunk=5))\n        documents = kw_table_index.search(query, search_kwargs={'k': self.top_k})\n        return str('\\n'.join([document.page_content for document in documents]))\n    else:\n        try:\n            embedding_model = ModelFactory.get_embedding_model(tenant_id=dataset.tenant_id, model_provider_name=dataset.embedding_model_provider, model_name=dataset.embedding_model)\n        except LLMBadRequestError:\n            return ''\n        except ProviderTokenNotInitError:\n            return ''\n        embeddings = CacheEmbedding(embedding_model)\n        vector_index = VectorIndex(dataset=dataset, config=current_app.config, embeddings=embeddings)\n        if self.top_k > 0:\n            documents = vector_index.search(query, search_type='similarity_score_threshold', search_kwargs={'k': self.top_k, 'score_threshold': self.score_threshold, 'filter': {'group_id': [dataset.id]}})\n        else:\n            documents = []\n        hit_callback = DatasetIndexToolCallbackHandler(dataset.id, self.conversation_message_task)\n        hit_callback.on_tool_end(documents)\n        document_score_list = {}\n        if dataset.indexing_technique != 'economy':\n            for item in documents:\n                document_score_list[item.metadata['doc_id']] = item.metadata['score']\n        document_context_list = []\n        index_node_ids = [document.metadata['doc_id'] for document in documents]\n        segments = DocumentSegment.query.filter(DocumentSegment.dataset_id == self.dataset_id, DocumentSegment.completed_at.isnot(None), DocumentSegment.status == 'completed', DocumentSegment.enabled == True, DocumentSegment.index_node_id.in_(index_node_ids)).all()\n        if segments:\n            index_node_id_to_position = {id: position for (position, id) in enumerate(index_node_ids)}\n            sorted_segments = sorted(segments, key=lambda segment: index_node_id_to_position.get(segment.index_node_id, float('inf')))\n            for segment in sorted_segments:\n                if segment.answer:\n                    document_context_list.append(f'question:{segment.content} answer:{segment.answer}')\n                else:\n                    document_context_list.append(segment.content)\n            if self.return_resource:\n                context_list = []\n                resource_number = 1\n                for segment in sorted_segments:\n                    context = {}\n                    document = Document.query.filter(Document.id == segment.document_id, Document.enabled == True, Document.archived == False).first()\n                    if dataset and document:\n                        source = {'position': resource_number, 'dataset_id': dataset.id, 'dataset_name': dataset.name, 'document_id': document.id, 'document_name': document.name, 'data_source_type': document.data_source_type, 'segment_id': segment.id, 'retriever_from': self.retriever_from}\n                        if dataset.indexing_technique != 'economy':\n                            source['score'] = document_score_list.get(segment.index_node_id)\n                        if self.retriever_from == 'dev':\n                            source['hit_count'] = segment.hit_count\n                            source['word_count'] = segment.word_count\n                            source['segment_position'] = segment.position\n                            source['index_node_hash'] = segment.index_node_hash\n                        if segment.answer:\n                            source['content'] = f'question:{segment.content} \\nanswer:{segment.answer}'\n                        else:\n                            source['content'] = segment.content\n                        context_list.append(source)\n                    resource_number += 1\n                hit_callback.return_retriever_resource_info(context_list)\n        return str('\\n'.join(document_context_list))",
            "def _run(self, query: str) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    dataset = db.session.query(Dataset).filter(Dataset.tenant_id == self.tenant_id, Dataset.id == self.dataset_id).first()\n    if not dataset:\n        return f'[{self.name} failed to find dataset with id {self.dataset_id}.]'\n    if dataset.indexing_technique == 'economy':\n        kw_table_index = KeywordTableIndex(dataset=dataset, config=KeywordTableConfig(max_keywords_per_chunk=5))\n        documents = kw_table_index.search(query, search_kwargs={'k': self.top_k})\n        return str('\\n'.join([document.page_content for document in documents]))\n    else:\n        try:\n            embedding_model = ModelFactory.get_embedding_model(tenant_id=dataset.tenant_id, model_provider_name=dataset.embedding_model_provider, model_name=dataset.embedding_model)\n        except LLMBadRequestError:\n            return ''\n        except ProviderTokenNotInitError:\n            return ''\n        embeddings = CacheEmbedding(embedding_model)\n        vector_index = VectorIndex(dataset=dataset, config=current_app.config, embeddings=embeddings)\n        if self.top_k > 0:\n            documents = vector_index.search(query, search_type='similarity_score_threshold', search_kwargs={'k': self.top_k, 'score_threshold': self.score_threshold, 'filter': {'group_id': [dataset.id]}})\n        else:\n            documents = []\n        hit_callback = DatasetIndexToolCallbackHandler(dataset.id, self.conversation_message_task)\n        hit_callback.on_tool_end(documents)\n        document_score_list = {}\n        if dataset.indexing_technique != 'economy':\n            for item in documents:\n                document_score_list[item.metadata['doc_id']] = item.metadata['score']\n        document_context_list = []\n        index_node_ids = [document.metadata['doc_id'] for document in documents]\n        segments = DocumentSegment.query.filter(DocumentSegment.dataset_id == self.dataset_id, DocumentSegment.completed_at.isnot(None), DocumentSegment.status == 'completed', DocumentSegment.enabled == True, DocumentSegment.index_node_id.in_(index_node_ids)).all()\n        if segments:\n            index_node_id_to_position = {id: position for (position, id) in enumerate(index_node_ids)}\n            sorted_segments = sorted(segments, key=lambda segment: index_node_id_to_position.get(segment.index_node_id, float('inf')))\n            for segment in sorted_segments:\n                if segment.answer:\n                    document_context_list.append(f'question:{segment.content} answer:{segment.answer}')\n                else:\n                    document_context_list.append(segment.content)\n            if self.return_resource:\n                context_list = []\n                resource_number = 1\n                for segment in sorted_segments:\n                    context = {}\n                    document = Document.query.filter(Document.id == segment.document_id, Document.enabled == True, Document.archived == False).first()\n                    if dataset and document:\n                        source = {'position': resource_number, 'dataset_id': dataset.id, 'dataset_name': dataset.name, 'document_id': document.id, 'document_name': document.name, 'data_source_type': document.data_source_type, 'segment_id': segment.id, 'retriever_from': self.retriever_from}\n                        if dataset.indexing_technique != 'economy':\n                            source['score'] = document_score_list.get(segment.index_node_id)\n                        if self.retriever_from == 'dev':\n                            source['hit_count'] = segment.hit_count\n                            source['word_count'] = segment.word_count\n                            source['segment_position'] = segment.position\n                            source['index_node_hash'] = segment.index_node_hash\n                        if segment.answer:\n                            source['content'] = f'question:{segment.content} \\nanswer:{segment.answer}'\n                        else:\n                            source['content'] = segment.content\n                        context_list.append(source)\n                    resource_number += 1\n                hit_callback.return_retriever_resource_info(context_list)\n        return str('\\n'.join(document_context_list))",
            "def _run(self, query: str) -> str:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    dataset = db.session.query(Dataset).filter(Dataset.tenant_id == self.tenant_id, Dataset.id == self.dataset_id).first()\n    if not dataset:\n        return f'[{self.name} failed to find dataset with id {self.dataset_id}.]'\n    if dataset.indexing_technique == 'economy':\n        kw_table_index = KeywordTableIndex(dataset=dataset, config=KeywordTableConfig(max_keywords_per_chunk=5))\n        documents = kw_table_index.search(query, search_kwargs={'k': self.top_k})\n        return str('\\n'.join([document.page_content for document in documents]))\n    else:\n        try:\n            embedding_model = ModelFactory.get_embedding_model(tenant_id=dataset.tenant_id, model_provider_name=dataset.embedding_model_provider, model_name=dataset.embedding_model)\n        except LLMBadRequestError:\n            return ''\n        except ProviderTokenNotInitError:\n            return ''\n        embeddings = CacheEmbedding(embedding_model)\n        vector_index = VectorIndex(dataset=dataset, config=current_app.config, embeddings=embeddings)\n        if self.top_k > 0:\n            documents = vector_index.search(query, search_type='similarity_score_threshold', search_kwargs={'k': self.top_k, 'score_threshold': self.score_threshold, 'filter': {'group_id': [dataset.id]}})\n        else:\n            documents = []\n        hit_callback = DatasetIndexToolCallbackHandler(dataset.id, self.conversation_message_task)\n        hit_callback.on_tool_end(documents)\n        document_score_list = {}\n        if dataset.indexing_technique != 'economy':\n            for item in documents:\n                document_score_list[item.metadata['doc_id']] = item.metadata['score']\n        document_context_list = []\n        index_node_ids = [document.metadata['doc_id'] for document in documents]\n        segments = DocumentSegment.query.filter(DocumentSegment.dataset_id == self.dataset_id, DocumentSegment.completed_at.isnot(None), DocumentSegment.status == 'completed', DocumentSegment.enabled == True, DocumentSegment.index_node_id.in_(index_node_ids)).all()\n        if segments:\n            index_node_id_to_position = {id: position for (position, id) in enumerate(index_node_ids)}\n            sorted_segments = sorted(segments, key=lambda segment: index_node_id_to_position.get(segment.index_node_id, float('inf')))\n            for segment in sorted_segments:\n                if segment.answer:\n                    document_context_list.append(f'question:{segment.content} answer:{segment.answer}')\n                else:\n                    document_context_list.append(segment.content)\n            if self.return_resource:\n                context_list = []\n                resource_number = 1\n                for segment in sorted_segments:\n                    context = {}\n                    document = Document.query.filter(Document.id == segment.document_id, Document.enabled == True, Document.archived == False).first()\n                    if dataset and document:\n                        source = {'position': resource_number, 'dataset_id': dataset.id, 'dataset_name': dataset.name, 'document_id': document.id, 'document_name': document.name, 'data_source_type': document.data_source_type, 'segment_id': segment.id, 'retriever_from': self.retriever_from}\n                        if dataset.indexing_technique != 'economy':\n                            source['score'] = document_score_list.get(segment.index_node_id)\n                        if self.retriever_from == 'dev':\n                            source['hit_count'] = segment.hit_count\n                            source['word_count'] = segment.word_count\n                            source['segment_position'] = segment.position\n                            source['index_node_hash'] = segment.index_node_hash\n                        if segment.answer:\n                            source['content'] = f'question:{segment.content} \\nanswer:{segment.answer}'\n                        else:\n                            source['content'] = segment.content\n                        context_list.append(source)\n                    resource_number += 1\n                hit_callback.return_retriever_resource_info(context_list)\n        return str('\\n'.join(document_context_list))"
        ]
    }
]