[
    {
        "func_name": "default_model",
        "original": "def default_model(self) -> Tuple[str, List[str]]:\n    \"\"\"\n        Overview:\n            Return this algorithm default model setting for demonstration.\n        Returns:\n            - model_info (:obj:`Tuple[str, List[str]]`): model name and mode import_names\n        .. note::\n            The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path. For WQMIX, ``ding.model.template.wqmix``\n        \"\"\"\n    return ('wqmix', ['ding.model.template.wqmix'])",
        "mutated": [
            "def default_model(self) -> Tuple[str, List[str]]:\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Return this algorithm default model setting for demonstration.\\n        Returns:\\n            - model_info (:obj:`Tuple[str, List[str]]`): model name and mode import_names\\n        .. note::\\n            The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path. For WQMIX, ``ding.model.template.wqmix``\\n        '\n    return ('wqmix', ['ding.model.template.wqmix'])",
            "def default_model(self) -> Tuple[str, List[str]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Return this algorithm default model setting for demonstration.\\n        Returns:\\n            - model_info (:obj:`Tuple[str, List[str]]`): model name and mode import_names\\n        .. note::\\n            The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path. For WQMIX, ``ding.model.template.wqmix``\\n        '\n    return ('wqmix', ['ding.model.template.wqmix'])",
            "def default_model(self) -> Tuple[str, List[str]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Return this algorithm default model setting for demonstration.\\n        Returns:\\n            - model_info (:obj:`Tuple[str, List[str]]`): model name and mode import_names\\n        .. note::\\n            The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path. For WQMIX, ``ding.model.template.wqmix``\\n        '\n    return ('wqmix', ['ding.model.template.wqmix'])",
            "def default_model(self) -> Tuple[str, List[str]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Return this algorithm default model setting for demonstration.\\n        Returns:\\n            - model_info (:obj:`Tuple[str, List[str]]`): model name and mode import_names\\n        .. note::\\n            The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path. For WQMIX, ``ding.model.template.wqmix``\\n        '\n    return ('wqmix', ['ding.model.template.wqmix'])",
            "def default_model(self) -> Tuple[str, List[str]]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Return this algorithm default model setting for demonstration.\\n        Returns:\\n            - model_info (:obj:`Tuple[str, List[str]]`): model name and mode import_names\\n        .. note::\\n            The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path. For WQMIX, ``ding.model.template.wqmix``\\n        '\n    return ('wqmix', ['ding.model.template.wqmix'])"
        ]
    },
    {
        "func_name": "_init_learn",
        "original": "def _init_learn(self) -> None:\n    \"\"\"\n        Overview:\n            Learn mode init method. Called by ``self.__init__``.\n            Init the learner model of WQMIXPolicy\n        Arguments:\n            .. note::\n\n                The _init_learn method takes the argument from the self._cfg.learn in the config file\n\n            - learning_rate (:obj:`float`): The learning rate fo the optimizer\n            - gamma (:obj:`float`): The discount factor\n            - agent_num (:obj:`int`): Since this is a multi-agent algorithm, we need to input the agent num.\n            - batch_size (:obj:`int`): Need batch size info to init hidden_state plugins\n        \"\"\"\n    self._priority = self._cfg.priority\n    self._priority_IS_weight = self._cfg.priority_IS_weight\n    assert not self._priority and (not self._priority_IS_weight), 'Priority is not implemented in WQMIX'\n    self._optimizer = RMSprop(params=list(self._model._q_network.parameters()) + list(self._model._mixer.parameters()), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=1e-05)\n    self._gamma = self._cfg.learn.discount_factor\n    self._optimizer_star = RMSprop(params=list(self._model._q_network_star.parameters()) + list(self._model._mixer_star.parameters()), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=1e-05)\n    self._learn_model = model_wrap(self._model, wrapper_name='hidden_state', state_num=self._cfg.learn.batch_size, init_fn=lambda : [None for _ in range(self._cfg.model.agent_num)])\n    self._learn_model.reset()",
        "mutated": [
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Learn mode init method. Called by ``self.__init__``.\\n            Init the learner model of WQMIXPolicy\\n        Arguments:\\n            .. note::\\n\\n                The _init_learn method takes the argument from the self._cfg.learn in the config file\\n\\n            - learning_rate (:obj:`float`): The learning rate fo the optimizer\\n            - gamma (:obj:`float`): The discount factor\\n            - agent_num (:obj:`int`): Since this is a multi-agent algorithm, we need to input the agent num.\\n            - batch_size (:obj:`int`): Need batch size info to init hidden_state plugins\\n        '\n    self._priority = self._cfg.priority\n    self._priority_IS_weight = self._cfg.priority_IS_weight\n    assert not self._priority and (not self._priority_IS_weight), 'Priority is not implemented in WQMIX'\n    self._optimizer = RMSprop(params=list(self._model._q_network.parameters()) + list(self._model._mixer.parameters()), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=1e-05)\n    self._gamma = self._cfg.learn.discount_factor\n    self._optimizer_star = RMSprop(params=list(self._model._q_network_star.parameters()) + list(self._model._mixer_star.parameters()), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=1e-05)\n    self._learn_model = model_wrap(self._model, wrapper_name='hidden_state', state_num=self._cfg.learn.batch_size, init_fn=lambda : [None for _ in range(self._cfg.model.agent_num)])\n    self._learn_model.reset()",
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Learn mode init method. Called by ``self.__init__``.\\n            Init the learner model of WQMIXPolicy\\n        Arguments:\\n            .. note::\\n\\n                The _init_learn method takes the argument from the self._cfg.learn in the config file\\n\\n            - learning_rate (:obj:`float`): The learning rate fo the optimizer\\n            - gamma (:obj:`float`): The discount factor\\n            - agent_num (:obj:`int`): Since this is a multi-agent algorithm, we need to input the agent num.\\n            - batch_size (:obj:`int`): Need batch size info to init hidden_state plugins\\n        '\n    self._priority = self._cfg.priority\n    self._priority_IS_weight = self._cfg.priority_IS_weight\n    assert not self._priority and (not self._priority_IS_weight), 'Priority is not implemented in WQMIX'\n    self._optimizer = RMSprop(params=list(self._model._q_network.parameters()) + list(self._model._mixer.parameters()), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=1e-05)\n    self._gamma = self._cfg.learn.discount_factor\n    self._optimizer_star = RMSprop(params=list(self._model._q_network_star.parameters()) + list(self._model._mixer_star.parameters()), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=1e-05)\n    self._learn_model = model_wrap(self._model, wrapper_name='hidden_state', state_num=self._cfg.learn.batch_size, init_fn=lambda : [None for _ in range(self._cfg.model.agent_num)])\n    self._learn_model.reset()",
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Learn mode init method. Called by ``self.__init__``.\\n            Init the learner model of WQMIXPolicy\\n        Arguments:\\n            .. note::\\n\\n                The _init_learn method takes the argument from the self._cfg.learn in the config file\\n\\n            - learning_rate (:obj:`float`): The learning rate fo the optimizer\\n            - gamma (:obj:`float`): The discount factor\\n            - agent_num (:obj:`int`): Since this is a multi-agent algorithm, we need to input the agent num.\\n            - batch_size (:obj:`int`): Need batch size info to init hidden_state plugins\\n        '\n    self._priority = self._cfg.priority\n    self._priority_IS_weight = self._cfg.priority_IS_weight\n    assert not self._priority and (not self._priority_IS_weight), 'Priority is not implemented in WQMIX'\n    self._optimizer = RMSprop(params=list(self._model._q_network.parameters()) + list(self._model._mixer.parameters()), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=1e-05)\n    self._gamma = self._cfg.learn.discount_factor\n    self._optimizer_star = RMSprop(params=list(self._model._q_network_star.parameters()) + list(self._model._mixer_star.parameters()), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=1e-05)\n    self._learn_model = model_wrap(self._model, wrapper_name='hidden_state', state_num=self._cfg.learn.batch_size, init_fn=lambda : [None for _ in range(self._cfg.model.agent_num)])\n    self._learn_model.reset()",
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Learn mode init method. Called by ``self.__init__``.\\n            Init the learner model of WQMIXPolicy\\n        Arguments:\\n            .. note::\\n\\n                The _init_learn method takes the argument from the self._cfg.learn in the config file\\n\\n            - learning_rate (:obj:`float`): The learning rate fo the optimizer\\n            - gamma (:obj:`float`): The discount factor\\n            - agent_num (:obj:`int`): Since this is a multi-agent algorithm, we need to input the agent num.\\n            - batch_size (:obj:`int`): Need batch size info to init hidden_state plugins\\n        '\n    self._priority = self._cfg.priority\n    self._priority_IS_weight = self._cfg.priority_IS_weight\n    assert not self._priority and (not self._priority_IS_weight), 'Priority is not implemented in WQMIX'\n    self._optimizer = RMSprop(params=list(self._model._q_network.parameters()) + list(self._model._mixer.parameters()), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=1e-05)\n    self._gamma = self._cfg.learn.discount_factor\n    self._optimizer_star = RMSprop(params=list(self._model._q_network_star.parameters()) + list(self._model._mixer_star.parameters()), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=1e-05)\n    self._learn_model = model_wrap(self._model, wrapper_name='hidden_state', state_num=self._cfg.learn.batch_size, init_fn=lambda : [None for _ in range(self._cfg.model.agent_num)])\n    self._learn_model.reset()",
            "def _init_learn(self) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Learn mode init method. Called by ``self.__init__``.\\n            Init the learner model of WQMIXPolicy\\n        Arguments:\\n            .. note::\\n\\n                The _init_learn method takes the argument from the self._cfg.learn in the config file\\n\\n            - learning_rate (:obj:`float`): The learning rate fo the optimizer\\n            - gamma (:obj:`float`): The discount factor\\n            - agent_num (:obj:`int`): Since this is a multi-agent algorithm, we need to input the agent num.\\n            - batch_size (:obj:`int`): Need batch size info to init hidden_state plugins\\n        '\n    self._priority = self._cfg.priority\n    self._priority_IS_weight = self._cfg.priority_IS_weight\n    assert not self._priority and (not self._priority_IS_weight), 'Priority is not implemented in WQMIX'\n    self._optimizer = RMSprop(params=list(self._model._q_network.parameters()) + list(self._model._mixer.parameters()), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=1e-05)\n    self._gamma = self._cfg.learn.discount_factor\n    self._optimizer_star = RMSprop(params=list(self._model._q_network_star.parameters()) + list(self._model._mixer_star.parameters()), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=1e-05)\n    self._learn_model = model_wrap(self._model, wrapper_name='hidden_state', state_num=self._cfg.learn.batch_size, init_fn=lambda : [None for _ in range(self._cfg.model.agent_num)])\n    self._learn_model.reset()"
        ]
    },
    {
        "func_name": "_data_preprocess_learn",
        "original": "def _data_preprocess_learn(self, data: List[Any]) -> dict:\n    \"\"\"\n        Overview:\n            Preprocess the data to fit the required data format for learning\n        Arguments:\n            - data (:obj:`List[Dict[str, Any]]`): the data collected from collect function\n        Returns:\n            - data (:obj:`Dict[str, Any]`): the processed data, from \\\\\n                [len=B, ele={dict_key: [len=T, ele=Tensor(any_dims)]}] -> {dict_key: Tensor([T, B, any_dims])}\n        \"\"\"\n    data = timestep_collate(data)\n    if self._cuda:\n        data = to_device(data, self._device)\n    data['weight'] = data.get('weight', None)\n    data['done'] = data['done'].float()\n    return data",
        "mutated": [
            "def _data_preprocess_learn(self, data: List[Any]) -> dict:\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Preprocess the data to fit the required data format for learning\\n        Arguments:\\n            - data (:obj:`List[Dict[str, Any]]`): the data collected from collect function\\n        Returns:\\n            - data (:obj:`Dict[str, Any]`): the processed data, from \\\\\\n                [len=B, ele={dict_key: [len=T, ele=Tensor(any_dims)]}] -> {dict_key: Tensor([T, B, any_dims])}\\n        '\n    data = timestep_collate(data)\n    if self._cuda:\n        data = to_device(data, self._device)\n    data['weight'] = data.get('weight', None)\n    data['done'] = data['done'].float()\n    return data",
            "def _data_preprocess_learn(self, data: List[Any]) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Preprocess the data to fit the required data format for learning\\n        Arguments:\\n            - data (:obj:`List[Dict[str, Any]]`): the data collected from collect function\\n        Returns:\\n            - data (:obj:`Dict[str, Any]`): the processed data, from \\\\\\n                [len=B, ele={dict_key: [len=T, ele=Tensor(any_dims)]}] -> {dict_key: Tensor([T, B, any_dims])}\\n        '\n    data = timestep_collate(data)\n    if self._cuda:\n        data = to_device(data, self._device)\n    data['weight'] = data.get('weight', None)\n    data['done'] = data['done'].float()\n    return data",
            "def _data_preprocess_learn(self, data: List[Any]) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Preprocess the data to fit the required data format for learning\\n        Arguments:\\n            - data (:obj:`List[Dict[str, Any]]`): the data collected from collect function\\n        Returns:\\n            - data (:obj:`Dict[str, Any]`): the processed data, from \\\\\\n                [len=B, ele={dict_key: [len=T, ele=Tensor(any_dims)]}] -> {dict_key: Tensor([T, B, any_dims])}\\n        '\n    data = timestep_collate(data)\n    if self._cuda:\n        data = to_device(data, self._device)\n    data['weight'] = data.get('weight', None)\n    data['done'] = data['done'].float()\n    return data",
            "def _data_preprocess_learn(self, data: List[Any]) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Preprocess the data to fit the required data format for learning\\n        Arguments:\\n            - data (:obj:`List[Dict[str, Any]]`): the data collected from collect function\\n        Returns:\\n            - data (:obj:`Dict[str, Any]`): the processed data, from \\\\\\n                [len=B, ele={dict_key: [len=T, ele=Tensor(any_dims)]}] -> {dict_key: Tensor([T, B, any_dims])}\\n        '\n    data = timestep_collate(data)\n    if self._cuda:\n        data = to_device(data, self._device)\n    data['weight'] = data.get('weight', None)\n    data['done'] = data['done'].float()\n    return data",
            "def _data_preprocess_learn(self, data: List[Any]) -> dict:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Preprocess the data to fit the required data format for learning\\n        Arguments:\\n            - data (:obj:`List[Dict[str, Any]]`): the data collected from collect function\\n        Returns:\\n            - data (:obj:`Dict[str, Any]`): the processed data, from \\\\\\n                [len=B, ele={dict_key: [len=T, ele=Tensor(any_dims)]}] -> {dict_key: Tensor([T, B, any_dims])}\\n        '\n    data = timestep_collate(data)\n    if self._cuda:\n        data = to_device(data, self._device)\n    data['weight'] = data.get('weight', None)\n    data['done'] = data['done'].float()\n    return data"
        ]
    },
    {
        "func_name": "_forward_learn",
        "original": "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    \"\"\"\n        Overview:\n            Forward and backward function of learn mode.\n        Arguments:\n            - data (:obj:`Dict[str, Any]`): Dict type data, a batch of data for training, values are torch.Tensor or \\\\\n                np.ndarray or dict/list combinations.\n        Returns:\n            - info_dict (:obj:`Dict[str, Any]`): Dict type data, a info dict indicated training result, which will be \\\\\n                recorded in text log and tensorboard, values are python scalar or a list of scalars.\n        ArgumentsKeys:\n            - necessary: ``obs``, ``next_obs``, ``action``, ``reward``, ``weight``, ``prev_state``, ``done``\n        ReturnsKeys:\n            - necessary: ``cur_lr``, ``total_loss``\n                - cur_lr (:obj:`float`): Current learning rate\n                - total_loss (:obj:`float`): The calculated loss\n        \"\"\"\n    data = self._data_preprocess_learn(data)\n    self._learn_model.train()\n    inputs = {'obs': data['obs'], 'action': data['action']}\n    self._learn_model.reset(state=data['prev_state'][0])\n    total_q = self._learn_model.forward(inputs, single_step=False, q_star=False)['total_q']\n    self._learn_model.reset(state=data['prev_state'][0])\n    total_q_star = self._learn_model.forward(inputs, single_step=False, q_star=True)['total_q']\n    next_inputs = {'obs': data['next_obs']}\n    self._learn_model.reset(state=data['prev_state'][1])\n    next_logit_detach = self._learn_model.forward(next_inputs, single_step=False, q_star=False)['logit'].clone().detach()\n    next_inputs = {'obs': data['next_obs'], 'action': next_logit_detach.argmax(dim=-1)}\n    with torch.no_grad():\n        self._learn_model.reset(state=data['prev_state'][1])\n        target_total_q = self._learn_model.forward(next_inputs, single_step=False, q_star=True)['total_q']\n    with torch.no_grad():\n        if data['done'] is not None:\n            target_v = self._gamma * (1 - data['done']) * target_total_q + data['reward']\n        else:\n            target_v = self._gamma * target_total_q + data['reward']\n    td_error = (total_q - target_v).clone().detach()\n    data_ = v_1step_td_data(total_q, target_total_q, data['reward'], data['done'], data['weight'])\n    (_, td_error_per_sample) = v_1step_td_error(data_, self._gamma)\n    data_star = v_1step_td_data(total_q_star, target_total_q, data['reward'], data['done'], data['weight'])\n    (loss_star, td_error_per_sample_star_) = v_1step_td_error(data_star, self._gamma)\n    alpha_to_use = self._cfg.learn.alpha\n    if self._cfg.learn.wqmix_ow:\n        ws = torch.full_like(td_error, alpha_to_use)\n        ws = torch.where(td_error < 0, torch.ones_like(td_error), ws)\n    else:\n        inputs = {'obs': data['obs']}\n        self._learn_model.reset(state=data['prev_state'][0])\n        logit_detach = self._learn_model.forward(inputs, single_step=False, q_star=False)['logit'].clone().detach()\n        cur_max_actions = logit_detach.argmax(dim=-1)\n        inputs = {'obs': data['obs'], 'action': cur_max_actions}\n        self._learn_model.reset(state=data['prev_state'][0])\n        max_action_qtot = self._learn_model.forward(inputs, single_step=False, q_star=True)['total_q']\n        is_max_action = (data['action'] == cur_max_actions).min(dim=2)[0]\n        qtot_larger = target_v > max_action_qtot\n        ws = torch.full_like(td_error, alpha_to_use)\n        ws = torch.where(is_max_action | qtot_larger, torch.ones_like(td_error), ws)\n    if data['weight'] is None:\n        data['weight'] = torch.ones_like(data['reward'])\n    loss_weighted = (ws.detach() * td_error_per_sample * data['weight']).mean()\n    self._optimizer.zero_grad()\n    self._optimizer_star.zero_grad()\n    loss_weighted.backward(retain_graph=True)\n    loss_star.backward()\n    grad_norm_q = torch.nn.utils.clip_grad_norm_(list(self._model._q_network.parameters()) + list(self._model._mixer.parameters()), self._cfg.learn.clip_value)\n    grad_norm_q_star = torch.nn.utils.clip_grad_norm_(list(self._model._q_network_star.parameters()) + list(self._model._mixer_star.parameters()), self._cfg.learn.clip_value)\n    self._optimizer.step()\n    self._optimizer_star.step()\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss_weighted.item(), 'total_q': total_q.mean().item() / self._cfg.model.agent_num, 'target_reward_total_q': target_v.mean().item() / self._cfg.model.agent_num, 'target_total_q': target_total_q.mean().item() / self._cfg.model.agent_num, 'grad_norm_q': grad_norm_q, 'grad_norm_q_star': grad_norm_q_star}",
        "mutated": [
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`Dict[str, Any]`): Dict type data, a batch of data for training, values are torch.Tensor or \\\\\\n                np.ndarray or dict/list combinations.\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): Dict type data, a info dict indicated training result, which will be \\\\\\n                recorded in text log and tensorboard, values are python scalar or a list of scalars.\\n        ArgumentsKeys:\\n            - necessary: ``obs``, ``next_obs``, ``action``, ``reward``, ``weight``, ``prev_state``, ``done``\\n        ReturnsKeys:\\n            - necessary: ``cur_lr``, ``total_loss``\\n                - cur_lr (:obj:`float`): Current learning rate\\n                - total_loss (:obj:`float`): The calculated loss\\n        '\n    data = self._data_preprocess_learn(data)\n    self._learn_model.train()\n    inputs = {'obs': data['obs'], 'action': data['action']}\n    self._learn_model.reset(state=data['prev_state'][0])\n    total_q = self._learn_model.forward(inputs, single_step=False, q_star=False)['total_q']\n    self._learn_model.reset(state=data['prev_state'][0])\n    total_q_star = self._learn_model.forward(inputs, single_step=False, q_star=True)['total_q']\n    next_inputs = {'obs': data['next_obs']}\n    self._learn_model.reset(state=data['prev_state'][1])\n    next_logit_detach = self._learn_model.forward(next_inputs, single_step=False, q_star=False)['logit'].clone().detach()\n    next_inputs = {'obs': data['next_obs'], 'action': next_logit_detach.argmax(dim=-1)}\n    with torch.no_grad():\n        self._learn_model.reset(state=data['prev_state'][1])\n        target_total_q = self._learn_model.forward(next_inputs, single_step=False, q_star=True)['total_q']\n    with torch.no_grad():\n        if data['done'] is not None:\n            target_v = self._gamma * (1 - data['done']) * target_total_q + data['reward']\n        else:\n            target_v = self._gamma * target_total_q + data['reward']\n    td_error = (total_q - target_v).clone().detach()\n    data_ = v_1step_td_data(total_q, target_total_q, data['reward'], data['done'], data['weight'])\n    (_, td_error_per_sample) = v_1step_td_error(data_, self._gamma)\n    data_star = v_1step_td_data(total_q_star, target_total_q, data['reward'], data['done'], data['weight'])\n    (loss_star, td_error_per_sample_star_) = v_1step_td_error(data_star, self._gamma)\n    alpha_to_use = self._cfg.learn.alpha\n    if self._cfg.learn.wqmix_ow:\n        ws = torch.full_like(td_error, alpha_to_use)\n        ws = torch.where(td_error < 0, torch.ones_like(td_error), ws)\n    else:\n        inputs = {'obs': data['obs']}\n        self._learn_model.reset(state=data['prev_state'][0])\n        logit_detach = self._learn_model.forward(inputs, single_step=False, q_star=False)['logit'].clone().detach()\n        cur_max_actions = logit_detach.argmax(dim=-1)\n        inputs = {'obs': data['obs'], 'action': cur_max_actions}\n        self._learn_model.reset(state=data['prev_state'][0])\n        max_action_qtot = self._learn_model.forward(inputs, single_step=False, q_star=True)['total_q']\n        is_max_action = (data['action'] == cur_max_actions).min(dim=2)[0]\n        qtot_larger = target_v > max_action_qtot\n        ws = torch.full_like(td_error, alpha_to_use)\n        ws = torch.where(is_max_action | qtot_larger, torch.ones_like(td_error), ws)\n    if data['weight'] is None:\n        data['weight'] = torch.ones_like(data['reward'])\n    loss_weighted = (ws.detach() * td_error_per_sample * data['weight']).mean()\n    self._optimizer.zero_grad()\n    self._optimizer_star.zero_grad()\n    loss_weighted.backward(retain_graph=True)\n    loss_star.backward()\n    grad_norm_q = torch.nn.utils.clip_grad_norm_(list(self._model._q_network.parameters()) + list(self._model._mixer.parameters()), self._cfg.learn.clip_value)\n    grad_norm_q_star = torch.nn.utils.clip_grad_norm_(list(self._model._q_network_star.parameters()) + list(self._model._mixer_star.parameters()), self._cfg.learn.clip_value)\n    self._optimizer.step()\n    self._optimizer_star.step()\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss_weighted.item(), 'total_q': total_q.mean().item() / self._cfg.model.agent_num, 'target_reward_total_q': target_v.mean().item() / self._cfg.model.agent_num, 'target_total_q': target_total_q.mean().item() / self._cfg.model.agent_num, 'grad_norm_q': grad_norm_q, 'grad_norm_q_star': grad_norm_q_star}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`Dict[str, Any]`): Dict type data, a batch of data for training, values are torch.Tensor or \\\\\\n                np.ndarray or dict/list combinations.\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): Dict type data, a info dict indicated training result, which will be \\\\\\n                recorded in text log and tensorboard, values are python scalar or a list of scalars.\\n        ArgumentsKeys:\\n            - necessary: ``obs``, ``next_obs``, ``action``, ``reward``, ``weight``, ``prev_state``, ``done``\\n        ReturnsKeys:\\n            - necessary: ``cur_lr``, ``total_loss``\\n                - cur_lr (:obj:`float`): Current learning rate\\n                - total_loss (:obj:`float`): The calculated loss\\n        '\n    data = self._data_preprocess_learn(data)\n    self._learn_model.train()\n    inputs = {'obs': data['obs'], 'action': data['action']}\n    self._learn_model.reset(state=data['prev_state'][0])\n    total_q = self._learn_model.forward(inputs, single_step=False, q_star=False)['total_q']\n    self._learn_model.reset(state=data['prev_state'][0])\n    total_q_star = self._learn_model.forward(inputs, single_step=False, q_star=True)['total_q']\n    next_inputs = {'obs': data['next_obs']}\n    self._learn_model.reset(state=data['prev_state'][1])\n    next_logit_detach = self._learn_model.forward(next_inputs, single_step=False, q_star=False)['logit'].clone().detach()\n    next_inputs = {'obs': data['next_obs'], 'action': next_logit_detach.argmax(dim=-1)}\n    with torch.no_grad():\n        self._learn_model.reset(state=data['prev_state'][1])\n        target_total_q = self._learn_model.forward(next_inputs, single_step=False, q_star=True)['total_q']\n    with torch.no_grad():\n        if data['done'] is not None:\n            target_v = self._gamma * (1 - data['done']) * target_total_q + data['reward']\n        else:\n            target_v = self._gamma * target_total_q + data['reward']\n    td_error = (total_q - target_v).clone().detach()\n    data_ = v_1step_td_data(total_q, target_total_q, data['reward'], data['done'], data['weight'])\n    (_, td_error_per_sample) = v_1step_td_error(data_, self._gamma)\n    data_star = v_1step_td_data(total_q_star, target_total_q, data['reward'], data['done'], data['weight'])\n    (loss_star, td_error_per_sample_star_) = v_1step_td_error(data_star, self._gamma)\n    alpha_to_use = self._cfg.learn.alpha\n    if self._cfg.learn.wqmix_ow:\n        ws = torch.full_like(td_error, alpha_to_use)\n        ws = torch.where(td_error < 0, torch.ones_like(td_error), ws)\n    else:\n        inputs = {'obs': data['obs']}\n        self._learn_model.reset(state=data['prev_state'][0])\n        logit_detach = self._learn_model.forward(inputs, single_step=False, q_star=False)['logit'].clone().detach()\n        cur_max_actions = logit_detach.argmax(dim=-1)\n        inputs = {'obs': data['obs'], 'action': cur_max_actions}\n        self._learn_model.reset(state=data['prev_state'][0])\n        max_action_qtot = self._learn_model.forward(inputs, single_step=False, q_star=True)['total_q']\n        is_max_action = (data['action'] == cur_max_actions).min(dim=2)[0]\n        qtot_larger = target_v > max_action_qtot\n        ws = torch.full_like(td_error, alpha_to_use)\n        ws = torch.where(is_max_action | qtot_larger, torch.ones_like(td_error), ws)\n    if data['weight'] is None:\n        data['weight'] = torch.ones_like(data['reward'])\n    loss_weighted = (ws.detach() * td_error_per_sample * data['weight']).mean()\n    self._optimizer.zero_grad()\n    self._optimizer_star.zero_grad()\n    loss_weighted.backward(retain_graph=True)\n    loss_star.backward()\n    grad_norm_q = torch.nn.utils.clip_grad_norm_(list(self._model._q_network.parameters()) + list(self._model._mixer.parameters()), self._cfg.learn.clip_value)\n    grad_norm_q_star = torch.nn.utils.clip_grad_norm_(list(self._model._q_network_star.parameters()) + list(self._model._mixer_star.parameters()), self._cfg.learn.clip_value)\n    self._optimizer.step()\n    self._optimizer_star.step()\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss_weighted.item(), 'total_q': total_q.mean().item() / self._cfg.model.agent_num, 'target_reward_total_q': target_v.mean().item() / self._cfg.model.agent_num, 'target_total_q': target_total_q.mean().item() / self._cfg.model.agent_num, 'grad_norm_q': grad_norm_q, 'grad_norm_q_star': grad_norm_q_star}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`Dict[str, Any]`): Dict type data, a batch of data for training, values are torch.Tensor or \\\\\\n                np.ndarray or dict/list combinations.\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): Dict type data, a info dict indicated training result, which will be \\\\\\n                recorded in text log and tensorboard, values are python scalar or a list of scalars.\\n        ArgumentsKeys:\\n            - necessary: ``obs``, ``next_obs``, ``action``, ``reward``, ``weight``, ``prev_state``, ``done``\\n        ReturnsKeys:\\n            - necessary: ``cur_lr``, ``total_loss``\\n                - cur_lr (:obj:`float`): Current learning rate\\n                - total_loss (:obj:`float`): The calculated loss\\n        '\n    data = self._data_preprocess_learn(data)\n    self._learn_model.train()\n    inputs = {'obs': data['obs'], 'action': data['action']}\n    self._learn_model.reset(state=data['prev_state'][0])\n    total_q = self._learn_model.forward(inputs, single_step=False, q_star=False)['total_q']\n    self._learn_model.reset(state=data['prev_state'][0])\n    total_q_star = self._learn_model.forward(inputs, single_step=False, q_star=True)['total_q']\n    next_inputs = {'obs': data['next_obs']}\n    self._learn_model.reset(state=data['prev_state'][1])\n    next_logit_detach = self._learn_model.forward(next_inputs, single_step=False, q_star=False)['logit'].clone().detach()\n    next_inputs = {'obs': data['next_obs'], 'action': next_logit_detach.argmax(dim=-1)}\n    with torch.no_grad():\n        self._learn_model.reset(state=data['prev_state'][1])\n        target_total_q = self._learn_model.forward(next_inputs, single_step=False, q_star=True)['total_q']\n    with torch.no_grad():\n        if data['done'] is not None:\n            target_v = self._gamma * (1 - data['done']) * target_total_q + data['reward']\n        else:\n            target_v = self._gamma * target_total_q + data['reward']\n    td_error = (total_q - target_v).clone().detach()\n    data_ = v_1step_td_data(total_q, target_total_q, data['reward'], data['done'], data['weight'])\n    (_, td_error_per_sample) = v_1step_td_error(data_, self._gamma)\n    data_star = v_1step_td_data(total_q_star, target_total_q, data['reward'], data['done'], data['weight'])\n    (loss_star, td_error_per_sample_star_) = v_1step_td_error(data_star, self._gamma)\n    alpha_to_use = self._cfg.learn.alpha\n    if self._cfg.learn.wqmix_ow:\n        ws = torch.full_like(td_error, alpha_to_use)\n        ws = torch.where(td_error < 0, torch.ones_like(td_error), ws)\n    else:\n        inputs = {'obs': data['obs']}\n        self._learn_model.reset(state=data['prev_state'][0])\n        logit_detach = self._learn_model.forward(inputs, single_step=False, q_star=False)['logit'].clone().detach()\n        cur_max_actions = logit_detach.argmax(dim=-1)\n        inputs = {'obs': data['obs'], 'action': cur_max_actions}\n        self._learn_model.reset(state=data['prev_state'][0])\n        max_action_qtot = self._learn_model.forward(inputs, single_step=False, q_star=True)['total_q']\n        is_max_action = (data['action'] == cur_max_actions).min(dim=2)[0]\n        qtot_larger = target_v > max_action_qtot\n        ws = torch.full_like(td_error, alpha_to_use)\n        ws = torch.where(is_max_action | qtot_larger, torch.ones_like(td_error), ws)\n    if data['weight'] is None:\n        data['weight'] = torch.ones_like(data['reward'])\n    loss_weighted = (ws.detach() * td_error_per_sample * data['weight']).mean()\n    self._optimizer.zero_grad()\n    self._optimizer_star.zero_grad()\n    loss_weighted.backward(retain_graph=True)\n    loss_star.backward()\n    grad_norm_q = torch.nn.utils.clip_grad_norm_(list(self._model._q_network.parameters()) + list(self._model._mixer.parameters()), self._cfg.learn.clip_value)\n    grad_norm_q_star = torch.nn.utils.clip_grad_norm_(list(self._model._q_network_star.parameters()) + list(self._model._mixer_star.parameters()), self._cfg.learn.clip_value)\n    self._optimizer.step()\n    self._optimizer_star.step()\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss_weighted.item(), 'total_q': total_q.mean().item() / self._cfg.model.agent_num, 'target_reward_total_q': target_v.mean().item() / self._cfg.model.agent_num, 'target_total_q': target_total_q.mean().item() / self._cfg.model.agent_num, 'grad_norm_q': grad_norm_q, 'grad_norm_q_star': grad_norm_q_star}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`Dict[str, Any]`): Dict type data, a batch of data for training, values are torch.Tensor or \\\\\\n                np.ndarray or dict/list combinations.\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): Dict type data, a info dict indicated training result, which will be \\\\\\n                recorded in text log and tensorboard, values are python scalar or a list of scalars.\\n        ArgumentsKeys:\\n            - necessary: ``obs``, ``next_obs``, ``action``, ``reward``, ``weight``, ``prev_state``, ``done``\\n        ReturnsKeys:\\n            - necessary: ``cur_lr``, ``total_loss``\\n                - cur_lr (:obj:`float`): Current learning rate\\n                - total_loss (:obj:`float`): The calculated loss\\n        '\n    data = self._data_preprocess_learn(data)\n    self._learn_model.train()\n    inputs = {'obs': data['obs'], 'action': data['action']}\n    self._learn_model.reset(state=data['prev_state'][0])\n    total_q = self._learn_model.forward(inputs, single_step=False, q_star=False)['total_q']\n    self._learn_model.reset(state=data['prev_state'][0])\n    total_q_star = self._learn_model.forward(inputs, single_step=False, q_star=True)['total_q']\n    next_inputs = {'obs': data['next_obs']}\n    self._learn_model.reset(state=data['prev_state'][1])\n    next_logit_detach = self._learn_model.forward(next_inputs, single_step=False, q_star=False)['logit'].clone().detach()\n    next_inputs = {'obs': data['next_obs'], 'action': next_logit_detach.argmax(dim=-1)}\n    with torch.no_grad():\n        self._learn_model.reset(state=data['prev_state'][1])\n        target_total_q = self._learn_model.forward(next_inputs, single_step=False, q_star=True)['total_q']\n    with torch.no_grad():\n        if data['done'] is not None:\n            target_v = self._gamma * (1 - data['done']) * target_total_q + data['reward']\n        else:\n            target_v = self._gamma * target_total_q + data['reward']\n    td_error = (total_q - target_v).clone().detach()\n    data_ = v_1step_td_data(total_q, target_total_q, data['reward'], data['done'], data['weight'])\n    (_, td_error_per_sample) = v_1step_td_error(data_, self._gamma)\n    data_star = v_1step_td_data(total_q_star, target_total_q, data['reward'], data['done'], data['weight'])\n    (loss_star, td_error_per_sample_star_) = v_1step_td_error(data_star, self._gamma)\n    alpha_to_use = self._cfg.learn.alpha\n    if self._cfg.learn.wqmix_ow:\n        ws = torch.full_like(td_error, alpha_to_use)\n        ws = torch.where(td_error < 0, torch.ones_like(td_error), ws)\n    else:\n        inputs = {'obs': data['obs']}\n        self._learn_model.reset(state=data['prev_state'][0])\n        logit_detach = self._learn_model.forward(inputs, single_step=False, q_star=False)['logit'].clone().detach()\n        cur_max_actions = logit_detach.argmax(dim=-1)\n        inputs = {'obs': data['obs'], 'action': cur_max_actions}\n        self._learn_model.reset(state=data['prev_state'][0])\n        max_action_qtot = self._learn_model.forward(inputs, single_step=False, q_star=True)['total_q']\n        is_max_action = (data['action'] == cur_max_actions).min(dim=2)[0]\n        qtot_larger = target_v > max_action_qtot\n        ws = torch.full_like(td_error, alpha_to_use)\n        ws = torch.where(is_max_action | qtot_larger, torch.ones_like(td_error), ws)\n    if data['weight'] is None:\n        data['weight'] = torch.ones_like(data['reward'])\n    loss_weighted = (ws.detach() * td_error_per_sample * data['weight']).mean()\n    self._optimizer.zero_grad()\n    self._optimizer_star.zero_grad()\n    loss_weighted.backward(retain_graph=True)\n    loss_star.backward()\n    grad_norm_q = torch.nn.utils.clip_grad_norm_(list(self._model._q_network.parameters()) + list(self._model._mixer.parameters()), self._cfg.learn.clip_value)\n    grad_norm_q_star = torch.nn.utils.clip_grad_norm_(list(self._model._q_network_star.parameters()) + list(self._model._mixer_star.parameters()), self._cfg.learn.clip_value)\n    self._optimizer.step()\n    self._optimizer_star.step()\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss_weighted.item(), 'total_q': total_q.mean().item() / self._cfg.model.agent_num, 'target_reward_total_q': target_v.mean().item() / self._cfg.model.agent_num, 'target_total_q': target_total_q.mean().item() / self._cfg.model.agent_num, 'grad_norm_q': grad_norm_q, 'grad_norm_q_star': grad_norm_q_star}",
            "def _forward_learn(self, data: dict) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Forward and backward function of learn mode.\\n        Arguments:\\n            - data (:obj:`Dict[str, Any]`): Dict type data, a batch of data for training, values are torch.Tensor or \\\\\\n                np.ndarray or dict/list combinations.\\n        Returns:\\n            - info_dict (:obj:`Dict[str, Any]`): Dict type data, a info dict indicated training result, which will be \\\\\\n                recorded in text log and tensorboard, values are python scalar or a list of scalars.\\n        ArgumentsKeys:\\n            - necessary: ``obs``, ``next_obs``, ``action``, ``reward``, ``weight``, ``prev_state``, ``done``\\n        ReturnsKeys:\\n            - necessary: ``cur_lr``, ``total_loss``\\n                - cur_lr (:obj:`float`): Current learning rate\\n                - total_loss (:obj:`float`): The calculated loss\\n        '\n    data = self._data_preprocess_learn(data)\n    self._learn_model.train()\n    inputs = {'obs': data['obs'], 'action': data['action']}\n    self._learn_model.reset(state=data['prev_state'][0])\n    total_q = self._learn_model.forward(inputs, single_step=False, q_star=False)['total_q']\n    self._learn_model.reset(state=data['prev_state'][0])\n    total_q_star = self._learn_model.forward(inputs, single_step=False, q_star=True)['total_q']\n    next_inputs = {'obs': data['next_obs']}\n    self._learn_model.reset(state=data['prev_state'][1])\n    next_logit_detach = self._learn_model.forward(next_inputs, single_step=False, q_star=False)['logit'].clone().detach()\n    next_inputs = {'obs': data['next_obs'], 'action': next_logit_detach.argmax(dim=-1)}\n    with torch.no_grad():\n        self._learn_model.reset(state=data['prev_state'][1])\n        target_total_q = self._learn_model.forward(next_inputs, single_step=False, q_star=True)['total_q']\n    with torch.no_grad():\n        if data['done'] is not None:\n            target_v = self._gamma * (1 - data['done']) * target_total_q + data['reward']\n        else:\n            target_v = self._gamma * target_total_q + data['reward']\n    td_error = (total_q - target_v).clone().detach()\n    data_ = v_1step_td_data(total_q, target_total_q, data['reward'], data['done'], data['weight'])\n    (_, td_error_per_sample) = v_1step_td_error(data_, self._gamma)\n    data_star = v_1step_td_data(total_q_star, target_total_q, data['reward'], data['done'], data['weight'])\n    (loss_star, td_error_per_sample_star_) = v_1step_td_error(data_star, self._gamma)\n    alpha_to_use = self._cfg.learn.alpha\n    if self._cfg.learn.wqmix_ow:\n        ws = torch.full_like(td_error, alpha_to_use)\n        ws = torch.where(td_error < 0, torch.ones_like(td_error), ws)\n    else:\n        inputs = {'obs': data['obs']}\n        self._learn_model.reset(state=data['prev_state'][0])\n        logit_detach = self._learn_model.forward(inputs, single_step=False, q_star=False)['logit'].clone().detach()\n        cur_max_actions = logit_detach.argmax(dim=-1)\n        inputs = {'obs': data['obs'], 'action': cur_max_actions}\n        self._learn_model.reset(state=data['prev_state'][0])\n        max_action_qtot = self._learn_model.forward(inputs, single_step=False, q_star=True)['total_q']\n        is_max_action = (data['action'] == cur_max_actions).min(dim=2)[0]\n        qtot_larger = target_v > max_action_qtot\n        ws = torch.full_like(td_error, alpha_to_use)\n        ws = torch.where(is_max_action | qtot_larger, torch.ones_like(td_error), ws)\n    if data['weight'] is None:\n        data['weight'] = torch.ones_like(data['reward'])\n    loss_weighted = (ws.detach() * td_error_per_sample * data['weight']).mean()\n    self._optimizer.zero_grad()\n    self._optimizer_star.zero_grad()\n    loss_weighted.backward(retain_graph=True)\n    loss_star.backward()\n    grad_norm_q = torch.nn.utils.clip_grad_norm_(list(self._model._q_network.parameters()) + list(self._model._mixer.parameters()), self._cfg.learn.clip_value)\n    grad_norm_q_star = torch.nn.utils.clip_grad_norm_(list(self._model._q_network_star.parameters()) + list(self._model._mixer_star.parameters()), self._cfg.learn.clip_value)\n    self._optimizer.step()\n    self._optimizer_star.step()\n    return {'cur_lr': self._optimizer.defaults['lr'], 'total_loss': loss_weighted.item(), 'total_q': total_q.mean().item() / self._cfg.model.agent_num, 'target_reward_total_q': target_v.mean().item() / self._cfg.model.agent_num, 'target_total_q': target_total_q.mean().item() / self._cfg.model.agent_num, 'grad_norm_q': grad_norm_q, 'grad_norm_q_star': grad_norm_q_star}"
        ]
    },
    {
        "func_name": "_state_dict_learn",
        "original": "def _state_dict_learn(self) -> Dict[str, Any]:\n    \"\"\"\n        Overview:\n            Return the state_dict of learn mode, usually including model and optimizer.\n        Returns:\n            - state_dict (:obj:`Dict[str, Any]`): the dict of current policy learn state, for saving and restoring.\n        \"\"\"\n    return {'model': self._learn_model.state_dict(), 'optimizer': self._optimizer.state_dict(), 'optimizer_star': self._optimizer_star.state_dict()}",
        "mutated": [
            "def _state_dict_learn(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Return the state_dict of learn mode, usually including model and optimizer.\\n        Returns:\\n            - state_dict (:obj:`Dict[str, Any]`): the dict of current policy learn state, for saving and restoring.\\n        '\n    return {'model': self._learn_model.state_dict(), 'optimizer': self._optimizer.state_dict(), 'optimizer_star': self._optimizer_star.state_dict()}",
            "def _state_dict_learn(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Return the state_dict of learn mode, usually including model and optimizer.\\n        Returns:\\n            - state_dict (:obj:`Dict[str, Any]`): the dict of current policy learn state, for saving and restoring.\\n        '\n    return {'model': self._learn_model.state_dict(), 'optimizer': self._optimizer.state_dict(), 'optimizer_star': self._optimizer_star.state_dict()}",
            "def _state_dict_learn(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Return the state_dict of learn mode, usually including model and optimizer.\\n        Returns:\\n            - state_dict (:obj:`Dict[str, Any]`): the dict of current policy learn state, for saving and restoring.\\n        '\n    return {'model': self._learn_model.state_dict(), 'optimizer': self._optimizer.state_dict(), 'optimizer_star': self._optimizer_star.state_dict()}",
            "def _state_dict_learn(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Return the state_dict of learn mode, usually including model and optimizer.\\n        Returns:\\n            - state_dict (:obj:`Dict[str, Any]`): the dict of current policy learn state, for saving and restoring.\\n        '\n    return {'model': self._learn_model.state_dict(), 'optimizer': self._optimizer.state_dict(), 'optimizer_star': self._optimizer_star.state_dict()}",
            "def _state_dict_learn(self) -> Dict[str, Any]:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Return the state_dict of learn mode, usually including model and optimizer.\\n        Returns:\\n            - state_dict (:obj:`Dict[str, Any]`): the dict of current policy learn state, for saving and restoring.\\n        '\n    return {'model': self._learn_model.state_dict(), 'optimizer': self._optimizer.state_dict(), 'optimizer_star': self._optimizer_star.state_dict()}"
        ]
    },
    {
        "func_name": "_load_state_dict_learn",
        "original": "def _load_state_dict_learn(self, state_dict: Dict[str, Any]) -> None:\n    \"\"\"\n        Overview:\n            Load the state_dict variable into policy learn mode.\n        Arguments:\n            - state_dict (:obj:`Dict[str, Any]`): the dict of policy learn state saved before.\n        .. tip::\n            If you want to only load some parts of model, you can simply set the ``strict`` argument in \\\\\n            load_state_dict to ``False``, or refer to ``ding.torch_utils.checkpoint_helper`` for more \\\\\n            complicated operation.\n        \"\"\"\n    self._learn_model.load_state_dict(state_dict['model'])\n    self._optimizer.load_state_dict(state_dict['optimizer'])\n    self._optimizer_star.load_state_dict(state_dict['optimizer_star'])",
        "mutated": [
            "def _load_state_dict_learn(self, state_dict: Dict[str, Any]) -> None:\n    if False:\n        i = 10\n    '\\n        Overview:\\n            Load the state_dict variable into policy learn mode.\\n        Arguments:\\n            - state_dict (:obj:`Dict[str, Any]`): the dict of policy learn state saved before.\\n        .. tip::\\n            If you want to only load some parts of model, you can simply set the ``strict`` argument in \\\\\\n            load_state_dict to ``False``, or refer to ``ding.torch_utils.checkpoint_helper`` for more \\\\\\n            complicated operation.\\n        '\n    self._learn_model.load_state_dict(state_dict['model'])\n    self._optimizer.load_state_dict(state_dict['optimizer'])\n    self._optimizer_star.load_state_dict(state_dict['optimizer_star'])",
            "def _load_state_dict_learn(self, state_dict: Dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Overview:\\n            Load the state_dict variable into policy learn mode.\\n        Arguments:\\n            - state_dict (:obj:`Dict[str, Any]`): the dict of policy learn state saved before.\\n        .. tip::\\n            If you want to only load some parts of model, you can simply set the ``strict`` argument in \\\\\\n            load_state_dict to ``False``, or refer to ``ding.torch_utils.checkpoint_helper`` for more \\\\\\n            complicated operation.\\n        '\n    self._learn_model.load_state_dict(state_dict['model'])\n    self._optimizer.load_state_dict(state_dict['optimizer'])\n    self._optimizer_star.load_state_dict(state_dict['optimizer_star'])",
            "def _load_state_dict_learn(self, state_dict: Dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Overview:\\n            Load the state_dict variable into policy learn mode.\\n        Arguments:\\n            - state_dict (:obj:`Dict[str, Any]`): the dict of policy learn state saved before.\\n        .. tip::\\n            If you want to only load some parts of model, you can simply set the ``strict`` argument in \\\\\\n            load_state_dict to ``False``, or refer to ``ding.torch_utils.checkpoint_helper`` for more \\\\\\n            complicated operation.\\n        '\n    self._learn_model.load_state_dict(state_dict['model'])\n    self._optimizer.load_state_dict(state_dict['optimizer'])\n    self._optimizer_star.load_state_dict(state_dict['optimizer_star'])",
            "def _load_state_dict_learn(self, state_dict: Dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Overview:\\n            Load the state_dict variable into policy learn mode.\\n        Arguments:\\n            - state_dict (:obj:`Dict[str, Any]`): the dict of policy learn state saved before.\\n        .. tip::\\n            If you want to only load some parts of model, you can simply set the ``strict`` argument in \\\\\\n            load_state_dict to ``False``, or refer to ``ding.torch_utils.checkpoint_helper`` for more \\\\\\n            complicated operation.\\n        '\n    self._learn_model.load_state_dict(state_dict['model'])\n    self._optimizer.load_state_dict(state_dict['optimizer'])\n    self._optimizer_star.load_state_dict(state_dict['optimizer_star'])",
            "def _load_state_dict_learn(self, state_dict: Dict[str, Any]) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Overview:\\n            Load the state_dict variable into policy learn mode.\\n        Arguments:\\n            - state_dict (:obj:`Dict[str, Any]`): the dict of policy learn state saved before.\\n        .. tip::\\n            If you want to only load some parts of model, you can simply set the ``strict`` argument in \\\\\\n            load_state_dict to ``False``, or refer to ``ding.torch_utils.checkpoint_helper`` for more \\\\\\n            complicated operation.\\n        '\n    self._learn_model.load_state_dict(state_dict['model'])\n    self._optimizer.load_state_dict(state_dict['optimizer'])\n    self._optimizer_star.load_state_dict(state_dict['optimizer_star'])"
        ]
    }
]