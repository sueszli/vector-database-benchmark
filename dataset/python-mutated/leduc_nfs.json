[
    {
        "func_name": "__init__",
        "original": "def __init__(self, env, nfsp_policies, mode):\n    game = env.game\n    player_ids = list(range(FLAGS.num_players))\n    super(NFSPPolicies, self).__init__(game, player_ids)\n    self._policies = nfsp_policies\n    self._mode = mode\n    self._obs = {'info_state': [None] * FLAGS.num_players, 'legal_actions': [None] * FLAGS.num_players}",
        "mutated": [
            "def __init__(self, env, nfsp_policies, mode):\n    if False:\n        i = 10\n    game = env.game\n    player_ids = list(range(FLAGS.num_players))\n    super(NFSPPolicies, self).__init__(game, player_ids)\n    self._policies = nfsp_policies\n    self._mode = mode\n    self._obs = {'info_state': [None] * FLAGS.num_players, 'legal_actions': [None] * FLAGS.num_players}",
            "def __init__(self, env, nfsp_policies, mode):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    game = env.game\n    player_ids = list(range(FLAGS.num_players))\n    super(NFSPPolicies, self).__init__(game, player_ids)\n    self._policies = nfsp_policies\n    self._mode = mode\n    self._obs = {'info_state': [None] * FLAGS.num_players, 'legal_actions': [None] * FLAGS.num_players}",
            "def __init__(self, env, nfsp_policies, mode):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    game = env.game\n    player_ids = list(range(FLAGS.num_players))\n    super(NFSPPolicies, self).__init__(game, player_ids)\n    self._policies = nfsp_policies\n    self._mode = mode\n    self._obs = {'info_state': [None] * FLAGS.num_players, 'legal_actions': [None] * FLAGS.num_players}",
            "def __init__(self, env, nfsp_policies, mode):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    game = env.game\n    player_ids = list(range(FLAGS.num_players))\n    super(NFSPPolicies, self).__init__(game, player_ids)\n    self._policies = nfsp_policies\n    self._mode = mode\n    self._obs = {'info_state': [None] * FLAGS.num_players, 'legal_actions': [None] * FLAGS.num_players}",
            "def __init__(self, env, nfsp_policies, mode):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    game = env.game\n    player_ids = list(range(FLAGS.num_players))\n    super(NFSPPolicies, self).__init__(game, player_ids)\n    self._policies = nfsp_policies\n    self._mode = mode\n    self._obs = {'info_state': [None] * FLAGS.num_players, 'legal_actions': [None] * FLAGS.num_players}"
        ]
    },
    {
        "func_name": "action_probabilities",
        "original": "def action_probabilities(self, state, player_id=None):\n    cur_player = state.current_player()\n    legal_actions = state.legal_actions(cur_player)\n    self._obs['current_player'] = cur_player\n    self._obs['info_state'][cur_player] = state.information_state_tensor(cur_player)\n    self._obs['legal_actions'][cur_player] = legal_actions\n    info_state = rl_environment.TimeStep(observations=self._obs, rewards=None, discounts=None, step_type=None)\n    with self._policies[cur_player].temp_mode_as(self._mode):\n        p = self._policies[cur_player].step(info_state, is_evaluation=True).probs\n    prob_dict = {action: p[action] for action in legal_actions}\n    return prob_dict",
        "mutated": [
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n    cur_player = state.current_player()\n    legal_actions = state.legal_actions(cur_player)\n    self._obs['current_player'] = cur_player\n    self._obs['info_state'][cur_player] = state.information_state_tensor(cur_player)\n    self._obs['legal_actions'][cur_player] = legal_actions\n    info_state = rl_environment.TimeStep(observations=self._obs, rewards=None, discounts=None, step_type=None)\n    with self._policies[cur_player].temp_mode_as(self._mode):\n        p = self._policies[cur_player].step(info_state, is_evaluation=True).probs\n    prob_dict = {action: p[action] for action in legal_actions}\n    return prob_dict",
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    cur_player = state.current_player()\n    legal_actions = state.legal_actions(cur_player)\n    self._obs['current_player'] = cur_player\n    self._obs['info_state'][cur_player] = state.information_state_tensor(cur_player)\n    self._obs['legal_actions'][cur_player] = legal_actions\n    info_state = rl_environment.TimeStep(observations=self._obs, rewards=None, discounts=None, step_type=None)\n    with self._policies[cur_player].temp_mode_as(self._mode):\n        p = self._policies[cur_player].step(info_state, is_evaluation=True).probs\n    prob_dict = {action: p[action] for action in legal_actions}\n    return prob_dict",
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    cur_player = state.current_player()\n    legal_actions = state.legal_actions(cur_player)\n    self._obs['current_player'] = cur_player\n    self._obs['info_state'][cur_player] = state.information_state_tensor(cur_player)\n    self._obs['legal_actions'][cur_player] = legal_actions\n    info_state = rl_environment.TimeStep(observations=self._obs, rewards=None, discounts=None, step_type=None)\n    with self._policies[cur_player].temp_mode_as(self._mode):\n        p = self._policies[cur_player].step(info_state, is_evaluation=True).probs\n    prob_dict = {action: p[action] for action in legal_actions}\n    return prob_dict",
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    cur_player = state.current_player()\n    legal_actions = state.legal_actions(cur_player)\n    self._obs['current_player'] = cur_player\n    self._obs['info_state'][cur_player] = state.information_state_tensor(cur_player)\n    self._obs['legal_actions'][cur_player] = legal_actions\n    info_state = rl_environment.TimeStep(observations=self._obs, rewards=None, discounts=None, step_type=None)\n    with self._policies[cur_player].temp_mode_as(self._mode):\n        p = self._policies[cur_player].step(info_state, is_evaluation=True).probs\n    prob_dict = {action: p[action] for action in legal_actions}\n    return prob_dict",
            "def action_probabilities(self, state, player_id=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    cur_player = state.current_player()\n    legal_actions = state.legal_actions(cur_player)\n    self._obs['current_player'] = cur_player\n    self._obs['info_state'][cur_player] = state.information_state_tensor(cur_player)\n    self._obs['legal_actions'][cur_player] = legal_actions\n    info_state = rl_environment.TimeStep(observations=self._obs, rewards=None, discounts=None, step_type=None)\n    with self._policies[cur_player].temp_mode_as(self._mode):\n        p = self._policies[cur_player].step(info_state, is_evaluation=True).probs\n    prob_dict = {action: p[action] for action in legal_actions}\n    return prob_dict"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(unused_argv):\n    logging.info('Loading %s', FLAGS.game_name)\n    game = FLAGS.game_name\n    num_players = FLAGS.num_players\n    env_configs = {'players': num_players}\n    env = rl_environment.Environment(game, **env_configs)\n    info_state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    hidden_layers_sizes = [int(l) for l in FLAGS.hidden_layers_sizes]\n    kwargs = {'replay_buffer_capacity': FLAGS.replay_buffer_capacity, 'reservoir_buffer_capacity': FLAGS.reservoir_buffer_capacity, 'min_buffer_size_to_learn': FLAGS.min_buffer_size_to_learn, 'anticipatory_param': FLAGS.anticipatory_param, 'batch_size': FLAGS.batch_size, 'learn_every': FLAGS.learn_every, 'rl_learning_rate': FLAGS.rl_learning_rate, 'sl_learning_rate': FLAGS.sl_learning_rate, 'optimizer_str': FLAGS.optimizer_str, 'loss_str': FLAGS.loss_str, 'update_target_network_every': FLAGS.update_target_network_every, 'discount_factor': FLAGS.discount_factor, 'epsilon_decay_duration': FLAGS.epsilon_decay_duration, 'epsilon_start': FLAGS.epsilon_start, 'epsilon_end': FLAGS.epsilon_end}\n    with tf.Session() as sess:\n        agents = [nfsp.NFSP(sess, idx, info_state_size, num_actions, hidden_layers_sizes, **kwargs) for idx in range(num_players)]\n        joint_avg_policy = NFSPPolicies(env, agents, nfsp.MODE.average_policy)\n        sess.run(tf.global_variables_initializer())\n        if FLAGS.use_checkpoints:\n            for agent in agents:\n                if agent.has_checkpoint(FLAGS.checkpoint_dir):\n                    agent.restore(FLAGS.checkpoint_dir)\n        for ep in range(FLAGS.num_train_episodes):\n            if (ep + 1) % FLAGS.eval_every == 0:\n                losses = [agent.loss for agent in agents]\n                logging.info('Losses: %s', losses)\n                if FLAGS.evaluation_metric == 'exploitability':\n                    expl = exploitability.exploitability(env.game, joint_avg_policy)\n                    logging.info('[%s] Exploitability AVG %s', ep + 1, expl)\n                elif FLAGS.evaluation_metric == 'nash_conv':\n                    nash_conv = exploitability.nash_conv(env.game, joint_avg_policy)\n                    logging.info('[%s] NashConv %s', ep + 1, nash_conv)\n                else:\n                    raise ValueError(' '.join(('Invalid evaluation metric, choose from', \"'exploitability', 'nash_conv'.\")))\n                if FLAGS.use_checkpoints:\n                    for agent in agents:\n                        agent.save(FLAGS.checkpoint_dir)\n                logging.info('_____________________________________________')\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = agents[player_id].step(time_step)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n            for agent in agents:\n                agent.step(time_step)",
        "mutated": [
            "def main(unused_argv):\n    if False:\n        i = 10\n    logging.info('Loading %s', FLAGS.game_name)\n    game = FLAGS.game_name\n    num_players = FLAGS.num_players\n    env_configs = {'players': num_players}\n    env = rl_environment.Environment(game, **env_configs)\n    info_state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    hidden_layers_sizes = [int(l) for l in FLAGS.hidden_layers_sizes]\n    kwargs = {'replay_buffer_capacity': FLAGS.replay_buffer_capacity, 'reservoir_buffer_capacity': FLAGS.reservoir_buffer_capacity, 'min_buffer_size_to_learn': FLAGS.min_buffer_size_to_learn, 'anticipatory_param': FLAGS.anticipatory_param, 'batch_size': FLAGS.batch_size, 'learn_every': FLAGS.learn_every, 'rl_learning_rate': FLAGS.rl_learning_rate, 'sl_learning_rate': FLAGS.sl_learning_rate, 'optimizer_str': FLAGS.optimizer_str, 'loss_str': FLAGS.loss_str, 'update_target_network_every': FLAGS.update_target_network_every, 'discount_factor': FLAGS.discount_factor, 'epsilon_decay_duration': FLAGS.epsilon_decay_duration, 'epsilon_start': FLAGS.epsilon_start, 'epsilon_end': FLAGS.epsilon_end}\n    with tf.Session() as sess:\n        agents = [nfsp.NFSP(sess, idx, info_state_size, num_actions, hidden_layers_sizes, **kwargs) for idx in range(num_players)]\n        joint_avg_policy = NFSPPolicies(env, agents, nfsp.MODE.average_policy)\n        sess.run(tf.global_variables_initializer())\n        if FLAGS.use_checkpoints:\n            for agent in agents:\n                if agent.has_checkpoint(FLAGS.checkpoint_dir):\n                    agent.restore(FLAGS.checkpoint_dir)\n        for ep in range(FLAGS.num_train_episodes):\n            if (ep + 1) % FLAGS.eval_every == 0:\n                losses = [agent.loss for agent in agents]\n                logging.info('Losses: %s', losses)\n                if FLAGS.evaluation_metric == 'exploitability':\n                    expl = exploitability.exploitability(env.game, joint_avg_policy)\n                    logging.info('[%s] Exploitability AVG %s', ep + 1, expl)\n                elif FLAGS.evaluation_metric == 'nash_conv':\n                    nash_conv = exploitability.nash_conv(env.game, joint_avg_policy)\n                    logging.info('[%s] NashConv %s', ep + 1, nash_conv)\n                else:\n                    raise ValueError(' '.join(('Invalid evaluation metric, choose from', \"'exploitability', 'nash_conv'.\")))\n                if FLAGS.use_checkpoints:\n                    for agent in agents:\n                        agent.save(FLAGS.checkpoint_dir)\n                logging.info('_____________________________________________')\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = agents[player_id].step(time_step)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n            for agent in agents:\n                agent.step(time_step)",
            "def main(unused_argv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    logging.info('Loading %s', FLAGS.game_name)\n    game = FLAGS.game_name\n    num_players = FLAGS.num_players\n    env_configs = {'players': num_players}\n    env = rl_environment.Environment(game, **env_configs)\n    info_state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    hidden_layers_sizes = [int(l) for l in FLAGS.hidden_layers_sizes]\n    kwargs = {'replay_buffer_capacity': FLAGS.replay_buffer_capacity, 'reservoir_buffer_capacity': FLAGS.reservoir_buffer_capacity, 'min_buffer_size_to_learn': FLAGS.min_buffer_size_to_learn, 'anticipatory_param': FLAGS.anticipatory_param, 'batch_size': FLAGS.batch_size, 'learn_every': FLAGS.learn_every, 'rl_learning_rate': FLAGS.rl_learning_rate, 'sl_learning_rate': FLAGS.sl_learning_rate, 'optimizer_str': FLAGS.optimizer_str, 'loss_str': FLAGS.loss_str, 'update_target_network_every': FLAGS.update_target_network_every, 'discount_factor': FLAGS.discount_factor, 'epsilon_decay_duration': FLAGS.epsilon_decay_duration, 'epsilon_start': FLAGS.epsilon_start, 'epsilon_end': FLAGS.epsilon_end}\n    with tf.Session() as sess:\n        agents = [nfsp.NFSP(sess, idx, info_state_size, num_actions, hidden_layers_sizes, **kwargs) for idx in range(num_players)]\n        joint_avg_policy = NFSPPolicies(env, agents, nfsp.MODE.average_policy)\n        sess.run(tf.global_variables_initializer())\n        if FLAGS.use_checkpoints:\n            for agent in agents:\n                if agent.has_checkpoint(FLAGS.checkpoint_dir):\n                    agent.restore(FLAGS.checkpoint_dir)\n        for ep in range(FLAGS.num_train_episodes):\n            if (ep + 1) % FLAGS.eval_every == 0:\n                losses = [agent.loss for agent in agents]\n                logging.info('Losses: %s', losses)\n                if FLAGS.evaluation_metric == 'exploitability':\n                    expl = exploitability.exploitability(env.game, joint_avg_policy)\n                    logging.info('[%s] Exploitability AVG %s', ep + 1, expl)\n                elif FLAGS.evaluation_metric == 'nash_conv':\n                    nash_conv = exploitability.nash_conv(env.game, joint_avg_policy)\n                    logging.info('[%s] NashConv %s', ep + 1, nash_conv)\n                else:\n                    raise ValueError(' '.join(('Invalid evaluation metric, choose from', \"'exploitability', 'nash_conv'.\")))\n                if FLAGS.use_checkpoints:\n                    for agent in agents:\n                        agent.save(FLAGS.checkpoint_dir)\n                logging.info('_____________________________________________')\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = agents[player_id].step(time_step)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n            for agent in agents:\n                agent.step(time_step)",
            "def main(unused_argv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    logging.info('Loading %s', FLAGS.game_name)\n    game = FLAGS.game_name\n    num_players = FLAGS.num_players\n    env_configs = {'players': num_players}\n    env = rl_environment.Environment(game, **env_configs)\n    info_state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    hidden_layers_sizes = [int(l) for l in FLAGS.hidden_layers_sizes]\n    kwargs = {'replay_buffer_capacity': FLAGS.replay_buffer_capacity, 'reservoir_buffer_capacity': FLAGS.reservoir_buffer_capacity, 'min_buffer_size_to_learn': FLAGS.min_buffer_size_to_learn, 'anticipatory_param': FLAGS.anticipatory_param, 'batch_size': FLAGS.batch_size, 'learn_every': FLAGS.learn_every, 'rl_learning_rate': FLAGS.rl_learning_rate, 'sl_learning_rate': FLAGS.sl_learning_rate, 'optimizer_str': FLAGS.optimizer_str, 'loss_str': FLAGS.loss_str, 'update_target_network_every': FLAGS.update_target_network_every, 'discount_factor': FLAGS.discount_factor, 'epsilon_decay_duration': FLAGS.epsilon_decay_duration, 'epsilon_start': FLAGS.epsilon_start, 'epsilon_end': FLAGS.epsilon_end}\n    with tf.Session() as sess:\n        agents = [nfsp.NFSP(sess, idx, info_state_size, num_actions, hidden_layers_sizes, **kwargs) for idx in range(num_players)]\n        joint_avg_policy = NFSPPolicies(env, agents, nfsp.MODE.average_policy)\n        sess.run(tf.global_variables_initializer())\n        if FLAGS.use_checkpoints:\n            for agent in agents:\n                if agent.has_checkpoint(FLAGS.checkpoint_dir):\n                    agent.restore(FLAGS.checkpoint_dir)\n        for ep in range(FLAGS.num_train_episodes):\n            if (ep + 1) % FLAGS.eval_every == 0:\n                losses = [agent.loss for agent in agents]\n                logging.info('Losses: %s', losses)\n                if FLAGS.evaluation_metric == 'exploitability':\n                    expl = exploitability.exploitability(env.game, joint_avg_policy)\n                    logging.info('[%s] Exploitability AVG %s', ep + 1, expl)\n                elif FLAGS.evaluation_metric == 'nash_conv':\n                    nash_conv = exploitability.nash_conv(env.game, joint_avg_policy)\n                    logging.info('[%s] NashConv %s', ep + 1, nash_conv)\n                else:\n                    raise ValueError(' '.join(('Invalid evaluation metric, choose from', \"'exploitability', 'nash_conv'.\")))\n                if FLAGS.use_checkpoints:\n                    for agent in agents:\n                        agent.save(FLAGS.checkpoint_dir)\n                logging.info('_____________________________________________')\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = agents[player_id].step(time_step)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n            for agent in agents:\n                agent.step(time_step)",
            "def main(unused_argv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    logging.info('Loading %s', FLAGS.game_name)\n    game = FLAGS.game_name\n    num_players = FLAGS.num_players\n    env_configs = {'players': num_players}\n    env = rl_environment.Environment(game, **env_configs)\n    info_state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    hidden_layers_sizes = [int(l) for l in FLAGS.hidden_layers_sizes]\n    kwargs = {'replay_buffer_capacity': FLAGS.replay_buffer_capacity, 'reservoir_buffer_capacity': FLAGS.reservoir_buffer_capacity, 'min_buffer_size_to_learn': FLAGS.min_buffer_size_to_learn, 'anticipatory_param': FLAGS.anticipatory_param, 'batch_size': FLAGS.batch_size, 'learn_every': FLAGS.learn_every, 'rl_learning_rate': FLAGS.rl_learning_rate, 'sl_learning_rate': FLAGS.sl_learning_rate, 'optimizer_str': FLAGS.optimizer_str, 'loss_str': FLAGS.loss_str, 'update_target_network_every': FLAGS.update_target_network_every, 'discount_factor': FLAGS.discount_factor, 'epsilon_decay_duration': FLAGS.epsilon_decay_duration, 'epsilon_start': FLAGS.epsilon_start, 'epsilon_end': FLAGS.epsilon_end}\n    with tf.Session() as sess:\n        agents = [nfsp.NFSP(sess, idx, info_state_size, num_actions, hidden_layers_sizes, **kwargs) for idx in range(num_players)]\n        joint_avg_policy = NFSPPolicies(env, agents, nfsp.MODE.average_policy)\n        sess.run(tf.global_variables_initializer())\n        if FLAGS.use_checkpoints:\n            for agent in agents:\n                if agent.has_checkpoint(FLAGS.checkpoint_dir):\n                    agent.restore(FLAGS.checkpoint_dir)\n        for ep in range(FLAGS.num_train_episodes):\n            if (ep + 1) % FLAGS.eval_every == 0:\n                losses = [agent.loss for agent in agents]\n                logging.info('Losses: %s', losses)\n                if FLAGS.evaluation_metric == 'exploitability':\n                    expl = exploitability.exploitability(env.game, joint_avg_policy)\n                    logging.info('[%s] Exploitability AVG %s', ep + 1, expl)\n                elif FLAGS.evaluation_metric == 'nash_conv':\n                    nash_conv = exploitability.nash_conv(env.game, joint_avg_policy)\n                    logging.info('[%s] NashConv %s', ep + 1, nash_conv)\n                else:\n                    raise ValueError(' '.join(('Invalid evaluation metric, choose from', \"'exploitability', 'nash_conv'.\")))\n                if FLAGS.use_checkpoints:\n                    for agent in agents:\n                        agent.save(FLAGS.checkpoint_dir)\n                logging.info('_____________________________________________')\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = agents[player_id].step(time_step)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n            for agent in agents:\n                agent.step(time_step)",
            "def main(unused_argv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    logging.info('Loading %s', FLAGS.game_name)\n    game = FLAGS.game_name\n    num_players = FLAGS.num_players\n    env_configs = {'players': num_players}\n    env = rl_environment.Environment(game, **env_configs)\n    info_state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    hidden_layers_sizes = [int(l) for l in FLAGS.hidden_layers_sizes]\n    kwargs = {'replay_buffer_capacity': FLAGS.replay_buffer_capacity, 'reservoir_buffer_capacity': FLAGS.reservoir_buffer_capacity, 'min_buffer_size_to_learn': FLAGS.min_buffer_size_to_learn, 'anticipatory_param': FLAGS.anticipatory_param, 'batch_size': FLAGS.batch_size, 'learn_every': FLAGS.learn_every, 'rl_learning_rate': FLAGS.rl_learning_rate, 'sl_learning_rate': FLAGS.sl_learning_rate, 'optimizer_str': FLAGS.optimizer_str, 'loss_str': FLAGS.loss_str, 'update_target_network_every': FLAGS.update_target_network_every, 'discount_factor': FLAGS.discount_factor, 'epsilon_decay_duration': FLAGS.epsilon_decay_duration, 'epsilon_start': FLAGS.epsilon_start, 'epsilon_end': FLAGS.epsilon_end}\n    with tf.Session() as sess:\n        agents = [nfsp.NFSP(sess, idx, info_state_size, num_actions, hidden_layers_sizes, **kwargs) for idx in range(num_players)]\n        joint_avg_policy = NFSPPolicies(env, agents, nfsp.MODE.average_policy)\n        sess.run(tf.global_variables_initializer())\n        if FLAGS.use_checkpoints:\n            for agent in agents:\n                if agent.has_checkpoint(FLAGS.checkpoint_dir):\n                    agent.restore(FLAGS.checkpoint_dir)\n        for ep in range(FLAGS.num_train_episodes):\n            if (ep + 1) % FLAGS.eval_every == 0:\n                losses = [agent.loss for agent in agents]\n                logging.info('Losses: %s', losses)\n                if FLAGS.evaluation_metric == 'exploitability':\n                    expl = exploitability.exploitability(env.game, joint_avg_policy)\n                    logging.info('[%s] Exploitability AVG %s', ep + 1, expl)\n                elif FLAGS.evaluation_metric == 'nash_conv':\n                    nash_conv = exploitability.nash_conv(env.game, joint_avg_policy)\n                    logging.info('[%s] NashConv %s', ep + 1, nash_conv)\n                else:\n                    raise ValueError(' '.join(('Invalid evaluation metric, choose from', \"'exploitability', 'nash_conv'.\")))\n                if FLAGS.use_checkpoints:\n                    for agent in agents:\n                        agent.save(FLAGS.checkpoint_dir)\n                logging.info('_____________________________________________')\n            time_step = env.reset()\n            while not time_step.last():\n                player_id = time_step.observations['current_player']\n                agent_output = agents[player_id].step(time_step)\n                action_list = [agent_output.action]\n                time_step = env.step(action_list)\n            for agent in agents:\n                agent.step(time_step)"
        ]
    }
]