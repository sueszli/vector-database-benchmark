[
    {
        "func_name": "__init__",
        "original": "def __init__(self, num_classes, in_channels, dim_channel, ori_channel, bbox_coder, loss_cls=dict(type='GaussionFocalLoss', loss_weight=1.0), loss_bbox=dict(type='L1Loss', loss_weight=0.1), loss_dir=None, loss_attr=None, norm_cfg=dict(type='GN', num_groups=32, requires_grad=True), init_cfg=None, **kwargs):\n    super().__init__(num_classes, in_channels, loss_cls=loss_cls, loss_bbox=loss_bbox, loss_dir=loss_dir, loss_attr=loss_attr, norm_cfg=norm_cfg, init_cfg=init_cfg, **kwargs)\n    self.dim_channel = dim_channel\n    self.ori_channel = ori_channel\n    self.bbox_coder = build_bbox_coder(bbox_coder)",
        "mutated": [
            "def __init__(self, num_classes, in_channels, dim_channel, ori_channel, bbox_coder, loss_cls=dict(type='GaussionFocalLoss', loss_weight=1.0), loss_bbox=dict(type='L1Loss', loss_weight=0.1), loss_dir=None, loss_attr=None, norm_cfg=dict(type='GN', num_groups=32, requires_grad=True), init_cfg=None, **kwargs):\n    if False:\n        i = 10\n    super().__init__(num_classes, in_channels, loss_cls=loss_cls, loss_bbox=loss_bbox, loss_dir=loss_dir, loss_attr=loss_attr, norm_cfg=norm_cfg, init_cfg=init_cfg, **kwargs)\n    self.dim_channel = dim_channel\n    self.ori_channel = ori_channel\n    self.bbox_coder = build_bbox_coder(bbox_coder)",
            "def __init__(self, num_classes, in_channels, dim_channel, ori_channel, bbox_coder, loss_cls=dict(type='GaussionFocalLoss', loss_weight=1.0), loss_bbox=dict(type='L1Loss', loss_weight=0.1), loss_dir=None, loss_attr=None, norm_cfg=dict(type='GN', num_groups=32, requires_grad=True), init_cfg=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(num_classes, in_channels, loss_cls=loss_cls, loss_bbox=loss_bbox, loss_dir=loss_dir, loss_attr=loss_attr, norm_cfg=norm_cfg, init_cfg=init_cfg, **kwargs)\n    self.dim_channel = dim_channel\n    self.ori_channel = ori_channel\n    self.bbox_coder = build_bbox_coder(bbox_coder)",
            "def __init__(self, num_classes, in_channels, dim_channel, ori_channel, bbox_coder, loss_cls=dict(type='GaussionFocalLoss', loss_weight=1.0), loss_bbox=dict(type='L1Loss', loss_weight=0.1), loss_dir=None, loss_attr=None, norm_cfg=dict(type='GN', num_groups=32, requires_grad=True), init_cfg=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(num_classes, in_channels, loss_cls=loss_cls, loss_bbox=loss_bbox, loss_dir=loss_dir, loss_attr=loss_attr, norm_cfg=norm_cfg, init_cfg=init_cfg, **kwargs)\n    self.dim_channel = dim_channel\n    self.ori_channel = ori_channel\n    self.bbox_coder = build_bbox_coder(bbox_coder)",
            "def __init__(self, num_classes, in_channels, dim_channel, ori_channel, bbox_coder, loss_cls=dict(type='GaussionFocalLoss', loss_weight=1.0), loss_bbox=dict(type='L1Loss', loss_weight=0.1), loss_dir=None, loss_attr=None, norm_cfg=dict(type='GN', num_groups=32, requires_grad=True), init_cfg=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(num_classes, in_channels, loss_cls=loss_cls, loss_bbox=loss_bbox, loss_dir=loss_dir, loss_attr=loss_attr, norm_cfg=norm_cfg, init_cfg=init_cfg, **kwargs)\n    self.dim_channel = dim_channel\n    self.ori_channel = ori_channel\n    self.bbox_coder = build_bbox_coder(bbox_coder)",
            "def __init__(self, num_classes, in_channels, dim_channel, ori_channel, bbox_coder, loss_cls=dict(type='GaussionFocalLoss', loss_weight=1.0), loss_bbox=dict(type='L1Loss', loss_weight=0.1), loss_dir=None, loss_attr=None, norm_cfg=dict(type='GN', num_groups=32, requires_grad=True), init_cfg=None, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(num_classes, in_channels, loss_cls=loss_cls, loss_bbox=loss_bbox, loss_dir=loss_dir, loss_attr=loss_attr, norm_cfg=norm_cfg, init_cfg=init_cfg, **kwargs)\n    self.dim_channel = dim_channel\n    self.ori_channel = ori_channel\n    self.bbox_coder = build_bbox_coder(bbox_coder)"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, feats):\n    \"\"\"Forward features from the upstream network.\n\n        Args:\n            feats (tuple[Tensor]): Features from the upstream network, each is\n                a 4D-tensor.\n\n        Returns:\n            tuple:\n                cls_scores (list[Tensor]): Box scores for each scale level,\n                    each is a 4D-tensor, the channel number is\n                    num_points * num_classes.\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\n                    level, each is a 4D-tensor, the channel number is\n                    num_points * bbox_code_size.\n        \"\"\"\n    return multi_apply(self.forward_single, feats)",
        "mutated": [
            "def forward(self, feats):\n    if False:\n        i = 10\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (tuple[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n        '\n    return multi_apply(self.forward_single, feats)",
            "def forward(self, feats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (tuple[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n        '\n    return multi_apply(self.forward_single, feats)",
            "def forward(self, feats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (tuple[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n        '\n    return multi_apply(self.forward_single, feats)",
            "def forward(self, feats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (tuple[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n        '\n    return multi_apply(self.forward_single, feats)",
            "def forward(self, feats):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Forward features from the upstream network.\\n\\n        Args:\\n            feats (tuple[Tensor]): Features from the upstream network, each is\\n                a 4D-tensor.\\n\\n        Returns:\\n            tuple:\\n                cls_scores (list[Tensor]): Box scores for each scale level,\\n                    each is a 4D-tensor, the channel number is\\n                    num_points * num_classes.\\n                bbox_preds (list[Tensor]): Box energies / deltas for each scale\\n                    level, each is a 4D-tensor, the channel number is\\n                    num_points * bbox_code_size.\\n        '\n    return multi_apply(self.forward_single, feats)"
        ]
    },
    {
        "func_name": "forward_single",
        "original": "def forward_single(self, x):\n    \"\"\"Forward features of a single scale level.\n\n        Args:\n            x (Tensor): Input feature map.\n\n        Returns:\n            tuple: Scores for each class, bbox of input feature maps.\n        \"\"\"\n    (cls_score, bbox_pred, dir_cls_pred, attr_pred, cls_feat, reg_feat) = super().forward_single(x)\n    cls_score = cls_score.sigmoid()\n    cls_score = cls_score.clamp(min=0.0001, max=1 - 0.0001)\n    offset_dims = bbox_pred[:, self.dim_channel, ...]\n    bbox_pred[:, self.dim_channel, ...] = offset_dims.sigmoid() - 0.5\n    vector_ori = bbox_pred[:, self.ori_channel, ...]\n    bbox_pred[:, self.ori_channel, ...] = F.normalize(vector_ori)\n    return (cls_score, bbox_pred)",
        "mutated": [
            "def forward_single(self, x):\n    if False:\n        i = 10\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): Input feature map.\\n\\n        Returns:\\n            tuple: Scores for each class, bbox of input feature maps.\\n        '\n    (cls_score, bbox_pred, dir_cls_pred, attr_pred, cls_feat, reg_feat) = super().forward_single(x)\n    cls_score = cls_score.sigmoid()\n    cls_score = cls_score.clamp(min=0.0001, max=1 - 0.0001)\n    offset_dims = bbox_pred[:, self.dim_channel, ...]\n    bbox_pred[:, self.dim_channel, ...] = offset_dims.sigmoid() - 0.5\n    vector_ori = bbox_pred[:, self.ori_channel, ...]\n    bbox_pred[:, self.ori_channel, ...] = F.normalize(vector_ori)\n    return (cls_score, bbox_pred)",
            "def forward_single(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): Input feature map.\\n\\n        Returns:\\n            tuple: Scores for each class, bbox of input feature maps.\\n        '\n    (cls_score, bbox_pred, dir_cls_pred, attr_pred, cls_feat, reg_feat) = super().forward_single(x)\n    cls_score = cls_score.sigmoid()\n    cls_score = cls_score.clamp(min=0.0001, max=1 - 0.0001)\n    offset_dims = bbox_pred[:, self.dim_channel, ...]\n    bbox_pred[:, self.dim_channel, ...] = offset_dims.sigmoid() - 0.5\n    vector_ori = bbox_pred[:, self.ori_channel, ...]\n    bbox_pred[:, self.ori_channel, ...] = F.normalize(vector_ori)\n    return (cls_score, bbox_pred)",
            "def forward_single(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): Input feature map.\\n\\n        Returns:\\n            tuple: Scores for each class, bbox of input feature maps.\\n        '\n    (cls_score, bbox_pred, dir_cls_pred, attr_pred, cls_feat, reg_feat) = super().forward_single(x)\n    cls_score = cls_score.sigmoid()\n    cls_score = cls_score.clamp(min=0.0001, max=1 - 0.0001)\n    offset_dims = bbox_pred[:, self.dim_channel, ...]\n    bbox_pred[:, self.dim_channel, ...] = offset_dims.sigmoid() - 0.5\n    vector_ori = bbox_pred[:, self.ori_channel, ...]\n    bbox_pred[:, self.ori_channel, ...] = F.normalize(vector_ori)\n    return (cls_score, bbox_pred)",
            "def forward_single(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): Input feature map.\\n\\n        Returns:\\n            tuple: Scores for each class, bbox of input feature maps.\\n        '\n    (cls_score, bbox_pred, dir_cls_pred, attr_pred, cls_feat, reg_feat) = super().forward_single(x)\n    cls_score = cls_score.sigmoid()\n    cls_score = cls_score.clamp(min=0.0001, max=1 - 0.0001)\n    offset_dims = bbox_pred[:, self.dim_channel, ...]\n    bbox_pred[:, self.dim_channel, ...] = offset_dims.sigmoid() - 0.5\n    vector_ori = bbox_pred[:, self.ori_channel, ...]\n    bbox_pred[:, self.ori_channel, ...] = F.normalize(vector_ori)\n    return (cls_score, bbox_pred)",
            "def forward_single(self, x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Forward features of a single scale level.\\n\\n        Args:\\n            x (Tensor): Input feature map.\\n\\n        Returns:\\n            tuple: Scores for each class, bbox of input feature maps.\\n        '\n    (cls_score, bbox_pred, dir_cls_pred, attr_pred, cls_feat, reg_feat) = super().forward_single(x)\n    cls_score = cls_score.sigmoid()\n    cls_score = cls_score.clamp(min=0.0001, max=1 - 0.0001)\n    offset_dims = bbox_pred[:, self.dim_channel, ...]\n    bbox_pred[:, self.dim_channel, ...] = offset_dims.sigmoid() - 0.5\n    vector_ori = bbox_pred[:, self.ori_channel, ...]\n    bbox_pred[:, self.ori_channel, ...] = F.normalize(vector_ori)\n    return (cls_score, bbox_pred)"
        ]
    },
    {
        "func_name": "get_bboxes",
        "original": "def get_bboxes(self, cls_scores, bbox_preds, img_metas, rescale=None):\n    \"\"\"Generate bboxes from bbox head predictions.\n\n        Args:\n            cls_scores (list[Tensor]): Box scores for each scale level.\n            bbox_preds (list[Tensor]): Box regression for each scale.\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            rescale (bool): If True, return boxes in original image space.\n\n        Returns:\n            list[tuple[:obj:`CameraInstance3DBoxes`, Tensor, Tensor, None]]:\n                Each item in result_list is 4-tuple.\n        \"\"\"\n    assert len(cls_scores) == len(bbox_preds) == 1\n    cam2imgs = torch.stack([cls_scores[0].new_tensor(img_meta['cam2img']) for img_meta in img_metas])\n    trans_mats = torch.stack([cls_scores[0].new_tensor(img_meta['trans_mat']) for img_meta in img_metas])\n    (batch_bboxes, batch_scores, batch_topk_labels) = self.decode_heatmap(cls_scores[0], bbox_preds[0], img_metas, cam2imgs=cam2imgs, trans_mats=trans_mats, topk=100, kernel=3)\n    result_list = []\n    for img_id in range(len(img_metas)):\n        bboxes = batch_bboxes[img_id]\n        scores = batch_scores[img_id]\n        labels = batch_topk_labels[img_id]\n        keep_idx = scores > 0.25\n        bboxes = bboxes[keep_idx]\n        scores = scores[keep_idx]\n        labels = labels[keep_idx]\n        bboxes = img_metas[img_id]['box_type_3d'](bboxes, box_dim=self.bbox_code_size, origin=(0.5, 0.5, 0.5))\n        attrs = None\n        result_list.append((bboxes, scores, labels, attrs))\n    return result_list",
        "mutated": [
            "def get_bboxes(self, cls_scores, bbox_preds, img_metas, rescale=None):\n    if False:\n        i = 10\n    'Generate bboxes from bbox head predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n            bbox_preds (list[Tensor]): Box regression for each scale.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            rescale (bool): If True, return boxes in original image space.\\n\\n        Returns:\\n            list[tuple[:obj:`CameraInstance3DBoxes`, Tensor, Tensor, None]]:\\n                Each item in result_list is 4-tuple.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == 1\n    cam2imgs = torch.stack([cls_scores[0].new_tensor(img_meta['cam2img']) for img_meta in img_metas])\n    trans_mats = torch.stack([cls_scores[0].new_tensor(img_meta['trans_mat']) for img_meta in img_metas])\n    (batch_bboxes, batch_scores, batch_topk_labels) = self.decode_heatmap(cls_scores[0], bbox_preds[0], img_metas, cam2imgs=cam2imgs, trans_mats=trans_mats, topk=100, kernel=3)\n    result_list = []\n    for img_id in range(len(img_metas)):\n        bboxes = batch_bboxes[img_id]\n        scores = batch_scores[img_id]\n        labels = batch_topk_labels[img_id]\n        keep_idx = scores > 0.25\n        bboxes = bboxes[keep_idx]\n        scores = scores[keep_idx]\n        labels = labels[keep_idx]\n        bboxes = img_metas[img_id]['box_type_3d'](bboxes, box_dim=self.bbox_code_size, origin=(0.5, 0.5, 0.5))\n        attrs = None\n        result_list.append((bboxes, scores, labels, attrs))\n    return result_list",
            "def get_bboxes(self, cls_scores, bbox_preds, img_metas, rescale=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Generate bboxes from bbox head predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n            bbox_preds (list[Tensor]): Box regression for each scale.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            rescale (bool): If True, return boxes in original image space.\\n\\n        Returns:\\n            list[tuple[:obj:`CameraInstance3DBoxes`, Tensor, Tensor, None]]:\\n                Each item in result_list is 4-tuple.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == 1\n    cam2imgs = torch.stack([cls_scores[0].new_tensor(img_meta['cam2img']) for img_meta in img_metas])\n    trans_mats = torch.stack([cls_scores[0].new_tensor(img_meta['trans_mat']) for img_meta in img_metas])\n    (batch_bboxes, batch_scores, batch_topk_labels) = self.decode_heatmap(cls_scores[0], bbox_preds[0], img_metas, cam2imgs=cam2imgs, trans_mats=trans_mats, topk=100, kernel=3)\n    result_list = []\n    for img_id in range(len(img_metas)):\n        bboxes = batch_bboxes[img_id]\n        scores = batch_scores[img_id]\n        labels = batch_topk_labels[img_id]\n        keep_idx = scores > 0.25\n        bboxes = bboxes[keep_idx]\n        scores = scores[keep_idx]\n        labels = labels[keep_idx]\n        bboxes = img_metas[img_id]['box_type_3d'](bboxes, box_dim=self.bbox_code_size, origin=(0.5, 0.5, 0.5))\n        attrs = None\n        result_list.append((bboxes, scores, labels, attrs))\n    return result_list",
            "def get_bboxes(self, cls_scores, bbox_preds, img_metas, rescale=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Generate bboxes from bbox head predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n            bbox_preds (list[Tensor]): Box regression for each scale.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            rescale (bool): If True, return boxes in original image space.\\n\\n        Returns:\\n            list[tuple[:obj:`CameraInstance3DBoxes`, Tensor, Tensor, None]]:\\n                Each item in result_list is 4-tuple.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == 1\n    cam2imgs = torch.stack([cls_scores[0].new_tensor(img_meta['cam2img']) for img_meta in img_metas])\n    trans_mats = torch.stack([cls_scores[0].new_tensor(img_meta['trans_mat']) for img_meta in img_metas])\n    (batch_bboxes, batch_scores, batch_topk_labels) = self.decode_heatmap(cls_scores[0], bbox_preds[0], img_metas, cam2imgs=cam2imgs, trans_mats=trans_mats, topk=100, kernel=3)\n    result_list = []\n    for img_id in range(len(img_metas)):\n        bboxes = batch_bboxes[img_id]\n        scores = batch_scores[img_id]\n        labels = batch_topk_labels[img_id]\n        keep_idx = scores > 0.25\n        bboxes = bboxes[keep_idx]\n        scores = scores[keep_idx]\n        labels = labels[keep_idx]\n        bboxes = img_metas[img_id]['box_type_3d'](bboxes, box_dim=self.bbox_code_size, origin=(0.5, 0.5, 0.5))\n        attrs = None\n        result_list.append((bboxes, scores, labels, attrs))\n    return result_list",
            "def get_bboxes(self, cls_scores, bbox_preds, img_metas, rescale=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Generate bboxes from bbox head predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n            bbox_preds (list[Tensor]): Box regression for each scale.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            rescale (bool): If True, return boxes in original image space.\\n\\n        Returns:\\n            list[tuple[:obj:`CameraInstance3DBoxes`, Tensor, Tensor, None]]:\\n                Each item in result_list is 4-tuple.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == 1\n    cam2imgs = torch.stack([cls_scores[0].new_tensor(img_meta['cam2img']) for img_meta in img_metas])\n    trans_mats = torch.stack([cls_scores[0].new_tensor(img_meta['trans_mat']) for img_meta in img_metas])\n    (batch_bboxes, batch_scores, batch_topk_labels) = self.decode_heatmap(cls_scores[0], bbox_preds[0], img_metas, cam2imgs=cam2imgs, trans_mats=trans_mats, topk=100, kernel=3)\n    result_list = []\n    for img_id in range(len(img_metas)):\n        bboxes = batch_bboxes[img_id]\n        scores = batch_scores[img_id]\n        labels = batch_topk_labels[img_id]\n        keep_idx = scores > 0.25\n        bboxes = bboxes[keep_idx]\n        scores = scores[keep_idx]\n        labels = labels[keep_idx]\n        bboxes = img_metas[img_id]['box_type_3d'](bboxes, box_dim=self.bbox_code_size, origin=(0.5, 0.5, 0.5))\n        attrs = None\n        result_list.append((bboxes, scores, labels, attrs))\n    return result_list",
            "def get_bboxes(self, cls_scores, bbox_preds, img_metas, rescale=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Generate bboxes from bbox head predictions.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n            bbox_preds (list[Tensor]): Box regression for each scale.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            rescale (bool): If True, return boxes in original image space.\\n\\n        Returns:\\n            list[tuple[:obj:`CameraInstance3DBoxes`, Tensor, Tensor, None]]:\\n                Each item in result_list is 4-tuple.\\n        '\n    assert len(cls_scores) == len(bbox_preds) == 1\n    cam2imgs = torch.stack([cls_scores[0].new_tensor(img_meta['cam2img']) for img_meta in img_metas])\n    trans_mats = torch.stack([cls_scores[0].new_tensor(img_meta['trans_mat']) for img_meta in img_metas])\n    (batch_bboxes, batch_scores, batch_topk_labels) = self.decode_heatmap(cls_scores[0], bbox_preds[0], img_metas, cam2imgs=cam2imgs, trans_mats=trans_mats, topk=100, kernel=3)\n    result_list = []\n    for img_id in range(len(img_metas)):\n        bboxes = batch_bboxes[img_id]\n        scores = batch_scores[img_id]\n        labels = batch_topk_labels[img_id]\n        keep_idx = scores > 0.25\n        bboxes = bboxes[keep_idx]\n        scores = scores[keep_idx]\n        labels = labels[keep_idx]\n        bboxes = img_metas[img_id]['box_type_3d'](bboxes, box_dim=self.bbox_code_size, origin=(0.5, 0.5, 0.5))\n        attrs = None\n        result_list.append((bboxes, scores, labels, attrs))\n    return result_list"
        ]
    },
    {
        "func_name": "decode_heatmap",
        "original": "def decode_heatmap(self, cls_score, reg_pred, img_metas, cam2imgs, trans_mats, topk=100, kernel=3):\n    \"\"\"Transform outputs into detections raw bbox predictions.\n\n        Args:\n            class_score (Tensor): Center predict heatmap,\n                shape (B, num_classes, H, W).\n            reg_pred (Tensor): Box regression map.\n                shape (B, channel, H , W).\n            img_metas (List[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            cam2imgs (Tensor): Camera intrinsic matrixs.\n                shape (B, 4, 4)\n            trans_mats (Tensor): Transformation matrix from original image\n                to feature map.\n                shape: (batch, 3, 3)\n            topk (int): Get top k center keypoints from heatmap. Default 100.\n            kernel (int): Max pooling kernel for extract local maximum pixels.\n               Default 3.\n\n        Returns:\n            tuple[torch.Tensor]: Decoded output of SMOKEHead, containing\n               the following Tensors:\n              - batch_bboxes (Tensor): Coords of each 3D box.\n                    shape (B, k, 7)\n              - batch_scores (Tensor): Scores of each 3D box.\n                    shape (B, k)\n              - batch_topk_labels (Tensor): Categories of each 3D box.\n                    shape (B, k)\n        \"\"\"\n    (img_h, img_w) = img_metas[0]['pad_shape'][:2]\n    (bs, _, feat_h, feat_w) = cls_score.shape\n    center_heatmap_pred = get_local_maximum(cls_score, kernel=kernel)\n    (*batch_dets, topk_ys, topk_xs) = get_topk_from_heatmap(center_heatmap_pred, k=topk)\n    (batch_scores, batch_index, batch_topk_labels) = batch_dets\n    regression = transpose_and_gather_feat(reg_pred, batch_index)\n    regression = regression.view(-1, 8)\n    points = torch.cat([topk_xs.view(-1, 1), topk_ys.view(-1, 1).float()], dim=1)\n    (locations, dimensions, orientations) = self.bbox_coder.decode(regression, points, batch_topk_labels, cam2imgs, trans_mats)\n    batch_bboxes = torch.cat((locations, dimensions, orientations), dim=1)\n    batch_bboxes = batch_bboxes.view(bs, -1, self.bbox_code_size)\n    return (batch_bboxes, batch_scores, batch_topk_labels)",
        "mutated": [
            "def decode_heatmap(self, cls_score, reg_pred, img_metas, cam2imgs, trans_mats, topk=100, kernel=3):\n    if False:\n        i = 10\n    'Transform outputs into detections raw bbox predictions.\\n\\n        Args:\\n            class_score (Tensor): Center predict heatmap,\\n                shape (B, num_classes, H, W).\\n            reg_pred (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            img_metas (List[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cam2imgs (Tensor): Camera intrinsic matrixs.\\n                shape (B, 4, 4)\\n            trans_mats (Tensor): Transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n            topk (int): Get top k center keypoints from heatmap. Default 100.\\n            kernel (int): Max pooling kernel for extract local maximum pixels.\\n               Default 3.\\n\\n        Returns:\\n            tuple[torch.Tensor]: Decoded output of SMOKEHead, containing\\n               the following Tensors:\\n              - batch_bboxes (Tensor): Coords of each 3D box.\\n                    shape (B, k, 7)\\n              - batch_scores (Tensor): Scores of each 3D box.\\n                    shape (B, k)\\n              - batch_topk_labels (Tensor): Categories of each 3D box.\\n                    shape (B, k)\\n        '\n    (img_h, img_w) = img_metas[0]['pad_shape'][:2]\n    (bs, _, feat_h, feat_w) = cls_score.shape\n    center_heatmap_pred = get_local_maximum(cls_score, kernel=kernel)\n    (*batch_dets, topk_ys, topk_xs) = get_topk_from_heatmap(center_heatmap_pred, k=topk)\n    (batch_scores, batch_index, batch_topk_labels) = batch_dets\n    regression = transpose_and_gather_feat(reg_pred, batch_index)\n    regression = regression.view(-1, 8)\n    points = torch.cat([topk_xs.view(-1, 1), topk_ys.view(-1, 1).float()], dim=1)\n    (locations, dimensions, orientations) = self.bbox_coder.decode(regression, points, batch_topk_labels, cam2imgs, trans_mats)\n    batch_bboxes = torch.cat((locations, dimensions, orientations), dim=1)\n    batch_bboxes = batch_bboxes.view(bs, -1, self.bbox_code_size)\n    return (batch_bboxes, batch_scores, batch_topk_labels)",
            "def decode_heatmap(self, cls_score, reg_pred, img_metas, cam2imgs, trans_mats, topk=100, kernel=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Transform outputs into detections raw bbox predictions.\\n\\n        Args:\\n            class_score (Tensor): Center predict heatmap,\\n                shape (B, num_classes, H, W).\\n            reg_pred (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            img_metas (List[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cam2imgs (Tensor): Camera intrinsic matrixs.\\n                shape (B, 4, 4)\\n            trans_mats (Tensor): Transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n            topk (int): Get top k center keypoints from heatmap. Default 100.\\n            kernel (int): Max pooling kernel for extract local maximum pixels.\\n               Default 3.\\n\\n        Returns:\\n            tuple[torch.Tensor]: Decoded output of SMOKEHead, containing\\n               the following Tensors:\\n              - batch_bboxes (Tensor): Coords of each 3D box.\\n                    shape (B, k, 7)\\n              - batch_scores (Tensor): Scores of each 3D box.\\n                    shape (B, k)\\n              - batch_topk_labels (Tensor): Categories of each 3D box.\\n                    shape (B, k)\\n        '\n    (img_h, img_w) = img_metas[0]['pad_shape'][:2]\n    (bs, _, feat_h, feat_w) = cls_score.shape\n    center_heatmap_pred = get_local_maximum(cls_score, kernel=kernel)\n    (*batch_dets, topk_ys, topk_xs) = get_topk_from_heatmap(center_heatmap_pred, k=topk)\n    (batch_scores, batch_index, batch_topk_labels) = batch_dets\n    regression = transpose_and_gather_feat(reg_pred, batch_index)\n    regression = regression.view(-1, 8)\n    points = torch.cat([topk_xs.view(-1, 1), topk_ys.view(-1, 1).float()], dim=1)\n    (locations, dimensions, orientations) = self.bbox_coder.decode(regression, points, batch_topk_labels, cam2imgs, trans_mats)\n    batch_bboxes = torch.cat((locations, dimensions, orientations), dim=1)\n    batch_bboxes = batch_bboxes.view(bs, -1, self.bbox_code_size)\n    return (batch_bboxes, batch_scores, batch_topk_labels)",
            "def decode_heatmap(self, cls_score, reg_pred, img_metas, cam2imgs, trans_mats, topk=100, kernel=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Transform outputs into detections raw bbox predictions.\\n\\n        Args:\\n            class_score (Tensor): Center predict heatmap,\\n                shape (B, num_classes, H, W).\\n            reg_pred (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            img_metas (List[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cam2imgs (Tensor): Camera intrinsic matrixs.\\n                shape (B, 4, 4)\\n            trans_mats (Tensor): Transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n            topk (int): Get top k center keypoints from heatmap. Default 100.\\n            kernel (int): Max pooling kernel for extract local maximum pixels.\\n               Default 3.\\n\\n        Returns:\\n            tuple[torch.Tensor]: Decoded output of SMOKEHead, containing\\n               the following Tensors:\\n              - batch_bboxes (Tensor): Coords of each 3D box.\\n                    shape (B, k, 7)\\n              - batch_scores (Tensor): Scores of each 3D box.\\n                    shape (B, k)\\n              - batch_topk_labels (Tensor): Categories of each 3D box.\\n                    shape (B, k)\\n        '\n    (img_h, img_w) = img_metas[0]['pad_shape'][:2]\n    (bs, _, feat_h, feat_w) = cls_score.shape\n    center_heatmap_pred = get_local_maximum(cls_score, kernel=kernel)\n    (*batch_dets, topk_ys, topk_xs) = get_topk_from_heatmap(center_heatmap_pred, k=topk)\n    (batch_scores, batch_index, batch_topk_labels) = batch_dets\n    regression = transpose_and_gather_feat(reg_pred, batch_index)\n    regression = regression.view(-1, 8)\n    points = torch.cat([topk_xs.view(-1, 1), topk_ys.view(-1, 1).float()], dim=1)\n    (locations, dimensions, orientations) = self.bbox_coder.decode(regression, points, batch_topk_labels, cam2imgs, trans_mats)\n    batch_bboxes = torch.cat((locations, dimensions, orientations), dim=1)\n    batch_bboxes = batch_bboxes.view(bs, -1, self.bbox_code_size)\n    return (batch_bboxes, batch_scores, batch_topk_labels)",
            "def decode_heatmap(self, cls_score, reg_pred, img_metas, cam2imgs, trans_mats, topk=100, kernel=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Transform outputs into detections raw bbox predictions.\\n\\n        Args:\\n            class_score (Tensor): Center predict heatmap,\\n                shape (B, num_classes, H, W).\\n            reg_pred (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            img_metas (List[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cam2imgs (Tensor): Camera intrinsic matrixs.\\n                shape (B, 4, 4)\\n            trans_mats (Tensor): Transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n            topk (int): Get top k center keypoints from heatmap. Default 100.\\n            kernel (int): Max pooling kernel for extract local maximum pixels.\\n               Default 3.\\n\\n        Returns:\\n            tuple[torch.Tensor]: Decoded output of SMOKEHead, containing\\n               the following Tensors:\\n              - batch_bboxes (Tensor): Coords of each 3D box.\\n                    shape (B, k, 7)\\n              - batch_scores (Tensor): Scores of each 3D box.\\n                    shape (B, k)\\n              - batch_topk_labels (Tensor): Categories of each 3D box.\\n                    shape (B, k)\\n        '\n    (img_h, img_w) = img_metas[0]['pad_shape'][:2]\n    (bs, _, feat_h, feat_w) = cls_score.shape\n    center_heatmap_pred = get_local_maximum(cls_score, kernel=kernel)\n    (*batch_dets, topk_ys, topk_xs) = get_topk_from_heatmap(center_heatmap_pred, k=topk)\n    (batch_scores, batch_index, batch_topk_labels) = batch_dets\n    regression = transpose_and_gather_feat(reg_pred, batch_index)\n    regression = regression.view(-1, 8)\n    points = torch.cat([topk_xs.view(-1, 1), topk_ys.view(-1, 1).float()], dim=1)\n    (locations, dimensions, orientations) = self.bbox_coder.decode(regression, points, batch_topk_labels, cam2imgs, trans_mats)\n    batch_bboxes = torch.cat((locations, dimensions, orientations), dim=1)\n    batch_bboxes = batch_bboxes.view(bs, -1, self.bbox_code_size)\n    return (batch_bboxes, batch_scores, batch_topk_labels)",
            "def decode_heatmap(self, cls_score, reg_pred, img_metas, cam2imgs, trans_mats, topk=100, kernel=3):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Transform outputs into detections raw bbox predictions.\\n\\n        Args:\\n            class_score (Tensor): Center predict heatmap,\\n                shape (B, num_classes, H, W).\\n            reg_pred (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n            img_metas (List[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            cam2imgs (Tensor): Camera intrinsic matrixs.\\n                shape (B, 4, 4)\\n            trans_mats (Tensor): Transformation matrix from original image\\n                to feature map.\\n                shape: (batch, 3, 3)\\n            topk (int): Get top k center keypoints from heatmap. Default 100.\\n            kernel (int): Max pooling kernel for extract local maximum pixels.\\n               Default 3.\\n\\n        Returns:\\n            tuple[torch.Tensor]: Decoded output of SMOKEHead, containing\\n               the following Tensors:\\n              - batch_bboxes (Tensor): Coords of each 3D box.\\n                    shape (B, k, 7)\\n              - batch_scores (Tensor): Scores of each 3D box.\\n                    shape (B, k)\\n              - batch_topk_labels (Tensor): Categories of each 3D box.\\n                    shape (B, k)\\n        '\n    (img_h, img_w) = img_metas[0]['pad_shape'][:2]\n    (bs, _, feat_h, feat_w) = cls_score.shape\n    center_heatmap_pred = get_local_maximum(cls_score, kernel=kernel)\n    (*batch_dets, topk_ys, topk_xs) = get_topk_from_heatmap(center_heatmap_pred, k=topk)\n    (batch_scores, batch_index, batch_topk_labels) = batch_dets\n    regression = transpose_and_gather_feat(reg_pred, batch_index)\n    regression = regression.view(-1, 8)\n    points = torch.cat([topk_xs.view(-1, 1), topk_ys.view(-1, 1).float()], dim=1)\n    (locations, dimensions, orientations) = self.bbox_coder.decode(regression, points, batch_topk_labels, cam2imgs, trans_mats)\n    batch_bboxes = torch.cat((locations, dimensions, orientations), dim=1)\n    batch_bboxes = batch_bboxes.view(bs, -1, self.bbox_code_size)\n    return (batch_bboxes, batch_scores, batch_topk_labels)"
        ]
    },
    {
        "func_name": "get_predictions",
        "original": "def get_predictions(self, labels3d, centers2d, gt_locations, gt_dimensions, gt_orientations, indices, img_metas, pred_reg):\n    \"\"\"Prepare predictions for computing loss.\n\n        Args:\n            labels3d (Tensor): Labels of each 3D box.\n                shape (B, max_objs, )\n            centers2d (Tensor): Coords of each projected 3D box\n                center on image. shape (B * max_objs, 2)\n            gt_locations (Tensor): Coords of each 3D box's location.\n                shape (B * max_objs, 3)\n            gt_dimensions (Tensor): Dimensions of each 3D box.\n                shape (N, 3)\n            gt_orientations (Tensor): Orientation(yaw) of each 3D box.\n                shape (N, 1)\n            indices (Tensor): Indices of the existence of the 3D box.\n                shape (B * max_objs, )\n            img_metas (list[dict]): Meta information of each image,\n                e.g., image size, scaling factor, etc.\n            pre_reg (Tensor): Box regression map.\n                shape (B, channel, H , W).\n\n        Returns:\n            dict: the dict has components below:\n            - bbox3d_yaws (:obj:`CameraInstance3DBoxes`):\n                bbox calculated using pred orientations.\n            - bbox3d_dims (:obj:`CameraInstance3DBoxes`):\n                bbox calculated using pred dimensions.\n            - bbox3d_locs (:obj:`CameraInstance3DBoxes`):\n                bbox calculated using pred locations.\n        \"\"\"\n    (batch, channel) = (pred_reg.shape[0], pred_reg.shape[1])\n    w = pred_reg.shape[3]\n    cam2imgs = torch.stack([gt_locations.new_tensor(img_meta['cam2img']) for img_meta in img_metas])\n    trans_mats = torch.stack([gt_locations.new_tensor(img_meta['trans_mat']) for img_meta in img_metas])\n    centers2d_inds = centers2d[:, 1] * w + centers2d[:, 0]\n    centers2d_inds = centers2d_inds.view(batch, -1)\n    pred_regression = transpose_and_gather_feat(pred_reg, centers2d_inds)\n    pred_regression_pois = pred_regression.view(-1, channel)\n    (locations, dimensions, orientations) = self.bbox_coder.decode(pred_regression_pois, centers2d, labels3d, cam2imgs, trans_mats, gt_locations)\n    (locations, dimensions, orientations) = (locations[indices], dimensions[indices], orientations[indices])\n    locations[:, 1] += dimensions[:, 1] / 2\n    gt_locations = gt_locations[indices]\n    assert len(locations) == len(gt_locations)\n    assert len(dimensions) == len(gt_dimensions)\n    assert len(orientations) == len(gt_orientations)\n    bbox3d_yaws = self.bbox_coder.encode(gt_locations, gt_dimensions, orientations, img_metas)\n    bbox3d_dims = self.bbox_coder.encode(gt_locations, dimensions, gt_orientations, img_metas)\n    bbox3d_locs = self.bbox_coder.encode(locations, gt_dimensions, gt_orientations, img_metas)\n    pred_bboxes = dict(ori=bbox3d_yaws, dim=bbox3d_dims, loc=bbox3d_locs)\n    return pred_bboxes",
        "mutated": [
            "def get_predictions(self, labels3d, centers2d, gt_locations, gt_dimensions, gt_orientations, indices, img_metas, pred_reg):\n    if False:\n        i = 10\n    \"Prepare predictions for computing loss.\\n\\n        Args:\\n            labels3d (Tensor): Labels of each 3D box.\\n                shape (B, max_objs, )\\n            centers2d (Tensor): Coords of each projected 3D box\\n                center on image. shape (B * max_objs, 2)\\n            gt_locations (Tensor): Coords of each 3D box's location.\\n                shape (B * max_objs, 3)\\n            gt_dimensions (Tensor): Dimensions of each 3D box.\\n                shape (N, 3)\\n            gt_orientations (Tensor): Orientation(yaw) of each 3D box.\\n                shape (N, 1)\\n            indices (Tensor): Indices of the existence of the 3D box.\\n                shape (B * max_objs, )\\n            img_metas (list[dict]): Meta information of each image,\\n                e.g., image size, scaling factor, etc.\\n            pre_reg (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n\\n        Returns:\\n            dict: the dict has components below:\\n            - bbox3d_yaws (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred orientations.\\n            - bbox3d_dims (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred dimensions.\\n            - bbox3d_locs (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred locations.\\n        \"\n    (batch, channel) = (pred_reg.shape[0], pred_reg.shape[1])\n    w = pred_reg.shape[3]\n    cam2imgs = torch.stack([gt_locations.new_tensor(img_meta['cam2img']) for img_meta in img_metas])\n    trans_mats = torch.stack([gt_locations.new_tensor(img_meta['trans_mat']) for img_meta in img_metas])\n    centers2d_inds = centers2d[:, 1] * w + centers2d[:, 0]\n    centers2d_inds = centers2d_inds.view(batch, -1)\n    pred_regression = transpose_and_gather_feat(pred_reg, centers2d_inds)\n    pred_regression_pois = pred_regression.view(-1, channel)\n    (locations, dimensions, orientations) = self.bbox_coder.decode(pred_regression_pois, centers2d, labels3d, cam2imgs, trans_mats, gt_locations)\n    (locations, dimensions, orientations) = (locations[indices], dimensions[indices], orientations[indices])\n    locations[:, 1] += dimensions[:, 1] / 2\n    gt_locations = gt_locations[indices]\n    assert len(locations) == len(gt_locations)\n    assert len(dimensions) == len(gt_dimensions)\n    assert len(orientations) == len(gt_orientations)\n    bbox3d_yaws = self.bbox_coder.encode(gt_locations, gt_dimensions, orientations, img_metas)\n    bbox3d_dims = self.bbox_coder.encode(gt_locations, dimensions, gt_orientations, img_metas)\n    bbox3d_locs = self.bbox_coder.encode(locations, gt_dimensions, gt_orientations, img_metas)\n    pred_bboxes = dict(ori=bbox3d_yaws, dim=bbox3d_dims, loc=bbox3d_locs)\n    return pred_bboxes",
            "def get_predictions(self, labels3d, centers2d, gt_locations, gt_dimensions, gt_orientations, indices, img_metas, pred_reg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Prepare predictions for computing loss.\\n\\n        Args:\\n            labels3d (Tensor): Labels of each 3D box.\\n                shape (B, max_objs, )\\n            centers2d (Tensor): Coords of each projected 3D box\\n                center on image. shape (B * max_objs, 2)\\n            gt_locations (Tensor): Coords of each 3D box's location.\\n                shape (B * max_objs, 3)\\n            gt_dimensions (Tensor): Dimensions of each 3D box.\\n                shape (N, 3)\\n            gt_orientations (Tensor): Orientation(yaw) of each 3D box.\\n                shape (N, 1)\\n            indices (Tensor): Indices of the existence of the 3D box.\\n                shape (B * max_objs, )\\n            img_metas (list[dict]): Meta information of each image,\\n                e.g., image size, scaling factor, etc.\\n            pre_reg (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n\\n        Returns:\\n            dict: the dict has components below:\\n            - bbox3d_yaws (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred orientations.\\n            - bbox3d_dims (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred dimensions.\\n            - bbox3d_locs (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred locations.\\n        \"\n    (batch, channel) = (pred_reg.shape[0], pred_reg.shape[1])\n    w = pred_reg.shape[3]\n    cam2imgs = torch.stack([gt_locations.new_tensor(img_meta['cam2img']) for img_meta in img_metas])\n    trans_mats = torch.stack([gt_locations.new_tensor(img_meta['trans_mat']) for img_meta in img_metas])\n    centers2d_inds = centers2d[:, 1] * w + centers2d[:, 0]\n    centers2d_inds = centers2d_inds.view(batch, -1)\n    pred_regression = transpose_and_gather_feat(pred_reg, centers2d_inds)\n    pred_regression_pois = pred_regression.view(-1, channel)\n    (locations, dimensions, orientations) = self.bbox_coder.decode(pred_regression_pois, centers2d, labels3d, cam2imgs, trans_mats, gt_locations)\n    (locations, dimensions, orientations) = (locations[indices], dimensions[indices], orientations[indices])\n    locations[:, 1] += dimensions[:, 1] / 2\n    gt_locations = gt_locations[indices]\n    assert len(locations) == len(gt_locations)\n    assert len(dimensions) == len(gt_dimensions)\n    assert len(orientations) == len(gt_orientations)\n    bbox3d_yaws = self.bbox_coder.encode(gt_locations, gt_dimensions, orientations, img_metas)\n    bbox3d_dims = self.bbox_coder.encode(gt_locations, dimensions, gt_orientations, img_metas)\n    bbox3d_locs = self.bbox_coder.encode(locations, gt_dimensions, gt_orientations, img_metas)\n    pred_bboxes = dict(ori=bbox3d_yaws, dim=bbox3d_dims, loc=bbox3d_locs)\n    return pred_bboxes",
            "def get_predictions(self, labels3d, centers2d, gt_locations, gt_dimensions, gt_orientations, indices, img_metas, pred_reg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Prepare predictions for computing loss.\\n\\n        Args:\\n            labels3d (Tensor): Labels of each 3D box.\\n                shape (B, max_objs, )\\n            centers2d (Tensor): Coords of each projected 3D box\\n                center on image. shape (B * max_objs, 2)\\n            gt_locations (Tensor): Coords of each 3D box's location.\\n                shape (B * max_objs, 3)\\n            gt_dimensions (Tensor): Dimensions of each 3D box.\\n                shape (N, 3)\\n            gt_orientations (Tensor): Orientation(yaw) of each 3D box.\\n                shape (N, 1)\\n            indices (Tensor): Indices of the existence of the 3D box.\\n                shape (B * max_objs, )\\n            img_metas (list[dict]): Meta information of each image,\\n                e.g., image size, scaling factor, etc.\\n            pre_reg (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n\\n        Returns:\\n            dict: the dict has components below:\\n            - bbox3d_yaws (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred orientations.\\n            - bbox3d_dims (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred dimensions.\\n            - bbox3d_locs (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred locations.\\n        \"\n    (batch, channel) = (pred_reg.shape[0], pred_reg.shape[1])\n    w = pred_reg.shape[3]\n    cam2imgs = torch.stack([gt_locations.new_tensor(img_meta['cam2img']) for img_meta in img_metas])\n    trans_mats = torch.stack([gt_locations.new_tensor(img_meta['trans_mat']) for img_meta in img_metas])\n    centers2d_inds = centers2d[:, 1] * w + centers2d[:, 0]\n    centers2d_inds = centers2d_inds.view(batch, -1)\n    pred_regression = transpose_and_gather_feat(pred_reg, centers2d_inds)\n    pred_regression_pois = pred_regression.view(-1, channel)\n    (locations, dimensions, orientations) = self.bbox_coder.decode(pred_regression_pois, centers2d, labels3d, cam2imgs, trans_mats, gt_locations)\n    (locations, dimensions, orientations) = (locations[indices], dimensions[indices], orientations[indices])\n    locations[:, 1] += dimensions[:, 1] / 2\n    gt_locations = gt_locations[indices]\n    assert len(locations) == len(gt_locations)\n    assert len(dimensions) == len(gt_dimensions)\n    assert len(orientations) == len(gt_orientations)\n    bbox3d_yaws = self.bbox_coder.encode(gt_locations, gt_dimensions, orientations, img_metas)\n    bbox3d_dims = self.bbox_coder.encode(gt_locations, dimensions, gt_orientations, img_metas)\n    bbox3d_locs = self.bbox_coder.encode(locations, gt_dimensions, gt_orientations, img_metas)\n    pred_bboxes = dict(ori=bbox3d_yaws, dim=bbox3d_dims, loc=bbox3d_locs)\n    return pred_bboxes",
            "def get_predictions(self, labels3d, centers2d, gt_locations, gt_dimensions, gt_orientations, indices, img_metas, pred_reg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Prepare predictions for computing loss.\\n\\n        Args:\\n            labels3d (Tensor): Labels of each 3D box.\\n                shape (B, max_objs, )\\n            centers2d (Tensor): Coords of each projected 3D box\\n                center on image. shape (B * max_objs, 2)\\n            gt_locations (Tensor): Coords of each 3D box's location.\\n                shape (B * max_objs, 3)\\n            gt_dimensions (Tensor): Dimensions of each 3D box.\\n                shape (N, 3)\\n            gt_orientations (Tensor): Orientation(yaw) of each 3D box.\\n                shape (N, 1)\\n            indices (Tensor): Indices of the existence of the 3D box.\\n                shape (B * max_objs, )\\n            img_metas (list[dict]): Meta information of each image,\\n                e.g., image size, scaling factor, etc.\\n            pre_reg (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n\\n        Returns:\\n            dict: the dict has components below:\\n            - bbox3d_yaws (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred orientations.\\n            - bbox3d_dims (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred dimensions.\\n            - bbox3d_locs (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred locations.\\n        \"\n    (batch, channel) = (pred_reg.shape[0], pred_reg.shape[1])\n    w = pred_reg.shape[3]\n    cam2imgs = torch.stack([gt_locations.new_tensor(img_meta['cam2img']) for img_meta in img_metas])\n    trans_mats = torch.stack([gt_locations.new_tensor(img_meta['trans_mat']) for img_meta in img_metas])\n    centers2d_inds = centers2d[:, 1] * w + centers2d[:, 0]\n    centers2d_inds = centers2d_inds.view(batch, -1)\n    pred_regression = transpose_and_gather_feat(pred_reg, centers2d_inds)\n    pred_regression_pois = pred_regression.view(-1, channel)\n    (locations, dimensions, orientations) = self.bbox_coder.decode(pred_regression_pois, centers2d, labels3d, cam2imgs, trans_mats, gt_locations)\n    (locations, dimensions, orientations) = (locations[indices], dimensions[indices], orientations[indices])\n    locations[:, 1] += dimensions[:, 1] / 2\n    gt_locations = gt_locations[indices]\n    assert len(locations) == len(gt_locations)\n    assert len(dimensions) == len(gt_dimensions)\n    assert len(orientations) == len(gt_orientations)\n    bbox3d_yaws = self.bbox_coder.encode(gt_locations, gt_dimensions, orientations, img_metas)\n    bbox3d_dims = self.bbox_coder.encode(gt_locations, dimensions, gt_orientations, img_metas)\n    bbox3d_locs = self.bbox_coder.encode(locations, gt_dimensions, gt_orientations, img_metas)\n    pred_bboxes = dict(ori=bbox3d_yaws, dim=bbox3d_dims, loc=bbox3d_locs)\n    return pred_bboxes",
            "def get_predictions(self, labels3d, centers2d, gt_locations, gt_dimensions, gt_orientations, indices, img_metas, pred_reg):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Prepare predictions for computing loss.\\n\\n        Args:\\n            labels3d (Tensor): Labels of each 3D box.\\n                shape (B, max_objs, )\\n            centers2d (Tensor): Coords of each projected 3D box\\n                center on image. shape (B * max_objs, 2)\\n            gt_locations (Tensor): Coords of each 3D box's location.\\n                shape (B * max_objs, 3)\\n            gt_dimensions (Tensor): Dimensions of each 3D box.\\n                shape (N, 3)\\n            gt_orientations (Tensor): Orientation(yaw) of each 3D box.\\n                shape (N, 1)\\n            indices (Tensor): Indices of the existence of the 3D box.\\n                shape (B * max_objs, )\\n            img_metas (list[dict]): Meta information of each image,\\n                e.g., image size, scaling factor, etc.\\n            pre_reg (Tensor): Box regression map.\\n                shape (B, channel, H , W).\\n\\n        Returns:\\n            dict: the dict has components below:\\n            - bbox3d_yaws (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred orientations.\\n            - bbox3d_dims (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred dimensions.\\n            - bbox3d_locs (:obj:`CameraInstance3DBoxes`):\\n                bbox calculated using pred locations.\\n        \"\n    (batch, channel) = (pred_reg.shape[0], pred_reg.shape[1])\n    w = pred_reg.shape[3]\n    cam2imgs = torch.stack([gt_locations.new_tensor(img_meta['cam2img']) for img_meta in img_metas])\n    trans_mats = torch.stack([gt_locations.new_tensor(img_meta['trans_mat']) for img_meta in img_metas])\n    centers2d_inds = centers2d[:, 1] * w + centers2d[:, 0]\n    centers2d_inds = centers2d_inds.view(batch, -1)\n    pred_regression = transpose_and_gather_feat(pred_reg, centers2d_inds)\n    pred_regression_pois = pred_regression.view(-1, channel)\n    (locations, dimensions, orientations) = self.bbox_coder.decode(pred_regression_pois, centers2d, labels3d, cam2imgs, trans_mats, gt_locations)\n    (locations, dimensions, orientations) = (locations[indices], dimensions[indices], orientations[indices])\n    locations[:, 1] += dimensions[:, 1] / 2\n    gt_locations = gt_locations[indices]\n    assert len(locations) == len(gt_locations)\n    assert len(dimensions) == len(gt_dimensions)\n    assert len(orientations) == len(gt_orientations)\n    bbox3d_yaws = self.bbox_coder.encode(gt_locations, gt_dimensions, orientations, img_metas)\n    bbox3d_dims = self.bbox_coder.encode(gt_locations, dimensions, gt_orientations, img_metas)\n    bbox3d_locs = self.bbox_coder.encode(locations, gt_dimensions, gt_orientations, img_metas)\n    pred_bboxes = dict(ori=bbox3d_yaws, dim=bbox3d_dims, loc=bbox3d_locs)\n    return pred_bboxes"
        ]
    },
    {
        "func_name": "get_targets",
        "original": "def get_targets(self, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, feat_shape, img_shape, img_metas):\n    \"\"\"Get training targets for batch images.\n\n        Args:\n            gt_bboxes (list[Tensor]): Ground truth bboxes of each image,\n                shape (num_gt, 4).\n            gt_labels (list[Tensor]): Ground truth labels of each box,\n                shape (num_gt,).\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D Ground\n                truth bboxes of each image,\n                shape (num_gt, bbox_code_size).\n            gt_labels_3d (list[Tensor]): 3D Ground truth labels of each\n                box, shape (num_gt,).\n            centers2d (list[Tensor]): Projected 3D centers onto 2D image,\n                shape (num_gt, 2).\n            feat_shape (tuple[int]): Feature map shape with value,\n                shape (B, _, H, W).\n            img_shape (tuple[int]): Image shape in [h, w] format.\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n\n        Returns:\n            tuple[Tensor, dict]: The Tensor value is the targets of\n                center heatmap, the dict has components below:\n              - gt_centers2d (Tensor): Coords of each projected 3D box\n                    center on image. shape (B * max_objs, 2)\n              - gt_labels3d (Tensor): Labels of each 3D box.\n                    shape (B, max_objs, )\n              - indices (Tensor): Indices of the existence of the 3D box.\n                    shape (B * max_objs, )\n              - affine_indices (Tensor): Indices of the affine of the 3D box.\n                    shape (N, )\n              - gt_locs (Tensor): Coords of each 3D box's location.\n                    shape (N, 3)\n              - gt_dims (Tensor): Dimensions of each 3D box.\n                    shape (N, 3)\n              - gt_yaws (Tensor): Orientation(yaw) of each 3D box.\n                    shape (N, 1)\n              - gt_cors (Tensor): Coords of the corners of each 3D box.\n                    shape (N, 8, 3)\n        \"\"\"\n    reg_mask = torch.stack([gt_bboxes[0].new_tensor(not img_meta['affine_aug'], dtype=torch.bool) for img_meta in img_metas])\n    (img_h, img_w) = img_shape[:2]\n    (bs, _, feat_h, feat_w) = feat_shape\n    width_ratio = float(feat_w / img_w)\n    height_ratio = float(feat_h / img_h)\n    assert width_ratio == height_ratio\n    center_heatmap_target = gt_bboxes[-1].new_zeros([bs, self.num_classes, feat_h, feat_w])\n    gt_centers2d = centers2d.copy()\n    for batch_id in range(bs):\n        gt_bbox = gt_bboxes[batch_id]\n        gt_label = gt_labels[batch_id]\n        gt_center2d = gt_centers2d[batch_id] * width_ratio\n        for (j, center) in enumerate(gt_center2d):\n            (center_x_int, center_y_int) = center.int()\n            scale_box_h = (gt_bbox[j][3] - gt_bbox[j][1]) * height_ratio\n            scale_box_w = (gt_bbox[j][2] - gt_bbox[j][0]) * width_ratio\n            radius = gaussian_radius([scale_box_h, scale_box_w], min_overlap=0.7)\n            radius = max(0, int(radius))\n            ind = gt_label[j]\n            gen_gaussian_target(center_heatmap_target[batch_id, ind], [center_x_int, center_y_int], radius)\n    avg_factor = max(1, center_heatmap_target.eq(1).sum())\n    num_ctrs = [center2d.shape[0] for center2d in centers2d]\n    max_objs = max(num_ctrs)\n    reg_inds = torch.cat([reg_mask[i].repeat(num_ctrs[i]) for i in range(bs)])\n    inds = torch.zeros((bs, max_objs), dtype=torch.bool).to(centers2d[0].device)\n    gt_bboxes_3d = [gt_bbox_3d.to(centers2d[0].device) for gt_bbox_3d in gt_bboxes_3d]\n    batch_centers2d = centers2d[0].new_zeros((bs, max_objs, 2))\n    batch_labels_3d = gt_labels_3d[0].new_zeros((bs, max_objs))\n    batch_gt_locations = gt_bboxes_3d[0].tensor.new_zeros((bs, max_objs, 3))\n    for i in range(bs):\n        inds[i, :num_ctrs[i]] = 1\n        batch_centers2d[i, :num_ctrs[i]] = centers2d[i]\n        batch_labels_3d[i, :num_ctrs[i]] = gt_labels_3d[i]\n        batch_gt_locations[i, :num_ctrs[i]] = gt_bboxes_3d[i].tensor[:, :3]\n    inds = inds.flatten()\n    batch_centers2d = batch_centers2d.view(-1, 2) * width_ratio\n    batch_gt_locations = batch_gt_locations.view(-1, 3)\n    gt_bboxes_3d = [gt_bbox_3d for gt_bbox_3d in gt_bboxes_3d if gt_bbox_3d.tensor.shape[0] > 0]\n    gt_dimensions = torch.cat([gt_bbox_3d.tensor[:, 3:6] for gt_bbox_3d in gt_bboxes_3d])\n    gt_orientations = torch.cat([gt_bbox_3d.tensor[:, 6].unsqueeze(-1) for gt_bbox_3d in gt_bboxes_3d])\n    gt_corners = torch.cat([gt_bbox_3d.corners for gt_bbox_3d in gt_bboxes_3d])\n    target_labels = dict(gt_centers2d=batch_centers2d.long(), gt_labels3d=batch_labels_3d, indices=inds, reg_indices=reg_inds, gt_locs=batch_gt_locations, gt_dims=gt_dimensions, gt_yaws=gt_orientations, gt_cors=gt_corners)\n    return (center_heatmap_target, avg_factor, target_labels)",
        "mutated": [
            "def get_targets(self, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, feat_shape, img_shape, img_metas):\n    if False:\n        i = 10\n    \"Get training targets for batch images.\\n\\n        Args:\\n            gt_bboxes (list[Tensor]): Ground truth bboxes of each image,\\n                shape (num_gt, 4).\\n            gt_labels (list[Tensor]): Ground truth labels of each box,\\n                shape (num_gt,).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D Ground\\n                truth bboxes of each image,\\n                shape (num_gt, bbox_code_size).\\n            gt_labels_3d (list[Tensor]): 3D Ground truth labels of each\\n                box, shape (num_gt,).\\n            centers2d (list[Tensor]): Projected 3D centers onto 2D image,\\n                shape (num_gt, 2).\\n            feat_shape (tuple[int]): Feature map shape with value,\\n                shape (B, _, H, W).\\n            img_shape (tuple[int]): Image shape in [h, w] format.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor, dict]: The Tensor value is the targets of\\n                center heatmap, the dict has components below:\\n              - gt_centers2d (Tensor): Coords of each projected 3D box\\n                    center on image. shape (B * max_objs, 2)\\n              - gt_labels3d (Tensor): Labels of each 3D box.\\n                    shape (B, max_objs, )\\n              - indices (Tensor): Indices of the existence of the 3D box.\\n                    shape (B * max_objs, )\\n              - affine_indices (Tensor): Indices of the affine of the 3D box.\\n                    shape (N, )\\n              - gt_locs (Tensor): Coords of each 3D box's location.\\n                    shape (N, 3)\\n              - gt_dims (Tensor): Dimensions of each 3D box.\\n                    shape (N, 3)\\n              - gt_yaws (Tensor): Orientation(yaw) of each 3D box.\\n                    shape (N, 1)\\n              - gt_cors (Tensor): Coords of the corners of each 3D box.\\n                    shape (N, 8, 3)\\n        \"\n    reg_mask = torch.stack([gt_bboxes[0].new_tensor(not img_meta['affine_aug'], dtype=torch.bool) for img_meta in img_metas])\n    (img_h, img_w) = img_shape[:2]\n    (bs, _, feat_h, feat_w) = feat_shape\n    width_ratio = float(feat_w / img_w)\n    height_ratio = float(feat_h / img_h)\n    assert width_ratio == height_ratio\n    center_heatmap_target = gt_bboxes[-1].new_zeros([bs, self.num_classes, feat_h, feat_w])\n    gt_centers2d = centers2d.copy()\n    for batch_id in range(bs):\n        gt_bbox = gt_bboxes[batch_id]\n        gt_label = gt_labels[batch_id]\n        gt_center2d = gt_centers2d[batch_id] * width_ratio\n        for (j, center) in enumerate(gt_center2d):\n            (center_x_int, center_y_int) = center.int()\n            scale_box_h = (gt_bbox[j][3] - gt_bbox[j][1]) * height_ratio\n            scale_box_w = (gt_bbox[j][2] - gt_bbox[j][0]) * width_ratio\n            radius = gaussian_radius([scale_box_h, scale_box_w], min_overlap=0.7)\n            radius = max(0, int(radius))\n            ind = gt_label[j]\n            gen_gaussian_target(center_heatmap_target[batch_id, ind], [center_x_int, center_y_int], radius)\n    avg_factor = max(1, center_heatmap_target.eq(1).sum())\n    num_ctrs = [center2d.shape[0] for center2d in centers2d]\n    max_objs = max(num_ctrs)\n    reg_inds = torch.cat([reg_mask[i].repeat(num_ctrs[i]) for i in range(bs)])\n    inds = torch.zeros((bs, max_objs), dtype=torch.bool).to(centers2d[0].device)\n    gt_bboxes_3d = [gt_bbox_3d.to(centers2d[0].device) for gt_bbox_3d in gt_bboxes_3d]\n    batch_centers2d = centers2d[0].new_zeros((bs, max_objs, 2))\n    batch_labels_3d = gt_labels_3d[0].new_zeros((bs, max_objs))\n    batch_gt_locations = gt_bboxes_3d[0].tensor.new_zeros((bs, max_objs, 3))\n    for i in range(bs):\n        inds[i, :num_ctrs[i]] = 1\n        batch_centers2d[i, :num_ctrs[i]] = centers2d[i]\n        batch_labels_3d[i, :num_ctrs[i]] = gt_labels_3d[i]\n        batch_gt_locations[i, :num_ctrs[i]] = gt_bboxes_3d[i].tensor[:, :3]\n    inds = inds.flatten()\n    batch_centers2d = batch_centers2d.view(-1, 2) * width_ratio\n    batch_gt_locations = batch_gt_locations.view(-1, 3)\n    gt_bboxes_3d = [gt_bbox_3d for gt_bbox_3d in gt_bboxes_3d if gt_bbox_3d.tensor.shape[0] > 0]\n    gt_dimensions = torch.cat([gt_bbox_3d.tensor[:, 3:6] for gt_bbox_3d in gt_bboxes_3d])\n    gt_orientations = torch.cat([gt_bbox_3d.tensor[:, 6].unsqueeze(-1) for gt_bbox_3d in gt_bboxes_3d])\n    gt_corners = torch.cat([gt_bbox_3d.corners for gt_bbox_3d in gt_bboxes_3d])\n    target_labels = dict(gt_centers2d=batch_centers2d.long(), gt_labels3d=batch_labels_3d, indices=inds, reg_indices=reg_inds, gt_locs=batch_gt_locations, gt_dims=gt_dimensions, gt_yaws=gt_orientations, gt_cors=gt_corners)\n    return (center_heatmap_target, avg_factor, target_labels)",
            "def get_targets(self, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, feat_shape, img_shape, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Get training targets for batch images.\\n\\n        Args:\\n            gt_bboxes (list[Tensor]): Ground truth bboxes of each image,\\n                shape (num_gt, 4).\\n            gt_labels (list[Tensor]): Ground truth labels of each box,\\n                shape (num_gt,).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D Ground\\n                truth bboxes of each image,\\n                shape (num_gt, bbox_code_size).\\n            gt_labels_3d (list[Tensor]): 3D Ground truth labels of each\\n                box, shape (num_gt,).\\n            centers2d (list[Tensor]): Projected 3D centers onto 2D image,\\n                shape (num_gt, 2).\\n            feat_shape (tuple[int]): Feature map shape with value,\\n                shape (B, _, H, W).\\n            img_shape (tuple[int]): Image shape in [h, w] format.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor, dict]: The Tensor value is the targets of\\n                center heatmap, the dict has components below:\\n              - gt_centers2d (Tensor): Coords of each projected 3D box\\n                    center on image. shape (B * max_objs, 2)\\n              - gt_labels3d (Tensor): Labels of each 3D box.\\n                    shape (B, max_objs, )\\n              - indices (Tensor): Indices of the existence of the 3D box.\\n                    shape (B * max_objs, )\\n              - affine_indices (Tensor): Indices of the affine of the 3D box.\\n                    shape (N, )\\n              - gt_locs (Tensor): Coords of each 3D box's location.\\n                    shape (N, 3)\\n              - gt_dims (Tensor): Dimensions of each 3D box.\\n                    shape (N, 3)\\n              - gt_yaws (Tensor): Orientation(yaw) of each 3D box.\\n                    shape (N, 1)\\n              - gt_cors (Tensor): Coords of the corners of each 3D box.\\n                    shape (N, 8, 3)\\n        \"\n    reg_mask = torch.stack([gt_bboxes[0].new_tensor(not img_meta['affine_aug'], dtype=torch.bool) for img_meta in img_metas])\n    (img_h, img_w) = img_shape[:2]\n    (bs, _, feat_h, feat_w) = feat_shape\n    width_ratio = float(feat_w / img_w)\n    height_ratio = float(feat_h / img_h)\n    assert width_ratio == height_ratio\n    center_heatmap_target = gt_bboxes[-1].new_zeros([bs, self.num_classes, feat_h, feat_w])\n    gt_centers2d = centers2d.copy()\n    for batch_id in range(bs):\n        gt_bbox = gt_bboxes[batch_id]\n        gt_label = gt_labels[batch_id]\n        gt_center2d = gt_centers2d[batch_id] * width_ratio\n        for (j, center) in enumerate(gt_center2d):\n            (center_x_int, center_y_int) = center.int()\n            scale_box_h = (gt_bbox[j][3] - gt_bbox[j][1]) * height_ratio\n            scale_box_w = (gt_bbox[j][2] - gt_bbox[j][0]) * width_ratio\n            radius = gaussian_radius([scale_box_h, scale_box_w], min_overlap=0.7)\n            radius = max(0, int(radius))\n            ind = gt_label[j]\n            gen_gaussian_target(center_heatmap_target[batch_id, ind], [center_x_int, center_y_int], radius)\n    avg_factor = max(1, center_heatmap_target.eq(1).sum())\n    num_ctrs = [center2d.shape[0] for center2d in centers2d]\n    max_objs = max(num_ctrs)\n    reg_inds = torch.cat([reg_mask[i].repeat(num_ctrs[i]) for i in range(bs)])\n    inds = torch.zeros((bs, max_objs), dtype=torch.bool).to(centers2d[0].device)\n    gt_bboxes_3d = [gt_bbox_3d.to(centers2d[0].device) for gt_bbox_3d in gt_bboxes_3d]\n    batch_centers2d = centers2d[0].new_zeros((bs, max_objs, 2))\n    batch_labels_3d = gt_labels_3d[0].new_zeros((bs, max_objs))\n    batch_gt_locations = gt_bboxes_3d[0].tensor.new_zeros((bs, max_objs, 3))\n    for i in range(bs):\n        inds[i, :num_ctrs[i]] = 1\n        batch_centers2d[i, :num_ctrs[i]] = centers2d[i]\n        batch_labels_3d[i, :num_ctrs[i]] = gt_labels_3d[i]\n        batch_gt_locations[i, :num_ctrs[i]] = gt_bboxes_3d[i].tensor[:, :3]\n    inds = inds.flatten()\n    batch_centers2d = batch_centers2d.view(-1, 2) * width_ratio\n    batch_gt_locations = batch_gt_locations.view(-1, 3)\n    gt_bboxes_3d = [gt_bbox_3d for gt_bbox_3d in gt_bboxes_3d if gt_bbox_3d.tensor.shape[0] > 0]\n    gt_dimensions = torch.cat([gt_bbox_3d.tensor[:, 3:6] for gt_bbox_3d in gt_bboxes_3d])\n    gt_orientations = torch.cat([gt_bbox_3d.tensor[:, 6].unsqueeze(-1) for gt_bbox_3d in gt_bboxes_3d])\n    gt_corners = torch.cat([gt_bbox_3d.corners for gt_bbox_3d in gt_bboxes_3d])\n    target_labels = dict(gt_centers2d=batch_centers2d.long(), gt_labels3d=batch_labels_3d, indices=inds, reg_indices=reg_inds, gt_locs=batch_gt_locations, gt_dims=gt_dimensions, gt_yaws=gt_orientations, gt_cors=gt_corners)\n    return (center_heatmap_target, avg_factor, target_labels)",
            "def get_targets(self, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, feat_shape, img_shape, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Get training targets for batch images.\\n\\n        Args:\\n            gt_bboxes (list[Tensor]): Ground truth bboxes of each image,\\n                shape (num_gt, 4).\\n            gt_labels (list[Tensor]): Ground truth labels of each box,\\n                shape (num_gt,).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D Ground\\n                truth bboxes of each image,\\n                shape (num_gt, bbox_code_size).\\n            gt_labels_3d (list[Tensor]): 3D Ground truth labels of each\\n                box, shape (num_gt,).\\n            centers2d (list[Tensor]): Projected 3D centers onto 2D image,\\n                shape (num_gt, 2).\\n            feat_shape (tuple[int]): Feature map shape with value,\\n                shape (B, _, H, W).\\n            img_shape (tuple[int]): Image shape in [h, w] format.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor, dict]: The Tensor value is the targets of\\n                center heatmap, the dict has components below:\\n              - gt_centers2d (Tensor): Coords of each projected 3D box\\n                    center on image. shape (B * max_objs, 2)\\n              - gt_labels3d (Tensor): Labels of each 3D box.\\n                    shape (B, max_objs, )\\n              - indices (Tensor): Indices of the existence of the 3D box.\\n                    shape (B * max_objs, )\\n              - affine_indices (Tensor): Indices of the affine of the 3D box.\\n                    shape (N, )\\n              - gt_locs (Tensor): Coords of each 3D box's location.\\n                    shape (N, 3)\\n              - gt_dims (Tensor): Dimensions of each 3D box.\\n                    shape (N, 3)\\n              - gt_yaws (Tensor): Orientation(yaw) of each 3D box.\\n                    shape (N, 1)\\n              - gt_cors (Tensor): Coords of the corners of each 3D box.\\n                    shape (N, 8, 3)\\n        \"\n    reg_mask = torch.stack([gt_bboxes[0].new_tensor(not img_meta['affine_aug'], dtype=torch.bool) for img_meta in img_metas])\n    (img_h, img_w) = img_shape[:2]\n    (bs, _, feat_h, feat_w) = feat_shape\n    width_ratio = float(feat_w / img_w)\n    height_ratio = float(feat_h / img_h)\n    assert width_ratio == height_ratio\n    center_heatmap_target = gt_bboxes[-1].new_zeros([bs, self.num_classes, feat_h, feat_w])\n    gt_centers2d = centers2d.copy()\n    for batch_id in range(bs):\n        gt_bbox = gt_bboxes[batch_id]\n        gt_label = gt_labels[batch_id]\n        gt_center2d = gt_centers2d[batch_id] * width_ratio\n        for (j, center) in enumerate(gt_center2d):\n            (center_x_int, center_y_int) = center.int()\n            scale_box_h = (gt_bbox[j][3] - gt_bbox[j][1]) * height_ratio\n            scale_box_w = (gt_bbox[j][2] - gt_bbox[j][0]) * width_ratio\n            radius = gaussian_radius([scale_box_h, scale_box_w], min_overlap=0.7)\n            radius = max(0, int(radius))\n            ind = gt_label[j]\n            gen_gaussian_target(center_heatmap_target[batch_id, ind], [center_x_int, center_y_int], radius)\n    avg_factor = max(1, center_heatmap_target.eq(1).sum())\n    num_ctrs = [center2d.shape[0] for center2d in centers2d]\n    max_objs = max(num_ctrs)\n    reg_inds = torch.cat([reg_mask[i].repeat(num_ctrs[i]) for i in range(bs)])\n    inds = torch.zeros((bs, max_objs), dtype=torch.bool).to(centers2d[0].device)\n    gt_bboxes_3d = [gt_bbox_3d.to(centers2d[0].device) for gt_bbox_3d in gt_bboxes_3d]\n    batch_centers2d = centers2d[0].new_zeros((bs, max_objs, 2))\n    batch_labels_3d = gt_labels_3d[0].new_zeros((bs, max_objs))\n    batch_gt_locations = gt_bboxes_3d[0].tensor.new_zeros((bs, max_objs, 3))\n    for i in range(bs):\n        inds[i, :num_ctrs[i]] = 1\n        batch_centers2d[i, :num_ctrs[i]] = centers2d[i]\n        batch_labels_3d[i, :num_ctrs[i]] = gt_labels_3d[i]\n        batch_gt_locations[i, :num_ctrs[i]] = gt_bboxes_3d[i].tensor[:, :3]\n    inds = inds.flatten()\n    batch_centers2d = batch_centers2d.view(-1, 2) * width_ratio\n    batch_gt_locations = batch_gt_locations.view(-1, 3)\n    gt_bboxes_3d = [gt_bbox_3d for gt_bbox_3d in gt_bboxes_3d if gt_bbox_3d.tensor.shape[0] > 0]\n    gt_dimensions = torch.cat([gt_bbox_3d.tensor[:, 3:6] for gt_bbox_3d in gt_bboxes_3d])\n    gt_orientations = torch.cat([gt_bbox_3d.tensor[:, 6].unsqueeze(-1) for gt_bbox_3d in gt_bboxes_3d])\n    gt_corners = torch.cat([gt_bbox_3d.corners for gt_bbox_3d in gt_bboxes_3d])\n    target_labels = dict(gt_centers2d=batch_centers2d.long(), gt_labels3d=batch_labels_3d, indices=inds, reg_indices=reg_inds, gt_locs=batch_gt_locations, gt_dims=gt_dimensions, gt_yaws=gt_orientations, gt_cors=gt_corners)\n    return (center_heatmap_target, avg_factor, target_labels)",
            "def get_targets(self, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, feat_shape, img_shape, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Get training targets for batch images.\\n\\n        Args:\\n            gt_bboxes (list[Tensor]): Ground truth bboxes of each image,\\n                shape (num_gt, 4).\\n            gt_labels (list[Tensor]): Ground truth labels of each box,\\n                shape (num_gt,).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D Ground\\n                truth bboxes of each image,\\n                shape (num_gt, bbox_code_size).\\n            gt_labels_3d (list[Tensor]): 3D Ground truth labels of each\\n                box, shape (num_gt,).\\n            centers2d (list[Tensor]): Projected 3D centers onto 2D image,\\n                shape (num_gt, 2).\\n            feat_shape (tuple[int]): Feature map shape with value,\\n                shape (B, _, H, W).\\n            img_shape (tuple[int]): Image shape in [h, w] format.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor, dict]: The Tensor value is the targets of\\n                center heatmap, the dict has components below:\\n              - gt_centers2d (Tensor): Coords of each projected 3D box\\n                    center on image. shape (B * max_objs, 2)\\n              - gt_labels3d (Tensor): Labels of each 3D box.\\n                    shape (B, max_objs, )\\n              - indices (Tensor): Indices of the existence of the 3D box.\\n                    shape (B * max_objs, )\\n              - affine_indices (Tensor): Indices of the affine of the 3D box.\\n                    shape (N, )\\n              - gt_locs (Tensor): Coords of each 3D box's location.\\n                    shape (N, 3)\\n              - gt_dims (Tensor): Dimensions of each 3D box.\\n                    shape (N, 3)\\n              - gt_yaws (Tensor): Orientation(yaw) of each 3D box.\\n                    shape (N, 1)\\n              - gt_cors (Tensor): Coords of the corners of each 3D box.\\n                    shape (N, 8, 3)\\n        \"\n    reg_mask = torch.stack([gt_bboxes[0].new_tensor(not img_meta['affine_aug'], dtype=torch.bool) for img_meta in img_metas])\n    (img_h, img_w) = img_shape[:2]\n    (bs, _, feat_h, feat_w) = feat_shape\n    width_ratio = float(feat_w / img_w)\n    height_ratio = float(feat_h / img_h)\n    assert width_ratio == height_ratio\n    center_heatmap_target = gt_bboxes[-1].new_zeros([bs, self.num_classes, feat_h, feat_w])\n    gt_centers2d = centers2d.copy()\n    for batch_id in range(bs):\n        gt_bbox = gt_bboxes[batch_id]\n        gt_label = gt_labels[batch_id]\n        gt_center2d = gt_centers2d[batch_id] * width_ratio\n        for (j, center) in enumerate(gt_center2d):\n            (center_x_int, center_y_int) = center.int()\n            scale_box_h = (gt_bbox[j][3] - gt_bbox[j][1]) * height_ratio\n            scale_box_w = (gt_bbox[j][2] - gt_bbox[j][0]) * width_ratio\n            radius = gaussian_radius([scale_box_h, scale_box_w], min_overlap=0.7)\n            radius = max(0, int(radius))\n            ind = gt_label[j]\n            gen_gaussian_target(center_heatmap_target[batch_id, ind], [center_x_int, center_y_int], radius)\n    avg_factor = max(1, center_heatmap_target.eq(1).sum())\n    num_ctrs = [center2d.shape[0] for center2d in centers2d]\n    max_objs = max(num_ctrs)\n    reg_inds = torch.cat([reg_mask[i].repeat(num_ctrs[i]) for i in range(bs)])\n    inds = torch.zeros((bs, max_objs), dtype=torch.bool).to(centers2d[0].device)\n    gt_bboxes_3d = [gt_bbox_3d.to(centers2d[0].device) for gt_bbox_3d in gt_bboxes_3d]\n    batch_centers2d = centers2d[0].new_zeros((bs, max_objs, 2))\n    batch_labels_3d = gt_labels_3d[0].new_zeros((bs, max_objs))\n    batch_gt_locations = gt_bboxes_3d[0].tensor.new_zeros((bs, max_objs, 3))\n    for i in range(bs):\n        inds[i, :num_ctrs[i]] = 1\n        batch_centers2d[i, :num_ctrs[i]] = centers2d[i]\n        batch_labels_3d[i, :num_ctrs[i]] = gt_labels_3d[i]\n        batch_gt_locations[i, :num_ctrs[i]] = gt_bboxes_3d[i].tensor[:, :3]\n    inds = inds.flatten()\n    batch_centers2d = batch_centers2d.view(-1, 2) * width_ratio\n    batch_gt_locations = batch_gt_locations.view(-1, 3)\n    gt_bboxes_3d = [gt_bbox_3d for gt_bbox_3d in gt_bboxes_3d if gt_bbox_3d.tensor.shape[0] > 0]\n    gt_dimensions = torch.cat([gt_bbox_3d.tensor[:, 3:6] for gt_bbox_3d in gt_bboxes_3d])\n    gt_orientations = torch.cat([gt_bbox_3d.tensor[:, 6].unsqueeze(-1) for gt_bbox_3d in gt_bboxes_3d])\n    gt_corners = torch.cat([gt_bbox_3d.corners for gt_bbox_3d in gt_bboxes_3d])\n    target_labels = dict(gt_centers2d=batch_centers2d.long(), gt_labels3d=batch_labels_3d, indices=inds, reg_indices=reg_inds, gt_locs=batch_gt_locations, gt_dims=gt_dimensions, gt_yaws=gt_orientations, gt_cors=gt_corners)\n    return (center_heatmap_target, avg_factor, target_labels)",
            "def get_targets(self, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, feat_shape, img_shape, img_metas):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Get training targets for batch images.\\n\\n        Args:\\n            gt_bboxes (list[Tensor]): Ground truth bboxes of each image,\\n                shape (num_gt, 4).\\n            gt_labels (list[Tensor]): Ground truth labels of each box,\\n                shape (num_gt,).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D Ground\\n                truth bboxes of each image,\\n                shape (num_gt, bbox_code_size).\\n            gt_labels_3d (list[Tensor]): 3D Ground truth labels of each\\n                box, shape (num_gt,).\\n            centers2d (list[Tensor]): Projected 3D centers onto 2D image,\\n                shape (num_gt, 2).\\n            feat_shape (tuple[int]): Feature map shape with value,\\n                shape (B, _, H, W).\\n            img_shape (tuple[int]): Image shape in [h, w] format.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n\\n        Returns:\\n            tuple[Tensor, dict]: The Tensor value is the targets of\\n                center heatmap, the dict has components below:\\n              - gt_centers2d (Tensor): Coords of each projected 3D box\\n                    center on image. shape (B * max_objs, 2)\\n              - gt_labels3d (Tensor): Labels of each 3D box.\\n                    shape (B, max_objs, )\\n              - indices (Tensor): Indices of the existence of the 3D box.\\n                    shape (B * max_objs, )\\n              - affine_indices (Tensor): Indices of the affine of the 3D box.\\n                    shape (N, )\\n              - gt_locs (Tensor): Coords of each 3D box's location.\\n                    shape (N, 3)\\n              - gt_dims (Tensor): Dimensions of each 3D box.\\n                    shape (N, 3)\\n              - gt_yaws (Tensor): Orientation(yaw) of each 3D box.\\n                    shape (N, 1)\\n              - gt_cors (Tensor): Coords of the corners of each 3D box.\\n                    shape (N, 8, 3)\\n        \"\n    reg_mask = torch.stack([gt_bboxes[0].new_tensor(not img_meta['affine_aug'], dtype=torch.bool) for img_meta in img_metas])\n    (img_h, img_w) = img_shape[:2]\n    (bs, _, feat_h, feat_w) = feat_shape\n    width_ratio = float(feat_w / img_w)\n    height_ratio = float(feat_h / img_h)\n    assert width_ratio == height_ratio\n    center_heatmap_target = gt_bboxes[-1].new_zeros([bs, self.num_classes, feat_h, feat_w])\n    gt_centers2d = centers2d.copy()\n    for batch_id in range(bs):\n        gt_bbox = gt_bboxes[batch_id]\n        gt_label = gt_labels[batch_id]\n        gt_center2d = gt_centers2d[batch_id] * width_ratio\n        for (j, center) in enumerate(gt_center2d):\n            (center_x_int, center_y_int) = center.int()\n            scale_box_h = (gt_bbox[j][3] - gt_bbox[j][1]) * height_ratio\n            scale_box_w = (gt_bbox[j][2] - gt_bbox[j][0]) * width_ratio\n            radius = gaussian_radius([scale_box_h, scale_box_w], min_overlap=0.7)\n            radius = max(0, int(radius))\n            ind = gt_label[j]\n            gen_gaussian_target(center_heatmap_target[batch_id, ind], [center_x_int, center_y_int], radius)\n    avg_factor = max(1, center_heatmap_target.eq(1).sum())\n    num_ctrs = [center2d.shape[0] for center2d in centers2d]\n    max_objs = max(num_ctrs)\n    reg_inds = torch.cat([reg_mask[i].repeat(num_ctrs[i]) for i in range(bs)])\n    inds = torch.zeros((bs, max_objs), dtype=torch.bool).to(centers2d[0].device)\n    gt_bboxes_3d = [gt_bbox_3d.to(centers2d[0].device) for gt_bbox_3d in gt_bboxes_3d]\n    batch_centers2d = centers2d[0].new_zeros((bs, max_objs, 2))\n    batch_labels_3d = gt_labels_3d[0].new_zeros((bs, max_objs))\n    batch_gt_locations = gt_bboxes_3d[0].tensor.new_zeros((bs, max_objs, 3))\n    for i in range(bs):\n        inds[i, :num_ctrs[i]] = 1\n        batch_centers2d[i, :num_ctrs[i]] = centers2d[i]\n        batch_labels_3d[i, :num_ctrs[i]] = gt_labels_3d[i]\n        batch_gt_locations[i, :num_ctrs[i]] = gt_bboxes_3d[i].tensor[:, :3]\n    inds = inds.flatten()\n    batch_centers2d = batch_centers2d.view(-1, 2) * width_ratio\n    batch_gt_locations = batch_gt_locations.view(-1, 3)\n    gt_bboxes_3d = [gt_bbox_3d for gt_bbox_3d in gt_bboxes_3d if gt_bbox_3d.tensor.shape[0] > 0]\n    gt_dimensions = torch.cat([gt_bbox_3d.tensor[:, 3:6] for gt_bbox_3d in gt_bboxes_3d])\n    gt_orientations = torch.cat([gt_bbox_3d.tensor[:, 6].unsqueeze(-1) for gt_bbox_3d in gt_bboxes_3d])\n    gt_corners = torch.cat([gt_bbox_3d.corners for gt_bbox_3d in gt_bboxes_3d])\n    target_labels = dict(gt_centers2d=batch_centers2d.long(), gt_labels3d=batch_labels_3d, indices=inds, reg_indices=reg_inds, gt_locs=batch_gt_locations, gt_dims=gt_dimensions, gt_yaws=gt_orientations, gt_cors=gt_corners)\n    return (center_heatmap_target, avg_factor, target_labels)"
        ]
    },
    {
        "func_name": "loss",
        "original": "def loss(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, img_metas, gt_bboxes_ignore=None):\n    \"\"\"Compute loss of the head.\n\n        Args:\n            cls_scores (list[Tensor]): Box scores for each scale level.\n                shape (num_gt, 4).\n            bbox_preds (list[Tensor]): Box dims is a 4D-tensor, the channel\n                number is bbox_code_size.\n                shape (B, 7, H, W).\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image.\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels (list[Tensor]): Class indices corresponding to each box.\n                shape (num_gts, ).\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D boxes ground\n                truth. it is the flipped gt_bboxes\n            gt_labels_3d (list[Tensor]): Same as gt_labels.\n            centers2d (list[Tensor]): 2D centers on the image.\n                shape (num_gts, 2).\n            depths (list[Tensor]): Depth ground truth.\n                shape (num_gts, ).\n            attr_labels (list[Tensor]): Attributes indices of each box.\n                In kitti it's None.\n            img_metas (list[dict]): Meta information of each image, e.g.,\n                image size, scaling factor, etc.\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\n                boxes can be ignored when computing the loss.\n                Default: None.\n\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components.\n        \"\"\"\n    assert len(cls_scores) == len(bbox_preds) == 1\n    assert attr_labels is None\n    assert gt_bboxes_ignore is None\n    center2d_heatmap = cls_scores[0]\n    pred_reg = bbox_preds[0]\n    (center2d_heatmap_target, avg_factor, target_labels) = self.get_targets(gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, center2d_heatmap.shape, img_metas[0]['pad_shape'], img_metas)\n    pred_bboxes = self.get_predictions(labels3d=target_labels['gt_labels3d'], centers2d=target_labels['gt_centers2d'], gt_locations=target_labels['gt_locs'], gt_dimensions=target_labels['gt_dims'], gt_orientations=target_labels['gt_yaws'], indices=target_labels['indices'], img_metas=img_metas, pred_reg=pred_reg)\n    loss_cls = self.loss_cls(center2d_heatmap, center2d_heatmap_target, avg_factor=avg_factor)\n    reg_inds = target_labels['reg_indices']\n    loss_bbox_oris = self.loss_bbox(pred_bboxes['ori'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox_dims = self.loss_bbox(pred_bboxes['dim'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox_locs = self.loss_bbox(pred_bboxes['loc'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox = loss_bbox_dims + loss_bbox_locs + loss_bbox_oris\n    loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox)\n    return loss_dict",
        "mutated": [
            "def loss(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, img_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n    \"Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n                shape (num_gt, 4).\\n            bbox_preds (list[Tensor]): Box dims is a 4D-tensor, the channel\\n                number is bbox_code_size.\\n                shape (B, 7, H, W).\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image.\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): Class indices corresponding to each box.\\n                shape (num_gts, ).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D boxes ground\\n                truth. it is the flipped gt_bboxes\\n            gt_labels_3d (list[Tensor]): Same as gt_labels.\\n            centers2d (list[Tensor]): 2D centers on the image.\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth.\\n                shape (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n                In kitti it's None.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\\n                boxes can be ignored when computing the loss.\\n                Default: None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        \"\n    assert len(cls_scores) == len(bbox_preds) == 1\n    assert attr_labels is None\n    assert gt_bboxes_ignore is None\n    center2d_heatmap = cls_scores[0]\n    pred_reg = bbox_preds[0]\n    (center2d_heatmap_target, avg_factor, target_labels) = self.get_targets(gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, center2d_heatmap.shape, img_metas[0]['pad_shape'], img_metas)\n    pred_bboxes = self.get_predictions(labels3d=target_labels['gt_labels3d'], centers2d=target_labels['gt_centers2d'], gt_locations=target_labels['gt_locs'], gt_dimensions=target_labels['gt_dims'], gt_orientations=target_labels['gt_yaws'], indices=target_labels['indices'], img_metas=img_metas, pred_reg=pred_reg)\n    loss_cls = self.loss_cls(center2d_heatmap, center2d_heatmap_target, avg_factor=avg_factor)\n    reg_inds = target_labels['reg_indices']\n    loss_bbox_oris = self.loss_bbox(pred_bboxes['ori'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox_dims = self.loss_bbox(pred_bboxes['dim'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox_locs = self.loss_bbox(pred_bboxes['loc'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox = loss_bbox_dims + loss_bbox_locs + loss_bbox_oris\n    loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox)\n    return loss_dict",
            "def loss(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, img_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n                shape (num_gt, 4).\\n            bbox_preds (list[Tensor]): Box dims is a 4D-tensor, the channel\\n                number is bbox_code_size.\\n                shape (B, 7, H, W).\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image.\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): Class indices corresponding to each box.\\n                shape (num_gts, ).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D boxes ground\\n                truth. it is the flipped gt_bboxes\\n            gt_labels_3d (list[Tensor]): Same as gt_labels.\\n            centers2d (list[Tensor]): 2D centers on the image.\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth.\\n                shape (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n                In kitti it's None.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\\n                boxes can be ignored when computing the loss.\\n                Default: None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        \"\n    assert len(cls_scores) == len(bbox_preds) == 1\n    assert attr_labels is None\n    assert gt_bboxes_ignore is None\n    center2d_heatmap = cls_scores[0]\n    pred_reg = bbox_preds[0]\n    (center2d_heatmap_target, avg_factor, target_labels) = self.get_targets(gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, center2d_heatmap.shape, img_metas[0]['pad_shape'], img_metas)\n    pred_bboxes = self.get_predictions(labels3d=target_labels['gt_labels3d'], centers2d=target_labels['gt_centers2d'], gt_locations=target_labels['gt_locs'], gt_dimensions=target_labels['gt_dims'], gt_orientations=target_labels['gt_yaws'], indices=target_labels['indices'], img_metas=img_metas, pred_reg=pred_reg)\n    loss_cls = self.loss_cls(center2d_heatmap, center2d_heatmap_target, avg_factor=avg_factor)\n    reg_inds = target_labels['reg_indices']\n    loss_bbox_oris = self.loss_bbox(pred_bboxes['ori'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox_dims = self.loss_bbox(pred_bboxes['dim'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox_locs = self.loss_bbox(pred_bboxes['loc'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox = loss_bbox_dims + loss_bbox_locs + loss_bbox_oris\n    loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox)\n    return loss_dict",
            "def loss(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, img_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n                shape (num_gt, 4).\\n            bbox_preds (list[Tensor]): Box dims is a 4D-tensor, the channel\\n                number is bbox_code_size.\\n                shape (B, 7, H, W).\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image.\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): Class indices corresponding to each box.\\n                shape (num_gts, ).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D boxes ground\\n                truth. it is the flipped gt_bboxes\\n            gt_labels_3d (list[Tensor]): Same as gt_labels.\\n            centers2d (list[Tensor]): 2D centers on the image.\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth.\\n                shape (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n                In kitti it's None.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\\n                boxes can be ignored when computing the loss.\\n                Default: None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        \"\n    assert len(cls_scores) == len(bbox_preds) == 1\n    assert attr_labels is None\n    assert gt_bboxes_ignore is None\n    center2d_heatmap = cls_scores[0]\n    pred_reg = bbox_preds[0]\n    (center2d_heatmap_target, avg_factor, target_labels) = self.get_targets(gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, center2d_heatmap.shape, img_metas[0]['pad_shape'], img_metas)\n    pred_bboxes = self.get_predictions(labels3d=target_labels['gt_labels3d'], centers2d=target_labels['gt_centers2d'], gt_locations=target_labels['gt_locs'], gt_dimensions=target_labels['gt_dims'], gt_orientations=target_labels['gt_yaws'], indices=target_labels['indices'], img_metas=img_metas, pred_reg=pred_reg)\n    loss_cls = self.loss_cls(center2d_heatmap, center2d_heatmap_target, avg_factor=avg_factor)\n    reg_inds = target_labels['reg_indices']\n    loss_bbox_oris = self.loss_bbox(pred_bboxes['ori'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox_dims = self.loss_bbox(pred_bboxes['dim'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox_locs = self.loss_bbox(pred_bboxes['loc'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox = loss_bbox_dims + loss_bbox_locs + loss_bbox_oris\n    loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox)\n    return loss_dict",
            "def loss(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, img_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n                shape (num_gt, 4).\\n            bbox_preds (list[Tensor]): Box dims is a 4D-tensor, the channel\\n                number is bbox_code_size.\\n                shape (B, 7, H, W).\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image.\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): Class indices corresponding to each box.\\n                shape (num_gts, ).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D boxes ground\\n                truth. it is the flipped gt_bboxes\\n            gt_labels_3d (list[Tensor]): Same as gt_labels.\\n            centers2d (list[Tensor]): 2D centers on the image.\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth.\\n                shape (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n                In kitti it's None.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\\n                boxes can be ignored when computing the loss.\\n                Default: None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        \"\n    assert len(cls_scores) == len(bbox_preds) == 1\n    assert attr_labels is None\n    assert gt_bboxes_ignore is None\n    center2d_heatmap = cls_scores[0]\n    pred_reg = bbox_preds[0]\n    (center2d_heatmap_target, avg_factor, target_labels) = self.get_targets(gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, center2d_heatmap.shape, img_metas[0]['pad_shape'], img_metas)\n    pred_bboxes = self.get_predictions(labels3d=target_labels['gt_labels3d'], centers2d=target_labels['gt_centers2d'], gt_locations=target_labels['gt_locs'], gt_dimensions=target_labels['gt_dims'], gt_orientations=target_labels['gt_yaws'], indices=target_labels['indices'], img_metas=img_metas, pred_reg=pred_reg)\n    loss_cls = self.loss_cls(center2d_heatmap, center2d_heatmap_target, avg_factor=avg_factor)\n    reg_inds = target_labels['reg_indices']\n    loss_bbox_oris = self.loss_bbox(pred_bboxes['ori'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox_dims = self.loss_bbox(pred_bboxes['dim'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox_locs = self.loss_bbox(pred_bboxes['loc'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox = loss_bbox_dims + loss_bbox_locs + loss_bbox_oris\n    loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox)\n    return loss_dict",
            "def loss(self, cls_scores, bbox_preds, gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, depths, attr_labels, img_metas, gt_bboxes_ignore=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Compute loss of the head.\\n\\n        Args:\\n            cls_scores (list[Tensor]): Box scores for each scale level.\\n                shape (num_gt, 4).\\n            bbox_preds (list[Tensor]): Box dims is a 4D-tensor, the channel\\n                number is bbox_code_size.\\n                shape (B, 7, H, W).\\n            gt_bboxes (list[Tensor]): Ground truth bboxes for each image.\\n                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.\\n            gt_labels (list[Tensor]): Class indices corresponding to each box.\\n                shape (num_gts, ).\\n            gt_bboxes_3d (list[:obj:`CameraInstance3DBoxes`]): 3D boxes ground\\n                truth. it is the flipped gt_bboxes\\n            gt_labels_3d (list[Tensor]): Same as gt_labels.\\n            centers2d (list[Tensor]): 2D centers on the image.\\n                shape (num_gts, 2).\\n            depths (list[Tensor]): Depth ground truth.\\n                shape (num_gts, ).\\n            attr_labels (list[Tensor]): Attributes indices of each box.\\n                In kitti it's None.\\n            img_metas (list[dict]): Meta information of each image, e.g.,\\n                image size, scaling factor, etc.\\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\\n                boxes can be ignored when computing the loss.\\n                Default: None.\\n\\n        Returns:\\n            dict[str, Tensor]: A dictionary of loss components.\\n        \"\n    assert len(cls_scores) == len(bbox_preds) == 1\n    assert attr_labels is None\n    assert gt_bboxes_ignore is None\n    center2d_heatmap = cls_scores[0]\n    pred_reg = bbox_preds[0]\n    (center2d_heatmap_target, avg_factor, target_labels) = self.get_targets(gt_bboxes, gt_labels, gt_bboxes_3d, gt_labels_3d, centers2d, center2d_heatmap.shape, img_metas[0]['pad_shape'], img_metas)\n    pred_bboxes = self.get_predictions(labels3d=target_labels['gt_labels3d'], centers2d=target_labels['gt_centers2d'], gt_locations=target_labels['gt_locs'], gt_dimensions=target_labels['gt_dims'], gt_orientations=target_labels['gt_yaws'], indices=target_labels['indices'], img_metas=img_metas, pred_reg=pred_reg)\n    loss_cls = self.loss_cls(center2d_heatmap, center2d_heatmap_target, avg_factor=avg_factor)\n    reg_inds = target_labels['reg_indices']\n    loss_bbox_oris = self.loss_bbox(pred_bboxes['ori'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox_dims = self.loss_bbox(pred_bboxes['dim'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox_locs = self.loss_bbox(pred_bboxes['loc'].corners[reg_inds, ...], target_labels['gt_cors'][reg_inds, ...])\n    loss_bbox = loss_bbox_dims + loss_bbox_locs + loss_bbox_oris\n    loss_dict = dict(loss_cls=loss_cls, loss_bbox=loss_bbox)\n    return loss_dict"
        ]
    }
]