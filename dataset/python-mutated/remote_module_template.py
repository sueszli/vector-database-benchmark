def get_remote_module_template(enable_moving_cpu_tensors_to_cuda: bool):
    if False:
        for i in range(10):
            print('nop')
    return _TEMPLATE_PREFIX + (_REMOTE_FORWARD_TEMPLATE_ENABLE_MOVING_CPU_TENSORS_TO_CUDA if enable_moving_cpu_tensors_to_cuda else _REMOTE_FORWARD_TEMPLATE)
_TEMPLATE_PREFIX = 'from typing import *\n\nimport torch\nimport torch.distributed.rpc as rpc\nfrom torch import Tensor\nfrom torch._jit_internal import Future\nfrom torch.distributed.rpc import RRef\nfrom typing import Tuple  # pyre-ignore: unused import\n\n\n{assign_module_interface_cls}\n\n\ndef forward_async(self, {arg_types}){arrow_and_future_return_type}:\n    args = (self.module_rref, self.device, self.is_device_map_set, {args})\n    kwargs = {{{kwargs}}}\n    return rpc.rpc_async(\n        self.module_rref.owner(),\n        _remote_forward,\n        args,\n        kwargs,\n    )\n\n\ndef forward(self, {arg_types}){arrow_and_return_type}:\n    args = (self.module_rref, self.device, self.is_device_map_set, {args})\n    kwargs = {{{kwargs}}}\n    ret_fut = rpc.rpc_async(\n        self.module_rref.owner(),\n        _remote_forward,\n        args,\n        kwargs,\n    )\n    return ret_fut.wait()\n\n\n_generated_methods = [\n    forward_async,\n    forward,\n]\n\n\n{jit_script_decorator}\n'
_REMOTE_FORWARD_TEMPLATE_ENABLE_MOVING_CPU_TENSORS_TO_CUDA = '\ndef _remote_forward(\n    module_rref: RRef[module_interface_cls], device: str, is_device_map_set: bool, {arg_types}){arrow_and_return_type}:\n    module = module_rref.local_value()\n    device = torch.device(device)\n\n    if device.type != "cuda":\n        return module.forward({args}, {kwargs})\n\n    # If the module is on a cuda device,\n    # move any CPU tensor in args or kwargs to the same cuda device.\n    # Since torch script does not support generator expression,\n    # have to use concatenation instead of\n    # ``tuple(i.to(device) if isinstance(i, Tensor) else i for i in *args)``.\n    args = ({args},)\n    out_args: Tuple[()] = ()\n    for arg in args:\n        arg = (arg.to(device),) if isinstance(arg, Tensor) else (arg,)\n        out_args = out_args + arg\n\n    kwargs = {{{kwargs}}}\n    for k, v in kwargs.items():\n        if isinstance(v, Tensor):\n            kwargs[k] = kwargs[k].to(device)\n\n    if is_device_map_set:\n        return module.forward(*out_args, {kwargs})\n\n    # If the device map is empty, then only CPU tensors are allowed to send over wire,\n    # so have to move any GPU tensor to CPU in the output.\n    # Since torch script does not support generator expression,\n    # have to use concatenation instead of\n    # ``tuple(i.cpu() if isinstance(i, Tensor) else i for i in module.forward(*out_args, {kwargs}))``.\n    ret: Tuple[()] = ()\n    for i in module.forward(*out_args, {kwargs}):\n        i = (i.cpu(),) if isinstance(i, Tensor) else (i,)\n        ret = ret + i\n    return ret\n'
_REMOTE_FORWARD_TEMPLATE = '\ndef _remote_forward(\n    module_rref: RRef[module_interface_cls], device: str, is_device_map_set: bool, {arg_types}){arrow_and_return_type}:\n    module = module_rref.local_value()\n\n    return module.forward({args}, {kwargs})\n'