[
    {
        "func_name": "_cubic_interpolate",
        "original": "def _cubic_interpolate(x1, f1, g1, x2, f2, g2, bounds=None):\n    \"\"\"Cubic interpolation between (x1, f1, g1) and (x2, f2, g2).\n        Use two points and their gradient to determine a cubic function and get the minimun point\n        between them in the cubic curve.\n\n    Reference:\n        Jorge Nocedal, Stephen J. Wright, Numerical Optimization, Second Edition, 2006.\n        pp59: formula 3.59\n\n    Args:\n        x1, f1, g1: point1's position, value and gradient.\n        x2, f2, g2: point2's position, value and gradient.\n        bounds: bounds of interpolation area\n\n    Returns:\n        min_pos: the minimun point between the specified points in the cubic curve.\n    \"\"\"\n    if bounds is not None:\n        (xmin_bound, xmax_bound) = bounds\n    else:\n        (xmin_bound, xmax_bound) = (x1, x2) if x1 <= x2 else (x2, x1)\n    d1 = g1 + g2 - 3 * (f1 - f2) / (x1 - x2)\n    d2_square = d1 ** 2 - g1 * g2\n    if d2_square >= 0:\n        d2 = d2_square.sqrt()\n        if x1 <= x2:\n            min_pos = x2 - (x2 - x1) * ((g2 + d2 - d1) / (g2 - g1 + 2 * d2))\n        else:\n            min_pos = x1 - (x1 - x2) * ((g1 + d2 - d1) / (g1 - g2 + 2 * d2))\n        return min(max(min_pos, xmin_bound), xmax_bound)\n    else:\n        return (xmin_bound + xmax_bound) / 2.0",
        "mutated": [
            "def _cubic_interpolate(x1, f1, g1, x2, f2, g2, bounds=None):\n    if False:\n        i = 10\n    \"Cubic interpolation between (x1, f1, g1) and (x2, f2, g2).\\n        Use two points and their gradient to determine a cubic function and get the minimun point\\n        between them in the cubic curve.\\n\\n    Reference:\\n        Jorge Nocedal, Stephen J. Wright, Numerical Optimization, Second Edition, 2006.\\n        pp59: formula 3.59\\n\\n    Args:\\n        x1, f1, g1: point1's position, value and gradient.\\n        x2, f2, g2: point2's position, value and gradient.\\n        bounds: bounds of interpolation area\\n\\n    Returns:\\n        min_pos: the minimun point between the specified points in the cubic curve.\\n    \"\n    if bounds is not None:\n        (xmin_bound, xmax_bound) = bounds\n    else:\n        (xmin_bound, xmax_bound) = (x1, x2) if x1 <= x2 else (x2, x1)\n    d1 = g1 + g2 - 3 * (f1 - f2) / (x1 - x2)\n    d2_square = d1 ** 2 - g1 * g2\n    if d2_square >= 0:\n        d2 = d2_square.sqrt()\n        if x1 <= x2:\n            min_pos = x2 - (x2 - x1) * ((g2 + d2 - d1) / (g2 - g1 + 2 * d2))\n        else:\n            min_pos = x1 - (x1 - x2) * ((g1 + d2 - d1) / (g1 - g2 + 2 * d2))\n        return min(max(min_pos, xmin_bound), xmax_bound)\n    else:\n        return (xmin_bound + xmax_bound) / 2.0",
            "def _cubic_interpolate(x1, f1, g1, x2, f2, g2, bounds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Cubic interpolation between (x1, f1, g1) and (x2, f2, g2).\\n        Use two points and their gradient to determine a cubic function and get the minimun point\\n        between them in the cubic curve.\\n\\n    Reference:\\n        Jorge Nocedal, Stephen J. Wright, Numerical Optimization, Second Edition, 2006.\\n        pp59: formula 3.59\\n\\n    Args:\\n        x1, f1, g1: point1's position, value and gradient.\\n        x2, f2, g2: point2's position, value and gradient.\\n        bounds: bounds of interpolation area\\n\\n    Returns:\\n        min_pos: the minimun point between the specified points in the cubic curve.\\n    \"\n    if bounds is not None:\n        (xmin_bound, xmax_bound) = bounds\n    else:\n        (xmin_bound, xmax_bound) = (x1, x2) if x1 <= x2 else (x2, x1)\n    d1 = g1 + g2 - 3 * (f1 - f2) / (x1 - x2)\n    d2_square = d1 ** 2 - g1 * g2\n    if d2_square >= 0:\n        d2 = d2_square.sqrt()\n        if x1 <= x2:\n            min_pos = x2 - (x2 - x1) * ((g2 + d2 - d1) / (g2 - g1 + 2 * d2))\n        else:\n            min_pos = x1 - (x1 - x2) * ((g1 + d2 - d1) / (g1 - g2 + 2 * d2))\n        return min(max(min_pos, xmin_bound), xmax_bound)\n    else:\n        return (xmin_bound + xmax_bound) / 2.0",
            "def _cubic_interpolate(x1, f1, g1, x2, f2, g2, bounds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Cubic interpolation between (x1, f1, g1) and (x2, f2, g2).\\n        Use two points and their gradient to determine a cubic function and get the minimun point\\n        between them in the cubic curve.\\n\\n    Reference:\\n        Jorge Nocedal, Stephen J. Wright, Numerical Optimization, Second Edition, 2006.\\n        pp59: formula 3.59\\n\\n    Args:\\n        x1, f1, g1: point1's position, value and gradient.\\n        x2, f2, g2: point2's position, value and gradient.\\n        bounds: bounds of interpolation area\\n\\n    Returns:\\n        min_pos: the minimun point between the specified points in the cubic curve.\\n    \"\n    if bounds is not None:\n        (xmin_bound, xmax_bound) = bounds\n    else:\n        (xmin_bound, xmax_bound) = (x1, x2) if x1 <= x2 else (x2, x1)\n    d1 = g1 + g2 - 3 * (f1 - f2) / (x1 - x2)\n    d2_square = d1 ** 2 - g1 * g2\n    if d2_square >= 0:\n        d2 = d2_square.sqrt()\n        if x1 <= x2:\n            min_pos = x2 - (x2 - x1) * ((g2 + d2 - d1) / (g2 - g1 + 2 * d2))\n        else:\n            min_pos = x1 - (x1 - x2) * ((g1 + d2 - d1) / (g1 - g2 + 2 * d2))\n        return min(max(min_pos, xmin_bound), xmax_bound)\n    else:\n        return (xmin_bound + xmax_bound) / 2.0",
            "def _cubic_interpolate(x1, f1, g1, x2, f2, g2, bounds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Cubic interpolation between (x1, f1, g1) and (x2, f2, g2).\\n        Use two points and their gradient to determine a cubic function and get the minimun point\\n        between them in the cubic curve.\\n\\n    Reference:\\n        Jorge Nocedal, Stephen J. Wright, Numerical Optimization, Second Edition, 2006.\\n        pp59: formula 3.59\\n\\n    Args:\\n        x1, f1, g1: point1's position, value and gradient.\\n        x2, f2, g2: point2's position, value and gradient.\\n        bounds: bounds of interpolation area\\n\\n    Returns:\\n        min_pos: the minimun point between the specified points in the cubic curve.\\n    \"\n    if bounds is not None:\n        (xmin_bound, xmax_bound) = bounds\n    else:\n        (xmin_bound, xmax_bound) = (x1, x2) if x1 <= x2 else (x2, x1)\n    d1 = g1 + g2 - 3 * (f1 - f2) / (x1 - x2)\n    d2_square = d1 ** 2 - g1 * g2\n    if d2_square >= 0:\n        d2 = d2_square.sqrt()\n        if x1 <= x2:\n            min_pos = x2 - (x2 - x1) * ((g2 + d2 - d1) / (g2 - g1 + 2 * d2))\n        else:\n            min_pos = x1 - (x1 - x2) * ((g1 + d2 - d1) / (g1 - g2 + 2 * d2))\n        return min(max(min_pos, xmin_bound), xmax_bound)\n    else:\n        return (xmin_bound + xmax_bound) / 2.0",
            "def _cubic_interpolate(x1, f1, g1, x2, f2, g2, bounds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Cubic interpolation between (x1, f1, g1) and (x2, f2, g2).\\n        Use two points and their gradient to determine a cubic function and get the minimun point\\n        between them in the cubic curve.\\n\\n    Reference:\\n        Jorge Nocedal, Stephen J. Wright, Numerical Optimization, Second Edition, 2006.\\n        pp59: formula 3.59\\n\\n    Args:\\n        x1, f1, g1: point1's position, value and gradient.\\n        x2, f2, g2: point2's position, value and gradient.\\n        bounds: bounds of interpolation area\\n\\n    Returns:\\n        min_pos: the minimun point between the specified points in the cubic curve.\\n    \"\n    if bounds is not None:\n        (xmin_bound, xmax_bound) = bounds\n    else:\n        (xmin_bound, xmax_bound) = (x1, x2) if x1 <= x2 else (x2, x1)\n    d1 = g1 + g2 - 3 * (f1 - f2) / (x1 - x2)\n    d2_square = d1 ** 2 - g1 * g2\n    if d2_square >= 0:\n        d2 = d2_square.sqrt()\n        if x1 <= x2:\n            min_pos = x2 - (x2 - x1) * ((g2 + d2 - d1) / (g2 - g1 + 2 * d2))\n        else:\n            min_pos = x1 - (x1 - x2) * ((g1 + d2 - d1) / (g1 - g2 + 2 * d2))\n        return min(max(min_pos, xmin_bound), xmax_bound)\n    else:\n        return (xmin_bound + xmax_bound) / 2.0"
        ]
    },
    {
        "func_name": "_strong_wolfe",
        "original": "def _strong_wolfe(obj_func, xk, alpha, d, loss, grad, gtd, c1=0.0001, c2=0.9, tolerance_change=1e-09, max_ls=25):\n    \"\"\"Implements of line search algorithm that satisfies the strong Wolfe conditions using double zoom.\n\n    Reference:\n        Jorge Nocedal, Stephen J. Wright, Numerical Optimization, Second Edition, 2006.\n        pp60: Algorithm 3.5 (Line Search Algorithm).\n\n    Args:\n        obj_func: the objective function to minimize. ```` accepts a multivariate input and returns a scalar.\n        xk (Tensor): the starting point of the iterates.\n        alpha (Scalar): the initial step size.\n        d (Tensor): search direction.\n        loss (scalar): the initial loss\n        grad (Tensor): the initial grad\n        c1 (Scalar): parameter for sufficient decrease condition.\n        c2 (Scalar): parameter for curvature condition.\n        tolerance_change (Scalar): terminates if the change of function value/position/parameter between\n            two iterations is smaller than this value.\n        max_ls(int): max iteration of line search.\n        alpha_max (float): max step length.\n\n    Returns:\n        loss_new (Scaler): loss of obj_func at final alpha.\n        grad_new, (Tensor): derivative of obj_func at final alpha.\n        alpha(Tensor): optimal step length, or 0. if the line search algorithm did not converge.\n        ls_func_evals (Scaler): number of objective function called in line search process.\n\n    Following summarizes the essentials of the strong Wolfe line search algorithm.\n    Some notations used in the description:\n\n        - `func` denotes the objective function.\n        - `obi_func` is a function of step size alpha, restricting `obj_func` on a line.\n\n            obi_func = func(xk + alpha * d),\n            where xk is the position of k'th iterate, d is the line search direction(decent direction),\n            and a is the step size.\n        - alpha : substitute of alpha\n        - a1 is alpha of last iteration, which is alpha_(i-1).\n        - a2 is alpha of current iteration, which is alpha_i.\n        - a_lo is alpha in left position when calls zoom, which is alpha_low.\n        - a_hi is alpha in right position when calls zoom, which is alpha_high.\n\n    Line Search Algorithm:\n        repeat\n            Compute obi_func(a2) and derphi(a2).\n            1. If obi_func(a2) > obi_func(0) + c_1 * a2 * obi_func'(0) or [obi_func(a2) >= obi_func(a1) and i > 1],\n                alpha= zoom(a1, a2) and stop;\n\n            2. If |obi_func'(a2)| <= -c_2 * obi_func'(0),\n                alpha= a2 and stop;\n\n            3. If obi_func'(a2) >= 0,\n                alpha= zoom(a2, a1) and stop;\n\n            a1 = a2\n            a2 = min(2 * a2, a2)\n            i = i + 1\n        end(repeat)\n\n    zoom(a_lo, a_hi) Algorithm:\n        repeat\n            aj = cubic_interpolation(a_lo, a_hi)\n            Compute obi_func(aj) and derphi(aj).\n            1. If obi_func(aj) > obi_func(0) + c_1 * aj * obi_func'(0) or obi_func(aj) >= obi_func(a_lo),\n                then a_hi <- aj;\n            2.\n                2.1. If |obi_func'(aj)| <= -c_2 * obi_func'(0), then alpha= a2 and stop;\n\n                2.2. If obi_func'(aj) * (a2 - a1) >= 0, then a_hi = a_lo\n\n                a_lo = aj;\n        end(repeat)\n    \"\"\"\n    d_norm = d.abs().max()\n    grad = grad.clone()\n    (loss_new, grad_new) = obj_func(xk, alpha, d)\n    ls_func_evals = 1\n    gtd_new = paddle.dot(grad_new, d)\n    (t_prev, f_prev, g_prev, gtd_prev) = (paddle.to_tensor(0, dtype=grad.dtype), loss, grad, gtd)\n    done = False\n    ls_iter = 0\n    while ls_iter < max_ls:\n        if loss_new > loss + c1 * alpha * gtd or (ls_iter > 1 and loss_new >= f_prev):\n            bracket = [t_prev, alpha]\n            bracket_f = [f_prev, loss_new]\n            bracket_g = [g_prev, grad_new.clone()]\n            bracket_gtd = [gtd_prev, gtd_new]\n            break\n        if paddle.abs(gtd_new) <= -c2 * gtd:\n            bracket = [alpha]\n            bracket_f = [loss_new]\n            bracket_g = [grad_new]\n            done = True\n            break\n        if gtd_new >= 0:\n            bracket = [t_prev, alpha]\n            bracket_f = [f_prev, loss_new]\n            bracket_g = [g_prev, grad_new.clone()]\n            bracket_gtd = [gtd_prev, gtd_new]\n            break\n        min_step = alpha + 0.01 * (alpha - t_prev)\n        max_step = alpha * 10\n        tmp = alpha\n        alpha = _cubic_interpolate(t_prev, f_prev, gtd_prev, alpha, loss_new, gtd_new, bounds=(min_step, max_step))\n        t_prev = tmp\n        f_prev = loss_new\n        g_prev = grad_new.clone()\n        gtd_prev = gtd_new\n        (loss_new, grad_new) = obj_func(xk, alpha, d)\n        ls_func_evals += 1\n        gtd_new = grad_new.dot(d)\n        ls_iter += 1\n    if ls_iter == max_ls:\n        bracket = [0, alpha]\n        bracket_f = [loss, loss_new]\n        bracket_g = [grad, grad_new]\n    insuf_progress = False\n    (low_pos, high_pos) = (0, 1) if bracket_f[0] <= bracket_f[-1] else (1, 0)\n    while not done and ls_iter < max_ls:\n        if paddle.abs(bracket[1] - bracket[0]) * d_norm < tolerance_change:\n            break\n        alpha = _cubic_interpolate(bracket[0], bracket_f[0], bracket_gtd[0], bracket[1], bracket_f[1], bracket_gtd[1])\n        eps = 0.1 * (max(bracket) - min(bracket))\n        if min(max(bracket) - alpha, alpha - min(bracket)) < eps:\n            if insuf_progress or alpha >= max(bracket) or alpha <= min(bracket):\n                if paddle.abs(alpha - max(bracket)) < paddle.abs(alpha - min(bracket)):\n                    alpha = max(bracket) - eps\n                else:\n                    alpha = min(bracket) + eps\n                insuf_progress = False\n            else:\n                insuf_progress = True\n        else:\n            insuf_progress = False\n        (loss_new, grad_new) = obj_func(xk, alpha, d)\n        ls_func_evals += 1\n        gtd_new = grad_new.dot(d)\n        ls_iter += 1\n        if loss_new > loss + c1 * alpha * gtd or loss_new >= bracket_f[low_pos]:\n            bracket[high_pos] = alpha\n            bracket_f[high_pos] = loss_new\n            bracket_g[high_pos] = grad_new.clone()\n            bracket_gtd[high_pos] = gtd_new\n            (low_pos, high_pos) = (0, 1) if bracket_f[0] <= bracket_f[1] else (1, 0)\n        else:\n            if paddle.abs(gtd_new) <= -c2 * gtd:\n                done = True\n            elif gtd_new * (bracket[high_pos] - bracket[low_pos]) >= 0:\n                bracket[high_pos] = bracket[low_pos]\n                bracket_f[high_pos] = bracket_f[low_pos]\n                bracket_g[high_pos] = bracket_g[low_pos]\n                bracket_gtd[high_pos] = bracket_gtd[low_pos]\n            bracket[low_pos] = alpha\n            bracket_f[low_pos] = loss_new\n            bracket_g[low_pos] = grad_new.clone()\n            bracket_gtd[low_pos] = gtd_new\n    alpha = bracket[low_pos]\n    loss_new = bracket_f[low_pos]\n    grad_new = bracket_g[low_pos]\n    return (loss_new, grad_new, alpha, ls_func_evals)",
        "mutated": [
            "def _strong_wolfe(obj_func, xk, alpha, d, loss, grad, gtd, c1=0.0001, c2=0.9, tolerance_change=1e-09, max_ls=25):\n    if False:\n        i = 10\n    \"Implements of line search algorithm that satisfies the strong Wolfe conditions using double zoom.\\n\\n    Reference:\\n        Jorge Nocedal, Stephen J. Wright, Numerical Optimization, Second Edition, 2006.\\n        pp60: Algorithm 3.5 (Line Search Algorithm).\\n\\n    Args:\\n        obj_func: the objective function to minimize. ```` accepts a multivariate input and returns a scalar.\\n        xk (Tensor): the starting point of the iterates.\\n        alpha (Scalar): the initial step size.\\n        d (Tensor): search direction.\\n        loss (scalar): the initial loss\\n        grad (Tensor): the initial grad\\n        c1 (Scalar): parameter for sufficient decrease condition.\\n        c2 (Scalar): parameter for curvature condition.\\n        tolerance_change (Scalar): terminates if the change of function value/position/parameter between\\n            two iterations is smaller than this value.\\n        max_ls(int): max iteration of line search.\\n        alpha_max (float): max step length.\\n\\n    Returns:\\n        loss_new (Scaler): loss of obj_func at final alpha.\\n        grad_new, (Tensor): derivative of obj_func at final alpha.\\n        alpha(Tensor): optimal step length, or 0. if the line search algorithm did not converge.\\n        ls_func_evals (Scaler): number of objective function called in line search process.\\n\\n    Following summarizes the essentials of the strong Wolfe line search algorithm.\\n    Some notations used in the description:\\n\\n        - `func` denotes the objective function.\\n        - `obi_func` is a function of step size alpha, restricting `obj_func` on a line.\\n\\n            obi_func = func(xk + alpha * d),\\n            where xk is the position of k'th iterate, d is the line search direction(decent direction),\\n            and a is the step size.\\n        - alpha : substitute of alpha\\n        - a1 is alpha of last iteration, which is alpha_(i-1).\\n        - a2 is alpha of current iteration, which is alpha_i.\\n        - a_lo is alpha in left position when calls zoom, which is alpha_low.\\n        - a_hi is alpha in right position when calls zoom, which is alpha_high.\\n\\n    Line Search Algorithm:\\n        repeat\\n            Compute obi_func(a2) and derphi(a2).\\n            1. If obi_func(a2) > obi_func(0) + c_1 * a2 * obi_func'(0) or [obi_func(a2) >= obi_func(a1) and i > 1],\\n                alpha= zoom(a1, a2) and stop;\\n\\n            2. If |obi_func'(a2)| <= -c_2 * obi_func'(0),\\n                alpha= a2 and stop;\\n\\n            3. If obi_func'(a2) >= 0,\\n                alpha= zoom(a2, a1) and stop;\\n\\n            a1 = a2\\n            a2 = min(2 * a2, a2)\\n            i = i + 1\\n        end(repeat)\\n\\n    zoom(a_lo, a_hi) Algorithm:\\n        repeat\\n            aj = cubic_interpolation(a_lo, a_hi)\\n            Compute obi_func(aj) and derphi(aj).\\n            1. If obi_func(aj) > obi_func(0) + c_1 * aj * obi_func'(0) or obi_func(aj) >= obi_func(a_lo),\\n                then a_hi <- aj;\\n            2.\\n                2.1. If |obi_func'(aj)| <= -c_2 * obi_func'(0), then alpha= a2 and stop;\\n\\n                2.2. If obi_func'(aj) * (a2 - a1) >= 0, then a_hi = a_lo\\n\\n                a_lo = aj;\\n        end(repeat)\\n    \"\n    d_norm = d.abs().max()\n    grad = grad.clone()\n    (loss_new, grad_new) = obj_func(xk, alpha, d)\n    ls_func_evals = 1\n    gtd_new = paddle.dot(grad_new, d)\n    (t_prev, f_prev, g_prev, gtd_prev) = (paddle.to_tensor(0, dtype=grad.dtype), loss, grad, gtd)\n    done = False\n    ls_iter = 0\n    while ls_iter < max_ls:\n        if loss_new > loss + c1 * alpha * gtd or (ls_iter > 1 and loss_new >= f_prev):\n            bracket = [t_prev, alpha]\n            bracket_f = [f_prev, loss_new]\n            bracket_g = [g_prev, grad_new.clone()]\n            bracket_gtd = [gtd_prev, gtd_new]\n            break\n        if paddle.abs(gtd_new) <= -c2 * gtd:\n            bracket = [alpha]\n            bracket_f = [loss_new]\n            bracket_g = [grad_new]\n            done = True\n            break\n        if gtd_new >= 0:\n            bracket = [t_prev, alpha]\n            bracket_f = [f_prev, loss_new]\n            bracket_g = [g_prev, grad_new.clone()]\n            bracket_gtd = [gtd_prev, gtd_new]\n            break\n        min_step = alpha + 0.01 * (alpha - t_prev)\n        max_step = alpha * 10\n        tmp = alpha\n        alpha = _cubic_interpolate(t_prev, f_prev, gtd_prev, alpha, loss_new, gtd_new, bounds=(min_step, max_step))\n        t_prev = tmp\n        f_prev = loss_new\n        g_prev = grad_new.clone()\n        gtd_prev = gtd_new\n        (loss_new, grad_new) = obj_func(xk, alpha, d)\n        ls_func_evals += 1\n        gtd_new = grad_new.dot(d)\n        ls_iter += 1\n    if ls_iter == max_ls:\n        bracket = [0, alpha]\n        bracket_f = [loss, loss_new]\n        bracket_g = [grad, grad_new]\n    insuf_progress = False\n    (low_pos, high_pos) = (0, 1) if bracket_f[0] <= bracket_f[-1] else (1, 0)\n    while not done and ls_iter < max_ls:\n        if paddle.abs(bracket[1] - bracket[0]) * d_norm < tolerance_change:\n            break\n        alpha = _cubic_interpolate(bracket[0], bracket_f[0], bracket_gtd[0], bracket[1], bracket_f[1], bracket_gtd[1])\n        eps = 0.1 * (max(bracket) - min(bracket))\n        if min(max(bracket) - alpha, alpha - min(bracket)) < eps:\n            if insuf_progress or alpha >= max(bracket) or alpha <= min(bracket):\n                if paddle.abs(alpha - max(bracket)) < paddle.abs(alpha - min(bracket)):\n                    alpha = max(bracket) - eps\n                else:\n                    alpha = min(bracket) + eps\n                insuf_progress = False\n            else:\n                insuf_progress = True\n        else:\n            insuf_progress = False\n        (loss_new, grad_new) = obj_func(xk, alpha, d)\n        ls_func_evals += 1\n        gtd_new = grad_new.dot(d)\n        ls_iter += 1\n        if loss_new > loss + c1 * alpha * gtd or loss_new >= bracket_f[low_pos]:\n            bracket[high_pos] = alpha\n            bracket_f[high_pos] = loss_new\n            bracket_g[high_pos] = grad_new.clone()\n            bracket_gtd[high_pos] = gtd_new\n            (low_pos, high_pos) = (0, 1) if bracket_f[0] <= bracket_f[1] else (1, 0)\n        else:\n            if paddle.abs(gtd_new) <= -c2 * gtd:\n                done = True\n            elif gtd_new * (bracket[high_pos] - bracket[low_pos]) >= 0:\n                bracket[high_pos] = bracket[low_pos]\n                bracket_f[high_pos] = bracket_f[low_pos]\n                bracket_g[high_pos] = bracket_g[low_pos]\n                bracket_gtd[high_pos] = bracket_gtd[low_pos]\n            bracket[low_pos] = alpha\n            bracket_f[low_pos] = loss_new\n            bracket_g[low_pos] = grad_new.clone()\n            bracket_gtd[low_pos] = gtd_new\n    alpha = bracket[low_pos]\n    loss_new = bracket_f[low_pos]\n    grad_new = bracket_g[low_pos]\n    return (loss_new, grad_new, alpha, ls_func_evals)",
            "def _strong_wolfe(obj_func, xk, alpha, d, loss, grad, gtd, c1=0.0001, c2=0.9, tolerance_change=1e-09, max_ls=25):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Implements of line search algorithm that satisfies the strong Wolfe conditions using double zoom.\\n\\n    Reference:\\n        Jorge Nocedal, Stephen J. Wright, Numerical Optimization, Second Edition, 2006.\\n        pp60: Algorithm 3.5 (Line Search Algorithm).\\n\\n    Args:\\n        obj_func: the objective function to minimize. ```` accepts a multivariate input and returns a scalar.\\n        xk (Tensor): the starting point of the iterates.\\n        alpha (Scalar): the initial step size.\\n        d (Tensor): search direction.\\n        loss (scalar): the initial loss\\n        grad (Tensor): the initial grad\\n        c1 (Scalar): parameter for sufficient decrease condition.\\n        c2 (Scalar): parameter for curvature condition.\\n        tolerance_change (Scalar): terminates if the change of function value/position/parameter between\\n            two iterations is smaller than this value.\\n        max_ls(int): max iteration of line search.\\n        alpha_max (float): max step length.\\n\\n    Returns:\\n        loss_new (Scaler): loss of obj_func at final alpha.\\n        grad_new, (Tensor): derivative of obj_func at final alpha.\\n        alpha(Tensor): optimal step length, or 0. if the line search algorithm did not converge.\\n        ls_func_evals (Scaler): number of objective function called in line search process.\\n\\n    Following summarizes the essentials of the strong Wolfe line search algorithm.\\n    Some notations used in the description:\\n\\n        - `func` denotes the objective function.\\n        - `obi_func` is a function of step size alpha, restricting `obj_func` on a line.\\n\\n            obi_func = func(xk + alpha * d),\\n            where xk is the position of k'th iterate, d is the line search direction(decent direction),\\n            and a is the step size.\\n        - alpha : substitute of alpha\\n        - a1 is alpha of last iteration, which is alpha_(i-1).\\n        - a2 is alpha of current iteration, which is alpha_i.\\n        - a_lo is alpha in left position when calls zoom, which is alpha_low.\\n        - a_hi is alpha in right position when calls zoom, which is alpha_high.\\n\\n    Line Search Algorithm:\\n        repeat\\n            Compute obi_func(a2) and derphi(a2).\\n            1. If obi_func(a2) > obi_func(0) + c_1 * a2 * obi_func'(0) or [obi_func(a2) >= obi_func(a1) and i > 1],\\n                alpha= zoom(a1, a2) and stop;\\n\\n            2. If |obi_func'(a2)| <= -c_2 * obi_func'(0),\\n                alpha= a2 and stop;\\n\\n            3. If obi_func'(a2) >= 0,\\n                alpha= zoom(a2, a1) and stop;\\n\\n            a1 = a2\\n            a2 = min(2 * a2, a2)\\n            i = i + 1\\n        end(repeat)\\n\\n    zoom(a_lo, a_hi) Algorithm:\\n        repeat\\n            aj = cubic_interpolation(a_lo, a_hi)\\n            Compute obi_func(aj) and derphi(aj).\\n            1. If obi_func(aj) > obi_func(0) + c_1 * aj * obi_func'(0) or obi_func(aj) >= obi_func(a_lo),\\n                then a_hi <- aj;\\n            2.\\n                2.1. If |obi_func'(aj)| <= -c_2 * obi_func'(0), then alpha= a2 and stop;\\n\\n                2.2. If obi_func'(aj) * (a2 - a1) >= 0, then a_hi = a_lo\\n\\n                a_lo = aj;\\n        end(repeat)\\n    \"\n    d_norm = d.abs().max()\n    grad = grad.clone()\n    (loss_new, grad_new) = obj_func(xk, alpha, d)\n    ls_func_evals = 1\n    gtd_new = paddle.dot(grad_new, d)\n    (t_prev, f_prev, g_prev, gtd_prev) = (paddle.to_tensor(0, dtype=grad.dtype), loss, grad, gtd)\n    done = False\n    ls_iter = 0\n    while ls_iter < max_ls:\n        if loss_new > loss + c1 * alpha * gtd or (ls_iter > 1 and loss_new >= f_prev):\n            bracket = [t_prev, alpha]\n            bracket_f = [f_prev, loss_new]\n            bracket_g = [g_prev, grad_new.clone()]\n            bracket_gtd = [gtd_prev, gtd_new]\n            break\n        if paddle.abs(gtd_new) <= -c2 * gtd:\n            bracket = [alpha]\n            bracket_f = [loss_new]\n            bracket_g = [grad_new]\n            done = True\n            break\n        if gtd_new >= 0:\n            bracket = [t_prev, alpha]\n            bracket_f = [f_prev, loss_new]\n            bracket_g = [g_prev, grad_new.clone()]\n            bracket_gtd = [gtd_prev, gtd_new]\n            break\n        min_step = alpha + 0.01 * (alpha - t_prev)\n        max_step = alpha * 10\n        tmp = alpha\n        alpha = _cubic_interpolate(t_prev, f_prev, gtd_prev, alpha, loss_new, gtd_new, bounds=(min_step, max_step))\n        t_prev = tmp\n        f_prev = loss_new\n        g_prev = grad_new.clone()\n        gtd_prev = gtd_new\n        (loss_new, grad_new) = obj_func(xk, alpha, d)\n        ls_func_evals += 1\n        gtd_new = grad_new.dot(d)\n        ls_iter += 1\n    if ls_iter == max_ls:\n        bracket = [0, alpha]\n        bracket_f = [loss, loss_new]\n        bracket_g = [grad, grad_new]\n    insuf_progress = False\n    (low_pos, high_pos) = (0, 1) if bracket_f[0] <= bracket_f[-1] else (1, 0)\n    while not done and ls_iter < max_ls:\n        if paddle.abs(bracket[1] - bracket[0]) * d_norm < tolerance_change:\n            break\n        alpha = _cubic_interpolate(bracket[0], bracket_f[0], bracket_gtd[0], bracket[1], bracket_f[1], bracket_gtd[1])\n        eps = 0.1 * (max(bracket) - min(bracket))\n        if min(max(bracket) - alpha, alpha - min(bracket)) < eps:\n            if insuf_progress or alpha >= max(bracket) or alpha <= min(bracket):\n                if paddle.abs(alpha - max(bracket)) < paddle.abs(alpha - min(bracket)):\n                    alpha = max(bracket) - eps\n                else:\n                    alpha = min(bracket) + eps\n                insuf_progress = False\n            else:\n                insuf_progress = True\n        else:\n            insuf_progress = False\n        (loss_new, grad_new) = obj_func(xk, alpha, d)\n        ls_func_evals += 1\n        gtd_new = grad_new.dot(d)\n        ls_iter += 1\n        if loss_new > loss + c1 * alpha * gtd or loss_new >= bracket_f[low_pos]:\n            bracket[high_pos] = alpha\n            bracket_f[high_pos] = loss_new\n            bracket_g[high_pos] = grad_new.clone()\n            bracket_gtd[high_pos] = gtd_new\n            (low_pos, high_pos) = (0, 1) if bracket_f[0] <= bracket_f[1] else (1, 0)\n        else:\n            if paddle.abs(gtd_new) <= -c2 * gtd:\n                done = True\n            elif gtd_new * (bracket[high_pos] - bracket[low_pos]) >= 0:\n                bracket[high_pos] = bracket[low_pos]\n                bracket_f[high_pos] = bracket_f[low_pos]\n                bracket_g[high_pos] = bracket_g[low_pos]\n                bracket_gtd[high_pos] = bracket_gtd[low_pos]\n            bracket[low_pos] = alpha\n            bracket_f[low_pos] = loss_new\n            bracket_g[low_pos] = grad_new.clone()\n            bracket_gtd[low_pos] = gtd_new\n    alpha = bracket[low_pos]\n    loss_new = bracket_f[low_pos]\n    grad_new = bracket_g[low_pos]\n    return (loss_new, grad_new, alpha, ls_func_evals)",
            "def _strong_wolfe(obj_func, xk, alpha, d, loss, grad, gtd, c1=0.0001, c2=0.9, tolerance_change=1e-09, max_ls=25):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Implements of line search algorithm that satisfies the strong Wolfe conditions using double zoom.\\n\\n    Reference:\\n        Jorge Nocedal, Stephen J. Wright, Numerical Optimization, Second Edition, 2006.\\n        pp60: Algorithm 3.5 (Line Search Algorithm).\\n\\n    Args:\\n        obj_func: the objective function to minimize. ```` accepts a multivariate input and returns a scalar.\\n        xk (Tensor): the starting point of the iterates.\\n        alpha (Scalar): the initial step size.\\n        d (Tensor): search direction.\\n        loss (scalar): the initial loss\\n        grad (Tensor): the initial grad\\n        c1 (Scalar): parameter for sufficient decrease condition.\\n        c2 (Scalar): parameter for curvature condition.\\n        tolerance_change (Scalar): terminates if the change of function value/position/parameter between\\n            two iterations is smaller than this value.\\n        max_ls(int): max iteration of line search.\\n        alpha_max (float): max step length.\\n\\n    Returns:\\n        loss_new (Scaler): loss of obj_func at final alpha.\\n        grad_new, (Tensor): derivative of obj_func at final alpha.\\n        alpha(Tensor): optimal step length, or 0. if the line search algorithm did not converge.\\n        ls_func_evals (Scaler): number of objective function called in line search process.\\n\\n    Following summarizes the essentials of the strong Wolfe line search algorithm.\\n    Some notations used in the description:\\n\\n        - `func` denotes the objective function.\\n        - `obi_func` is a function of step size alpha, restricting `obj_func` on a line.\\n\\n            obi_func = func(xk + alpha * d),\\n            where xk is the position of k'th iterate, d is the line search direction(decent direction),\\n            and a is the step size.\\n        - alpha : substitute of alpha\\n        - a1 is alpha of last iteration, which is alpha_(i-1).\\n        - a2 is alpha of current iteration, which is alpha_i.\\n        - a_lo is alpha in left position when calls zoom, which is alpha_low.\\n        - a_hi is alpha in right position when calls zoom, which is alpha_high.\\n\\n    Line Search Algorithm:\\n        repeat\\n            Compute obi_func(a2) and derphi(a2).\\n            1. If obi_func(a2) > obi_func(0) + c_1 * a2 * obi_func'(0) or [obi_func(a2) >= obi_func(a1) and i > 1],\\n                alpha= zoom(a1, a2) and stop;\\n\\n            2. If |obi_func'(a2)| <= -c_2 * obi_func'(0),\\n                alpha= a2 and stop;\\n\\n            3. If obi_func'(a2) >= 0,\\n                alpha= zoom(a2, a1) and stop;\\n\\n            a1 = a2\\n            a2 = min(2 * a2, a2)\\n            i = i + 1\\n        end(repeat)\\n\\n    zoom(a_lo, a_hi) Algorithm:\\n        repeat\\n            aj = cubic_interpolation(a_lo, a_hi)\\n            Compute obi_func(aj) and derphi(aj).\\n            1. If obi_func(aj) > obi_func(0) + c_1 * aj * obi_func'(0) or obi_func(aj) >= obi_func(a_lo),\\n                then a_hi <- aj;\\n            2.\\n                2.1. If |obi_func'(aj)| <= -c_2 * obi_func'(0), then alpha= a2 and stop;\\n\\n                2.2. If obi_func'(aj) * (a2 - a1) >= 0, then a_hi = a_lo\\n\\n                a_lo = aj;\\n        end(repeat)\\n    \"\n    d_norm = d.abs().max()\n    grad = grad.clone()\n    (loss_new, grad_new) = obj_func(xk, alpha, d)\n    ls_func_evals = 1\n    gtd_new = paddle.dot(grad_new, d)\n    (t_prev, f_prev, g_prev, gtd_prev) = (paddle.to_tensor(0, dtype=grad.dtype), loss, grad, gtd)\n    done = False\n    ls_iter = 0\n    while ls_iter < max_ls:\n        if loss_new > loss + c1 * alpha * gtd or (ls_iter > 1 and loss_new >= f_prev):\n            bracket = [t_prev, alpha]\n            bracket_f = [f_prev, loss_new]\n            bracket_g = [g_prev, grad_new.clone()]\n            bracket_gtd = [gtd_prev, gtd_new]\n            break\n        if paddle.abs(gtd_new) <= -c2 * gtd:\n            bracket = [alpha]\n            bracket_f = [loss_new]\n            bracket_g = [grad_new]\n            done = True\n            break\n        if gtd_new >= 0:\n            bracket = [t_prev, alpha]\n            bracket_f = [f_prev, loss_new]\n            bracket_g = [g_prev, grad_new.clone()]\n            bracket_gtd = [gtd_prev, gtd_new]\n            break\n        min_step = alpha + 0.01 * (alpha - t_prev)\n        max_step = alpha * 10\n        tmp = alpha\n        alpha = _cubic_interpolate(t_prev, f_prev, gtd_prev, alpha, loss_new, gtd_new, bounds=(min_step, max_step))\n        t_prev = tmp\n        f_prev = loss_new\n        g_prev = grad_new.clone()\n        gtd_prev = gtd_new\n        (loss_new, grad_new) = obj_func(xk, alpha, d)\n        ls_func_evals += 1\n        gtd_new = grad_new.dot(d)\n        ls_iter += 1\n    if ls_iter == max_ls:\n        bracket = [0, alpha]\n        bracket_f = [loss, loss_new]\n        bracket_g = [grad, grad_new]\n    insuf_progress = False\n    (low_pos, high_pos) = (0, 1) if bracket_f[0] <= bracket_f[-1] else (1, 0)\n    while not done and ls_iter < max_ls:\n        if paddle.abs(bracket[1] - bracket[0]) * d_norm < tolerance_change:\n            break\n        alpha = _cubic_interpolate(bracket[0], bracket_f[0], bracket_gtd[0], bracket[1], bracket_f[1], bracket_gtd[1])\n        eps = 0.1 * (max(bracket) - min(bracket))\n        if min(max(bracket) - alpha, alpha - min(bracket)) < eps:\n            if insuf_progress or alpha >= max(bracket) or alpha <= min(bracket):\n                if paddle.abs(alpha - max(bracket)) < paddle.abs(alpha - min(bracket)):\n                    alpha = max(bracket) - eps\n                else:\n                    alpha = min(bracket) + eps\n                insuf_progress = False\n            else:\n                insuf_progress = True\n        else:\n            insuf_progress = False\n        (loss_new, grad_new) = obj_func(xk, alpha, d)\n        ls_func_evals += 1\n        gtd_new = grad_new.dot(d)\n        ls_iter += 1\n        if loss_new > loss + c1 * alpha * gtd or loss_new >= bracket_f[low_pos]:\n            bracket[high_pos] = alpha\n            bracket_f[high_pos] = loss_new\n            bracket_g[high_pos] = grad_new.clone()\n            bracket_gtd[high_pos] = gtd_new\n            (low_pos, high_pos) = (0, 1) if bracket_f[0] <= bracket_f[1] else (1, 0)\n        else:\n            if paddle.abs(gtd_new) <= -c2 * gtd:\n                done = True\n            elif gtd_new * (bracket[high_pos] - bracket[low_pos]) >= 0:\n                bracket[high_pos] = bracket[low_pos]\n                bracket_f[high_pos] = bracket_f[low_pos]\n                bracket_g[high_pos] = bracket_g[low_pos]\n                bracket_gtd[high_pos] = bracket_gtd[low_pos]\n            bracket[low_pos] = alpha\n            bracket_f[low_pos] = loss_new\n            bracket_g[low_pos] = grad_new.clone()\n            bracket_gtd[low_pos] = gtd_new\n    alpha = bracket[low_pos]\n    loss_new = bracket_f[low_pos]\n    grad_new = bracket_g[low_pos]\n    return (loss_new, grad_new, alpha, ls_func_evals)",
            "def _strong_wolfe(obj_func, xk, alpha, d, loss, grad, gtd, c1=0.0001, c2=0.9, tolerance_change=1e-09, max_ls=25):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Implements of line search algorithm that satisfies the strong Wolfe conditions using double zoom.\\n\\n    Reference:\\n        Jorge Nocedal, Stephen J. Wright, Numerical Optimization, Second Edition, 2006.\\n        pp60: Algorithm 3.5 (Line Search Algorithm).\\n\\n    Args:\\n        obj_func: the objective function to minimize. ```` accepts a multivariate input and returns a scalar.\\n        xk (Tensor): the starting point of the iterates.\\n        alpha (Scalar): the initial step size.\\n        d (Tensor): search direction.\\n        loss (scalar): the initial loss\\n        grad (Tensor): the initial grad\\n        c1 (Scalar): parameter for sufficient decrease condition.\\n        c2 (Scalar): parameter for curvature condition.\\n        tolerance_change (Scalar): terminates if the change of function value/position/parameter between\\n            two iterations is smaller than this value.\\n        max_ls(int): max iteration of line search.\\n        alpha_max (float): max step length.\\n\\n    Returns:\\n        loss_new (Scaler): loss of obj_func at final alpha.\\n        grad_new, (Tensor): derivative of obj_func at final alpha.\\n        alpha(Tensor): optimal step length, or 0. if the line search algorithm did not converge.\\n        ls_func_evals (Scaler): number of objective function called in line search process.\\n\\n    Following summarizes the essentials of the strong Wolfe line search algorithm.\\n    Some notations used in the description:\\n\\n        - `func` denotes the objective function.\\n        - `obi_func` is a function of step size alpha, restricting `obj_func` on a line.\\n\\n            obi_func = func(xk + alpha * d),\\n            where xk is the position of k'th iterate, d is the line search direction(decent direction),\\n            and a is the step size.\\n        - alpha : substitute of alpha\\n        - a1 is alpha of last iteration, which is alpha_(i-1).\\n        - a2 is alpha of current iteration, which is alpha_i.\\n        - a_lo is alpha in left position when calls zoom, which is alpha_low.\\n        - a_hi is alpha in right position when calls zoom, which is alpha_high.\\n\\n    Line Search Algorithm:\\n        repeat\\n            Compute obi_func(a2) and derphi(a2).\\n            1. If obi_func(a2) > obi_func(0) + c_1 * a2 * obi_func'(0) or [obi_func(a2) >= obi_func(a1) and i > 1],\\n                alpha= zoom(a1, a2) and stop;\\n\\n            2. If |obi_func'(a2)| <= -c_2 * obi_func'(0),\\n                alpha= a2 and stop;\\n\\n            3. If obi_func'(a2) >= 0,\\n                alpha= zoom(a2, a1) and stop;\\n\\n            a1 = a2\\n            a2 = min(2 * a2, a2)\\n            i = i + 1\\n        end(repeat)\\n\\n    zoom(a_lo, a_hi) Algorithm:\\n        repeat\\n            aj = cubic_interpolation(a_lo, a_hi)\\n            Compute obi_func(aj) and derphi(aj).\\n            1. If obi_func(aj) > obi_func(0) + c_1 * aj * obi_func'(0) or obi_func(aj) >= obi_func(a_lo),\\n                then a_hi <- aj;\\n            2.\\n                2.1. If |obi_func'(aj)| <= -c_2 * obi_func'(0), then alpha= a2 and stop;\\n\\n                2.2. If obi_func'(aj) * (a2 - a1) >= 0, then a_hi = a_lo\\n\\n                a_lo = aj;\\n        end(repeat)\\n    \"\n    d_norm = d.abs().max()\n    grad = grad.clone()\n    (loss_new, grad_new) = obj_func(xk, alpha, d)\n    ls_func_evals = 1\n    gtd_new = paddle.dot(grad_new, d)\n    (t_prev, f_prev, g_prev, gtd_prev) = (paddle.to_tensor(0, dtype=grad.dtype), loss, grad, gtd)\n    done = False\n    ls_iter = 0\n    while ls_iter < max_ls:\n        if loss_new > loss + c1 * alpha * gtd or (ls_iter > 1 and loss_new >= f_prev):\n            bracket = [t_prev, alpha]\n            bracket_f = [f_prev, loss_new]\n            bracket_g = [g_prev, grad_new.clone()]\n            bracket_gtd = [gtd_prev, gtd_new]\n            break\n        if paddle.abs(gtd_new) <= -c2 * gtd:\n            bracket = [alpha]\n            bracket_f = [loss_new]\n            bracket_g = [grad_new]\n            done = True\n            break\n        if gtd_new >= 0:\n            bracket = [t_prev, alpha]\n            bracket_f = [f_prev, loss_new]\n            bracket_g = [g_prev, grad_new.clone()]\n            bracket_gtd = [gtd_prev, gtd_new]\n            break\n        min_step = alpha + 0.01 * (alpha - t_prev)\n        max_step = alpha * 10\n        tmp = alpha\n        alpha = _cubic_interpolate(t_prev, f_prev, gtd_prev, alpha, loss_new, gtd_new, bounds=(min_step, max_step))\n        t_prev = tmp\n        f_prev = loss_new\n        g_prev = grad_new.clone()\n        gtd_prev = gtd_new\n        (loss_new, grad_new) = obj_func(xk, alpha, d)\n        ls_func_evals += 1\n        gtd_new = grad_new.dot(d)\n        ls_iter += 1\n    if ls_iter == max_ls:\n        bracket = [0, alpha]\n        bracket_f = [loss, loss_new]\n        bracket_g = [grad, grad_new]\n    insuf_progress = False\n    (low_pos, high_pos) = (0, 1) if bracket_f[0] <= bracket_f[-1] else (1, 0)\n    while not done and ls_iter < max_ls:\n        if paddle.abs(bracket[1] - bracket[0]) * d_norm < tolerance_change:\n            break\n        alpha = _cubic_interpolate(bracket[0], bracket_f[0], bracket_gtd[0], bracket[1], bracket_f[1], bracket_gtd[1])\n        eps = 0.1 * (max(bracket) - min(bracket))\n        if min(max(bracket) - alpha, alpha - min(bracket)) < eps:\n            if insuf_progress or alpha >= max(bracket) or alpha <= min(bracket):\n                if paddle.abs(alpha - max(bracket)) < paddle.abs(alpha - min(bracket)):\n                    alpha = max(bracket) - eps\n                else:\n                    alpha = min(bracket) + eps\n                insuf_progress = False\n            else:\n                insuf_progress = True\n        else:\n            insuf_progress = False\n        (loss_new, grad_new) = obj_func(xk, alpha, d)\n        ls_func_evals += 1\n        gtd_new = grad_new.dot(d)\n        ls_iter += 1\n        if loss_new > loss + c1 * alpha * gtd or loss_new >= bracket_f[low_pos]:\n            bracket[high_pos] = alpha\n            bracket_f[high_pos] = loss_new\n            bracket_g[high_pos] = grad_new.clone()\n            bracket_gtd[high_pos] = gtd_new\n            (low_pos, high_pos) = (0, 1) if bracket_f[0] <= bracket_f[1] else (1, 0)\n        else:\n            if paddle.abs(gtd_new) <= -c2 * gtd:\n                done = True\n            elif gtd_new * (bracket[high_pos] - bracket[low_pos]) >= 0:\n                bracket[high_pos] = bracket[low_pos]\n                bracket_f[high_pos] = bracket_f[low_pos]\n                bracket_g[high_pos] = bracket_g[low_pos]\n                bracket_gtd[high_pos] = bracket_gtd[low_pos]\n            bracket[low_pos] = alpha\n            bracket_f[low_pos] = loss_new\n            bracket_g[low_pos] = grad_new.clone()\n            bracket_gtd[low_pos] = gtd_new\n    alpha = bracket[low_pos]\n    loss_new = bracket_f[low_pos]\n    grad_new = bracket_g[low_pos]\n    return (loss_new, grad_new, alpha, ls_func_evals)",
            "def _strong_wolfe(obj_func, xk, alpha, d, loss, grad, gtd, c1=0.0001, c2=0.9, tolerance_change=1e-09, max_ls=25):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Implements of line search algorithm that satisfies the strong Wolfe conditions using double zoom.\\n\\n    Reference:\\n        Jorge Nocedal, Stephen J. Wright, Numerical Optimization, Second Edition, 2006.\\n        pp60: Algorithm 3.5 (Line Search Algorithm).\\n\\n    Args:\\n        obj_func: the objective function to minimize. ```` accepts a multivariate input and returns a scalar.\\n        xk (Tensor): the starting point of the iterates.\\n        alpha (Scalar): the initial step size.\\n        d (Tensor): search direction.\\n        loss (scalar): the initial loss\\n        grad (Tensor): the initial grad\\n        c1 (Scalar): parameter for sufficient decrease condition.\\n        c2 (Scalar): parameter for curvature condition.\\n        tolerance_change (Scalar): terminates if the change of function value/position/parameter between\\n            two iterations is smaller than this value.\\n        max_ls(int): max iteration of line search.\\n        alpha_max (float): max step length.\\n\\n    Returns:\\n        loss_new (Scaler): loss of obj_func at final alpha.\\n        grad_new, (Tensor): derivative of obj_func at final alpha.\\n        alpha(Tensor): optimal step length, or 0. if the line search algorithm did not converge.\\n        ls_func_evals (Scaler): number of objective function called in line search process.\\n\\n    Following summarizes the essentials of the strong Wolfe line search algorithm.\\n    Some notations used in the description:\\n\\n        - `func` denotes the objective function.\\n        - `obi_func` is a function of step size alpha, restricting `obj_func` on a line.\\n\\n            obi_func = func(xk + alpha * d),\\n            where xk is the position of k'th iterate, d is the line search direction(decent direction),\\n            and a is the step size.\\n        - alpha : substitute of alpha\\n        - a1 is alpha of last iteration, which is alpha_(i-1).\\n        - a2 is alpha of current iteration, which is alpha_i.\\n        - a_lo is alpha in left position when calls zoom, which is alpha_low.\\n        - a_hi is alpha in right position when calls zoom, which is alpha_high.\\n\\n    Line Search Algorithm:\\n        repeat\\n            Compute obi_func(a2) and derphi(a2).\\n            1. If obi_func(a2) > obi_func(0) + c_1 * a2 * obi_func'(0) or [obi_func(a2) >= obi_func(a1) and i > 1],\\n                alpha= zoom(a1, a2) and stop;\\n\\n            2. If |obi_func'(a2)| <= -c_2 * obi_func'(0),\\n                alpha= a2 and stop;\\n\\n            3. If obi_func'(a2) >= 0,\\n                alpha= zoom(a2, a1) and stop;\\n\\n            a1 = a2\\n            a2 = min(2 * a2, a2)\\n            i = i + 1\\n        end(repeat)\\n\\n    zoom(a_lo, a_hi) Algorithm:\\n        repeat\\n            aj = cubic_interpolation(a_lo, a_hi)\\n            Compute obi_func(aj) and derphi(aj).\\n            1. If obi_func(aj) > obi_func(0) + c_1 * aj * obi_func'(0) or obi_func(aj) >= obi_func(a_lo),\\n                then a_hi <- aj;\\n            2.\\n                2.1. If |obi_func'(aj)| <= -c_2 * obi_func'(0), then alpha= a2 and stop;\\n\\n                2.2. If obi_func'(aj) * (a2 - a1) >= 0, then a_hi = a_lo\\n\\n                a_lo = aj;\\n        end(repeat)\\n    \"\n    d_norm = d.abs().max()\n    grad = grad.clone()\n    (loss_new, grad_new) = obj_func(xk, alpha, d)\n    ls_func_evals = 1\n    gtd_new = paddle.dot(grad_new, d)\n    (t_prev, f_prev, g_prev, gtd_prev) = (paddle.to_tensor(0, dtype=grad.dtype), loss, grad, gtd)\n    done = False\n    ls_iter = 0\n    while ls_iter < max_ls:\n        if loss_new > loss + c1 * alpha * gtd or (ls_iter > 1 and loss_new >= f_prev):\n            bracket = [t_prev, alpha]\n            bracket_f = [f_prev, loss_new]\n            bracket_g = [g_prev, grad_new.clone()]\n            bracket_gtd = [gtd_prev, gtd_new]\n            break\n        if paddle.abs(gtd_new) <= -c2 * gtd:\n            bracket = [alpha]\n            bracket_f = [loss_new]\n            bracket_g = [grad_new]\n            done = True\n            break\n        if gtd_new >= 0:\n            bracket = [t_prev, alpha]\n            bracket_f = [f_prev, loss_new]\n            bracket_g = [g_prev, grad_new.clone()]\n            bracket_gtd = [gtd_prev, gtd_new]\n            break\n        min_step = alpha + 0.01 * (alpha - t_prev)\n        max_step = alpha * 10\n        tmp = alpha\n        alpha = _cubic_interpolate(t_prev, f_prev, gtd_prev, alpha, loss_new, gtd_new, bounds=(min_step, max_step))\n        t_prev = tmp\n        f_prev = loss_new\n        g_prev = grad_new.clone()\n        gtd_prev = gtd_new\n        (loss_new, grad_new) = obj_func(xk, alpha, d)\n        ls_func_evals += 1\n        gtd_new = grad_new.dot(d)\n        ls_iter += 1\n    if ls_iter == max_ls:\n        bracket = [0, alpha]\n        bracket_f = [loss, loss_new]\n        bracket_g = [grad, grad_new]\n    insuf_progress = False\n    (low_pos, high_pos) = (0, 1) if bracket_f[0] <= bracket_f[-1] else (1, 0)\n    while not done and ls_iter < max_ls:\n        if paddle.abs(bracket[1] - bracket[0]) * d_norm < tolerance_change:\n            break\n        alpha = _cubic_interpolate(bracket[0], bracket_f[0], bracket_gtd[0], bracket[1], bracket_f[1], bracket_gtd[1])\n        eps = 0.1 * (max(bracket) - min(bracket))\n        if min(max(bracket) - alpha, alpha - min(bracket)) < eps:\n            if insuf_progress or alpha >= max(bracket) or alpha <= min(bracket):\n                if paddle.abs(alpha - max(bracket)) < paddle.abs(alpha - min(bracket)):\n                    alpha = max(bracket) - eps\n                else:\n                    alpha = min(bracket) + eps\n                insuf_progress = False\n            else:\n                insuf_progress = True\n        else:\n            insuf_progress = False\n        (loss_new, grad_new) = obj_func(xk, alpha, d)\n        ls_func_evals += 1\n        gtd_new = grad_new.dot(d)\n        ls_iter += 1\n        if loss_new > loss + c1 * alpha * gtd or loss_new >= bracket_f[low_pos]:\n            bracket[high_pos] = alpha\n            bracket_f[high_pos] = loss_new\n            bracket_g[high_pos] = grad_new.clone()\n            bracket_gtd[high_pos] = gtd_new\n            (low_pos, high_pos) = (0, 1) if bracket_f[0] <= bracket_f[1] else (1, 0)\n        else:\n            if paddle.abs(gtd_new) <= -c2 * gtd:\n                done = True\n            elif gtd_new * (bracket[high_pos] - bracket[low_pos]) >= 0:\n                bracket[high_pos] = bracket[low_pos]\n                bracket_f[high_pos] = bracket_f[low_pos]\n                bracket_g[high_pos] = bracket_g[low_pos]\n                bracket_gtd[high_pos] = bracket_gtd[low_pos]\n            bracket[low_pos] = alpha\n            bracket_f[low_pos] = loss_new\n            bracket_g[low_pos] = grad_new.clone()\n            bracket_gtd[low_pos] = gtd_new\n    alpha = bracket[low_pos]\n    loss_new = bracket_f[low_pos]\n    grad_new = bracket_g[low_pos]\n    return (loss_new, grad_new, alpha, ls_func_evals)"
        ]
    }
]