[
    {
        "func_name": "rshift_time",
        "original": "def rshift_time(tensor_2d, fill=misc.BF_EOS_INT):\n    \"\"\"Right shifts a 2D tensor along the time dimension (axis-1).\"\"\"\n    dim_0 = tf.shape(tensor_2d)[0]\n    fill_tensor = tf.fill([dim_0, 1], fill)\n    return tf.concat([fill_tensor, tensor_2d[:, :-1]], axis=1)",
        "mutated": [
            "def rshift_time(tensor_2d, fill=misc.BF_EOS_INT):\n    if False:\n        i = 10\n    'Right shifts a 2D tensor along the time dimension (axis-1).'\n    dim_0 = tf.shape(tensor_2d)[0]\n    fill_tensor = tf.fill([dim_0, 1], fill)\n    return tf.concat([fill_tensor, tensor_2d[:, :-1]], axis=1)",
            "def rshift_time(tensor_2d, fill=misc.BF_EOS_INT):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Right shifts a 2D tensor along the time dimension (axis-1).'\n    dim_0 = tf.shape(tensor_2d)[0]\n    fill_tensor = tf.fill([dim_0, 1], fill)\n    return tf.concat([fill_tensor, tensor_2d[:, :-1]], axis=1)",
            "def rshift_time(tensor_2d, fill=misc.BF_EOS_INT):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Right shifts a 2D tensor along the time dimension (axis-1).'\n    dim_0 = tf.shape(tensor_2d)[0]\n    fill_tensor = tf.fill([dim_0, 1], fill)\n    return tf.concat([fill_tensor, tensor_2d[:, :-1]], axis=1)",
            "def rshift_time(tensor_2d, fill=misc.BF_EOS_INT):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Right shifts a 2D tensor along the time dimension (axis-1).'\n    dim_0 = tf.shape(tensor_2d)[0]\n    fill_tensor = tf.fill([dim_0, 1], fill)\n    return tf.concat([fill_tensor, tensor_2d[:, :-1]], axis=1)",
            "def rshift_time(tensor_2d, fill=misc.BF_EOS_INT):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Right shifts a 2D tensor along the time dimension (axis-1).'\n    dim_0 = tf.shape(tensor_2d)[0]\n    fill_tensor = tf.fill([dim_0, 1], fill)\n    return tf.concat([fill_tensor, tensor_2d[:, :-1]], axis=1)"
        ]
    },
    {
        "func_name": "join",
        "original": "def join(a, b):\n    if a is None or len(a) == 0:\n        return b\n    if b is None or len(b) == 0:\n        return a\n    return np.concatenate((a, b))",
        "mutated": [
            "def join(a, b):\n    if False:\n        i = 10\n    if a is None or len(a) == 0:\n        return b\n    if b is None or len(b) == 0:\n        return a\n    return np.concatenate((a, b))",
            "def join(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if a is None or len(a) == 0:\n        return b\n    if b is None or len(b) == 0:\n        return a\n    return np.concatenate((a, b))",
            "def join(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if a is None or len(a) == 0:\n        return b\n    if b is None or len(b) == 0:\n        return a\n    return np.concatenate((a, b))",
            "def join(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if a is None or len(a) == 0:\n        return b\n    if b is None or len(b) == 0:\n        return a\n    return np.concatenate((a, b))",
            "def join(a, b):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if a is None or len(a) == 0:\n        return b\n    if b is None or len(b) == 0:\n        return a\n    return np.concatenate((a, b))"
        ]
    },
    {
        "func_name": "make_optimizer",
        "original": "def make_optimizer(kind, lr):\n    if kind == 'sgd':\n        return tf.train.GradientDescentOptimizer(lr)\n    elif kind == 'adam':\n        return tf.train.AdamOptimizer(lr)\n    elif kind == 'rmsprop':\n        return tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.99)\n    else:\n        raise ValueError('Optimizer type \"%s\" not recognized.' % kind)",
        "mutated": [
            "def make_optimizer(kind, lr):\n    if False:\n        i = 10\n    if kind == 'sgd':\n        return tf.train.GradientDescentOptimizer(lr)\n    elif kind == 'adam':\n        return tf.train.AdamOptimizer(lr)\n    elif kind == 'rmsprop':\n        return tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.99)\n    else:\n        raise ValueError('Optimizer type \"%s\" not recognized.' % kind)",
            "def make_optimizer(kind, lr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if kind == 'sgd':\n        return tf.train.GradientDescentOptimizer(lr)\n    elif kind == 'adam':\n        return tf.train.AdamOptimizer(lr)\n    elif kind == 'rmsprop':\n        return tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.99)\n    else:\n        raise ValueError('Optimizer type \"%s\" not recognized.' % kind)",
            "def make_optimizer(kind, lr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if kind == 'sgd':\n        return tf.train.GradientDescentOptimizer(lr)\n    elif kind == 'adam':\n        return tf.train.AdamOptimizer(lr)\n    elif kind == 'rmsprop':\n        return tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.99)\n    else:\n        raise ValueError('Optimizer type \"%s\" not recognized.' % kind)",
            "def make_optimizer(kind, lr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if kind == 'sgd':\n        return tf.train.GradientDescentOptimizer(lr)\n    elif kind == 'adam':\n        return tf.train.AdamOptimizer(lr)\n    elif kind == 'rmsprop':\n        return tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.99)\n    else:\n        raise ValueError('Optimizer type \"%s\" not recognized.' % kind)",
            "def make_optimizer(kind, lr):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if kind == 'sgd':\n        return tf.train.GradientDescentOptimizer(lr)\n    elif kind == 'adam':\n        return tf.train.AdamOptimizer(lr)\n    elif kind == 'rmsprop':\n        return tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.99)\n    else:\n        raise ValueError('Optimizer type \"%s\" not recognized.' % kind)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, cell, output_size, dtype=tf.float32, suppress_index=None):\n    self.cell = cell\n    self._output_size = output_size\n    self._dtype = dtype\n    self._suppress_index = suppress_index\n    self.smallest_float = -2.4e+38",
        "mutated": [
            "def __init__(self, cell, output_size, dtype=tf.float32, suppress_index=None):\n    if False:\n        i = 10\n    self.cell = cell\n    self._output_size = output_size\n    self._dtype = dtype\n    self._suppress_index = suppress_index\n    self.smallest_float = -2.4e+38",
            "def __init__(self, cell, output_size, dtype=tf.float32, suppress_index=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.cell = cell\n    self._output_size = output_size\n    self._dtype = dtype\n    self._suppress_index = suppress_index\n    self.smallest_float = -2.4e+38",
            "def __init__(self, cell, output_size, dtype=tf.float32, suppress_index=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.cell = cell\n    self._output_size = output_size\n    self._dtype = dtype\n    self._suppress_index = suppress_index\n    self.smallest_float = -2.4e+38",
            "def __init__(self, cell, output_size, dtype=tf.float32, suppress_index=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.cell = cell\n    self._output_size = output_size\n    self._dtype = dtype\n    self._suppress_index = suppress_index\n    self.smallest_float = -2.4e+38",
            "def __init__(self, cell, output_size, dtype=tf.float32, suppress_index=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.cell = cell\n    self._output_size = output_size\n    self._dtype = dtype\n    self._suppress_index = suppress_index\n    self.smallest_float = -2.4e+38"
        ]
    },
    {
        "func_name": "__call__",
        "original": "def __call__(self, inputs, state, scope=None):\n    with tf.variable_scope(type(self).__name__):\n        (outputs, state) = self.cell(inputs, state, scope=scope)\n        logits = tf.matmul(outputs, tf.get_variable('w_output', [self.cell.output_size, self.output_size], dtype=self._dtype))\n        if self._suppress_index is not None:\n            batch_size = tf.shape(logits)[0]\n            logits = tf.concat([logits[:, :self._suppress_index], tf.fill([batch_size, 1], self.smallest_float), logits[:, self._suppress_index + 1:]], axis=1)\n    return (logits, state)",
        "mutated": [
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n    with tf.variable_scope(type(self).__name__):\n        (outputs, state) = self.cell(inputs, state, scope=scope)\n        logits = tf.matmul(outputs, tf.get_variable('w_output', [self.cell.output_size, self.output_size], dtype=self._dtype))\n        if self._suppress_index is not None:\n            batch_size = tf.shape(logits)[0]\n            logits = tf.concat([logits[:, :self._suppress_index], tf.fill([batch_size, 1], self.smallest_float), logits[:, self._suppress_index + 1:]], axis=1)\n    return (logits, state)",
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    with tf.variable_scope(type(self).__name__):\n        (outputs, state) = self.cell(inputs, state, scope=scope)\n        logits = tf.matmul(outputs, tf.get_variable('w_output', [self.cell.output_size, self.output_size], dtype=self._dtype))\n        if self._suppress_index is not None:\n            batch_size = tf.shape(logits)[0]\n            logits = tf.concat([logits[:, :self._suppress_index], tf.fill([batch_size, 1], self.smallest_float), logits[:, self._suppress_index + 1:]], axis=1)\n    return (logits, state)",
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    with tf.variable_scope(type(self).__name__):\n        (outputs, state) = self.cell(inputs, state, scope=scope)\n        logits = tf.matmul(outputs, tf.get_variable('w_output', [self.cell.output_size, self.output_size], dtype=self._dtype))\n        if self._suppress_index is not None:\n            batch_size = tf.shape(logits)[0]\n            logits = tf.concat([logits[:, :self._suppress_index], tf.fill([batch_size, 1], self.smallest_float), logits[:, self._suppress_index + 1:]], axis=1)\n    return (logits, state)",
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    with tf.variable_scope(type(self).__name__):\n        (outputs, state) = self.cell(inputs, state, scope=scope)\n        logits = tf.matmul(outputs, tf.get_variable('w_output', [self.cell.output_size, self.output_size], dtype=self._dtype))\n        if self._suppress_index is not None:\n            batch_size = tf.shape(logits)[0]\n            logits = tf.concat([logits[:, :self._suppress_index], tf.fill([batch_size, 1], self.smallest_float), logits[:, self._suppress_index + 1:]], axis=1)\n    return (logits, state)",
            "def __call__(self, inputs, state, scope=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    with tf.variable_scope(type(self).__name__):\n        (outputs, state) = self.cell(inputs, state, scope=scope)\n        logits = tf.matmul(outputs, tf.get_variable('w_output', [self.cell.output_size, self.output_size], dtype=self._dtype))\n        if self._suppress_index is not None:\n            batch_size = tf.shape(logits)[0]\n            logits = tf.concat([logits[:, :self._suppress_index], tf.fill([batch_size, 1], self.smallest_float), logits[:, self._suppress_index + 1:]], axis=1)\n    return (logits, state)"
        ]
    },
    {
        "func_name": "output_size",
        "original": "@property\ndef output_size(self):\n    return self._output_size",
        "mutated": [
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n    return self._output_size",
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self._output_size",
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self._output_size",
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self._output_size",
            "@property\ndef output_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self._output_size"
        ]
    },
    {
        "func_name": "state_size",
        "original": "@property\ndef state_size(self):\n    return self.cell.state_size",
        "mutated": [
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n    return self.cell.state_size",
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self.cell.state_size",
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self.cell.state_size",
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self.cell.state_size",
            "@property\ndef state_size(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self.cell.state_size"
        ]
    },
    {
        "func_name": "zero_state",
        "original": "def zero_state(self, batch_size, dtype):\n    return self.cell.zero_state(batch_size, dtype)",
        "mutated": [
            "def zero_state(self, batch_size, dtype):\n    if False:\n        i = 10\n    return self.cell.zero_state(batch_size, dtype)",
            "def zero_state(self, batch_size, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self.cell.zero_state(batch_size, dtype)",
            "def zero_state(self, batch_size, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self.cell.zero_state(batch_size, dtype)",
            "def zero_state(self, batch_size, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self.cell.zero_state(batch_size, dtype)",
            "def zero_state(self, batch_size, dtype):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self.cell.zero_state(batch_size, dtype)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, *args, **kwargs):\n    super(AttrDict, self).__init__(*args, **kwargs)\n    self.__dict__ = self",
        "mutated": [
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n    super(AttrDict, self).__init__(*args, **kwargs)\n    self.__dict__ = self",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super(AttrDict, self).__init__(*args, **kwargs)\n    self.__dict__ = self",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super(AttrDict, self).__init__(*args, **kwargs)\n    self.__dict__ = self",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super(AttrDict, self).__init__(*args, **kwargs)\n    self.__dict__ = self",
            "def __init__(self, *args, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super(AttrDict, self).__init__(*args, **kwargs)\n    self.__dict__ = self"
        ]
    },
    {
        "func_name": "loop_fn",
        "original": "def loop_fn(loop_time, cell_output, cell_state, loop_state):\n    \"\"\"Function called by tf.nn.raw_rnn to instantiate body of the while_loop.\n\n      See https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn for more\n      information.\n\n      When time is 0, and cell_output, cell_state, loop_state are all None,\n      `loop_fn` will create the initial input, internal cell state, and loop\n      state. When time > 0, `loop_fn` will operate on previous cell output,\n      state, and loop state.\n\n      Args:\n        loop_time: A scalar tensor holding the current timestep (zero based\n            counting).\n        cell_output: Output of the raw_rnn cell at the current timestep.\n        cell_state: Cell internal state at the current timestep.\n        loop_state: Additional loop state. These tensors were returned by the\n            previous call to `loop_fn`.\n\n      Returns:\n        elements_finished: Bool tensor of shape [batch_size] which marks each\n            sequence in the batch as being finished or not finished.\n        next_input: A tensor containing input to be fed into the cell at the\n            next timestep.\n        next_cell_state: Cell internal state to be fed into the cell at the\n            next timestep.\n        emit_output: Tensor to be added to the TensorArray returned by raw_rnn\n            as output from the while_loop.\n        next_loop_state: Additional loop state. These tensors will be fed back\n            into the next call to `loop_fn` as `loop_state`.\n      \"\"\"\n    if cell_output is None:\n        next_cell_state = self.policy_cell.zero_state(batch_size, dtype)\n        elements_finished = tf.zeros([batch_size], tf.bool)\n        output_lengths = tf.ones([batch_size], dtype=tf.int32)\n        next_input = tf.gather(obs_embeddings, initial_state)\n        emit_output = None\n        next_loop_state = (tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True), output_lengths, elements_finished)\n    else:\n        scaled_logits = cell_output * config.softmax_tr\n        (prev_chosen, prev_output_lengths, prev_elements_finished) = loop_state\n        next_cell_state = cell_state\n        chosen_outputs = tf.to_int32(tf.where(tf.logical_not(prev_elements_finished), tf.multinomial(logits=scaled_logits, num_samples=1)[:, 0], tf.zeros([batch_size], dtype=tf.int64)))\n        elements_finished = tf.logical_or(tf.equal(chosen_outputs, misc.BF_EOS_INT), loop_time >= global_config.timestep_limit)\n        output_lengths = tf.where(elements_finished, prev_output_lengths, tf.tile(tf.expand_dims(loop_time + 1, 0), [batch_size]))\n        next_input = tf.gather(obs_embeddings, chosen_outputs)\n        emit_output = scaled_logits\n        next_loop_state = (prev_chosen.write(loop_time - 1, chosen_outputs), output_lengths, tf.logical_or(prev_elements_finished, elements_finished))\n    return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)",
        "mutated": [
            "def loop_fn(loop_time, cell_output, cell_state, loop_state):\n    if False:\n        i = 10\n    'Function called by tf.nn.raw_rnn to instantiate body of the while_loop.\\n\\n      See https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn for more\\n      information.\\n\\n      When time is 0, and cell_output, cell_state, loop_state are all None,\\n      `loop_fn` will create the initial input, internal cell state, and loop\\n      state. When time > 0, `loop_fn` will operate on previous cell output,\\n      state, and loop state.\\n\\n      Args:\\n        loop_time: A scalar tensor holding the current timestep (zero based\\n            counting).\\n        cell_output: Output of the raw_rnn cell at the current timestep.\\n        cell_state: Cell internal state at the current timestep.\\n        loop_state: Additional loop state. These tensors were returned by the\\n            previous call to `loop_fn`.\\n\\n      Returns:\\n        elements_finished: Bool tensor of shape [batch_size] which marks each\\n            sequence in the batch as being finished or not finished.\\n        next_input: A tensor containing input to be fed into the cell at the\\n            next timestep.\\n        next_cell_state: Cell internal state to be fed into the cell at the\\n            next timestep.\\n        emit_output: Tensor to be added to the TensorArray returned by raw_rnn\\n            as output from the while_loop.\\n        next_loop_state: Additional loop state. These tensors will be fed back\\n            into the next call to `loop_fn` as `loop_state`.\\n      '\n    if cell_output is None:\n        next_cell_state = self.policy_cell.zero_state(batch_size, dtype)\n        elements_finished = tf.zeros([batch_size], tf.bool)\n        output_lengths = tf.ones([batch_size], dtype=tf.int32)\n        next_input = tf.gather(obs_embeddings, initial_state)\n        emit_output = None\n        next_loop_state = (tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True), output_lengths, elements_finished)\n    else:\n        scaled_logits = cell_output * config.softmax_tr\n        (prev_chosen, prev_output_lengths, prev_elements_finished) = loop_state\n        next_cell_state = cell_state\n        chosen_outputs = tf.to_int32(tf.where(tf.logical_not(prev_elements_finished), tf.multinomial(logits=scaled_logits, num_samples=1)[:, 0], tf.zeros([batch_size], dtype=tf.int64)))\n        elements_finished = tf.logical_or(tf.equal(chosen_outputs, misc.BF_EOS_INT), loop_time >= global_config.timestep_limit)\n        output_lengths = tf.where(elements_finished, prev_output_lengths, tf.tile(tf.expand_dims(loop_time + 1, 0), [batch_size]))\n        next_input = tf.gather(obs_embeddings, chosen_outputs)\n        emit_output = scaled_logits\n        next_loop_state = (prev_chosen.write(loop_time - 1, chosen_outputs), output_lengths, tf.logical_or(prev_elements_finished, elements_finished))\n    return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)",
            "def loop_fn(loop_time, cell_output, cell_state, loop_state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Function called by tf.nn.raw_rnn to instantiate body of the while_loop.\\n\\n      See https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn for more\\n      information.\\n\\n      When time is 0, and cell_output, cell_state, loop_state are all None,\\n      `loop_fn` will create the initial input, internal cell state, and loop\\n      state. When time > 0, `loop_fn` will operate on previous cell output,\\n      state, and loop state.\\n\\n      Args:\\n        loop_time: A scalar tensor holding the current timestep (zero based\\n            counting).\\n        cell_output: Output of the raw_rnn cell at the current timestep.\\n        cell_state: Cell internal state at the current timestep.\\n        loop_state: Additional loop state. These tensors were returned by the\\n            previous call to `loop_fn`.\\n\\n      Returns:\\n        elements_finished: Bool tensor of shape [batch_size] which marks each\\n            sequence in the batch as being finished or not finished.\\n        next_input: A tensor containing input to be fed into the cell at the\\n            next timestep.\\n        next_cell_state: Cell internal state to be fed into the cell at the\\n            next timestep.\\n        emit_output: Tensor to be added to the TensorArray returned by raw_rnn\\n            as output from the while_loop.\\n        next_loop_state: Additional loop state. These tensors will be fed back\\n            into the next call to `loop_fn` as `loop_state`.\\n      '\n    if cell_output is None:\n        next_cell_state = self.policy_cell.zero_state(batch_size, dtype)\n        elements_finished = tf.zeros([batch_size], tf.bool)\n        output_lengths = tf.ones([batch_size], dtype=tf.int32)\n        next_input = tf.gather(obs_embeddings, initial_state)\n        emit_output = None\n        next_loop_state = (tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True), output_lengths, elements_finished)\n    else:\n        scaled_logits = cell_output * config.softmax_tr\n        (prev_chosen, prev_output_lengths, prev_elements_finished) = loop_state\n        next_cell_state = cell_state\n        chosen_outputs = tf.to_int32(tf.where(tf.logical_not(prev_elements_finished), tf.multinomial(logits=scaled_logits, num_samples=1)[:, 0], tf.zeros([batch_size], dtype=tf.int64)))\n        elements_finished = tf.logical_or(tf.equal(chosen_outputs, misc.BF_EOS_INT), loop_time >= global_config.timestep_limit)\n        output_lengths = tf.where(elements_finished, prev_output_lengths, tf.tile(tf.expand_dims(loop_time + 1, 0), [batch_size]))\n        next_input = tf.gather(obs_embeddings, chosen_outputs)\n        emit_output = scaled_logits\n        next_loop_state = (prev_chosen.write(loop_time - 1, chosen_outputs), output_lengths, tf.logical_or(prev_elements_finished, elements_finished))\n    return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)",
            "def loop_fn(loop_time, cell_output, cell_state, loop_state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Function called by tf.nn.raw_rnn to instantiate body of the while_loop.\\n\\n      See https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn for more\\n      information.\\n\\n      When time is 0, and cell_output, cell_state, loop_state are all None,\\n      `loop_fn` will create the initial input, internal cell state, and loop\\n      state. When time > 0, `loop_fn` will operate on previous cell output,\\n      state, and loop state.\\n\\n      Args:\\n        loop_time: A scalar tensor holding the current timestep (zero based\\n            counting).\\n        cell_output: Output of the raw_rnn cell at the current timestep.\\n        cell_state: Cell internal state at the current timestep.\\n        loop_state: Additional loop state. These tensors were returned by the\\n            previous call to `loop_fn`.\\n\\n      Returns:\\n        elements_finished: Bool tensor of shape [batch_size] which marks each\\n            sequence in the batch as being finished or not finished.\\n        next_input: A tensor containing input to be fed into the cell at the\\n            next timestep.\\n        next_cell_state: Cell internal state to be fed into the cell at the\\n            next timestep.\\n        emit_output: Tensor to be added to the TensorArray returned by raw_rnn\\n            as output from the while_loop.\\n        next_loop_state: Additional loop state. These tensors will be fed back\\n            into the next call to `loop_fn` as `loop_state`.\\n      '\n    if cell_output is None:\n        next_cell_state = self.policy_cell.zero_state(batch_size, dtype)\n        elements_finished = tf.zeros([batch_size], tf.bool)\n        output_lengths = tf.ones([batch_size], dtype=tf.int32)\n        next_input = tf.gather(obs_embeddings, initial_state)\n        emit_output = None\n        next_loop_state = (tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True), output_lengths, elements_finished)\n    else:\n        scaled_logits = cell_output * config.softmax_tr\n        (prev_chosen, prev_output_lengths, prev_elements_finished) = loop_state\n        next_cell_state = cell_state\n        chosen_outputs = tf.to_int32(tf.where(tf.logical_not(prev_elements_finished), tf.multinomial(logits=scaled_logits, num_samples=1)[:, 0], tf.zeros([batch_size], dtype=tf.int64)))\n        elements_finished = tf.logical_or(tf.equal(chosen_outputs, misc.BF_EOS_INT), loop_time >= global_config.timestep_limit)\n        output_lengths = tf.where(elements_finished, prev_output_lengths, tf.tile(tf.expand_dims(loop_time + 1, 0), [batch_size]))\n        next_input = tf.gather(obs_embeddings, chosen_outputs)\n        emit_output = scaled_logits\n        next_loop_state = (prev_chosen.write(loop_time - 1, chosen_outputs), output_lengths, tf.logical_or(prev_elements_finished, elements_finished))\n    return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)",
            "def loop_fn(loop_time, cell_output, cell_state, loop_state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Function called by tf.nn.raw_rnn to instantiate body of the while_loop.\\n\\n      See https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn for more\\n      information.\\n\\n      When time is 0, and cell_output, cell_state, loop_state are all None,\\n      `loop_fn` will create the initial input, internal cell state, and loop\\n      state. When time > 0, `loop_fn` will operate on previous cell output,\\n      state, and loop state.\\n\\n      Args:\\n        loop_time: A scalar tensor holding the current timestep (zero based\\n            counting).\\n        cell_output: Output of the raw_rnn cell at the current timestep.\\n        cell_state: Cell internal state at the current timestep.\\n        loop_state: Additional loop state. These tensors were returned by the\\n            previous call to `loop_fn`.\\n\\n      Returns:\\n        elements_finished: Bool tensor of shape [batch_size] which marks each\\n            sequence in the batch as being finished or not finished.\\n        next_input: A tensor containing input to be fed into the cell at the\\n            next timestep.\\n        next_cell_state: Cell internal state to be fed into the cell at the\\n            next timestep.\\n        emit_output: Tensor to be added to the TensorArray returned by raw_rnn\\n            as output from the while_loop.\\n        next_loop_state: Additional loop state. These tensors will be fed back\\n            into the next call to `loop_fn` as `loop_state`.\\n      '\n    if cell_output is None:\n        next_cell_state = self.policy_cell.zero_state(batch_size, dtype)\n        elements_finished = tf.zeros([batch_size], tf.bool)\n        output_lengths = tf.ones([batch_size], dtype=tf.int32)\n        next_input = tf.gather(obs_embeddings, initial_state)\n        emit_output = None\n        next_loop_state = (tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True), output_lengths, elements_finished)\n    else:\n        scaled_logits = cell_output * config.softmax_tr\n        (prev_chosen, prev_output_lengths, prev_elements_finished) = loop_state\n        next_cell_state = cell_state\n        chosen_outputs = tf.to_int32(tf.where(tf.logical_not(prev_elements_finished), tf.multinomial(logits=scaled_logits, num_samples=1)[:, 0], tf.zeros([batch_size], dtype=tf.int64)))\n        elements_finished = tf.logical_or(tf.equal(chosen_outputs, misc.BF_EOS_INT), loop_time >= global_config.timestep_limit)\n        output_lengths = tf.where(elements_finished, prev_output_lengths, tf.tile(tf.expand_dims(loop_time + 1, 0), [batch_size]))\n        next_input = tf.gather(obs_embeddings, chosen_outputs)\n        emit_output = scaled_logits\n        next_loop_state = (prev_chosen.write(loop_time - 1, chosen_outputs), output_lengths, tf.logical_or(prev_elements_finished, elements_finished))\n    return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)",
            "def loop_fn(loop_time, cell_output, cell_state, loop_state):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Function called by tf.nn.raw_rnn to instantiate body of the while_loop.\\n\\n      See https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn for more\\n      information.\\n\\n      When time is 0, and cell_output, cell_state, loop_state are all None,\\n      `loop_fn` will create the initial input, internal cell state, and loop\\n      state. When time > 0, `loop_fn` will operate on previous cell output,\\n      state, and loop state.\\n\\n      Args:\\n        loop_time: A scalar tensor holding the current timestep (zero based\\n            counting).\\n        cell_output: Output of the raw_rnn cell at the current timestep.\\n        cell_state: Cell internal state at the current timestep.\\n        loop_state: Additional loop state. These tensors were returned by the\\n            previous call to `loop_fn`.\\n\\n      Returns:\\n        elements_finished: Bool tensor of shape [batch_size] which marks each\\n            sequence in the batch as being finished or not finished.\\n        next_input: A tensor containing input to be fed into the cell at the\\n            next timestep.\\n        next_cell_state: Cell internal state to be fed into the cell at the\\n            next timestep.\\n        emit_output: Tensor to be added to the TensorArray returned by raw_rnn\\n            as output from the while_loop.\\n        next_loop_state: Additional loop state. These tensors will be fed back\\n            into the next call to `loop_fn` as `loop_state`.\\n      '\n    if cell_output is None:\n        next_cell_state = self.policy_cell.zero_state(batch_size, dtype)\n        elements_finished = tf.zeros([batch_size], tf.bool)\n        output_lengths = tf.ones([batch_size], dtype=tf.int32)\n        next_input = tf.gather(obs_embeddings, initial_state)\n        emit_output = None\n        next_loop_state = (tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True), output_lengths, elements_finished)\n    else:\n        scaled_logits = cell_output * config.softmax_tr\n        (prev_chosen, prev_output_lengths, prev_elements_finished) = loop_state\n        next_cell_state = cell_state\n        chosen_outputs = tf.to_int32(tf.where(tf.logical_not(prev_elements_finished), tf.multinomial(logits=scaled_logits, num_samples=1)[:, 0], tf.zeros([batch_size], dtype=tf.int64)))\n        elements_finished = tf.logical_or(tf.equal(chosen_outputs, misc.BF_EOS_INT), loop_time >= global_config.timestep_limit)\n        output_lengths = tf.where(elements_finished, prev_output_lengths, tf.tile(tf.expand_dims(loop_time + 1, 0), [batch_size]))\n        next_input = tf.gather(obs_embeddings, chosen_outputs)\n        emit_output = scaled_logits\n        next_loop_state = (prev_chosen.write(loop_time - 1, chosen_outputs), output_lengths, tf.logical_or(prev_elements_finished, elements_finished))\n    return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, global_config, task_id=0, logging_file=None, experience_replay_file=None, global_best_reward_fn=None, found_solution_op=None, assign_code_solution_fn=None, program_count=None, do_iw_summaries=False, stop_on_success=True, dtype=tf.float32, verbose_level=0, is_local=True):\n    self.config = config = global_config.agent\n    self.logging_file = logging_file\n    self.experience_replay_file = experience_replay_file\n    self.task_id = task_id\n    self.verbose_level = verbose_level\n    self.global_best_reward_fn = global_best_reward_fn\n    self.found_solution_op = found_solution_op\n    self.assign_code_solution_fn = assign_code_solution_fn\n    self.parent_scope_name = tf.get_variable_scope().name\n    self.dtype = dtype\n    self.allow_eos_token = config.eos_token\n    self.stop_on_success = stop_on_success\n    self.pi_loss_hparam = config.pi_loss_hparam\n    self.vf_loss_hparam = config.vf_loss_hparam\n    self.is_local = is_local\n    self.top_reward = 0.0\n    self.embeddings_trainable = True\n    self.no_op = tf.no_op()\n    self.learning_rate = tf.constant(config.lr, dtype=dtype, name='learning_rate')\n    self.initializer = tf.contrib.layers.variance_scaling_initializer(factor=config.param_init_factor, mode='FAN_AVG', uniform=True, dtype=dtype)\n    tf.get_variable_scope().set_initializer(self.initializer)\n    self.a2c = config.ema_baseline_decay == 0\n    if not self.a2c:\n        logging.info('Using exponential moving average REINFORCE baselines.')\n        self.ema_baseline_decay = config.ema_baseline_decay\n        self.ema_by_len = [0.0] * global_config.timestep_limit\n    else:\n        logging.info('Using advantage (a2c) with learned value function.')\n        self.ema_baseline_decay = 0.0\n        self.ema_by_len = None\n    if config.topk and config.topk_loss_hparam:\n        self.topk_loss_hparam = config.topk_loss_hparam\n        self.topk_batch_size = config.topk_batch_size\n        if self.topk_batch_size <= 0:\n            raise ValueError('topk_batch_size must be a positive integer. Got %s', self.topk_batch_size)\n        self.top_episodes = utils.MaxUniquePriorityQueue(config.topk)\n        logging.info('Made max-priorty-queue with capacity %d', self.top_episodes.capacity)\n    else:\n        self.top_episodes = None\n        self.topk_loss_hparam = 0.0\n        logging.info('No max-priorty-queue')\n    self.replay_temperature = config.replay_temperature\n    self.num_replay_per_batch = int(global_config.batch_size * config.alpha)\n    self.num_on_policy_per_batch = global_config.batch_size - self.num_replay_per_batch\n    self.replay_alpha = self.num_replay_per_batch / float(global_config.batch_size)\n    logging.info('num_replay_per_batch: %d', self.num_replay_per_batch)\n    logging.info('num_on_policy_per_batch: %d', self.num_on_policy_per_batch)\n    logging.info('replay_alpha: %s', self.replay_alpha)\n    if self.num_replay_per_batch > 0:\n        start_time = time.time()\n        self.experience_replay = utils.RouletteWheel(unique_mode=True, save_file=experience_replay_file)\n        logging.info('Took %s sec to load replay buffer from disk.', int(time.time() - start_time))\n        logging.info('Replay buffer file location: \"%s\"', self.experience_replay.save_file)\n    else:\n        self.experience_replay = None\n    if program_count is not None:\n        self.program_count = program_count\n        self.program_count_add_ph = tf.placeholder(tf.int64, [], 'program_count_add_ph')\n        self.program_count_add_op = self.program_count.assign_add(self.program_count_add_ph)\n    batch_size = global_config.batch_size\n    logging.info('batch_size: %d', batch_size)\n    self.policy_cell = LinearWrapper(tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(cell_size) for cell_size in config.policy_lstm_sizes]), self.action_space, dtype=dtype, suppress_index=None if self.allow_eos_token else misc.BF_EOS_INT)\n    self.value_cell = LinearWrapper(tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(cell_size) for cell_size in config.value_lstm_sizes]), 1, dtype=dtype)\n    obs_embedding_scope = 'obs_embed'\n    with tf.variable_scope(obs_embedding_scope, initializer=tf.random_uniform_initializer(minval=-1.0, maxval=1.0)):\n        obs_embeddings = tf.get_variable('embeddings', [self.observation_space, config.obs_embedding_size], dtype=dtype, trainable=self.embeddings_trainable)\n        self.obs_embeddings = obs_embeddings\n    initial_state = tf.fill([batch_size], misc.BF_EOS_INT)\n\n    def loop_fn(loop_time, cell_output, cell_state, loop_state):\n        \"\"\"Function called by tf.nn.raw_rnn to instantiate body of the while_loop.\n\n      See https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn for more\n      information.\n\n      When time is 0, and cell_output, cell_state, loop_state are all None,\n      `loop_fn` will create the initial input, internal cell state, and loop\n      state. When time > 0, `loop_fn` will operate on previous cell output,\n      state, and loop state.\n\n      Args:\n        loop_time: A scalar tensor holding the current timestep (zero based\n            counting).\n        cell_output: Output of the raw_rnn cell at the current timestep.\n        cell_state: Cell internal state at the current timestep.\n        loop_state: Additional loop state. These tensors were returned by the\n            previous call to `loop_fn`.\n\n      Returns:\n        elements_finished: Bool tensor of shape [batch_size] which marks each\n            sequence in the batch as being finished or not finished.\n        next_input: A tensor containing input to be fed into the cell at the\n            next timestep.\n        next_cell_state: Cell internal state to be fed into the cell at the\n            next timestep.\n        emit_output: Tensor to be added to the TensorArray returned by raw_rnn\n            as output from the while_loop.\n        next_loop_state: Additional loop state. These tensors will be fed back\n            into the next call to `loop_fn` as `loop_state`.\n      \"\"\"\n        if cell_output is None:\n            next_cell_state = self.policy_cell.zero_state(batch_size, dtype)\n            elements_finished = tf.zeros([batch_size], tf.bool)\n            output_lengths = tf.ones([batch_size], dtype=tf.int32)\n            next_input = tf.gather(obs_embeddings, initial_state)\n            emit_output = None\n            next_loop_state = (tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True), output_lengths, elements_finished)\n        else:\n            scaled_logits = cell_output * config.softmax_tr\n            (prev_chosen, prev_output_lengths, prev_elements_finished) = loop_state\n            next_cell_state = cell_state\n            chosen_outputs = tf.to_int32(tf.where(tf.logical_not(prev_elements_finished), tf.multinomial(logits=scaled_logits, num_samples=1)[:, 0], tf.zeros([batch_size], dtype=tf.int64)))\n            elements_finished = tf.logical_or(tf.equal(chosen_outputs, misc.BF_EOS_INT), loop_time >= global_config.timestep_limit)\n            output_lengths = tf.where(elements_finished, prev_output_lengths, tf.tile(tf.expand_dims(loop_time + 1, 0), [batch_size]))\n            next_input = tf.gather(obs_embeddings, chosen_outputs)\n            emit_output = scaled_logits\n            next_loop_state = (prev_chosen.write(loop_time - 1, chosen_outputs), output_lengths, tf.logical_or(prev_elements_finished, elements_finished))\n        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n    with tf.variable_scope('policy'):\n        (decoder_outputs_ta, _, (sampled_output_ta, output_lengths, _)) = tf.nn.raw_rnn(cell=self.policy_cell, loop_fn=loop_fn)\n    policy_logits = tf.transpose(decoder_outputs_ta.stack(), (1, 0, 2), name='policy_logits')\n    sampled_tokens = tf.transpose(sampled_output_ta.stack(), (1, 0), name='sampled_tokens')\n    rshift_sampled_tokens = rshift_time(sampled_tokens, fill=misc.BF_EOS_INT)\n    if self.a2c:\n        with tf.variable_scope('value'):\n            (value_output, _) = tf.nn.dynamic_rnn(self.value_cell, tf.gather(obs_embeddings, rshift_sampled_tokens), sequence_length=output_lengths, dtype=dtype)\n        value = tf.squeeze(value_output, axis=[2])\n    else:\n        value = tf.zeros([], dtype=dtype)\n    self.sampled_batch = AttrDict(logits=policy_logits, value=value, tokens=sampled_tokens, episode_lengths=output_lengths, probs=tf.nn.softmax(policy_logits), log_probs=tf.nn.log_softmax(policy_logits))\n    self.adjusted_lengths = tf.placeholder(tf.int32, [None], name='adjusted_lengths')\n    self.policy_multipliers = tf.placeholder(dtype, [None, None], name='policy_multipliers')\n    self.empirical_values = tf.placeholder(dtype, [None, None], name='empirical_values')\n    self.off_policy_targets = tf.placeholder(tf.int32, [None, None], name='off_policy_targets')\n    self.off_policy_target_lengths = tf.placeholder(tf.int32, [None], name='off_policy_target_lengths')\n    self.actions = tf.placeholder(tf.int32, [None, None], name='actions')\n    inputs = rshift_time(self.actions, fill=misc.BF_EOS_INT)\n    with tf.variable_scope('policy', reuse=True):\n        (logits, _) = tf.nn.dynamic_rnn(self.policy_cell, tf.gather(obs_embeddings, inputs), sequence_length=self.adjusted_lengths, dtype=dtype)\n    if self.a2c:\n        with tf.variable_scope('value', reuse=True):\n            (value_output, _) = tf.nn.dynamic_rnn(self.value_cell, tf.gather(obs_embeddings, inputs), sequence_length=self.adjusted_lengths, dtype=dtype)\n        value2 = tf.squeeze(value_output, axis=[2])\n    else:\n        value2 = tf.zeros([], dtype=dtype)\n    self.given_batch = AttrDict(logits=logits, value=value2, tokens=sampled_tokens, episode_lengths=self.adjusted_lengths, probs=tf.nn.softmax(logits), log_probs=tf.nn.log_softmax(logits))\n    max_episode_length = tf.shape(self.actions)[1]\n    range_row = tf.expand_dims(tf.range(max_episode_length), 0)\n    episode_masks = tf.cast(tf.less(range_row, tf.expand_dims(self.given_batch.episode_lengths, 1)), dtype=dtype)\n    episode_masks_3d = tf.expand_dims(episode_masks, 2)\n    self.a_probs = a_probs = self.given_batch.probs * episode_masks_3d\n    self.a_log_probs = a_log_probs = self.given_batch.log_probs * episode_masks_3d\n    self.a_value = a_value = self.given_batch.value * episode_masks\n    self.a_policy_multipliers = a_policy_multipliers = self.policy_multipliers * episode_masks\n    if self.a2c:\n        self.a_empirical_values = a_empirical_values = self.empirical_values * episode_masks\n    acs_onehot = tf.one_hot(self.actions, self.action_space, dtype=dtype)\n    self.acs_onehot = acs_onehot\n    chosen_masked_log_probs = acs_onehot * a_log_probs\n    pi_target = tf.expand_dims(a_policy_multipliers, -1)\n    pi_loss_per_step = chosen_masked_log_probs * pi_target\n    self.pi_loss = pi_loss = -tf.reduce_mean(tf.reduce_sum(pi_loss_per_step, axis=[1, 2]), axis=0) * MAGIC_LOSS_MULTIPLIER\n    assert len(self.pi_loss.shape) == 0\n    self.chosen_log_probs = tf.reduce_sum(chosen_masked_log_probs, axis=2)\n    self.chosen_probs = tf.reduce_sum(acs_onehot * a_probs, axis=2)\n    if self.a2c:\n        vf_loss_per_step = tf.square(a_value - a_empirical_values)\n        self.vf_loss = vf_loss = tf.reduce_mean(tf.reduce_sum(vf_loss_per_step, axis=1), axis=0) * MAGIC_LOSS_MULTIPLIER\n        assert len(self.vf_loss.shape) == 0\n    else:\n        self.vf_loss = vf_loss = 0.0\n    self.entropy = entropy = -tf.reduce_mean(tf.reduce_sum(a_probs * a_log_probs, axis=[1, 2]), axis=0) * MAGIC_LOSS_MULTIPLIER\n    self.negentropy = -entropy\n    assert len(self.negentropy.shape) == 0\n    self.offp_switch = tf.placeholder(dtype, [], name='offp_switch')\n    if self.top_episodes is not None:\n        offp_inputs = tf.gather(obs_embeddings, rshift_time(self.off_policy_targets, fill=misc.BF_EOS_INT))\n        with tf.variable_scope('policy', reuse=True):\n            (offp_logits, _) = tf.nn.dynamic_rnn(self.policy_cell, offp_inputs, self.off_policy_target_lengths, dtype=dtype)\n        topk_loss_per_step = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.off_policy_targets, logits=offp_logits, name='topk_loss_per_logit')\n        topk_loss = tf.reduce_mean(tf.reduce_sum(topk_loss_per_step, axis=1), axis=0)\n        assert len(topk_loss.shape) == 0\n        self.topk_loss = topk_loss * self.offp_switch\n        logging.info('Including off policy loss.')\n    else:\n        self.topk_loss = topk_loss = 0.0\n    self.entropy_hparam = tf.constant(config.entropy_beta, dtype=dtype, name='entropy_beta')\n    self.pi_loss_term = pi_loss * self.pi_loss_hparam\n    self.vf_loss_term = vf_loss * self.vf_loss_hparam\n    self.entropy_loss_term = self.negentropy * self.entropy_hparam\n    self.topk_loss_term = self.topk_loss_hparam * topk_loss\n    self.loss = self.pi_loss_term + self.vf_loss_term + self.entropy_loss_term + self.topk_loss_term\n    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n    self.trainable_variables = params\n    self.sync_variables = self.trainable_variables\n    non_embedding_params = [p for p in params if obs_embedding_scope not in p.name]\n    self.non_embedding_params = non_embedding_params\n    self.params = params\n    if config.regularizer:\n        logging.info('Adding L2 regularizer with scale %.2f.', config.regularizer)\n        self.regularizer = config.regularizer * sum((tf.nn.l2_loss(w) for w in non_embedding_params))\n        self.loss += self.regularizer\n    else:\n        logging.info('Skipping regularizer.')\n        self.regularizer = 0.0\n    if self.is_local:\n        unclipped_grads = tf.gradients(self.loss, params)\n        self.dense_unclipped_grads = [tf.convert_to_tensor(g) for g in unclipped_grads]\n        (self.grads, self.global_grad_norm) = tf.clip_by_global_norm(unclipped_grads, config.grad_clip_threshold)\n        self.gradients_dict = dict(zip(params, self.grads))\n        self.optimizer = make_optimizer(config.optimizer, self.learning_rate)\n        self.all_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, tf.get_variable_scope().name)\n    self.do_iw_summaries = do_iw_summaries\n    if self.do_iw_summaries:\n        b = None\n        self.log_iw_replay_ph = tf.placeholder(tf.float32, [b], 'log_iw_replay_ph')\n        self.log_iw_policy_ph = tf.placeholder(tf.float32, [b], 'log_iw_policy_ph')\n        self.log_prob_replay_ph = tf.placeholder(tf.float32, [b], 'log_prob_replay_ph')\n        self.log_prob_policy_ph = tf.placeholder(tf.float32, [b], 'log_prob_policy_ph')\n        self.log_norm_replay_weights_ph = tf.placeholder(tf.float32, [b], 'log_norm_replay_weights_ph')\n        self.iw_summary_op = tf.summary.merge([tf.summary.histogram('is/log_iw_replay', self.log_iw_replay_ph), tf.summary.histogram('is/log_iw_policy', self.log_iw_policy_ph), tf.summary.histogram('is/log_prob_replay', self.log_prob_replay_ph), tf.summary.histogram('is/log_prob_policy', self.log_prob_policy_ph), tf.summary.histogram('is/log_norm_replay_weights', self.log_norm_replay_weights_ph)])",
        "mutated": [
            "def __init__(self, global_config, task_id=0, logging_file=None, experience_replay_file=None, global_best_reward_fn=None, found_solution_op=None, assign_code_solution_fn=None, program_count=None, do_iw_summaries=False, stop_on_success=True, dtype=tf.float32, verbose_level=0, is_local=True):\n    if False:\n        i = 10\n    self.config = config = global_config.agent\n    self.logging_file = logging_file\n    self.experience_replay_file = experience_replay_file\n    self.task_id = task_id\n    self.verbose_level = verbose_level\n    self.global_best_reward_fn = global_best_reward_fn\n    self.found_solution_op = found_solution_op\n    self.assign_code_solution_fn = assign_code_solution_fn\n    self.parent_scope_name = tf.get_variable_scope().name\n    self.dtype = dtype\n    self.allow_eos_token = config.eos_token\n    self.stop_on_success = stop_on_success\n    self.pi_loss_hparam = config.pi_loss_hparam\n    self.vf_loss_hparam = config.vf_loss_hparam\n    self.is_local = is_local\n    self.top_reward = 0.0\n    self.embeddings_trainable = True\n    self.no_op = tf.no_op()\n    self.learning_rate = tf.constant(config.lr, dtype=dtype, name='learning_rate')\n    self.initializer = tf.contrib.layers.variance_scaling_initializer(factor=config.param_init_factor, mode='FAN_AVG', uniform=True, dtype=dtype)\n    tf.get_variable_scope().set_initializer(self.initializer)\n    self.a2c = config.ema_baseline_decay == 0\n    if not self.a2c:\n        logging.info('Using exponential moving average REINFORCE baselines.')\n        self.ema_baseline_decay = config.ema_baseline_decay\n        self.ema_by_len = [0.0] * global_config.timestep_limit\n    else:\n        logging.info('Using advantage (a2c) with learned value function.')\n        self.ema_baseline_decay = 0.0\n        self.ema_by_len = None\n    if config.topk and config.topk_loss_hparam:\n        self.topk_loss_hparam = config.topk_loss_hparam\n        self.topk_batch_size = config.topk_batch_size\n        if self.topk_batch_size <= 0:\n            raise ValueError('topk_batch_size must be a positive integer. Got %s', self.topk_batch_size)\n        self.top_episodes = utils.MaxUniquePriorityQueue(config.topk)\n        logging.info('Made max-priorty-queue with capacity %d', self.top_episodes.capacity)\n    else:\n        self.top_episodes = None\n        self.topk_loss_hparam = 0.0\n        logging.info('No max-priorty-queue')\n    self.replay_temperature = config.replay_temperature\n    self.num_replay_per_batch = int(global_config.batch_size * config.alpha)\n    self.num_on_policy_per_batch = global_config.batch_size - self.num_replay_per_batch\n    self.replay_alpha = self.num_replay_per_batch / float(global_config.batch_size)\n    logging.info('num_replay_per_batch: %d', self.num_replay_per_batch)\n    logging.info('num_on_policy_per_batch: %d', self.num_on_policy_per_batch)\n    logging.info('replay_alpha: %s', self.replay_alpha)\n    if self.num_replay_per_batch > 0:\n        start_time = time.time()\n        self.experience_replay = utils.RouletteWheel(unique_mode=True, save_file=experience_replay_file)\n        logging.info('Took %s sec to load replay buffer from disk.', int(time.time() - start_time))\n        logging.info('Replay buffer file location: \"%s\"', self.experience_replay.save_file)\n    else:\n        self.experience_replay = None\n    if program_count is not None:\n        self.program_count = program_count\n        self.program_count_add_ph = tf.placeholder(tf.int64, [], 'program_count_add_ph')\n        self.program_count_add_op = self.program_count.assign_add(self.program_count_add_ph)\n    batch_size = global_config.batch_size\n    logging.info('batch_size: %d', batch_size)\n    self.policy_cell = LinearWrapper(tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(cell_size) for cell_size in config.policy_lstm_sizes]), self.action_space, dtype=dtype, suppress_index=None if self.allow_eos_token else misc.BF_EOS_INT)\n    self.value_cell = LinearWrapper(tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(cell_size) for cell_size in config.value_lstm_sizes]), 1, dtype=dtype)\n    obs_embedding_scope = 'obs_embed'\n    with tf.variable_scope(obs_embedding_scope, initializer=tf.random_uniform_initializer(minval=-1.0, maxval=1.0)):\n        obs_embeddings = tf.get_variable('embeddings', [self.observation_space, config.obs_embedding_size], dtype=dtype, trainable=self.embeddings_trainable)\n        self.obs_embeddings = obs_embeddings\n    initial_state = tf.fill([batch_size], misc.BF_EOS_INT)\n\n    def loop_fn(loop_time, cell_output, cell_state, loop_state):\n        \"\"\"Function called by tf.nn.raw_rnn to instantiate body of the while_loop.\n\n      See https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn for more\n      information.\n\n      When time is 0, and cell_output, cell_state, loop_state are all None,\n      `loop_fn` will create the initial input, internal cell state, and loop\n      state. When time > 0, `loop_fn` will operate on previous cell output,\n      state, and loop state.\n\n      Args:\n        loop_time: A scalar tensor holding the current timestep (zero based\n            counting).\n        cell_output: Output of the raw_rnn cell at the current timestep.\n        cell_state: Cell internal state at the current timestep.\n        loop_state: Additional loop state. These tensors were returned by the\n            previous call to `loop_fn`.\n\n      Returns:\n        elements_finished: Bool tensor of shape [batch_size] which marks each\n            sequence in the batch as being finished or not finished.\n        next_input: A tensor containing input to be fed into the cell at the\n            next timestep.\n        next_cell_state: Cell internal state to be fed into the cell at the\n            next timestep.\n        emit_output: Tensor to be added to the TensorArray returned by raw_rnn\n            as output from the while_loop.\n        next_loop_state: Additional loop state. These tensors will be fed back\n            into the next call to `loop_fn` as `loop_state`.\n      \"\"\"\n        if cell_output is None:\n            next_cell_state = self.policy_cell.zero_state(batch_size, dtype)\n            elements_finished = tf.zeros([batch_size], tf.bool)\n            output_lengths = tf.ones([batch_size], dtype=tf.int32)\n            next_input = tf.gather(obs_embeddings, initial_state)\n            emit_output = None\n            next_loop_state = (tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True), output_lengths, elements_finished)\n        else:\n            scaled_logits = cell_output * config.softmax_tr\n            (prev_chosen, prev_output_lengths, prev_elements_finished) = loop_state\n            next_cell_state = cell_state\n            chosen_outputs = tf.to_int32(tf.where(tf.logical_not(prev_elements_finished), tf.multinomial(logits=scaled_logits, num_samples=1)[:, 0], tf.zeros([batch_size], dtype=tf.int64)))\n            elements_finished = tf.logical_or(tf.equal(chosen_outputs, misc.BF_EOS_INT), loop_time >= global_config.timestep_limit)\n            output_lengths = tf.where(elements_finished, prev_output_lengths, tf.tile(tf.expand_dims(loop_time + 1, 0), [batch_size]))\n            next_input = tf.gather(obs_embeddings, chosen_outputs)\n            emit_output = scaled_logits\n            next_loop_state = (prev_chosen.write(loop_time - 1, chosen_outputs), output_lengths, tf.logical_or(prev_elements_finished, elements_finished))\n        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n    with tf.variable_scope('policy'):\n        (decoder_outputs_ta, _, (sampled_output_ta, output_lengths, _)) = tf.nn.raw_rnn(cell=self.policy_cell, loop_fn=loop_fn)\n    policy_logits = tf.transpose(decoder_outputs_ta.stack(), (1, 0, 2), name='policy_logits')\n    sampled_tokens = tf.transpose(sampled_output_ta.stack(), (1, 0), name='sampled_tokens')\n    rshift_sampled_tokens = rshift_time(sampled_tokens, fill=misc.BF_EOS_INT)\n    if self.a2c:\n        with tf.variable_scope('value'):\n            (value_output, _) = tf.nn.dynamic_rnn(self.value_cell, tf.gather(obs_embeddings, rshift_sampled_tokens), sequence_length=output_lengths, dtype=dtype)\n        value = tf.squeeze(value_output, axis=[2])\n    else:\n        value = tf.zeros([], dtype=dtype)\n    self.sampled_batch = AttrDict(logits=policy_logits, value=value, tokens=sampled_tokens, episode_lengths=output_lengths, probs=tf.nn.softmax(policy_logits), log_probs=tf.nn.log_softmax(policy_logits))\n    self.adjusted_lengths = tf.placeholder(tf.int32, [None], name='adjusted_lengths')\n    self.policy_multipliers = tf.placeholder(dtype, [None, None], name='policy_multipliers')\n    self.empirical_values = tf.placeholder(dtype, [None, None], name='empirical_values')\n    self.off_policy_targets = tf.placeholder(tf.int32, [None, None], name='off_policy_targets')\n    self.off_policy_target_lengths = tf.placeholder(tf.int32, [None], name='off_policy_target_lengths')\n    self.actions = tf.placeholder(tf.int32, [None, None], name='actions')\n    inputs = rshift_time(self.actions, fill=misc.BF_EOS_INT)\n    with tf.variable_scope('policy', reuse=True):\n        (logits, _) = tf.nn.dynamic_rnn(self.policy_cell, tf.gather(obs_embeddings, inputs), sequence_length=self.adjusted_lengths, dtype=dtype)\n    if self.a2c:\n        with tf.variable_scope('value', reuse=True):\n            (value_output, _) = tf.nn.dynamic_rnn(self.value_cell, tf.gather(obs_embeddings, inputs), sequence_length=self.adjusted_lengths, dtype=dtype)\n        value2 = tf.squeeze(value_output, axis=[2])\n    else:\n        value2 = tf.zeros([], dtype=dtype)\n    self.given_batch = AttrDict(logits=logits, value=value2, tokens=sampled_tokens, episode_lengths=self.adjusted_lengths, probs=tf.nn.softmax(logits), log_probs=tf.nn.log_softmax(logits))\n    max_episode_length = tf.shape(self.actions)[1]\n    range_row = tf.expand_dims(tf.range(max_episode_length), 0)\n    episode_masks = tf.cast(tf.less(range_row, tf.expand_dims(self.given_batch.episode_lengths, 1)), dtype=dtype)\n    episode_masks_3d = tf.expand_dims(episode_masks, 2)\n    self.a_probs = a_probs = self.given_batch.probs * episode_masks_3d\n    self.a_log_probs = a_log_probs = self.given_batch.log_probs * episode_masks_3d\n    self.a_value = a_value = self.given_batch.value * episode_masks\n    self.a_policy_multipliers = a_policy_multipliers = self.policy_multipliers * episode_masks\n    if self.a2c:\n        self.a_empirical_values = a_empirical_values = self.empirical_values * episode_masks\n    acs_onehot = tf.one_hot(self.actions, self.action_space, dtype=dtype)\n    self.acs_onehot = acs_onehot\n    chosen_masked_log_probs = acs_onehot * a_log_probs\n    pi_target = tf.expand_dims(a_policy_multipliers, -1)\n    pi_loss_per_step = chosen_masked_log_probs * pi_target\n    self.pi_loss = pi_loss = -tf.reduce_mean(tf.reduce_sum(pi_loss_per_step, axis=[1, 2]), axis=0) * MAGIC_LOSS_MULTIPLIER\n    assert len(self.pi_loss.shape) == 0\n    self.chosen_log_probs = tf.reduce_sum(chosen_masked_log_probs, axis=2)\n    self.chosen_probs = tf.reduce_sum(acs_onehot * a_probs, axis=2)\n    if self.a2c:\n        vf_loss_per_step = tf.square(a_value - a_empirical_values)\n        self.vf_loss = vf_loss = tf.reduce_mean(tf.reduce_sum(vf_loss_per_step, axis=1), axis=0) * MAGIC_LOSS_MULTIPLIER\n        assert len(self.vf_loss.shape) == 0\n    else:\n        self.vf_loss = vf_loss = 0.0\n    self.entropy = entropy = -tf.reduce_mean(tf.reduce_sum(a_probs * a_log_probs, axis=[1, 2]), axis=0) * MAGIC_LOSS_MULTIPLIER\n    self.negentropy = -entropy\n    assert len(self.negentropy.shape) == 0\n    self.offp_switch = tf.placeholder(dtype, [], name='offp_switch')\n    if self.top_episodes is not None:\n        offp_inputs = tf.gather(obs_embeddings, rshift_time(self.off_policy_targets, fill=misc.BF_EOS_INT))\n        with tf.variable_scope('policy', reuse=True):\n            (offp_logits, _) = tf.nn.dynamic_rnn(self.policy_cell, offp_inputs, self.off_policy_target_lengths, dtype=dtype)\n        topk_loss_per_step = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.off_policy_targets, logits=offp_logits, name='topk_loss_per_logit')\n        topk_loss = tf.reduce_mean(tf.reduce_sum(topk_loss_per_step, axis=1), axis=0)\n        assert len(topk_loss.shape) == 0\n        self.topk_loss = topk_loss * self.offp_switch\n        logging.info('Including off policy loss.')\n    else:\n        self.topk_loss = topk_loss = 0.0\n    self.entropy_hparam = tf.constant(config.entropy_beta, dtype=dtype, name='entropy_beta')\n    self.pi_loss_term = pi_loss * self.pi_loss_hparam\n    self.vf_loss_term = vf_loss * self.vf_loss_hparam\n    self.entropy_loss_term = self.negentropy * self.entropy_hparam\n    self.topk_loss_term = self.topk_loss_hparam * topk_loss\n    self.loss = self.pi_loss_term + self.vf_loss_term + self.entropy_loss_term + self.topk_loss_term\n    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n    self.trainable_variables = params\n    self.sync_variables = self.trainable_variables\n    non_embedding_params = [p for p in params if obs_embedding_scope not in p.name]\n    self.non_embedding_params = non_embedding_params\n    self.params = params\n    if config.regularizer:\n        logging.info('Adding L2 regularizer with scale %.2f.', config.regularizer)\n        self.regularizer = config.regularizer * sum((tf.nn.l2_loss(w) for w in non_embedding_params))\n        self.loss += self.regularizer\n    else:\n        logging.info('Skipping regularizer.')\n        self.regularizer = 0.0\n    if self.is_local:\n        unclipped_grads = tf.gradients(self.loss, params)\n        self.dense_unclipped_grads = [tf.convert_to_tensor(g) for g in unclipped_grads]\n        (self.grads, self.global_grad_norm) = tf.clip_by_global_norm(unclipped_grads, config.grad_clip_threshold)\n        self.gradients_dict = dict(zip(params, self.grads))\n        self.optimizer = make_optimizer(config.optimizer, self.learning_rate)\n        self.all_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, tf.get_variable_scope().name)\n    self.do_iw_summaries = do_iw_summaries\n    if self.do_iw_summaries:\n        b = None\n        self.log_iw_replay_ph = tf.placeholder(tf.float32, [b], 'log_iw_replay_ph')\n        self.log_iw_policy_ph = tf.placeholder(tf.float32, [b], 'log_iw_policy_ph')\n        self.log_prob_replay_ph = tf.placeholder(tf.float32, [b], 'log_prob_replay_ph')\n        self.log_prob_policy_ph = tf.placeholder(tf.float32, [b], 'log_prob_policy_ph')\n        self.log_norm_replay_weights_ph = tf.placeholder(tf.float32, [b], 'log_norm_replay_weights_ph')\n        self.iw_summary_op = tf.summary.merge([tf.summary.histogram('is/log_iw_replay', self.log_iw_replay_ph), tf.summary.histogram('is/log_iw_policy', self.log_iw_policy_ph), tf.summary.histogram('is/log_prob_replay', self.log_prob_replay_ph), tf.summary.histogram('is/log_prob_policy', self.log_prob_policy_ph), tf.summary.histogram('is/log_norm_replay_weights', self.log_norm_replay_weights_ph)])",
            "def __init__(self, global_config, task_id=0, logging_file=None, experience_replay_file=None, global_best_reward_fn=None, found_solution_op=None, assign_code_solution_fn=None, program_count=None, do_iw_summaries=False, stop_on_success=True, dtype=tf.float32, verbose_level=0, is_local=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.config = config = global_config.agent\n    self.logging_file = logging_file\n    self.experience_replay_file = experience_replay_file\n    self.task_id = task_id\n    self.verbose_level = verbose_level\n    self.global_best_reward_fn = global_best_reward_fn\n    self.found_solution_op = found_solution_op\n    self.assign_code_solution_fn = assign_code_solution_fn\n    self.parent_scope_name = tf.get_variable_scope().name\n    self.dtype = dtype\n    self.allow_eos_token = config.eos_token\n    self.stop_on_success = stop_on_success\n    self.pi_loss_hparam = config.pi_loss_hparam\n    self.vf_loss_hparam = config.vf_loss_hparam\n    self.is_local = is_local\n    self.top_reward = 0.0\n    self.embeddings_trainable = True\n    self.no_op = tf.no_op()\n    self.learning_rate = tf.constant(config.lr, dtype=dtype, name='learning_rate')\n    self.initializer = tf.contrib.layers.variance_scaling_initializer(factor=config.param_init_factor, mode='FAN_AVG', uniform=True, dtype=dtype)\n    tf.get_variable_scope().set_initializer(self.initializer)\n    self.a2c = config.ema_baseline_decay == 0\n    if not self.a2c:\n        logging.info('Using exponential moving average REINFORCE baselines.')\n        self.ema_baseline_decay = config.ema_baseline_decay\n        self.ema_by_len = [0.0] * global_config.timestep_limit\n    else:\n        logging.info('Using advantage (a2c) with learned value function.')\n        self.ema_baseline_decay = 0.0\n        self.ema_by_len = None\n    if config.topk and config.topk_loss_hparam:\n        self.topk_loss_hparam = config.topk_loss_hparam\n        self.topk_batch_size = config.topk_batch_size\n        if self.topk_batch_size <= 0:\n            raise ValueError('topk_batch_size must be a positive integer. Got %s', self.topk_batch_size)\n        self.top_episodes = utils.MaxUniquePriorityQueue(config.topk)\n        logging.info('Made max-priorty-queue with capacity %d', self.top_episodes.capacity)\n    else:\n        self.top_episodes = None\n        self.topk_loss_hparam = 0.0\n        logging.info('No max-priorty-queue')\n    self.replay_temperature = config.replay_temperature\n    self.num_replay_per_batch = int(global_config.batch_size * config.alpha)\n    self.num_on_policy_per_batch = global_config.batch_size - self.num_replay_per_batch\n    self.replay_alpha = self.num_replay_per_batch / float(global_config.batch_size)\n    logging.info('num_replay_per_batch: %d', self.num_replay_per_batch)\n    logging.info('num_on_policy_per_batch: %d', self.num_on_policy_per_batch)\n    logging.info('replay_alpha: %s', self.replay_alpha)\n    if self.num_replay_per_batch > 0:\n        start_time = time.time()\n        self.experience_replay = utils.RouletteWheel(unique_mode=True, save_file=experience_replay_file)\n        logging.info('Took %s sec to load replay buffer from disk.', int(time.time() - start_time))\n        logging.info('Replay buffer file location: \"%s\"', self.experience_replay.save_file)\n    else:\n        self.experience_replay = None\n    if program_count is not None:\n        self.program_count = program_count\n        self.program_count_add_ph = tf.placeholder(tf.int64, [], 'program_count_add_ph')\n        self.program_count_add_op = self.program_count.assign_add(self.program_count_add_ph)\n    batch_size = global_config.batch_size\n    logging.info('batch_size: %d', batch_size)\n    self.policy_cell = LinearWrapper(tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(cell_size) for cell_size in config.policy_lstm_sizes]), self.action_space, dtype=dtype, suppress_index=None if self.allow_eos_token else misc.BF_EOS_INT)\n    self.value_cell = LinearWrapper(tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(cell_size) for cell_size in config.value_lstm_sizes]), 1, dtype=dtype)\n    obs_embedding_scope = 'obs_embed'\n    with tf.variable_scope(obs_embedding_scope, initializer=tf.random_uniform_initializer(minval=-1.0, maxval=1.0)):\n        obs_embeddings = tf.get_variable('embeddings', [self.observation_space, config.obs_embedding_size], dtype=dtype, trainable=self.embeddings_trainable)\n        self.obs_embeddings = obs_embeddings\n    initial_state = tf.fill([batch_size], misc.BF_EOS_INT)\n\n    def loop_fn(loop_time, cell_output, cell_state, loop_state):\n        \"\"\"Function called by tf.nn.raw_rnn to instantiate body of the while_loop.\n\n      See https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn for more\n      information.\n\n      When time is 0, and cell_output, cell_state, loop_state are all None,\n      `loop_fn` will create the initial input, internal cell state, and loop\n      state. When time > 0, `loop_fn` will operate on previous cell output,\n      state, and loop state.\n\n      Args:\n        loop_time: A scalar tensor holding the current timestep (zero based\n            counting).\n        cell_output: Output of the raw_rnn cell at the current timestep.\n        cell_state: Cell internal state at the current timestep.\n        loop_state: Additional loop state. These tensors were returned by the\n            previous call to `loop_fn`.\n\n      Returns:\n        elements_finished: Bool tensor of shape [batch_size] which marks each\n            sequence in the batch as being finished or not finished.\n        next_input: A tensor containing input to be fed into the cell at the\n            next timestep.\n        next_cell_state: Cell internal state to be fed into the cell at the\n            next timestep.\n        emit_output: Tensor to be added to the TensorArray returned by raw_rnn\n            as output from the while_loop.\n        next_loop_state: Additional loop state. These tensors will be fed back\n            into the next call to `loop_fn` as `loop_state`.\n      \"\"\"\n        if cell_output is None:\n            next_cell_state = self.policy_cell.zero_state(batch_size, dtype)\n            elements_finished = tf.zeros([batch_size], tf.bool)\n            output_lengths = tf.ones([batch_size], dtype=tf.int32)\n            next_input = tf.gather(obs_embeddings, initial_state)\n            emit_output = None\n            next_loop_state = (tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True), output_lengths, elements_finished)\n        else:\n            scaled_logits = cell_output * config.softmax_tr\n            (prev_chosen, prev_output_lengths, prev_elements_finished) = loop_state\n            next_cell_state = cell_state\n            chosen_outputs = tf.to_int32(tf.where(tf.logical_not(prev_elements_finished), tf.multinomial(logits=scaled_logits, num_samples=1)[:, 0], tf.zeros([batch_size], dtype=tf.int64)))\n            elements_finished = tf.logical_or(tf.equal(chosen_outputs, misc.BF_EOS_INT), loop_time >= global_config.timestep_limit)\n            output_lengths = tf.where(elements_finished, prev_output_lengths, tf.tile(tf.expand_dims(loop_time + 1, 0), [batch_size]))\n            next_input = tf.gather(obs_embeddings, chosen_outputs)\n            emit_output = scaled_logits\n            next_loop_state = (prev_chosen.write(loop_time - 1, chosen_outputs), output_lengths, tf.logical_or(prev_elements_finished, elements_finished))\n        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n    with tf.variable_scope('policy'):\n        (decoder_outputs_ta, _, (sampled_output_ta, output_lengths, _)) = tf.nn.raw_rnn(cell=self.policy_cell, loop_fn=loop_fn)\n    policy_logits = tf.transpose(decoder_outputs_ta.stack(), (1, 0, 2), name='policy_logits')\n    sampled_tokens = tf.transpose(sampled_output_ta.stack(), (1, 0), name='sampled_tokens')\n    rshift_sampled_tokens = rshift_time(sampled_tokens, fill=misc.BF_EOS_INT)\n    if self.a2c:\n        with tf.variable_scope('value'):\n            (value_output, _) = tf.nn.dynamic_rnn(self.value_cell, tf.gather(obs_embeddings, rshift_sampled_tokens), sequence_length=output_lengths, dtype=dtype)\n        value = tf.squeeze(value_output, axis=[2])\n    else:\n        value = tf.zeros([], dtype=dtype)\n    self.sampled_batch = AttrDict(logits=policy_logits, value=value, tokens=sampled_tokens, episode_lengths=output_lengths, probs=tf.nn.softmax(policy_logits), log_probs=tf.nn.log_softmax(policy_logits))\n    self.adjusted_lengths = tf.placeholder(tf.int32, [None], name='adjusted_lengths')\n    self.policy_multipliers = tf.placeholder(dtype, [None, None], name='policy_multipliers')\n    self.empirical_values = tf.placeholder(dtype, [None, None], name='empirical_values')\n    self.off_policy_targets = tf.placeholder(tf.int32, [None, None], name='off_policy_targets')\n    self.off_policy_target_lengths = tf.placeholder(tf.int32, [None], name='off_policy_target_lengths')\n    self.actions = tf.placeholder(tf.int32, [None, None], name='actions')\n    inputs = rshift_time(self.actions, fill=misc.BF_EOS_INT)\n    with tf.variable_scope('policy', reuse=True):\n        (logits, _) = tf.nn.dynamic_rnn(self.policy_cell, tf.gather(obs_embeddings, inputs), sequence_length=self.adjusted_lengths, dtype=dtype)\n    if self.a2c:\n        with tf.variable_scope('value', reuse=True):\n            (value_output, _) = tf.nn.dynamic_rnn(self.value_cell, tf.gather(obs_embeddings, inputs), sequence_length=self.adjusted_lengths, dtype=dtype)\n        value2 = tf.squeeze(value_output, axis=[2])\n    else:\n        value2 = tf.zeros([], dtype=dtype)\n    self.given_batch = AttrDict(logits=logits, value=value2, tokens=sampled_tokens, episode_lengths=self.adjusted_lengths, probs=tf.nn.softmax(logits), log_probs=tf.nn.log_softmax(logits))\n    max_episode_length = tf.shape(self.actions)[1]\n    range_row = tf.expand_dims(tf.range(max_episode_length), 0)\n    episode_masks = tf.cast(tf.less(range_row, tf.expand_dims(self.given_batch.episode_lengths, 1)), dtype=dtype)\n    episode_masks_3d = tf.expand_dims(episode_masks, 2)\n    self.a_probs = a_probs = self.given_batch.probs * episode_masks_3d\n    self.a_log_probs = a_log_probs = self.given_batch.log_probs * episode_masks_3d\n    self.a_value = a_value = self.given_batch.value * episode_masks\n    self.a_policy_multipliers = a_policy_multipliers = self.policy_multipliers * episode_masks\n    if self.a2c:\n        self.a_empirical_values = a_empirical_values = self.empirical_values * episode_masks\n    acs_onehot = tf.one_hot(self.actions, self.action_space, dtype=dtype)\n    self.acs_onehot = acs_onehot\n    chosen_masked_log_probs = acs_onehot * a_log_probs\n    pi_target = tf.expand_dims(a_policy_multipliers, -1)\n    pi_loss_per_step = chosen_masked_log_probs * pi_target\n    self.pi_loss = pi_loss = -tf.reduce_mean(tf.reduce_sum(pi_loss_per_step, axis=[1, 2]), axis=0) * MAGIC_LOSS_MULTIPLIER\n    assert len(self.pi_loss.shape) == 0\n    self.chosen_log_probs = tf.reduce_sum(chosen_masked_log_probs, axis=2)\n    self.chosen_probs = tf.reduce_sum(acs_onehot * a_probs, axis=2)\n    if self.a2c:\n        vf_loss_per_step = tf.square(a_value - a_empirical_values)\n        self.vf_loss = vf_loss = tf.reduce_mean(tf.reduce_sum(vf_loss_per_step, axis=1), axis=0) * MAGIC_LOSS_MULTIPLIER\n        assert len(self.vf_loss.shape) == 0\n    else:\n        self.vf_loss = vf_loss = 0.0\n    self.entropy = entropy = -tf.reduce_mean(tf.reduce_sum(a_probs * a_log_probs, axis=[1, 2]), axis=0) * MAGIC_LOSS_MULTIPLIER\n    self.negentropy = -entropy\n    assert len(self.negentropy.shape) == 0\n    self.offp_switch = tf.placeholder(dtype, [], name='offp_switch')\n    if self.top_episodes is not None:\n        offp_inputs = tf.gather(obs_embeddings, rshift_time(self.off_policy_targets, fill=misc.BF_EOS_INT))\n        with tf.variable_scope('policy', reuse=True):\n            (offp_logits, _) = tf.nn.dynamic_rnn(self.policy_cell, offp_inputs, self.off_policy_target_lengths, dtype=dtype)\n        topk_loss_per_step = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.off_policy_targets, logits=offp_logits, name='topk_loss_per_logit')\n        topk_loss = tf.reduce_mean(tf.reduce_sum(topk_loss_per_step, axis=1), axis=0)\n        assert len(topk_loss.shape) == 0\n        self.topk_loss = topk_loss * self.offp_switch\n        logging.info('Including off policy loss.')\n    else:\n        self.topk_loss = topk_loss = 0.0\n    self.entropy_hparam = tf.constant(config.entropy_beta, dtype=dtype, name='entropy_beta')\n    self.pi_loss_term = pi_loss * self.pi_loss_hparam\n    self.vf_loss_term = vf_loss * self.vf_loss_hparam\n    self.entropy_loss_term = self.negentropy * self.entropy_hparam\n    self.topk_loss_term = self.topk_loss_hparam * topk_loss\n    self.loss = self.pi_loss_term + self.vf_loss_term + self.entropy_loss_term + self.topk_loss_term\n    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n    self.trainable_variables = params\n    self.sync_variables = self.trainable_variables\n    non_embedding_params = [p for p in params if obs_embedding_scope not in p.name]\n    self.non_embedding_params = non_embedding_params\n    self.params = params\n    if config.regularizer:\n        logging.info('Adding L2 regularizer with scale %.2f.', config.regularizer)\n        self.regularizer = config.regularizer * sum((tf.nn.l2_loss(w) for w in non_embedding_params))\n        self.loss += self.regularizer\n    else:\n        logging.info('Skipping regularizer.')\n        self.regularizer = 0.0\n    if self.is_local:\n        unclipped_grads = tf.gradients(self.loss, params)\n        self.dense_unclipped_grads = [tf.convert_to_tensor(g) for g in unclipped_grads]\n        (self.grads, self.global_grad_norm) = tf.clip_by_global_norm(unclipped_grads, config.grad_clip_threshold)\n        self.gradients_dict = dict(zip(params, self.grads))\n        self.optimizer = make_optimizer(config.optimizer, self.learning_rate)\n        self.all_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, tf.get_variable_scope().name)\n    self.do_iw_summaries = do_iw_summaries\n    if self.do_iw_summaries:\n        b = None\n        self.log_iw_replay_ph = tf.placeholder(tf.float32, [b], 'log_iw_replay_ph')\n        self.log_iw_policy_ph = tf.placeholder(tf.float32, [b], 'log_iw_policy_ph')\n        self.log_prob_replay_ph = tf.placeholder(tf.float32, [b], 'log_prob_replay_ph')\n        self.log_prob_policy_ph = tf.placeholder(tf.float32, [b], 'log_prob_policy_ph')\n        self.log_norm_replay_weights_ph = tf.placeholder(tf.float32, [b], 'log_norm_replay_weights_ph')\n        self.iw_summary_op = tf.summary.merge([tf.summary.histogram('is/log_iw_replay', self.log_iw_replay_ph), tf.summary.histogram('is/log_iw_policy', self.log_iw_policy_ph), tf.summary.histogram('is/log_prob_replay', self.log_prob_replay_ph), tf.summary.histogram('is/log_prob_policy', self.log_prob_policy_ph), tf.summary.histogram('is/log_norm_replay_weights', self.log_norm_replay_weights_ph)])",
            "def __init__(self, global_config, task_id=0, logging_file=None, experience_replay_file=None, global_best_reward_fn=None, found_solution_op=None, assign_code_solution_fn=None, program_count=None, do_iw_summaries=False, stop_on_success=True, dtype=tf.float32, verbose_level=0, is_local=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.config = config = global_config.agent\n    self.logging_file = logging_file\n    self.experience_replay_file = experience_replay_file\n    self.task_id = task_id\n    self.verbose_level = verbose_level\n    self.global_best_reward_fn = global_best_reward_fn\n    self.found_solution_op = found_solution_op\n    self.assign_code_solution_fn = assign_code_solution_fn\n    self.parent_scope_name = tf.get_variable_scope().name\n    self.dtype = dtype\n    self.allow_eos_token = config.eos_token\n    self.stop_on_success = stop_on_success\n    self.pi_loss_hparam = config.pi_loss_hparam\n    self.vf_loss_hparam = config.vf_loss_hparam\n    self.is_local = is_local\n    self.top_reward = 0.0\n    self.embeddings_trainable = True\n    self.no_op = tf.no_op()\n    self.learning_rate = tf.constant(config.lr, dtype=dtype, name='learning_rate')\n    self.initializer = tf.contrib.layers.variance_scaling_initializer(factor=config.param_init_factor, mode='FAN_AVG', uniform=True, dtype=dtype)\n    tf.get_variable_scope().set_initializer(self.initializer)\n    self.a2c = config.ema_baseline_decay == 0\n    if not self.a2c:\n        logging.info('Using exponential moving average REINFORCE baselines.')\n        self.ema_baseline_decay = config.ema_baseline_decay\n        self.ema_by_len = [0.0] * global_config.timestep_limit\n    else:\n        logging.info('Using advantage (a2c) with learned value function.')\n        self.ema_baseline_decay = 0.0\n        self.ema_by_len = None\n    if config.topk and config.topk_loss_hparam:\n        self.topk_loss_hparam = config.topk_loss_hparam\n        self.topk_batch_size = config.topk_batch_size\n        if self.topk_batch_size <= 0:\n            raise ValueError('topk_batch_size must be a positive integer. Got %s', self.topk_batch_size)\n        self.top_episodes = utils.MaxUniquePriorityQueue(config.topk)\n        logging.info('Made max-priorty-queue with capacity %d', self.top_episodes.capacity)\n    else:\n        self.top_episodes = None\n        self.topk_loss_hparam = 0.0\n        logging.info('No max-priorty-queue')\n    self.replay_temperature = config.replay_temperature\n    self.num_replay_per_batch = int(global_config.batch_size * config.alpha)\n    self.num_on_policy_per_batch = global_config.batch_size - self.num_replay_per_batch\n    self.replay_alpha = self.num_replay_per_batch / float(global_config.batch_size)\n    logging.info('num_replay_per_batch: %d', self.num_replay_per_batch)\n    logging.info('num_on_policy_per_batch: %d', self.num_on_policy_per_batch)\n    logging.info('replay_alpha: %s', self.replay_alpha)\n    if self.num_replay_per_batch > 0:\n        start_time = time.time()\n        self.experience_replay = utils.RouletteWheel(unique_mode=True, save_file=experience_replay_file)\n        logging.info('Took %s sec to load replay buffer from disk.', int(time.time() - start_time))\n        logging.info('Replay buffer file location: \"%s\"', self.experience_replay.save_file)\n    else:\n        self.experience_replay = None\n    if program_count is not None:\n        self.program_count = program_count\n        self.program_count_add_ph = tf.placeholder(tf.int64, [], 'program_count_add_ph')\n        self.program_count_add_op = self.program_count.assign_add(self.program_count_add_ph)\n    batch_size = global_config.batch_size\n    logging.info('batch_size: %d', batch_size)\n    self.policy_cell = LinearWrapper(tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(cell_size) for cell_size in config.policy_lstm_sizes]), self.action_space, dtype=dtype, suppress_index=None if self.allow_eos_token else misc.BF_EOS_INT)\n    self.value_cell = LinearWrapper(tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(cell_size) for cell_size in config.value_lstm_sizes]), 1, dtype=dtype)\n    obs_embedding_scope = 'obs_embed'\n    with tf.variable_scope(obs_embedding_scope, initializer=tf.random_uniform_initializer(minval=-1.0, maxval=1.0)):\n        obs_embeddings = tf.get_variable('embeddings', [self.observation_space, config.obs_embedding_size], dtype=dtype, trainable=self.embeddings_trainable)\n        self.obs_embeddings = obs_embeddings\n    initial_state = tf.fill([batch_size], misc.BF_EOS_INT)\n\n    def loop_fn(loop_time, cell_output, cell_state, loop_state):\n        \"\"\"Function called by tf.nn.raw_rnn to instantiate body of the while_loop.\n\n      See https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn for more\n      information.\n\n      When time is 0, and cell_output, cell_state, loop_state are all None,\n      `loop_fn` will create the initial input, internal cell state, and loop\n      state. When time > 0, `loop_fn` will operate on previous cell output,\n      state, and loop state.\n\n      Args:\n        loop_time: A scalar tensor holding the current timestep (zero based\n            counting).\n        cell_output: Output of the raw_rnn cell at the current timestep.\n        cell_state: Cell internal state at the current timestep.\n        loop_state: Additional loop state. These tensors were returned by the\n            previous call to `loop_fn`.\n\n      Returns:\n        elements_finished: Bool tensor of shape [batch_size] which marks each\n            sequence in the batch as being finished or not finished.\n        next_input: A tensor containing input to be fed into the cell at the\n            next timestep.\n        next_cell_state: Cell internal state to be fed into the cell at the\n            next timestep.\n        emit_output: Tensor to be added to the TensorArray returned by raw_rnn\n            as output from the while_loop.\n        next_loop_state: Additional loop state. These tensors will be fed back\n            into the next call to `loop_fn` as `loop_state`.\n      \"\"\"\n        if cell_output is None:\n            next_cell_state = self.policy_cell.zero_state(batch_size, dtype)\n            elements_finished = tf.zeros([batch_size], tf.bool)\n            output_lengths = tf.ones([batch_size], dtype=tf.int32)\n            next_input = tf.gather(obs_embeddings, initial_state)\n            emit_output = None\n            next_loop_state = (tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True), output_lengths, elements_finished)\n        else:\n            scaled_logits = cell_output * config.softmax_tr\n            (prev_chosen, prev_output_lengths, prev_elements_finished) = loop_state\n            next_cell_state = cell_state\n            chosen_outputs = tf.to_int32(tf.where(tf.logical_not(prev_elements_finished), tf.multinomial(logits=scaled_logits, num_samples=1)[:, 0], tf.zeros([batch_size], dtype=tf.int64)))\n            elements_finished = tf.logical_or(tf.equal(chosen_outputs, misc.BF_EOS_INT), loop_time >= global_config.timestep_limit)\n            output_lengths = tf.where(elements_finished, prev_output_lengths, tf.tile(tf.expand_dims(loop_time + 1, 0), [batch_size]))\n            next_input = tf.gather(obs_embeddings, chosen_outputs)\n            emit_output = scaled_logits\n            next_loop_state = (prev_chosen.write(loop_time - 1, chosen_outputs), output_lengths, tf.logical_or(prev_elements_finished, elements_finished))\n        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n    with tf.variable_scope('policy'):\n        (decoder_outputs_ta, _, (sampled_output_ta, output_lengths, _)) = tf.nn.raw_rnn(cell=self.policy_cell, loop_fn=loop_fn)\n    policy_logits = tf.transpose(decoder_outputs_ta.stack(), (1, 0, 2), name='policy_logits')\n    sampled_tokens = tf.transpose(sampled_output_ta.stack(), (1, 0), name='sampled_tokens')\n    rshift_sampled_tokens = rshift_time(sampled_tokens, fill=misc.BF_EOS_INT)\n    if self.a2c:\n        with tf.variable_scope('value'):\n            (value_output, _) = tf.nn.dynamic_rnn(self.value_cell, tf.gather(obs_embeddings, rshift_sampled_tokens), sequence_length=output_lengths, dtype=dtype)\n        value = tf.squeeze(value_output, axis=[2])\n    else:\n        value = tf.zeros([], dtype=dtype)\n    self.sampled_batch = AttrDict(logits=policy_logits, value=value, tokens=sampled_tokens, episode_lengths=output_lengths, probs=tf.nn.softmax(policy_logits), log_probs=tf.nn.log_softmax(policy_logits))\n    self.adjusted_lengths = tf.placeholder(tf.int32, [None], name='adjusted_lengths')\n    self.policy_multipliers = tf.placeholder(dtype, [None, None], name='policy_multipliers')\n    self.empirical_values = tf.placeholder(dtype, [None, None], name='empirical_values')\n    self.off_policy_targets = tf.placeholder(tf.int32, [None, None], name='off_policy_targets')\n    self.off_policy_target_lengths = tf.placeholder(tf.int32, [None], name='off_policy_target_lengths')\n    self.actions = tf.placeholder(tf.int32, [None, None], name='actions')\n    inputs = rshift_time(self.actions, fill=misc.BF_EOS_INT)\n    with tf.variable_scope('policy', reuse=True):\n        (logits, _) = tf.nn.dynamic_rnn(self.policy_cell, tf.gather(obs_embeddings, inputs), sequence_length=self.adjusted_lengths, dtype=dtype)\n    if self.a2c:\n        with tf.variable_scope('value', reuse=True):\n            (value_output, _) = tf.nn.dynamic_rnn(self.value_cell, tf.gather(obs_embeddings, inputs), sequence_length=self.adjusted_lengths, dtype=dtype)\n        value2 = tf.squeeze(value_output, axis=[2])\n    else:\n        value2 = tf.zeros([], dtype=dtype)\n    self.given_batch = AttrDict(logits=logits, value=value2, tokens=sampled_tokens, episode_lengths=self.adjusted_lengths, probs=tf.nn.softmax(logits), log_probs=tf.nn.log_softmax(logits))\n    max_episode_length = tf.shape(self.actions)[1]\n    range_row = tf.expand_dims(tf.range(max_episode_length), 0)\n    episode_masks = tf.cast(tf.less(range_row, tf.expand_dims(self.given_batch.episode_lengths, 1)), dtype=dtype)\n    episode_masks_3d = tf.expand_dims(episode_masks, 2)\n    self.a_probs = a_probs = self.given_batch.probs * episode_masks_3d\n    self.a_log_probs = a_log_probs = self.given_batch.log_probs * episode_masks_3d\n    self.a_value = a_value = self.given_batch.value * episode_masks\n    self.a_policy_multipliers = a_policy_multipliers = self.policy_multipliers * episode_masks\n    if self.a2c:\n        self.a_empirical_values = a_empirical_values = self.empirical_values * episode_masks\n    acs_onehot = tf.one_hot(self.actions, self.action_space, dtype=dtype)\n    self.acs_onehot = acs_onehot\n    chosen_masked_log_probs = acs_onehot * a_log_probs\n    pi_target = tf.expand_dims(a_policy_multipliers, -1)\n    pi_loss_per_step = chosen_masked_log_probs * pi_target\n    self.pi_loss = pi_loss = -tf.reduce_mean(tf.reduce_sum(pi_loss_per_step, axis=[1, 2]), axis=0) * MAGIC_LOSS_MULTIPLIER\n    assert len(self.pi_loss.shape) == 0\n    self.chosen_log_probs = tf.reduce_sum(chosen_masked_log_probs, axis=2)\n    self.chosen_probs = tf.reduce_sum(acs_onehot * a_probs, axis=2)\n    if self.a2c:\n        vf_loss_per_step = tf.square(a_value - a_empirical_values)\n        self.vf_loss = vf_loss = tf.reduce_mean(tf.reduce_sum(vf_loss_per_step, axis=1), axis=0) * MAGIC_LOSS_MULTIPLIER\n        assert len(self.vf_loss.shape) == 0\n    else:\n        self.vf_loss = vf_loss = 0.0\n    self.entropy = entropy = -tf.reduce_mean(tf.reduce_sum(a_probs * a_log_probs, axis=[1, 2]), axis=0) * MAGIC_LOSS_MULTIPLIER\n    self.negentropy = -entropy\n    assert len(self.negentropy.shape) == 0\n    self.offp_switch = tf.placeholder(dtype, [], name='offp_switch')\n    if self.top_episodes is not None:\n        offp_inputs = tf.gather(obs_embeddings, rshift_time(self.off_policy_targets, fill=misc.BF_EOS_INT))\n        with tf.variable_scope('policy', reuse=True):\n            (offp_logits, _) = tf.nn.dynamic_rnn(self.policy_cell, offp_inputs, self.off_policy_target_lengths, dtype=dtype)\n        topk_loss_per_step = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.off_policy_targets, logits=offp_logits, name='topk_loss_per_logit')\n        topk_loss = tf.reduce_mean(tf.reduce_sum(topk_loss_per_step, axis=1), axis=0)\n        assert len(topk_loss.shape) == 0\n        self.topk_loss = topk_loss * self.offp_switch\n        logging.info('Including off policy loss.')\n    else:\n        self.topk_loss = topk_loss = 0.0\n    self.entropy_hparam = tf.constant(config.entropy_beta, dtype=dtype, name='entropy_beta')\n    self.pi_loss_term = pi_loss * self.pi_loss_hparam\n    self.vf_loss_term = vf_loss * self.vf_loss_hparam\n    self.entropy_loss_term = self.negentropy * self.entropy_hparam\n    self.topk_loss_term = self.topk_loss_hparam * topk_loss\n    self.loss = self.pi_loss_term + self.vf_loss_term + self.entropy_loss_term + self.topk_loss_term\n    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n    self.trainable_variables = params\n    self.sync_variables = self.trainable_variables\n    non_embedding_params = [p for p in params if obs_embedding_scope not in p.name]\n    self.non_embedding_params = non_embedding_params\n    self.params = params\n    if config.regularizer:\n        logging.info('Adding L2 regularizer with scale %.2f.', config.regularizer)\n        self.regularizer = config.regularizer * sum((tf.nn.l2_loss(w) for w in non_embedding_params))\n        self.loss += self.regularizer\n    else:\n        logging.info('Skipping regularizer.')\n        self.regularizer = 0.0\n    if self.is_local:\n        unclipped_grads = tf.gradients(self.loss, params)\n        self.dense_unclipped_grads = [tf.convert_to_tensor(g) for g in unclipped_grads]\n        (self.grads, self.global_grad_norm) = tf.clip_by_global_norm(unclipped_grads, config.grad_clip_threshold)\n        self.gradients_dict = dict(zip(params, self.grads))\n        self.optimizer = make_optimizer(config.optimizer, self.learning_rate)\n        self.all_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, tf.get_variable_scope().name)\n    self.do_iw_summaries = do_iw_summaries\n    if self.do_iw_summaries:\n        b = None\n        self.log_iw_replay_ph = tf.placeholder(tf.float32, [b], 'log_iw_replay_ph')\n        self.log_iw_policy_ph = tf.placeholder(tf.float32, [b], 'log_iw_policy_ph')\n        self.log_prob_replay_ph = tf.placeholder(tf.float32, [b], 'log_prob_replay_ph')\n        self.log_prob_policy_ph = tf.placeholder(tf.float32, [b], 'log_prob_policy_ph')\n        self.log_norm_replay_weights_ph = tf.placeholder(tf.float32, [b], 'log_norm_replay_weights_ph')\n        self.iw_summary_op = tf.summary.merge([tf.summary.histogram('is/log_iw_replay', self.log_iw_replay_ph), tf.summary.histogram('is/log_iw_policy', self.log_iw_policy_ph), tf.summary.histogram('is/log_prob_replay', self.log_prob_replay_ph), tf.summary.histogram('is/log_prob_policy', self.log_prob_policy_ph), tf.summary.histogram('is/log_norm_replay_weights', self.log_norm_replay_weights_ph)])",
            "def __init__(self, global_config, task_id=0, logging_file=None, experience_replay_file=None, global_best_reward_fn=None, found_solution_op=None, assign_code_solution_fn=None, program_count=None, do_iw_summaries=False, stop_on_success=True, dtype=tf.float32, verbose_level=0, is_local=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.config = config = global_config.agent\n    self.logging_file = logging_file\n    self.experience_replay_file = experience_replay_file\n    self.task_id = task_id\n    self.verbose_level = verbose_level\n    self.global_best_reward_fn = global_best_reward_fn\n    self.found_solution_op = found_solution_op\n    self.assign_code_solution_fn = assign_code_solution_fn\n    self.parent_scope_name = tf.get_variable_scope().name\n    self.dtype = dtype\n    self.allow_eos_token = config.eos_token\n    self.stop_on_success = stop_on_success\n    self.pi_loss_hparam = config.pi_loss_hparam\n    self.vf_loss_hparam = config.vf_loss_hparam\n    self.is_local = is_local\n    self.top_reward = 0.0\n    self.embeddings_trainable = True\n    self.no_op = tf.no_op()\n    self.learning_rate = tf.constant(config.lr, dtype=dtype, name='learning_rate')\n    self.initializer = tf.contrib.layers.variance_scaling_initializer(factor=config.param_init_factor, mode='FAN_AVG', uniform=True, dtype=dtype)\n    tf.get_variable_scope().set_initializer(self.initializer)\n    self.a2c = config.ema_baseline_decay == 0\n    if not self.a2c:\n        logging.info('Using exponential moving average REINFORCE baselines.')\n        self.ema_baseline_decay = config.ema_baseline_decay\n        self.ema_by_len = [0.0] * global_config.timestep_limit\n    else:\n        logging.info('Using advantage (a2c) with learned value function.')\n        self.ema_baseline_decay = 0.0\n        self.ema_by_len = None\n    if config.topk and config.topk_loss_hparam:\n        self.topk_loss_hparam = config.topk_loss_hparam\n        self.topk_batch_size = config.topk_batch_size\n        if self.topk_batch_size <= 0:\n            raise ValueError('topk_batch_size must be a positive integer. Got %s', self.topk_batch_size)\n        self.top_episodes = utils.MaxUniquePriorityQueue(config.topk)\n        logging.info('Made max-priorty-queue with capacity %d', self.top_episodes.capacity)\n    else:\n        self.top_episodes = None\n        self.topk_loss_hparam = 0.0\n        logging.info('No max-priorty-queue')\n    self.replay_temperature = config.replay_temperature\n    self.num_replay_per_batch = int(global_config.batch_size * config.alpha)\n    self.num_on_policy_per_batch = global_config.batch_size - self.num_replay_per_batch\n    self.replay_alpha = self.num_replay_per_batch / float(global_config.batch_size)\n    logging.info('num_replay_per_batch: %d', self.num_replay_per_batch)\n    logging.info('num_on_policy_per_batch: %d', self.num_on_policy_per_batch)\n    logging.info('replay_alpha: %s', self.replay_alpha)\n    if self.num_replay_per_batch > 0:\n        start_time = time.time()\n        self.experience_replay = utils.RouletteWheel(unique_mode=True, save_file=experience_replay_file)\n        logging.info('Took %s sec to load replay buffer from disk.', int(time.time() - start_time))\n        logging.info('Replay buffer file location: \"%s\"', self.experience_replay.save_file)\n    else:\n        self.experience_replay = None\n    if program_count is not None:\n        self.program_count = program_count\n        self.program_count_add_ph = tf.placeholder(tf.int64, [], 'program_count_add_ph')\n        self.program_count_add_op = self.program_count.assign_add(self.program_count_add_ph)\n    batch_size = global_config.batch_size\n    logging.info('batch_size: %d', batch_size)\n    self.policy_cell = LinearWrapper(tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(cell_size) for cell_size in config.policy_lstm_sizes]), self.action_space, dtype=dtype, suppress_index=None if self.allow_eos_token else misc.BF_EOS_INT)\n    self.value_cell = LinearWrapper(tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(cell_size) for cell_size in config.value_lstm_sizes]), 1, dtype=dtype)\n    obs_embedding_scope = 'obs_embed'\n    with tf.variable_scope(obs_embedding_scope, initializer=tf.random_uniform_initializer(minval=-1.0, maxval=1.0)):\n        obs_embeddings = tf.get_variable('embeddings', [self.observation_space, config.obs_embedding_size], dtype=dtype, trainable=self.embeddings_trainable)\n        self.obs_embeddings = obs_embeddings\n    initial_state = tf.fill([batch_size], misc.BF_EOS_INT)\n\n    def loop_fn(loop_time, cell_output, cell_state, loop_state):\n        \"\"\"Function called by tf.nn.raw_rnn to instantiate body of the while_loop.\n\n      See https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn for more\n      information.\n\n      When time is 0, and cell_output, cell_state, loop_state are all None,\n      `loop_fn` will create the initial input, internal cell state, and loop\n      state. When time > 0, `loop_fn` will operate on previous cell output,\n      state, and loop state.\n\n      Args:\n        loop_time: A scalar tensor holding the current timestep (zero based\n            counting).\n        cell_output: Output of the raw_rnn cell at the current timestep.\n        cell_state: Cell internal state at the current timestep.\n        loop_state: Additional loop state. These tensors were returned by the\n            previous call to `loop_fn`.\n\n      Returns:\n        elements_finished: Bool tensor of shape [batch_size] which marks each\n            sequence in the batch as being finished or not finished.\n        next_input: A tensor containing input to be fed into the cell at the\n            next timestep.\n        next_cell_state: Cell internal state to be fed into the cell at the\n            next timestep.\n        emit_output: Tensor to be added to the TensorArray returned by raw_rnn\n            as output from the while_loop.\n        next_loop_state: Additional loop state. These tensors will be fed back\n            into the next call to `loop_fn` as `loop_state`.\n      \"\"\"\n        if cell_output is None:\n            next_cell_state = self.policy_cell.zero_state(batch_size, dtype)\n            elements_finished = tf.zeros([batch_size], tf.bool)\n            output_lengths = tf.ones([batch_size], dtype=tf.int32)\n            next_input = tf.gather(obs_embeddings, initial_state)\n            emit_output = None\n            next_loop_state = (tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True), output_lengths, elements_finished)\n        else:\n            scaled_logits = cell_output * config.softmax_tr\n            (prev_chosen, prev_output_lengths, prev_elements_finished) = loop_state\n            next_cell_state = cell_state\n            chosen_outputs = tf.to_int32(tf.where(tf.logical_not(prev_elements_finished), tf.multinomial(logits=scaled_logits, num_samples=1)[:, 0], tf.zeros([batch_size], dtype=tf.int64)))\n            elements_finished = tf.logical_or(tf.equal(chosen_outputs, misc.BF_EOS_INT), loop_time >= global_config.timestep_limit)\n            output_lengths = tf.where(elements_finished, prev_output_lengths, tf.tile(tf.expand_dims(loop_time + 1, 0), [batch_size]))\n            next_input = tf.gather(obs_embeddings, chosen_outputs)\n            emit_output = scaled_logits\n            next_loop_state = (prev_chosen.write(loop_time - 1, chosen_outputs), output_lengths, tf.logical_or(prev_elements_finished, elements_finished))\n        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n    with tf.variable_scope('policy'):\n        (decoder_outputs_ta, _, (sampled_output_ta, output_lengths, _)) = tf.nn.raw_rnn(cell=self.policy_cell, loop_fn=loop_fn)\n    policy_logits = tf.transpose(decoder_outputs_ta.stack(), (1, 0, 2), name='policy_logits')\n    sampled_tokens = tf.transpose(sampled_output_ta.stack(), (1, 0), name='sampled_tokens')\n    rshift_sampled_tokens = rshift_time(sampled_tokens, fill=misc.BF_EOS_INT)\n    if self.a2c:\n        with tf.variable_scope('value'):\n            (value_output, _) = tf.nn.dynamic_rnn(self.value_cell, tf.gather(obs_embeddings, rshift_sampled_tokens), sequence_length=output_lengths, dtype=dtype)\n        value = tf.squeeze(value_output, axis=[2])\n    else:\n        value = tf.zeros([], dtype=dtype)\n    self.sampled_batch = AttrDict(logits=policy_logits, value=value, tokens=sampled_tokens, episode_lengths=output_lengths, probs=tf.nn.softmax(policy_logits), log_probs=tf.nn.log_softmax(policy_logits))\n    self.adjusted_lengths = tf.placeholder(tf.int32, [None], name='adjusted_lengths')\n    self.policy_multipliers = tf.placeholder(dtype, [None, None], name='policy_multipliers')\n    self.empirical_values = tf.placeholder(dtype, [None, None], name='empirical_values')\n    self.off_policy_targets = tf.placeholder(tf.int32, [None, None], name='off_policy_targets')\n    self.off_policy_target_lengths = tf.placeholder(tf.int32, [None], name='off_policy_target_lengths')\n    self.actions = tf.placeholder(tf.int32, [None, None], name='actions')\n    inputs = rshift_time(self.actions, fill=misc.BF_EOS_INT)\n    with tf.variable_scope('policy', reuse=True):\n        (logits, _) = tf.nn.dynamic_rnn(self.policy_cell, tf.gather(obs_embeddings, inputs), sequence_length=self.adjusted_lengths, dtype=dtype)\n    if self.a2c:\n        with tf.variable_scope('value', reuse=True):\n            (value_output, _) = tf.nn.dynamic_rnn(self.value_cell, tf.gather(obs_embeddings, inputs), sequence_length=self.adjusted_lengths, dtype=dtype)\n        value2 = tf.squeeze(value_output, axis=[2])\n    else:\n        value2 = tf.zeros([], dtype=dtype)\n    self.given_batch = AttrDict(logits=logits, value=value2, tokens=sampled_tokens, episode_lengths=self.adjusted_lengths, probs=tf.nn.softmax(logits), log_probs=tf.nn.log_softmax(logits))\n    max_episode_length = tf.shape(self.actions)[1]\n    range_row = tf.expand_dims(tf.range(max_episode_length), 0)\n    episode_masks = tf.cast(tf.less(range_row, tf.expand_dims(self.given_batch.episode_lengths, 1)), dtype=dtype)\n    episode_masks_3d = tf.expand_dims(episode_masks, 2)\n    self.a_probs = a_probs = self.given_batch.probs * episode_masks_3d\n    self.a_log_probs = a_log_probs = self.given_batch.log_probs * episode_masks_3d\n    self.a_value = a_value = self.given_batch.value * episode_masks\n    self.a_policy_multipliers = a_policy_multipliers = self.policy_multipliers * episode_masks\n    if self.a2c:\n        self.a_empirical_values = a_empirical_values = self.empirical_values * episode_masks\n    acs_onehot = tf.one_hot(self.actions, self.action_space, dtype=dtype)\n    self.acs_onehot = acs_onehot\n    chosen_masked_log_probs = acs_onehot * a_log_probs\n    pi_target = tf.expand_dims(a_policy_multipliers, -1)\n    pi_loss_per_step = chosen_masked_log_probs * pi_target\n    self.pi_loss = pi_loss = -tf.reduce_mean(tf.reduce_sum(pi_loss_per_step, axis=[1, 2]), axis=0) * MAGIC_LOSS_MULTIPLIER\n    assert len(self.pi_loss.shape) == 0\n    self.chosen_log_probs = tf.reduce_sum(chosen_masked_log_probs, axis=2)\n    self.chosen_probs = tf.reduce_sum(acs_onehot * a_probs, axis=2)\n    if self.a2c:\n        vf_loss_per_step = tf.square(a_value - a_empirical_values)\n        self.vf_loss = vf_loss = tf.reduce_mean(tf.reduce_sum(vf_loss_per_step, axis=1), axis=0) * MAGIC_LOSS_MULTIPLIER\n        assert len(self.vf_loss.shape) == 0\n    else:\n        self.vf_loss = vf_loss = 0.0\n    self.entropy = entropy = -tf.reduce_mean(tf.reduce_sum(a_probs * a_log_probs, axis=[1, 2]), axis=0) * MAGIC_LOSS_MULTIPLIER\n    self.negentropy = -entropy\n    assert len(self.negentropy.shape) == 0\n    self.offp_switch = tf.placeholder(dtype, [], name='offp_switch')\n    if self.top_episodes is not None:\n        offp_inputs = tf.gather(obs_embeddings, rshift_time(self.off_policy_targets, fill=misc.BF_EOS_INT))\n        with tf.variable_scope('policy', reuse=True):\n            (offp_logits, _) = tf.nn.dynamic_rnn(self.policy_cell, offp_inputs, self.off_policy_target_lengths, dtype=dtype)\n        topk_loss_per_step = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.off_policy_targets, logits=offp_logits, name='topk_loss_per_logit')\n        topk_loss = tf.reduce_mean(tf.reduce_sum(topk_loss_per_step, axis=1), axis=0)\n        assert len(topk_loss.shape) == 0\n        self.topk_loss = topk_loss * self.offp_switch\n        logging.info('Including off policy loss.')\n    else:\n        self.topk_loss = topk_loss = 0.0\n    self.entropy_hparam = tf.constant(config.entropy_beta, dtype=dtype, name='entropy_beta')\n    self.pi_loss_term = pi_loss * self.pi_loss_hparam\n    self.vf_loss_term = vf_loss * self.vf_loss_hparam\n    self.entropy_loss_term = self.negentropy * self.entropy_hparam\n    self.topk_loss_term = self.topk_loss_hparam * topk_loss\n    self.loss = self.pi_loss_term + self.vf_loss_term + self.entropy_loss_term + self.topk_loss_term\n    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n    self.trainable_variables = params\n    self.sync_variables = self.trainable_variables\n    non_embedding_params = [p for p in params if obs_embedding_scope not in p.name]\n    self.non_embedding_params = non_embedding_params\n    self.params = params\n    if config.regularizer:\n        logging.info('Adding L2 regularizer with scale %.2f.', config.regularizer)\n        self.regularizer = config.regularizer * sum((tf.nn.l2_loss(w) for w in non_embedding_params))\n        self.loss += self.regularizer\n    else:\n        logging.info('Skipping regularizer.')\n        self.regularizer = 0.0\n    if self.is_local:\n        unclipped_grads = tf.gradients(self.loss, params)\n        self.dense_unclipped_grads = [tf.convert_to_tensor(g) for g in unclipped_grads]\n        (self.grads, self.global_grad_norm) = tf.clip_by_global_norm(unclipped_grads, config.grad_clip_threshold)\n        self.gradients_dict = dict(zip(params, self.grads))\n        self.optimizer = make_optimizer(config.optimizer, self.learning_rate)\n        self.all_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, tf.get_variable_scope().name)\n    self.do_iw_summaries = do_iw_summaries\n    if self.do_iw_summaries:\n        b = None\n        self.log_iw_replay_ph = tf.placeholder(tf.float32, [b], 'log_iw_replay_ph')\n        self.log_iw_policy_ph = tf.placeholder(tf.float32, [b], 'log_iw_policy_ph')\n        self.log_prob_replay_ph = tf.placeholder(tf.float32, [b], 'log_prob_replay_ph')\n        self.log_prob_policy_ph = tf.placeholder(tf.float32, [b], 'log_prob_policy_ph')\n        self.log_norm_replay_weights_ph = tf.placeholder(tf.float32, [b], 'log_norm_replay_weights_ph')\n        self.iw_summary_op = tf.summary.merge([tf.summary.histogram('is/log_iw_replay', self.log_iw_replay_ph), tf.summary.histogram('is/log_iw_policy', self.log_iw_policy_ph), tf.summary.histogram('is/log_prob_replay', self.log_prob_replay_ph), tf.summary.histogram('is/log_prob_policy', self.log_prob_policy_ph), tf.summary.histogram('is/log_norm_replay_weights', self.log_norm_replay_weights_ph)])",
            "def __init__(self, global_config, task_id=0, logging_file=None, experience_replay_file=None, global_best_reward_fn=None, found_solution_op=None, assign_code_solution_fn=None, program_count=None, do_iw_summaries=False, stop_on_success=True, dtype=tf.float32, verbose_level=0, is_local=True):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.config = config = global_config.agent\n    self.logging_file = logging_file\n    self.experience_replay_file = experience_replay_file\n    self.task_id = task_id\n    self.verbose_level = verbose_level\n    self.global_best_reward_fn = global_best_reward_fn\n    self.found_solution_op = found_solution_op\n    self.assign_code_solution_fn = assign_code_solution_fn\n    self.parent_scope_name = tf.get_variable_scope().name\n    self.dtype = dtype\n    self.allow_eos_token = config.eos_token\n    self.stop_on_success = stop_on_success\n    self.pi_loss_hparam = config.pi_loss_hparam\n    self.vf_loss_hparam = config.vf_loss_hparam\n    self.is_local = is_local\n    self.top_reward = 0.0\n    self.embeddings_trainable = True\n    self.no_op = tf.no_op()\n    self.learning_rate = tf.constant(config.lr, dtype=dtype, name='learning_rate')\n    self.initializer = tf.contrib.layers.variance_scaling_initializer(factor=config.param_init_factor, mode='FAN_AVG', uniform=True, dtype=dtype)\n    tf.get_variable_scope().set_initializer(self.initializer)\n    self.a2c = config.ema_baseline_decay == 0\n    if not self.a2c:\n        logging.info('Using exponential moving average REINFORCE baselines.')\n        self.ema_baseline_decay = config.ema_baseline_decay\n        self.ema_by_len = [0.0] * global_config.timestep_limit\n    else:\n        logging.info('Using advantage (a2c) with learned value function.')\n        self.ema_baseline_decay = 0.0\n        self.ema_by_len = None\n    if config.topk and config.topk_loss_hparam:\n        self.topk_loss_hparam = config.topk_loss_hparam\n        self.topk_batch_size = config.topk_batch_size\n        if self.topk_batch_size <= 0:\n            raise ValueError('topk_batch_size must be a positive integer. Got %s', self.topk_batch_size)\n        self.top_episodes = utils.MaxUniquePriorityQueue(config.topk)\n        logging.info('Made max-priorty-queue with capacity %d', self.top_episodes.capacity)\n    else:\n        self.top_episodes = None\n        self.topk_loss_hparam = 0.0\n        logging.info('No max-priorty-queue')\n    self.replay_temperature = config.replay_temperature\n    self.num_replay_per_batch = int(global_config.batch_size * config.alpha)\n    self.num_on_policy_per_batch = global_config.batch_size - self.num_replay_per_batch\n    self.replay_alpha = self.num_replay_per_batch / float(global_config.batch_size)\n    logging.info('num_replay_per_batch: %d', self.num_replay_per_batch)\n    logging.info('num_on_policy_per_batch: %d', self.num_on_policy_per_batch)\n    logging.info('replay_alpha: %s', self.replay_alpha)\n    if self.num_replay_per_batch > 0:\n        start_time = time.time()\n        self.experience_replay = utils.RouletteWheel(unique_mode=True, save_file=experience_replay_file)\n        logging.info('Took %s sec to load replay buffer from disk.', int(time.time() - start_time))\n        logging.info('Replay buffer file location: \"%s\"', self.experience_replay.save_file)\n    else:\n        self.experience_replay = None\n    if program_count is not None:\n        self.program_count = program_count\n        self.program_count_add_ph = tf.placeholder(tf.int64, [], 'program_count_add_ph')\n        self.program_count_add_op = self.program_count.assign_add(self.program_count_add_ph)\n    batch_size = global_config.batch_size\n    logging.info('batch_size: %d', batch_size)\n    self.policy_cell = LinearWrapper(tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(cell_size) for cell_size in config.policy_lstm_sizes]), self.action_space, dtype=dtype, suppress_index=None if self.allow_eos_token else misc.BF_EOS_INT)\n    self.value_cell = LinearWrapper(tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(cell_size) for cell_size in config.value_lstm_sizes]), 1, dtype=dtype)\n    obs_embedding_scope = 'obs_embed'\n    with tf.variable_scope(obs_embedding_scope, initializer=tf.random_uniform_initializer(minval=-1.0, maxval=1.0)):\n        obs_embeddings = tf.get_variable('embeddings', [self.observation_space, config.obs_embedding_size], dtype=dtype, trainable=self.embeddings_trainable)\n        self.obs_embeddings = obs_embeddings\n    initial_state = tf.fill([batch_size], misc.BF_EOS_INT)\n\n    def loop_fn(loop_time, cell_output, cell_state, loop_state):\n        \"\"\"Function called by tf.nn.raw_rnn to instantiate body of the while_loop.\n\n      See https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn for more\n      information.\n\n      When time is 0, and cell_output, cell_state, loop_state are all None,\n      `loop_fn` will create the initial input, internal cell state, and loop\n      state. When time > 0, `loop_fn` will operate on previous cell output,\n      state, and loop state.\n\n      Args:\n        loop_time: A scalar tensor holding the current timestep (zero based\n            counting).\n        cell_output: Output of the raw_rnn cell at the current timestep.\n        cell_state: Cell internal state at the current timestep.\n        loop_state: Additional loop state. These tensors were returned by the\n            previous call to `loop_fn`.\n\n      Returns:\n        elements_finished: Bool tensor of shape [batch_size] which marks each\n            sequence in the batch as being finished or not finished.\n        next_input: A tensor containing input to be fed into the cell at the\n            next timestep.\n        next_cell_state: Cell internal state to be fed into the cell at the\n            next timestep.\n        emit_output: Tensor to be added to the TensorArray returned by raw_rnn\n            as output from the while_loop.\n        next_loop_state: Additional loop state. These tensors will be fed back\n            into the next call to `loop_fn` as `loop_state`.\n      \"\"\"\n        if cell_output is None:\n            next_cell_state = self.policy_cell.zero_state(batch_size, dtype)\n            elements_finished = tf.zeros([batch_size], tf.bool)\n            output_lengths = tf.ones([batch_size], dtype=tf.int32)\n            next_input = tf.gather(obs_embeddings, initial_state)\n            emit_output = None\n            next_loop_state = (tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True), output_lengths, elements_finished)\n        else:\n            scaled_logits = cell_output * config.softmax_tr\n            (prev_chosen, prev_output_lengths, prev_elements_finished) = loop_state\n            next_cell_state = cell_state\n            chosen_outputs = tf.to_int32(tf.where(tf.logical_not(prev_elements_finished), tf.multinomial(logits=scaled_logits, num_samples=1)[:, 0], tf.zeros([batch_size], dtype=tf.int64)))\n            elements_finished = tf.logical_or(tf.equal(chosen_outputs, misc.BF_EOS_INT), loop_time >= global_config.timestep_limit)\n            output_lengths = tf.where(elements_finished, prev_output_lengths, tf.tile(tf.expand_dims(loop_time + 1, 0), [batch_size]))\n            next_input = tf.gather(obs_embeddings, chosen_outputs)\n            emit_output = scaled_logits\n            next_loop_state = (prev_chosen.write(loop_time - 1, chosen_outputs), output_lengths, tf.logical_or(prev_elements_finished, elements_finished))\n        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n    with tf.variable_scope('policy'):\n        (decoder_outputs_ta, _, (sampled_output_ta, output_lengths, _)) = tf.nn.raw_rnn(cell=self.policy_cell, loop_fn=loop_fn)\n    policy_logits = tf.transpose(decoder_outputs_ta.stack(), (1, 0, 2), name='policy_logits')\n    sampled_tokens = tf.transpose(sampled_output_ta.stack(), (1, 0), name='sampled_tokens')\n    rshift_sampled_tokens = rshift_time(sampled_tokens, fill=misc.BF_EOS_INT)\n    if self.a2c:\n        with tf.variable_scope('value'):\n            (value_output, _) = tf.nn.dynamic_rnn(self.value_cell, tf.gather(obs_embeddings, rshift_sampled_tokens), sequence_length=output_lengths, dtype=dtype)\n        value = tf.squeeze(value_output, axis=[2])\n    else:\n        value = tf.zeros([], dtype=dtype)\n    self.sampled_batch = AttrDict(logits=policy_logits, value=value, tokens=sampled_tokens, episode_lengths=output_lengths, probs=tf.nn.softmax(policy_logits), log_probs=tf.nn.log_softmax(policy_logits))\n    self.adjusted_lengths = tf.placeholder(tf.int32, [None], name='adjusted_lengths')\n    self.policy_multipliers = tf.placeholder(dtype, [None, None], name='policy_multipliers')\n    self.empirical_values = tf.placeholder(dtype, [None, None], name='empirical_values')\n    self.off_policy_targets = tf.placeholder(tf.int32, [None, None], name='off_policy_targets')\n    self.off_policy_target_lengths = tf.placeholder(tf.int32, [None], name='off_policy_target_lengths')\n    self.actions = tf.placeholder(tf.int32, [None, None], name='actions')\n    inputs = rshift_time(self.actions, fill=misc.BF_EOS_INT)\n    with tf.variable_scope('policy', reuse=True):\n        (logits, _) = tf.nn.dynamic_rnn(self.policy_cell, tf.gather(obs_embeddings, inputs), sequence_length=self.adjusted_lengths, dtype=dtype)\n    if self.a2c:\n        with tf.variable_scope('value', reuse=True):\n            (value_output, _) = tf.nn.dynamic_rnn(self.value_cell, tf.gather(obs_embeddings, inputs), sequence_length=self.adjusted_lengths, dtype=dtype)\n        value2 = tf.squeeze(value_output, axis=[2])\n    else:\n        value2 = tf.zeros([], dtype=dtype)\n    self.given_batch = AttrDict(logits=logits, value=value2, tokens=sampled_tokens, episode_lengths=self.adjusted_lengths, probs=tf.nn.softmax(logits), log_probs=tf.nn.log_softmax(logits))\n    max_episode_length = tf.shape(self.actions)[1]\n    range_row = tf.expand_dims(tf.range(max_episode_length), 0)\n    episode_masks = tf.cast(tf.less(range_row, tf.expand_dims(self.given_batch.episode_lengths, 1)), dtype=dtype)\n    episode_masks_3d = tf.expand_dims(episode_masks, 2)\n    self.a_probs = a_probs = self.given_batch.probs * episode_masks_3d\n    self.a_log_probs = a_log_probs = self.given_batch.log_probs * episode_masks_3d\n    self.a_value = a_value = self.given_batch.value * episode_masks\n    self.a_policy_multipliers = a_policy_multipliers = self.policy_multipliers * episode_masks\n    if self.a2c:\n        self.a_empirical_values = a_empirical_values = self.empirical_values * episode_masks\n    acs_onehot = tf.one_hot(self.actions, self.action_space, dtype=dtype)\n    self.acs_onehot = acs_onehot\n    chosen_masked_log_probs = acs_onehot * a_log_probs\n    pi_target = tf.expand_dims(a_policy_multipliers, -1)\n    pi_loss_per_step = chosen_masked_log_probs * pi_target\n    self.pi_loss = pi_loss = -tf.reduce_mean(tf.reduce_sum(pi_loss_per_step, axis=[1, 2]), axis=0) * MAGIC_LOSS_MULTIPLIER\n    assert len(self.pi_loss.shape) == 0\n    self.chosen_log_probs = tf.reduce_sum(chosen_masked_log_probs, axis=2)\n    self.chosen_probs = tf.reduce_sum(acs_onehot * a_probs, axis=2)\n    if self.a2c:\n        vf_loss_per_step = tf.square(a_value - a_empirical_values)\n        self.vf_loss = vf_loss = tf.reduce_mean(tf.reduce_sum(vf_loss_per_step, axis=1), axis=0) * MAGIC_LOSS_MULTIPLIER\n        assert len(self.vf_loss.shape) == 0\n    else:\n        self.vf_loss = vf_loss = 0.0\n    self.entropy = entropy = -tf.reduce_mean(tf.reduce_sum(a_probs * a_log_probs, axis=[1, 2]), axis=0) * MAGIC_LOSS_MULTIPLIER\n    self.negentropy = -entropy\n    assert len(self.negentropy.shape) == 0\n    self.offp_switch = tf.placeholder(dtype, [], name='offp_switch')\n    if self.top_episodes is not None:\n        offp_inputs = tf.gather(obs_embeddings, rshift_time(self.off_policy_targets, fill=misc.BF_EOS_INT))\n        with tf.variable_scope('policy', reuse=True):\n            (offp_logits, _) = tf.nn.dynamic_rnn(self.policy_cell, offp_inputs, self.off_policy_target_lengths, dtype=dtype)\n        topk_loss_per_step = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.off_policy_targets, logits=offp_logits, name='topk_loss_per_logit')\n        topk_loss = tf.reduce_mean(tf.reduce_sum(topk_loss_per_step, axis=1), axis=0)\n        assert len(topk_loss.shape) == 0\n        self.topk_loss = topk_loss * self.offp_switch\n        logging.info('Including off policy loss.')\n    else:\n        self.topk_loss = topk_loss = 0.0\n    self.entropy_hparam = tf.constant(config.entropy_beta, dtype=dtype, name='entropy_beta')\n    self.pi_loss_term = pi_loss * self.pi_loss_hparam\n    self.vf_loss_term = vf_loss * self.vf_loss_hparam\n    self.entropy_loss_term = self.negentropy * self.entropy_hparam\n    self.topk_loss_term = self.topk_loss_hparam * topk_loss\n    self.loss = self.pi_loss_term + self.vf_loss_term + self.entropy_loss_term + self.topk_loss_term\n    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, tf.get_variable_scope().name)\n    self.trainable_variables = params\n    self.sync_variables = self.trainable_variables\n    non_embedding_params = [p for p in params if obs_embedding_scope not in p.name]\n    self.non_embedding_params = non_embedding_params\n    self.params = params\n    if config.regularizer:\n        logging.info('Adding L2 regularizer with scale %.2f.', config.regularizer)\n        self.regularizer = config.regularizer * sum((tf.nn.l2_loss(w) for w in non_embedding_params))\n        self.loss += self.regularizer\n    else:\n        logging.info('Skipping regularizer.')\n        self.regularizer = 0.0\n    if self.is_local:\n        unclipped_grads = tf.gradients(self.loss, params)\n        self.dense_unclipped_grads = [tf.convert_to_tensor(g) for g in unclipped_grads]\n        (self.grads, self.global_grad_norm) = tf.clip_by_global_norm(unclipped_grads, config.grad_clip_threshold)\n        self.gradients_dict = dict(zip(params, self.grads))\n        self.optimizer = make_optimizer(config.optimizer, self.learning_rate)\n        self.all_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, tf.get_variable_scope().name)\n    self.do_iw_summaries = do_iw_summaries\n    if self.do_iw_summaries:\n        b = None\n        self.log_iw_replay_ph = tf.placeholder(tf.float32, [b], 'log_iw_replay_ph')\n        self.log_iw_policy_ph = tf.placeholder(tf.float32, [b], 'log_iw_policy_ph')\n        self.log_prob_replay_ph = tf.placeholder(tf.float32, [b], 'log_prob_replay_ph')\n        self.log_prob_policy_ph = tf.placeholder(tf.float32, [b], 'log_prob_policy_ph')\n        self.log_norm_replay_weights_ph = tf.placeholder(tf.float32, [b], 'log_norm_replay_weights_ph')\n        self.iw_summary_op = tf.summary.merge([tf.summary.histogram('is/log_iw_replay', self.log_iw_replay_ph), tf.summary.histogram('is/log_iw_policy', self.log_iw_policy_ph), tf.summary.histogram('is/log_prob_replay', self.log_prob_replay_ph), tf.summary.histogram('is/log_prob_policy', self.log_prob_policy_ph), tf.summary.histogram('is/log_norm_replay_weights', self.log_norm_replay_weights_ph)])"
        ]
    },
    {
        "func_name": "_remove_prefix",
        "original": "def _remove_prefix(prefix, name):\n    assert name.startswith(prefix)\n    return name[len(prefix):]",
        "mutated": [
            "def _remove_prefix(prefix, name):\n    if False:\n        i = 10\n    assert name.startswith(prefix)\n    return name[len(prefix):]",
            "def _remove_prefix(prefix, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    assert name.startswith(prefix)\n    return name[len(prefix):]",
            "def _remove_prefix(prefix, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    assert name.startswith(prefix)\n    return name[len(prefix):]",
            "def _remove_prefix(prefix, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    assert name.startswith(prefix)\n    return name[len(prefix):]",
            "def _remove_prefix(prefix, name):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    assert name.startswith(prefix)\n    return name[len(prefix):]"
        ]
    },
    {
        "func_name": "make_summary_ops",
        "original": "def make_summary_ops(self):\n    \"\"\"Construct summary ops for the model.\"\"\"\n    size = tf.cast(tf.reduce_sum(self.given_batch.episode_lengths), dtype=self.dtype)\n    offp_size = tf.cast(tf.reduce_sum(self.off_policy_target_lengths), dtype=self.dtype)\n    scope_prefix = self.parent_scope_name\n\n    def _remove_prefix(prefix, name):\n        assert name.startswith(prefix)\n        return name[len(prefix):]\n    self.rl_summary_op = tf.summary.merge([tf.summary.scalar('model/policy_loss', self.pi_loss / size), tf.summary.scalar('model/value_loss', self.vf_loss / size), tf.summary.scalar('model/topk_loss', self.topk_loss / offp_size), tf.summary.scalar('model/entropy', self.entropy / size), tf.summary.scalar('model/loss', self.loss / size), tf.summary.scalar('model/grad_norm', tf.global_norm(self.grads)), tf.summary.scalar('model/unclipped_grad_norm', self.global_grad_norm), tf.summary.scalar('model/non_embedding_var_norm', tf.global_norm(self.non_embedding_params)), tf.summary.scalar('hparams/entropy_beta', self.entropy_hparam), tf.summary.scalar('hparams/topk_loss_hparam', self.topk_loss_hparam), tf.summary.scalar('hparams/learning_rate', self.learning_rate), tf.summary.scalar('model/trainable_var_norm', tf.global_norm(self.trainable_variables)), tf.summary.scalar('loss/loss', self.loss), tf.summary.scalar('loss/entropy', self.entropy_loss_term), tf.summary.scalar('loss/vf', self.vf_loss_term), tf.summary.scalar('loss/policy', self.pi_loss_term), tf.summary.scalar('loss/offp', self.topk_loss_term)] + [tf.summary.scalar('param_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(p)) for p in self.params] + [tf.summary.scalar('grad_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(g)) for (p, g) in zip(self.params, self.grads)] + [tf.summary.scalar('unclipped_grad_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(g)) for (p, g) in zip(self.params, self.dense_unclipped_grads)])\n    self.text_summary_placeholder = tf.placeholder(tf.string, shape=[])\n    self.rl_text_summary_op = tf.summary.text('rl', self.text_summary_placeholder)",
        "mutated": [
            "def make_summary_ops(self):\n    if False:\n        i = 10\n    'Construct summary ops for the model.'\n    size = tf.cast(tf.reduce_sum(self.given_batch.episode_lengths), dtype=self.dtype)\n    offp_size = tf.cast(tf.reduce_sum(self.off_policy_target_lengths), dtype=self.dtype)\n    scope_prefix = self.parent_scope_name\n\n    def _remove_prefix(prefix, name):\n        assert name.startswith(prefix)\n        return name[len(prefix):]\n    self.rl_summary_op = tf.summary.merge([tf.summary.scalar('model/policy_loss', self.pi_loss / size), tf.summary.scalar('model/value_loss', self.vf_loss / size), tf.summary.scalar('model/topk_loss', self.topk_loss / offp_size), tf.summary.scalar('model/entropy', self.entropy / size), tf.summary.scalar('model/loss', self.loss / size), tf.summary.scalar('model/grad_norm', tf.global_norm(self.grads)), tf.summary.scalar('model/unclipped_grad_norm', self.global_grad_norm), tf.summary.scalar('model/non_embedding_var_norm', tf.global_norm(self.non_embedding_params)), tf.summary.scalar('hparams/entropy_beta', self.entropy_hparam), tf.summary.scalar('hparams/topk_loss_hparam', self.topk_loss_hparam), tf.summary.scalar('hparams/learning_rate', self.learning_rate), tf.summary.scalar('model/trainable_var_norm', tf.global_norm(self.trainable_variables)), tf.summary.scalar('loss/loss', self.loss), tf.summary.scalar('loss/entropy', self.entropy_loss_term), tf.summary.scalar('loss/vf', self.vf_loss_term), tf.summary.scalar('loss/policy', self.pi_loss_term), tf.summary.scalar('loss/offp', self.topk_loss_term)] + [tf.summary.scalar('param_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(p)) for p in self.params] + [tf.summary.scalar('grad_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(g)) for (p, g) in zip(self.params, self.grads)] + [tf.summary.scalar('unclipped_grad_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(g)) for (p, g) in zip(self.params, self.dense_unclipped_grads)])\n    self.text_summary_placeholder = tf.placeholder(tf.string, shape=[])\n    self.rl_text_summary_op = tf.summary.text('rl', self.text_summary_placeholder)",
            "def make_summary_ops(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Construct summary ops for the model.'\n    size = tf.cast(tf.reduce_sum(self.given_batch.episode_lengths), dtype=self.dtype)\n    offp_size = tf.cast(tf.reduce_sum(self.off_policy_target_lengths), dtype=self.dtype)\n    scope_prefix = self.parent_scope_name\n\n    def _remove_prefix(prefix, name):\n        assert name.startswith(prefix)\n        return name[len(prefix):]\n    self.rl_summary_op = tf.summary.merge([tf.summary.scalar('model/policy_loss', self.pi_loss / size), tf.summary.scalar('model/value_loss', self.vf_loss / size), tf.summary.scalar('model/topk_loss', self.topk_loss / offp_size), tf.summary.scalar('model/entropy', self.entropy / size), tf.summary.scalar('model/loss', self.loss / size), tf.summary.scalar('model/grad_norm', tf.global_norm(self.grads)), tf.summary.scalar('model/unclipped_grad_norm', self.global_grad_norm), tf.summary.scalar('model/non_embedding_var_norm', tf.global_norm(self.non_embedding_params)), tf.summary.scalar('hparams/entropy_beta', self.entropy_hparam), tf.summary.scalar('hparams/topk_loss_hparam', self.topk_loss_hparam), tf.summary.scalar('hparams/learning_rate', self.learning_rate), tf.summary.scalar('model/trainable_var_norm', tf.global_norm(self.trainable_variables)), tf.summary.scalar('loss/loss', self.loss), tf.summary.scalar('loss/entropy', self.entropy_loss_term), tf.summary.scalar('loss/vf', self.vf_loss_term), tf.summary.scalar('loss/policy', self.pi_loss_term), tf.summary.scalar('loss/offp', self.topk_loss_term)] + [tf.summary.scalar('param_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(p)) for p in self.params] + [tf.summary.scalar('grad_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(g)) for (p, g) in zip(self.params, self.grads)] + [tf.summary.scalar('unclipped_grad_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(g)) for (p, g) in zip(self.params, self.dense_unclipped_grads)])\n    self.text_summary_placeholder = tf.placeholder(tf.string, shape=[])\n    self.rl_text_summary_op = tf.summary.text('rl', self.text_summary_placeholder)",
            "def make_summary_ops(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Construct summary ops for the model.'\n    size = tf.cast(tf.reduce_sum(self.given_batch.episode_lengths), dtype=self.dtype)\n    offp_size = tf.cast(tf.reduce_sum(self.off_policy_target_lengths), dtype=self.dtype)\n    scope_prefix = self.parent_scope_name\n\n    def _remove_prefix(prefix, name):\n        assert name.startswith(prefix)\n        return name[len(prefix):]\n    self.rl_summary_op = tf.summary.merge([tf.summary.scalar('model/policy_loss', self.pi_loss / size), tf.summary.scalar('model/value_loss', self.vf_loss / size), tf.summary.scalar('model/topk_loss', self.topk_loss / offp_size), tf.summary.scalar('model/entropy', self.entropy / size), tf.summary.scalar('model/loss', self.loss / size), tf.summary.scalar('model/grad_norm', tf.global_norm(self.grads)), tf.summary.scalar('model/unclipped_grad_norm', self.global_grad_norm), tf.summary.scalar('model/non_embedding_var_norm', tf.global_norm(self.non_embedding_params)), tf.summary.scalar('hparams/entropy_beta', self.entropy_hparam), tf.summary.scalar('hparams/topk_loss_hparam', self.topk_loss_hparam), tf.summary.scalar('hparams/learning_rate', self.learning_rate), tf.summary.scalar('model/trainable_var_norm', tf.global_norm(self.trainable_variables)), tf.summary.scalar('loss/loss', self.loss), tf.summary.scalar('loss/entropy', self.entropy_loss_term), tf.summary.scalar('loss/vf', self.vf_loss_term), tf.summary.scalar('loss/policy', self.pi_loss_term), tf.summary.scalar('loss/offp', self.topk_loss_term)] + [tf.summary.scalar('param_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(p)) for p in self.params] + [tf.summary.scalar('grad_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(g)) for (p, g) in zip(self.params, self.grads)] + [tf.summary.scalar('unclipped_grad_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(g)) for (p, g) in zip(self.params, self.dense_unclipped_grads)])\n    self.text_summary_placeholder = tf.placeholder(tf.string, shape=[])\n    self.rl_text_summary_op = tf.summary.text('rl', self.text_summary_placeholder)",
            "def make_summary_ops(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Construct summary ops for the model.'\n    size = tf.cast(tf.reduce_sum(self.given_batch.episode_lengths), dtype=self.dtype)\n    offp_size = tf.cast(tf.reduce_sum(self.off_policy_target_lengths), dtype=self.dtype)\n    scope_prefix = self.parent_scope_name\n\n    def _remove_prefix(prefix, name):\n        assert name.startswith(prefix)\n        return name[len(prefix):]\n    self.rl_summary_op = tf.summary.merge([tf.summary.scalar('model/policy_loss', self.pi_loss / size), tf.summary.scalar('model/value_loss', self.vf_loss / size), tf.summary.scalar('model/topk_loss', self.topk_loss / offp_size), tf.summary.scalar('model/entropy', self.entropy / size), tf.summary.scalar('model/loss', self.loss / size), tf.summary.scalar('model/grad_norm', tf.global_norm(self.grads)), tf.summary.scalar('model/unclipped_grad_norm', self.global_grad_norm), tf.summary.scalar('model/non_embedding_var_norm', tf.global_norm(self.non_embedding_params)), tf.summary.scalar('hparams/entropy_beta', self.entropy_hparam), tf.summary.scalar('hparams/topk_loss_hparam', self.topk_loss_hparam), tf.summary.scalar('hparams/learning_rate', self.learning_rate), tf.summary.scalar('model/trainable_var_norm', tf.global_norm(self.trainable_variables)), tf.summary.scalar('loss/loss', self.loss), tf.summary.scalar('loss/entropy', self.entropy_loss_term), tf.summary.scalar('loss/vf', self.vf_loss_term), tf.summary.scalar('loss/policy', self.pi_loss_term), tf.summary.scalar('loss/offp', self.topk_loss_term)] + [tf.summary.scalar('param_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(p)) for p in self.params] + [tf.summary.scalar('grad_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(g)) for (p, g) in zip(self.params, self.grads)] + [tf.summary.scalar('unclipped_grad_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(g)) for (p, g) in zip(self.params, self.dense_unclipped_grads)])\n    self.text_summary_placeholder = tf.placeholder(tf.string, shape=[])\n    self.rl_text_summary_op = tf.summary.text('rl', self.text_summary_placeholder)",
            "def make_summary_ops(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Construct summary ops for the model.'\n    size = tf.cast(tf.reduce_sum(self.given_batch.episode_lengths), dtype=self.dtype)\n    offp_size = tf.cast(tf.reduce_sum(self.off_policy_target_lengths), dtype=self.dtype)\n    scope_prefix = self.parent_scope_name\n\n    def _remove_prefix(prefix, name):\n        assert name.startswith(prefix)\n        return name[len(prefix):]\n    self.rl_summary_op = tf.summary.merge([tf.summary.scalar('model/policy_loss', self.pi_loss / size), tf.summary.scalar('model/value_loss', self.vf_loss / size), tf.summary.scalar('model/topk_loss', self.topk_loss / offp_size), tf.summary.scalar('model/entropy', self.entropy / size), tf.summary.scalar('model/loss', self.loss / size), tf.summary.scalar('model/grad_norm', tf.global_norm(self.grads)), tf.summary.scalar('model/unclipped_grad_norm', self.global_grad_norm), tf.summary.scalar('model/non_embedding_var_norm', tf.global_norm(self.non_embedding_params)), tf.summary.scalar('hparams/entropy_beta', self.entropy_hparam), tf.summary.scalar('hparams/topk_loss_hparam', self.topk_loss_hparam), tf.summary.scalar('hparams/learning_rate', self.learning_rate), tf.summary.scalar('model/trainable_var_norm', tf.global_norm(self.trainable_variables)), tf.summary.scalar('loss/loss', self.loss), tf.summary.scalar('loss/entropy', self.entropy_loss_term), tf.summary.scalar('loss/vf', self.vf_loss_term), tf.summary.scalar('loss/policy', self.pi_loss_term), tf.summary.scalar('loss/offp', self.topk_loss_term)] + [tf.summary.scalar('param_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(p)) for p in self.params] + [tf.summary.scalar('grad_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(g)) for (p, g) in zip(self.params, self.grads)] + [tf.summary.scalar('unclipped_grad_norms/' + _remove_prefix(scope_prefix + '/', p.name), tf.norm(g)) for (p, g) in zip(self.params, self.dense_unclipped_grads)])\n    self.text_summary_placeholder = tf.placeholder(tf.string, shape=[])\n    self.rl_text_summary_op = tf.summary.text('rl', self.text_summary_placeholder)"
        ]
    },
    {
        "func_name": "_rl_text_summary",
        "original": "def _rl_text_summary(self, session, step, npe, tot_r, num_steps, input_case, code_output, code, reason):\n    \"\"\"Logs summary about a single episode and creates a text_summary for TB.\n\n    Args:\n      session: tf.Session instance.\n      step: Global training step.\n      npe: Number of programs executed so far.\n      tot_r: Total reward.\n      num_steps: Number of timesteps in the episode (i.e. code length).\n      input_case: Inputs for test cases.\n      code_output: Outputs produced by running the code on the inputs.\n      code: String representation of the code.\n      reason: Reason for the reward assigned by the task.\n\n    Returns:\n      Serialized text summary data for tensorboard.\n    \"\"\"\n    if not input_case:\n        input_case = ' '\n    if not code_output:\n        code_output = ' '\n    if not code:\n        code = ' '\n    text = 'Tot R: **%.2f**;  Len: **%d**;  Reason: **%s**\\n\\nInput: **`%s`**; Output: **`%s`**\\n\\nCode: **`%s`**' % (tot_r, num_steps, reason, input_case, code_output, code)\n    text_summary = session.run(self.rl_text_summary_op, {self.text_summary_placeholder: text})\n    logging.info('Step %d.\\t NPE: %d\\t Reason: %s.\\t Tot R: %.2f.\\t Length: %d. \\tInput: %s \\tOutput: %s \\tProgram: %s', step, npe, reason, tot_r, num_steps, input_case, code_output, code)\n    return text_summary",
        "mutated": [
            "def _rl_text_summary(self, session, step, npe, tot_r, num_steps, input_case, code_output, code, reason):\n    if False:\n        i = 10\n    'Logs summary about a single episode and creates a text_summary for TB.\\n\\n    Args:\\n      session: tf.Session instance.\\n      step: Global training step.\\n      npe: Number of programs executed so far.\\n      tot_r: Total reward.\\n      num_steps: Number of timesteps in the episode (i.e. code length).\\n      input_case: Inputs for test cases.\\n      code_output: Outputs produced by running the code on the inputs.\\n      code: String representation of the code.\\n      reason: Reason for the reward assigned by the task.\\n\\n    Returns:\\n      Serialized text summary data for tensorboard.\\n    '\n    if not input_case:\n        input_case = ' '\n    if not code_output:\n        code_output = ' '\n    if not code:\n        code = ' '\n    text = 'Tot R: **%.2f**;  Len: **%d**;  Reason: **%s**\\n\\nInput: **`%s`**; Output: **`%s`**\\n\\nCode: **`%s`**' % (tot_r, num_steps, reason, input_case, code_output, code)\n    text_summary = session.run(self.rl_text_summary_op, {self.text_summary_placeholder: text})\n    logging.info('Step %d.\\t NPE: %d\\t Reason: %s.\\t Tot R: %.2f.\\t Length: %d. \\tInput: %s \\tOutput: %s \\tProgram: %s', step, npe, reason, tot_r, num_steps, input_case, code_output, code)\n    return text_summary",
            "def _rl_text_summary(self, session, step, npe, tot_r, num_steps, input_case, code_output, code, reason):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Logs summary about a single episode and creates a text_summary for TB.\\n\\n    Args:\\n      session: tf.Session instance.\\n      step: Global training step.\\n      npe: Number of programs executed so far.\\n      tot_r: Total reward.\\n      num_steps: Number of timesteps in the episode (i.e. code length).\\n      input_case: Inputs for test cases.\\n      code_output: Outputs produced by running the code on the inputs.\\n      code: String representation of the code.\\n      reason: Reason for the reward assigned by the task.\\n\\n    Returns:\\n      Serialized text summary data for tensorboard.\\n    '\n    if not input_case:\n        input_case = ' '\n    if not code_output:\n        code_output = ' '\n    if not code:\n        code = ' '\n    text = 'Tot R: **%.2f**;  Len: **%d**;  Reason: **%s**\\n\\nInput: **`%s`**; Output: **`%s`**\\n\\nCode: **`%s`**' % (tot_r, num_steps, reason, input_case, code_output, code)\n    text_summary = session.run(self.rl_text_summary_op, {self.text_summary_placeholder: text})\n    logging.info('Step %d.\\t NPE: %d\\t Reason: %s.\\t Tot R: %.2f.\\t Length: %d. \\tInput: %s \\tOutput: %s \\tProgram: %s', step, npe, reason, tot_r, num_steps, input_case, code_output, code)\n    return text_summary",
            "def _rl_text_summary(self, session, step, npe, tot_r, num_steps, input_case, code_output, code, reason):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Logs summary about a single episode and creates a text_summary for TB.\\n\\n    Args:\\n      session: tf.Session instance.\\n      step: Global training step.\\n      npe: Number of programs executed so far.\\n      tot_r: Total reward.\\n      num_steps: Number of timesteps in the episode (i.e. code length).\\n      input_case: Inputs for test cases.\\n      code_output: Outputs produced by running the code on the inputs.\\n      code: String representation of the code.\\n      reason: Reason for the reward assigned by the task.\\n\\n    Returns:\\n      Serialized text summary data for tensorboard.\\n    '\n    if not input_case:\n        input_case = ' '\n    if not code_output:\n        code_output = ' '\n    if not code:\n        code = ' '\n    text = 'Tot R: **%.2f**;  Len: **%d**;  Reason: **%s**\\n\\nInput: **`%s`**; Output: **`%s`**\\n\\nCode: **`%s`**' % (tot_r, num_steps, reason, input_case, code_output, code)\n    text_summary = session.run(self.rl_text_summary_op, {self.text_summary_placeholder: text})\n    logging.info('Step %d.\\t NPE: %d\\t Reason: %s.\\t Tot R: %.2f.\\t Length: %d. \\tInput: %s \\tOutput: %s \\tProgram: %s', step, npe, reason, tot_r, num_steps, input_case, code_output, code)\n    return text_summary",
            "def _rl_text_summary(self, session, step, npe, tot_r, num_steps, input_case, code_output, code, reason):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Logs summary about a single episode and creates a text_summary for TB.\\n\\n    Args:\\n      session: tf.Session instance.\\n      step: Global training step.\\n      npe: Number of programs executed so far.\\n      tot_r: Total reward.\\n      num_steps: Number of timesteps in the episode (i.e. code length).\\n      input_case: Inputs for test cases.\\n      code_output: Outputs produced by running the code on the inputs.\\n      code: String representation of the code.\\n      reason: Reason for the reward assigned by the task.\\n\\n    Returns:\\n      Serialized text summary data for tensorboard.\\n    '\n    if not input_case:\n        input_case = ' '\n    if not code_output:\n        code_output = ' '\n    if not code:\n        code = ' '\n    text = 'Tot R: **%.2f**;  Len: **%d**;  Reason: **%s**\\n\\nInput: **`%s`**; Output: **`%s`**\\n\\nCode: **`%s`**' % (tot_r, num_steps, reason, input_case, code_output, code)\n    text_summary = session.run(self.rl_text_summary_op, {self.text_summary_placeholder: text})\n    logging.info('Step %d.\\t NPE: %d\\t Reason: %s.\\t Tot R: %.2f.\\t Length: %d. \\tInput: %s \\tOutput: %s \\tProgram: %s', step, npe, reason, tot_r, num_steps, input_case, code_output, code)\n    return text_summary",
            "def _rl_text_summary(self, session, step, npe, tot_r, num_steps, input_case, code_output, code, reason):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Logs summary about a single episode and creates a text_summary for TB.\\n\\n    Args:\\n      session: tf.Session instance.\\n      step: Global training step.\\n      npe: Number of programs executed so far.\\n      tot_r: Total reward.\\n      num_steps: Number of timesteps in the episode (i.e. code length).\\n      input_case: Inputs for test cases.\\n      code_output: Outputs produced by running the code on the inputs.\\n      code: String representation of the code.\\n      reason: Reason for the reward assigned by the task.\\n\\n    Returns:\\n      Serialized text summary data for tensorboard.\\n    '\n    if not input_case:\n        input_case = ' '\n    if not code_output:\n        code_output = ' '\n    if not code:\n        code = ' '\n    text = 'Tot R: **%.2f**;  Len: **%d**;  Reason: **%s**\\n\\nInput: **`%s`**; Output: **`%s`**\\n\\nCode: **`%s`**' % (tot_r, num_steps, reason, input_case, code_output, code)\n    text_summary = session.run(self.rl_text_summary_op, {self.text_summary_placeholder: text})\n    logging.info('Step %d.\\t NPE: %d\\t Reason: %s.\\t Tot R: %.2f.\\t Length: %d. \\tInput: %s \\tOutput: %s \\tProgram: %s', step, npe, reason, tot_r, num_steps, input_case, code_output, code)\n    return text_summary"
        ]
    },
    {
        "func_name": "_rl_reward_summary",
        "original": "def _rl_reward_summary(self, total_rewards):\n    \"\"\"Create summary ops that report on episode rewards.\n\n    Creates summaries for average, median, max, and min rewards in the batch.\n\n    Args:\n      total_rewards: Tensor of shape [batch_size] containing the total reward\n          from each episode in the batch.\n\n    Returns:\n      tf.Summary op.\n    \"\"\"\n    tr = np.asarray(total_rewards)\n    reward_summary = tf.Summary(value=[tf.Summary.Value(tag='reward/avg', simple_value=np.mean(tr)), tf.Summary.Value(tag='reward/med', simple_value=np.median(tr)), tf.Summary.Value(tag='reward/max', simple_value=np.max(tr)), tf.Summary.Value(tag='reward/min', simple_value=np.min(tr))])\n    return reward_summary",
        "mutated": [
            "def _rl_reward_summary(self, total_rewards):\n    if False:\n        i = 10\n    'Create summary ops that report on episode rewards.\\n\\n    Creates summaries for average, median, max, and min rewards in the batch.\\n\\n    Args:\\n      total_rewards: Tensor of shape [batch_size] containing the total reward\\n          from each episode in the batch.\\n\\n    Returns:\\n      tf.Summary op.\\n    '\n    tr = np.asarray(total_rewards)\n    reward_summary = tf.Summary(value=[tf.Summary.Value(tag='reward/avg', simple_value=np.mean(tr)), tf.Summary.Value(tag='reward/med', simple_value=np.median(tr)), tf.Summary.Value(tag='reward/max', simple_value=np.max(tr)), tf.Summary.Value(tag='reward/min', simple_value=np.min(tr))])\n    return reward_summary",
            "def _rl_reward_summary(self, total_rewards):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Create summary ops that report on episode rewards.\\n\\n    Creates summaries for average, median, max, and min rewards in the batch.\\n\\n    Args:\\n      total_rewards: Tensor of shape [batch_size] containing the total reward\\n          from each episode in the batch.\\n\\n    Returns:\\n      tf.Summary op.\\n    '\n    tr = np.asarray(total_rewards)\n    reward_summary = tf.Summary(value=[tf.Summary.Value(tag='reward/avg', simple_value=np.mean(tr)), tf.Summary.Value(tag='reward/med', simple_value=np.median(tr)), tf.Summary.Value(tag='reward/max', simple_value=np.max(tr)), tf.Summary.Value(tag='reward/min', simple_value=np.min(tr))])\n    return reward_summary",
            "def _rl_reward_summary(self, total_rewards):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Create summary ops that report on episode rewards.\\n\\n    Creates summaries for average, median, max, and min rewards in the batch.\\n\\n    Args:\\n      total_rewards: Tensor of shape [batch_size] containing the total reward\\n          from each episode in the batch.\\n\\n    Returns:\\n      tf.Summary op.\\n    '\n    tr = np.asarray(total_rewards)\n    reward_summary = tf.Summary(value=[tf.Summary.Value(tag='reward/avg', simple_value=np.mean(tr)), tf.Summary.Value(tag='reward/med', simple_value=np.median(tr)), tf.Summary.Value(tag='reward/max', simple_value=np.max(tr)), tf.Summary.Value(tag='reward/min', simple_value=np.min(tr))])\n    return reward_summary",
            "def _rl_reward_summary(self, total_rewards):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Create summary ops that report on episode rewards.\\n\\n    Creates summaries for average, median, max, and min rewards in the batch.\\n\\n    Args:\\n      total_rewards: Tensor of shape [batch_size] containing the total reward\\n          from each episode in the batch.\\n\\n    Returns:\\n      tf.Summary op.\\n    '\n    tr = np.asarray(total_rewards)\n    reward_summary = tf.Summary(value=[tf.Summary.Value(tag='reward/avg', simple_value=np.mean(tr)), tf.Summary.Value(tag='reward/med', simple_value=np.median(tr)), tf.Summary.Value(tag='reward/max', simple_value=np.max(tr)), tf.Summary.Value(tag='reward/min', simple_value=np.min(tr))])\n    return reward_summary",
            "def _rl_reward_summary(self, total_rewards):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Create summary ops that report on episode rewards.\\n\\n    Creates summaries for average, median, max, and min rewards in the batch.\\n\\n    Args:\\n      total_rewards: Tensor of shape [batch_size] containing the total reward\\n          from each episode in the batch.\\n\\n    Returns:\\n      tf.Summary op.\\n    '\n    tr = np.asarray(total_rewards)\n    reward_summary = tf.Summary(value=[tf.Summary.Value(tag='reward/avg', simple_value=np.mean(tr)), tf.Summary.Value(tag='reward/med', simple_value=np.median(tr)), tf.Summary.Value(tag='reward/max', simple_value=np.max(tr)), tf.Summary.Value(tag='reward/min', simple_value=np.min(tr))])\n    return reward_summary"
        ]
    },
    {
        "func_name": "_iw_summary",
        "original": "def _iw_summary(self, session, replay_iw, replay_log_probs, norm_replay_weights, on_policy_iw, on_policy_log_probs):\n    \"\"\"Compute summaries for importance weights at a given batch.\n\n    Args:\n      session: tf.Session instance.\n      replay_iw: Importance weights for episodes from replay buffer.\n      replay_log_probs: Total log probabilities of the replay episodes under the\n          current policy.\n      norm_replay_weights: Normalized replay weights, i.e. values in `replay_iw`\n          divided by the total weight in the entire replay buffer. Note, this is\n          also the probability of selecting each episode from the replay buffer\n          (in a roulette wheel replay buffer).\n      on_policy_iw: Importance weights for episodes sampled from the current\n          policy.\n      on_policy_log_probs: Total log probabilities of the on-policy episodes\n          under the current policy.\n\n    Returns:\n      Serialized TF summaries. Use a summary writer to write these summaries to\n      disk.\n    \"\"\"\n    return session.run(self.iw_summary_op, {self.log_iw_replay_ph: np.log(replay_iw), self.log_iw_policy_ph: np.log(on_policy_iw), self.log_norm_replay_weights_ph: np.log(norm_replay_weights), self.log_prob_replay_ph: replay_log_probs, self.log_prob_policy_ph: on_policy_log_probs})",
        "mutated": [
            "def _iw_summary(self, session, replay_iw, replay_log_probs, norm_replay_weights, on_policy_iw, on_policy_log_probs):\n    if False:\n        i = 10\n    'Compute summaries for importance weights at a given batch.\\n\\n    Args:\\n      session: tf.Session instance.\\n      replay_iw: Importance weights for episodes from replay buffer.\\n      replay_log_probs: Total log probabilities of the replay episodes under the\\n          current policy.\\n      norm_replay_weights: Normalized replay weights, i.e. values in `replay_iw`\\n          divided by the total weight in the entire replay buffer. Note, this is\\n          also the probability of selecting each episode from the replay buffer\\n          (in a roulette wheel replay buffer).\\n      on_policy_iw: Importance weights for episodes sampled from the current\\n          policy.\\n      on_policy_log_probs: Total log probabilities of the on-policy episodes\\n          under the current policy.\\n\\n    Returns:\\n      Serialized TF summaries. Use a summary writer to write these summaries to\\n      disk.\\n    '\n    return session.run(self.iw_summary_op, {self.log_iw_replay_ph: np.log(replay_iw), self.log_iw_policy_ph: np.log(on_policy_iw), self.log_norm_replay_weights_ph: np.log(norm_replay_weights), self.log_prob_replay_ph: replay_log_probs, self.log_prob_policy_ph: on_policy_log_probs})",
            "def _iw_summary(self, session, replay_iw, replay_log_probs, norm_replay_weights, on_policy_iw, on_policy_log_probs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute summaries for importance weights at a given batch.\\n\\n    Args:\\n      session: tf.Session instance.\\n      replay_iw: Importance weights for episodes from replay buffer.\\n      replay_log_probs: Total log probabilities of the replay episodes under the\\n          current policy.\\n      norm_replay_weights: Normalized replay weights, i.e. values in `replay_iw`\\n          divided by the total weight in the entire replay buffer. Note, this is\\n          also the probability of selecting each episode from the replay buffer\\n          (in a roulette wheel replay buffer).\\n      on_policy_iw: Importance weights for episodes sampled from the current\\n          policy.\\n      on_policy_log_probs: Total log probabilities of the on-policy episodes\\n          under the current policy.\\n\\n    Returns:\\n      Serialized TF summaries. Use a summary writer to write these summaries to\\n      disk.\\n    '\n    return session.run(self.iw_summary_op, {self.log_iw_replay_ph: np.log(replay_iw), self.log_iw_policy_ph: np.log(on_policy_iw), self.log_norm_replay_weights_ph: np.log(norm_replay_weights), self.log_prob_replay_ph: replay_log_probs, self.log_prob_policy_ph: on_policy_log_probs})",
            "def _iw_summary(self, session, replay_iw, replay_log_probs, norm_replay_weights, on_policy_iw, on_policy_log_probs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute summaries for importance weights at a given batch.\\n\\n    Args:\\n      session: tf.Session instance.\\n      replay_iw: Importance weights for episodes from replay buffer.\\n      replay_log_probs: Total log probabilities of the replay episodes under the\\n          current policy.\\n      norm_replay_weights: Normalized replay weights, i.e. values in `replay_iw`\\n          divided by the total weight in the entire replay buffer. Note, this is\\n          also the probability of selecting each episode from the replay buffer\\n          (in a roulette wheel replay buffer).\\n      on_policy_iw: Importance weights for episodes sampled from the current\\n          policy.\\n      on_policy_log_probs: Total log probabilities of the on-policy episodes\\n          under the current policy.\\n\\n    Returns:\\n      Serialized TF summaries. Use a summary writer to write these summaries to\\n      disk.\\n    '\n    return session.run(self.iw_summary_op, {self.log_iw_replay_ph: np.log(replay_iw), self.log_iw_policy_ph: np.log(on_policy_iw), self.log_norm_replay_weights_ph: np.log(norm_replay_weights), self.log_prob_replay_ph: replay_log_probs, self.log_prob_policy_ph: on_policy_log_probs})",
            "def _iw_summary(self, session, replay_iw, replay_log_probs, norm_replay_weights, on_policy_iw, on_policy_log_probs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute summaries for importance weights at a given batch.\\n\\n    Args:\\n      session: tf.Session instance.\\n      replay_iw: Importance weights for episodes from replay buffer.\\n      replay_log_probs: Total log probabilities of the replay episodes under the\\n          current policy.\\n      norm_replay_weights: Normalized replay weights, i.e. values in `replay_iw`\\n          divided by the total weight in the entire replay buffer. Note, this is\\n          also the probability of selecting each episode from the replay buffer\\n          (in a roulette wheel replay buffer).\\n      on_policy_iw: Importance weights for episodes sampled from the current\\n          policy.\\n      on_policy_log_probs: Total log probabilities of the on-policy episodes\\n          under the current policy.\\n\\n    Returns:\\n      Serialized TF summaries. Use a summary writer to write these summaries to\\n      disk.\\n    '\n    return session.run(self.iw_summary_op, {self.log_iw_replay_ph: np.log(replay_iw), self.log_iw_policy_ph: np.log(on_policy_iw), self.log_norm_replay_weights_ph: np.log(norm_replay_weights), self.log_prob_replay_ph: replay_log_probs, self.log_prob_policy_ph: on_policy_log_probs})",
            "def _iw_summary(self, session, replay_iw, replay_log_probs, norm_replay_weights, on_policy_iw, on_policy_log_probs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute summaries for importance weights at a given batch.\\n\\n    Args:\\n      session: tf.Session instance.\\n      replay_iw: Importance weights for episodes from replay buffer.\\n      replay_log_probs: Total log probabilities of the replay episodes under the\\n          current policy.\\n      norm_replay_weights: Normalized replay weights, i.e. values in `replay_iw`\\n          divided by the total weight in the entire replay buffer. Note, this is\\n          also the probability of selecting each episode from the replay buffer\\n          (in a roulette wheel replay buffer).\\n      on_policy_iw: Importance weights for episodes sampled from the current\\n          policy.\\n      on_policy_log_probs: Total log probabilities of the on-policy episodes\\n          under the current policy.\\n\\n    Returns:\\n      Serialized TF summaries. Use a summary writer to write these summaries to\\n      disk.\\n    '\n    return session.run(self.iw_summary_op, {self.log_iw_replay_ph: np.log(replay_iw), self.log_iw_policy_ph: np.log(on_policy_iw), self.log_norm_replay_weights_ph: np.log(norm_replay_weights), self.log_prob_replay_ph: replay_log_probs, self.log_prob_policy_ph: on_policy_log_probs})"
        ]
    },
    {
        "func_name": "_compute_iw",
        "original": "def _compute_iw(self, policy_log_probs, replay_weights):\n    \"\"\"Compute importance weights for a batch of episodes.\n\n    Arguments are iterables of length batch_size.\n\n    Args:\n      policy_log_probs: Log probability of each episode under the current\n          policy.\n      replay_weights: Weight of each episode in the replay buffer. 0 for\n          episodes not sampled from the replay buffer (i.e. sampled from the\n          policy).\n\n    Returns:\n      Numpy array of shape [batch_size] containing the importance weight for\n      each episode in the batch.\n    \"\"\"\n    log_total_replay_weight = log(self.experience_replay.total_weight)\n    a = float(self.replay_alpha)\n    a_com = 1.0 - a\n    importance_weights = np.asarray([1.0 / (a_com + a * exp(log(replay_weight) - log_total_replay_weight - log_p)) if replay_weight > 0 else 1.0 / a_com for (log_p, replay_weight) in zip(policy_log_probs, replay_weights)])\n    return importance_weights",
        "mutated": [
            "def _compute_iw(self, policy_log_probs, replay_weights):\n    if False:\n        i = 10\n    'Compute importance weights for a batch of episodes.\\n\\n    Arguments are iterables of length batch_size.\\n\\n    Args:\\n      policy_log_probs: Log probability of each episode under the current\\n          policy.\\n      replay_weights: Weight of each episode in the replay buffer. 0 for\\n          episodes not sampled from the replay buffer (i.e. sampled from the\\n          policy).\\n\\n    Returns:\\n      Numpy array of shape [batch_size] containing the importance weight for\\n      each episode in the batch.\\n    '\n    log_total_replay_weight = log(self.experience_replay.total_weight)\n    a = float(self.replay_alpha)\n    a_com = 1.0 - a\n    importance_weights = np.asarray([1.0 / (a_com + a * exp(log(replay_weight) - log_total_replay_weight - log_p)) if replay_weight > 0 else 1.0 / a_com for (log_p, replay_weight) in zip(policy_log_probs, replay_weights)])\n    return importance_weights",
            "def _compute_iw(self, policy_log_probs, replay_weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute importance weights for a batch of episodes.\\n\\n    Arguments are iterables of length batch_size.\\n\\n    Args:\\n      policy_log_probs: Log probability of each episode under the current\\n          policy.\\n      replay_weights: Weight of each episode in the replay buffer. 0 for\\n          episodes not sampled from the replay buffer (i.e. sampled from the\\n          policy).\\n\\n    Returns:\\n      Numpy array of shape [batch_size] containing the importance weight for\\n      each episode in the batch.\\n    '\n    log_total_replay_weight = log(self.experience_replay.total_weight)\n    a = float(self.replay_alpha)\n    a_com = 1.0 - a\n    importance_weights = np.asarray([1.0 / (a_com + a * exp(log(replay_weight) - log_total_replay_weight - log_p)) if replay_weight > 0 else 1.0 / a_com for (log_p, replay_weight) in zip(policy_log_probs, replay_weights)])\n    return importance_weights",
            "def _compute_iw(self, policy_log_probs, replay_weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute importance weights for a batch of episodes.\\n\\n    Arguments are iterables of length batch_size.\\n\\n    Args:\\n      policy_log_probs: Log probability of each episode under the current\\n          policy.\\n      replay_weights: Weight of each episode in the replay buffer. 0 for\\n          episodes not sampled from the replay buffer (i.e. sampled from the\\n          policy).\\n\\n    Returns:\\n      Numpy array of shape [batch_size] containing the importance weight for\\n      each episode in the batch.\\n    '\n    log_total_replay_weight = log(self.experience_replay.total_weight)\n    a = float(self.replay_alpha)\n    a_com = 1.0 - a\n    importance_weights = np.asarray([1.0 / (a_com + a * exp(log(replay_weight) - log_total_replay_weight - log_p)) if replay_weight > 0 else 1.0 / a_com for (log_p, replay_weight) in zip(policy_log_probs, replay_weights)])\n    return importance_weights",
            "def _compute_iw(self, policy_log_probs, replay_weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute importance weights for a batch of episodes.\\n\\n    Arguments are iterables of length batch_size.\\n\\n    Args:\\n      policy_log_probs: Log probability of each episode under the current\\n          policy.\\n      replay_weights: Weight of each episode in the replay buffer. 0 for\\n          episodes not sampled from the replay buffer (i.e. sampled from the\\n          policy).\\n\\n    Returns:\\n      Numpy array of shape [batch_size] containing the importance weight for\\n      each episode in the batch.\\n    '\n    log_total_replay_weight = log(self.experience_replay.total_weight)\n    a = float(self.replay_alpha)\n    a_com = 1.0 - a\n    importance_weights = np.asarray([1.0 / (a_com + a * exp(log(replay_weight) - log_total_replay_weight - log_p)) if replay_weight > 0 else 1.0 / a_com for (log_p, replay_weight) in zip(policy_log_probs, replay_weights)])\n    return importance_weights",
            "def _compute_iw(self, policy_log_probs, replay_weights):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute importance weights for a batch of episodes.\\n\\n    Arguments are iterables of length batch_size.\\n\\n    Args:\\n      policy_log_probs: Log probability of each episode under the current\\n          policy.\\n      replay_weights: Weight of each episode in the replay buffer. 0 for\\n          episodes not sampled from the replay buffer (i.e. sampled from the\\n          policy).\\n\\n    Returns:\\n      Numpy array of shape [batch_size] containing the importance weight for\\n      each episode in the batch.\\n    '\n    log_total_replay_weight = log(self.experience_replay.total_weight)\n    a = float(self.replay_alpha)\n    a_com = 1.0 - a\n    importance_weights = np.asarray([1.0 / (a_com + a * exp(log(replay_weight) - log_total_replay_weight - log_p)) if replay_weight > 0 else 1.0 / a_com for (log_p, replay_weight) in zip(policy_log_probs, replay_weights)])\n    return importance_weights"
        ]
    },
    {
        "func_name": "update_step",
        "original": "def update_step(self, session, rl_batch, train_op, global_step_op, return_gradients=False):\n    \"\"\"Perform gradient update on the model.\n\n    Args:\n      session: tf.Session instance.\n      rl_batch: RLBatch instance from data.py. Use DataManager to create a\n          RLBatch for each call to update_step. RLBatch contains a batch of\n          tasks.\n      train_op: A TF op which will perform the gradient update. LMAgent does not\n          own its training op, so that trainers can do distributed training\n          and construct a specialized training op.\n      global_step_op: A TF op which will return the current global step when\n          run (should not increment it).\n      return_gradients: If True, the gradients will be saved and returned from\n          this method call. This is useful for testing.\n\n    Returns:\n      Results from the update step in a UpdateStepResult namedtuple, including\n      global step, global NPE, serialized summaries, and optionally gradients.\n    \"\"\"\n    assert self.is_local\n    if self.experience_replay is None:\n        num_programs_from_policy = rl_batch.batch_size\n        (batch_actions, batch_values, episode_lengths) = session.run([self.sampled_batch.tokens, self.sampled_batch.value, self.sampled_batch.episode_lengths])\n        if episode_lengths.size == 0:\n            logging.warn('Shapes:\\nbatch_actions.shape: %s\\nbatch_values.shape: %s\\nepisode_lengths.shape: %s\\n', batch_actions.shape, batch_values.shape, episode_lengths.shape)\n        code_scores = compute_rewards(rl_batch, batch_actions, episode_lengths)\n        code_strings = code_scores.code_strings\n        batch_tot_r = code_scores.total_rewards\n        test_cases = code_scores.test_cases\n        code_outputs = code_scores.code_outputs\n        reasons = code_scores.reasons\n        (batch_targets, batch_returns) = process_episodes(code_scores.batch_rewards, episode_lengths, a2c=self.a2c, baselines=self.ema_by_len, batch_values=batch_values)\n        batch_policy_multipliers = batch_targets\n        batch_emp_values = batch_returns if self.a2c else [[]]\n        adjusted_lengths = episode_lengths\n        if self.top_episodes:\n            assert len(self.top_episodes) > 0\n            off_policy_targets = [item for (item, _) in self.top_episodes.random_sample(self.topk_batch_size)]\n            off_policy_target_lengths = [len(t) for t in off_policy_targets]\n            off_policy_targets = utils.stack_pad(off_policy_targets, pad_axes=0, dtype=np.int32)\n            offp_switch = 1\n        else:\n            off_policy_targets = [[0]]\n            off_policy_target_lengths = [1]\n            offp_switch = 0\n        fetches = {'global_step': global_step_op, 'program_count': self.program_count, 'summaries': self.rl_summary_op, 'train_op': train_op, 'gradients': self.gradients_dict if return_gradients else self.no_op}\n        fetched = session.run(fetches, {self.actions: batch_actions, self.empirical_values: batch_emp_values, self.policy_multipliers: batch_policy_multipliers, self.adjusted_lengths: adjusted_lengths, self.off_policy_targets: off_policy_targets, self.off_policy_target_lengths: off_policy_target_lengths, self.offp_switch: offp_switch})\n        combined_adjusted_lengths = adjusted_lengths\n        combined_returns = batch_returns\n    else:\n        (batch_actions, batch_values, episode_lengths, log_probs) = session.run([self.sampled_batch.tokens, self.sampled_batch.value, self.sampled_batch.episode_lengths, self.sampled_batch.log_probs])\n        if episode_lengths.size == 0:\n            logging.warn('Shapes:\\nbatch_actions.shape: %s\\nbatch_values.shape: %s\\nepisode_lengths.shape: %s\\n', batch_actions.shape, batch_values.shape, episode_lengths.shape)\n        empty_replay_buffer = self.experience_replay.is_empty() if self.experience_replay is not None else True\n        num_programs_from_replay_buff = self.num_replay_per_batch if not empty_replay_buffer else 0\n        num_programs_from_policy = rl_batch.batch_size - num_programs_from_replay_buff\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            result = self.experience_replay.sample_many(num_programs_from_replay_buff)\n            (experience_samples, replay_weights) = zip(*result)\n            (replay_actions, replay_rewards, _, replay_adjusted_lengths) = zip(*experience_samples)\n            replay_batch_actions = utils.stack_pad(replay_actions, pad_axes=0, dtype=np.int32)\n            (all_replay_log_probs,) = session.run([self.given_batch.log_probs], {self.actions: replay_batch_actions, self.adjusted_lengths: replay_adjusted_lengths})\n            replay_log_probs = [np.choose(replay_actions[i], all_replay_log_probs[i, :l].T).sum() for (i, l) in enumerate(replay_adjusted_lengths)]\n        else:\n            replay_actions = None\n            replay_policy_multipliers = None\n            replay_adjusted_lengths = None\n            replay_log_probs = None\n            replay_weights = None\n            replay_returns = None\n            on_policy_weights = [0] * num_programs_from_replay_buff\n        assert not self.a2c\n        code_scores = compute_rewards(rl_batch, batch_actions, episode_lengths, batch_size=num_programs_from_policy)\n        code_strings = code_scores.code_strings\n        batch_tot_r = code_scores.total_rewards\n        test_cases = code_scores.test_cases\n        code_outputs = code_scores.code_outputs\n        reasons = code_scores.reasons\n        p = num_programs_from_policy\n        (batch_targets, batch_returns) = process_episodes(code_scores.batch_rewards, episode_lengths[:p], a2c=False, baselines=self.ema_by_len)\n        batch_policy_multipliers = batch_targets\n        batch_emp_values = [[]]\n        on_policy_returns = batch_returns\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            offp_batch_rewards = [[0.0] * (l - 1) + [r] for (l, r) in zip(replay_adjusted_lengths, replay_rewards)]\n            assert len(offp_batch_rewards) == num_programs_from_replay_buff\n            assert len(replay_adjusted_lengths) == num_programs_from_replay_buff\n            (replay_batch_targets, replay_returns) = process_episodes(offp_batch_rewards, replay_adjusted_lengths, a2c=False, baselines=self.ema_by_len)\n            replay_policy_multipliers = [replay_batch_targets[i, :l] for (i, l) in enumerate(replay_adjusted_lengths[:num_programs_from_replay_buff])]\n        adjusted_lengths = episode_lengths[:num_programs_from_policy]\n        if self.top_episodes:\n            assert len(self.top_episodes) > 0\n            off_policy_targets = [item for (item, _) in self.top_episodes.random_sample(self.topk_batch_size)]\n            off_policy_target_lengths = [len(t) for t in off_policy_targets]\n            off_policy_targets = utils.stack_pad(off_policy_targets, pad_axes=0, dtype=np.int32)\n            offp_switch = 1\n        else:\n            off_policy_targets = [[0]]\n            off_policy_target_lengths = [1]\n            offp_switch = 0\n        if num_programs_from_policy:\n            separate_actions = [batch_actions[i, :l] for (i, l) in enumerate(adjusted_lengths)]\n            chosen_log_probs = [np.choose(separate_actions[i], log_probs[i, :l].T) for (i, l) in enumerate(adjusted_lengths)]\n            new_experiences = [(separate_actions[i], batch_tot_r[i], chosen_log_probs[i].sum(), l) for (i, l) in enumerate(adjusted_lengths)]\n            on_policy_policy_multipliers = [batch_policy_multipliers[i, :l] for (i, l) in enumerate(adjusted_lengths)]\n            (on_policy_actions, _, on_policy_log_probs, on_policy_adjusted_lengths) = zip(*new_experiences)\n        else:\n            new_experiences = []\n            on_policy_policy_multipliers = []\n            on_policy_actions = []\n            on_policy_log_probs = []\n            on_policy_adjusted_lengths = []\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            on_policy_weights = [0] * num_programs_from_policy\n            for (i, cs) in enumerate(code_strings):\n                if self.experience_replay.has_key(cs):\n                    on_policy_weights[i] = self.experience_replay.get_weight(cs)\n        combined_actions = join(replay_actions, on_policy_actions)\n        combined_policy_multipliers = join(replay_policy_multipliers, on_policy_policy_multipliers)\n        combined_adjusted_lengths = join(replay_adjusted_lengths, on_policy_adjusted_lengths)\n        combined_returns = join(replay_returns, on_policy_returns)\n        combined_actions = utils.stack_pad(combined_actions, pad_axes=0)\n        combined_policy_multipliers = utils.stack_pad(combined_policy_multipliers, pad_axes=0)\n        combined_on_policy_log_probs = join(replay_log_probs, on_policy_log_probs)\n        combined_q_weights = join(replay_weights, on_policy_weights)\n        if empty_replay_buffer:\n            combined_policy_multipliers *= 0\n        elif not num_programs_from_replay_buff:\n            combined_policy_multipliers = np.ones([len(combined_actions), 1], dtype=np.float32)\n        else:\n            importance_weights = self._compute_iw(combined_on_policy_log_probs, combined_q_weights)\n            if self.config.iw_normalize:\n                importance_weights *= float(rl_batch.batch_size) / importance_weights.sum()\n            combined_policy_multipliers *= importance_weights.reshape(-1, 1)\n        assert self.program_count is not None\n        fetches = {'global_step': global_step_op, 'program_count': self.program_count, 'summaries': self.rl_summary_op, 'train_op': train_op, 'gradients': self.gradients_dict if return_gradients else self.no_op}\n        fetched = session.run(fetches, {self.actions: combined_actions, self.empirical_values: [[]], self.policy_multipliers: combined_policy_multipliers, self.adjusted_lengths: combined_adjusted_lengths, self.off_policy_targets: off_policy_targets, self.off_policy_target_lengths: off_policy_target_lengths, self.offp_switch: offp_switch})\n        self.experience_replay.add_many(objs=new_experiences, weights=[exp(r / self.replay_temperature) for r in batch_tot_r], keys=code_strings)\n    session.run([self.program_count_add_op], {self.program_count_add_ph: num_programs_from_policy})\n    if not self.a2c:\n        for i in xrange(rl_batch.batch_size):\n            episode_length = combined_adjusted_lengths[i]\n            empirical_returns = combined_returns[i, :episode_length]\n            for j in xrange(episode_length):\n                self.ema_by_len[j] = self.ema_baseline_decay * self.ema_by_len[j] + (1 - self.ema_baseline_decay) * empirical_returns[j]\n    global_step = fetched['global_step']\n    global_npe = fetched['program_count']\n    core_summaries = fetched['summaries']\n    summaries_list = [core_summaries]\n    if num_programs_from_policy:\n        s_i = 0\n        text_summary = self._rl_text_summary(session, global_step, global_npe, batch_tot_r[s_i], episode_lengths[s_i], test_cases[s_i], code_outputs[s_i], code_strings[s_i], reasons[s_i])\n        reward_summary = self._rl_reward_summary(batch_tot_r)\n        is_best = False\n        if self.global_best_reward_fn:\n            best_reward = np.max(batch_tot_r)\n            is_best = self.global_best_reward_fn(session, best_reward)\n        if self.found_solution_op is not None and 'correct' in reasons:\n            session.run(self.found_solution_op)\n            if self.stop_on_success:\n                solutions = [{'code': code_strings[i], 'reward': batch_tot_r[i], 'npe': global_npe} for i in xrange(len(reasons)) if reasons[i] == 'correct']\n            elif is_best:\n                solutions = [{'code': code_strings[np.argmax(batch_tot_r)], 'reward': np.max(batch_tot_r), 'npe': global_npe}]\n            else:\n                solutions = []\n            if solutions:\n                if self.assign_code_solution_fn:\n                    self.assign_code_solution_fn(session, solutions[0]['code'])\n                with tf.gfile.FastGFile(self.logging_file, 'a') as writer:\n                    for solution_dict in solutions:\n                        writer.write(str(solution_dict) + '\\n')\n        max_i = np.argmax(batch_tot_r)\n        max_tot_r = batch_tot_r[max_i]\n        if max_tot_r >= self.top_reward:\n            if max_tot_r >= self.top_reward:\n                self.top_reward = max_tot_r\n            logging.info('Top code: r=%.2f, \\t%s', max_tot_r, code_strings[max_i])\n        if self.top_episodes is not None:\n            self.top_episodes.push(max_tot_r, tuple(batch_actions[max_i, :episode_lengths[max_i]]))\n        summaries_list += [text_summary, reward_summary]\n        if self.do_iw_summaries and (not empty_replay_buffer):\n            norm_replay_weights = [w / self.experience_replay.total_weight for w in replay_weights]\n            replay_iw = self._compute_iw(replay_log_probs, replay_weights)\n            on_policy_iw = self._compute_iw(on_policy_log_probs, on_policy_weights)\n            summaries_list.append(self._iw_summary(session, replay_iw, replay_log_probs, norm_replay_weights, on_policy_iw, on_policy_log_probs))\n    return UpdateStepResult(global_step=global_step, global_npe=global_npe, summaries_list=summaries_list, gradients_dict=fetched['gradients'])",
        "mutated": [
            "def update_step(self, session, rl_batch, train_op, global_step_op, return_gradients=False):\n    if False:\n        i = 10\n    'Perform gradient update on the model.\\n\\n    Args:\\n      session: tf.Session instance.\\n      rl_batch: RLBatch instance from data.py. Use DataManager to create a\\n          RLBatch for each call to update_step. RLBatch contains a batch of\\n          tasks.\\n      train_op: A TF op which will perform the gradient update. LMAgent does not\\n          own its training op, so that trainers can do distributed training\\n          and construct a specialized training op.\\n      global_step_op: A TF op which will return the current global step when\\n          run (should not increment it).\\n      return_gradients: If True, the gradients will be saved and returned from\\n          this method call. This is useful for testing.\\n\\n    Returns:\\n      Results from the update step in a UpdateStepResult namedtuple, including\\n      global step, global NPE, serialized summaries, and optionally gradients.\\n    '\n    assert self.is_local\n    if self.experience_replay is None:\n        num_programs_from_policy = rl_batch.batch_size\n        (batch_actions, batch_values, episode_lengths) = session.run([self.sampled_batch.tokens, self.sampled_batch.value, self.sampled_batch.episode_lengths])\n        if episode_lengths.size == 0:\n            logging.warn('Shapes:\\nbatch_actions.shape: %s\\nbatch_values.shape: %s\\nepisode_lengths.shape: %s\\n', batch_actions.shape, batch_values.shape, episode_lengths.shape)\n        code_scores = compute_rewards(rl_batch, batch_actions, episode_lengths)\n        code_strings = code_scores.code_strings\n        batch_tot_r = code_scores.total_rewards\n        test_cases = code_scores.test_cases\n        code_outputs = code_scores.code_outputs\n        reasons = code_scores.reasons\n        (batch_targets, batch_returns) = process_episodes(code_scores.batch_rewards, episode_lengths, a2c=self.a2c, baselines=self.ema_by_len, batch_values=batch_values)\n        batch_policy_multipliers = batch_targets\n        batch_emp_values = batch_returns if self.a2c else [[]]\n        adjusted_lengths = episode_lengths\n        if self.top_episodes:\n            assert len(self.top_episodes) > 0\n            off_policy_targets = [item for (item, _) in self.top_episodes.random_sample(self.topk_batch_size)]\n            off_policy_target_lengths = [len(t) for t in off_policy_targets]\n            off_policy_targets = utils.stack_pad(off_policy_targets, pad_axes=0, dtype=np.int32)\n            offp_switch = 1\n        else:\n            off_policy_targets = [[0]]\n            off_policy_target_lengths = [1]\n            offp_switch = 0\n        fetches = {'global_step': global_step_op, 'program_count': self.program_count, 'summaries': self.rl_summary_op, 'train_op': train_op, 'gradients': self.gradients_dict if return_gradients else self.no_op}\n        fetched = session.run(fetches, {self.actions: batch_actions, self.empirical_values: batch_emp_values, self.policy_multipliers: batch_policy_multipliers, self.adjusted_lengths: adjusted_lengths, self.off_policy_targets: off_policy_targets, self.off_policy_target_lengths: off_policy_target_lengths, self.offp_switch: offp_switch})\n        combined_adjusted_lengths = adjusted_lengths\n        combined_returns = batch_returns\n    else:\n        (batch_actions, batch_values, episode_lengths, log_probs) = session.run([self.sampled_batch.tokens, self.sampled_batch.value, self.sampled_batch.episode_lengths, self.sampled_batch.log_probs])\n        if episode_lengths.size == 0:\n            logging.warn('Shapes:\\nbatch_actions.shape: %s\\nbatch_values.shape: %s\\nepisode_lengths.shape: %s\\n', batch_actions.shape, batch_values.shape, episode_lengths.shape)\n        empty_replay_buffer = self.experience_replay.is_empty() if self.experience_replay is not None else True\n        num_programs_from_replay_buff = self.num_replay_per_batch if not empty_replay_buffer else 0\n        num_programs_from_policy = rl_batch.batch_size - num_programs_from_replay_buff\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            result = self.experience_replay.sample_many(num_programs_from_replay_buff)\n            (experience_samples, replay_weights) = zip(*result)\n            (replay_actions, replay_rewards, _, replay_adjusted_lengths) = zip(*experience_samples)\n            replay_batch_actions = utils.stack_pad(replay_actions, pad_axes=0, dtype=np.int32)\n            (all_replay_log_probs,) = session.run([self.given_batch.log_probs], {self.actions: replay_batch_actions, self.adjusted_lengths: replay_adjusted_lengths})\n            replay_log_probs = [np.choose(replay_actions[i], all_replay_log_probs[i, :l].T).sum() for (i, l) in enumerate(replay_adjusted_lengths)]\n        else:\n            replay_actions = None\n            replay_policy_multipliers = None\n            replay_adjusted_lengths = None\n            replay_log_probs = None\n            replay_weights = None\n            replay_returns = None\n            on_policy_weights = [0] * num_programs_from_replay_buff\n        assert not self.a2c\n        code_scores = compute_rewards(rl_batch, batch_actions, episode_lengths, batch_size=num_programs_from_policy)\n        code_strings = code_scores.code_strings\n        batch_tot_r = code_scores.total_rewards\n        test_cases = code_scores.test_cases\n        code_outputs = code_scores.code_outputs\n        reasons = code_scores.reasons\n        p = num_programs_from_policy\n        (batch_targets, batch_returns) = process_episodes(code_scores.batch_rewards, episode_lengths[:p], a2c=False, baselines=self.ema_by_len)\n        batch_policy_multipliers = batch_targets\n        batch_emp_values = [[]]\n        on_policy_returns = batch_returns\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            offp_batch_rewards = [[0.0] * (l - 1) + [r] for (l, r) in zip(replay_adjusted_lengths, replay_rewards)]\n            assert len(offp_batch_rewards) == num_programs_from_replay_buff\n            assert len(replay_adjusted_lengths) == num_programs_from_replay_buff\n            (replay_batch_targets, replay_returns) = process_episodes(offp_batch_rewards, replay_adjusted_lengths, a2c=False, baselines=self.ema_by_len)\n            replay_policy_multipliers = [replay_batch_targets[i, :l] for (i, l) in enumerate(replay_adjusted_lengths[:num_programs_from_replay_buff])]\n        adjusted_lengths = episode_lengths[:num_programs_from_policy]\n        if self.top_episodes:\n            assert len(self.top_episodes) > 0\n            off_policy_targets = [item for (item, _) in self.top_episodes.random_sample(self.topk_batch_size)]\n            off_policy_target_lengths = [len(t) for t in off_policy_targets]\n            off_policy_targets = utils.stack_pad(off_policy_targets, pad_axes=0, dtype=np.int32)\n            offp_switch = 1\n        else:\n            off_policy_targets = [[0]]\n            off_policy_target_lengths = [1]\n            offp_switch = 0\n        if num_programs_from_policy:\n            separate_actions = [batch_actions[i, :l] for (i, l) in enumerate(adjusted_lengths)]\n            chosen_log_probs = [np.choose(separate_actions[i], log_probs[i, :l].T) for (i, l) in enumerate(adjusted_lengths)]\n            new_experiences = [(separate_actions[i], batch_tot_r[i], chosen_log_probs[i].sum(), l) for (i, l) in enumerate(adjusted_lengths)]\n            on_policy_policy_multipliers = [batch_policy_multipliers[i, :l] for (i, l) in enumerate(adjusted_lengths)]\n            (on_policy_actions, _, on_policy_log_probs, on_policy_adjusted_lengths) = zip(*new_experiences)\n        else:\n            new_experiences = []\n            on_policy_policy_multipliers = []\n            on_policy_actions = []\n            on_policy_log_probs = []\n            on_policy_adjusted_lengths = []\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            on_policy_weights = [0] * num_programs_from_policy\n            for (i, cs) in enumerate(code_strings):\n                if self.experience_replay.has_key(cs):\n                    on_policy_weights[i] = self.experience_replay.get_weight(cs)\n        combined_actions = join(replay_actions, on_policy_actions)\n        combined_policy_multipliers = join(replay_policy_multipliers, on_policy_policy_multipliers)\n        combined_adjusted_lengths = join(replay_adjusted_lengths, on_policy_adjusted_lengths)\n        combined_returns = join(replay_returns, on_policy_returns)\n        combined_actions = utils.stack_pad(combined_actions, pad_axes=0)\n        combined_policy_multipliers = utils.stack_pad(combined_policy_multipliers, pad_axes=0)\n        combined_on_policy_log_probs = join(replay_log_probs, on_policy_log_probs)\n        combined_q_weights = join(replay_weights, on_policy_weights)\n        if empty_replay_buffer:\n            combined_policy_multipliers *= 0\n        elif not num_programs_from_replay_buff:\n            combined_policy_multipliers = np.ones([len(combined_actions), 1], dtype=np.float32)\n        else:\n            importance_weights = self._compute_iw(combined_on_policy_log_probs, combined_q_weights)\n            if self.config.iw_normalize:\n                importance_weights *= float(rl_batch.batch_size) / importance_weights.sum()\n            combined_policy_multipliers *= importance_weights.reshape(-1, 1)\n        assert self.program_count is not None\n        fetches = {'global_step': global_step_op, 'program_count': self.program_count, 'summaries': self.rl_summary_op, 'train_op': train_op, 'gradients': self.gradients_dict if return_gradients else self.no_op}\n        fetched = session.run(fetches, {self.actions: combined_actions, self.empirical_values: [[]], self.policy_multipliers: combined_policy_multipliers, self.adjusted_lengths: combined_adjusted_lengths, self.off_policy_targets: off_policy_targets, self.off_policy_target_lengths: off_policy_target_lengths, self.offp_switch: offp_switch})\n        self.experience_replay.add_many(objs=new_experiences, weights=[exp(r / self.replay_temperature) for r in batch_tot_r], keys=code_strings)\n    session.run([self.program_count_add_op], {self.program_count_add_ph: num_programs_from_policy})\n    if not self.a2c:\n        for i in xrange(rl_batch.batch_size):\n            episode_length = combined_adjusted_lengths[i]\n            empirical_returns = combined_returns[i, :episode_length]\n            for j in xrange(episode_length):\n                self.ema_by_len[j] = self.ema_baseline_decay * self.ema_by_len[j] + (1 - self.ema_baseline_decay) * empirical_returns[j]\n    global_step = fetched['global_step']\n    global_npe = fetched['program_count']\n    core_summaries = fetched['summaries']\n    summaries_list = [core_summaries]\n    if num_programs_from_policy:\n        s_i = 0\n        text_summary = self._rl_text_summary(session, global_step, global_npe, batch_tot_r[s_i], episode_lengths[s_i], test_cases[s_i], code_outputs[s_i], code_strings[s_i], reasons[s_i])\n        reward_summary = self._rl_reward_summary(batch_tot_r)\n        is_best = False\n        if self.global_best_reward_fn:\n            best_reward = np.max(batch_tot_r)\n            is_best = self.global_best_reward_fn(session, best_reward)\n        if self.found_solution_op is not None and 'correct' in reasons:\n            session.run(self.found_solution_op)\n            if self.stop_on_success:\n                solutions = [{'code': code_strings[i], 'reward': batch_tot_r[i], 'npe': global_npe} for i in xrange(len(reasons)) if reasons[i] == 'correct']\n            elif is_best:\n                solutions = [{'code': code_strings[np.argmax(batch_tot_r)], 'reward': np.max(batch_tot_r), 'npe': global_npe}]\n            else:\n                solutions = []\n            if solutions:\n                if self.assign_code_solution_fn:\n                    self.assign_code_solution_fn(session, solutions[0]['code'])\n                with tf.gfile.FastGFile(self.logging_file, 'a') as writer:\n                    for solution_dict in solutions:\n                        writer.write(str(solution_dict) + '\\n')\n        max_i = np.argmax(batch_tot_r)\n        max_tot_r = batch_tot_r[max_i]\n        if max_tot_r >= self.top_reward:\n            if max_tot_r >= self.top_reward:\n                self.top_reward = max_tot_r\n            logging.info('Top code: r=%.2f, \\t%s', max_tot_r, code_strings[max_i])\n        if self.top_episodes is not None:\n            self.top_episodes.push(max_tot_r, tuple(batch_actions[max_i, :episode_lengths[max_i]]))\n        summaries_list += [text_summary, reward_summary]\n        if self.do_iw_summaries and (not empty_replay_buffer):\n            norm_replay_weights = [w / self.experience_replay.total_weight for w in replay_weights]\n            replay_iw = self._compute_iw(replay_log_probs, replay_weights)\n            on_policy_iw = self._compute_iw(on_policy_log_probs, on_policy_weights)\n            summaries_list.append(self._iw_summary(session, replay_iw, replay_log_probs, norm_replay_weights, on_policy_iw, on_policy_log_probs))\n    return UpdateStepResult(global_step=global_step, global_npe=global_npe, summaries_list=summaries_list, gradients_dict=fetched['gradients'])",
            "def update_step(self, session, rl_batch, train_op, global_step_op, return_gradients=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Perform gradient update on the model.\\n\\n    Args:\\n      session: tf.Session instance.\\n      rl_batch: RLBatch instance from data.py. Use DataManager to create a\\n          RLBatch for each call to update_step. RLBatch contains a batch of\\n          tasks.\\n      train_op: A TF op which will perform the gradient update. LMAgent does not\\n          own its training op, so that trainers can do distributed training\\n          and construct a specialized training op.\\n      global_step_op: A TF op which will return the current global step when\\n          run (should not increment it).\\n      return_gradients: If True, the gradients will be saved and returned from\\n          this method call. This is useful for testing.\\n\\n    Returns:\\n      Results from the update step in a UpdateStepResult namedtuple, including\\n      global step, global NPE, serialized summaries, and optionally gradients.\\n    '\n    assert self.is_local\n    if self.experience_replay is None:\n        num_programs_from_policy = rl_batch.batch_size\n        (batch_actions, batch_values, episode_lengths) = session.run([self.sampled_batch.tokens, self.sampled_batch.value, self.sampled_batch.episode_lengths])\n        if episode_lengths.size == 0:\n            logging.warn('Shapes:\\nbatch_actions.shape: %s\\nbatch_values.shape: %s\\nepisode_lengths.shape: %s\\n', batch_actions.shape, batch_values.shape, episode_lengths.shape)\n        code_scores = compute_rewards(rl_batch, batch_actions, episode_lengths)\n        code_strings = code_scores.code_strings\n        batch_tot_r = code_scores.total_rewards\n        test_cases = code_scores.test_cases\n        code_outputs = code_scores.code_outputs\n        reasons = code_scores.reasons\n        (batch_targets, batch_returns) = process_episodes(code_scores.batch_rewards, episode_lengths, a2c=self.a2c, baselines=self.ema_by_len, batch_values=batch_values)\n        batch_policy_multipliers = batch_targets\n        batch_emp_values = batch_returns if self.a2c else [[]]\n        adjusted_lengths = episode_lengths\n        if self.top_episodes:\n            assert len(self.top_episodes) > 0\n            off_policy_targets = [item for (item, _) in self.top_episodes.random_sample(self.topk_batch_size)]\n            off_policy_target_lengths = [len(t) for t in off_policy_targets]\n            off_policy_targets = utils.stack_pad(off_policy_targets, pad_axes=0, dtype=np.int32)\n            offp_switch = 1\n        else:\n            off_policy_targets = [[0]]\n            off_policy_target_lengths = [1]\n            offp_switch = 0\n        fetches = {'global_step': global_step_op, 'program_count': self.program_count, 'summaries': self.rl_summary_op, 'train_op': train_op, 'gradients': self.gradients_dict if return_gradients else self.no_op}\n        fetched = session.run(fetches, {self.actions: batch_actions, self.empirical_values: batch_emp_values, self.policy_multipliers: batch_policy_multipliers, self.adjusted_lengths: adjusted_lengths, self.off_policy_targets: off_policy_targets, self.off_policy_target_lengths: off_policy_target_lengths, self.offp_switch: offp_switch})\n        combined_adjusted_lengths = adjusted_lengths\n        combined_returns = batch_returns\n    else:\n        (batch_actions, batch_values, episode_lengths, log_probs) = session.run([self.sampled_batch.tokens, self.sampled_batch.value, self.sampled_batch.episode_lengths, self.sampled_batch.log_probs])\n        if episode_lengths.size == 0:\n            logging.warn('Shapes:\\nbatch_actions.shape: %s\\nbatch_values.shape: %s\\nepisode_lengths.shape: %s\\n', batch_actions.shape, batch_values.shape, episode_lengths.shape)\n        empty_replay_buffer = self.experience_replay.is_empty() if self.experience_replay is not None else True\n        num_programs_from_replay_buff = self.num_replay_per_batch if not empty_replay_buffer else 0\n        num_programs_from_policy = rl_batch.batch_size - num_programs_from_replay_buff\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            result = self.experience_replay.sample_many(num_programs_from_replay_buff)\n            (experience_samples, replay_weights) = zip(*result)\n            (replay_actions, replay_rewards, _, replay_adjusted_lengths) = zip(*experience_samples)\n            replay_batch_actions = utils.stack_pad(replay_actions, pad_axes=0, dtype=np.int32)\n            (all_replay_log_probs,) = session.run([self.given_batch.log_probs], {self.actions: replay_batch_actions, self.adjusted_lengths: replay_adjusted_lengths})\n            replay_log_probs = [np.choose(replay_actions[i], all_replay_log_probs[i, :l].T).sum() for (i, l) in enumerate(replay_adjusted_lengths)]\n        else:\n            replay_actions = None\n            replay_policy_multipliers = None\n            replay_adjusted_lengths = None\n            replay_log_probs = None\n            replay_weights = None\n            replay_returns = None\n            on_policy_weights = [0] * num_programs_from_replay_buff\n        assert not self.a2c\n        code_scores = compute_rewards(rl_batch, batch_actions, episode_lengths, batch_size=num_programs_from_policy)\n        code_strings = code_scores.code_strings\n        batch_tot_r = code_scores.total_rewards\n        test_cases = code_scores.test_cases\n        code_outputs = code_scores.code_outputs\n        reasons = code_scores.reasons\n        p = num_programs_from_policy\n        (batch_targets, batch_returns) = process_episodes(code_scores.batch_rewards, episode_lengths[:p], a2c=False, baselines=self.ema_by_len)\n        batch_policy_multipliers = batch_targets\n        batch_emp_values = [[]]\n        on_policy_returns = batch_returns\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            offp_batch_rewards = [[0.0] * (l - 1) + [r] for (l, r) in zip(replay_adjusted_lengths, replay_rewards)]\n            assert len(offp_batch_rewards) == num_programs_from_replay_buff\n            assert len(replay_adjusted_lengths) == num_programs_from_replay_buff\n            (replay_batch_targets, replay_returns) = process_episodes(offp_batch_rewards, replay_adjusted_lengths, a2c=False, baselines=self.ema_by_len)\n            replay_policy_multipliers = [replay_batch_targets[i, :l] for (i, l) in enumerate(replay_adjusted_lengths[:num_programs_from_replay_buff])]\n        adjusted_lengths = episode_lengths[:num_programs_from_policy]\n        if self.top_episodes:\n            assert len(self.top_episodes) > 0\n            off_policy_targets = [item for (item, _) in self.top_episodes.random_sample(self.topk_batch_size)]\n            off_policy_target_lengths = [len(t) for t in off_policy_targets]\n            off_policy_targets = utils.stack_pad(off_policy_targets, pad_axes=0, dtype=np.int32)\n            offp_switch = 1\n        else:\n            off_policy_targets = [[0]]\n            off_policy_target_lengths = [1]\n            offp_switch = 0\n        if num_programs_from_policy:\n            separate_actions = [batch_actions[i, :l] for (i, l) in enumerate(adjusted_lengths)]\n            chosen_log_probs = [np.choose(separate_actions[i], log_probs[i, :l].T) for (i, l) in enumerate(adjusted_lengths)]\n            new_experiences = [(separate_actions[i], batch_tot_r[i], chosen_log_probs[i].sum(), l) for (i, l) in enumerate(adjusted_lengths)]\n            on_policy_policy_multipliers = [batch_policy_multipliers[i, :l] for (i, l) in enumerate(adjusted_lengths)]\n            (on_policy_actions, _, on_policy_log_probs, on_policy_adjusted_lengths) = zip(*new_experiences)\n        else:\n            new_experiences = []\n            on_policy_policy_multipliers = []\n            on_policy_actions = []\n            on_policy_log_probs = []\n            on_policy_adjusted_lengths = []\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            on_policy_weights = [0] * num_programs_from_policy\n            for (i, cs) in enumerate(code_strings):\n                if self.experience_replay.has_key(cs):\n                    on_policy_weights[i] = self.experience_replay.get_weight(cs)\n        combined_actions = join(replay_actions, on_policy_actions)\n        combined_policy_multipliers = join(replay_policy_multipliers, on_policy_policy_multipliers)\n        combined_adjusted_lengths = join(replay_adjusted_lengths, on_policy_adjusted_lengths)\n        combined_returns = join(replay_returns, on_policy_returns)\n        combined_actions = utils.stack_pad(combined_actions, pad_axes=0)\n        combined_policy_multipliers = utils.stack_pad(combined_policy_multipliers, pad_axes=0)\n        combined_on_policy_log_probs = join(replay_log_probs, on_policy_log_probs)\n        combined_q_weights = join(replay_weights, on_policy_weights)\n        if empty_replay_buffer:\n            combined_policy_multipliers *= 0\n        elif not num_programs_from_replay_buff:\n            combined_policy_multipliers = np.ones([len(combined_actions), 1], dtype=np.float32)\n        else:\n            importance_weights = self._compute_iw(combined_on_policy_log_probs, combined_q_weights)\n            if self.config.iw_normalize:\n                importance_weights *= float(rl_batch.batch_size) / importance_weights.sum()\n            combined_policy_multipliers *= importance_weights.reshape(-1, 1)\n        assert self.program_count is not None\n        fetches = {'global_step': global_step_op, 'program_count': self.program_count, 'summaries': self.rl_summary_op, 'train_op': train_op, 'gradients': self.gradients_dict if return_gradients else self.no_op}\n        fetched = session.run(fetches, {self.actions: combined_actions, self.empirical_values: [[]], self.policy_multipliers: combined_policy_multipliers, self.adjusted_lengths: combined_adjusted_lengths, self.off_policy_targets: off_policy_targets, self.off_policy_target_lengths: off_policy_target_lengths, self.offp_switch: offp_switch})\n        self.experience_replay.add_many(objs=new_experiences, weights=[exp(r / self.replay_temperature) for r in batch_tot_r], keys=code_strings)\n    session.run([self.program_count_add_op], {self.program_count_add_ph: num_programs_from_policy})\n    if not self.a2c:\n        for i in xrange(rl_batch.batch_size):\n            episode_length = combined_adjusted_lengths[i]\n            empirical_returns = combined_returns[i, :episode_length]\n            for j in xrange(episode_length):\n                self.ema_by_len[j] = self.ema_baseline_decay * self.ema_by_len[j] + (1 - self.ema_baseline_decay) * empirical_returns[j]\n    global_step = fetched['global_step']\n    global_npe = fetched['program_count']\n    core_summaries = fetched['summaries']\n    summaries_list = [core_summaries]\n    if num_programs_from_policy:\n        s_i = 0\n        text_summary = self._rl_text_summary(session, global_step, global_npe, batch_tot_r[s_i], episode_lengths[s_i], test_cases[s_i], code_outputs[s_i], code_strings[s_i], reasons[s_i])\n        reward_summary = self._rl_reward_summary(batch_tot_r)\n        is_best = False\n        if self.global_best_reward_fn:\n            best_reward = np.max(batch_tot_r)\n            is_best = self.global_best_reward_fn(session, best_reward)\n        if self.found_solution_op is not None and 'correct' in reasons:\n            session.run(self.found_solution_op)\n            if self.stop_on_success:\n                solutions = [{'code': code_strings[i], 'reward': batch_tot_r[i], 'npe': global_npe} for i in xrange(len(reasons)) if reasons[i] == 'correct']\n            elif is_best:\n                solutions = [{'code': code_strings[np.argmax(batch_tot_r)], 'reward': np.max(batch_tot_r), 'npe': global_npe}]\n            else:\n                solutions = []\n            if solutions:\n                if self.assign_code_solution_fn:\n                    self.assign_code_solution_fn(session, solutions[0]['code'])\n                with tf.gfile.FastGFile(self.logging_file, 'a') as writer:\n                    for solution_dict in solutions:\n                        writer.write(str(solution_dict) + '\\n')\n        max_i = np.argmax(batch_tot_r)\n        max_tot_r = batch_tot_r[max_i]\n        if max_tot_r >= self.top_reward:\n            if max_tot_r >= self.top_reward:\n                self.top_reward = max_tot_r\n            logging.info('Top code: r=%.2f, \\t%s', max_tot_r, code_strings[max_i])\n        if self.top_episodes is not None:\n            self.top_episodes.push(max_tot_r, tuple(batch_actions[max_i, :episode_lengths[max_i]]))\n        summaries_list += [text_summary, reward_summary]\n        if self.do_iw_summaries and (not empty_replay_buffer):\n            norm_replay_weights = [w / self.experience_replay.total_weight for w in replay_weights]\n            replay_iw = self._compute_iw(replay_log_probs, replay_weights)\n            on_policy_iw = self._compute_iw(on_policy_log_probs, on_policy_weights)\n            summaries_list.append(self._iw_summary(session, replay_iw, replay_log_probs, norm_replay_weights, on_policy_iw, on_policy_log_probs))\n    return UpdateStepResult(global_step=global_step, global_npe=global_npe, summaries_list=summaries_list, gradients_dict=fetched['gradients'])",
            "def update_step(self, session, rl_batch, train_op, global_step_op, return_gradients=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Perform gradient update on the model.\\n\\n    Args:\\n      session: tf.Session instance.\\n      rl_batch: RLBatch instance from data.py. Use DataManager to create a\\n          RLBatch for each call to update_step. RLBatch contains a batch of\\n          tasks.\\n      train_op: A TF op which will perform the gradient update. LMAgent does not\\n          own its training op, so that trainers can do distributed training\\n          and construct a specialized training op.\\n      global_step_op: A TF op which will return the current global step when\\n          run (should not increment it).\\n      return_gradients: If True, the gradients will be saved and returned from\\n          this method call. This is useful for testing.\\n\\n    Returns:\\n      Results from the update step in a UpdateStepResult namedtuple, including\\n      global step, global NPE, serialized summaries, and optionally gradients.\\n    '\n    assert self.is_local\n    if self.experience_replay is None:\n        num_programs_from_policy = rl_batch.batch_size\n        (batch_actions, batch_values, episode_lengths) = session.run([self.sampled_batch.tokens, self.sampled_batch.value, self.sampled_batch.episode_lengths])\n        if episode_lengths.size == 0:\n            logging.warn('Shapes:\\nbatch_actions.shape: %s\\nbatch_values.shape: %s\\nepisode_lengths.shape: %s\\n', batch_actions.shape, batch_values.shape, episode_lengths.shape)\n        code_scores = compute_rewards(rl_batch, batch_actions, episode_lengths)\n        code_strings = code_scores.code_strings\n        batch_tot_r = code_scores.total_rewards\n        test_cases = code_scores.test_cases\n        code_outputs = code_scores.code_outputs\n        reasons = code_scores.reasons\n        (batch_targets, batch_returns) = process_episodes(code_scores.batch_rewards, episode_lengths, a2c=self.a2c, baselines=self.ema_by_len, batch_values=batch_values)\n        batch_policy_multipliers = batch_targets\n        batch_emp_values = batch_returns if self.a2c else [[]]\n        adjusted_lengths = episode_lengths\n        if self.top_episodes:\n            assert len(self.top_episodes) > 0\n            off_policy_targets = [item for (item, _) in self.top_episodes.random_sample(self.topk_batch_size)]\n            off_policy_target_lengths = [len(t) for t in off_policy_targets]\n            off_policy_targets = utils.stack_pad(off_policy_targets, pad_axes=0, dtype=np.int32)\n            offp_switch = 1\n        else:\n            off_policy_targets = [[0]]\n            off_policy_target_lengths = [1]\n            offp_switch = 0\n        fetches = {'global_step': global_step_op, 'program_count': self.program_count, 'summaries': self.rl_summary_op, 'train_op': train_op, 'gradients': self.gradients_dict if return_gradients else self.no_op}\n        fetched = session.run(fetches, {self.actions: batch_actions, self.empirical_values: batch_emp_values, self.policy_multipliers: batch_policy_multipliers, self.adjusted_lengths: adjusted_lengths, self.off_policy_targets: off_policy_targets, self.off_policy_target_lengths: off_policy_target_lengths, self.offp_switch: offp_switch})\n        combined_adjusted_lengths = adjusted_lengths\n        combined_returns = batch_returns\n    else:\n        (batch_actions, batch_values, episode_lengths, log_probs) = session.run([self.sampled_batch.tokens, self.sampled_batch.value, self.sampled_batch.episode_lengths, self.sampled_batch.log_probs])\n        if episode_lengths.size == 0:\n            logging.warn('Shapes:\\nbatch_actions.shape: %s\\nbatch_values.shape: %s\\nepisode_lengths.shape: %s\\n', batch_actions.shape, batch_values.shape, episode_lengths.shape)\n        empty_replay_buffer = self.experience_replay.is_empty() if self.experience_replay is not None else True\n        num_programs_from_replay_buff = self.num_replay_per_batch if not empty_replay_buffer else 0\n        num_programs_from_policy = rl_batch.batch_size - num_programs_from_replay_buff\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            result = self.experience_replay.sample_many(num_programs_from_replay_buff)\n            (experience_samples, replay_weights) = zip(*result)\n            (replay_actions, replay_rewards, _, replay_adjusted_lengths) = zip(*experience_samples)\n            replay_batch_actions = utils.stack_pad(replay_actions, pad_axes=0, dtype=np.int32)\n            (all_replay_log_probs,) = session.run([self.given_batch.log_probs], {self.actions: replay_batch_actions, self.adjusted_lengths: replay_adjusted_lengths})\n            replay_log_probs = [np.choose(replay_actions[i], all_replay_log_probs[i, :l].T).sum() for (i, l) in enumerate(replay_adjusted_lengths)]\n        else:\n            replay_actions = None\n            replay_policy_multipliers = None\n            replay_adjusted_lengths = None\n            replay_log_probs = None\n            replay_weights = None\n            replay_returns = None\n            on_policy_weights = [0] * num_programs_from_replay_buff\n        assert not self.a2c\n        code_scores = compute_rewards(rl_batch, batch_actions, episode_lengths, batch_size=num_programs_from_policy)\n        code_strings = code_scores.code_strings\n        batch_tot_r = code_scores.total_rewards\n        test_cases = code_scores.test_cases\n        code_outputs = code_scores.code_outputs\n        reasons = code_scores.reasons\n        p = num_programs_from_policy\n        (batch_targets, batch_returns) = process_episodes(code_scores.batch_rewards, episode_lengths[:p], a2c=False, baselines=self.ema_by_len)\n        batch_policy_multipliers = batch_targets\n        batch_emp_values = [[]]\n        on_policy_returns = batch_returns\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            offp_batch_rewards = [[0.0] * (l - 1) + [r] for (l, r) in zip(replay_adjusted_lengths, replay_rewards)]\n            assert len(offp_batch_rewards) == num_programs_from_replay_buff\n            assert len(replay_adjusted_lengths) == num_programs_from_replay_buff\n            (replay_batch_targets, replay_returns) = process_episodes(offp_batch_rewards, replay_adjusted_lengths, a2c=False, baselines=self.ema_by_len)\n            replay_policy_multipliers = [replay_batch_targets[i, :l] for (i, l) in enumerate(replay_adjusted_lengths[:num_programs_from_replay_buff])]\n        adjusted_lengths = episode_lengths[:num_programs_from_policy]\n        if self.top_episodes:\n            assert len(self.top_episodes) > 0\n            off_policy_targets = [item for (item, _) in self.top_episodes.random_sample(self.topk_batch_size)]\n            off_policy_target_lengths = [len(t) for t in off_policy_targets]\n            off_policy_targets = utils.stack_pad(off_policy_targets, pad_axes=0, dtype=np.int32)\n            offp_switch = 1\n        else:\n            off_policy_targets = [[0]]\n            off_policy_target_lengths = [1]\n            offp_switch = 0\n        if num_programs_from_policy:\n            separate_actions = [batch_actions[i, :l] for (i, l) in enumerate(adjusted_lengths)]\n            chosen_log_probs = [np.choose(separate_actions[i], log_probs[i, :l].T) for (i, l) in enumerate(adjusted_lengths)]\n            new_experiences = [(separate_actions[i], batch_tot_r[i], chosen_log_probs[i].sum(), l) for (i, l) in enumerate(adjusted_lengths)]\n            on_policy_policy_multipliers = [batch_policy_multipliers[i, :l] for (i, l) in enumerate(adjusted_lengths)]\n            (on_policy_actions, _, on_policy_log_probs, on_policy_adjusted_lengths) = zip(*new_experiences)\n        else:\n            new_experiences = []\n            on_policy_policy_multipliers = []\n            on_policy_actions = []\n            on_policy_log_probs = []\n            on_policy_adjusted_lengths = []\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            on_policy_weights = [0] * num_programs_from_policy\n            for (i, cs) in enumerate(code_strings):\n                if self.experience_replay.has_key(cs):\n                    on_policy_weights[i] = self.experience_replay.get_weight(cs)\n        combined_actions = join(replay_actions, on_policy_actions)\n        combined_policy_multipliers = join(replay_policy_multipliers, on_policy_policy_multipliers)\n        combined_adjusted_lengths = join(replay_adjusted_lengths, on_policy_adjusted_lengths)\n        combined_returns = join(replay_returns, on_policy_returns)\n        combined_actions = utils.stack_pad(combined_actions, pad_axes=0)\n        combined_policy_multipliers = utils.stack_pad(combined_policy_multipliers, pad_axes=0)\n        combined_on_policy_log_probs = join(replay_log_probs, on_policy_log_probs)\n        combined_q_weights = join(replay_weights, on_policy_weights)\n        if empty_replay_buffer:\n            combined_policy_multipliers *= 0\n        elif not num_programs_from_replay_buff:\n            combined_policy_multipliers = np.ones([len(combined_actions), 1], dtype=np.float32)\n        else:\n            importance_weights = self._compute_iw(combined_on_policy_log_probs, combined_q_weights)\n            if self.config.iw_normalize:\n                importance_weights *= float(rl_batch.batch_size) / importance_weights.sum()\n            combined_policy_multipliers *= importance_weights.reshape(-1, 1)\n        assert self.program_count is not None\n        fetches = {'global_step': global_step_op, 'program_count': self.program_count, 'summaries': self.rl_summary_op, 'train_op': train_op, 'gradients': self.gradients_dict if return_gradients else self.no_op}\n        fetched = session.run(fetches, {self.actions: combined_actions, self.empirical_values: [[]], self.policy_multipliers: combined_policy_multipliers, self.adjusted_lengths: combined_adjusted_lengths, self.off_policy_targets: off_policy_targets, self.off_policy_target_lengths: off_policy_target_lengths, self.offp_switch: offp_switch})\n        self.experience_replay.add_many(objs=new_experiences, weights=[exp(r / self.replay_temperature) for r in batch_tot_r], keys=code_strings)\n    session.run([self.program_count_add_op], {self.program_count_add_ph: num_programs_from_policy})\n    if not self.a2c:\n        for i in xrange(rl_batch.batch_size):\n            episode_length = combined_adjusted_lengths[i]\n            empirical_returns = combined_returns[i, :episode_length]\n            for j in xrange(episode_length):\n                self.ema_by_len[j] = self.ema_baseline_decay * self.ema_by_len[j] + (1 - self.ema_baseline_decay) * empirical_returns[j]\n    global_step = fetched['global_step']\n    global_npe = fetched['program_count']\n    core_summaries = fetched['summaries']\n    summaries_list = [core_summaries]\n    if num_programs_from_policy:\n        s_i = 0\n        text_summary = self._rl_text_summary(session, global_step, global_npe, batch_tot_r[s_i], episode_lengths[s_i], test_cases[s_i], code_outputs[s_i], code_strings[s_i], reasons[s_i])\n        reward_summary = self._rl_reward_summary(batch_tot_r)\n        is_best = False\n        if self.global_best_reward_fn:\n            best_reward = np.max(batch_tot_r)\n            is_best = self.global_best_reward_fn(session, best_reward)\n        if self.found_solution_op is not None and 'correct' in reasons:\n            session.run(self.found_solution_op)\n            if self.stop_on_success:\n                solutions = [{'code': code_strings[i], 'reward': batch_tot_r[i], 'npe': global_npe} for i in xrange(len(reasons)) if reasons[i] == 'correct']\n            elif is_best:\n                solutions = [{'code': code_strings[np.argmax(batch_tot_r)], 'reward': np.max(batch_tot_r), 'npe': global_npe}]\n            else:\n                solutions = []\n            if solutions:\n                if self.assign_code_solution_fn:\n                    self.assign_code_solution_fn(session, solutions[0]['code'])\n                with tf.gfile.FastGFile(self.logging_file, 'a') as writer:\n                    for solution_dict in solutions:\n                        writer.write(str(solution_dict) + '\\n')\n        max_i = np.argmax(batch_tot_r)\n        max_tot_r = batch_tot_r[max_i]\n        if max_tot_r >= self.top_reward:\n            if max_tot_r >= self.top_reward:\n                self.top_reward = max_tot_r\n            logging.info('Top code: r=%.2f, \\t%s', max_tot_r, code_strings[max_i])\n        if self.top_episodes is not None:\n            self.top_episodes.push(max_tot_r, tuple(batch_actions[max_i, :episode_lengths[max_i]]))\n        summaries_list += [text_summary, reward_summary]\n        if self.do_iw_summaries and (not empty_replay_buffer):\n            norm_replay_weights = [w / self.experience_replay.total_weight for w in replay_weights]\n            replay_iw = self._compute_iw(replay_log_probs, replay_weights)\n            on_policy_iw = self._compute_iw(on_policy_log_probs, on_policy_weights)\n            summaries_list.append(self._iw_summary(session, replay_iw, replay_log_probs, norm_replay_weights, on_policy_iw, on_policy_log_probs))\n    return UpdateStepResult(global_step=global_step, global_npe=global_npe, summaries_list=summaries_list, gradients_dict=fetched['gradients'])",
            "def update_step(self, session, rl_batch, train_op, global_step_op, return_gradients=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Perform gradient update on the model.\\n\\n    Args:\\n      session: tf.Session instance.\\n      rl_batch: RLBatch instance from data.py. Use DataManager to create a\\n          RLBatch for each call to update_step. RLBatch contains a batch of\\n          tasks.\\n      train_op: A TF op which will perform the gradient update. LMAgent does not\\n          own its training op, so that trainers can do distributed training\\n          and construct a specialized training op.\\n      global_step_op: A TF op which will return the current global step when\\n          run (should not increment it).\\n      return_gradients: If True, the gradients will be saved and returned from\\n          this method call. This is useful for testing.\\n\\n    Returns:\\n      Results from the update step in a UpdateStepResult namedtuple, including\\n      global step, global NPE, serialized summaries, and optionally gradients.\\n    '\n    assert self.is_local\n    if self.experience_replay is None:\n        num_programs_from_policy = rl_batch.batch_size\n        (batch_actions, batch_values, episode_lengths) = session.run([self.sampled_batch.tokens, self.sampled_batch.value, self.sampled_batch.episode_lengths])\n        if episode_lengths.size == 0:\n            logging.warn('Shapes:\\nbatch_actions.shape: %s\\nbatch_values.shape: %s\\nepisode_lengths.shape: %s\\n', batch_actions.shape, batch_values.shape, episode_lengths.shape)\n        code_scores = compute_rewards(rl_batch, batch_actions, episode_lengths)\n        code_strings = code_scores.code_strings\n        batch_tot_r = code_scores.total_rewards\n        test_cases = code_scores.test_cases\n        code_outputs = code_scores.code_outputs\n        reasons = code_scores.reasons\n        (batch_targets, batch_returns) = process_episodes(code_scores.batch_rewards, episode_lengths, a2c=self.a2c, baselines=self.ema_by_len, batch_values=batch_values)\n        batch_policy_multipliers = batch_targets\n        batch_emp_values = batch_returns if self.a2c else [[]]\n        adjusted_lengths = episode_lengths\n        if self.top_episodes:\n            assert len(self.top_episodes) > 0\n            off_policy_targets = [item for (item, _) in self.top_episodes.random_sample(self.topk_batch_size)]\n            off_policy_target_lengths = [len(t) for t in off_policy_targets]\n            off_policy_targets = utils.stack_pad(off_policy_targets, pad_axes=0, dtype=np.int32)\n            offp_switch = 1\n        else:\n            off_policy_targets = [[0]]\n            off_policy_target_lengths = [1]\n            offp_switch = 0\n        fetches = {'global_step': global_step_op, 'program_count': self.program_count, 'summaries': self.rl_summary_op, 'train_op': train_op, 'gradients': self.gradients_dict if return_gradients else self.no_op}\n        fetched = session.run(fetches, {self.actions: batch_actions, self.empirical_values: batch_emp_values, self.policy_multipliers: batch_policy_multipliers, self.adjusted_lengths: adjusted_lengths, self.off_policy_targets: off_policy_targets, self.off_policy_target_lengths: off_policy_target_lengths, self.offp_switch: offp_switch})\n        combined_adjusted_lengths = adjusted_lengths\n        combined_returns = batch_returns\n    else:\n        (batch_actions, batch_values, episode_lengths, log_probs) = session.run([self.sampled_batch.tokens, self.sampled_batch.value, self.sampled_batch.episode_lengths, self.sampled_batch.log_probs])\n        if episode_lengths.size == 0:\n            logging.warn('Shapes:\\nbatch_actions.shape: %s\\nbatch_values.shape: %s\\nepisode_lengths.shape: %s\\n', batch_actions.shape, batch_values.shape, episode_lengths.shape)\n        empty_replay_buffer = self.experience_replay.is_empty() if self.experience_replay is not None else True\n        num_programs_from_replay_buff = self.num_replay_per_batch if not empty_replay_buffer else 0\n        num_programs_from_policy = rl_batch.batch_size - num_programs_from_replay_buff\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            result = self.experience_replay.sample_many(num_programs_from_replay_buff)\n            (experience_samples, replay_weights) = zip(*result)\n            (replay_actions, replay_rewards, _, replay_adjusted_lengths) = zip(*experience_samples)\n            replay_batch_actions = utils.stack_pad(replay_actions, pad_axes=0, dtype=np.int32)\n            (all_replay_log_probs,) = session.run([self.given_batch.log_probs], {self.actions: replay_batch_actions, self.adjusted_lengths: replay_adjusted_lengths})\n            replay_log_probs = [np.choose(replay_actions[i], all_replay_log_probs[i, :l].T).sum() for (i, l) in enumerate(replay_adjusted_lengths)]\n        else:\n            replay_actions = None\n            replay_policy_multipliers = None\n            replay_adjusted_lengths = None\n            replay_log_probs = None\n            replay_weights = None\n            replay_returns = None\n            on_policy_weights = [0] * num_programs_from_replay_buff\n        assert not self.a2c\n        code_scores = compute_rewards(rl_batch, batch_actions, episode_lengths, batch_size=num_programs_from_policy)\n        code_strings = code_scores.code_strings\n        batch_tot_r = code_scores.total_rewards\n        test_cases = code_scores.test_cases\n        code_outputs = code_scores.code_outputs\n        reasons = code_scores.reasons\n        p = num_programs_from_policy\n        (batch_targets, batch_returns) = process_episodes(code_scores.batch_rewards, episode_lengths[:p], a2c=False, baselines=self.ema_by_len)\n        batch_policy_multipliers = batch_targets\n        batch_emp_values = [[]]\n        on_policy_returns = batch_returns\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            offp_batch_rewards = [[0.0] * (l - 1) + [r] for (l, r) in zip(replay_adjusted_lengths, replay_rewards)]\n            assert len(offp_batch_rewards) == num_programs_from_replay_buff\n            assert len(replay_adjusted_lengths) == num_programs_from_replay_buff\n            (replay_batch_targets, replay_returns) = process_episodes(offp_batch_rewards, replay_adjusted_lengths, a2c=False, baselines=self.ema_by_len)\n            replay_policy_multipliers = [replay_batch_targets[i, :l] for (i, l) in enumerate(replay_adjusted_lengths[:num_programs_from_replay_buff])]\n        adjusted_lengths = episode_lengths[:num_programs_from_policy]\n        if self.top_episodes:\n            assert len(self.top_episodes) > 0\n            off_policy_targets = [item for (item, _) in self.top_episodes.random_sample(self.topk_batch_size)]\n            off_policy_target_lengths = [len(t) for t in off_policy_targets]\n            off_policy_targets = utils.stack_pad(off_policy_targets, pad_axes=0, dtype=np.int32)\n            offp_switch = 1\n        else:\n            off_policy_targets = [[0]]\n            off_policy_target_lengths = [1]\n            offp_switch = 0\n        if num_programs_from_policy:\n            separate_actions = [batch_actions[i, :l] for (i, l) in enumerate(adjusted_lengths)]\n            chosen_log_probs = [np.choose(separate_actions[i], log_probs[i, :l].T) for (i, l) in enumerate(adjusted_lengths)]\n            new_experiences = [(separate_actions[i], batch_tot_r[i], chosen_log_probs[i].sum(), l) for (i, l) in enumerate(adjusted_lengths)]\n            on_policy_policy_multipliers = [batch_policy_multipliers[i, :l] for (i, l) in enumerate(adjusted_lengths)]\n            (on_policy_actions, _, on_policy_log_probs, on_policy_adjusted_lengths) = zip(*new_experiences)\n        else:\n            new_experiences = []\n            on_policy_policy_multipliers = []\n            on_policy_actions = []\n            on_policy_log_probs = []\n            on_policy_adjusted_lengths = []\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            on_policy_weights = [0] * num_programs_from_policy\n            for (i, cs) in enumerate(code_strings):\n                if self.experience_replay.has_key(cs):\n                    on_policy_weights[i] = self.experience_replay.get_weight(cs)\n        combined_actions = join(replay_actions, on_policy_actions)\n        combined_policy_multipliers = join(replay_policy_multipliers, on_policy_policy_multipliers)\n        combined_adjusted_lengths = join(replay_adjusted_lengths, on_policy_adjusted_lengths)\n        combined_returns = join(replay_returns, on_policy_returns)\n        combined_actions = utils.stack_pad(combined_actions, pad_axes=0)\n        combined_policy_multipliers = utils.stack_pad(combined_policy_multipliers, pad_axes=0)\n        combined_on_policy_log_probs = join(replay_log_probs, on_policy_log_probs)\n        combined_q_weights = join(replay_weights, on_policy_weights)\n        if empty_replay_buffer:\n            combined_policy_multipliers *= 0\n        elif not num_programs_from_replay_buff:\n            combined_policy_multipliers = np.ones([len(combined_actions), 1], dtype=np.float32)\n        else:\n            importance_weights = self._compute_iw(combined_on_policy_log_probs, combined_q_weights)\n            if self.config.iw_normalize:\n                importance_weights *= float(rl_batch.batch_size) / importance_weights.sum()\n            combined_policy_multipliers *= importance_weights.reshape(-1, 1)\n        assert self.program_count is not None\n        fetches = {'global_step': global_step_op, 'program_count': self.program_count, 'summaries': self.rl_summary_op, 'train_op': train_op, 'gradients': self.gradients_dict if return_gradients else self.no_op}\n        fetched = session.run(fetches, {self.actions: combined_actions, self.empirical_values: [[]], self.policy_multipliers: combined_policy_multipliers, self.adjusted_lengths: combined_adjusted_lengths, self.off_policy_targets: off_policy_targets, self.off_policy_target_lengths: off_policy_target_lengths, self.offp_switch: offp_switch})\n        self.experience_replay.add_many(objs=new_experiences, weights=[exp(r / self.replay_temperature) for r in batch_tot_r], keys=code_strings)\n    session.run([self.program_count_add_op], {self.program_count_add_ph: num_programs_from_policy})\n    if not self.a2c:\n        for i in xrange(rl_batch.batch_size):\n            episode_length = combined_adjusted_lengths[i]\n            empirical_returns = combined_returns[i, :episode_length]\n            for j in xrange(episode_length):\n                self.ema_by_len[j] = self.ema_baseline_decay * self.ema_by_len[j] + (1 - self.ema_baseline_decay) * empirical_returns[j]\n    global_step = fetched['global_step']\n    global_npe = fetched['program_count']\n    core_summaries = fetched['summaries']\n    summaries_list = [core_summaries]\n    if num_programs_from_policy:\n        s_i = 0\n        text_summary = self._rl_text_summary(session, global_step, global_npe, batch_tot_r[s_i], episode_lengths[s_i], test_cases[s_i], code_outputs[s_i], code_strings[s_i], reasons[s_i])\n        reward_summary = self._rl_reward_summary(batch_tot_r)\n        is_best = False\n        if self.global_best_reward_fn:\n            best_reward = np.max(batch_tot_r)\n            is_best = self.global_best_reward_fn(session, best_reward)\n        if self.found_solution_op is not None and 'correct' in reasons:\n            session.run(self.found_solution_op)\n            if self.stop_on_success:\n                solutions = [{'code': code_strings[i], 'reward': batch_tot_r[i], 'npe': global_npe} for i in xrange(len(reasons)) if reasons[i] == 'correct']\n            elif is_best:\n                solutions = [{'code': code_strings[np.argmax(batch_tot_r)], 'reward': np.max(batch_tot_r), 'npe': global_npe}]\n            else:\n                solutions = []\n            if solutions:\n                if self.assign_code_solution_fn:\n                    self.assign_code_solution_fn(session, solutions[0]['code'])\n                with tf.gfile.FastGFile(self.logging_file, 'a') as writer:\n                    for solution_dict in solutions:\n                        writer.write(str(solution_dict) + '\\n')\n        max_i = np.argmax(batch_tot_r)\n        max_tot_r = batch_tot_r[max_i]\n        if max_tot_r >= self.top_reward:\n            if max_tot_r >= self.top_reward:\n                self.top_reward = max_tot_r\n            logging.info('Top code: r=%.2f, \\t%s', max_tot_r, code_strings[max_i])\n        if self.top_episodes is not None:\n            self.top_episodes.push(max_tot_r, tuple(batch_actions[max_i, :episode_lengths[max_i]]))\n        summaries_list += [text_summary, reward_summary]\n        if self.do_iw_summaries and (not empty_replay_buffer):\n            norm_replay_weights = [w / self.experience_replay.total_weight for w in replay_weights]\n            replay_iw = self._compute_iw(replay_log_probs, replay_weights)\n            on_policy_iw = self._compute_iw(on_policy_log_probs, on_policy_weights)\n            summaries_list.append(self._iw_summary(session, replay_iw, replay_log_probs, norm_replay_weights, on_policy_iw, on_policy_log_probs))\n    return UpdateStepResult(global_step=global_step, global_npe=global_npe, summaries_list=summaries_list, gradients_dict=fetched['gradients'])",
            "def update_step(self, session, rl_batch, train_op, global_step_op, return_gradients=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Perform gradient update on the model.\\n\\n    Args:\\n      session: tf.Session instance.\\n      rl_batch: RLBatch instance from data.py. Use DataManager to create a\\n          RLBatch for each call to update_step. RLBatch contains a batch of\\n          tasks.\\n      train_op: A TF op which will perform the gradient update. LMAgent does not\\n          own its training op, so that trainers can do distributed training\\n          and construct a specialized training op.\\n      global_step_op: A TF op which will return the current global step when\\n          run (should not increment it).\\n      return_gradients: If True, the gradients will be saved and returned from\\n          this method call. This is useful for testing.\\n\\n    Returns:\\n      Results from the update step in a UpdateStepResult namedtuple, including\\n      global step, global NPE, serialized summaries, and optionally gradients.\\n    '\n    assert self.is_local\n    if self.experience_replay is None:\n        num_programs_from_policy = rl_batch.batch_size\n        (batch_actions, batch_values, episode_lengths) = session.run([self.sampled_batch.tokens, self.sampled_batch.value, self.sampled_batch.episode_lengths])\n        if episode_lengths.size == 0:\n            logging.warn('Shapes:\\nbatch_actions.shape: %s\\nbatch_values.shape: %s\\nepisode_lengths.shape: %s\\n', batch_actions.shape, batch_values.shape, episode_lengths.shape)\n        code_scores = compute_rewards(rl_batch, batch_actions, episode_lengths)\n        code_strings = code_scores.code_strings\n        batch_tot_r = code_scores.total_rewards\n        test_cases = code_scores.test_cases\n        code_outputs = code_scores.code_outputs\n        reasons = code_scores.reasons\n        (batch_targets, batch_returns) = process_episodes(code_scores.batch_rewards, episode_lengths, a2c=self.a2c, baselines=self.ema_by_len, batch_values=batch_values)\n        batch_policy_multipliers = batch_targets\n        batch_emp_values = batch_returns if self.a2c else [[]]\n        adjusted_lengths = episode_lengths\n        if self.top_episodes:\n            assert len(self.top_episodes) > 0\n            off_policy_targets = [item for (item, _) in self.top_episodes.random_sample(self.topk_batch_size)]\n            off_policy_target_lengths = [len(t) for t in off_policy_targets]\n            off_policy_targets = utils.stack_pad(off_policy_targets, pad_axes=0, dtype=np.int32)\n            offp_switch = 1\n        else:\n            off_policy_targets = [[0]]\n            off_policy_target_lengths = [1]\n            offp_switch = 0\n        fetches = {'global_step': global_step_op, 'program_count': self.program_count, 'summaries': self.rl_summary_op, 'train_op': train_op, 'gradients': self.gradients_dict if return_gradients else self.no_op}\n        fetched = session.run(fetches, {self.actions: batch_actions, self.empirical_values: batch_emp_values, self.policy_multipliers: batch_policy_multipliers, self.adjusted_lengths: adjusted_lengths, self.off_policy_targets: off_policy_targets, self.off_policy_target_lengths: off_policy_target_lengths, self.offp_switch: offp_switch})\n        combined_adjusted_lengths = adjusted_lengths\n        combined_returns = batch_returns\n    else:\n        (batch_actions, batch_values, episode_lengths, log_probs) = session.run([self.sampled_batch.tokens, self.sampled_batch.value, self.sampled_batch.episode_lengths, self.sampled_batch.log_probs])\n        if episode_lengths.size == 0:\n            logging.warn('Shapes:\\nbatch_actions.shape: %s\\nbatch_values.shape: %s\\nepisode_lengths.shape: %s\\n', batch_actions.shape, batch_values.shape, episode_lengths.shape)\n        empty_replay_buffer = self.experience_replay.is_empty() if self.experience_replay is not None else True\n        num_programs_from_replay_buff = self.num_replay_per_batch if not empty_replay_buffer else 0\n        num_programs_from_policy = rl_batch.batch_size - num_programs_from_replay_buff\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            result = self.experience_replay.sample_many(num_programs_from_replay_buff)\n            (experience_samples, replay_weights) = zip(*result)\n            (replay_actions, replay_rewards, _, replay_adjusted_lengths) = zip(*experience_samples)\n            replay_batch_actions = utils.stack_pad(replay_actions, pad_axes=0, dtype=np.int32)\n            (all_replay_log_probs,) = session.run([self.given_batch.log_probs], {self.actions: replay_batch_actions, self.adjusted_lengths: replay_adjusted_lengths})\n            replay_log_probs = [np.choose(replay_actions[i], all_replay_log_probs[i, :l].T).sum() for (i, l) in enumerate(replay_adjusted_lengths)]\n        else:\n            replay_actions = None\n            replay_policy_multipliers = None\n            replay_adjusted_lengths = None\n            replay_log_probs = None\n            replay_weights = None\n            replay_returns = None\n            on_policy_weights = [0] * num_programs_from_replay_buff\n        assert not self.a2c\n        code_scores = compute_rewards(rl_batch, batch_actions, episode_lengths, batch_size=num_programs_from_policy)\n        code_strings = code_scores.code_strings\n        batch_tot_r = code_scores.total_rewards\n        test_cases = code_scores.test_cases\n        code_outputs = code_scores.code_outputs\n        reasons = code_scores.reasons\n        p = num_programs_from_policy\n        (batch_targets, batch_returns) = process_episodes(code_scores.batch_rewards, episode_lengths[:p], a2c=False, baselines=self.ema_by_len)\n        batch_policy_multipliers = batch_targets\n        batch_emp_values = [[]]\n        on_policy_returns = batch_returns\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            offp_batch_rewards = [[0.0] * (l - 1) + [r] for (l, r) in zip(replay_adjusted_lengths, replay_rewards)]\n            assert len(offp_batch_rewards) == num_programs_from_replay_buff\n            assert len(replay_adjusted_lengths) == num_programs_from_replay_buff\n            (replay_batch_targets, replay_returns) = process_episodes(offp_batch_rewards, replay_adjusted_lengths, a2c=False, baselines=self.ema_by_len)\n            replay_policy_multipliers = [replay_batch_targets[i, :l] for (i, l) in enumerate(replay_adjusted_lengths[:num_programs_from_replay_buff])]\n        adjusted_lengths = episode_lengths[:num_programs_from_policy]\n        if self.top_episodes:\n            assert len(self.top_episodes) > 0\n            off_policy_targets = [item for (item, _) in self.top_episodes.random_sample(self.topk_batch_size)]\n            off_policy_target_lengths = [len(t) for t in off_policy_targets]\n            off_policy_targets = utils.stack_pad(off_policy_targets, pad_axes=0, dtype=np.int32)\n            offp_switch = 1\n        else:\n            off_policy_targets = [[0]]\n            off_policy_target_lengths = [1]\n            offp_switch = 0\n        if num_programs_from_policy:\n            separate_actions = [batch_actions[i, :l] for (i, l) in enumerate(adjusted_lengths)]\n            chosen_log_probs = [np.choose(separate_actions[i], log_probs[i, :l].T) for (i, l) in enumerate(adjusted_lengths)]\n            new_experiences = [(separate_actions[i], batch_tot_r[i], chosen_log_probs[i].sum(), l) for (i, l) in enumerate(adjusted_lengths)]\n            on_policy_policy_multipliers = [batch_policy_multipliers[i, :l] for (i, l) in enumerate(adjusted_lengths)]\n            (on_policy_actions, _, on_policy_log_probs, on_policy_adjusted_lengths) = zip(*new_experiences)\n        else:\n            new_experiences = []\n            on_policy_policy_multipliers = []\n            on_policy_actions = []\n            on_policy_log_probs = []\n            on_policy_adjusted_lengths = []\n        if not empty_replay_buffer and num_programs_from_replay_buff:\n            on_policy_weights = [0] * num_programs_from_policy\n            for (i, cs) in enumerate(code_strings):\n                if self.experience_replay.has_key(cs):\n                    on_policy_weights[i] = self.experience_replay.get_weight(cs)\n        combined_actions = join(replay_actions, on_policy_actions)\n        combined_policy_multipliers = join(replay_policy_multipliers, on_policy_policy_multipliers)\n        combined_adjusted_lengths = join(replay_adjusted_lengths, on_policy_adjusted_lengths)\n        combined_returns = join(replay_returns, on_policy_returns)\n        combined_actions = utils.stack_pad(combined_actions, pad_axes=0)\n        combined_policy_multipliers = utils.stack_pad(combined_policy_multipliers, pad_axes=0)\n        combined_on_policy_log_probs = join(replay_log_probs, on_policy_log_probs)\n        combined_q_weights = join(replay_weights, on_policy_weights)\n        if empty_replay_buffer:\n            combined_policy_multipliers *= 0\n        elif not num_programs_from_replay_buff:\n            combined_policy_multipliers = np.ones([len(combined_actions), 1], dtype=np.float32)\n        else:\n            importance_weights = self._compute_iw(combined_on_policy_log_probs, combined_q_weights)\n            if self.config.iw_normalize:\n                importance_weights *= float(rl_batch.batch_size) / importance_weights.sum()\n            combined_policy_multipliers *= importance_weights.reshape(-1, 1)\n        assert self.program_count is not None\n        fetches = {'global_step': global_step_op, 'program_count': self.program_count, 'summaries': self.rl_summary_op, 'train_op': train_op, 'gradients': self.gradients_dict if return_gradients else self.no_op}\n        fetched = session.run(fetches, {self.actions: combined_actions, self.empirical_values: [[]], self.policy_multipliers: combined_policy_multipliers, self.adjusted_lengths: combined_adjusted_lengths, self.off_policy_targets: off_policy_targets, self.off_policy_target_lengths: off_policy_target_lengths, self.offp_switch: offp_switch})\n        self.experience_replay.add_many(objs=new_experiences, weights=[exp(r / self.replay_temperature) for r in batch_tot_r], keys=code_strings)\n    session.run([self.program_count_add_op], {self.program_count_add_ph: num_programs_from_policy})\n    if not self.a2c:\n        for i in xrange(rl_batch.batch_size):\n            episode_length = combined_adjusted_lengths[i]\n            empirical_returns = combined_returns[i, :episode_length]\n            for j in xrange(episode_length):\n                self.ema_by_len[j] = self.ema_baseline_decay * self.ema_by_len[j] + (1 - self.ema_baseline_decay) * empirical_returns[j]\n    global_step = fetched['global_step']\n    global_npe = fetched['program_count']\n    core_summaries = fetched['summaries']\n    summaries_list = [core_summaries]\n    if num_programs_from_policy:\n        s_i = 0\n        text_summary = self._rl_text_summary(session, global_step, global_npe, batch_tot_r[s_i], episode_lengths[s_i], test_cases[s_i], code_outputs[s_i], code_strings[s_i], reasons[s_i])\n        reward_summary = self._rl_reward_summary(batch_tot_r)\n        is_best = False\n        if self.global_best_reward_fn:\n            best_reward = np.max(batch_tot_r)\n            is_best = self.global_best_reward_fn(session, best_reward)\n        if self.found_solution_op is not None and 'correct' in reasons:\n            session.run(self.found_solution_op)\n            if self.stop_on_success:\n                solutions = [{'code': code_strings[i], 'reward': batch_tot_r[i], 'npe': global_npe} for i in xrange(len(reasons)) if reasons[i] == 'correct']\n            elif is_best:\n                solutions = [{'code': code_strings[np.argmax(batch_tot_r)], 'reward': np.max(batch_tot_r), 'npe': global_npe}]\n            else:\n                solutions = []\n            if solutions:\n                if self.assign_code_solution_fn:\n                    self.assign_code_solution_fn(session, solutions[0]['code'])\n                with tf.gfile.FastGFile(self.logging_file, 'a') as writer:\n                    for solution_dict in solutions:\n                        writer.write(str(solution_dict) + '\\n')\n        max_i = np.argmax(batch_tot_r)\n        max_tot_r = batch_tot_r[max_i]\n        if max_tot_r >= self.top_reward:\n            if max_tot_r >= self.top_reward:\n                self.top_reward = max_tot_r\n            logging.info('Top code: r=%.2f, \\t%s', max_tot_r, code_strings[max_i])\n        if self.top_episodes is not None:\n            self.top_episodes.push(max_tot_r, tuple(batch_actions[max_i, :episode_lengths[max_i]]))\n        summaries_list += [text_summary, reward_summary]\n        if self.do_iw_summaries and (not empty_replay_buffer):\n            norm_replay_weights = [w / self.experience_replay.total_weight for w in replay_weights]\n            replay_iw = self._compute_iw(replay_log_probs, replay_weights)\n            on_policy_iw = self._compute_iw(on_policy_log_probs, on_policy_weights)\n            summaries_list.append(self._iw_summary(session, replay_iw, replay_log_probs, norm_replay_weights, on_policy_iw, on_policy_log_probs))\n    return UpdateStepResult(global_step=global_step, global_npe=global_npe, summaries_list=summaries_list, gradients_dict=fetched['gradients'])"
        ]
    },
    {
        "func_name": "io_to_text",
        "original": "def io_to_text(io_case, io_type):\n    if isinstance(io_case, misc.IOTuple):\n        return ','.join([io_to_text(e, io_type) for e in io_case])\n    if io_type == misc.IOType.string:\n        return misc.tokens_to_text(io_case)\n    if io_type == misc.IOType.integer or io_type == misc.IOType.boolean:\n        if len(io_case) == 1:\n            return str(io_case[0])\n        return str(io_case)",
        "mutated": [
            "def io_to_text(io_case, io_type):\n    if False:\n        i = 10\n    if isinstance(io_case, misc.IOTuple):\n        return ','.join([io_to_text(e, io_type) for e in io_case])\n    if io_type == misc.IOType.string:\n        return misc.tokens_to_text(io_case)\n    if io_type == misc.IOType.integer or io_type == misc.IOType.boolean:\n        if len(io_case) == 1:\n            return str(io_case[0])\n        return str(io_case)",
            "def io_to_text(io_case, io_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if isinstance(io_case, misc.IOTuple):\n        return ','.join([io_to_text(e, io_type) for e in io_case])\n    if io_type == misc.IOType.string:\n        return misc.tokens_to_text(io_case)\n    if io_type == misc.IOType.integer or io_type == misc.IOType.boolean:\n        if len(io_case) == 1:\n            return str(io_case[0])\n        return str(io_case)",
            "def io_to_text(io_case, io_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if isinstance(io_case, misc.IOTuple):\n        return ','.join([io_to_text(e, io_type) for e in io_case])\n    if io_type == misc.IOType.string:\n        return misc.tokens_to_text(io_case)\n    if io_type == misc.IOType.integer or io_type == misc.IOType.boolean:\n        if len(io_case) == 1:\n            return str(io_case[0])\n        return str(io_case)",
            "def io_to_text(io_case, io_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if isinstance(io_case, misc.IOTuple):\n        return ','.join([io_to_text(e, io_type) for e in io_case])\n    if io_type == misc.IOType.string:\n        return misc.tokens_to_text(io_case)\n    if io_type == misc.IOType.integer or io_type == misc.IOType.boolean:\n        if len(io_case) == 1:\n            return str(io_case[0])\n        return str(io_case)",
            "def io_to_text(io_case, io_type):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if isinstance(io_case, misc.IOTuple):\n        return ','.join([io_to_text(e, io_type) for e in io_case])\n    if io_type == misc.IOType.string:\n        return misc.tokens_to_text(io_case)\n    if io_type == misc.IOType.integer or io_type == misc.IOType.boolean:\n        if len(io_case) == 1:\n            return str(io_case[0])\n        return str(io_case)"
        ]
    },
    {
        "func_name": "compute_rewards",
        "original": "def compute_rewards(rl_batch, batch_actions, episode_lengths, batch_size=None):\n    \"\"\"Compute rewards for each episode in the batch.\n\n  Args:\n    rl_batch: A data.RLBatch instance. This holds information about the task\n        each episode is solving, and a reward function for each episode.\n    batch_actions: Contains batch of episodes. Each sequence of actions will be\n        converted into a BF program and then scored. A numpy array of shape\n        [batch_size, max_sequence_length].\n    episode_lengths: The sequence length of each episode in the batch. Iterable\n        of length batch_size.\n    batch_size: (optional) number of programs to score. Use this to limit the\n        number of programs executed from this batch. For example, when doing\n        importance sampling some of the on-policy episodes will be discarded\n        and they should not be executed. `batch_size` can be less than or equal\n        to the size of the input batch.\n\n  Returns:\n    CodeScoreInfo namedtuple instance. This holds not just the computed rewards,\n    but additional information computed during code execution which can be used\n    for debugging and monitoring. this includes: BF code strings, test cases\n    the code was executed on, code outputs from those test cases, and reasons\n    for success or failure.\n  \"\"\"\n    code_strings = [''.join([misc.bf_int2char(a) for a in action_sequence[:l]]) for (action_sequence, l) in zip(batch_actions, episode_lengths)]\n    if batch_size is None:\n        batch_size = len(code_strings)\n    else:\n        assert batch_size <= len(code_strings)\n        code_strings = code_strings[:batch_size]\n    if isinstance(rl_batch.reward_fns, (list, tuple)):\n        assert len(rl_batch.reward_fns) >= batch_size\n        r_fn_results = [rl_batch.reward_fns[i](code_strings[i]) for i in xrange(batch_size)]\n    else:\n        r_fn_results = rl_batch.reward_fns(code_strings)\n    batch_rewards = [r.episode_rewards for r in r_fn_results]\n    total_rewards = [sum(b) for b in batch_rewards]\n    test_cases = [io_to_text(r.input_case, r.input_type) for r in r_fn_results]\n    code_outputs = [io_to_text(r.code_output, r.output_type) for r in r_fn_results]\n    reasons = [r.reason for r in r_fn_results]\n    return CodeScoreInfo(code_strings=code_strings, batch_rewards=batch_rewards, total_rewards=total_rewards, test_cases=test_cases, code_outputs=code_outputs, reasons=reasons)",
        "mutated": [
            "def compute_rewards(rl_batch, batch_actions, episode_lengths, batch_size=None):\n    if False:\n        i = 10\n    'Compute rewards for each episode in the batch.\\n\\n  Args:\\n    rl_batch: A data.RLBatch instance. This holds information about the task\\n        each episode is solving, and a reward function for each episode.\\n    batch_actions: Contains batch of episodes. Each sequence of actions will be\\n        converted into a BF program and then scored. A numpy array of shape\\n        [batch_size, max_sequence_length].\\n    episode_lengths: The sequence length of each episode in the batch. Iterable\\n        of length batch_size.\\n    batch_size: (optional) number of programs to score. Use this to limit the\\n        number of programs executed from this batch. For example, when doing\\n        importance sampling some of the on-policy episodes will be discarded\\n        and they should not be executed. `batch_size` can be less than or equal\\n        to the size of the input batch.\\n\\n  Returns:\\n    CodeScoreInfo namedtuple instance. This holds not just the computed rewards,\\n    but additional information computed during code execution which can be used\\n    for debugging and monitoring. this includes: BF code strings, test cases\\n    the code was executed on, code outputs from those test cases, and reasons\\n    for success or failure.\\n  '\n    code_strings = [''.join([misc.bf_int2char(a) for a in action_sequence[:l]]) for (action_sequence, l) in zip(batch_actions, episode_lengths)]\n    if batch_size is None:\n        batch_size = len(code_strings)\n    else:\n        assert batch_size <= len(code_strings)\n        code_strings = code_strings[:batch_size]\n    if isinstance(rl_batch.reward_fns, (list, tuple)):\n        assert len(rl_batch.reward_fns) >= batch_size\n        r_fn_results = [rl_batch.reward_fns[i](code_strings[i]) for i in xrange(batch_size)]\n    else:\n        r_fn_results = rl_batch.reward_fns(code_strings)\n    batch_rewards = [r.episode_rewards for r in r_fn_results]\n    total_rewards = [sum(b) for b in batch_rewards]\n    test_cases = [io_to_text(r.input_case, r.input_type) for r in r_fn_results]\n    code_outputs = [io_to_text(r.code_output, r.output_type) for r in r_fn_results]\n    reasons = [r.reason for r in r_fn_results]\n    return CodeScoreInfo(code_strings=code_strings, batch_rewards=batch_rewards, total_rewards=total_rewards, test_cases=test_cases, code_outputs=code_outputs, reasons=reasons)",
            "def compute_rewards(rl_batch, batch_actions, episode_lengths, batch_size=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute rewards for each episode in the batch.\\n\\n  Args:\\n    rl_batch: A data.RLBatch instance. This holds information about the task\\n        each episode is solving, and a reward function for each episode.\\n    batch_actions: Contains batch of episodes. Each sequence of actions will be\\n        converted into a BF program and then scored. A numpy array of shape\\n        [batch_size, max_sequence_length].\\n    episode_lengths: The sequence length of each episode in the batch. Iterable\\n        of length batch_size.\\n    batch_size: (optional) number of programs to score. Use this to limit the\\n        number of programs executed from this batch. For example, when doing\\n        importance sampling some of the on-policy episodes will be discarded\\n        and they should not be executed. `batch_size` can be less than or equal\\n        to the size of the input batch.\\n\\n  Returns:\\n    CodeScoreInfo namedtuple instance. This holds not just the computed rewards,\\n    but additional information computed during code execution which can be used\\n    for debugging and monitoring. this includes: BF code strings, test cases\\n    the code was executed on, code outputs from those test cases, and reasons\\n    for success or failure.\\n  '\n    code_strings = [''.join([misc.bf_int2char(a) for a in action_sequence[:l]]) for (action_sequence, l) in zip(batch_actions, episode_lengths)]\n    if batch_size is None:\n        batch_size = len(code_strings)\n    else:\n        assert batch_size <= len(code_strings)\n        code_strings = code_strings[:batch_size]\n    if isinstance(rl_batch.reward_fns, (list, tuple)):\n        assert len(rl_batch.reward_fns) >= batch_size\n        r_fn_results = [rl_batch.reward_fns[i](code_strings[i]) for i in xrange(batch_size)]\n    else:\n        r_fn_results = rl_batch.reward_fns(code_strings)\n    batch_rewards = [r.episode_rewards for r in r_fn_results]\n    total_rewards = [sum(b) for b in batch_rewards]\n    test_cases = [io_to_text(r.input_case, r.input_type) for r in r_fn_results]\n    code_outputs = [io_to_text(r.code_output, r.output_type) for r in r_fn_results]\n    reasons = [r.reason for r in r_fn_results]\n    return CodeScoreInfo(code_strings=code_strings, batch_rewards=batch_rewards, total_rewards=total_rewards, test_cases=test_cases, code_outputs=code_outputs, reasons=reasons)",
            "def compute_rewards(rl_batch, batch_actions, episode_lengths, batch_size=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute rewards for each episode in the batch.\\n\\n  Args:\\n    rl_batch: A data.RLBatch instance. This holds information about the task\\n        each episode is solving, and a reward function for each episode.\\n    batch_actions: Contains batch of episodes. Each sequence of actions will be\\n        converted into a BF program and then scored. A numpy array of shape\\n        [batch_size, max_sequence_length].\\n    episode_lengths: The sequence length of each episode in the batch. Iterable\\n        of length batch_size.\\n    batch_size: (optional) number of programs to score. Use this to limit the\\n        number of programs executed from this batch. For example, when doing\\n        importance sampling some of the on-policy episodes will be discarded\\n        and they should not be executed. `batch_size` can be less than or equal\\n        to the size of the input batch.\\n\\n  Returns:\\n    CodeScoreInfo namedtuple instance. This holds not just the computed rewards,\\n    but additional information computed during code execution which can be used\\n    for debugging and monitoring. this includes: BF code strings, test cases\\n    the code was executed on, code outputs from those test cases, and reasons\\n    for success or failure.\\n  '\n    code_strings = [''.join([misc.bf_int2char(a) for a in action_sequence[:l]]) for (action_sequence, l) in zip(batch_actions, episode_lengths)]\n    if batch_size is None:\n        batch_size = len(code_strings)\n    else:\n        assert batch_size <= len(code_strings)\n        code_strings = code_strings[:batch_size]\n    if isinstance(rl_batch.reward_fns, (list, tuple)):\n        assert len(rl_batch.reward_fns) >= batch_size\n        r_fn_results = [rl_batch.reward_fns[i](code_strings[i]) for i in xrange(batch_size)]\n    else:\n        r_fn_results = rl_batch.reward_fns(code_strings)\n    batch_rewards = [r.episode_rewards for r in r_fn_results]\n    total_rewards = [sum(b) for b in batch_rewards]\n    test_cases = [io_to_text(r.input_case, r.input_type) for r in r_fn_results]\n    code_outputs = [io_to_text(r.code_output, r.output_type) for r in r_fn_results]\n    reasons = [r.reason for r in r_fn_results]\n    return CodeScoreInfo(code_strings=code_strings, batch_rewards=batch_rewards, total_rewards=total_rewards, test_cases=test_cases, code_outputs=code_outputs, reasons=reasons)",
            "def compute_rewards(rl_batch, batch_actions, episode_lengths, batch_size=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute rewards for each episode in the batch.\\n\\n  Args:\\n    rl_batch: A data.RLBatch instance. This holds information about the task\\n        each episode is solving, and a reward function for each episode.\\n    batch_actions: Contains batch of episodes. Each sequence of actions will be\\n        converted into a BF program and then scored. A numpy array of shape\\n        [batch_size, max_sequence_length].\\n    episode_lengths: The sequence length of each episode in the batch. Iterable\\n        of length batch_size.\\n    batch_size: (optional) number of programs to score. Use this to limit the\\n        number of programs executed from this batch. For example, when doing\\n        importance sampling some of the on-policy episodes will be discarded\\n        and they should not be executed. `batch_size` can be less than or equal\\n        to the size of the input batch.\\n\\n  Returns:\\n    CodeScoreInfo namedtuple instance. This holds not just the computed rewards,\\n    but additional information computed during code execution which can be used\\n    for debugging and monitoring. this includes: BF code strings, test cases\\n    the code was executed on, code outputs from those test cases, and reasons\\n    for success or failure.\\n  '\n    code_strings = [''.join([misc.bf_int2char(a) for a in action_sequence[:l]]) for (action_sequence, l) in zip(batch_actions, episode_lengths)]\n    if batch_size is None:\n        batch_size = len(code_strings)\n    else:\n        assert batch_size <= len(code_strings)\n        code_strings = code_strings[:batch_size]\n    if isinstance(rl_batch.reward_fns, (list, tuple)):\n        assert len(rl_batch.reward_fns) >= batch_size\n        r_fn_results = [rl_batch.reward_fns[i](code_strings[i]) for i in xrange(batch_size)]\n    else:\n        r_fn_results = rl_batch.reward_fns(code_strings)\n    batch_rewards = [r.episode_rewards for r in r_fn_results]\n    total_rewards = [sum(b) for b in batch_rewards]\n    test_cases = [io_to_text(r.input_case, r.input_type) for r in r_fn_results]\n    code_outputs = [io_to_text(r.code_output, r.output_type) for r in r_fn_results]\n    reasons = [r.reason for r in r_fn_results]\n    return CodeScoreInfo(code_strings=code_strings, batch_rewards=batch_rewards, total_rewards=total_rewards, test_cases=test_cases, code_outputs=code_outputs, reasons=reasons)",
            "def compute_rewards(rl_batch, batch_actions, episode_lengths, batch_size=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute rewards for each episode in the batch.\\n\\n  Args:\\n    rl_batch: A data.RLBatch instance. This holds information about the task\\n        each episode is solving, and a reward function for each episode.\\n    batch_actions: Contains batch of episodes. Each sequence of actions will be\\n        converted into a BF program and then scored. A numpy array of shape\\n        [batch_size, max_sequence_length].\\n    episode_lengths: The sequence length of each episode in the batch. Iterable\\n        of length batch_size.\\n    batch_size: (optional) number of programs to score. Use this to limit the\\n        number of programs executed from this batch. For example, when doing\\n        importance sampling some of the on-policy episodes will be discarded\\n        and they should not be executed. `batch_size` can be less than or equal\\n        to the size of the input batch.\\n\\n  Returns:\\n    CodeScoreInfo namedtuple instance. This holds not just the computed rewards,\\n    but additional information computed during code execution which can be used\\n    for debugging and monitoring. this includes: BF code strings, test cases\\n    the code was executed on, code outputs from those test cases, and reasons\\n    for success or failure.\\n  '\n    code_strings = [''.join([misc.bf_int2char(a) for a in action_sequence[:l]]) for (action_sequence, l) in zip(batch_actions, episode_lengths)]\n    if batch_size is None:\n        batch_size = len(code_strings)\n    else:\n        assert batch_size <= len(code_strings)\n        code_strings = code_strings[:batch_size]\n    if isinstance(rl_batch.reward_fns, (list, tuple)):\n        assert len(rl_batch.reward_fns) >= batch_size\n        r_fn_results = [rl_batch.reward_fns[i](code_strings[i]) for i in xrange(batch_size)]\n    else:\n        r_fn_results = rl_batch.reward_fns(code_strings)\n    batch_rewards = [r.episode_rewards for r in r_fn_results]\n    total_rewards = [sum(b) for b in batch_rewards]\n    test_cases = [io_to_text(r.input_case, r.input_type) for r in r_fn_results]\n    code_outputs = [io_to_text(r.code_output, r.output_type) for r in r_fn_results]\n    reasons = [r.reason for r in r_fn_results]\n    return CodeScoreInfo(code_strings=code_strings, batch_rewards=batch_rewards, total_rewards=total_rewards, test_cases=test_cases, code_outputs=code_outputs, reasons=reasons)"
        ]
    },
    {
        "func_name": "process_episodes",
        "original": "def process_episodes(batch_rewards, episode_lengths, a2c=False, baselines=None, batch_values=None):\n    \"\"\"Compute REINFORCE targets.\n\n  REINFORCE here takes the form:\n  grad_t = grad[log(pi(a_t|c_t))*target_t]\n  where c_t is context: i.e. RNN state or environment state (or both).\n\n  Two types of targets are supported:\n  1) Advantage actor critic (a2c).\n  2) Vanilla REINFORCE with baseline.\n\n  Args:\n    batch_rewards: Rewards received in each episode in the batch. A numpy array\n        of shape [batch_size, max_sequence_length]. Note, these are per-timestep\n        rewards, not total reward.\n    episode_lengths: Length of each episode. An iterable of length batch_size.\n    a2c: A bool. Whether to compute a2c targets (True) or vanilla targets\n        (False).\n    baselines: If a2c is False, provide baselines for each timestep. This is a\n        list (or indexable container) of length max_time. Note: baselines are\n        shared across all episodes, which is why there is no batch dimension.\n        It is up to the caller to update baselines accordingly.\n    batch_values: If a2c is True, provide values computed by a value estimator.\n        A numpy array of shape [batch_size, max_sequence_length].\n\n  Returns:\n    batch_targets: REINFORCE targets for each episode and timestep. A numpy\n        array of shape [batch_size, max_sequence_length].\n    batch_returns: Returns computed for each episode and timestep. This is for\n        reference, and is not used in the REINFORCE gradient update (but was\n        used to compute the targets). A numpy array of shape\n        [batch_size, max_sequence_length].\n  \"\"\"\n    num_programs = len(batch_rewards)\n    assert num_programs <= len(episode_lengths)\n    batch_returns = [None] * num_programs\n    batch_targets = [None] * num_programs\n    for i in xrange(num_programs):\n        episode_length = episode_lengths[i]\n        assert len(batch_rewards[i]) == episode_length\n        if a2c:\n            assert batch_values is not None\n            episode_values = batch_values[i, :episode_length]\n            episode_rewards = batch_rewards[i]\n            (emp_val, gen_adv) = rollout_lib.discounted_advantage_and_rewards(episode_rewards, episode_values, gamma=1.0, lambda_=1.0)\n            batch_returns[i] = emp_val\n            batch_targets[i] = gen_adv\n        else:\n            assert baselines is not None\n            empirical_returns = rollout_lib.discount(batch_rewards[i], gamma=1.0)\n            targets = [None] * episode_length\n            for j in xrange(episode_length):\n                targets[j] = empirical_returns[j] - baselines[j]\n            batch_returns[i] = empirical_returns\n            batch_targets[i] = targets\n    batch_returns = utils.stack_pad(batch_returns, 0)\n    if num_programs:\n        batch_targets = utils.stack_pad(batch_targets, 0)\n    else:\n        batch_targets = np.array([], dtype=np.float32)\n    return (batch_targets, batch_returns)",
        "mutated": [
            "def process_episodes(batch_rewards, episode_lengths, a2c=False, baselines=None, batch_values=None):\n    if False:\n        i = 10\n    'Compute REINFORCE targets.\\n\\n  REINFORCE here takes the form:\\n  grad_t = grad[log(pi(a_t|c_t))*target_t]\\n  where c_t is context: i.e. RNN state or environment state (or both).\\n\\n  Two types of targets are supported:\\n  1) Advantage actor critic (a2c).\\n  2) Vanilla REINFORCE with baseline.\\n\\n  Args:\\n    batch_rewards: Rewards received in each episode in the batch. A numpy array\\n        of shape [batch_size, max_sequence_length]. Note, these are per-timestep\\n        rewards, not total reward.\\n    episode_lengths: Length of each episode. An iterable of length batch_size.\\n    a2c: A bool. Whether to compute a2c targets (True) or vanilla targets\\n        (False).\\n    baselines: If a2c is False, provide baselines for each timestep. This is a\\n        list (or indexable container) of length max_time. Note: baselines are\\n        shared across all episodes, which is why there is no batch dimension.\\n        It is up to the caller to update baselines accordingly.\\n    batch_values: If a2c is True, provide values computed by a value estimator.\\n        A numpy array of shape [batch_size, max_sequence_length].\\n\\n  Returns:\\n    batch_targets: REINFORCE targets for each episode and timestep. A numpy\\n        array of shape [batch_size, max_sequence_length].\\n    batch_returns: Returns computed for each episode and timestep. This is for\\n        reference, and is not used in the REINFORCE gradient update (but was\\n        used to compute the targets). A numpy array of shape\\n        [batch_size, max_sequence_length].\\n  '\n    num_programs = len(batch_rewards)\n    assert num_programs <= len(episode_lengths)\n    batch_returns = [None] * num_programs\n    batch_targets = [None] * num_programs\n    for i in xrange(num_programs):\n        episode_length = episode_lengths[i]\n        assert len(batch_rewards[i]) == episode_length\n        if a2c:\n            assert batch_values is not None\n            episode_values = batch_values[i, :episode_length]\n            episode_rewards = batch_rewards[i]\n            (emp_val, gen_adv) = rollout_lib.discounted_advantage_and_rewards(episode_rewards, episode_values, gamma=1.0, lambda_=1.0)\n            batch_returns[i] = emp_val\n            batch_targets[i] = gen_adv\n        else:\n            assert baselines is not None\n            empirical_returns = rollout_lib.discount(batch_rewards[i], gamma=1.0)\n            targets = [None] * episode_length\n            for j in xrange(episode_length):\n                targets[j] = empirical_returns[j] - baselines[j]\n            batch_returns[i] = empirical_returns\n            batch_targets[i] = targets\n    batch_returns = utils.stack_pad(batch_returns, 0)\n    if num_programs:\n        batch_targets = utils.stack_pad(batch_targets, 0)\n    else:\n        batch_targets = np.array([], dtype=np.float32)\n    return (batch_targets, batch_returns)",
            "def process_episodes(batch_rewards, episode_lengths, a2c=False, baselines=None, batch_values=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Compute REINFORCE targets.\\n\\n  REINFORCE here takes the form:\\n  grad_t = grad[log(pi(a_t|c_t))*target_t]\\n  where c_t is context: i.e. RNN state or environment state (or both).\\n\\n  Two types of targets are supported:\\n  1) Advantage actor critic (a2c).\\n  2) Vanilla REINFORCE with baseline.\\n\\n  Args:\\n    batch_rewards: Rewards received in each episode in the batch. A numpy array\\n        of shape [batch_size, max_sequence_length]. Note, these are per-timestep\\n        rewards, not total reward.\\n    episode_lengths: Length of each episode. An iterable of length batch_size.\\n    a2c: A bool. Whether to compute a2c targets (True) or vanilla targets\\n        (False).\\n    baselines: If a2c is False, provide baselines for each timestep. This is a\\n        list (or indexable container) of length max_time. Note: baselines are\\n        shared across all episodes, which is why there is no batch dimension.\\n        It is up to the caller to update baselines accordingly.\\n    batch_values: If a2c is True, provide values computed by a value estimator.\\n        A numpy array of shape [batch_size, max_sequence_length].\\n\\n  Returns:\\n    batch_targets: REINFORCE targets for each episode and timestep. A numpy\\n        array of shape [batch_size, max_sequence_length].\\n    batch_returns: Returns computed for each episode and timestep. This is for\\n        reference, and is not used in the REINFORCE gradient update (but was\\n        used to compute the targets). A numpy array of shape\\n        [batch_size, max_sequence_length].\\n  '\n    num_programs = len(batch_rewards)\n    assert num_programs <= len(episode_lengths)\n    batch_returns = [None] * num_programs\n    batch_targets = [None] * num_programs\n    for i in xrange(num_programs):\n        episode_length = episode_lengths[i]\n        assert len(batch_rewards[i]) == episode_length\n        if a2c:\n            assert batch_values is not None\n            episode_values = batch_values[i, :episode_length]\n            episode_rewards = batch_rewards[i]\n            (emp_val, gen_adv) = rollout_lib.discounted_advantage_and_rewards(episode_rewards, episode_values, gamma=1.0, lambda_=1.0)\n            batch_returns[i] = emp_val\n            batch_targets[i] = gen_adv\n        else:\n            assert baselines is not None\n            empirical_returns = rollout_lib.discount(batch_rewards[i], gamma=1.0)\n            targets = [None] * episode_length\n            for j in xrange(episode_length):\n                targets[j] = empirical_returns[j] - baselines[j]\n            batch_returns[i] = empirical_returns\n            batch_targets[i] = targets\n    batch_returns = utils.stack_pad(batch_returns, 0)\n    if num_programs:\n        batch_targets = utils.stack_pad(batch_targets, 0)\n    else:\n        batch_targets = np.array([], dtype=np.float32)\n    return (batch_targets, batch_returns)",
            "def process_episodes(batch_rewards, episode_lengths, a2c=False, baselines=None, batch_values=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Compute REINFORCE targets.\\n\\n  REINFORCE here takes the form:\\n  grad_t = grad[log(pi(a_t|c_t))*target_t]\\n  where c_t is context: i.e. RNN state or environment state (or both).\\n\\n  Two types of targets are supported:\\n  1) Advantage actor critic (a2c).\\n  2) Vanilla REINFORCE with baseline.\\n\\n  Args:\\n    batch_rewards: Rewards received in each episode in the batch. A numpy array\\n        of shape [batch_size, max_sequence_length]. Note, these are per-timestep\\n        rewards, not total reward.\\n    episode_lengths: Length of each episode. An iterable of length batch_size.\\n    a2c: A bool. Whether to compute a2c targets (True) or vanilla targets\\n        (False).\\n    baselines: If a2c is False, provide baselines for each timestep. This is a\\n        list (or indexable container) of length max_time. Note: baselines are\\n        shared across all episodes, which is why there is no batch dimension.\\n        It is up to the caller to update baselines accordingly.\\n    batch_values: If a2c is True, provide values computed by a value estimator.\\n        A numpy array of shape [batch_size, max_sequence_length].\\n\\n  Returns:\\n    batch_targets: REINFORCE targets for each episode and timestep. A numpy\\n        array of shape [batch_size, max_sequence_length].\\n    batch_returns: Returns computed for each episode and timestep. This is for\\n        reference, and is not used in the REINFORCE gradient update (but was\\n        used to compute the targets). A numpy array of shape\\n        [batch_size, max_sequence_length].\\n  '\n    num_programs = len(batch_rewards)\n    assert num_programs <= len(episode_lengths)\n    batch_returns = [None] * num_programs\n    batch_targets = [None] * num_programs\n    for i in xrange(num_programs):\n        episode_length = episode_lengths[i]\n        assert len(batch_rewards[i]) == episode_length\n        if a2c:\n            assert batch_values is not None\n            episode_values = batch_values[i, :episode_length]\n            episode_rewards = batch_rewards[i]\n            (emp_val, gen_adv) = rollout_lib.discounted_advantage_and_rewards(episode_rewards, episode_values, gamma=1.0, lambda_=1.0)\n            batch_returns[i] = emp_val\n            batch_targets[i] = gen_adv\n        else:\n            assert baselines is not None\n            empirical_returns = rollout_lib.discount(batch_rewards[i], gamma=1.0)\n            targets = [None] * episode_length\n            for j in xrange(episode_length):\n                targets[j] = empirical_returns[j] - baselines[j]\n            batch_returns[i] = empirical_returns\n            batch_targets[i] = targets\n    batch_returns = utils.stack_pad(batch_returns, 0)\n    if num_programs:\n        batch_targets = utils.stack_pad(batch_targets, 0)\n    else:\n        batch_targets = np.array([], dtype=np.float32)\n    return (batch_targets, batch_returns)",
            "def process_episodes(batch_rewards, episode_lengths, a2c=False, baselines=None, batch_values=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Compute REINFORCE targets.\\n\\n  REINFORCE here takes the form:\\n  grad_t = grad[log(pi(a_t|c_t))*target_t]\\n  where c_t is context: i.e. RNN state or environment state (or both).\\n\\n  Two types of targets are supported:\\n  1) Advantage actor critic (a2c).\\n  2) Vanilla REINFORCE with baseline.\\n\\n  Args:\\n    batch_rewards: Rewards received in each episode in the batch. A numpy array\\n        of shape [batch_size, max_sequence_length]. Note, these are per-timestep\\n        rewards, not total reward.\\n    episode_lengths: Length of each episode. An iterable of length batch_size.\\n    a2c: A bool. Whether to compute a2c targets (True) or vanilla targets\\n        (False).\\n    baselines: If a2c is False, provide baselines for each timestep. This is a\\n        list (or indexable container) of length max_time. Note: baselines are\\n        shared across all episodes, which is why there is no batch dimension.\\n        It is up to the caller to update baselines accordingly.\\n    batch_values: If a2c is True, provide values computed by a value estimator.\\n        A numpy array of shape [batch_size, max_sequence_length].\\n\\n  Returns:\\n    batch_targets: REINFORCE targets for each episode and timestep. A numpy\\n        array of shape [batch_size, max_sequence_length].\\n    batch_returns: Returns computed for each episode and timestep. This is for\\n        reference, and is not used in the REINFORCE gradient update (but was\\n        used to compute the targets). A numpy array of shape\\n        [batch_size, max_sequence_length].\\n  '\n    num_programs = len(batch_rewards)\n    assert num_programs <= len(episode_lengths)\n    batch_returns = [None] * num_programs\n    batch_targets = [None] * num_programs\n    for i in xrange(num_programs):\n        episode_length = episode_lengths[i]\n        assert len(batch_rewards[i]) == episode_length\n        if a2c:\n            assert batch_values is not None\n            episode_values = batch_values[i, :episode_length]\n            episode_rewards = batch_rewards[i]\n            (emp_val, gen_adv) = rollout_lib.discounted_advantage_and_rewards(episode_rewards, episode_values, gamma=1.0, lambda_=1.0)\n            batch_returns[i] = emp_val\n            batch_targets[i] = gen_adv\n        else:\n            assert baselines is not None\n            empirical_returns = rollout_lib.discount(batch_rewards[i], gamma=1.0)\n            targets = [None] * episode_length\n            for j in xrange(episode_length):\n                targets[j] = empirical_returns[j] - baselines[j]\n            batch_returns[i] = empirical_returns\n            batch_targets[i] = targets\n    batch_returns = utils.stack_pad(batch_returns, 0)\n    if num_programs:\n        batch_targets = utils.stack_pad(batch_targets, 0)\n    else:\n        batch_targets = np.array([], dtype=np.float32)\n    return (batch_targets, batch_returns)",
            "def process_episodes(batch_rewards, episode_lengths, a2c=False, baselines=None, batch_values=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Compute REINFORCE targets.\\n\\n  REINFORCE here takes the form:\\n  grad_t = grad[log(pi(a_t|c_t))*target_t]\\n  where c_t is context: i.e. RNN state or environment state (or both).\\n\\n  Two types of targets are supported:\\n  1) Advantage actor critic (a2c).\\n  2) Vanilla REINFORCE with baseline.\\n\\n  Args:\\n    batch_rewards: Rewards received in each episode in the batch. A numpy array\\n        of shape [batch_size, max_sequence_length]. Note, these are per-timestep\\n        rewards, not total reward.\\n    episode_lengths: Length of each episode. An iterable of length batch_size.\\n    a2c: A bool. Whether to compute a2c targets (True) or vanilla targets\\n        (False).\\n    baselines: If a2c is False, provide baselines for each timestep. This is a\\n        list (or indexable container) of length max_time. Note: baselines are\\n        shared across all episodes, which is why there is no batch dimension.\\n        It is up to the caller to update baselines accordingly.\\n    batch_values: If a2c is True, provide values computed by a value estimator.\\n        A numpy array of shape [batch_size, max_sequence_length].\\n\\n  Returns:\\n    batch_targets: REINFORCE targets for each episode and timestep. A numpy\\n        array of shape [batch_size, max_sequence_length].\\n    batch_returns: Returns computed for each episode and timestep. This is for\\n        reference, and is not used in the REINFORCE gradient update (but was\\n        used to compute the targets). A numpy array of shape\\n        [batch_size, max_sequence_length].\\n  '\n    num_programs = len(batch_rewards)\n    assert num_programs <= len(episode_lengths)\n    batch_returns = [None] * num_programs\n    batch_targets = [None] * num_programs\n    for i in xrange(num_programs):\n        episode_length = episode_lengths[i]\n        assert len(batch_rewards[i]) == episode_length\n        if a2c:\n            assert batch_values is not None\n            episode_values = batch_values[i, :episode_length]\n            episode_rewards = batch_rewards[i]\n            (emp_val, gen_adv) = rollout_lib.discounted_advantage_and_rewards(episode_rewards, episode_values, gamma=1.0, lambda_=1.0)\n            batch_returns[i] = emp_val\n            batch_targets[i] = gen_adv\n        else:\n            assert baselines is not None\n            empirical_returns = rollout_lib.discount(batch_rewards[i], gamma=1.0)\n            targets = [None] * episode_length\n            for j in xrange(episode_length):\n                targets[j] = empirical_returns[j] - baselines[j]\n            batch_returns[i] = empirical_returns\n            batch_targets[i] = targets\n    batch_returns = utils.stack_pad(batch_returns, 0)\n    if num_programs:\n        batch_targets = utils.stack_pad(batch_targets, 0)\n    else:\n        batch_targets = np.array([], dtype=np.float32)\n    return (batch_targets, batch_returns)"
        ]
    }
]