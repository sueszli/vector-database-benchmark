[
    {
        "func_name": "dispersion_poisson",
        "original": "def dispersion_poisson(results):\n    \"\"\"Score/LM type tests for Poisson variance assumptions\n\n    .. deprecated:: 0.14\n\n       dispersion_poisson moved to discrete._diagnostic_count\n\n    Null Hypothesis is\n\n    H0: var(y) = E(y) and assuming E(y) is correctly specified\n    H1: var(y) ~= E(y)\n\n    The tests are based on the constrained model, i.e. the Poisson model.\n    The tests differ in their assumed alternatives, and in their maintained\n    assumptions.\n\n    Parameters\n    ----------\n    results : Poisson results instance\n        This can be a results instance for either a discrete Poisson or a GLM\n        with family Poisson.\n\n    Returns\n    -------\n    res : ndarray, shape (7, 2)\n       each row contains the test statistic and p-value for one of the 7 tests\n       computed here.\n    description : 2-D list of strings\n       Each test has two strings a descriptive name and a string for the\n       alternative hypothesis.\n    \"\"\"\n    msg = 'dispersion_poisson here is deprecated, use the version in discrete._diagnostic_count'\n    warnings.warn(msg, FutureWarning)\n    from statsmodels.discrete._diagnostics_count import test_poisson_dispersion\n    return test_poisson_dispersion(results, _old=True)",
        "mutated": [
            "def dispersion_poisson(results):\n    if False:\n        i = 10\n    'Score/LM type tests for Poisson variance assumptions\\n\\n    .. deprecated:: 0.14\\n\\n       dispersion_poisson moved to discrete._diagnostic_count\\n\\n    Null Hypothesis is\\n\\n    H0: var(y) = E(y) and assuming E(y) is correctly specified\\n    H1: var(y) ~= E(y)\\n\\n    The tests are based on the constrained model, i.e. the Poisson model.\\n    The tests differ in their assumed alternatives, and in their maintained\\n    assumptions.\\n\\n    Parameters\\n    ----------\\n    results : Poisson results instance\\n        This can be a results instance for either a discrete Poisson or a GLM\\n        with family Poisson.\\n\\n    Returns\\n    -------\\n    res : ndarray, shape (7, 2)\\n       each row contains the test statistic and p-value for one of the 7 tests\\n       computed here.\\n    description : 2-D list of strings\\n       Each test has two strings a descriptive name and a string for the\\n       alternative hypothesis.\\n    '\n    msg = 'dispersion_poisson here is deprecated, use the version in discrete._diagnostic_count'\n    warnings.warn(msg, FutureWarning)\n    from statsmodels.discrete._diagnostics_count import test_poisson_dispersion\n    return test_poisson_dispersion(results, _old=True)",
            "def dispersion_poisson(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Score/LM type tests for Poisson variance assumptions\\n\\n    .. deprecated:: 0.14\\n\\n       dispersion_poisson moved to discrete._diagnostic_count\\n\\n    Null Hypothesis is\\n\\n    H0: var(y) = E(y) and assuming E(y) is correctly specified\\n    H1: var(y) ~= E(y)\\n\\n    The tests are based on the constrained model, i.e. the Poisson model.\\n    The tests differ in their assumed alternatives, and in their maintained\\n    assumptions.\\n\\n    Parameters\\n    ----------\\n    results : Poisson results instance\\n        This can be a results instance for either a discrete Poisson or a GLM\\n        with family Poisson.\\n\\n    Returns\\n    -------\\n    res : ndarray, shape (7, 2)\\n       each row contains the test statistic and p-value for one of the 7 tests\\n       computed here.\\n    description : 2-D list of strings\\n       Each test has two strings a descriptive name and a string for the\\n       alternative hypothesis.\\n    '\n    msg = 'dispersion_poisson here is deprecated, use the version in discrete._diagnostic_count'\n    warnings.warn(msg, FutureWarning)\n    from statsmodels.discrete._diagnostics_count import test_poisson_dispersion\n    return test_poisson_dispersion(results, _old=True)",
            "def dispersion_poisson(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Score/LM type tests for Poisson variance assumptions\\n\\n    .. deprecated:: 0.14\\n\\n       dispersion_poisson moved to discrete._diagnostic_count\\n\\n    Null Hypothesis is\\n\\n    H0: var(y) = E(y) and assuming E(y) is correctly specified\\n    H1: var(y) ~= E(y)\\n\\n    The tests are based on the constrained model, i.e. the Poisson model.\\n    The tests differ in their assumed alternatives, and in their maintained\\n    assumptions.\\n\\n    Parameters\\n    ----------\\n    results : Poisson results instance\\n        This can be a results instance for either a discrete Poisson or a GLM\\n        with family Poisson.\\n\\n    Returns\\n    -------\\n    res : ndarray, shape (7, 2)\\n       each row contains the test statistic and p-value for one of the 7 tests\\n       computed here.\\n    description : 2-D list of strings\\n       Each test has two strings a descriptive name and a string for the\\n       alternative hypothesis.\\n    '\n    msg = 'dispersion_poisson here is deprecated, use the version in discrete._diagnostic_count'\n    warnings.warn(msg, FutureWarning)\n    from statsmodels.discrete._diagnostics_count import test_poisson_dispersion\n    return test_poisson_dispersion(results, _old=True)",
            "def dispersion_poisson(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Score/LM type tests for Poisson variance assumptions\\n\\n    .. deprecated:: 0.14\\n\\n       dispersion_poisson moved to discrete._diagnostic_count\\n\\n    Null Hypothesis is\\n\\n    H0: var(y) = E(y) and assuming E(y) is correctly specified\\n    H1: var(y) ~= E(y)\\n\\n    The tests are based on the constrained model, i.e. the Poisson model.\\n    The tests differ in their assumed alternatives, and in their maintained\\n    assumptions.\\n\\n    Parameters\\n    ----------\\n    results : Poisson results instance\\n        This can be a results instance for either a discrete Poisson or a GLM\\n        with family Poisson.\\n\\n    Returns\\n    -------\\n    res : ndarray, shape (7, 2)\\n       each row contains the test statistic and p-value for one of the 7 tests\\n       computed here.\\n    description : 2-D list of strings\\n       Each test has two strings a descriptive name and a string for the\\n       alternative hypothesis.\\n    '\n    msg = 'dispersion_poisson here is deprecated, use the version in discrete._diagnostic_count'\n    warnings.warn(msg, FutureWarning)\n    from statsmodels.discrete._diagnostics_count import test_poisson_dispersion\n    return test_poisson_dispersion(results, _old=True)",
            "def dispersion_poisson(results):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Score/LM type tests for Poisson variance assumptions\\n\\n    .. deprecated:: 0.14\\n\\n       dispersion_poisson moved to discrete._diagnostic_count\\n\\n    Null Hypothesis is\\n\\n    H0: var(y) = E(y) and assuming E(y) is correctly specified\\n    H1: var(y) ~= E(y)\\n\\n    The tests are based on the constrained model, i.e. the Poisson model.\\n    The tests differ in their assumed alternatives, and in their maintained\\n    assumptions.\\n\\n    Parameters\\n    ----------\\n    results : Poisson results instance\\n        This can be a results instance for either a discrete Poisson or a GLM\\n        with family Poisson.\\n\\n    Returns\\n    -------\\n    res : ndarray, shape (7, 2)\\n       each row contains the test statistic and p-value for one of the 7 tests\\n       computed here.\\n    description : 2-D list of strings\\n       Each test has two strings a descriptive name and a string for the\\n       alternative hypothesis.\\n    '\n    msg = 'dispersion_poisson here is deprecated, use the version in discrete._diagnostic_count'\n    warnings.warn(msg, FutureWarning)\n    from statsmodels.discrete._diagnostics_count import test_poisson_dispersion\n    return test_poisson_dispersion(results, _old=True)"
        ]
    },
    {
        "func_name": "dispersion_poisson_generic",
        "original": "def dispersion_poisson_generic(results, exog_new_test, exog_new_control=None, include_score=False, use_endog=True, cov_type='HC3', cov_kwds=None, use_t=False):\n    \"\"\"A variable addition test for the variance function\n\n    .. deprecated:: 0.14\n\n       dispersion_poisson_generic moved to discrete._diagnostic_count\n\n    This uses an artificial regression to calculate a variant of an LM or\n    generalized score test for the specification of the variance assumption\n    in a Poisson model. The performed test is a Wald test on the coefficients\n    of the `exog_new_test`.\n\n    Warning: insufficiently tested, especially for options\n    \"\"\"\n    msg = 'dispersion_poisson_generic here is deprecated, use the version in discrete._diagnostic_count'\n    warnings.warn(msg, FutureWarning)\n    from statsmodels.discrete._diagnostics_count import _test_poisson_dispersion_generic\n    res_test = _test_poisson_dispersion_generic(results, exog_new_test, exog_new_control=exog_new_control, include_score=include_score, use_endog=use_endog, cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\n    return res_test",
        "mutated": [
            "def dispersion_poisson_generic(results, exog_new_test, exog_new_control=None, include_score=False, use_endog=True, cov_type='HC3', cov_kwds=None, use_t=False):\n    if False:\n        i = 10\n    'A variable addition test for the variance function\\n\\n    .. deprecated:: 0.14\\n\\n       dispersion_poisson_generic moved to discrete._diagnostic_count\\n\\n    This uses an artificial regression to calculate a variant of an LM or\\n    generalized score test for the specification of the variance assumption\\n    in a Poisson model. The performed test is a Wald test on the coefficients\\n    of the `exog_new_test`.\\n\\n    Warning: insufficiently tested, especially for options\\n    '\n    msg = 'dispersion_poisson_generic here is deprecated, use the version in discrete._diagnostic_count'\n    warnings.warn(msg, FutureWarning)\n    from statsmodels.discrete._diagnostics_count import _test_poisson_dispersion_generic\n    res_test = _test_poisson_dispersion_generic(results, exog_new_test, exog_new_control=exog_new_control, include_score=include_score, use_endog=use_endog, cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\n    return res_test",
            "def dispersion_poisson_generic(results, exog_new_test, exog_new_control=None, include_score=False, use_endog=True, cov_type='HC3', cov_kwds=None, use_t=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'A variable addition test for the variance function\\n\\n    .. deprecated:: 0.14\\n\\n       dispersion_poisson_generic moved to discrete._diagnostic_count\\n\\n    This uses an artificial regression to calculate a variant of an LM or\\n    generalized score test for the specification of the variance assumption\\n    in a Poisson model. The performed test is a Wald test on the coefficients\\n    of the `exog_new_test`.\\n\\n    Warning: insufficiently tested, especially for options\\n    '\n    msg = 'dispersion_poisson_generic here is deprecated, use the version in discrete._diagnostic_count'\n    warnings.warn(msg, FutureWarning)\n    from statsmodels.discrete._diagnostics_count import _test_poisson_dispersion_generic\n    res_test = _test_poisson_dispersion_generic(results, exog_new_test, exog_new_control=exog_new_control, include_score=include_score, use_endog=use_endog, cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\n    return res_test",
            "def dispersion_poisson_generic(results, exog_new_test, exog_new_control=None, include_score=False, use_endog=True, cov_type='HC3', cov_kwds=None, use_t=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'A variable addition test for the variance function\\n\\n    .. deprecated:: 0.14\\n\\n       dispersion_poisson_generic moved to discrete._diagnostic_count\\n\\n    This uses an artificial regression to calculate a variant of an LM or\\n    generalized score test for the specification of the variance assumption\\n    in a Poisson model. The performed test is a Wald test on the coefficients\\n    of the `exog_new_test`.\\n\\n    Warning: insufficiently tested, especially for options\\n    '\n    msg = 'dispersion_poisson_generic here is deprecated, use the version in discrete._diagnostic_count'\n    warnings.warn(msg, FutureWarning)\n    from statsmodels.discrete._diagnostics_count import _test_poisson_dispersion_generic\n    res_test = _test_poisson_dispersion_generic(results, exog_new_test, exog_new_control=exog_new_control, include_score=include_score, use_endog=use_endog, cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\n    return res_test",
            "def dispersion_poisson_generic(results, exog_new_test, exog_new_control=None, include_score=False, use_endog=True, cov_type='HC3', cov_kwds=None, use_t=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'A variable addition test for the variance function\\n\\n    .. deprecated:: 0.14\\n\\n       dispersion_poisson_generic moved to discrete._diagnostic_count\\n\\n    This uses an artificial regression to calculate a variant of an LM or\\n    generalized score test for the specification of the variance assumption\\n    in a Poisson model. The performed test is a Wald test on the coefficients\\n    of the `exog_new_test`.\\n\\n    Warning: insufficiently tested, especially for options\\n    '\n    msg = 'dispersion_poisson_generic here is deprecated, use the version in discrete._diagnostic_count'\n    warnings.warn(msg, FutureWarning)\n    from statsmodels.discrete._diagnostics_count import _test_poisson_dispersion_generic\n    res_test = _test_poisson_dispersion_generic(results, exog_new_test, exog_new_control=exog_new_control, include_score=include_score, use_endog=use_endog, cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\n    return res_test",
            "def dispersion_poisson_generic(results, exog_new_test, exog_new_control=None, include_score=False, use_endog=True, cov_type='HC3', cov_kwds=None, use_t=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'A variable addition test for the variance function\\n\\n    .. deprecated:: 0.14\\n\\n       dispersion_poisson_generic moved to discrete._diagnostic_count\\n\\n    This uses an artificial regression to calculate a variant of an LM or\\n    generalized score test for the specification of the variance assumption\\n    in a Poisson model. The performed test is a Wald test on the coefficients\\n    of the `exog_new_test`.\\n\\n    Warning: insufficiently tested, especially for options\\n    '\n    msg = 'dispersion_poisson_generic here is deprecated, use the version in discrete._diagnostic_count'\n    warnings.warn(msg, FutureWarning)\n    from statsmodels.discrete._diagnostics_count import _test_poisson_dispersion_generic\n    res_test = _test_poisson_dispersion_generic(results, exog_new_test, exog_new_control=exog_new_control, include_score=include_score, use_endog=use_endog, cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\n    return res_test"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, **kwds):\n    self.__dict__.update(kwds)",
        "mutated": [
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n    self.__dict__.update(kwds)",
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.__dict__.update(kwds)",
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.__dict__.update(kwds)",
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.__dict__.update(kwds)",
            "def __init__(self, **kwds):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.__dict__.update(kwds)"
        ]
    },
    {
        "func_name": "summary",
        "original": "def summary(self):\n    txt = 'Specification Test (LM, score)\\n'\n    stat = [self.c1, self.c2, self.c3]\n    pval = [self.pval1, self.pval2, self.pval3]\n    description = ['nonrobust', 'dispersed', 'HC']\n    for row in zip(description, stat, pval):\n        txt += '%-12s  statistic = %6.4f  pvalue = %6.5f\\n' % row\n    txt += '\\nAssumptions:\\n'\n    txt += 'nonrobust: variance is correctly specified\\n'\n    txt += 'dispersed: variance correctly specified up to scale factor\\n'\n    txt += 'HC       : robust to any heteroscedasticity\\n'\n    txt += 'test is not robust to correlation across observations'\n    return txt",
        "mutated": [
            "def summary(self):\n    if False:\n        i = 10\n    txt = 'Specification Test (LM, score)\\n'\n    stat = [self.c1, self.c2, self.c3]\n    pval = [self.pval1, self.pval2, self.pval3]\n    description = ['nonrobust', 'dispersed', 'HC']\n    for row in zip(description, stat, pval):\n        txt += '%-12s  statistic = %6.4f  pvalue = %6.5f\\n' % row\n    txt += '\\nAssumptions:\\n'\n    txt += 'nonrobust: variance is correctly specified\\n'\n    txt += 'dispersed: variance correctly specified up to scale factor\\n'\n    txt += 'HC       : robust to any heteroscedasticity\\n'\n    txt += 'test is not robust to correlation across observations'\n    return txt",
            "def summary(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    txt = 'Specification Test (LM, score)\\n'\n    stat = [self.c1, self.c2, self.c3]\n    pval = [self.pval1, self.pval2, self.pval3]\n    description = ['nonrobust', 'dispersed', 'HC']\n    for row in zip(description, stat, pval):\n        txt += '%-12s  statistic = %6.4f  pvalue = %6.5f\\n' % row\n    txt += '\\nAssumptions:\\n'\n    txt += 'nonrobust: variance is correctly specified\\n'\n    txt += 'dispersed: variance correctly specified up to scale factor\\n'\n    txt += 'HC       : robust to any heteroscedasticity\\n'\n    txt += 'test is not robust to correlation across observations'\n    return txt",
            "def summary(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    txt = 'Specification Test (LM, score)\\n'\n    stat = [self.c1, self.c2, self.c3]\n    pval = [self.pval1, self.pval2, self.pval3]\n    description = ['nonrobust', 'dispersed', 'HC']\n    for row in zip(description, stat, pval):\n        txt += '%-12s  statistic = %6.4f  pvalue = %6.5f\\n' % row\n    txt += '\\nAssumptions:\\n'\n    txt += 'nonrobust: variance is correctly specified\\n'\n    txt += 'dispersed: variance correctly specified up to scale factor\\n'\n    txt += 'HC       : robust to any heteroscedasticity\\n'\n    txt += 'test is not robust to correlation across observations'\n    return txt",
            "def summary(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    txt = 'Specification Test (LM, score)\\n'\n    stat = [self.c1, self.c2, self.c3]\n    pval = [self.pval1, self.pval2, self.pval3]\n    description = ['nonrobust', 'dispersed', 'HC']\n    for row in zip(description, stat, pval):\n        txt += '%-12s  statistic = %6.4f  pvalue = %6.5f\\n' % row\n    txt += '\\nAssumptions:\\n'\n    txt += 'nonrobust: variance is correctly specified\\n'\n    txt += 'dispersed: variance correctly specified up to scale factor\\n'\n    txt += 'HC       : robust to any heteroscedasticity\\n'\n    txt += 'test is not robust to correlation across observations'\n    return txt",
            "def summary(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    txt = 'Specification Test (LM, score)\\n'\n    stat = [self.c1, self.c2, self.c3]\n    pval = [self.pval1, self.pval2, self.pval3]\n    description = ['nonrobust', 'dispersed', 'HC']\n    for row in zip(description, stat, pval):\n        txt += '%-12s  statistic = %6.4f  pvalue = %6.5f\\n' % row\n    txt += '\\nAssumptions:\\n'\n    txt += 'nonrobust: variance is correctly specified\\n'\n    txt += 'dispersed: variance correctly specified up to scale factor\\n'\n    txt += 'HC       : robust to any heteroscedasticity\\n'\n    txt += 'test is not robust to correlation across observations'\n    return txt"
        ]
    },
    {
        "func_name": "lm_test_glm",
        "original": "def lm_test_glm(result, exog_extra, mean_deriv=None):\n    \"\"\"score/lagrange multiplier test for GLM\n\n    Wooldridge procedure for test of mean function in GLM\n\n    Parameters\n    ----------\n    results : GLMResults instance\n        results instance with the constrained model\n    exog_extra : ndarray or None\n        additional exogenous variables for variable addition test\n        This can be set to None if mean_deriv is provided.\n    mean_deriv : None or ndarray\n        Extra moment condition that correspond to the partial derivative of\n        a mean function with respect to some parameters.\n\n    Returns\n    -------\n    test_results : Results instance\n        The results instance has the following attributes which are score\n        statistic and p-value for 3 versions of the score test.\n\n        c1, pval1 : nonrobust score_test results\n        c2, pval2 : score test results robust to over or under dispersion\n        c3, pval3 : score test results fully robust to any heteroscedasticity\n\n        The test results instance also has a simple summary method.\n\n    Notes\n    -----\n    TODO: add `df` to results and make df detection more robust\n\n    This implements the auxiliary regression procedure of Wooldridge,\n    implemented based on the presentation in chapter 8 in Handbook of\n    Applied Econometrics 2.\n\n    References\n    ----------\n    Wooldridge, Jeffrey M. 1997. \u201cQuasi-Likelihood Methods for Count Data.\u201d\n    Handbook of Applied Econometrics 2: 352\u2013406.\n\n    and other articles and text book by Wooldridge\n\n    \"\"\"\n    if hasattr(result, '_result'):\n        res = result._result\n    else:\n        res = result\n    mod = result.model\n    nobs = mod.endog.shape[0]\n    dlinkinv = mod.family.link.inverse_deriv\n    dm = lambda x, linpred: dlinkinv(linpred)[:, None] * x\n    var_func = mod.family.variance\n    x = result.model.exog\n    x2 = exog_extra\n    try:\n        lin_pred = res.predict(which='linear')\n    except TypeError:\n        lin_pred = res.predict(linear=True)\n    dm_incl = dm(x, lin_pred)\n    if x2 is not None:\n        dm_excl = dm(x2, lin_pred)\n        if mean_deriv is not None:\n            dm_excl = np.column_stack((dm_excl, mean_deriv))\n    elif mean_deriv is not None:\n        dm_excl = mean_deriv\n    else:\n        raise ValueError('either exog_extra or mean_deriv have to be provided')\n    k_constraint = dm_excl.shape[1]\n    fittedvalues = res.predict()\n    v = var_func(fittedvalues)\n    std = np.sqrt(v)\n    res_ols1 = OLS(res.resid_response / std, np.column_stack((dm_incl, dm_excl)) / std[:, None]).fit()\n    c1 = res_ols1.ess\n    pval1 = stats.chi2.sf(c1, k_constraint)\n    c2 = nobs * res_ols1.rsquared\n    pval2 = stats.chi2.sf(c2, k_constraint)\n    from statsmodels.stats.multivariate_tools import partial_project\n    pp = partial_project(dm_excl / std[:, None], dm_incl / std[:, None])\n    resid_p = res.resid_response / std\n    res_ols3 = OLS(np.ones(nobs), pp.resid * resid_p[:, None]).fit()\n    c3b = res_ols3.ess\n    pval3 = stats.chi2.sf(c3b, k_constraint)\n    tres = TestResults(c1=c1, pval1=pval1, c2=c2, pval2=pval2, c3=c3b, pval3=pval3)\n    return tres",
        "mutated": [
            "def lm_test_glm(result, exog_extra, mean_deriv=None):\n    if False:\n        i = 10\n    'score/lagrange multiplier test for GLM\\n\\n    Wooldridge procedure for test of mean function in GLM\\n\\n    Parameters\\n    ----------\\n    results : GLMResults instance\\n        results instance with the constrained model\\n    exog_extra : ndarray or None\\n        additional exogenous variables for variable addition test\\n        This can be set to None if mean_deriv is provided.\\n    mean_deriv : None or ndarray\\n        Extra moment condition that correspond to the partial derivative of\\n        a mean function with respect to some parameters.\\n\\n    Returns\\n    -------\\n    test_results : Results instance\\n        The results instance has the following attributes which are score\\n        statistic and p-value for 3 versions of the score test.\\n\\n        c1, pval1 : nonrobust score_test results\\n        c2, pval2 : score test results robust to over or under dispersion\\n        c3, pval3 : score test results fully robust to any heteroscedasticity\\n\\n        The test results instance also has a simple summary method.\\n\\n    Notes\\n    -----\\n    TODO: add `df` to results and make df detection more robust\\n\\n    This implements the auxiliary regression procedure of Wooldridge,\\n    implemented based on the presentation in chapter 8 in Handbook of\\n    Applied Econometrics 2.\\n\\n    References\\n    ----------\\n    Wooldridge, Jeffrey M. 1997. \u201cQuasi-Likelihood Methods for Count Data.\u201d\\n    Handbook of Applied Econometrics 2: 352\u2013406.\\n\\n    and other articles and text book by Wooldridge\\n\\n    '\n    if hasattr(result, '_result'):\n        res = result._result\n    else:\n        res = result\n    mod = result.model\n    nobs = mod.endog.shape[0]\n    dlinkinv = mod.family.link.inverse_deriv\n    dm = lambda x, linpred: dlinkinv(linpred)[:, None] * x\n    var_func = mod.family.variance\n    x = result.model.exog\n    x2 = exog_extra\n    try:\n        lin_pred = res.predict(which='linear')\n    except TypeError:\n        lin_pred = res.predict(linear=True)\n    dm_incl = dm(x, lin_pred)\n    if x2 is not None:\n        dm_excl = dm(x2, lin_pred)\n        if mean_deriv is not None:\n            dm_excl = np.column_stack((dm_excl, mean_deriv))\n    elif mean_deriv is not None:\n        dm_excl = mean_deriv\n    else:\n        raise ValueError('either exog_extra or mean_deriv have to be provided')\n    k_constraint = dm_excl.shape[1]\n    fittedvalues = res.predict()\n    v = var_func(fittedvalues)\n    std = np.sqrt(v)\n    res_ols1 = OLS(res.resid_response / std, np.column_stack((dm_incl, dm_excl)) / std[:, None]).fit()\n    c1 = res_ols1.ess\n    pval1 = stats.chi2.sf(c1, k_constraint)\n    c2 = nobs * res_ols1.rsquared\n    pval2 = stats.chi2.sf(c2, k_constraint)\n    from statsmodels.stats.multivariate_tools import partial_project\n    pp = partial_project(dm_excl / std[:, None], dm_incl / std[:, None])\n    resid_p = res.resid_response / std\n    res_ols3 = OLS(np.ones(nobs), pp.resid * resid_p[:, None]).fit()\n    c3b = res_ols3.ess\n    pval3 = stats.chi2.sf(c3b, k_constraint)\n    tres = TestResults(c1=c1, pval1=pval1, c2=c2, pval2=pval2, c3=c3b, pval3=pval3)\n    return tres",
            "def lm_test_glm(result, exog_extra, mean_deriv=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'score/lagrange multiplier test for GLM\\n\\n    Wooldridge procedure for test of mean function in GLM\\n\\n    Parameters\\n    ----------\\n    results : GLMResults instance\\n        results instance with the constrained model\\n    exog_extra : ndarray or None\\n        additional exogenous variables for variable addition test\\n        This can be set to None if mean_deriv is provided.\\n    mean_deriv : None or ndarray\\n        Extra moment condition that correspond to the partial derivative of\\n        a mean function with respect to some parameters.\\n\\n    Returns\\n    -------\\n    test_results : Results instance\\n        The results instance has the following attributes which are score\\n        statistic and p-value for 3 versions of the score test.\\n\\n        c1, pval1 : nonrobust score_test results\\n        c2, pval2 : score test results robust to over or under dispersion\\n        c3, pval3 : score test results fully robust to any heteroscedasticity\\n\\n        The test results instance also has a simple summary method.\\n\\n    Notes\\n    -----\\n    TODO: add `df` to results and make df detection more robust\\n\\n    This implements the auxiliary regression procedure of Wooldridge,\\n    implemented based on the presentation in chapter 8 in Handbook of\\n    Applied Econometrics 2.\\n\\n    References\\n    ----------\\n    Wooldridge, Jeffrey M. 1997. \u201cQuasi-Likelihood Methods for Count Data.\u201d\\n    Handbook of Applied Econometrics 2: 352\u2013406.\\n\\n    and other articles and text book by Wooldridge\\n\\n    '\n    if hasattr(result, '_result'):\n        res = result._result\n    else:\n        res = result\n    mod = result.model\n    nobs = mod.endog.shape[0]\n    dlinkinv = mod.family.link.inverse_deriv\n    dm = lambda x, linpred: dlinkinv(linpred)[:, None] * x\n    var_func = mod.family.variance\n    x = result.model.exog\n    x2 = exog_extra\n    try:\n        lin_pred = res.predict(which='linear')\n    except TypeError:\n        lin_pred = res.predict(linear=True)\n    dm_incl = dm(x, lin_pred)\n    if x2 is not None:\n        dm_excl = dm(x2, lin_pred)\n        if mean_deriv is not None:\n            dm_excl = np.column_stack((dm_excl, mean_deriv))\n    elif mean_deriv is not None:\n        dm_excl = mean_deriv\n    else:\n        raise ValueError('either exog_extra or mean_deriv have to be provided')\n    k_constraint = dm_excl.shape[1]\n    fittedvalues = res.predict()\n    v = var_func(fittedvalues)\n    std = np.sqrt(v)\n    res_ols1 = OLS(res.resid_response / std, np.column_stack((dm_incl, dm_excl)) / std[:, None]).fit()\n    c1 = res_ols1.ess\n    pval1 = stats.chi2.sf(c1, k_constraint)\n    c2 = nobs * res_ols1.rsquared\n    pval2 = stats.chi2.sf(c2, k_constraint)\n    from statsmodels.stats.multivariate_tools import partial_project\n    pp = partial_project(dm_excl / std[:, None], dm_incl / std[:, None])\n    resid_p = res.resid_response / std\n    res_ols3 = OLS(np.ones(nobs), pp.resid * resid_p[:, None]).fit()\n    c3b = res_ols3.ess\n    pval3 = stats.chi2.sf(c3b, k_constraint)\n    tres = TestResults(c1=c1, pval1=pval1, c2=c2, pval2=pval2, c3=c3b, pval3=pval3)\n    return tres",
            "def lm_test_glm(result, exog_extra, mean_deriv=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'score/lagrange multiplier test for GLM\\n\\n    Wooldridge procedure for test of mean function in GLM\\n\\n    Parameters\\n    ----------\\n    results : GLMResults instance\\n        results instance with the constrained model\\n    exog_extra : ndarray or None\\n        additional exogenous variables for variable addition test\\n        This can be set to None if mean_deriv is provided.\\n    mean_deriv : None or ndarray\\n        Extra moment condition that correspond to the partial derivative of\\n        a mean function with respect to some parameters.\\n\\n    Returns\\n    -------\\n    test_results : Results instance\\n        The results instance has the following attributes which are score\\n        statistic and p-value for 3 versions of the score test.\\n\\n        c1, pval1 : nonrobust score_test results\\n        c2, pval2 : score test results robust to over or under dispersion\\n        c3, pval3 : score test results fully robust to any heteroscedasticity\\n\\n        The test results instance also has a simple summary method.\\n\\n    Notes\\n    -----\\n    TODO: add `df` to results and make df detection more robust\\n\\n    This implements the auxiliary regression procedure of Wooldridge,\\n    implemented based on the presentation in chapter 8 in Handbook of\\n    Applied Econometrics 2.\\n\\n    References\\n    ----------\\n    Wooldridge, Jeffrey M. 1997. \u201cQuasi-Likelihood Methods for Count Data.\u201d\\n    Handbook of Applied Econometrics 2: 352\u2013406.\\n\\n    and other articles and text book by Wooldridge\\n\\n    '\n    if hasattr(result, '_result'):\n        res = result._result\n    else:\n        res = result\n    mod = result.model\n    nobs = mod.endog.shape[0]\n    dlinkinv = mod.family.link.inverse_deriv\n    dm = lambda x, linpred: dlinkinv(linpred)[:, None] * x\n    var_func = mod.family.variance\n    x = result.model.exog\n    x2 = exog_extra\n    try:\n        lin_pred = res.predict(which='linear')\n    except TypeError:\n        lin_pred = res.predict(linear=True)\n    dm_incl = dm(x, lin_pred)\n    if x2 is not None:\n        dm_excl = dm(x2, lin_pred)\n        if mean_deriv is not None:\n            dm_excl = np.column_stack((dm_excl, mean_deriv))\n    elif mean_deriv is not None:\n        dm_excl = mean_deriv\n    else:\n        raise ValueError('either exog_extra or mean_deriv have to be provided')\n    k_constraint = dm_excl.shape[1]\n    fittedvalues = res.predict()\n    v = var_func(fittedvalues)\n    std = np.sqrt(v)\n    res_ols1 = OLS(res.resid_response / std, np.column_stack((dm_incl, dm_excl)) / std[:, None]).fit()\n    c1 = res_ols1.ess\n    pval1 = stats.chi2.sf(c1, k_constraint)\n    c2 = nobs * res_ols1.rsquared\n    pval2 = stats.chi2.sf(c2, k_constraint)\n    from statsmodels.stats.multivariate_tools import partial_project\n    pp = partial_project(dm_excl / std[:, None], dm_incl / std[:, None])\n    resid_p = res.resid_response / std\n    res_ols3 = OLS(np.ones(nobs), pp.resid * resid_p[:, None]).fit()\n    c3b = res_ols3.ess\n    pval3 = stats.chi2.sf(c3b, k_constraint)\n    tres = TestResults(c1=c1, pval1=pval1, c2=c2, pval2=pval2, c3=c3b, pval3=pval3)\n    return tres",
            "def lm_test_glm(result, exog_extra, mean_deriv=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'score/lagrange multiplier test for GLM\\n\\n    Wooldridge procedure for test of mean function in GLM\\n\\n    Parameters\\n    ----------\\n    results : GLMResults instance\\n        results instance with the constrained model\\n    exog_extra : ndarray or None\\n        additional exogenous variables for variable addition test\\n        This can be set to None if mean_deriv is provided.\\n    mean_deriv : None or ndarray\\n        Extra moment condition that correspond to the partial derivative of\\n        a mean function with respect to some parameters.\\n\\n    Returns\\n    -------\\n    test_results : Results instance\\n        The results instance has the following attributes which are score\\n        statistic and p-value for 3 versions of the score test.\\n\\n        c1, pval1 : nonrobust score_test results\\n        c2, pval2 : score test results robust to over or under dispersion\\n        c3, pval3 : score test results fully robust to any heteroscedasticity\\n\\n        The test results instance also has a simple summary method.\\n\\n    Notes\\n    -----\\n    TODO: add `df` to results and make df detection more robust\\n\\n    This implements the auxiliary regression procedure of Wooldridge,\\n    implemented based on the presentation in chapter 8 in Handbook of\\n    Applied Econometrics 2.\\n\\n    References\\n    ----------\\n    Wooldridge, Jeffrey M. 1997. \u201cQuasi-Likelihood Methods for Count Data.\u201d\\n    Handbook of Applied Econometrics 2: 352\u2013406.\\n\\n    and other articles and text book by Wooldridge\\n\\n    '\n    if hasattr(result, '_result'):\n        res = result._result\n    else:\n        res = result\n    mod = result.model\n    nobs = mod.endog.shape[0]\n    dlinkinv = mod.family.link.inverse_deriv\n    dm = lambda x, linpred: dlinkinv(linpred)[:, None] * x\n    var_func = mod.family.variance\n    x = result.model.exog\n    x2 = exog_extra\n    try:\n        lin_pred = res.predict(which='linear')\n    except TypeError:\n        lin_pred = res.predict(linear=True)\n    dm_incl = dm(x, lin_pred)\n    if x2 is not None:\n        dm_excl = dm(x2, lin_pred)\n        if mean_deriv is not None:\n            dm_excl = np.column_stack((dm_excl, mean_deriv))\n    elif mean_deriv is not None:\n        dm_excl = mean_deriv\n    else:\n        raise ValueError('either exog_extra or mean_deriv have to be provided')\n    k_constraint = dm_excl.shape[1]\n    fittedvalues = res.predict()\n    v = var_func(fittedvalues)\n    std = np.sqrt(v)\n    res_ols1 = OLS(res.resid_response / std, np.column_stack((dm_incl, dm_excl)) / std[:, None]).fit()\n    c1 = res_ols1.ess\n    pval1 = stats.chi2.sf(c1, k_constraint)\n    c2 = nobs * res_ols1.rsquared\n    pval2 = stats.chi2.sf(c2, k_constraint)\n    from statsmodels.stats.multivariate_tools import partial_project\n    pp = partial_project(dm_excl / std[:, None], dm_incl / std[:, None])\n    resid_p = res.resid_response / std\n    res_ols3 = OLS(np.ones(nobs), pp.resid * resid_p[:, None]).fit()\n    c3b = res_ols3.ess\n    pval3 = stats.chi2.sf(c3b, k_constraint)\n    tres = TestResults(c1=c1, pval1=pval1, c2=c2, pval2=pval2, c3=c3b, pval3=pval3)\n    return tres",
            "def lm_test_glm(result, exog_extra, mean_deriv=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'score/lagrange multiplier test for GLM\\n\\n    Wooldridge procedure for test of mean function in GLM\\n\\n    Parameters\\n    ----------\\n    results : GLMResults instance\\n        results instance with the constrained model\\n    exog_extra : ndarray or None\\n        additional exogenous variables for variable addition test\\n        This can be set to None if mean_deriv is provided.\\n    mean_deriv : None or ndarray\\n        Extra moment condition that correspond to the partial derivative of\\n        a mean function with respect to some parameters.\\n\\n    Returns\\n    -------\\n    test_results : Results instance\\n        The results instance has the following attributes which are score\\n        statistic and p-value for 3 versions of the score test.\\n\\n        c1, pval1 : nonrobust score_test results\\n        c2, pval2 : score test results robust to over or under dispersion\\n        c3, pval3 : score test results fully robust to any heteroscedasticity\\n\\n        The test results instance also has a simple summary method.\\n\\n    Notes\\n    -----\\n    TODO: add `df` to results and make df detection more robust\\n\\n    This implements the auxiliary regression procedure of Wooldridge,\\n    implemented based on the presentation in chapter 8 in Handbook of\\n    Applied Econometrics 2.\\n\\n    References\\n    ----------\\n    Wooldridge, Jeffrey M. 1997. \u201cQuasi-Likelihood Methods for Count Data.\u201d\\n    Handbook of Applied Econometrics 2: 352\u2013406.\\n\\n    and other articles and text book by Wooldridge\\n\\n    '\n    if hasattr(result, '_result'):\n        res = result._result\n    else:\n        res = result\n    mod = result.model\n    nobs = mod.endog.shape[0]\n    dlinkinv = mod.family.link.inverse_deriv\n    dm = lambda x, linpred: dlinkinv(linpred)[:, None] * x\n    var_func = mod.family.variance\n    x = result.model.exog\n    x2 = exog_extra\n    try:\n        lin_pred = res.predict(which='linear')\n    except TypeError:\n        lin_pred = res.predict(linear=True)\n    dm_incl = dm(x, lin_pred)\n    if x2 is not None:\n        dm_excl = dm(x2, lin_pred)\n        if mean_deriv is not None:\n            dm_excl = np.column_stack((dm_excl, mean_deriv))\n    elif mean_deriv is not None:\n        dm_excl = mean_deriv\n    else:\n        raise ValueError('either exog_extra or mean_deriv have to be provided')\n    k_constraint = dm_excl.shape[1]\n    fittedvalues = res.predict()\n    v = var_func(fittedvalues)\n    std = np.sqrt(v)\n    res_ols1 = OLS(res.resid_response / std, np.column_stack((dm_incl, dm_excl)) / std[:, None]).fit()\n    c1 = res_ols1.ess\n    pval1 = stats.chi2.sf(c1, k_constraint)\n    c2 = nobs * res_ols1.rsquared\n    pval2 = stats.chi2.sf(c2, k_constraint)\n    from statsmodels.stats.multivariate_tools import partial_project\n    pp = partial_project(dm_excl / std[:, None], dm_incl / std[:, None])\n    resid_p = res.resid_response / std\n    res_ols3 = OLS(np.ones(nobs), pp.resid * resid_p[:, None]).fit()\n    c3b = res_ols3.ess\n    pval3 = stats.chi2.sf(c3b, k_constraint)\n    tres = TestResults(c1=c1, pval1=pval1, c2=c2, pval2=pval2, c3=c3b, pval3=pval3)\n    return tres"
        ]
    },
    {
        "func_name": "cm_test_robust",
        "original": "def cm_test_robust(resid, resid_deriv, instruments, weights=1):\n    \"\"\"score/lagrange multiplier of Wooldridge\n\n    generic version of Wooldridge procedure for test of conditional moments\n\n    Limitation: This version allows only for one unconditional moment\n    restriction, i.e. resid is scalar for each observation.\n    Another limitation is that it assumes independent observations, no\n    correlation in residuals and weights cannot be replaced by cross-observation\n    whitening.\n\n    Parameters\n    ----------\n    resid : ndarray, (nobs, )\n        conditional moment restriction, E(r | x, params) = 0\n    resid_deriv : ndarray, (nobs, k_params)\n        derivative of conditional moment restriction with respect to parameters\n    instruments : ndarray, (nobs, k_instruments)\n        indicator variables of Wooldridge, multiplies the conditional momen\n        restriction\n    weights : ndarray\n        This is a weights function as used in WLS. The moment\n        restrictions are multiplied by weights. This corresponds to the\n        inverse of the variance in a heteroskedastic model.\n\n    Returns\n    -------\n    test_results : Results instance\n        ???  TODO\n\n    Notes\n    -----\n    This implements the auxiliary regression procedure of Wooldridge,\n    implemented based on procedure 2.1 in Wooldridge 1990.\n\n    Wooldridge allows for multivariate conditional moments (`resid`)\n    TODO: check dimensions for multivariate case for extension\n\n    References\n    ----------\n    Wooldridge\n    Wooldridge\n    and more Wooldridge\n\n    \"\"\"\n    nobs = resid.shape[0]\n    from statsmodels.stats.multivariate_tools import partial_project\n    w_sqrt = np.sqrt(weights)\n    if np.size(weights) > 1:\n        w_sqrt = w_sqrt[:, None]\n    pp = partial_project(instruments * w_sqrt, resid_deriv * w_sqrt)\n    mom_resid = pp.resid\n    moms_test = mom_resid * resid[:, None] * w_sqrt\n    k_constraint = moms_test.shape[1]\n    cov = moms_test.T.dot(moms_test)\n    diff = moms_test.sum(0)\n    stat = diff.dot(np.linalg.solve(cov, diff))\n    stat2 = OLS(np.ones(nobs), moms_test).fit().ess\n    pval = stats.chi2.sf(stat, k_constraint)\n    return (stat, pval, stat2)",
        "mutated": [
            "def cm_test_robust(resid, resid_deriv, instruments, weights=1):\n    if False:\n        i = 10\n    'score/lagrange multiplier of Wooldridge\\n\\n    generic version of Wooldridge procedure for test of conditional moments\\n\\n    Limitation: This version allows only for one unconditional moment\\n    restriction, i.e. resid is scalar for each observation.\\n    Another limitation is that it assumes independent observations, no\\n    correlation in residuals and weights cannot be replaced by cross-observation\\n    whitening.\\n\\n    Parameters\\n    ----------\\n    resid : ndarray, (nobs, )\\n        conditional moment restriction, E(r | x, params) = 0\\n    resid_deriv : ndarray, (nobs, k_params)\\n        derivative of conditional moment restriction with respect to parameters\\n    instruments : ndarray, (nobs, k_instruments)\\n        indicator variables of Wooldridge, multiplies the conditional momen\\n        restriction\\n    weights : ndarray\\n        This is a weights function as used in WLS. The moment\\n        restrictions are multiplied by weights. This corresponds to the\\n        inverse of the variance in a heteroskedastic model.\\n\\n    Returns\\n    -------\\n    test_results : Results instance\\n        ???  TODO\\n\\n    Notes\\n    -----\\n    This implements the auxiliary regression procedure of Wooldridge,\\n    implemented based on procedure 2.1 in Wooldridge 1990.\\n\\n    Wooldridge allows for multivariate conditional moments (`resid`)\\n    TODO: check dimensions for multivariate case for extension\\n\\n    References\\n    ----------\\n    Wooldridge\\n    Wooldridge\\n    and more Wooldridge\\n\\n    '\n    nobs = resid.shape[0]\n    from statsmodels.stats.multivariate_tools import partial_project\n    w_sqrt = np.sqrt(weights)\n    if np.size(weights) > 1:\n        w_sqrt = w_sqrt[:, None]\n    pp = partial_project(instruments * w_sqrt, resid_deriv * w_sqrt)\n    mom_resid = pp.resid\n    moms_test = mom_resid * resid[:, None] * w_sqrt\n    k_constraint = moms_test.shape[1]\n    cov = moms_test.T.dot(moms_test)\n    diff = moms_test.sum(0)\n    stat = diff.dot(np.linalg.solve(cov, diff))\n    stat2 = OLS(np.ones(nobs), moms_test).fit().ess\n    pval = stats.chi2.sf(stat, k_constraint)\n    return (stat, pval, stat2)",
            "def cm_test_robust(resid, resid_deriv, instruments, weights=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'score/lagrange multiplier of Wooldridge\\n\\n    generic version of Wooldridge procedure for test of conditional moments\\n\\n    Limitation: This version allows only for one unconditional moment\\n    restriction, i.e. resid is scalar for each observation.\\n    Another limitation is that it assumes independent observations, no\\n    correlation in residuals and weights cannot be replaced by cross-observation\\n    whitening.\\n\\n    Parameters\\n    ----------\\n    resid : ndarray, (nobs, )\\n        conditional moment restriction, E(r | x, params) = 0\\n    resid_deriv : ndarray, (nobs, k_params)\\n        derivative of conditional moment restriction with respect to parameters\\n    instruments : ndarray, (nobs, k_instruments)\\n        indicator variables of Wooldridge, multiplies the conditional momen\\n        restriction\\n    weights : ndarray\\n        This is a weights function as used in WLS. The moment\\n        restrictions are multiplied by weights. This corresponds to the\\n        inverse of the variance in a heteroskedastic model.\\n\\n    Returns\\n    -------\\n    test_results : Results instance\\n        ???  TODO\\n\\n    Notes\\n    -----\\n    This implements the auxiliary regression procedure of Wooldridge,\\n    implemented based on procedure 2.1 in Wooldridge 1990.\\n\\n    Wooldridge allows for multivariate conditional moments (`resid`)\\n    TODO: check dimensions for multivariate case for extension\\n\\n    References\\n    ----------\\n    Wooldridge\\n    Wooldridge\\n    and more Wooldridge\\n\\n    '\n    nobs = resid.shape[0]\n    from statsmodels.stats.multivariate_tools import partial_project\n    w_sqrt = np.sqrt(weights)\n    if np.size(weights) > 1:\n        w_sqrt = w_sqrt[:, None]\n    pp = partial_project(instruments * w_sqrt, resid_deriv * w_sqrt)\n    mom_resid = pp.resid\n    moms_test = mom_resid * resid[:, None] * w_sqrt\n    k_constraint = moms_test.shape[1]\n    cov = moms_test.T.dot(moms_test)\n    diff = moms_test.sum(0)\n    stat = diff.dot(np.linalg.solve(cov, diff))\n    stat2 = OLS(np.ones(nobs), moms_test).fit().ess\n    pval = stats.chi2.sf(stat, k_constraint)\n    return (stat, pval, stat2)",
            "def cm_test_robust(resid, resid_deriv, instruments, weights=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'score/lagrange multiplier of Wooldridge\\n\\n    generic version of Wooldridge procedure for test of conditional moments\\n\\n    Limitation: This version allows only for one unconditional moment\\n    restriction, i.e. resid is scalar for each observation.\\n    Another limitation is that it assumes independent observations, no\\n    correlation in residuals and weights cannot be replaced by cross-observation\\n    whitening.\\n\\n    Parameters\\n    ----------\\n    resid : ndarray, (nobs, )\\n        conditional moment restriction, E(r | x, params) = 0\\n    resid_deriv : ndarray, (nobs, k_params)\\n        derivative of conditional moment restriction with respect to parameters\\n    instruments : ndarray, (nobs, k_instruments)\\n        indicator variables of Wooldridge, multiplies the conditional momen\\n        restriction\\n    weights : ndarray\\n        This is a weights function as used in WLS. The moment\\n        restrictions are multiplied by weights. This corresponds to the\\n        inverse of the variance in a heteroskedastic model.\\n\\n    Returns\\n    -------\\n    test_results : Results instance\\n        ???  TODO\\n\\n    Notes\\n    -----\\n    This implements the auxiliary regression procedure of Wooldridge,\\n    implemented based on procedure 2.1 in Wooldridge 1990.\\n\\n    Wooldridge allows for multivariate conditional moments (`resid`)\\n    TODO: check dimensions for multivariate case for extension\\n\\n    References\\n    ----------\\n    Wooldridge\\n    Wooldridge\\n    and more Wooldridge\\n\\n    '\n    nobs = resid.shape[0]\n    from statsmodels.stats.multivariate_tools import partial_project\n    w_sqrt = np.sqrt(weights)\n    if np.size(weights) > 1:\n        w_sqrt = w_sqrt[:, None]\n    pp = partial_project(instruments * w_sqrt, resid_deriv * w_sqrt)\n    mom_resid = pp.resid\n    moms_test = mom_resid * resid[:, None] * w_sqrt\n    k_constraint = moms_test.shape[1]\n    cov = moms_test.T.dot(moms_test)\n    diff = moms_test.sum(0)\n    stat = diff.dot(np.linalg.solve(cov, diff))\n    stat2 = OLS(np.ones(nobs), moms_test).fit().ess\n    pval = stats.chi2.sf(stat, k_constraint)\n    return (stat, pval, stat2)",
            "def cm_test_robust(resid, resid_deriv, instruments, weights=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'score/lagrange multiplier of Wooldridge\\n\\n    generic version of Wooldridge procedure for test of conditional moments\\n\\n    Limitation: This version allows only for one unconditional moment\\n    restriction, i.e. resid is scalar for each observation.\\n    Another limitation is that it assumes independent observations, no\\n    correlation in residuals and weights cannot be replaced by cross-observation\\n    whitening.\\n\\n    Parameters\\n    ----------\\n    resid : ndarray, (nobs, )\\n        conditional moment restriction, E(r | x, params) = 0\\n    resid_deriv : ndarray, (nobs, k_params)\\n        derivative of conditional moment restriction with respect to parameters\\n    instruments : ndarray, (nobs, k_instruments)\\n        indicator variables of Wooldridge, multiplies the conditional momen\\n        restriction\\n    weights : ndarray\\n        This is a weights function as used in WLS. The moment\\n        restrictions are multiplied by weights. This corresponds to the\\n        inverse of the variance in a heteroskedastic model.\\n\\n    Returns\\n    -------\\n    test_results : Results instance\\n        ???  TODO\\n\\n    Notes\\n    -----\\n    This implements the auxiliary regression procedure of Wooldridge,\\n    implemented based on procedure 2.1 in Wooldridge 1990.\\n\\n    Wooldridge allows for multivariate conditional moments (`resid`)\\n    TODO: check dimensions for multivariate case for extension\\n\\n    References\\n    ----------\\n    Wooldridge\\n    Wooldridge\\n    and more Wooldridge\\n\\n    '\n    nobs = resid.shape[0]\n    from statsmodels.stats.multivariate_tools import partial_project\n    w_sqrt = np.sqrt(weights)\n    if np.size(weights) > 1:\n        w_sqrt = w_sqrt[:, None]\n    pp = partial_project(instruments * w_sqrt, resid_deriv * w_sqrt)\n    mom_resid = pp.resid\n    moms_test = mom_resid * resid[:, None] * w_sqrt\n    k_constraint = moms_test.shape[1]\n    cov = moms_test.T.dot(moms_test)\n    diff = moms_test.sum(0)\n    stat = diff.dot(np.linalg.solve(cov, diff))\n    stat2 = OLS(np.ones(nobs), moms_test).fit().ess\n    pval = stats.chi2.sf(stat, k_constraint)\n    return (stat, pval, stat2)",
            "def cm_test_robust(resid, resid_deriv, instruments, weights=1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'score/lagrange multiplier of Wooldridge\\n\\n    generic version of Wooldridge procedure for test of conditional moments\\n\\n    Limitation: This version allows only for one unconditional moment\\n    restriction, i.e. resid is scalar for each observation.\\n    Another limitation is that it assumes independent observations, no\\n    correlation in residuals and weights cannot be replaced by cross-observation\\n    whitening.\\n\\n    Parameters\\n    ----------\\n    resid : ndarray, (nobs, )\\n        conditional moment restriction, E(r | x, params) = 0\\n    resid_deriv : ndarray, (nobs, k_params)\\n        derivative of conditional moment restriction with respect to parameters\\n    instruments : ndarray, (nobs, k_instruments)\\n        indicator variables of Wooldridge, multiplies the conditional momen\\n        restriction\\n    weights : ndarray\\n        This is a weights function as used in WLS. The moment\\n        restrictions are multiplied by weights. This corresponds to the\\n        inverse of the variance in a heteroskedastic model.\\n\\n    Returns\\n    -------\\n    test_results : Results instance\\n        ???  TODO\\n\\n    Notes\\n    -----\\n    This implements the auxiliary regression procedure of Wooldridge,\\n    implemented based on procedure 2.1 in Wooldridge 1990.\\n\\n    Wooldridge allows for multivariate conditional moments (`resid`)\\n    TODO: check dimensions for multivariate case for extension\\n\\n    References\\n    ----------\\n    Wooldridge\\n    Wooldridge\\n    and more Wooldridge\\n\\n    '\n    nobs = resid.shape[0]\n    from statsmodels.stats.multivariate_tools import partial_project\n    w_sqrt = np.sqrt(weights)\n    if np.size(weights) > 1:\n        w_sqrt = w_sqrt[:, None]\n    pp = partial_project(instruments * w_sqrt, resid_deriv * w_sqrt)\n    mom_resid = pp.resid\n    moms_test = mom_resid * resid[:, None] * w_sqrt\n    k_constraint = moms_test.shape[1]\n    cov = moms_test.T.dot(moms_test)\n    diff = moms_test.sum(0)\n    stat = diff.dot(np.linalg.solve(cov, diff))\n    stat2 = OLS(np.ones(nobs), moms_test).fit().ess\n    pval = stats.chi2.sf(stat, k_constraint)\n    return (stat, pval, stat2)"
        ]
    },
    {
        "func_name": "lm_robust",
        "original": "def lm_robust(score, constraint_matrix, score_deriv_inv, cov_score, cov_params=None):\n    \"\"\"general formula for score/LM test\n\n    generalized score or lagrange multiplier test for implicit constraints\n\n    `r(params) = 0`, with gradient `R = d r / d params`\n\n    linear constraints are given by `R params - q = 0`\n\n    It is assumed that all arrays are evaluated at the constrained estimates.\n\n    Parameters\n    ----------\n    score : ndarray, 1-D\n        derivative of objective function at estimated parameters\n        of constrained model\n    constraint_matrix R : ndarray\n        Linear restriction matrix or Jacobian of nonlinear constraints\n    hessian_inv, Ainv : ndarray, symmetric, square\n        inverse of second derivative of objective function\n        TODO: could be OPG or any other estimator if information matrix\n        equality holds\n    cov_score B :  ndarray, symmetric, square\n        covariance matrix of the score. This is the inner part of a sandwich\n        estimator.\n    cov_params V :  ndarray, symmetric, square\n        covariance of full parameter vector evaluated at constrained parameter\n        estimate. This can be specified instead of cov_score B.\n\n    Returns\n    -------\n    lm_stat : float\n        score/lagrange multiplier statistic\n\n    Notes\n    -----\n\n    \"\"\"\n    (R, Ainv, B, V) = (constraint_matrix, score_deriv_inv, cov_score, cov_params)\n    tmp = R.dot(Ainv)\n    wscore = tmp.dot(score)\n    if B is None and V is None:\n        lm_stat = score.dot(Ainv.dot(score))\n    else:\n        if V is None:\n            inner = tmp.dot(B).dot(tmp.T)\n        else:\n            inner = R.dot(V).dot(R.T)\n        lm_stat = wscore.dot(np.linalg.solve(inner, wscore))\n    return lm_stat",
        "mutated": [
            "def lm_robust(score, constraint_matrix, score_deriv_inv, cov_score, cov_params=None):\n    if False:\n        i = 10\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for implicit constraints\\n\\n    `r(params) = 0`, with gradient `R = d r / d params`\\n\\n    linear constraints are given by `R params - q = 0`\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    constraint_matrix R : ndarray\\n        Linear restriction matrix or Jacobian of nonlinear constraints\\n    hessian_inv, Ainv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n\\n    Notes\\n    -----\\n\\n    '\n    (R, Ainv, B, V) = (constraint_matrix, score_deriv_inv, cov_score, cov_params)\n    tmp = R.dot(Ainv)\n    wscore = tmp.dot(score)\n    if B is None and V is None:\n        lm_stat = score.dot(Ainv.dot(score))\n    else:\n        if V is None:\n            inner = tmp.dot(B).dot(tmp.T)\n        else:\n            inner = R.dot(V).dot(R.T)\n        lm_stat = wscore.dot(np.linalg.solve(inner, wscore))\n    return lm_stat",
            "def lm_robust(score, constraint_matrix, score_deriv_inv, cov_score, cov_params=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for implicit constraints\\n\\n    `r(params) = 0`, with gradient `R = d r / d params`\\n\\n    linear constraints are given by `R params - q = 0`\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    constraint_matrix R : ndarray\\n        Linear restriction matrix or Jacobian of nonlinear constraints\\n    hessian_inv, Ainv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n\\n    Notes\\n    -----\\n\\n    '\n    (R, Ainv, B, V) = (constraint_matrix, score_deriv_inv, cov_score, cov_params)\n    tmp = R.dot(Ainv)\n    wscore = tmp.dot(score)\n    if B is None and V is None:\n        lm_stat = score.dot(Ainv.dot(score))\n    else:\n        if V is None:\n            inner = tmp.dot(B).dot(tmp.T)\n        else:\n            inner = R.dot(V).dot(R.T)\n        lm_stat = wscore.dot(np.linalg.solve(inner, wscore))\n    return lm_stat",
            "def lm_robust(score, constraint_matrix, score_deriv_inv, cov_score, cov_params=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for implicit constraints\\n\\n    `r(params) = 0`, with gradient `R = d r / d params`\\n\\n    linear constraints are given by `R params - q = 0`\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    constraint_matrix R : ndarray\\n        Linear restriction matrix or Jacobian of nonlinear constraints\\n    hessian_inv, Ainv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n\\n    Notes\\n    -----\\n\\n    '\n    (R, Ainv, B, V) = (constraint_matrix, score_deriv_inv, cov_score, cov_params)\n    tmp = R.dot(Ainv)\n    wscore = tmp.dot(score)\n    if B is None and V is None:\n        lm_stat = score.dot(Ainv.dot(score))\n    else:\n        if V is None:\n            inner = tmp.dot(B).dot(tmp.T)\n        else:\n            inner = R.dot(V).dot(R.T)\n        lm_stat = wscore.dot(np.linalg.solve(inner, wscore))\n    return lm_stat",
            "def lm_robust(score, constraint_matrix, score_deriv_inv, cov_score, cov_params=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for implicit constraints\\n\\n    `r(params) = 0`, with gradient `R = d r / d params`\\n\\n    linear constraints are given by `R params - q = 0`\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    constraint_matrix R : ndarray\\n        Linear restriction matrix or Jacobian of nonlinear constraints\\n    hessian_inv, Ainv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n\\n    Notes\\n    -----\\n\\n    '\n    (R, Ainv, B, V) = (constraint_matrix, score_deriv_inv, cov_score, cov_params)\n    tmp = R.dot(Ainv)\n    wscore = tmp.dot(score)\n    if B is None and V is None:\n        lm_stat = score.dot(Ainv.dot(score))\n    else:\n        if V is None:\n            inner = tmp.dot(B).dot(tmp.T)\n        else:\n            inner = R.dot(V).dot(R.T)\n        lm_stat = wscore.dot(np.linalg.solve(inner, wscore))\n    return lm_stat",
            "def lm_robust(score, constraint_matrix, score_deriv_inv, cov_score, cov_params=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for implicit constraints\\n\\n    `r(params) = 0`, with gradient `R = d r / d params`\\n\\n    linear constraints are given by `R params - q = 0`\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    constraint_matrix R : ndarray\\n        Linear restriction matrix or Jacobian of nonlinear constraints\\n    hessian_inv, Ainv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n\\n    Notes\\n    -----\\n\\n    '\n    (R, Ainv, B, V) = (constraint_matrix, score_deriv_inv, cov_score, cov_params)\n    tmp = R.dot(Ainv)\n    wscore = tmp.dot(score)\n    if B is None and V is None:\n        lm_stat = score.dot(Ainv.dot(score))\n    else:\n        if V is None:\n            inner = tmp.dot(B).dot(tmp.T)\n        else:\n            inner = R.dot(V).dot(R.T)\n        lm_stat = wscore.dot(np.linalg.solve(inner, wscore))\n    return lm_stat"
        ]
    },
    {
        "func_name": "lm_robust_subset",
        "original": "def lm_robust_subset(score, k_constraints, score_deriv_inv, cov_score):\n    \"\"\"general formula for score/LM test\n\n    generalized score or lagrange multiplier test for constraints on a subset\n    of parameters\n\n    `params_1 = value`, where params_1 is a subset of the unconstrained\n    parameter vector.\n\n    It is assumed that all arrays are evaluated at the constrained estimates.\n\n    Parameters\n    ----------\n    score : ndarray, 1-D\n        derivative of objective function at estimated parameters\n        of constrained model\n    k_constraint : int\n        number of constraints\n    score_deriv_inv : ndarray, symmetric, square\n        inverse of second derivative of objective function\n        TODO: could be OPG or any other estimator if information matrix\n        equality holds\n    cov_score B :  ndarray, symmetric, square\n        covariance matrix of the score. This is the inner part of a sandwich\n        estimator.\n    not cov_params V :  ndarray, symmetric, square\n        covariance of full parameter vector evaluated at constrained parameter\n        estimate. This can be specified instead of cov_score B.\n\n    Returns\n    -------\n    lm_stat : float\n        score/lagrange multiplier statistic\n    p-value : float\n        p-value of the LM test based on chisquare distribution\n\n    Notes\n    -----\n    The implementation is based on Boos 1992 section 4.1. The same derivation\n    is also in other articles and in text books.\n\n    \"\"\"\n    h_uu = score_deriv_inv[:-k_constraints, :-k_constraints]\n    h_cu = score_deriv_inv[-k_constraints:, :-k_constraints]\n    tmp_proj = h_cu.dot(np.linalg.inv(h_uu))\n    tmp = np.column_stack((-tmp_proj, np.eye(k_constraints)))\n    cov_score_constraints = tmp.dot(cov_score.dot(tmp.T))\n    lm_stat = score.dot(np.linalg.solve(cov_score_constraints, score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
        "mutated": [
            "def lm_robust_subset(score, k_constraints, score_deriv_inv, cov_score):\n    if False:\n        i = 10\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for constraints on a subset\\n    of parameters\\n\\n    `params_1 = value`, where params_1 is a subset of the unconstrained\\n    parameter vector.\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    k_constraint : int\\n        number of constraints\\n    score_deriv_inv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    not cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    The implementation is based on Boos 1992 section 4.1. The same derivation\\n    is also in other articles and in text books.\\n\\n    '\n    h_uu = score_deriv_inv[:-k_constraints, :-k_constraints]\n    h_cu = score_deriv_inv[-k_constraints:, :-k_constraints]\n    tmp_proj = h_cu.dot(np.linalg.inv(h_uu))\n    tmp = np.column_stack((-tmp_proj, np.eye(k_constraints)))\n    cov_score_constraints = tmp.dot(cov_score.dot(tmp.T))\n    lm_stat = score.dot(np.linalg.solve(cov_score_constraints, score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
            "def lm_robust_subset(score, k_constraints, score_deriv_inv, cov_score):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for constraints on a subset\\n    of parameters\\n\\n    `params_1 = value`, where params_1 is a subset of the unconstrained\\n    parameter vector.\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    k_constraint : int\\n        number of constraints\\n    score_deriv_inv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    not cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    The implementation is based on Boos 1992 section 4.1. The same derivation\\n    is also in other articles and in text books.\\n\\n    '\n    h_uu = score_deriv_inv[:-k_constraints, :-k_constraints]\n    h_cu = score_deriv_inv[-k_constraints:, :-k_constraints]\n    tmp_proj = h_cu.dot(np.linalg.inv(h_uu))\n    tmp = np.column_stack((-tmp_proj, np.eye(k_constraints)))\n    cov_score_constraints = tmp.dot(cov_score.dot(tmp.T))\n    lm_stat = score.dot(np.linalg.solve(cov_score_constraints, score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
            "def lm_robust_subset(score, k_constraints, score_deriv_inv, cov_score):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for constraints on a subset\\n    of parameters\\n\\n    `params_1 = value`, where params_1 is a subset of the unconstrained\\n    parameter vector.\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    k_constraint : int\\n        number of constraints\\n    score_deriv_inv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    not cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    The implementation is based on Boos 1992 section 4.1. The same derivation\\n    is also in other articles and in text books.\\n\\n    '\n    h_uu = score_deriv_inv[:-k_constraints, :-k_constraints]\n    h_cu = score_deriv_inv[-k_constraints:, :-k_constraints]\n    tmp_proj = h_cu.dot(np.linalg.inv(h_uu))\n    tmp = np.column_stack((-tmp_proj, np.eye(k_constraints)))\n    cov_score_constraints = tmp.dot(cov_score.dot(tmp.T))\n    lm_stat = score.dot(np.linalg.solve(cov_score_constraints, score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
            "def lm_robust_subset(score, k_constraints, score_deriv_inv, cov_score):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for constraints on a subset\\n    of parameters\\n\\n    `params_1 = value`, where params_1 is a subset of the unconstrained\\n    parameter vector.\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    k_constraint : int\\n        number of constraints\\n    score_deriv_inv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    not cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    The implementation is based on Boos 1992 section 4.1. The same derivation\\n    is also in other articles and in text books.\\n\\n    '\n    h_uu = score_deriv_inv[:-k_constraints, :-k_constraints]\n    h_cu = score_deriv_inv[-k_constraints:, :-k_constraints]\n    tmp_proj = h_cu.dot(np.linalg.inv(h_uu))\n    tmp = np.column_stack((-tmp_proj, np.eye(k_constraints)))\n    cov_score_constraints = tmp.dot(cov_score.dot(tmp.T))\n    lm_stat = score.dot(np.linalg.solve(cov_score_constraints, score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
            "def lm_robust_subset(score, k_constraints, score_deriv_inv, cov_score):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'general formula for score/LM test\\n\\n    generalized score or lagrange multiplier test for constraints on a subset\\n    of parameters\\n\\n    `params_1 = value`, where params_1 is a subset of the unconstrained\\n    parameter vector.\\n\\n    It is assumed that all arrays are evaluated at the constrained estimates.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    k_constraint : int\\n        number of constraints\\n    score_deriv_inv : ndarray, symmetric, square\\n        inverse of second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n    not cov_params V :  ndarray, symmetric, square\\n        covariance of full parameter vector evaluated at constrained parameter\\n        estimate. This can be specified instead of cov_score B.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    The implementation is based on Boos 1992 section 4.1. The same derivation\\n    is also in other articles and in text books.\\n\\n    '\n    h_uu = score_deriv_inv[:-k_constraints, :-k_constraints]\n    h_cu = score_deriv_inv[-k_constraints:, :-k_constraints]\n    tmp_proj = h_cu.dot(np.linalg.inv(h_uu))\n    tmp = np.column_stack((-tmp_proj, np.eye(k_constraints)))\n    cov_score_constraints = tmp.dot(cov_score.dot(tmp.T))\n    lm_stat = score.dot(np.linalg.solve(cov_score_constraints, score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)"
        ]
    },
    {
        "func_name": "lm_robust_subset_parts",
        "original": "def lm_robust_subset_parts(score, k_constraints, score_deriv_uu, score_deriv_cu, cov_score_cc, cov_score_cu, cov_score_uu):\n    \"\"\"robust generalized score tests on subset of parameters\n\n    This is the same as lm_robust_subset with arguments in parts of\n    partitioned matrices.\n    This can be useful, when we have the parts based on different estimation\n    procedures, i.e. when we do not have the full unconstrained model.\n\n    Calculates mainly the covariance of the constraint part of the score.\n\n    Parameters\n    ----------\n    score : ndarray, 1-D\n        derivative of objective function at estimated parameters\n        of constrained model. These is the score component for the restricted\n        part under hypothesis. The unconstrained part of the score is assumed\n        to be zero.\n    k_constraint : int\n        number of constraints\n    score_deriv_uu : ndarray, symmetric, square\n        first derivative of moment equation or second derivative of objective\n        function for the unconstrained part\n        TODO: could be OPG or any other estimator if information matrix\n        equality holds\n    score_deriv_cu : ndarray\n        first cross derivative of moment equation or second cross\n        derivative of objective function between.\n    cov_score_cc :  ndarray\n        covariance matrix of the score for the unconstrained part.\n        This is the inner part of a sandwich estimator.\n    cov_score_cu :  ndarray\n        covariance matrix of the score for the off-diagonal block, i.e.\n        covariance between constrained and unconstrained part.\n    cov_score_uu :  ndarray\n        covariance matrix of the score for the unconstrained part.\n\n    Returns\n    -------\n    lm_stat : float\n        score/lagrange multiplier statistic\n    p-value : float\n        p-value of the LM test based on chisquare distribution\n\n    Notes\n    -----\n    TODO: these function should just return the covariance of the score\n    instead of calculating the score/lm test.\n\n    Implementation similar to lm_robust_subset and is based on Boos 1992,\n    section 4.1 in the form attributed to Breslow (1990). It does not use the\n    computation attributed to Kent (1982) and Engle (1984).\n    \"\"\"\n    tmp_proj = np.linalg.solve(score_deriv_uu, score_deriv_cu.T).T\n    tmp = tmp_proj.dot(cov_score_cu.T)\n    cov = cov_score_cc - tmp\n    cov -= tmp.T\n    cov += tmp_proj.dot(cov_score_uu).dot(tmp_proj.T)\n    lm_stat = score.dot(np.linalg.solve(cov, score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
        "mutated": [
            "def lm_robust_subset_parts(score, k_constraints, score_deriv_uu, score_deriv_cu, cov_score_cc, cov_score_cu, cov_score_uu):\n    if False:\n        i = 10\n    'robust generalized score tests on subset of parameters\\n\\n    This is the same as lm_robust_subset with arguments in parts of\\n    partitioned matrices.\\n    This can be useful, when we have the parts based on different estimation\\n    procedures, i.e. when we do not have the full unconstrained model.\\n\\n    Calculates mainly the covariance of the constraint part of the score.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model. These is the score component for the restricted\\n        part under hypothesis. The unconstrained part of the score is assumed\\n        to be zero.\\n    k_constraint : int\\n        number of constraints\\n    score_deriv_uu : ndarray, symmetric, square\\n        first derivative of moment equation or second derivative of objective\\n        function for the unconstrained part\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    score_deriv_cu : ndarray\\n        first cross derivative of moment equation or second cross\\n        derivative of objective function between.\\n    cov_score_cc :  ndarray\\n        covariance matrix of the score for the unconstrained part.\\n        This is the inner part of a sandwich estimator.\\n    cov_score_cu :  ndarray\\n        covariance matrix of the score for the off-diagonal block, i.e.\\n        covariance between constrained and unconstrained part.\\n    cov_score_uu :  ndarray\\n        covariance matrix of the score for the unconstrained part.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    TODO: these function should just return the covariance of the score\\n    instead of calculating the score/lm test.\\n\\n    Implementation similar to lm_robust_subset and is based on Boos 1992,\\n    section 4.1 in the form attributed to Breslow (1990). It does not use the\\n    computation attributed to Kent (1982) and Engle (1984).\\n    '\n    tmp_proj = np.linalg.solve(score_deriv_uu, score_deriv_cu.T).T\n    tmp = tmp_proj.dot(cov_score_cu.T)\n    cov = cov_score_cc - tmp\n    cov -= tmp.T\n    cov += tmp_proj.dot(cov_score_uu).dot(tmp_proj.T)\n    lm_stat = score.dot(np.linalg.solve(cov, score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
            "def lm_robust_subset_parts(score, k_constraints, score_deriv_uu, score_deriv_cu, cov_score_cc, cov_score_cu, cov_score_uu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'robust generalized score tests on subset of parameters\\n\\n    This is the same as lm_robust_subset with arguments in parts of\\n    partitioned matrices.\\n    This can be useful, when we have the parts based on different estimation\\n    procedures, i.e. when we do not have the full unconstrained model.\\n\\n    Calculates mainly the covariance of the constraint part of the score.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model. These is the score component for the restricted\\n        part under hypothesis. The unconstrained part of the score is assumed\\n        to be zero.\\n    k_constraint : int\\n        number of constraints\\n    score_deriv_uu : ndarray, symmetric, square\\n        first derivative of moment equation or second derivative of objective\\n        function for the unconstrained part\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    score_deriv_cu : ndarray\\n        first cross derivative of moment equation or second cross\\n        derivative of objective function between.\\n    cov_score_cc :  ndarray\\n        covariance matrix of the score for the unconstrained part.\\n        This is the inner part of a sandwich estimator.\\n    cov_score_cu :  ndarray\\n        covariance matrix of the score for the off-diagonal block, i.e.\\n        covariance between constrained and unconstrained part.\\n    cov_score_uu :  ndarray\\n        covariance matrix of the score for the unconstrained part.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    TODO: these function should just return the covariance of the score\\n    instead of calculating the score/lm test.\\n\\n    Implementation similar to lm_robust_subset and is based on Boos 1992,\\n    section 4.1 in the form attributed to Breslow (1990). It does not use the\\n    computation attributed to Kent (1982) and Engle (1984).\\n    '\n    tmp_proj = np.linalg.solve(score_deriv_uu, score_deriv_cu.T).T\n    tmp = tmp_proj.dot(cov_score_cu.T)\n    cov = cov_score_cc - tmp\n    cov -= tmp.T\n    cov += tmp_proj.dot(cov_score_uu).dot(tmp_proj.T)\n    lm_stat = score.dot(np.linalg.solve(cov, score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
            "def lm_robust_subset_parts(score, k_constraints, score_deriv_uu, score_deriv_cu, cov_score_cc, cov_score_cu, cov_score_uu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'robust generalized score tests on subset of parameters\\n\\n    This is the same as lm_robust_subset with arguments in parts of\\n    partitioned matrices.\\n    This can be useful, when we have the parts based on different estimation\\n    procedures, i.e. when we do not have the full unconstrained model.\\n\\n    Calculates mainly the covariance of the constraint part of the score.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model. These is the score component for the restricted\\n        part under hypothesis. The unconstrained part of the score is assumed\\n        to be zero.\\n    k_constraint : int\\n        number of constraints\\n    score_deriv_uu : ndarray, symmetric, square\\n        first derivative of moment equation or second derivative of objective\\n        function for the unconstrained part\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    score_deriv_cu : ndarray\\n        first cross derivative of moment equation or second cross\\n        derivative of objective function between.\\n    cov_score_cc :  ndarray\\n        covariance matrix of the score for the unconstrained part.\\n        This is the inner part of a sandwich estimator.\\n    cov_score_cu :  ndarray\\n        covariance matrix of the score for the off-diagonal block, i.e.\\n        covariance between constrained and unconstrained part.\\n    cov_score_uu :  ndarray\\n        covariance matrix of the score for the unconstrained part.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    TODO: these function should just return the covariance of the score\\n    instead of calculating the score/lm test.\\n\\n    Implementation similar to lm_robust_subset and is based on Boos 1992,\\n    section 4.1 in the form attributed to Breslow (1990). It does not use the\\n    computation attributed to Kent (1982) and Engle (1984).\\n    '\n    tmp_proj = np.linalg.solve(score_deriv_uu, score_deriv_cu.T).T\n    tmp = tmp_proj.dot(cov_score_cu.T)\n    cov = cov_score_cc - tmp\n    cov -= tmp.T\n    cov += tmp_proj.dot(cov_score_uu).dot(tmp_proj.T)\n    lm_stat = score.dot(np.linalg.solve(cov, score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
            "def lm_robust_subset_parts(score, k_constraints, score_deriv_uu, score_deriv_cu, cov_score_cc, cov_score_cu, cov_score_uu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'robust generalized score tests on subset of parameters\\n\\n    This is the same as lm_robust_subset with arguments in parts of\\n    partitioned matrices.\\n    This can be useful, when we have the parts based on different estimation\\n    procedures, i.e. when we do not have the full unconstrained model.\\n\\n    Calculates mainly the covariance of the constraint part of the score.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model. These is the score component for the restricted\\n        part under hypothesis. The unconstrained part of the score is assumed\\n        to be zero.\\n    k_constraint : int\\n        number of constraints\\n    score_deriv_uu : ndarray, symmetric, square\\n        first derivative of moment equation or second derivative of objective\\n        function for the unconstrained part\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    score_deriv_cu : ndarray\\n        first cross derivative of moment equation or second cross\\n        derivative of objective function between.\\n    cov_score_cc :  ndarray\\n        covariance matrix of the score for the unconstrained part.\\n        This is the inner part of a sandwich estimator.\\n    cov_score_cu :  ndarray\\n        covariance matrix of the score for the off-diagonal block, i.e.\\n        covariance between constrained and unconstrained part.\\n    cov_score_uu :  ndarray\\n        covariance matrix of the score for the unconstrained part.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    TODO: these function should just return the covariance of the score\\n    instead of calculating the score/lm test.\\n\\n    Implementation similar to lm_robust_subset and is based on Boos 1992,\\n    section 4.1 in the form attributed to Breslow (1990). It does not use the\\n    computation attributed to Kent (1982) and Engle (1984).\\n    '\n    tmp_proj = np.linalg.solve(score_deriv_uu, score_deriv_cu.T).T\n    tmp = tmp_proj.dot(cov_score_cu.T)\n    cov = cov_score_cc - tmp\n    cov -= tmp.T\n    cov += tmp_proj.dot(cov_score_uu).dot(tmp_proj.T)\n    lm_stat = score.dot(np.linalg.solve(cov, score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
            "def lm_robust_subset_parts(score, k_constraints, score_deriv_uu, score_deriv_cu, cov_score_cc, cov_score_cu, cov_score_uu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'robust generalized score tests on subset of parameters\\n\\n    This is the same as lm_robust_subset with arguments in parts of\\n    partitioned matrices.\\n    This can be useful, when we have the parts based on different estimation\\n    procedures, i.e. when we do not have the full unconstrained model.\\n\\n    Calculates mainly the covariance of the constraint part of the score.\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model. These is the score component for the restricted\\n        part under hypothesis. The unconstrained part of the score is assumed\\n        to be zero.\\n    k_constraint : int\\n        number of constraints\\n    score_deriv_uu : ndarray, symmetric, square\\n        first derivative of moment equation or second derivative of objective\\n        function for the unconstrained part\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    score_deriv_cu : ndarray\\n        first cross derivative of moment equation or second cross\\n        derivative of objective function between.\\n    cov_score_cc :  ndarray\\n        covariance matrix of the score for the unconstrained part.\\n        This is the inner part of a sandwich estimator.\\n    cov_score_cu :  ndarray\\n        covariance matrix of the score for the off-diagonal block, i.e.\\n        covariance between constrained and unconstrained part.\\n    cov_score_uu :  ndarray\\n        covariance matrix of the score for the unconstrained part.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    TODO: these function should just return the covariance of the score\\n    instead of calculating the score/lm test.\\n\\n    Implementation similar to lm_robust_subset and is based on Boos 1992,\\n    section 4.1 in the form attributed to Breslow (1990). It does not use the\\n    computation attributed to Kent (1982) and Engle (1984).\\n    '\n    tmp_proj = np.linalg.solve(score_deriv_uu, score_deriv_cu.T).T\n    tmp = tmp_proj.dot(cov_score_cu.T)\n    cov = cov_score_cc - tmp\n    cov -= tmp.T\n    cov += tmp_proj.dot(cov_score_uu).dot(tmp_proj.T)\n    lm_stat = score.dot(np.linalg.solve(cov, score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)"
        ]
    },
    {
        "func_name": "lm_robust_reparameterized",
        "original": "def lm_robust_reparameterized(score, params_deriv, score_deriv, cov_score):\n    \"\"\"robust generalized score test for transformed parameters\n\n    The parameters are given by a nonlinear transformation of the estimated\n    reduced parameters\n\n    `params = g(params_reduced)`  with jacobian `G = d g / d params_reduced`\n\n    score and other arrays are for full parameter space `params`\n\n    Parameters\n    ----------\n    score : ndarray, 1-D\n        derivative of objective function at estimated parameters\n        of constrained model\n    params_deriv : ndarray\n        Jacobian G of the parameter trasnformation\n    score_deriv : ndarray, symmetric, square\n        second derivative of objective function\n        TODO: could be OPG or any other estimator if information matrix\n        equality holds\n    cov_score B :  ndarray, symmetric, square\n        covariance matrix of the score. This is the inner part of a sandwich\n        estimator.\n\n    Returns\n    -------\n    lm_stat : float\n        score/lagrange multiplier statistic\n    p-value : float\n        p-value of the LM test based on chisquare distribution\n\n    Notes\n    -----\n    Boos 1992, section 4.3, expression for T_{GS} just before example 6\n    \"\"\"\n    (k_params, k_reduced) = params_deriv.shape\n    k_constraints = k_params - k_reduced\n    G = params_deriv\n    tmp_c0 = np.linalg.pinv(G.T.dot(score_deriv.dot(G)))\n    tmp_c1 = score_deriv.dot(G.dot(tmp_c0.dot(G.T)))\n    tmp_c = np.eye(k_params) - tmp_c1\n    cov = tmp_c.dot(cov_score.dot(tmp_c.T))\n    lm_stat = score.dot(np.linalg.pinv(cov).dot(score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
        "mutated": [
            "def lm_robust_reparameterized(score, params_deriv, score_deriv, cov_score):\n    if False:\n        i = 10\n    'robust generalized score test for transformed parameters\\n\\n    The parameters are given by a nonlinear transformation of the estimated\\n    reduced parameters\\n\\n    `params = g(params_reduced)`  with jacobian `G = d g / d params_reduced`\\n\\n    score and other arrays are for full parameter space `params`\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    params_deriv : ndarray\\n        Jacobian G of the parameter trasnformation\\n    score_deriv : ndarray, symmetric, square\\n        second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    Boos 1992, section 4.3, expression for T_{GS} just before example 6\\n    '\n    (k_params, k_reduced) = params_deriv.shape\n    k_constraints = k_params - k_reduced\n    G = params_deriv\n    tmp_c0 = np.linalg.pinv(G.T.dot(score_deriv.dot(G)))\n    tmp_c1 = score_deriv.dot(G.dot(tmp_c0.dot(G.T)))\n    tmp_c = np.eye(k_params) - tmp_c1\n    cov = tmp_c.dot(cov_score.dot(tmp_c.T))\n    lm_stat = score.dot(np.linalg.pinv(cov).dot(score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
            "def lm_robust_reparameterized(score, params_deriv, score_deriv, cov_score):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'robust generalized score test for transformed parameters\\n\\n    The parameters are given by a nonlinear transformation of the estimated\\n    reduced parameters\\n\\n    `params = g(params_reduced)`  with jacobian `G = d g / d params_reduced`\\n\\n    score and other arrays are for full parameter space `params`\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    params_deriv : ndarray\\n        Jacobian G of the parameter trasnformation\\n    score_deriv : ndarray, symmetric, square\\n        second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    Boos 1992, section 4.3, expression for T_{GS} just before example 6\\n    '\n    (k_params, k_reduced) = params_deriv.shape\n    k_constraints = k_params - k_reduced\n    G = params_deriv\n    tmp_c0 = np.linalg.pinv(G.T.dot(score_deriv.dot(G)))\n    tmp_c1 = score_deriv.dot(G.dot(tmp_c0.dot(G.T)))\n    tmp_c = np.eye(k_params) - tmp_c1\n    cov = tmp_c.dot(cov_score.dot(tmp_c.T))\n    lm_stat = score.dot(np.linalg.pinv(cov).dot(score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
            "def lm_robust_reparameterized(score, params_deriv, score_deriv, cov_score):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'robust generalized score test for transformed parameters\\n\\n    The parameters are given by a nonlinear transformation of the estimated\\n    reduced parameters\\n\\n    `params = g(params_reduced)`  with jacobian `G = d g / d params_reduced`\\n\\n    score and other arrays are for full parameter space `params`\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    params_deriv : ndarray\\n        Jacobian G of the parameter trasnformation\\n    score_deriv : ndarray, symmetric, square\\n        second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    Boos 1992, section 4.3, expression for T_{GS} just before example 6\\n    '\n    (k_params, k_reduced) = params_deriv.shape\n    k_constraints = k_params - k_reduced\n    G = params_deriv\n    tmp_c0 = np.linalg.pinv(G.T.dot(score_deriv.dot(G)))\n    tmp_c1 = score_deriv.dot(G.dot(tmp_c0.dot(G.T)))\n    tmp_c = np.eye(k_params) - tmp_c1\n    cov = tmp_c.dot(cov_score.dot(tmp_c.T))\n    lm_stat = score.dot(np.linalg.pinv(cov).dot(score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
            "def lm_robust_reparameterized(score, params_deriv, score_deriv, cov_score):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'robust generalized score test for transformed parameters\\n\\n    The parameters are given by a nonlinear transformation of the estimated\\n    reduced parameters\\n\\n    `params = g(params_reduced)`  with jacobian `G = d g / d params_reduced`\\n\\n    score and other arrays are for full parameter space `params`\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    params_deriv : ndarray\\n        Jacobian G of the parameter trasnformation\\n    score_deriv : ndarray, symmetric, square\\n        second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    Boos 1992, section 4.3, expression for T_{GS} just before example 6\\n    '\n    (k_params, k_reduced) = params_deriv.shape\n    k_constraints = k_params - k_reduced\n    G = params_deriv\n    tmp_c0 = np.linalg.pinv(G.T.dot(score_deriv.dot(G)))\n    tmp_c1 = score_deriv.dot(G.dot(tmp_c0.dot(G.T)))\n    tmp_c = np.eye(k_params) - tmp_c1\n    cov = tmp_c.dot(cov_score.dot(tmp_c.T))\n    lm_stat = score.dot(np.linalg.pinv(cov).dot(score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)",
            "def lm_robust_reparameterized(score, params_deriv, score_deriv, cov_score):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'robust generalized score test for transformed parameters\\n\\n    The parameters are given by a nonlinear transformation of the estimated\\n    reduced parameters\\n\\n    `params = g(params_reduced)`  with jacobian `G = d g / d params_reduced`\\n\\n    score and other arrays are for full parameter space `params`\\n\\n    Parameters\\n    ----------\\n    score : ndarray, 1-D\\n        derivative of objective function at estimated parameters\\n        of constrained model\\n    params_deriv : ndarray\\n        Jacobian G of the parameter trasnformation\\n    score_deriv : ndarray, symmetric, square\\n        second derivative of objective function\\n        TODO: could be OPG or any other estimator if information matrix\\n        equality holds\\n    cov_score B :  ndarray, symmetric, square\\n        covariance matrix of the score. This is the inner part of a sandwich\\n        estimator.\\n\\n    Returns\\n    -------\\n    lm_stat : float\\n        score/lagrange multiplier statistic\\n    p-value : float\\n        p-value of the LM test based on chisquare distribution\\n\\n    Notes\\n    -----\\n    Boos 1992, section 4.3, expression for T_{GS} just before example 6\\n    '\n    (k_params, k_reduced) = params_deriv.shape\n    k_constraints = k_params - k_reduced\n    G = params_deriv\n    tmp_c0 = np.linalg.pinv(G.T.dot(score_deriv.dot(G)))\n    tmp_c1 = score_deriv.dot(G.dot(tmp_c0.dot(G.T)))\n    tmp_c = np.eye(k_params) - tmp_c1\n    cov = tmp_c.dot(cov_score.dot(tmp_c.T))\n    lm_stat = score.dot(np.linalg.pinv(cov).dot(score))\n    pval = stats.chi2.sf(lm_stat, k_constraints)\n    return (lm_stat, pval)"
        ]
    },
    {
        "func_name": "conditional_moment_test_generic",
        "original": "def conditional_moment_test_generic(mom_test, mom_test_deriv, mom_incl, mom_incl_deriv, var_mom_all=None, cov_type='OPG', cov_kwds=None):\n    \"\"\"generic conditional moment test\n\n    This is mainly intended as internal function in support of diagnostic\n    and specification tests. It has no conversion and checking of correct\n    arguments.\n\n    Parameters\n    ----------\n    mom_test : ndarray, 2-D (nobs, k_constraints)\n        moment conditions that will be tested to be zero\n    mom_test_deriv : ndarray, 2-D, square (k_constraints, k_constraints)\n        derivative of moment conditions under test with respect to the\n        parameters of the model summed over observations.\n    mom_incl : ndarray, 2-D (nobs, k_params)\n        moment conditions that where use in estimation, assumed to be zero\n        This is score_obs in the case of (Q)MLE\n    mom_incl_deriv : ndarray, 2-D, square (k_params, k_params)\n        derivative of moment conditions of estimator summed over observations\n        This is the information matrix or Hessian in the case of (Q)MLE.\n    var_mom_all : None, or ndarray, 2-D, (k, k) with k = k_constraints + k_params\n        Expected product or variance of the joint (column_stacked) moment\n        conditions. The stacking should have the variance of the moment\n        conditions under test in the first k_constraint rows and columns.\n        If it is not None, then it will be estimated based on cov_type.\n        I think: This is the Hessian of the extended or alternative model\n        under full MLE and score test assuming information matrix identity\n        holds.\n\n    Returns\n    -------\n    results\n\n    Notes\n    -----\n    TODO: cov_type other than OPG is missing\n    initial implementation based on Cameron Trived countbook 1998 p.48, p.56\n\n    also included: mom_incl can be None if expected mom_test_deriv is zero.\n\n    References\n    ----------\n    Cameron and Trivedi 1998 count book\n    Wooldridge ???\n    Pagan and Vella 1989\n    \"\"\"\n    if cov_type != 'OPG':\n        raise NotImplementedError\n    k_constraints = mom_test.shape[1]\n    if mom_incl is None:\n        if var_mom_all is None:\n            var_cm = mom_test.T.dot(mom_test)\n        else:\n            var_cm = var_mom_all\n    else:\n        if var_mom_all is None:\n            mom_all = np.column_stack((mom_test, mom_incl))\n            var_mom_all = mom_all.T.dot(mom_all)\n        tmp = mom_test_deriv.dot(np.linalg.pinv(mom_incl_deriv))\n        h = np.column_stack((np.eye(k_constraints), -tmp))\n        var_cm = h.dot(var_mom_all.dot(h.T))\n    var_cm_inv = np.linalg.pinv(var_cm)\n    mom_test_sum = mom_test.sum(0)\n    statistic = mom_test_sum.dot(var_cm_inv.dot(mom_test_sum))\n    pval = stats.chi2.sf(statistic, k_constraints)\n    se = np.sqrt(np.diag(var_cm))\n    tvalues = mom_test_sum / se\n    pvalues = stats.norm.sf(np.abs(tvalues))\n    res = ResultsGeneric(var_cm=var_cm, stat_cmt=statistic, pval_cmt=pval, tvalues=tvalues, pvalues=pvalues)\n    return res",
        "mutated": [
            "def conditional_moment_test_generic(mom_test, mom_test_deriv, mom_incl, mom_incl_deriv, var_mom_all=None, cov_type='OPG', cov_kwds=None):\n    if False:\n        i = 10\n    'generic conditional moment test\\n\\n    This is mainly intended as internal function in support of diagnostic\\n    and specification tests. It has no conversion and checking of correct\\n    arguments.\\n\\n    Parameters\\n    ----------\\n    mom_test : ndarray, 2-D (nobs, k_constraints)\\n        moment conditions that will be tested to be zero\\n    mom_test_deriv : ndarray, 2-D, square (k_constraints, k_constraints)\\n        derivative of moment conditions under test with respect to the\\n        parameters of the model summed over observations.\\n    mom_incl : ndarray, 2-D (nobs, k_params)\\n        moment conditions that where use in estimation, assumed to be zero\\n        This is score_obs in the case of (Q)MLE\\n    mom_incl_deriv : ndarray, 2-D, square (k_params, k_params)\\n        derivative of moment conditions of estimator summed over observations\\n        This is the information matrix or Hessian in the case of (Q)MLE.\\n    var_mom_all : None, or ndarray, 2-D, (k, k) with k = k_constraints + k_params\\n        Expected product or variance of the joint (column_stacked) moment\\n        conditions. The stacking should have the variance of the moment\\n        conditions under test in the first k_constraint rows and columns.\\n        If it is not None, then it will be estimated based on cov_type.\\n        I think: This is the Hessian of the extended or alternative model\\n        under full MLE and score test assuming information matrix identity\\n        holds.\\n\\n    Returns\\n    -------\\n    results\\n\\n    Notes\\n    -----\\n    TODO: cov_type other than OPG is missing\\n    initial implementation based on Cameron Trived countbook 1998 p.48, p.56\\n\\n    also included: mom_incl can be None if expected mom_test_deriv is zero.\\n\\n    References\\n    ----------\\n    Cameron and Trivedi 1998 count book\\n    Wooldridge ???\\n    Pagan and Vella 1989\\n    '\n    if cov_type != 'OPG':\n        raise NotImplementedError\n    k_constraints = mom_test.shape[1]\n    if mom_incl is None:\n        if var_mom_all is None:\n            var_cm = mom_test.T.dot(mom_test)\n        else:\n            var_cm = var_mom_all\n    else:\n        if var_mom_all is None:\n            mom_all = np.column_stack((mom_test, mom_incl))\n            var_mom_all = mom_all.T.dot(mom_all)\n        tmp = mom_test_deriv.dot(np.linalg.pinv(mom_incl_deriv))\n        h = np.column_stack((np.eye(k_constraints), -tmp))\n        var_cm = h.dot(var_mom_all.dot(h.T))\n    var_cm_inv = np.linalg.pinv(var_cm)\n    mom_test_sum = mom_test.sum(0)\n    statistic = mom_test_sum.dot(var_cm_inv.dot(mom_test_sum))\n    pval = stats.chi2.sf(statistic, k_constraints)\n    se = np.sqrt(np.diag(var_cm))\n    tvalues = mom_test_sum / se\n    pvalues = stats.norm.sf(np.abs(tvalues))\n    res = ResultsGeneric(var_cm=var_cm, stat_cmt=statistic, pval_cmt=pval, tvalues=tvalues, pvalues=pvalues)\n    return res",
            "def conditional_moment_test_generic(mom_test, mom_test_deriv, mom_incl, mom_incl_deriv, var_mom_all=None, cov_type='OPG', cov_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'generic conditional moment test\\n\\n    This is mainly intended as internal function in support of diagnostic\\n    and specification tests. It has no conversion and checking of correct\\n    arguments.\\n\\n    Parameters\\n    ----------\\n    mom_test : ndarray, 2-D (nobs, k_constraints)\\n        moment conditions that will be tested to be zero\\n    mom_test_deriv : ndarray, 2-D, square (k_constraints, k_constraints)\\n        derivative of moment conditions under test with respect to the\\n        parameters of the model summed over observations.\\n    mom_incl : ndarray, 2-D (nobs, k_params)\\n        moment conditions that where use in estimation, assumed to be zero\\n        This is score_obs in the case of (Q)MLE\\n    mom_incl_deriv : ndarray, 2-D, square (k_params, k_params)\\n        derivative of moment conditions of estimator summed over observations\\n        This is the information matrix or Hessian in the case of (Q)MLE.\\n    var_mom_all : None, or ndarray, 2-D, (k, k) with k = k_constraints + k_params\\n        Expected product or variance of the joint (column_stacked) moment\\n        conditions. The stacking should have the variance of the moment\\n        conditions under test in the first k_constraint rows and columns.\\n        If it is not None, then it will be estimated based on cov_type.\\n        I think: This is the Hessian of the extended or alternative model\\n        under full MLE and score test assuming information matrix identity\\n        holds.\\n\\n    Returns\\n    -------\\n    results\\n\\n    Notes\\n    -----\\n    TODO: cov_type other than OPG is missing\\n    initial implementation based on Cameron Trived countbook 1998 p.48, p.56\\n\\n    also included: mom_incl can be None if expected mom_test_deriv is zero.\\n\\n    References\\n    ----------\\n    Cameron and Trivedi 1998 count book\\n    Wooldridge ???\\n    Pagan and Vella 1989\\n    '\n    if cov_type != 'OPG':\n        raise NotImplementedError\n    k_constraints = mom_test.shape[1]\n    if mom_incl is None:\n        if var_mom_all is None:\n            var_cm = mom_test.T.dot(mom_test)\n        else:\n            var_cm = var_mom_all\n    else:\n        if var_mom_all is None:\n            mom_all = np.column_stack((mom_test, mom_incl))\n            var_mom_all = mom_all.T.dot(mom_all)\n        tmp = mom_test_deriv.dot(np.linalg.pinv(mom_incl_deriv))\n        h = np.column_stack((np.eye(k_constraints), -tmp))\n        var_cm = h.dot(var_mom_all.dot(h.T))\n    var_cm_inv = np.linalg.pinv(var_cm)\n    mom_test_sum = mom_test.sum(0)\n    statistic = mom_test_sum.dot(var_cm_inv.dot(mom_test_sum))\n    pval = stats.chi2.sf(statistic, k_constraints)\n    se = np.sqrt(np.diag(var_cm))\n    tvalues = mom_test_sum / se\n    pvalues = stats.norm.sf(np.abs(tvalues))\n    res = ResultsGeneric(var_cm=var_cm, stat_cmt=statistic, pval_cmt=pval, tvalues=tvalues, pvalues=pvalues)\n    return res",
            "def conditional_moment_test_generic(mom_test, mom_test_deriv, mom_incl, mom_incl_deriv, var_mom_all=None, cov_type='OPG', cov_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'generic conditional moment test\\n\\n    This is mainly intended as internal function in support of diagnostic\\n    and specification tests. It has no conversion and checking of correct\\n    arguments.\\n\\n    Parameters\\n    ----------\\n    mom_test : ndarray, 2-D (nobs, k_constraints)\\n        moment conditions that will be tested to be zero\\n    mom_test_deriv : ndarray, 2-D, square (k_constraints, k_constraints)\\n        derivative of moment conditions under test with respect to the\\n        parameters of the model summed over observations.\\n    mom_incl : ndarray, 2-D (nobs, k_params)\\n        moment conditions that where use in estimation, assumed to be zero\\n        This is score_obs in the case of (Q)MLE\\n    mom_incl_deriv : ndarray, 2-D, square (k_params, k_params)\\n        derivative of moment conditions of estimator summed over observations\\n        This is the information matrix or Hessian in the case of (Q)MLE.\\n    var_mom_all : None, or ndarray, 2-D, (k, k) with k = k_constraints + k_params\\n        Expected product or variance of the joint (column_stacked) moment\\n        conditions. The stacking should have the variance of the moment\\n        conditions under test in the first k_constraint rows and columns.\\n        If it is not None, then it will be estimated based on cov_type.\\n        I think: This is the Hessian of the extended or alternative model\\n        under full MLE and score test assuming information matrix identity\\n        holds.\\n\\n    Returns\\n    -------\\n    results\\n\\n    Notes\\n    -----\\n    TODO: cov_type other than OPG is missing\\n    initial implementation based on Cameron Trived countbook 1998 p.48, p.56\\n\\n    also included: mom_incl can be None if expected mom_test_deriv is zero.\\n\\n    References\\n    ----------\\n    Cameron and Trivedi 1998 count book\\n    Wooldridge ???\\n    Pagan and Vella 1989\\n    '\n    if cov_type != 'OPG':\n        raise NotImplementedError\n    k_constraints = mom_test.shape[1]\n    if mom_incl is None:\n        if var_mom_all is None:\n            var_cm = mom_test.T.dot(mom_test)\n        else:\n            var_cm = var_mom_all\n    else:\n        if var_mom_all is None:\n            mom_all = np.column_stack((mom_test, mom_incl))\n            var_mom_all = mom_all.T.dot(mom_all)\n        tmp = mom_test_deriv.dot(np.linalg.pinv(mom_incl_deriv))\n        h = np.column_stack((np.eye(k_constraints), -tmp))\n        var_cm = h.dot(var_mom_all.dot(h.T))\n    var_cm_inv = np.linalg.pinv(var_cm)\n    mom_test_sum = mom_test.sum(0)\n    statistic = mom_test_sum.dot(var_cm_inv.dot(mom_test_sum))\n    pval = stats.chi2.sf(statistic, k_constraints)\n    se = np.sqrt(np.diag(var_cm))\n    tvalues = mom_test_sum / se\n    pvalues = stats.norm.sf(np.abs(tvalues))\n    res = ResultsGeneric(var_cm=var_cm, stat_cmt=statistic, pval_cmt=pval, tvalues=tvalues, pvalues=pvalues)\n    return res",
            "def conditional_moment_test_generic(mom_test, mom_test_deriv, mom_incl, mom_incl_deriv, var_mom_all=None, cov_type='OPG', cov_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'generic conditional moment test\\n\\n    This is mainly intended as internal function in support of diagnostic\\n    and specification tests. It has no conversion and checking of correct\\n    arguments.\\n\\n    Parameters\\n    ----------\\n    mom_test : ndarray, 2-D (nobs, k_constraints)\\n        moment conditions that will be tested to be zero\\n    mom_test_deriv : ndarray, 2-D, square (k_constraints, k_constraints)\\n        derivative of moment conditions under test with respect to the\\n        parameters of the model summed over observations.\\n    mom_incl : ndarray, 2-D (nobs, k_params)\\n        moment conditions that where use in estimation, assumed to be zero\\n        This is score_obs in the case of (Q)MLE\\n    mom_incl_deriv : ndarray, 2-D, square (k_params, k_params)\\n        derivative of moment conditions of estimator summed over observations\\n        This is the information matrix or Hessian in the case of (Q)MLE.\\n    var_mom_all : None, or ndarray, 2-D, (k, k) with k = k_constraints + k_params\\n        Expected product or variance of the joint (column_stacked) moment\\n        conditions. The stacking should have the variance of the moment\\n        conditions under test in the first k_constraint rows and columns.\\n        If it is not None, then it will be estimated based on cov_type.\\n        I think: This is the Hessian of the extended or alternative model\\n        under full MLE and score test assuming information matrix identity\\n        holds.\\n\\n    Returns\\n    -------\\n    results\\n\\n    Notes\\n    -----\\n    TODO: cov_type other than OPG is missing\\n    initial implementation based on Cameron Trived countbook 1998 p.48, p.56\\n\\n    also included: mom_incl can be None if expected mom_test_deriv is zero.\\n\\n    References\\n    ----------\\n    Cameron and Trivedi 1998 count book\\n    Wooldridge ???\\n    Pagan and Vella 1989\\n    '\n    if cov_type != 'OPG':\n        raise NotImplementedError\n    k_constraints = mom_test.shape[1]\n    if mom_incl is None:\n        if var_mom_all is None:\n            var_cm = mom_test.T.dot(mom_test)\n        else:\n            var_cm = var_mom_all\n    else:\n        if var_mom_all is None:\n            mom_all = np.column_stack((mom_test, mom_incl))\n            var_mom_all = mom_all.T.dot(mom_all)\n        tmp = mom_test_deriv.dot(np.linalg.pinv(mom_incl_deriv))\n        h = np.column_stack((np.eye(k_constraints), -tmp))\n        var_cm = h.dot(var_mom_all.dot(h.T))\n    var_cm_inv = np.linalg.pinv(var_cm)\n    mom_test_sum = mom_test.sum(0)\n    statistic = mom_test_sum.dot(var_cm_inv.dot(mom_test_sum))\n    pval = stats.chi2.sf(statistic, k_constraints)\n    se = np.sqrt(np.diag(var_cm))\n    tvalues = mom_test_sum / se\n    pvalues = stats.norm.sf(np.abs(tvalues))\n    res = ResultsGeneric(var_cm=var_cm, stat_cmt=statistic, pval_cmt=pval, tvalues=tvalues, pvalues=pvalues)\n    return res",
            "def conditional_moment_test_generic(mom_test, mom_test_deriv, mom_incl, mom_incl_deriv, var_mom_all=None, cov_type='OPG', cov_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'generic conditional moment test\\n\\n    This is mainly intended as internal function in support of diagnostic\\n    and specification tests. It has no conversion and checking of correct\\n    arguments.\\n\\n    Parameters\\n    ----------\\n    mom_test : ndarray, 2-D (nobs, k_constraints)\\n        moment conditions that will be tested to be zero\\n    mom_test_deriv : ndarray, 2-D, square (k_constraints, k_constraints)\\n        derivative of moment conditions under test with respect to the\\n        parameters of the model summed over observations.\\n    mom_incl : ndarray, 2-D (nobs, k_params)\\n        moment conditions that where use in estimation, assumed to be zero\\n        This is score_obs in the case of (Q)MLE\\n    mom_incl_deriv : ndarray, 2-D, square (k_params, k_params)\\n        derivative of moment conditions of estimator summed over observations\\n        This is the information matrix or Hessian in the case of (Q)MLE.\\n    var_mom_all : None, or ndarray, 2-D, (k, k) with k = k_constraints + k_params\\n        Expected product or variance of the joint (column_stacked) moment\\n        conditions. The stacking should have the variance of the moment\\n        conditions under test in the first k_constraint rows and columns.\\n        If it is not None, then it will be estimated based on cov_type.\\n        I think: This is the Hessian of the extended or alternative model\\n        under full MLE and score test assuming information matrix identity\\n        holds.\\n\\n    Returns\\n    -------\\n    results\\n\\n    Notes\\n    -----\\n    TODO: cov_type other than OPG is missing\\n    initial implementation based on Cameron Trived countbook 1998 p.48, p.56\\n\\n    also included: mom_incl can be None if expected mom_test_deriv is zero.\\n\\n    References\\n    ----------\\n    Cameron and Trivedi 1998 count book\\n    Wooldridge ???\\n    Pagan and Vella 1989\\n    '\n    if cov_type != 'OPG':\n        raise NotImplementedError\n    k_constraints = mom_test.shape[1]\n    if mom_incl is None:\n        if var_mom_all is None:\n            var_cm = mom_test.T.dot(mom_test)\n        else:\n            var_cm = var_mom_all\n    else:\n        if var_mom_all is None:\n            mom_all = np.column_stack((mom_test, mom_incl))\n            var_mom_all = mom_all.T.dot(mom_all)\n        tmp = mom_test_deriv.dot(np.linalg.pinv(mom_incl_deriv))\n        h = np.column_stack((np.eye(k_constraints), -tmp))\n        var_cm = h.dot(var_mom_all.dot(h.T))\n    var_cm_inv = np.linalg.pinv(var_cm)\n    mom_test_sum = mom_test.sum(0)\n    statistic = mom_test_sum.dot(var_cm_inv.dot(mom_test_sum))\n    pval = stats.chi2.sf(statistic, k_constraints)\n    se = np.sqrt(np.diag(var_cm))\n    tvalues = mom_test_sum / se\n    pvalues = stats.norm.sf(np.abs(tvalues))\n    res = ResultsGeneric(var_cm=var_cm, stat_cmt=statistic, pval_cmt=pval, tvalues=tvalues, pvalues=pvalues)\n    return res"
        ]
    },
    {
        "func_name": "conditional_moment_test_regression",
        "original": "def conditional_moment_test_regression(mom_test, mom_test_deriv=None, mom_incl=None, mom_incl_deriv=None, var_mom_all=None, demean=False, cov_type='OPG', cov_kwds=None):\n    \"\"\"generic conditional moment test based artificial regression\n\n    this is very experimental, no options implemented yet\n\n    so far\n    OPG regression, or\n    artificial regression with Robust Wald test\n\n    The latter is (as far as I can see) the same as an overidentifying test\n    in GMM where the test statistic is the value of the GMM objective function\n    and it is assumed that parameters were estimated with optimial GMM, i.e.\n    the weight matrix equal to the expectation of the score variance.\n    \"\"\"\n    (nobs, k_constraints) = mom_test.shape\n    endog = np.ones(nobs)\n    if mom_incl is not None:\n        ex = np.column_stack((mom_test, mom_incl))\n    else:\n        ex = mom_test\n    if demean:\n        ex -= ex.mean(0)\n    if cov_type == 'OPG':\n        res = OLS(endog, ex).fit()\n        statistic = nobs * res.rsquared\n        pval = stats.chi2.sf(statistic, k_constraints)\n    else:\n        res = OLS(endog, ex).fit(cov_type=cov_type, cov_kwds=cov_kwds)\n        tres = res.wald_test(np.eye(ex.shape[1]))\n        statistic = tres.statistic\n        pval = tres.pvalue\n    return (statistic, pval)",
        "mutated": [
            "def conditional_moment_test_regression(mom_test, mom_test_deriv=None, mom_incl=None, mom_incl_deriv=None, var_mom_all=None, demean=False, cov_type='OPG', cov_kwds=None):\n    if False:\n        i = 10\n    'generic conditional moment test based artificial regression\\n\\n    this is very experimental, no options implemented yet\\n\\n    so far\\n    OPG regression, or\\n    artificial regression with Robust Wald test\\n\\n    The latter is (as far as I can see) the same as an overidentifying test\\n    in GMM where the test statistic is the value of the GMM objective function\\n    and it is assumed that parameters were estimated with optimial GMM, i.e.\\n    the weight matrix equal to the expectation of the score variance.\\n    '\n    (nobs, k_constraints) = mom_test.shape\n    endog = np.ones(nobs)\n    if mom_incl is not None:\n        ex = np.column_stack((mom_test, mom_incl))\n    else:\n        ex = mom_test\n    if demean:\n        ex -= ex.mean(0)\n    if cov_type == 'OPG':\n        res = OLS(endog, ex).fit()\n        statistic = nobs * res.rsquared\n        pval = stats.chi2.sf(statistic, k_constraints)\n    else:\n        res = OLS(endog, ex).fit(cov_type=cov_type, cov_kwds=cov_kwds)\n        tres = res.wald_test(np.eye(ex.shape[1]))\n        statistic = tres.statistic\n        pval = tres.pvalue\n    return (statistic, pval)",
            "def conditional_moment_test_regression(mom_test, mom_test_deriv=None, mom_incl=None, mom_incl_deriv=None, var_mom_all=None, demean=False, cov_type='OPG', cov_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'generic conditional moment test based artificial regression\\n\\n    this is very experimental, no options implemented yet\\n\\n    so far\\n    OPG regression, or\\n    artificial regression with Robust Wald test\\n\\n    The latter is (as far as I can see) the same as an overidentifying test\\n    in GMM where the test statistic is the value of the GMM objective function\\n    and it is assumed that parameters were estimated with optimial GMM, i.e.\\n    the weight matrix equal to the expectation of the score variance.\\n    '\n    (nobs, k_constraints) = mom_test.shape\n    endog = np.ones(nobs)\n    if mom_incl is not None:\n        ex = np.column_stack((mom_test, mom_incl))\n    else:\n        ex = mom_test\n    if demean:\n        ex -= ex.mean(0)\n    if cov_type == 'OPG':\n        res = OLS(endog, ex).fit()\n        statistic = nobs * res.rsquared\n        pval = stats.chi2.sf(statistic, k_constraints)\n    else:\n        res = OLS(endog, ex).fit(cov_type=cov_type, cov_kwds=cov_kwds)\n        tres = res.wald_test(np.eye(ex.shape[1]))\n        statistic = tres.statistic\n        pval = tres.pvalue\n    return (statistic, pval)",
            "def conditional_moment_test_regression(mom_test, mom_test_deriv=None, mom_incl=None, mom_incl_deriv=None, var_mom_all=None, demean=False, cov_type='OPG', cov_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'generic conditional moment test based artificial regression\\n\\n    this is very experimental, no options implemented yet\\n\\n    so far\\n    OPG regression, or\\n    artificial regression with Robust Wald test\\n\\n    The latter is (as far as I can see) the same as an overidentifying test\\n    in GMM where the test statistic is the value of the GMM objective function\\n    and it is assumed that parameters were estimated with optimial GMM, i.e.\\n    the weight matrix equal to the expectation of the score variance.\\n    '\n    (nobs, k_constraints) = mom_test.shape\n    endog = np.ones(nobs)\n    if mom_incl is not None:\n        ex = np.column_stack((mom_test, mom_incl))\n    else:\n        ex = mom_test\n    if demean:\n        ex -= ex.mean(0)\n    if cov_type == 'OPG':\n        res = OLS(endog, ex).fit()\n        statistic = nobs * res.rsquared\n        pval = stats.chi2.sf(statistic, k_constraints)\n    else:\n        res = OLS(endog, ex).fit(cov_type=cov_type, cov_kwds=cov_kwds)\n        tres = res.wald_test(np.eye(ex.shape[1]))\n        statistic = tres.statistic\n        pval = tres.pvalue\n    return (statistic, pval)",
            "def conditional_moment_test_regression(mom_test, mom_test_deriv=None, mom_incl=None, mom_incl_deriv=None, var_mom_all=None, demean=False, cov_type='OPG', cov_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'generic conditional moment test based artificial regression\\n\\n    this is very experimental, no options implemented yet\\n\\n    so far\\n    OPG regression, or\\n    artificial regression with Robust Wald test\\n\\n    The latter is (as far as I can see) the same as an overidentifying test\\n    in GMM where the test statistic is the value of the GMM objective function\\n    and it is assumed that parameters were estimated with optimial GMM, i.e.\\n    the weight matrix equal to the expectation of the score variance.\\n    '\n    (nobs, k_constraints) = mom_test.shape\n    endog = np.ones(nobs)\n    if mom_incl is not None:\n        ex = np.column_stack((mom_test, mom_incl))\n    else:\n        ex = mom_test\n    if demean:\n        ex -= ex.mean(0)\n    if cov_type == 'OPG':\n        res = OLS(endog, ex).fit()\n        statistic = nobs * res.rsquared\n        pval = stats.chi2.sf(statistic, k_constraints)\n    else:\n        res = OLS(endog, ex).fit(cov_type=cov_type, cov_kwds=cov_kwds)\n        tres = res.wald_test(np.eye(ex.shape[1]))\n        statistic = tres.statistic\n        pval = tres.pvalue\n    return (statistic, pval)",
            "def conditional_moment_test_regression(mom_test, mom_test_deriv=None, mom_incl=None, mom_incl_deriv=None, var_mom_all=None, demean=False, cov_type='OPG', cov_kwds=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'generic conditional moment test based artificial regression\\n\\n    this is very experimental, no options implemented yet\\n\\n    so far\\n    OPG regression, or\\n    artificial regression with Robust Wald test\\n\\n    The latter is (as far as I can see) the same as an overidentifying test\\n    in GMM where the test statistic is the value of the GMM objective function\\n    and it is assumed that parameters were estimated with optimial GMM, i.e.\\n    the weight matrix equal to the expectation of the score variance.\\n    '\n    (nobs, k_constraints) = mom_test.shape\n    endog = np.ones(nobs)\n    if mom_incl is not None:\n        ex = np.column_stack((mom_test, mom_incl))\n    else:\n        ex = mom_test\n    if demean:\n        ex -= ex.mean(0)\n    if cov_type == 'OPG':\n        res = OLS(endog, ex).fit()\n        statistic = nobs * res.rsquared\n        pval = stats.chi2.sf(statistic, k_constraints)\n    else:\n        res = OLS(endog, ex).fit(cov_type=cov_type, cov_kwds=cov_kwds)\n        tres = res.wald_test(np.eye(ex.shape[1]))\n        statistic = tres.statistic\n        pval = tres.pvalue\n    return (statistic, pval)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, moments, cov_moments, moments_deriv, weights, transf_mt):\n    self.moments = moments\n    self.cov_moments = cov_moments\n    self.moments_deriv = moments_deriv\n    self.weights = weights\n    self.transf_mt = transf_mt\n    self.moments_constraint = transf_mt.dot(moments)\n    self.htw = moments_deriv.T.dot(weights)\n    self.k_moments = self.moments.shape[-1]\n    self.k_constraints = self.transf_mt.shape[0]",
        "mutated": [
            "def __init__(self, moments, cov_moments, moments_deriv, weights, transf_mt):\n    if False:\n        i = 10\n    self.moments = moments\n    self.cov_moments = cov_moments\n    self.moments_deriv = moments_deriv\n    self.weights = weights\n    self.transf_mt = transf_mt\n    self.moments_constraint = transf_mt.dot(moments)\n    self.htw = moments_deriv.T.dot(weights)\n    self.k_moments = self.moments.shape[-1]\n    self.k_constraints = self.transf_mt.shape[0]",
            "def __init__(self, moments, cov_moments, moments_deriv, weights, transf_mt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.moments = moments\n    self.cov_moments = cov_moments\n    self.moments_deriv = moments_deriv\n    self.weights = weights\n    self.transf_mt = transf_mt\n    self.moments_constraint = transf_mt.dot(moments)\n    self.htw = moments_deriv.T.dot(weights)\n    self.k_moments = self.moments.shape[-1]\n    self.k_constraints = self.transf_mt.shape[0]",
            "def __init__(self, moments, cov_moments, moments_deriv, weights, transf_mt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.moments = moments\n    self.cov_moments = cov_moments\n    self.moments_deriv = moments_deriv\n    self.weights = weights\n    self.transf_mt = transf_mt\n    self.moments_constraint = transf_mt.dot(moments)\n    self.htw = moments_deriv.T.dot(weights)\n    self.k_moments = self.moments.shape[-1]\n    self.k_constraints = self.transf_mt.shape[0]",
            "def __init__(self, moments, cov_moments, moments_deriv, weights, transf_mt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.moments = moments\n    self.cov_moments = cov_moments\n    self.moments_deriv = moments_deriv\n    self.weights = weights\n    self.transf_mt = transf_mt\n    self.moments_constraint = transf_mt.dot(moments)\n    self.htw = moments_deriv.T.dot(weights)\n    self.k_moments = self.moments.shape[-1]\n    self.k_constraints = self.transf_mt.shape[0]",
            "def __init__(self, moments, cov_moments, moments_deriv, weights, transf_mt):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.moments = moments\n    self.cov_moments = cov_moments\n    self.moments_deriv = moments_deriv\n    self.weights = weights\n    self.transf_mt = transf_mt\n    self.moments_constraint = transf_mt.dot(moments)\n    self.htw = moments_deriv.T.dot(weights)\n    self.k_moments = self.moments.shape[-1]\n    self.k_constraints = self.transf_mt.shape[0]"
        ]
    },
    {
        "func_name": "asy_transf_params",
        "original": "@cache_readonly\ndef asy_transf_params(self):\n    moments_deriv = self.moments_deriv\n    htw = self.htw\n    res = np.linalg.solve(htw.dot(moments_deriv), htw)\n    return -res",
        "mutated": [
            "@cache_readonly\ndef asy_transf_params(self):\n    if False:\n        i = 10\n    moments_deriv = self.moments_deriv\n    htw = self.htw\n    res = np.linalg.solve(htw.dot(moments_deriv), htw)\n    return -res",
            "@cache_readonly\ndef asy_transf_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    moments_deriv = self.moments_deriv\n    htw = self.htw\n    res = np.linalg.solve(htw.dot(moments_deriv), htw)\n    return -res",
            "@cache_readonly\ndef asy_transf_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    moments_deriv = self.moments_deriv\n    htw = self.htw\n    res = np.linalg.solve(htw.dot(moments_deriv), htw)\n    return -res",
            "@cache_readonly\ndef asy_transf_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    moments_deriv = self.moments_deriv\n    htw = self.htw\n    res = np.linalg.solve(htw.dot(moments_deriv), htw)\n    return -res",
            "@cache_readonly\ndef asy_transf_params(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    moments_deriv = self.moments_deriv\n    htw = self.htw\n    res = np.linalg.solve(htw.dot(moments_deriv), htw)\n    return -res"
        ]
    },
    {
        "func_name": "project_w",
        "original": "@cache_readonly\ndef project_w(self):\n    moments_deriv = self.moments_deriv\n    res = moments_deriv.dot(self.asy_transf_params)\n    res += np.eye(res.shape[0])\n    return res",
        "mutated": [
            "@cache_readonly\ndef project_w(self):\n    if False:\n        i = 10\n    moments_deriv = self.moments_deriv\n    res = moments_deriv.dot(self.asy_transf_params)\n    res += np.eye(res.shape[0])\n    return res",
            "@cache_readonly\ndef project_w(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    moments_deriv = self.moments_deriv\n    res = moments_deriv.dot(self.asy_transf_params)\n    res += np.eye(res.shape[0])\n    return res",
            "@cache_readonly\ndef project_w(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    moments_deriv = self.moments_deriv\n    res = moments_deriv.dot(self.asy_transf_params)\n    res += np.eye(res.shape[0])\n    return res",
            "@cache_readonly\ndef project_w(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    moments_deriv = self.moments_deriv\n    res = moments_deriv.dot(self.asy_transf_params)\n    res += np.eye(res.shape[0])\n    return res",
            "@cache_readonly\ndef project_w(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    moments_deriv = self.moments_deriv\n    res = moments_deriv.dot(self.asy_transf_params)\n    res += np.eye(res.shape[0])\n    return res"
        ]
    },
    {
        "func_name": "asy_transform_mom_constraints",
        "original": "@cache_readonly\ndef asy_transform_mom_constraints(self):\n    res = self.transf_mt.dot(self.project_w)\n    return res",
        "mutated": [
            "@cache_readonly\ndef asy_transform_mom_constraints(self):\n    if False:\n        i = 10\n    res = self.transf_mt.dot(self.project_w)\n    return res",
            "@cache_readonly\ndef asy_transform_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    res = self.transf_mt.dot(self.project_w)\n    return res",
            "@cache_readonly\ndef asy_transform_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    res = self.transf_mt.dot(self.project_w)\n    return res",
            "@cache_readonly\ndef asy_transform_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    res = self.transf_mt.dot(self.project_w)\n    return res",
            "@cache_readonly\ndef asy_transform_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    res = self.transf_mt.dot(self.project_w)\n    return res"
        ]
    },
    {
        "func_name": "asy_cov_moments",
        "original": "@cache_readonly\ndef asy_cov_moments(self):\n    \"\"\"\n\n        `sqrt(T) * g_T(b_0) asy N(K delta, V)`\n\n        mean is not implemented,\n        V is the same as cov_moments in __init__ argument\n        \"\"\"\n    return self.cov_moments",
        "mutated": [
            "@cache_readonly\ndef asy_cov_moments(self):\n    if False:\n        i = 10\n    '\\n\\n        `sqrt(T) * g_T(b_0) asy N(K delta, V)`\\n\\n        mean is not implemented,\\n        V is the same as cov_moments in __init__ argument\\n        '\n    return self.cov_moments",
            "@cache_readonly\ndef asy_cov_moments(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n\\n        `sqrt(T) * g_T(b_0) asy N(K delta, V)`\\n\\n        mean is not implemented,\\n        V is the same as cov_moments in __init__ argument\\n        '\n    return self.cov_moments",
            "@cache_readonly\ndef asy_cov_moments(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n\\n        `sqrt(T) * g_T(b_0) asy N(K delta, V)`\\n\\n        mean is not implemented,\\n        V is the same as cov_moments in __init__ argument\\n        '\n    return self.cov_moments",
            "@cache_readonly\ndef asy_cov_moments(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n\\n        `sqrt(T) * g_T(b_0) asy N(K delta, V)`\\n\\n        mean is not implemented,\\n        V is the same as cov_moments in __init__ argument\\n        '\n    return self.cov_moments",
            "@cache_readonly\ndef asy_cov_moments(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n\\n        `sqrt(T) * g_T(b_0) asy N(K delta, V)`\\n\\n        mean is not implemented,\\n        V is the same as cov_moments in __init__ argument\\n        '\n    return self.cov_moments"
        ]
    },
    {
        "func_name": "cov_mom_constraints",
        "original": "@cache_readonly\ndef cov_mom_constraints(self):\n    transf = self.asy_transform_mom_constraints\n    return transf.dot(self.asy_cov_moments).dot(transf.T)",
        "mutated": [
            "@cache_readonly\ndef cov_mom_constraints(self):\n    if False:\n        i = 10\n    transf = self.asy_transform_mom_constraints\n    return transf.dot(self.asy_cov_moments).dot(transf.T)",
            "@cache_readonly\ndef cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    transf = self.asy_transform_mom_constraints\n    return transf.dot(self.asy_cov_moments).dot(transf.T)",
            "@cache_readonly\ndef cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    transf = self.asy_transform_mom_constraints\n    return transf.dot(self.asy_cov_moments).dot(transf.T)",
            "@cache_readonly\ndef cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    transf = self.asy_transform_mom_constraints\n    return transf.dot(self.asy_cov_moments).dot(transf.T)",
            "@cache_readonly\ndef cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    transf = self.asy_transform_mom_constraints\n    return transf.dot(self.asy_cov_moments).dot(transf.T)"
        ]
    },
    {
        "func_name": "rank_cov_mom_constraints",
        "original": "@cache_readonly\ndef rank_cov_mom_constraints(self):\n    return np.linalg.matrix_rank(self.cov_mom_constraints)",
        "mutated": [
            "@cache_readonly\ndef rank_cov_mom_constraints(self):\n    if False:\n        i = 10\n    return np.linalg.matrix_rank(self.cov_mom_constraints)",
            "@cache_readonly\ndef rank_cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return np.linalg.matrix_rank(self.cov_mom_constraints)",
            "@cache_readonly\ndef rank_cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return np.linalg.matrix_rank(self.cov_mom_constraints)",
            "@cache_readonly\ndef rank_cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return np.linalg.matrix_rank(self.cov_mom_constraints)",
            "@cache_readonly\ndef rank_cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return np.linalg.matrix_rank(self.cov_mom_constraints)"
        ]
    },
    {
        "func_name": "ztest",
        "original": "def ztest(self):\n    \"\"\"statistic, p-value and degrees of freedom of separate moment test\n\n        currently two sided test only\n\n        TODO: This can use generic ztest/ttest features and return\n        ContrastResults\n        \"\"\"\n    diff = self.moments_constraint\n    bse = np.sqrt(np.diag(self.cov_mom_constraints))\n    stat = diff / bse\n    pval = stats.norm.sf(np.abs(stat)) * 2\n    return (stat, pval)",
        "mutated": [
            "def ztest(self):\n    if False:\n        i = 10\n    'statistic, p-value and degrees of freedom of separate moment test\\n\\n        currently two sided test only\\n\\n        TODO: This can use generic ztest/ttest features and return\\n        ContrastResults\\n        '\n    diff = self.moments_constraint\n    bse = np.sqrt(np.diag(self.cov_mom_constraints))\n    stat = diff / bse\n    pval = stats.norm.sf(np.abs(stat)) * 2\n    return (stat, pval)",
            "def ztest(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'statistic, p-value and degrees of freedom of separate moment test\\n\\n        currently two sided test only\\n\\n        TODO: This can use generic ztest/ttest features and return\\n        ContrastResults\\n        '\n    diff = self.moments_constraint\n    bse = np.sqrt(np.diag(self.cov_mom_constraints))\n    stat = diff / bse\n    pval = stats.norm.sf(np.abs(stat)) * 2\n    return (stat, pval)",
            "def ztest(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'statistic, p-value and degrees of freedom of separate moment test\\n\\n        currently two sided test only\\n\\n        TODO: This can use generic ztest/ttest features and return\\n        ContrastResults\\n        '\n    diff = self.moments_constraint\n    bse = np.sqrt(np.diag(self.cov_mom_constraints))\n    stat = diff / bse\n    pval = stats.norm.sf(np.abs(stat)) * 2\n    return (stat, pval)",
            "def ztest(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'statistic, p-value and degrees of freedom of separate moment test\\n\\n        currently two sided test only\\n\\n        TODO: This can use generic ztest/ttest features and return\\n        ContrastResults\\n        '\n    diff = self.moments_constraint\n    bse = np.sqrt(np.diag(self.cov_mom_constraints))\n    stat = diff / bse\n    pval = stats.norm.sf(np.abs(stat)) * 2\n    return (stat, pval)",
            "def ztest(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'statistic, p-value and degrees of freedom of separate moment test\\n\\n        currently two sided test only\\n\\n        TODO: This can use generic ztest/ttest features and return\\n        ContrastResults\\n        '\n    diff = self.moments_constraint\n    bse = np.sqrt(np.diag(self.cov_mom_constraints))\n    stat = diff / bse\n    pval = stats.norm.sf(np.abs(stat)) * 2\n    return (stat, pval)"
        ]
    },
    {
        "func_name": "chisquare",
        "original": "@cache_readonly\ndef chisquare(self):\n    \"\"\"statistic, p-value and degrees of freedom of joint moment test\n        \"\"\"\n    diff = self.moments_constraint\n    cov = self.cov_mom_constraints\n    stat = diff.T.dot(np.linalg.pinv(cov).dot(diff))\n    df = self.rank_cov_mom_constraints\n    pval = stats.chi2.sf(stat, df)\n    return (stat, pval, df)",
        "mutated": [
            "@cache_readonly\ndef chisquare(self):\n    if False:\n        i = 10\n    'statistic, p-value and degrees of freedom of joint moment test\\n        '\n    diff = self.moments_constraint\n    cov = self.cov_mom_constraints\n    stat = diff.T.dot(np.linalg.pinv(cov).dot(diff))\n    df = self.rank_cov_mom_constraints\n    pval = stats.chi2.sf(stat, df)\n    return (stat, pval, df)",
            "@cache_readonly\ndef chisquare(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'statistic, p-value and degrees of freedom of joint moment test\\n        '\n    diff = self.moments_constraint\n    cov = self.cov_mom_constraints\n    stat = diff.T.dot(np.linalg.pinv(cov).dot(diff))\n    df = self.rank_cov_mom_constraints\n    pval = stats.chi2.sf(stat, df)\n    return (stat, pval, df)",
            "@cache_readonly\ndef chisquare(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'statistic, p-value and degrees of freedom of joint moment test\\n        '\n    diff = self.moments_constraint\n    cov = self.cov_mom_constraints\n    stat = diff.T.dot(np.linalg.pinv(cov).dot(diff))\n    df = self.rank_cov_mom_constraints\n    pval = stats.chi2.sf(stat, df)\n    return (stat, pval, df)",
            "@cache_readonly\ndef chisquare(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'statistic, p-value and degrees of freedom of joint moment test\\n        '\n    diff = self.moments_constraint\n    cov = self.cov_mom_constraints\n    stat = diff.T.dot(np.linalg.pinv(cov).dot(diff))\n    df = self.rank_cov_mom_constraints\n    pval = stats.chi2.sf(stat, df)\n    return (stat, pval, df)",
            "@cache_readonly\ndef chisquare(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'statistic, p-value and degrees of freedom of joint moment test\\n        '\n    diff = self.moments_constraint\n    cov = self.cov_mom_constraints\n    stat = diff.T.dot(np.linalg.pinv(cov).dot(diff))\n    df = self.rank_cov_mom_constraints\n    pval = stats.chi2.sf(stat, df)\n    return (stat, pval, df)"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, score, score_deriv, moments, moments_deriv, cov_moments):\n    self.score = score\n    self.score_deriv = score_deriv\n    self.moments = moments\n    self.moments_deriv = moments_deriv\n    self.cov_moments_all = cov_moments\n    self.k_moments_test = moments.shape[-1]\n    self.k_params = score.shape[-1]\n    self.k_moments_all = self.k_params + self.k_moments_test",
        "mutated": [
            "def __init__(self, score, score_deriv, moments, moments_deriv, cov_moments):\n    if False:\n        i = 10\n    self.score = score\n    self.score_deriv = score_deriv\n    self.moments = moments\n    self.moments_deriv = moments_deriv\n    self.cov_moments_all = cov_moments\n    self.k_moments_test = moments.shape[-1]\n    self.k_params = score.shape[-1]\n    self.k_moments_all = self.k_params + self.k_moments_test",
            "def __init__(self, score, score_deriv, moments, moments_deriv, cov_moments):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.score = score\n    self.score_deriv = score_deriv\n    self.moments = moments\n    self.moments_deriv = moments_deriv\n    self.cov_moments_all = cov_moments\n    self.k_moments_test = moments.shape[-1]\n    self.k_params = score.shape[-1]\n    self.k_moments_all = self.k_params + self.k_moments_test",
            "def __init__(self, score, score_deriv, moments, moments_deriv, cov_moments):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.score = score\n    self.score_deriv = score_deriv\n    self.moments = moments\n    self.moments_deriv = moments_deriv\n    self.cov_moments_all = cov_moments\n    self.k_moments_test = moments.shape[-1]\n    self.k_params = score.shape[-1]\n    self.k_moments_all = self.k_params + self.k_moments_test",
            "def __init__(self, score, score_deriv, moments, moments_deriv, cov_moments):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.score = score\n    self.score_deriv = score_deriv\n    self.moments = moments\n    self.moments_deriv = moments_deriv\n    self.cov_moments_all = cov_moments\n    self.k_moments_test = moments.shape[-1]\n    self.k_params = score.shape[-1]\n    self.k_moments_all = self.k_params + self.k_moments_test",
            "def __init__(self, score, score_deriv, moments, moments_deriv, cov_moments):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.score = score\n    self.score_deriv = score_deriv\n    self.moments = moments\n    self.moments_deriv = moments_deriv\n    self.cov_moments_all = cov_moments\n    self.k_moments_test = moments.shape[-1]\n    self.k_params = score.shape[-1]\n    self.k_moments_all = self.k_params + self.k_moments_test"
        ]
    },
    {
        "func_name": "cov_params_all",
        "original": "@cache_readonly\ndef cov_params_all(self):\n    m_deriv = np.zeros((self.k_moments_all, self.k_moments_all))\n    m_deriv[:self.k_params, :self.k_params] = self.score_deriv\n    m_deriv[self.k_params:, :self.k_params] = self.moments_deriv\n    m_deriv[self.k_params:, self.k_params:] = np.eye(self.k_moments_test)\n    m_deriv_inv = np.linalg.inv(m_deriv)\n    cov = m_deriv_inv.dot(self.cov_moments_all.dot(m_deriv_inv.T))\n    return cov",
        "mutated": [
            "@cache_readonly\ndef cov_params_all(self):\n    if False:\n        i = 10\n    m_deriv = np.zeros((self.k_moments_all, self.k_moments_all))\n    m_deriv[:self.k_params, :self.k_params] = self.score_deriv\n    m_deriv[self.k_params:, :self.k_params] = self.moments_deriv\n    m_deriv[self.k_params:, self.k_params:] = np.eye(self.k_moments_test)\n    m_deriv_inv = np.linalg.inv(m_deriv)\n    cov = m_deriv_inv.dot(self.cov_moments_all.dot(m_deriv_inv.T))\n    return cov",
            "@cache_readonly\ndef cov_params_all(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    m_deriv = np.zeros((self.k_moments_all, self.k_moments_all))\n    m_deriv[:self.k_params, :self.k_params] = self.score_deriv\n    m_deriv[self.k_params:, :self.k_params] = self.moments_deriv\n    m_deriv[self.k_params:, self.k_params:] = np.eye(self.k_moments_test)\n    m_deriv_inv = np.linalg.inv(m_deriv)\n    cov = m_deriv_inv.dot(self.cov_moments_all.dot(m_deriv_inv.T))\n    return cov",
            "@cache_readonly\ndef cov_params_all(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    m_deriv = np.zeros((self.k_moments_all, self.k_moments_all))\n    m_deriv[:self.k_params, :self.k_params] = self.score_deriv\n    m_deriv[self.k_params:, :self.k_params] = self.moments_deriv\n    m_deriv[self.k_params:, self.k_params:] = np.eye(self.k_moments_test)\n    m_deriv_inv = np.linalg.inv(m_deriv)\n    cov = m_deriv_inv.dot(self.cov_moments_all.dot(m_deriv_inv.T))\n    return cov",
            "@cache_readonly\ndef cov_params_all(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    m_deriv = np.zeros((self.k_moments_all, self.k_moments_all))\n    m_deriv[:self.k_params, :self.k_params] = self.score_deriv\n    m_deriv[self.k_params:, :self.k_params] = self.moments_deriv\n    m_deriv[self.k_params:, self.k_params:] = np.eye(self.k_moments_test)\n    m_deriv_inv = np.linalg.inv(m_deriv)\n    cov = m_deriv_inv.dot(self.cov_moments_all.dot(m_deriv_inv.T))\n    return cov",
            "@cache_readonly\ndef cov_params_all(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    m_deriv = np.zeros((self.k_moments_all, self.k_moments_all))\n    m_deriv[:self.k_params, :self.k_params] = self.score_deriv\n    m_deriv[self.k_params:, :self.k_params] = self.moments_deriv\n    m_deriv[self.k_params:, self.k_params:] = np.eye(self.k_moments_test)\n    m_deriv_inv = np.linalg.inv(m_deriv)\n    cov = m_deriv_inv.dot(self.cov_moments_all.dot(m_deriv_inv.T))\n    return cov"
        ]
    },
    {
        "func_name": "cov_mom_constraints",
        "original": "@cache_readonly\ndef cov_mom_constraints(self):\n    return self.cov_params_all[self.k_params:, self.k_params:]",
        "mutated": [
            "@cache_readonly\ndef cov_mom_constraints(self):\n    if False:\n        i = 10\n    return self.cov_params_all[self.k_params:, self.k_params:]",
            "@cache_readonly\ndef cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return self.cov_params_all[self.k_params:, self.k_params:]",
            "@cache_readonly\ndef cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return self.cov_params_all[self.k_params:, self.k_params:]",
            "@cache_readonly\ndef cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return self.cov_params_all[self.k_params:, self.k_params:]",
            "@cache_readonly\ndef cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return self.cov_params_all[self.k_params:, self.k_params:]"
        ]
    },
    {
        "func_name": "rank_cov_mom_constraints",
        "original": "@cache_readonly\ndef rank_cov_mom_constraints(self):\n    return np.linalg.matrix_rank(self.cov_mom_constraints)",
        "mutated": [
            "@cache_readonly\ndef rank_cov_mom_constraints(self):\n    if False:\n        i = 10\n    return np.linalg.matrix_rank(self.cov_mom_constraints)",
            "@cache_readonly\ndef rank_cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return np.linalg.matrix_rank(self.cov_mom_constraints)",
            "@cache_readonly\ndef rank_cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return np.linalg.matrix_rank(self.cov_mom_constraints)",
            "@cache_readonly\ndef rank_cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return np.linalg.matrix_rank(self.cov_mom_constraints)",
            "@cache_readonly\ndef rank_cov_mom_constraints(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return np.linalg.matrix_rank(self.cov_mom_constraints)"
        ]
    },
    {
        "func_name": "ztest",
        "original": "def ztest(self):\n    \"\"\"statistic, p-value and degrees of freedom of separate moment test\n\n        currently two sided test only\n\n        TODO: This can use generic ztest/ttest features and return\n        ContrastResults\n        \"\"\"\n    diff = self.moments_constraint\n    bse = np.sqrt(np.diag(self.cov_mom_constraints))\n    stat = diff / bse\n    pval = stats.norm.sf(np.abs(stat)) * 2\n    return (stat, pval)",
        "mutated": [
            "def ztest(self):\n    if False:\n        i = 10\n    'statistic, p-value and degrees of freedom of separate moment test\\n\\n        currently two sided test only\\n\\n        TODO: This can use generic ztest/ttest features and return\\n        ContrastResults\\n        '\n    diff = self.moments_constraint\n    bse = np.sqrt(np.diag(self.cov_mom_constraints))\n    stat = diff / bse\n    pval = stats.norm.sf(np.abs(stat)) * 2\n    return (stat, pval)",
            "def ztest(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'statistic, p-value and degrees of freedom of separate moment test\\n\\n        currently two sided test only\\n\\n        TODO: This can use generic ztest/ttest features and return\\n        ContrastResults\\n        '\n    diff = self.moments_constraint\n    bse = np.sqrt(np.diag(self.cov_mom_constraints))\n    stat = diff / bse\n    pval = stats.norm.sf(np.abs(stat)) * 2\n    return (stat, pval)",
            "def ztest(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'statistic, p-value and degrees of freedom of separate moment test\\n\\n        currently two sided test only\\n\\n        TODO: This can use generic ztest/ttest features and return\\n        ContrastResults\\n        '\n    diff = self.moments_constraint\n    bse = np.sqrt(np.diag(self.cov_mom_constraints))\n    stat = diff / bse\n    pval = stats.norm.sf(np.abs(stat)) * 2\n    return (stat, pval)",
            "def ztest(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'statistic, p-value and degrees of freedom of separate moment test\\n\\n        currently two sided test only\\n\\n        TODO: This can use generic ztest/ttest features and return\\n        ContrastResults\\n        '\n    diff = self.moments_constraint\n    bse = np.sqrt(np.diag(self.cov_mom_constraints))\n    stat = diff / bse\n    pval = stats.norm.sf(np.abs(stat)) * 2\n    return (stat, pval)",
            "def ztest(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'statistic, p-value and degrees of freedom of separate moment test\\n\\n        currently two sided test only\\n\\n        TODO: This can use generic ztest/ttest features and return\\n        ContrastResults\\n        '\n    diff = self.moments_constraint\n    bse = np.sqrt(np.diag(self.cov_mom_constraints))\n    stat = diff / bse\n    pval = stats.norm.sf(np.abs(stat)) * 2\n    return (stat, pval)"
        ]
    },
    {
        "func_name": "chisquare",
        "original": "@cache_readonly\ndef chisquare(self):\n    \"\"\"statistic, p-value and degrees of freedom of joint moment test\n        \"\"\"\n    diff = self.moments\n    cov = self.cov_mom_constraints\n    stat = diff.T.dot(np.linalg.pinv(cov).dot(diff))\n    df = self.rank_cov_mom_constraints\n    pval = stats.chi2.sf(stat, df)\n    return (stat, pval, df)",
        "mutated": [
            "@cache_readonly\ndef chisquare(self):\n    if False:\n        i = 10\n    'statistic, p-value and degrees of freedom of joint moment test\\n        '\n    diff = self.moments\n    cov = self.cov_mom_constraints\n    stat = diff.T.dot(np.linalg.pinv(cov).dot(diff))\n    df = self.rank_cov_mom_constraints\n    pval = stats.chi2.sf(stat, df)\n    return (stat, pval, df)",
            "@cache_readonly\ndef chisquare(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'statistic, p-value and degrees of freedom of joint moment test\\n        '\n    diff = self.moments\n    cov = self.cov_mom_constraints\n    stat = diff.T.dot(np.linalg.pinv(cov).dot(diff))\n    df = self.rank_cov_mom_constraints\n    pval = stats.chi2.sf(stat, df)\n    return (stat, pval, df)",
            "@cache_readonly\ndef chisquare(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'statistic, p-value and degrees of freedom of joint moment test\\n        '\n    diff = self.moments\n    cov = self.cov_mom_constraints\n    stat = diff.T.dot(np.linalg.pinv(cov).dot(diff))\n    df = self.rank_cov_mom_constraints\n    pval = stats.chi2.sf(stat, df)\n    return (stat, pval, df)",
            "@cache_readonly\ndef chisquare(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'statistic, p-value and degrees of freedom of joint moment test\\n        '\n    diff = self.moments\n    cov = self.cov_mom_constraints\n    stat = diff.T.dot(np.linalg.pinv(cov).dot(diff))\n    df = self.rank_cov_mom_constraints\n    pval = stats.chi2.sf(stat, df)\n    return (stat, pval, df)",
            "@cache_readonly\ndef chisquare(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'statistic, p-value and degrees of freedom of joint moment test\\n        '\n    diff = self.moments\n    cov = self.cov_mom_constraints\n    stat = diff.T.dot(np.linalg.pinv(cov).dot(diff))\n    df = self.rank_cov_mom_constraints\n    pval = stats.chi2.sf(stat, df)\n    return (stat, pval, df)"
        ]
    }
]