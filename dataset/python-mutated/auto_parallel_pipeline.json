[
    {
        "func_name": "is_reshard_op",
        "original": "def is_reshard_op(op):\n    return op.has_attr('op_namescope') and '/auto_parallel/reshard' in op.attr('op_namescope')",
        "mutated": [
            "def is_reshard_op(op):\n    if False:\n        i = 10\n    return op.has_attr('op_namescope') and '/auto_parallel/reshard' in op.attr('op_namescope')",
            "def is_reshard_op(op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return op.has_attr('op_namescope') and '/auto_parallel/reshard' in op.attr('op_namescope')",
            "def is_reshard_op(op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return op.has_attr('op_namescope') and '/auto_parallel/reshard' in op.attr('op_namescope')",
            "def is_reshard_op(op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return op.has_attr('op_namescope') and '/auto_parallel/reshard' in op.attr('op_namescope')",
            "def is_reshard_op(op):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return op.has_attr('op_namescope') and '/auto_parallel/reshard' in op.attr('op_namescope')"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self):\n    super().__init__()\n    self.set_attr('dist_context', None)",
        "mutated": [
            "def __init__(self):\n    if False:\n        i = 10\n    super().__init__()\n    self.set_attr('dist_context', None)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.set_attr('dist_context', None)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.set_attr('dist_context', None)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.set_attr('dist_context', None)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.set_attr('dist_context', None)"
        ]
    },
    {
        "func_name": "_check_self",
        "original": "def _check_self(self):\n    if self.get_attr('dist_context') is None:\n        return False\n    return True",
        "mutated": [
            "def _check_self(self):\n    if False:\n        i = 10\n    if self.get_attr('dist_context') is None:\n        return False\n    return True",
            "def _check_self(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if self.get_attr('dist_context') is None:\n        return False\n    return True",
            "def _check_self(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if self.get_attr('dist_context') is None:\n        return False\n    return True",
            "def _check_self(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if self.get_attr('dist_context') is None:\n        return False\n    return True",
            "def _check_self(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if self.get_attr('dist_context') is None:\n        return False\n    return True"
        ]
    },
    {
        "func_name": "_check_conflict",
        "original": "def _check_conflict(self, other_pass):\n    return True",
        "mutated": [
            "def _check_conflict(self, other_pass):\n    if False:\n        i = 10\n    return True",
            "def _check_conflict(self, other_pass):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return True",
            "def _check_conflict(self, other_pass):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return True",
            "def _check_conflict(self, other_pass):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return True",
            "def _check_conflict(self, other_pass):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return True"
        ]
    },
    {
        "func_name": "_apply_single_impl",
        "original": "def _apply_single_impl(self, main_program, startup_program, context):\n    self._dist_context = self.get_attr('dist_context')\n    self._acc_steps = self.get_attr('accumulate_steps')\n    self._mode = self.get_attr('schedule_mode')\n    self._gen_bsz = self.get_attr('generation_batch_size')\n    self._program = main_program\n    self._cur_rank = int(os.getenv('PADDLE_TRAINER_ID', 0))\n    trainer_endpoints = os.getenv('PADDLE_TRAINER_ENDPOINTS', '').split(',')\n    self._nrank = len(trainer_endpoints)\n    self._pp_stages = len(self._dist_context.process_meshes)\n    self._cur_pp_stage = self._get_pp_stage(self._cur_rank)\n    if self._mode == '1F1B':\n        _insert_sync_for_fthenb_1f1b(self._program)\n        self._task_1f1b()\n    elif self._mode == 'F-Then-B':\n        raise NotImplementedError('F-Then-B has not been implemented')\n    elif self._mode == 'stream':\n        self._insert_sync_ops_for_stream()\n        self._task_stream()\n    else:\n        raise ValueError(f\"Now only 'F-then-B', '1F1B' and 'stream' are supported.The given value is {self._mode}.\")",
        "mutated": [
            "def _apply_single_impl(self, main_program, startup_program, context):\n    if False:\n        i = 10\n    self._dist_context = self.get_attr('dist_context')\n    self._acc_steps = self.get_attr('accumulate_steps')\n    self._mode = self.get_attr('schedule_mode')\n    self._gen_bsz = self.get_attr('generation_batch_size')\n    self._program = main_program\n    self._cur_rank = int(os.getenv('PADDLE_TRAINER_ID', 0))\n    trainer_endpoints = os.getenv('PADDLE_TRAINER_ENDPOINTS', '').split(',')\n    self._nrank = len(trainer_endpoints)\n    self._pp_stages = len(self._dist_context.process_meshes)\n    self._cur_pp_stage = self._get_pp_stage(self._cur_rank)\n    if self._mode == '1F1B':\n        _insert_sync_for_fthenb_1f1b(self._program)\n        self._task_1f1b()\n    elif self._mode == 'F-Then-B':\n        raise NotImplementedError('F-Then-B has not been implemented')\n    elif self._mode == 'stream':\n        self._insert_sync_ops_for_stream()\n        self._task_stream()\n    else:\n        raise ValueError(f\"Now only 'F-then-B', '1F1B' and 'stream' are supported.The given value is {self._mode}.\")",
            "def _apply_single_impl(self, main_program, startup_program, context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self._dist_context = self.get_attr('dist_context')\n    self._acc_steps = self.get_attr('accumulate_steps')\n    self._mode = self.get_attr('schedule_mode')\n    self._gen_bsz = self.get_attr('generation_batch_size')\n    self._program = main_program\n    self._cur_rank = int(os.getenv('PADDLE_TRAINER_ID', 0))\n    trainer_endpoints = os.getenv('PADDLE_TRAINER_ENDPOINTS', '').split(',')\n    self._nrank = len(trainer_endpoints)\n    self._pp_stages = len(self._dist_context.process_meshes)\n    self._cur_pp_stage = self._get_pp_stage(self._cur_rank)\n    if self._mode == '1F1B':\n        _insert_sync_for_fthenb_1f1b(self._program)\n        self._task_1f1b()\n    elif self._mode == 'F-Then-B':\n        raise NotImplementedError('F-Then-B has not been implemented')\n    elif self._mode == 'stream':\n        self._insert_sync_ops_for_stream()\n        self._task_stream()\n    else:\n        raise ValueError(f\"Now only 'F-then-B', '1F1B' and 'stream' are supported.The given value is {self._mode}.\")",
            "def _apply_single_impl(self, main_program, startup_program, context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self._dist_context = self.get_attr('dist_context')\n    self._acc_steps = self.get_attr('accumulate_steps')\n    self._mode = self.get_attr('schedule_mode')\n    self._gen_bsz = self.get_attr('generation_batch_size')\n    self._program = main_program\n    self._cur_rank = int(os.getenv('PADDLE_TRAINER_ID', 0))\n    trainer_endpoints = os.getenv('PADDLE_TRAINER_ENDPOINTS', '').split(',')\n    self._nrank = len(trainer_endpoints)\n    self._pp_stages = len(self._dist_context.process_meshes)\n    self._cur_pp_stage = self._get_pp_stage(self._cur_rank)\n    if self._mode == '1F1B':\n        _insert_sync_for_fthenb_1f1b(self._program)\n        self._task_1f1b()\n    elif self._mode == 'F-Then-B':\n        raise NotImplementedError('F-Then-B has not been implemented')\n    elif self._mode == 'stream':\n        self._insert_sync_ops_for_stream()\n        self._task_stream()\n    else:\n        raise ValueError(f\"Now only 'F-then-B', '1F1B' and 'stream' are supported.The given value is {self._mode}.\")",
            "def _apply_single_impl(self, main_program, startup_program, context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self._dist_context = self.get_attr('dist_context')\n    self._acc_steps = self.get_attr('accumulate_steps')\n    self._mode = self.get_attr('schedule_mode')\n    self._gen_bsz = self.get_attr('generation_batch_size')\n    self._program = main_program\n    self._cur_rank = int(os.getenv('PADDLE_TRAINER_ID', 0))\n    trainer_endpoints = os.getenv('PADDLE_TRAINER_ENDPOINTS', '').split(',')\n    self._nrank = len(trainer_endpoints)\n    self._pp_stages = len(self._dist_context.process_meshes)\n    self._cur_pp_stage = self._get_pp_stage(self._cur_rank)\n    if self._mode == '1F1B':\n        _insert_sync_for_fthenb_1f1b(self._program)\n        self._task_1f1b()\n    elif self._mode == 'F-Then-B':\n        raise NotImplementedError('F-Then-B has not been implemented')\n    elif self._mode == 'stream':\n        self._insert_sync_ops_for_stream()\n        self._task_stream()\n    else:\n        raise ValueError(f\"Now only 'F-then-B', '1F1B' and 'stream' are supported.The given value is {self._mode}.\")",
            "def _apply_single_impl(self, main_program, startup_program, context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self._dist_context = self.get_attr('dist_context')\n    self._acc_steps = self.get_attr('accumulate_steps')\n    self._mode = self.get_attr('schedule_mode')\n    self._gen_bsz = self.get_attr('generation_batch_size')\n    self._program = main_program\n    self._cur_rank = int(os.getenv('PADDLE_TRAINER_ID', 0))\n    trainer_endpoints = os.getenv('PADDLE_TRAINER_ENDPOINTS', '').split(',')\n    self._nrank = len(trainer_endpoints)\n    self._pp_stages = len(self._dist_context.process_meshes)\n    self._cur_pp_stage = self._get_pp_stage(self._cur_rank)\n    if self._mode == '1F1B':\n        _insert_sync_for_fthenb_1f1b(self._program)\n        self._task_1f1b()\n    elif self._mode == 'F-Then-B':\n        raise NotImplementedError('F-Then-B has not been implemented')\n    elif self._mode == 'stream':\n        self._insert_sync_ops_for_stream()\n        self._task_stream()\n    else:\n        raise ValueError(f\"Now only 'F-then-B', '1F1B' and 'stream' are supported.The given value is {self._mode}.\")"
        ]
    },
    {
        "func_name": "_insert_sync_ops_for_stream",
        "original": "def _insert_sync_ops_for_stream(self):\n    for block in self._program.blocks:\n        offset = 0\n        send_vars = []\n        for (index, op) in enumerate(list(block.ops)):\n            if op.type in ['send_v2', 'recv_v2']:\n                op._set_attr('dynamic_shape', False)\n            if op.type == 'send_v2':\n                op._set_attr('use_calc_stream', False)\n                op_role = op.attr('op_role')\n                var_name = op.input_arg_names[0]\n                var = block.var(var_name)\n                block._insert_op_without_sync(index=index + offset, type='c_sync_calc_stream', inputs={'X': [var]}, outputs={'Out': [var]}, attrs={'op_role': op_role})\n                offset += 1\n                send_vars.append(var_name)\n        for var_name in send_vars:\n            nop_op = block.append_op(type='nop')\n            nop_op.desc.set_input('X', [var_name])\n            nop_op.desc.set_output('Out', [var_name])\n        block._sync_with_cpp()",
        "mutated": [
            "def _insert_sync_ops_for_stream(self):\n    if False:\n        i = 10\n    for block in self._program.blocks:\n        offset = 0\n        send_vars = []\n        for (index, op) in enumerate(list(block.ops)):\n            if op.type in ['send_v2', 'recv_v2']:\n                op._set_attr('dynamic_shape', False)\n            if op.type == 'send_v2':\n                op._set_attr('use_calc_stream', False)\n                op_role = op.attr('op_role')\n                var_name = op.input_arg_names[0]\n                var = block.var(var_name)\n                block._insert_op_without_sync(index=index + offset, type='c_sync_calc_stream', inputs={'X': [var]}, outputs={'Out': [var]}, attrs={'op_role': op_role})\n                offset += 1\n                send_vars.append(var_name)\n        for var_name in send_vars:\n            nop_op = block.append_op(type='nop')\n            nop_op.desc.set_input('X', [var_name])\n            nop_op.desc.set_output('Out', [var_name])\n        block._sync_with_cpp()",
            "def _insert_sync_ops_for_stream(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for block in self._program.blocks:\n        offset = 0\n        send_vars = []\n        for (index, op) in enumerate(list(block.ops)):\n            if op.type in ['send_v2', 'recv_v2']:\n                op._set_attr('dynamic_shape', False)\n            if op.type == 'send_v2':\n                op._set_attr('use_calc_stream', False)\n                op_role = op.attr('op_role')\n                var_name = op.input_arg_names[0]\n                var = block.var(var_name)\n                block._insert_op_without_sync(index=index + offset, type='c_sync_calc_stream', inputs={'X': [var]}, outputs={'Out': [var]}, attrs={'op_role': op_role})\n                offset += 1\n                send_vars.append(var_name)\n        for var_name in send_vars:\n            nop_op = block.append_op(type='nop')\n            nop_op.desc.set_input('X', [var_name])\n            nop_op.desc.set_output('Out', [var_name])\n        block._sync_with_cpp()",
            "def _insert_sync_ops_for_stream(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for block in self._program.blocks:\n        offset = 0\n        send_vars = []\n        for (index, op) in enumerate(list(block.ops)):\n            if op.type in ['send_v2', 'recv_v2']:\n                op._set_attr('dynamic_shape', False)\n            if op.type == 'send_v2':\n                op._set_attr('use_calc_stream', False)\n                op_role = op.attr('op_role')\n                var_name = op.input_arg_names[0]\n                var = block.var(var_name)\n                block._insert_op_without_sync(index=index + offset, type='c_sync_calc_stream', inputs={'X': [var]}, outputs={'Out': [var]}, attrs={'op_role': op_role})\n                offset += 1\n                send_vars.append(var_name)\n        for var_name in send_vars:\n            nop_op = block.append_op(type='nop')\n            nop_op.desc.set_input('X', [var_name])\n            nop_op.desc.set_output('Out', [var_name])\n        block._sync_with_cpp()",
            "def _insert_sync_ops_for_stream(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for block in self._program.blocks:\n        offset = 0\n        send_vars = []\n        for (index, op) in enumerate(list(block.ops)):\n            if op.type in ['send_v2', 'recv_v2']:\n                op._set_attr('dynamic_shape', False)\n            if op.type == 'send_v2':\n                op._set_attr('use_calc_stream', False)\n                op_role = op.attr('op_role')\n                var_name = op.input_arg_names[0]\n                var = block.var(var_name)\n                block._insert_op_without_sync(index=index + offset, type='c_sync_calc_stream', inputs={'X': [var]}, outputs={'Out': [var]}, attrs={'op_role': op_role})\n                offset += 1\n                send_vars.append(var_name)\n        for var_name in send_vars:\n            nop_op = block.append_op(type='nop')\n            nop_op.desc.set_input('X', [var_name])\n            nop_op.desc.set_output('Out', [var_name])\n        block._sync_with_cpp()",
            "def _insert_sync_ops_for_stream(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for block in self._program.blocks:\n        offset = 0\n        send_vars = []\n        for (index, op) in enumerate(list(block.ops)):\n            if op.type in ['send_v2', 'recv_v2']:\n                op._set_attr('dynamic_shape', False)\n            if op.type == 'send_v2':\n                op._set_attr('use_calc_stream', False)\n                op_role = op.attr('op_role')\n                var_name = op.input_arg_names[0]\n                var = block.var(var_name)\n                block._insert_op_without_sync(index=index + offset, type='c_sync_calc_stream', inputs={'X': [var]}, outputs={'Out': [var]}, attrs={'op_role': op_role})\n                offset += 1\n                send_vars.append(var_name)\n        for var_name in send_vars:\n            nop_op = block.append_op(type='nop')\n            nop_op.desc.set_input('X', [var_name])\n            nop_op.desc.set_output('Out', [var_name])\n        block._sync_with_cpp()"
        ]
    },
    {
        "func_name": "_get_pp_stage",
        "original": "def _get_pp_stage(self, rank):\n    pp_idx = None\n    for (idx, process_mesh) in enumerate(self._dist_context.process_meshes):\n        if rank in process_mesh.process_ids:\n            pp_idx = idx\n            break\n    return pp_idx",
        "mutated": [
            "def _get_pp_stage(self, rank):\n    if False:\n        i = 10\n    pp_idx = None\n    for (idx, process_mesh) in enumerate(self._dist_context.process_meshes):\n        if rank in process_mesh.process_ids:\n            pp_idx = idx\n            break\n    return pp_idx",
            "def _get_pp_stage(self, rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    pp_idx = None\n    for (idx, process_mesh) in enumerate(self._dist_context.process_meshes):\n        if rank in process_mesh.process_ids:\n            pp_idx = idx\n            break\n    return pp_idx",
            "def _get_pp_stage(self, rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    pp_idx = None\n    for (idx, process_mesh) in enumerate(self._dist_context.process_meshes):\n        if rank in process_mesh.process_ids:\n            pp_idx = idx\n            break\n    return pp_idx",
            "def _get_pp_stage(self, rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    pp_idx = None\n    for (idx, process_mesh) in enumerate(self._dist_context.process_meshes):\n        if rank in process_mesh.process_ids:\n            pp_idx = idx\n            break\n    return pp_idx",
            "def _get_pp_stage(self, rank):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    pp_idx = None\n    for (idx, process_mesh) in enumerate(self._dist_context.process_meshes):\n        if rank in process_mesh.process_ids:\n            pp_idx = idx\n            break\n    return pp_idx"
        ]
    },
    {
        "func_name": "_task_1f1b",
        "original": "def _task_1f1b(self):\n    num_of_functionality = 4\n    lr_prog = Program()\n    fwd_prog = Program()\n    bwd_prog = Program()\n    opt_prog = Program()\n    for (idx, src_block) in enumerate(self._program.blocks):\n        if idx == 0:\n            lr_block = lr_prog.block(0)\n            fwd_block = fwd_prog.block(0)\n            bwd_block = bwd_prog.block(0)\n            opt_block = opt_prog.block(0)\n        else:\n            lr_block = lr_prog._create_block(parent_idx=src_block.parent_idx)\n            fwd_block = fwd_prog._create_block(parent_idx=src_block.parent_idx)\n            bwd_block = bwd_prog._create_block(parent_idx=src_block.parent_idx)\n            opt_block = opt_prog._create_block(parent_idx=src_block.parent_idx)\n            lr_block._set_forward_block_idx(src_block.forward_block_idx)\n            fwd_block._set_forward_block_idx(src_block.forward_block_idx)\n            bwd_block._set_forward_block_idx(src_block.forward_block_idx)\n            opt_block._set_forward_block_idx(src_block.forward_block_idx)\n        for op in src_block.ops:\n            if is_lr_sched_op(op):\n                _create_program(src_block, lr_block, op)\n            if is_forward_op(op):\n                _create_program(src_block, fwd_block, op)\n            elif is_backward_op(op):\n                _create_program(src_block, bwd_block, op)\n            elif is_optimize_op(op):\n                _create_program(src_block, opt_block, op)\n            else:\n                raise ValueError('The op role: ' + str(op.attr('op_role')) + \" isn't one of LRSched, Forward, Backward or Optimizer.\")\n    lr_prog._sync_with_cpp()\n    fwd_prog._sync_with_cpp()\n    bwd_prog._sync_with_cpp()\n    opt_prog._sync_with_cpp()\n    lr_prog._rollback()\n    fwd_prog._rollback()\n    bwd_prog._rollback()\n    opt_prog._rollback()\n    lr_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=lr_prog, task_id=int(self._cur_rank * num_of_functionality + 0), node_type='Amplifier', lazy_initialize=True)\n    lr_task_node.set_run_pre_steps(self._acc_steps)\n    fwd_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=fwd_prog, task_id=int(self._cur_rank * num_of_functionality + 1), node_type='Compute', lazy_initialize=True)\n    bwd_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=bwd_prog, task_id=int(self._cur_rank * num_of_functionality + 2), node_type='Compute', lazy_initialize=True)\n    opt_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=opt_prog, task_id=int(self._cur_rank * num_of_functionality + 3), node_type='Amplifier', lazy_initialize=True)\n    opt_task_node.set_run_pre_steps(self._acc_steps)\n    opt_task_node.set_run_at_offset(self._acc_steps - 1)\n    task_nodes = {'lr': lr_task_node, 'fwd': fwd_task_node, 'bwd': bwd_task_node, 'opt': opt_task_node}\n    up_down_streams = self._dist_context.up_down_streams\n    pp_upstream_ranks = up_down_streams.ups(self._cur_rank)\n    pp_downstream_ranks = up_down_streams.downs(self._cur_rank)\n    for (i, (task_role, task_node)) in enumerate(task_nodes.items()):\n        cur_id = int(self._cur_rank * num_of_functionality + i)\n        ups = []\n        downs = []\n        pp_buff_size = int(self._pp_stages - self._cur_pp_stage)\n        prev_id = cur_id - 1\n        next_id = cur_id + 1\n        if task_role != 'lr':\n            buf_size = pp_buff_size if task_role == 'bwd' else 2\n            ups.append((prev_id, buf_size))\n        if task_role != 'opt':\n            buf_size = pp_buff_size if task_role == 'fwd' else 2\n            downs.append((next_id, buf_size))\n        for upstream in pp_upstream_ranks:\n            upstream_id = int(upstream * num_of_functionality + i)\n            if task_role == 'fwd':\n                if upstream != -1:\n                    ups.append((upstream_id, 2))\n            elif task_role == 'bwd':\n                if upstream != -1:\n                    downs.append((upstream_id, 2))\n        for downstream in pp_downstream_ranks:\n            downstream_id = int(downstream * num_of_functionality + i)\n            if task_role == 'fwd':\n                if downstream != -1:\n                    downs.append((downstream_id, 2))\n            elif task_role == 'bwd':\n                if downstream != -1:\n                    ups.append((downstream_id, 2))\n        for up in ups:\n            print('Task:', cur_id, \"'s upstream includes:\", up[0], ', buffer size is:', up[1])\n            task_node.add_upstream_task(up[0], up[1])\n        for down in downs:\n            print('Task:', cur_id, \"'s downstream includes:\", down[0], ', buffer size is:', down[1])\n            task_node.add_downstream_task(down[0], down[1])\n    task_id_to_rank = {}\n    for i in range(self._nrank):\n        for j in range(num_of_functionality):\n            task_id_to_rank[int(i * num_of_functionality + j)] = i\n    self._program._pipeline_opt = {}\n    self._program._pipeline_opt['fleet_opt'] = {'tasks': list(task_nodes.values()), 'task_id_to_rank': task_id_to_rank, 'num_micro_batches': self._acc_steps}",
        "mutated": [
            "def _task_1f1b(self):\n    if False:\n        i = 10\n    num_of_functionality = 4\n    lr_prog = Program()\n    fwd_prog = Program()\n    bwd_prog = Program()\n    opt_prog = Program()\n    for (idx, src_block) in enumerate(self._program.blocks):\n        if idx == 0:\n            lr_block = lr_prog.block(0)\n            fwd_block = fwd_prog.block(0)\n            bwd_block = bwd_prog.block(0)\n            opt_block = opt_prog.block(0)\n        else:\n            lr_block = lr_prog._create_block(parent_idx=src_block.parent_idx)\n            fwd_block = fwd_prog._create_block(parent_idx=src_block.parent_idx)\n            bwd_block = bwd_prog._create_block(parent_idx=src_block.parent_idx)\n            opt_block = opt_prog._create_block(parent_idx=src_block.parent_idx)\n            lr_block._set_forward_block_idx(src_block.forward_block_idx)\n            fwd_block._set_forward_block_idx(src_block.forward_block_idx)\n            bwd_block._set_forward_block_idx(src_block.forward_block_idx)\n            opt_block._set_forward_block_idx(src_block.forward_block_idx)\n        for op in src_block.ops:\n            if is_lr_sched_op(op):\n                _create_program(src_block, lr_block, op)\n            if is_forward_op(op):\n                _create_program(src_block, fwd_block, op)\n            elif is_backward_op(op):\n                _create_program(src_block, bwd_block, op)\n            elif is_optimize_op(op):\n                _create_program(src_block, opt_block, op)\n            else:\n                raise ValueError('The op role: ' + str(op.attr('op_role')) + \" isn't one of LRSched, Forward, Backward or Optimizer.\")\n    lr_prog._sync_with_cpp()\n    fwd_prog._sync_with_cpp()\n    bwd_prog._sync_with_cpp()\n    opt_prog._sync_with_cpp()\n    lr_prog._rollback()\n    fwd_prog._rollback()\n    bwd_prog._rollback()\n    opt_prog._rollback()\n    lr_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=lr_prog, task_id=int(self._cur_rank * num_of_functionality + 0), node_type='Amplifier', lazy_initialize=True)\n    lr_task_node.set_run_pre_steps(self._acc_steps)\n    fwd_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=fwd_prog, task_id=int(self._cur_rank * num_of_functionality + 1), node_type='Compute', lazy_initialize=True)\n    bwd_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=bwd_prog, task_id=int(self._cur_rank * num_of_functionality + 2), node_type='Compute', lazy_initialize=True)\n    opt_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=opt_prog, task_id=int(self._cur_rank * num_of_functionality + 3), node_type='Amplifier', lazy_initialize=True)\n    opt_task_node.set_run_pre_steps(self._acc_steps)\n    opt_task_node.set_run_at_offset(self._acc_steps - 1)\n    task_nodes = {'lr': lr_task_node, 'fwd': fwd_task_node, 'bwd': bwd_task_node, 'opt': opt_task_node}\n    up_down_streams = self._dist_context.up_down_streams\n    pp_upstream_ranks = up_down_streams.ups(self._cur_rank)\n    pp_downstream_ranks = up_down_streams.downs(self._cur_rank)\n    for (i, (task_role, task_node)) in enumerate(task_nodes.items()):\n        cur_id = int(self._cur_rank * num_of_functionality + i)\n        ups = []\n        downs = []\n        pp_buff_size = int(self._pp_stages - self._cur_pp_stage)\n        prev_id = cur_id - 1\n        next_id = cur_id + 1\n        if task_role != 'lr':\n            buf_size = pp_buff_size if task_role == 'bwd' else 2\n            ups.append((prev_id, buf_size))\n        if task_role != 'opt':\n            buf_size = pp_buff_size if task_role == 'fwd' else 2\n            downs.append((next_id, buf_size))\n        for upstream in pp_upstream_ranks:\n            upstream_id = int(upstream * num_of_functionality + i)\n            if task_role == 'fwd':\n                if upstream != -1:\n                    ups.append((upstream_id, 2))\n            elif task_role == 'bwd':\n                if upstream != -1:\n                    downs.append((upstream_id, 2))\n        for downstream in pp_downstream_ranks:\n            downstream_id = int(downstream * num_of_functionality + i)\n            if task_role == 'fwd':\n                if downstream != -1:\n                    downs.append((downstream_id, 2))\n            elif task_role == 'bwd':\n                if downstream != -1:\n                    ups.append((downstream_id, 2))\n        for up in ups:\n            print('Task:', cur_id, \"'s upstream includes:\", up[0], ', buffer size is:', up[1])\n            task_node.add_upstream_task(up[0], up[1])\n        for down in downs:\n            print('Task:', cur_id, \"'s downstream includes:\", down[0], ', buffer size is:', down[1])\n            task_node.add_downstream_task(down[0], down[1])\n    task_id_to_rank = {}\n    for i in range(self._nrank):\n        for j in range(num_of_functionality):\n            task_id_to_rank[int(i * num_of_functionality + j)] = i\n    self._program._pipeline_opt = {}\n    self._program._pipeline_opt['fleet_opt'] = {'tasks': list(task_nodes.values()), 'task_id_to_rank': task_id_to_rank, 'num_micro_batches': self._acc_steps}",
            "def _task_1f1b(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    num_of_functionality = 4\n    lr_prog = Program()\n    fwd_prog = Program()\n    bwd_prog = Program()\n    opt_prog = Program()\n    for (idx, src_block) in enumerate(self._program.blocks):\n        if idx == 0:\n            lr_block = lr_prog.block(0)\n            fwd_block = fwd_prog.block(0)\n            bwd_block = bwd_prog.block(0)\n            opt_block = opt_prog.block(0)\n        else:\n            lr_block = lr_prog._create_block(parent_idx=src_block.parent_idx)\n            fwd_block = fwd_prog._create_block(parent_idx=src_block.parent_idx)\n            bwd_block = bwd_prog._create_block(parent_idx=src_block.parent_idx)\n            opt_block = opt_prog._create_block(parent_idx=src_block.parent_idx)\n            lr_block._set_forward_block_idx(src_block.forward_block_idx)\n            fwd_block._set_forward_block_idx(src_block.forward_block_idx)\n            bwd_block._set_forward_block_idx(src_block.forward_block_idx)\n            opt_block._set_forward_block_idx(src_block.forward_block_idx)\n        for op in src_block.ops:\n            if is_lr_sched_op(op):\n                _create_program(src_block, lr_block, op)\n            if is_forward_op(op):\n                _create_program(src_block, fwd_block, op)\n            elif is_backward_op(op):\n                _create_program(src_block, bwd_block, op)\n            elif is_optimize_op(op):\n                _create_program(src_block, opt_block, op)\n            else:\n                raise ValueError('The op role: ' + str(op.attr('op_role')) + \" isn't one of LRSched, Forward, Backward or Optimizer.\")\n    lr_prog._sync_with_cpp()\n    fwd_prog._sync_with_cpp()\n    bwd_prog._sync_with_cpp()\n    opt_prog._sync_with_cpp()\n    lr_prog._rollback()\n    fwd_prog._rollback()\n    bwd_prog._rollback()\n    opt_prog._rollback()\n    lr_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=lr_prog, task_id=int(self._cur_rank * num_of_functionality + 0), node_type='Amplifier', lazy_initialize=True)\n    lr_task_node.set_run_pre_steps(self._acc_steps)\n    fwd_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=fwd_prog, task_id=int(self._cur_rank * num_of_functionality + 1), node_type='Compute', lazy_initialize=True)\n    bwd_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=bwd_prog, task_id=int(self._cur_rank * num_of_functionality + 2), node_type='Compute', lazy_initialize=True)\n    opt_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=opt_prog, task_id=int(self._cur_rank * num_of_functionality + 3), node_type='Amplifier', lazy_initialize=True)\n    opt_task_node.set_run_pre_steps(self._acc_steps)\n    opt_task_node.set_run_at_offset(self._acc_steps - 1)\n    task_nodes = {'lr': lr_task_node, 'fwd': fwd_task_node, 'bwd': bwd_task_node, 'opt': opt_task_node}\n    up_down_streams = self._dist_context.up_down_streams\n    pp_upstream_ranks = up_down_streams.ups(self._cur_rank)\n    pp_downstream_ranks = up_down_streams.downs(self._cur_rank)\n    for (i, (task_role, task_node)) in enumerate(task_nodes.items()):\n        cur_id = int(self._cur_rank * num_of_functionality + i)\n        ups = []\n        downs = []\n        pp_buff_size = int(self._pp_stages - self._cur_pp_stage)\n        prev_id = cur_id - 1\n        next_id = cur_id + 1\n        if task_role != 'lr':\n            buf_size = pp_buff_size if task_role == 'bwd' else 2\n            ups.append((prev_id, buf_size))\n        if task_role != 'opt':\n            buf_size = pp_buff_size if task_role == 'fwd' else 2\n            downs.append((next_id, buf_size))\n        for upstream in pp_upstream_ranks:\n            upstream_id = int(upstream * num_of_functionality + i)\n            if task_role == 'fwd':\n                if upstream != -1:\n                    ups.append((upstream_id, 2))\n            elif task_role == 'bwd':\n                if upstream != -1:\n                    downs.append((upstream_id, 2))\n        for downstream in pp_downstream_ranks:\n            downstream_id = int(downstream * num_of_functionality + i)\n            if task_role == 'fwd':\n                if downstream != -1:\n                    downs.append((downstream_id, 2))\n            elif task_role == 'bwd':\n                if downstream != -1:\n                    ups.append((downstream_id, 2))\n        for up in ups:\n            print('Task:', cur_id, \"'s upstream includes:\", up[0], ', buffer size is:', up[1])\n            task_node.add_upstream_task(up[0], up[1])\n        for down in downs:\n            print('Task:', cur_id, \"'s downstream includes:\", down[0], ', buffer size is:', down[1])\n            task_node.add_downstream_task(down[0], down[1])\n    task_id_to_rank = {}\n    for i in range(self._nrank):\n        for j in range(num_of_functionality):\n            task_id_to_rank[int(i * num_of_functionality + j)] = i\n    self._program._pipeline_opt = {}\n    self._program._pipeline_opt['fleet_opt'] = {'tasks': list(task_nodes.values()), 'task_id_to_rank': task_id_to_rank, 'num_micro_batches': self._acc_steps}",
            "def _task_1f1b(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    num_of_functionality = 4\n    lr_prog = Program()\n    fwd_prog = Program()\n    bwd_prog = Program()\n    opt_prog = Program()\n    for (idx, src_block) in enumerate(self._program.blocks):\n        if idx == 0:\n            lr_block = lr_prog.block(0)\n            fwd_block = fwd_prog.block(0)\n            bwd_block = bwd_prog.block(0)\n            opt_block = opt_prog.block(0)\n        else:\n            lr_block = lr_prog._create_block(parent_idx=src_block.parent_idx)\n            fwd_block = fwd_prog._create_block(parent_idx=src_block.parent_idx)\n            bwd_block = bwd_prog._create_block(parent_idx=src_block.parent_idx)\n            opt_block = opt_prog._create_block(parent_idx=src_block.parent_idx)\n            lr_block._set_forward_block_idx(src_block.forward_block_idx)\n            fwd_block._set_forward_block_idx(src_block.forward_block_idx)\n            bwd_block._set_forward_block_idx(src_block.forward_block_idx)\n            opt_block._set_forward_block_idx(src_block.forward_block_idx)\n        for op in src_block.ops:\n            if is_lr_sched_op(op):\n                _create_program(src_block, lr_block, op)\n            if is_forward_op(op):\n                _create_program(src_block, fwd_block, op)\n            elif is_backward_op(op):\n                _create_program(src_block, bwd_block, op)\n            elif is_optimize_op(op):\n                _create_program(src_block, opt_block, op)\n            else:\n                raise ValueError('The op role: ' + str(op.attr('op_role')) + \" isn't one of LRSched, Forward, Backward or Optimizer.\")\n    lr_prog._sync_with_cpp()\n    fwd_prog._sync_with_cpp()\n    bwd_prog._sync_with_cpp()\n    opt_prog._sync_with_cpp()\n    lr_prog._rollback()\n    fwd_prog._rollback()\n    bwd_prog._rollback()\n    opt_prog._rollback()\n    lr_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=lr_prog, task_id=int(self._cur_rank * num_of_functionality + 0), node_type='Amplifier', lazy_initialize=True)\n    lr_task_node.set_run_pre_steps(self._acc_steps)\n    fwd_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=fwd_prog, task_id=int(self._cur_rank * num_of_functionality + 1), node_type='Compute', lazy_initialize=True)\n    bwd_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=bwd_prog, task_id=int(self._cur_rank * num_of_functionality + 2), node_type='Compute', lazy_initialize=True)\n    opt_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=opt_prog, task_id=int(self._cur_rank * num_of_functionality + 3), node_type='Amplifier', lazy_initialize=True)\n    opt_task_node.set_run_pre_steps(self._acc_steps)\n    opt_task_node.set_run_at_offset(self._acc_steps - 1)\n    task_nodes = {'lr': lr_task_node, 'fwd': fwd_task_node, 'bwd': bwd_task_node, 'opt': opt_task_node}\n    up_down_streams = self._dist_context.up_down_streams\n    pp_upstream_ranks = up_down_streams.ups(self._cur_rank)\n    pp_downstream_ranks = up_down_streams.downs(self._cur_rank)\n    for (i, (task_role, task_node)) in enumerate(task_nodes.items()):\n        cur_id = int(self._cur_rank * num_of_functionality + i)\n        ups = []\n        downs = []\n        pp_buff_size = int(self._pp_stages - self._cur_pp_stage)\n        prev_id = cur_id - 1\n        next_id = cur_id + 1\n        if task_role != 'lr':\n            buf_size = pp_buff_size if task_role == 'bwd' else 2\n            ups.append((prev_id, buf_size))\n        if task_role != 'opt':\n            buf_size = pp_buff_size if task_role == 'fwd' else 2\n            downs.append((next_id, buf_size))\n        for upstream in pp_upstream_ranks:\n            upstream_id = int(upstream * num_of_functionality + i)\n            if task_role == 'fwd':\n                if upstream != -1:\n                    ups.append((upstream_id, 2))\n            elif task_role == 'bwd':\n                if upstream != -1:\n                    downs.append((upstream_id, 2))\n        for downstream in pp_downstream_ranks:\n            downstream_id = int(downstream * num_of_functionality + i)\n            if task_role == 'fwd':\n                if downstream != -1:\n                    downs.append((downstream_id, 2))\n            elif task_role == 'bwd':\n                if downstream != -1:\n                    ups.append((downstream_id, 2))\n        for up in ups:\n            print('Task:', cur_id, \"'s upstream includes:\", up[0], ', buffer size is:', up[1])\n            task_node.add_upstream_task(up[0], up[1])\n        for down in downs:\n            print('Task:', cur_id, \"'s downstream includes:\", down[0], ', buffer size is:', down[1])\n            task_node.add_downstream_task(down[0], down[1])\n    task_id_to_rank = {}\n    for i in range(self._nrank):\n        for j in range(num_of_functionality):\n            task_id_to_rank[int(i * num_of_functionality + j)] = i\n    self._program._pipeline_opt = {}\n    self._program._pipeline_opt['fleet_opt'] = {'tasks': list(task_nodes.values()), 'task_id_to_rank': task_id_to_rank, 'num_micro_batches': self._acc_steps}",
            "def _task_1f1b(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    num_of_functionality = 4\n    lr_prog = Program()\n    fwd_prog = Program()\n    bwd_prog = Program()\n    opt_prog = Program()\n    for (idx, src_block) in enumerate(self._program.blocks):\n        if idx == 0:\n            lr_block = lr_prog.block(0)\n            fwd_block = fwd_prog.block(0)\n            bwd_block = bwd_prog.block(0)\n            opt_block = opt_prog.block(0)\n        else:\n            lr_block = lr_prog._create_block(parent_idx=src_block.parent_idx)\n            fwd_block = fwd_prog._create_block(parent_idx=src_block.parent_idx)\n            bwd_block = bwd_prog._create_block(parent_idx=src_block.parent_idx)\n            opt_block = opt_prog._create_block(parent_idx=src_block.parent_idx)\n            lr_block._set_forward_block_idx(src_block.forward_block_idx)\n            fwd_block._set_forward_block_idx(src_block.forward_block_idx)\n            bwd_block._set_forward_block_idx(src_block.forward_block_idx)\n            opt_block._set_forward_block_idx(src_block.forward_block_idx)\n        for op in src_block.ops:\n            if is_lr_sched_op(op):\n                _create_program(src_block, lr_block, op)\n            if is_forward_op(op):\n                _create_program(src_block, fwd_block, op)\n            elif is_backward_op(op):\n                _create_program(src_block, bwd_block, op)\n            elif is_optimize_op(op):\n                _create_program(src_block, opt_block, op)\n            else:\n                raise ValueError('The op role: ' + str(op.attr('op_role')) + \" isn't one of LRSched, Forward, Backward or Optimizer.\")\n    lr_prog._sync_with_cpp()\n    fwd_prog._sync_with_cpp()\n    bwd_prog._sync_with_cpp()\n    opt_prog._sync_with_cpp()\n    lr_prog._rollback()\n    fwd_prog._rollback()\n    bwd_prog._rollback()\n    opt_prog._rollback()\n    lr_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=lr_prog, task_id=int(self._cur_rank * num_of_functionality + 0), node_type='Amplifier', lazy_initialize=True)\n    lr_task_node.set_run_pre_steps(self._acc_steps)\n    fwd_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=fwd_prog, task_id=int(self._cur_rank * num_of_functionality + 1), node_type='Compute', lazy_initialize=True)\n    bwd_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=bwd_prog, task_id=int(self._cur_rank * num_of_functionality + 2), node_type='Compute', lazy_initialize=True)\n    opt_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=opt_prog, task_id=int(self._cur_rank * num_of_functionality + 3), node_type='Amplifier', lazy_initialize=True)\n    opt_task_node.set_run_pre_steps(self._acc_steps)\n    opt_task_node.set_run_at_offset(self._acc_steps - 1)\n    task_nodes = {'lr': lr_task_node, 'fwd': fwd_task_node, 'bwd': bwd_task_node, 'opt': opt_task_node}\n    up_down_streams = self._dist_context.up_down_streams\n    pp_upstream_ranks = up_down_streams.ups(self._cur_rank)\n    pp_downstream_ranks = up_down_streams.downs(self._cur_rank)\n    for (i, (task_role, task_node)) in enumerate(task_nodes.items()):\n        cur_id = int(self._cur_rank * num_of_functionality + i)\n        ups = []\n        downs = []\n        pp_buff_size = int(self._pp_stages - self._cur_pp_stage)\n        prev_id = cur_id - 1\n        next_id = cur_id + 1\n        if task_role != 'lr':\n            buf_size = pp_buff_size if task_role == 'bwd' else 2\n            ups.append((prev_id, buf_size))\n        if task_role != 'opt':\n            buf_size = pp_buff_size if task_role == 'fwd' else 2\n            downs.append((next_id, buf_size))\n        for upstream in pp_upstream_ranks:\n            upstream_id = int(upstream * num_of_functionality + i)\n            if task_role == 'fwd':\n                if upstream != -1:\n                    ups.append((upstream_id, 2))\n            elif task_role == 'bwd':\n                if upstream != -1:\n                    downs.append((upstream_id, 2))\n        for downstream in pp_downstream_ranks:\n            downstream_id = int(downstream * num_of_functionality + i)\n            if task_role == 'fwd':\n                if downstream != -1:\n                    downs.append((downstream_id, 2))\n            elif task_role == 'bwd':\n                if downstream != -1:\n                    ups.append((downstream_id, 2))\n        for up in ups:\n            print('Task:', cur_id, \"'s upstream includes:\", up[0], ', buffer size is:', up[1])\n            task_node.add_upstream_task(up[0], up[1])\n        for down in downs:\n            print('Task:', cur_id, \"'s downstream includes:\", down[0], ', buffer size is:', down[1])\n            task_node.add_downstream_task(down[0], down[1])\n    task_id_to_rank = {}\n    for i in range(self._nrank):\n        for j in range(num_of_functionality):\n            task_id_to_rank[int(i * num_of_functionality + j)] = i\n    self._program._pipeline_opt = {}\n    self._program._pipeline_opt['fleet_opt'] = {'tasks': list(task_nodes.values()), 'task_id_to_rank': task_id_to_rank, 'num_micro_batches': self._acc_steps}",
            "def _task_1f1b(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    num_of_functionality = 4\n    lr_prog = Program()\n    fwd_prog = Program()\n    bwd_prog = Program()\n    opt_prog = Program()\n    for (idx, src_block) in enumerate(self._program.blocks):\n        if idx == 0:\n            lr_block = lr_prog.block(0)\n            fwd_block = fwd_prog.block(0)\n            bwd_block = bwd_prog.block(0)\n            opt_block = opt_prog.block(0)\n        else:\n            lr_block = lr_prog._create_block(parent_idx=src_block.parent_idx)\n            fwd_block = fwd_prog._create_block(parent_idx=src_block.parent_idx)\n            bwd_block = bwd_prog._create_block(parent_idx=src_block.parent_idx)\n            opt_block = opt_prog._create_block(parent_idx=src_block.parent_idx)\n            lr_block._set_forward_block_idx(src_block.forward_block_idx)\n            fwd_block._set_forward_block_idx(src_block.forward_block_idx)\n            bwd_block._set_forward_block_idx(src_block.forward_block_idx)\n            opt_block._set_forward_block_idx(src_block.forward_block_idx)\n        for op in src_block.ops:\n            if is_lr_sched_op(op):\n                _create_program(src_block, lr_block, op)\n            if is_forward_op(op):\n                _create_program(src_block, fwd_block, op)\n            elif is_backward_op(op):\n                _create_program(src_block, bwd_block, op)\n            elif is_optimize_op(op):\n                _create_program(src_block, opt_block, op)\n            else:\n                raise ValueError('The op role: ' + str(op.attr('op_role')) + \" isn't one of LRSched, Forward, Backward or Optimizer.\")\n    lr_prog._sync_with_cpp()\n    fwd_prog._sync_with_cpp()\n    bwd_prog._sync_with_cpp()\n    opt_prog._sync_with_cpp()\n    lr_prog._rollback()\n    fwd_prog._rollback()\n    bwd_prog._rollback()\n    opt_prog._rollback()\n    lr_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=lr_prog, task_id=int(self._cur_rank * num_of_functionality + 0), node_type='Amplifier', lazy_initialize=True)\n    lr_task_node.set_run_pre_steps(self._acc_steps)\n    fwd_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=fwd_prog, task_id=int(self._cur_rank * num_of_functionality + 1), node_type='Compute', lazy_initialize=True)\n    bwd_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=bwd_prog, task_id=int(self._cur_rank * num_of_functionality + 2), node_type='Compute', lazy_initialize=True)\n    opt_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, program=opt_prog, task_id=int(self._cur_rank * num_of_functionality + 3), node_type='Amplifier', lazy_initialize=True)\n    opt_task_node.set_run_pre_steps(self._acc_steps)\n    opt_task_node.set_run_at_offset(self._acc_steps - 1)\n    task_nodes = {'lr': lr_task_node, 'fwd': fwd_task_node, 'bwd': bwd_task_node, 'opt': opt_task_node}\n    up_down_streams = self._dist_context.up_down_streams\n    pp_upstream_ranks = up_down_streams.ups(self._cur_rank)\n    pp_downstream_ranks = up_down_streams.downs(self._cur_rank)\n    for (i, (task_role, task_node)) in enumerate(task_nodes.items()):\n        cur_id = int(self._cur_rank * num_of_functionality + i)\n        ups = []\n        downs = []\n        pp_buff_size = int(self._pp_stages - self._cur_pp_stage)\n        prev_id = cur_id - 1\n        next_id = cur_id + 1\n        if task_role != 'lr':\n            buf_size = pp_buff_size if task_role == 'bwd' else 2\n            ups.append((prev_id, buf_size))\n        if task_role != 'opt':\n            buf_size = pp_buff_size if task_role == 'fwd' else 2\n            downs.append((next_id, buf_size))\n        for upstream in pp_upstream_ranks:\n            upstream_id = int(upstream * num_of_functionality + i)\n            if task_role == 'fwd':\n                if upstream != -1:\n                    ups.append((upstream_id, 2))\n            elif task_role == 'bwd':\n                if upstream != -1:\n                    downs.append((upstream_id, 2))\n        for downstream in pp_downstream_ranks:\n            downstream_id = int(downstream * num_of_functionality + i)\n            if task_role == 'fwd':\n                if downstream != -1:\n                    downs.append((downstream_id, 2))\n            elif task_role == 'bwd':\n                if downstream != -1:\n                    ups.append((downstream_id, 2))\n        for up in ups:\n            print('Task:', cur_id, \"'s upstream includes:\", up[0], ', buffer size is:', up[1])\n            task_node.add_upstream_task(up[0], up[1])\n        for down in downs:\n            print('Task:', cur_id, \"'s downstream includes:\", down[0], ', buffer size is:', down[1])\n            task_node.add_downstream_task(down[0], down[1])\n    task_id_to_rank = {}\n    for i in range(self._nrank):\n        for j in range(num_of_functionality):\n            task_id_to_rank[int(i * num_of_functionality + j)] = i\n    self._program._pipeline_opt = {}\n    self._program._pipeline_opt['fleet_opt'] = {'tasks': list(task_nodes.values()), 'task_id_to_rank': task_id_to_rank, 'num_micro_batches': self._acc_steps}"
        ]
    },
    {
        "func_name": "_task_stream",
        "original": "def _task_stream(self):\n    num_of_functionality = 5\n    start_prog = Program()\n    cond_prog = Program()\n    end_prog = Program()\n    send_prog = Program()\n    recv_prog = Program()\n    cond_var_name = None\n    send_vars_name = set()\n    recv_vars_name = {}\n    for (ib, src_block) in enumerate(self._program.blocks):\n        if ib == 0:\n            strat_block = start_prog.block(0)\n            end_block = end_prog.block(0)\n            is_after_while_op = False\n            for op in src_block.ops:\n                if op.type == 'while':\n                    assert len(op.input('Condition')) == 1\n                    cond_var_name = op.input('Condition')[0]\n                    is_after_while_op = True\n                    continue\n                if not is_after_while_op:\n                    _create_program(src_block, strat_block, op, force_create=True)\n                else:\n                    _create_program(src_block, end_block, op, force_create=True)\n        elif ib == 1:\n            send_block = send_prog.block(0)\n            recv_block = recv_prog.block(0)\n            is_after_send_op = False\n            is_after_recv_op = False\n            for (i, op) in enumerate(src_block.ops):\n                if op.type == 'send_v2' and (not is_after_send_op):\n                    is_after_send_op = True\n                if is_after_send_op and (not is_after_recv_op) and (op.type == 'recv_v2'):\n                    is_after_recv_op = True\n                if not is_after_send_op or not is_after_recv_op:\n                    if self._cur_pp_stage == self._pp_stages - 1:\n                        if op.type == 'c_sync_calc_stream' and src_block.ops[i + 1].type == 'send_v2':\n                            continue\n                        if op.type == 'nop':\n                            continue\n                        if op.type not in ['recv_2', 'assign', 'c_allgather'] and op.has_attr('op_namescope') and ('/auto_parallel/reshard' in op.attr('op_namescope')):\n                            if len(op.desc.input_arg_names()) > 0 and '@RESHARD' not in op.desc.input_arg_names()[0]:\n                                send_vars_name.add(op.desc.input_arg_names()[0])\n                                if op.type == 'send_v2':\n                                    remove_process_group(op.attr('ring_id'))\n                                continue\n                            if op.type == 'send_v2':\n                                remove_process_group(op.attr('ring_id'))\n                                continue\n                    _create_program(src_block, send_block, op, force_create=True)\n                    continue\n                if is_after_send_op and is_after_recv_op:\n                    if op.has_attr('op_namescope') and '/auto_parallel/reshard' in op.attr('op_namescope'):\n                        if op.type in ['send_v2', 'recv_v2']:\n                            remove_process_group(op.attr('ring_id'))\n                        var_name = op.desc.output_arg_names()[0]\n                        index = var_name.find('@')\n                        if index > 0:\n                            old_var_name = var_name[:index]\n                        else:\n                            old_var_name = var_name\n                        recv_vars_name[var_name] = old_var_name\n                        if not src_block._find_var_recursive(old_var_name):\n                            src_var = src_block._var_recursive(var_name)\n                            recv_block.create_var(type=src_var.type, name=old_var_name, shape=src_var.shape, dtype=src_var.dtype, lod_level=src_var.lod_level, persistable=src_var.persistable, error_clip=src_var.error_clip, stop_gradient=src_var.stop_gradient, is_data=src_var.is_data, belong_to_optimizer=src_var.belong_to_optimizer)\n                        continue\n                    for in_name in op.desc.input_arg_names():\n                        if in_name in recv_vars_name:\n                            op.desc._rename_input(in_name, recv_vars_name[in_name])\n                    _create_program(src_block, recv_block, op, force_create=True)\n                    continue\n        else:\n            raise Exception('Only support generation condition.')\n    start_prog._sync_with_cpp()\n    end_prog._sync_with_cpp()\n    send_prog._sync_with_cpp()\n    recv_prog._sync_with_cpp()\n    assert cond_var_name is not None\n    send_task_node_var_dtype = {}\n    send_task_node_var_shape = {}\n    recv_task_node_var_dtype = {}\n    recv_task_node_var_shape = {}\n    for var_name in list(send_vars_name):\n        var = send_prog.global_block().vars[var_name]\n        dtype = str(var.dtype)\n        send_task_node_var_dtype[var_name] = dtype[dtype.find('paddle.') + len('paddle.'):]\n        send_task_node_var_shape[var_name] = var.shape\n    for var_name in list(set(recv_vars_name.values())):\n        var = recv_prog.global_block().vars[var_name]\n        dtype = str(var.dtype)\n        recv_task_node_var_dtype[var_name] = dtype[dtype.find('paddle.') + len('paddle.'):]\n        recv_task_node_var_shape[var_name] = var.shape\n    vars_to_dtype = []\n    vars_to_shape = []\n    if len(send_task_node_var_dtype) > 0:\n        assert len(recv_task_node_var_dtype) == 0\n        vars_to_dtype = send_task_node_var_dtype\n        vars_to_shape = send_task_node_var_shape\n    if len(recv_task_node_var_dtype) > 0:\n        assert len(send_task_node_var_dtype) == 0\n        vars_to_dtype = recv_task_node_var_dtype\n        vars_to_shape = recv_task_node_var_shape\n    start_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Start', task_id=int(self._cur_rank * num_of_functionality + 0), program=start_prog, lazy_initialize=True)\n    cond_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Cond', task_id=int(self._cur_rank * num_of_functionality + 1), program=cond_prog, cond_var_name=cond_var_name, lazy_initialize=True)\n    send_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 2), program=send_prog, lazy_initialize=True)\n    recv_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 3), program=recv_prog, lazy_initialize=True, vars_to_dtype=vars_to_dtype, vars_to_shape=vars_to_shape)\n    end_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 4), program=end_prog, lazy_initialize=True)\n    inf = -1\n    pp_buff_size = int(self._pp_stages - self._cur_pp_stage)\n    start_task_node.add_downstream_task(cond_task_node.task_id(), self._gen_bsz)\n    print('Task ', start_task_node.task_id(), \"'s downstream is:\", cond_task_node.task_id(), ', buffer size is:', self._gen_bsz)\n    cond_task_node.add_upstream_task(start_task_node.task_id(), self._gen_bsz)\n    print('Task ', cond_task_node.task_id(), \"'s upstream is:\", start_task_node.task_id(), ', buffer size is:', self._gen_bsz)\n    cond_task_node.add_downstream_task(send_task_node.task_id(), inf)\n    print('Task ', cond_task_node.task_id(), \"'s downstream is:\", send_task_node.task_id(), ', buffer size is:', inf)\n    send_task_node.add_upstream_task(cond_task_node.task_id(), inf)\n    print('Task ', send_task_node.task_id(), \"'s upstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    send_task_node.add_downstream_task(recv_task_node.task_id(), pp_buff_size)\n    print('Task ', send_task_node.task_id(), \"'s downstream is:\", recv_task_node.task_id(), ', buffer size is:', pp_buff_size)\n    recv_task_node.add_upstream_task(send_task_node.task_id(), pp_buff_size)\n    print('Task ', recv_task_node.task_id(), \"'s upstream is:\", send_task_node.task_id(), ', buffer size is:', pp_buff_size)\n    recv_task_node.add_downstream_task(cond_task_node.task_id(), inf, core.DependType.LOOP)\n    print('Task ', recv_task_node.task_id(), \"'s downstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    cond_task_node.add_upstream_task(recv_task_node.task_id(), inf, core.DependType.LOOP)\n    print('Task ', cond_task_node.task_id(), \"'s upstream is:\", recv_task_node.task_id(), ', buffer size is:', inf)\n    cond_task_node.add_downstream_task(end_task_node.task_id(), inf, core.DependType.STOP_LOOP)\n    print('Task ', cond_task_node.task_id(), \"'s downstream is:\", end_task_node.task_id(), ', buffer size is:', inf)\n    end_task_node.add_upstream_task(cond_task_node.task_id(), inf, core.DependType.STOP_LOOP)\n    print('Task ', end_task_node.task_id(), \"'s upstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    up_down_streams = self._dist_context.up_down_streams\n    pp_upstream_ranks = up_down_streams.ups(self._cur_rank)\n    pp_downstream_ranks = up_down_streams.downs(self._cur_rank)\n    for upstream_rank in pp_upstream_ranks:\n        upstream_pp_stage = self._get_pp_stage(upstream_rank)\n        if upstream_pp_stage < self._pp_stages - 1:\n            upstream_task_id = int(upstream_rank * num_of_functionality + 2)\n            send_task_node.add_upstream_task(upstream_task_id)\n            print('Task ', send_task_node.task_id(), \"'s upstream is:\", upstream_task_id, ', buffer size is:', 2)\n        else:\n            upstream_task_id = int(upstream_rank * num_of_functionality + 3)\n            recv_task_node.add_upstream_task(upstream_task_id)\n            print('Task ', recv_task_node.task_id(), \"'s upstream is:\", upstream_task_id, ', buffer size is:', 2)\n    for downstream_rank in pp_downstream_ranks:\n        if self._cur_pp_stage < self._pp_stages - 1:\n            downstream_task_id = int(downstream_rank * num_of_functionality + 2)\n            send_task_node.add_downstream_task(downstream_task_id)\n            print('Task ', send_task_node.task_id(), \"'s downstream is:\", downstream_task_id, ', buffer size is:', 2)\n        else:\n            downstream_task_id = int(downstream_rank * num_of_functionality + 3)\n            recv_task_node.add_downstream_task(downstream_task_id)\n            print('Task ', recv_task_node.task_id(), \"'s downstream is:\", downstream_task_id, ', buffer size is:', 2)\n    task_id_to_rank = {}\n    for i in range(self._nrank):\n        for j in range(num_of_functionality):\n            task_id_to_rank[int(i * num_of_functionality + j)] = i\n    self._program._pipeline_opt = {'fleet_opt': {'tasks': [start_task_node, cond_task_node, send_task_node, recv_task_node, end_task_node], 'task_id_to_rank': task_id_to_rank, 'num_micro_batches': self._acc_steps, 'inference_generation': True}}",
        "mutated": [
            "def _task_stream(self):\n    if False:\n        i = 10\n    num_of_functionality = 5\n    start_prog = Program()\n    cond_prog = Program()\n    end_prog = Program()\n    send_prog = Program()\n    recv_prog = Program()\n    cond_var_name = None\n    send_vars_name = set()\n    recv_vars_name = {}\n    for (ib, src_block) in enumerate(self._program.blocks):\n        if ib == 0:\n            strat_block = start_prog.block(0)\n            end_block = end_prog.block(0)\n            is_after_while_op = False\n            for op in src_block.ops:\n                if op.type == 'while':\n                    assert len(op.input('Condition')) == 1\n                    cond_var_name = op.input('Condition')[0]\n                    is_after_while_op = True\n                    continue\n                if not is_after_while_op:\n                    _create_program(src_block, strat_block, op, force_create=True)\n                else:\n                    _create_program(src_block, end_block, op, force_create=True)\n        elif ib == 1:\n            send_block = send_prog.block(0)\n            recv_block = recv_prog.block(0)\n            is_after_send_op = False\n            is_after_recv_op = False\n            for (i, op) in enumerate(src_block.ops):\n                if op.type == 'send_v2' and (not is_after_send_op):\n                    is_after_send_op = True\n                if is_after_send_op and (not is_after_recv_op) and (op.type == 'recv_v2'):\n                    is_after_recv_op = True\n                if not is_after_send_op or not is_after_recv_op:\n                    if self._cur_pp_stage == self._pp_stages - 1:\n                        if op.type == 'c_sync_calc_stream' and src_block.ops[i + 1].type == 'send_v2':\n                            continue\n                        if op.type == 'nop':\n                            continue\n                        if op.type not in ['recv_2', 'assign', 'c_allgather'] and op.has_attr('op_namescope') and ('/auto_parallel/reshard' in op.attr('op_namescope')):\n                            if len(op.desc.input_arg_names()) > 0 and '@RESHARD' not in op.desc.input_arg_names()[0]:\n                                send_vars_name.add(op.desc.input_arg_names()[0])\n                                if op.type == 'send_v2':\n                                    remove_process_group(op.attr('ring_id'))\n                                continue\n                            if op.type == 'send_v2':\n                                remove_process_group(op.attr('ring_id'))\n                                continue\n                    _create_program(src_block, send_block, op, force_create=True)\n                    continue\n                if is_after_send_op and is_after_recv_op:\n                    if op.has_attr('op_namescope') and '/auto_parallel/reshard' in op.attr('op_namescope'):\n                        if op.type in ['send_v2', 'recv_v2']:\n                            remove_process_group(op.attr('ring_id'))\n                        var_name = op.desc.output_arg_names()[0]\n                        index = var_name.find('@')\n                        if index > 0:\n                            old_var_name = var_name[:index]\n                        else:\n                            old_var_name = var_name\n                        recv_vars_name[var_name] = old_var_name\n                        if not src_block._find_var_recursive(old_var_name):\n                            src_var = src_block._var_recursive(var_name)\n                            recv_block.create_var(type=src_var.type, name=old_var_name, shape=src_var.shape, dtype=src_var.dtype, lod_level=src_var.lod_level, persistable=src_var.persistable, error_clip=src_var.error_clip, stop_gradient=src_var.stop_gradient, is_data=src_var.is_data, belong_to_optimizer=src_var.belong_to_optimizer)\n                        continue\n                    for in_name in op.desc.input_arg_names():\n                        if in_name in recv_vars_name:\n                            op.desc._rename_input(in_name, recv_vars_name[in_name])\n                    _create_program(src_block, recv_block, op, force_create=True)\n                    continue\n        else:\n            raise Exception('Only support generation condition.')\n    start_prog._sync_with_cpp()\n    end_prog._sync_with_cpp()\n    send_prog._sync_with_cpp()\n    recv_prog._sync_with_cpp()\n    assert cond_var_name is not None\n    send_task_node_var_dtype = {}\n    send_task_node_var_shape = {}\n    recv_task_node_var_dtype = {}\n    recv_task_node_var_shape = {}\n    for var_name in list(send_vars_name):\n        var = send_prog.global_block().vars[var_name]\n        dtype = str(var.dtype)\n        send_task_node_var_dtype[var_name] = dtype[dtype.find('paddle.') + len('paddle.'):]\n        send_task_node_var_shape[var_name] = var.shape\n    for var_name in list(set(recv_vars_name.values())):\n        var = recv_prog.global_block().vars[var_name]\n        dtype = str(var.dtype)\n        recv_task_node_var_dtype[var_name] = dtype[dtype.find('paddle.') + len('paddle.'):]\n        recv_task_node_var_shape[var_name] = var.shape\n    vars_to_dtype = []\n    vars_to_shape = []\n    if len(send_task_node_var_dtype) > 0:\n        assert len(recv_task_node_var_dtype) == 0\n        vars_to_dtype = send_task_node_var_dtype\n        vars_to_shape = send_task_node_var_shape\n    if len(recv_task_node_var_dtype) > 0:\n        assert len(send_task_node_var_dtype) == 0\n        vars_to_dtype = recv_task_node_var_dtype\n        vars_to_shape = recv_task_node_var_shape\n    start_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Start', task_id=int(self._cur_rank * num_of_functionality + 0), program=start_prog, lazy_initialize=True)\n    cond_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Cond', task_id=int(self._cur_rank * num_of_functionality + 1), program=cond_prog, cond_var_name=cond_var_name, lazy_initialize=True)\n    send_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 2), program=send_prog, lazy_initialize=True)\n    recv_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 3), program=recv_prog, lazy_initialize=True, vars_to_dtype=vars_to_dtype, vars_to_shape=vars_to_shape)\n    end_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 4), program=end_prog, lazy_initialize=True)\n    inf = -1\n    pp_buff_size = int(self._pp_stages - self._cur_pp_stage)\n    start_task_node.add_downstream_task(cond_task_node.task_id(), self._gen_bsz)\n    print('Task ', start_task_node.task_id(), \"'s downstream is:\", cond_task_node.task_id(), ', buffer size is:', self._gen_bsz)\n    cond_task_node.add_upstream_task(start_task_node.task_id(), self._gen_bsz)\n    print('Task ', cond_task_node.task_id(), \"'s upstream is:\", start_task_node.task_id(), ', buffer size is:', self._gen_bsz)\n    cond_task_node.add_downstream_task(send_task_node.task_id(), inf)\n    print('Task ', cond_task_node.task_id(), \"'s downstream is:\", send_task_node.task_id(), ', buffer size is:', inf)\n    send_task_node.add_upstream_task(cond_task_node.task_id(), inf)\n    print('Task ', send_task_node.task_id(), \"'s upstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    send_task_node.add_downstream_task(recv_task_node.task_id(), pp_buff_size)\n    print('Task ', send_task_node.task_id(), \"'s downstream is:\", recv_task_node.task_id(), ', buffer size is:', pp_buff_size)\n    recv_task_node.add_upstream_task(send_task_node.task_id(), pp_buff_size)\n    print('Task ', recv_task_node.task_id(), \"'s upstream is:\", send_task_node.task_id(), ', buffer size is:', pp_buff_size)\n    recv_task_node.add_downstream_task(cond_task_node.task_id(), inf, core.DependType.LOOP)\n    print('Task ', recv_task_node.task_id(), \"'s downstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    cond_task_node.add_upstream_task(recv_task_node.task_id(), inf, core.DependType.LOOP)\n    print('Task ', cond_task_node.task_id(), \"'s upstream is:\", recv_task_node.task_id(), ', buffer size is:', inf)\n    cond_task_node.add_downstream_task(end_task_node.task_id(), inf, core.DependType.STOP_LOOP)\n    print('Task ', cond_task_node.task_id(), \"'s downstream is:\", end_task_node.task_id(), ', buffer size is:', inf)\n    end_task_node.add_upstream_task(cond_task_node.task_id(), inf, core.DependType.STOP_LOOP)\n    print('Task ', end_task_node.task_id(), \"'s upstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    up_down_streams = self._dist_context.up_down_streams\n    pp_upstream_ranks = up_down_streams.ups(self._cur_rank)\n    pp_downstream_ranks = up_down_streams.downs(self._cur_rank)\n    for upstream_rank in pp_upstream_ranks:\n        upstream_pp_stage = self._get_pp_stage(upstream_rank)\n        if upstream_pp_stage < self._pp_stages - 1:\n            upstream_task_id = int(upstream_rank * num_of_functionality + 2)\n            send_task_node.add_upstream_task(upstream_task_id)\n            print('Task ', send_task_node.task_id(), \"'s upstream is:\", upstream_task_id, ', buffer size is:', 2)\n        else:\n            upstream_task_id = int(upstream_rank * num_of_functionality + 3)\n            recv_task_node.add_upstream_task(upstream_task_id)\n            print('Task ', recv_task_node.task_id(), \"'s upstream is:\", upstream_task_id, ', buffer size is:', 2)\n    for downstream_rank in pp_downstream_ranks:\n        if self._cur_pp_stage < self._pp_stages - 1:\n            downstream_task_id = int(downstream_rank * num_of_functionality + 2)\n            send_task_node.add_downstream_task(downstream_task_id)\n            print('Task ', send_task_node.task_id(), \"'s downstream is:\", downstream_task_id, ', buffer size is:', 2)\n        else:\n            downstream_task_id = int(downstream_rank * num_of_functionality + 3)\n            recv_task_node.add_downstream_task(downstream_task_id)\n            print('Task ', recv_task_node.task_id(), \"'s downstream is:\", downstream_task_id, ', buffer size is:', 2)\n    task_id_to_rank = {}\n    for i in range(self._nrank):\n        for j in range(num_of_functionality):\n            task_id_to_rank[int(i * num_of_functionality + j)] = i\n    self._program._pipeline_opt = {'fleet_opt': {'tasks': [start_task_node, cond_task_node, send_task_node, recv_task_node, end_task_node], 'task_id_to_rank': task_id_to_rank, 'num_micro_batches': self._acc_steps, 'inference_generation': True}}",
            "def _task_stream(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    num_of_functionality = 5\n    start_prog = Program()\n    cond_prog = Program()\n    end_prog = Program()\n    send_prog = Program()\n    recv_prog = Program()\n    cond_var_name = None\n    send_vars_name = set()\n    recv_vars_name = {}\n    for (ib, src_block) in enumerate(self._program.blocks):\n        if ib == 0:\n            strat_block = start_prog.block(0)\n            end_block = end_prog.block(0)\n            is_after_while_op = False\n            for op in src_block.ops:\n                if op.type == 'while':\n                    assert len(op.input('Condition')) == 1\n                    cond_var_name = op.input('Condition')[0]\n                    is_after_while_op = True\n                    continue\n                if not is_after_while_op:\n                    _create_program(src_block, strat_block, op, force_create=True)\n                else:\n                    _create_program(src_block, end_block, op, force_create=True)\n        elif ib == 1:\n            send_block = send_prog.block(0)\n            recv_block = recv_prog.block(0)\n            is_after_send_op = False\n            is_after_recv_op = False\n            for (i, op) in enumerate(src_block.ops):\n                if op.type == 'send_v2' and (not is_after_send_op):\n                    is_after_send_op = True\n                if is_after_send_op and (not is_after_recv_op) and (op.type == 'recv_v2'):\n                    is_after_recv_op = True\n                if not is_after_send_op or not is_after_recv_op:\n                    if self._cur_pp_stage == self._pp_stages - 1:\n                        if op.type == 'c_sync_calc_stream' and src_block.ops[i + 1].type == 'send_v2':\n                            continue\n                        if op.type == 'nop':\n                            continue\n                        if op.type not in ['recv_2', 'assign', 'c_allgather'] and op.has_attr('op_namescope') and ('/auto_parallel/reshard' in op.attr('op_namescope')):\n                            if len(op.desc.input_arg_names()) > 0 and '@RESHARD' not in op.desc.input_arg_names()[0]:\n                                send_vars_name.add(op.desc.input_arg_names()[0])\n                                if op.type == 'send_v2':\n                                    remove_process_group(op.attr('ring_id'))\n                                continue\n                            if op.type == 'send_v2':\n                                remove_process_group(op.attr('ring_id'))\n                                continue\n                    _create_program(src_block, send_block, op, force_create=True)\n                    continue\n                if is_after_send_op and is_after_recv_op:\n                    if op.has_attr('op_namescope') and '/auto_parallel/reshard' in op.attr('op_namescope'):\n                        if op.type in ['send_v2', 'recv_v2']:\n                            remove_process_group(op.attr('ring_id'))\n                        var_name = op.desc.output_arg_names()[0]\n                        index = var_name.find('@')\n                        if index > 0:\n                            old_var_name = var_name[:index]\n                        else:\n                            old_var_name = var_name\n                        recv_vars_name[var_name] = old_var_name\n                        if not src_block._find_var_recursive(old_var_name):\n                            src_var = src_block._var_recursive(var_name)\n                            recv_block.create_var(type=src_var.type, name=old_var_name, shape=src_var.shape, dtype=src_var.dtype, lod_level=src_var.lod_level, persistable=src_var.persistable, error_clip=src_var.error_clip, stop_gradient=src_var.stop_gradient, is_data=src_var.is_data, belong_to_optimizer=src_var.belong_to_optimizer)\n                        continue\n                    for in_name in op.desc.input_arg_names():\n                        if in_name in recv_vars_name:\n                            op.desc._rename_input(in_name, recv_vars_name[in_name])\n                    _create_program(src_block, recv_block, op, force_create=True)\n                    continue\n        else:\n            raise Exception('Only support generation condition.')\n    start_prog._sync_with_cpp()\n    end_prog._sync_with_cpp()\n    send_prog._sync_with_cpp()\n    recv_prog._sync_with_cpp()\n    assert cond_var_name is not None\n    send_task_node_var_dtype = {}\n    send_task_node_var_shape = {}\n    recv_task_node_var_dtype = {}\n    recv_task_node_var_shape = {}\n    for var_name in list(send_vars_name):\n        var = send_prog.global_block().vars[var_name]\n        dtype = str(var.dtype)\n        send_task_node_var_dtype[var_name] = dtype[dtype.find('paddle.') + len('paddle.'):]\n        send_task_node_var_shape[var_name] = var.shape\n    for var_name in list(set(recv_vars_name.values())):\n        var = recv_prog.global_block().vars[var_name]\n        dtype = str(var.dtype)\n        recv_task_node_var_dtype[var_name] = dtype[dtype.find('paddle.') + len('paddle.'):]\n        recv_task_node_var_shape[var_name] = var.shape\n    vars_to_dtype = []\n    vars_to_shape = []\n    if len(send_task_node_var_dtype) > 0:\n        assert len(recv_task_node_var_dtype) == 0\n        vars_to_dtype = send_task_node_var_dtype\n        vars_to_shape = send_task_node_var_shape\n    if len(recv_task_node_var_dtype) > 0:\n        assert len(send_task_node_var_dtype) == 0\n        vars_to_dtype = recv_task_node_var_dtype\n        vars_to_shape = recv_task_node_var_shape\n    start_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Start', task_id=int(self._cur_rank * num_of_functionality + 0), program=start_prog, lazy_initialize=True)\n    cond_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Cond', task_id=int(self._cur_rank * num_of_functionality + 1), program=cond_prog, cond_var_name=cond_var_name, lazy_initialize=True)\n    send_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 2), program=send_prog, lazy_initialize=True)\n    recv_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 3), program=recv_prog, lazy_initialize=True, vars_to_dtype=vars_to_dtype, vars_to_shape=vars_to_shape)\n    end_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 4), program=end_prog, lazy_initialize=True)\n    inf = -1\n    pp_buff_size = int(self._pp_stages - self._cur_pp_stage)\n    start_task_node.add_downstream_task(cond_task_node.task_id(), self._gen_bsz)\n    print('Task ', start_task_node.task_id(), \"'s downstream is:\", cond_task_node.task_id(), ', buffer size is:', self._gen_bsz)\n    cond_task_node.add_upstream_task(start_task_node.task_id(), self._gen_bsz)\n    print('Task ', cond_task_node.task_id(), \"'s upstream is:\", start_task_node.task_id(), ', buffer size is:', self._gen_bsz)\n    cond_task_node.add_downstream_task(send_task_node.task_id(), inf)\n    print('Task ', cond_task_node.task_id(), \"'s downstream is:\", send_task_node.task_id(), ', buffer size is:', inf)\n    send_task_node.add_upstream_task(cond_task_node.task_id(), inf)\n    print('Task ', send_task_node.task_id(), \"'s upstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    send_task_node.add_downstream_task(recv_task_node.task_id(), pp_buff_size)\n    print('Task ', send_task_node.task_id(), \"'s downstream is:\", recv_task_node.task_id(), ', buffer size is:', pp_buff_size)\n    recv_task_node.add_upstream_task(send_task_node.task_id(), pp_buff_size)\n    print('Task ', recv_task_node.task_id(), \"'s upstream is:\", send_task_node.task_id(), ', buffer size is:', pp_buff_size)\n    recv_task_node.add_downstream_task(cond_task_node.task_id(), inf, core.DependType.LOOP)\n    print('Task ', recv_task_node.task_id(), \"'s downstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    cond_task_node.add_upstream_task(recv_task_node.task_id(), inf, core.DependType.LOOP)\n    print('Task ', cond_task_node.task_id(), \"'s upstream is:\", recv_task_node.task_id(), ', buffer size is:', inf)\n    cond_task_node.add_downstream_task(end_task_node.task_id(), inf, core.DependType.STOP_LOOP)\n    print('Task ', cond_task_node.task_id(), \"'s downstream is:\", end_task_node.task_id(), ', buffer size is:', inf)\n    end_task_node.add_upstream_task(cond_task_node.task_id(), inf, core.DependType.STOP_LOOP)\n    print('Task ', end_task_node.task_id(), \"'s upstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    up_down_streams = self._dist_context.up_down_streams\n    pp_upstream_ranks = up_down_streams.ups(self._cur_rank)\n    pp_downstream_ranks = up_down_streams.downs(self._cur_rank)\n    for upstream_rank in pp_upstream_ranks:\n        upstream_pp_stage = self._get_pp_stage(upstream_rank)\n        if upstream_pp_stage < self._pp_stages - 1:\n            upstream_task_id = int(upstream_rank * num_of_functionality + 2)\n            send_task_node.add_upstream_task(upstream_task_id)\n            print('Task ', send_task_node.task_id(), \"'s upstream is:\", upstream_task_id, ', buffer size is:', 2)\n        else:\n            upstream_task_id = int(upstream_rank * num_of_functionality + 3)\n            recv_task_node.add_upstream_task(upstream_task_id)\n            print('Task ', recv_task_node.task_id(), \"'s upstream is:\", upstream_task_id, ', buffer size is:', 2)\n    for downstream_rank in pp_downstream_ranks:\n        if self._cur_pp_stage < self._pp_stages - 1:\n            downstream_task_id = int(downstream_rank * num_of_functionality + 2)\n            send_task_node.add_downstream_task(downstream_task_id)\n            print('Task ', send_task_node.task_id(), \"'s downstream is:\", downstream_task_id, ', buffer size is:', 2)\n        else:\n            downstream_task_id = int(downstream_rank * num_of_functionality + 3)\n            recv_task_node.add_downstream_task(downstream_task_id)\n            print('Task ', recv_task_node.task_id(), \"'s downstream is:\", downstream_task_id, ', buffer size is:', 2)\n    task_id_to_rank = {}\n    for i in range(self._nrank):\n        for j in range(num_of_functionality):\n            task_id_to_rank[int(i * num_of_functionality + j)] = i\n    self._program._pipeline_opt = {'fleet_opt': {'tasks': [start_task_node, cond_task_node, send_task_node, recv_task_node, end_task_node], 'task_id_to_rank': task_id_to_rank, 'num_micro_batches': self._acc_steps, 'inference_generation': True}}",
            "def _task_stream(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    num_of_functionality = 5\n    start_prog = Program()\n    cond_prog = Program()\n    end_prog = Program()\n    send_prog = Program()\n    recv_prog = Program()\n    cond_var_name = None\n    send_vars_name = set()\n    recv_vars_name = {}\n    for (ib, src_block) in enumerate(self._program.blocks):\n        if ib == 0:\n            strat_block = start_prog.block(0)\n            end_block = end_prog.block(0)\n            is_after_while_op = False\n            for op in src_block.ops:\n                if op.type == 'while':\n                    assert len(op.input('Condition')) == 1\n                    cond_var_name = op.input('Condition')[0]\n                    is_after_while_op = True\n                    continue\n                if not is_after_while_op:\n                    _create_program(src_block, strat_block, op, force_create=True)\n                else:\n                    _create_program(src_block, end_block, op, force_create=True)\n        elif ib == 1:\n            send_block = send_prog.block(0)\n            recv_block = recv_prog.block(0)\n            is_after_send_op = False\n            is_after_recv_op = False\n            for (i, op) in enumerate(src_block.ops):\n                if op.type == 'send_v2' and (not is_after_send_op):\n                    is_after_send_op = True\n                if is_after_send_op and (not is_after_recv_op) and (op.type == 'recv_v2'):\n                    is_after_recv_op = True\n                if not is_after_send_op or not is_after_recv_op:\n                    if self._cur_pp_stage == self._pp_stages - 1:\n                        if op.type == 'c_sync_calc_stream' and src_block.ops[i + 1].type == 'send_v2':\n                            continue\n                        if op.type == 'nop':\n                            continue\n                        if op.type not in ['recv_2', 'assign', 'c_allgather'] and op.has_attr('op_namescope') and ('/auto_parallel/reshard' in op.attr('op_namescope')):\n                            if len(op.desc.input_arg_names()) > 0 and '@RESHARD' not in op.desc.input_arg_names()[0]:\n                                send_vars_name.add(op.desc.input_arg_names()[0])\n                                if op.type == 'send_v2':\n                                    remove_process_group(op.attr('ring_id'))\n                                continue\n                            if op.type == 'send_v2':\n                                remove_process_group(op.attr('ring_id'))\n                                continue\n                    _create_program(src_block, send_block, op, force_create=True)\n                    continue\n                if is_after_send_op and is_after_recv_op:\n                    if op.has_attr('op_namescope') and '/auto_parallel/reshard' in op.attr('op_namescope'):\n                        if op.type in ['send_v2', 'recv_v2']:\n                            remove_process_group(op.attr('ring_id'))\n                        var_name = op.desc.output_arg_names()[0]\n                        index = var_name.find('@')\n                        if index > 0:\n                            old_var_name = var_name[:index]\n                        else:\n                            old_var_name = var_name\n                        recv_vars_name[var_name] = old_var_name\n                        if not src_block._find_var_recursive(old_var_name):\n                            src_var = src_block._var_recursive(var_name)\n                            recv_block.create_var(type=src_var.type, name=old_var_name, shape=src_var.shape, dtype=src_var.dtype, lod_level=src_var.lod_level, persistable=src_var.persistable, error_clip=src_var.error_clip, stop_gradient=src_var.stop_gradient, is_data=src_var.is_data, belong_to_optimizer=src_var.belong_to_optimizer)\n                        continue\n                    for in_name in op.desc.input_arg_names():\n                        if in_name in recv_vars_name:\n                            op.desc._rename_input(in_name, recv_vars_name[in_name])\n                    _create_program(src_block, recv_block, op, force_create=True)\n                    continue\n        else:\n            raise Exception('Only support generation condition.')\n    start_prog._sync_with_cpp()\n    end_prog._sync_with_cpp()\n    send_prog._sync_with_cpp()\n    recv_prog._sync_with_cpp()\n    assert cond_var_name is not None\n    send_task_node_var_dtype = {}\n    send_task_node_var_shape = {}\n    recv_task_node_var_dtype = {}\n    recv_task_node_var_shape = {}\n    for var_name in list(send_vars_name):\n        var = send_prog.global_block().vars[var_name]\n        dtype = str(var.dtype)\n        send_task_node_var_dtype[var_name] = dtype[dtype.find('paddle.') + len('paddle.'):]\n        send_task_node_var_shape[var_name] = var.shape\n    for var_name in list(set(recv_vars_name.values())):\n        var = recv_prog.global_block().vars[var_name]\n        dtype = str(var.dtype)\n        recv_task_node_var_dtype[var_name] = dtype[dtype.find('paddle.') + len('paddle.'):]\n        recv_task_node_var_shape[var_name] = var.shape\n    vars_to_dtype = []\n    vars_to_shape = []\n    if len(send_task_node_var_dtype) > 0:\n        assert len(recv_task_node_var_dtype) == 0\n        vars_to_dtype = send_task_node_var_dtype\n        vars_to_shape = send_task_node_var_shape\n    if len(recv_task_node_var_dtype) > 0:\n        assert len(send_task_node_var_dtype) == 0\n        vars_to_dtype = recv_task_node_var_dtype\n        vars_to_shape = recv_task_node_var_shape\n    start_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Start', task_id=int(self._cur_rank * num_of_functionality + 0), program=start_prog, lazy_initialize=True)\n    cond_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Cond', task_id=int(self._cur_rank * num_of_functionality + 1), program=cond_prog, cond_var_name=cond_var_name, lazy_initialize=True)\n    send_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 2), program=send_prog, lazy_initialize=True)\n    recv_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 3), program=recv_prog, lazy_initialize=True, vars_to_dtype=vars_to_dtype, vars_to_shape=vars_to_shape)\n    end_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 4), program=end_prog, lazy_initialize=True)\n    inf = -1\n    pp_buff_size = int(self._pp_stages - self._cur_pp_stage)\n    start_task_node.add_downstream_task(cond_task_node.task_id(), self._gen_bsz)\n    print('Task ', start_task_node.task_id(), \"'s downstream is:\", cond_task_node.task_id(), ', buffer size is:', self._gen_bsz)\n    cond_task_node.add_upstream_task(start_task_node.task_id(), self._gen_bsz)\n    print('Task ', cond_task_node.task_id(), \"'s upstream is:\", start_task_node.task_id(), ', buffer size is:', self._gen_bsz)\n    cond_task_node.add_downstream_task(send_task_node.task_id(), inf)\n    print('Task ', cond_task_node.task_id(), \"'s downstream is:\", send_task_node.task_id(), ', buffer size is:', inf)\n    send_task_node.add_upstream_task(cond_task_node.task_id(), inf)\n    print('Task ', send_task_node.task_id(), \"'s upstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    send_task_node.add_downstream_task(recv_task_node.task_id(), pp_buff_size)\n    print('Task ', send_task_node.task_id(), \"'s downstream is:\", recv_task_node.task_id(), ', buffer size is:', pp_buff_size)\n    recv_task_node.add_upstream_task(send_task_node.task_id(), pp_buff_size)\n    print('Task ', recv_task_node.task_id(), \"'s upstream is:\", send_task_node.task_id(), ', buffer size is:', pp_buff_size)\n    recv_task_node.add_downstream_task(cond_task_node.task_id(), inf, core.DependType.LOOP)\n    print('Task ', recv_task_node.task_id(), \"'s downstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    cond_task_node.add_upstream_task(recv_task_node.task_id(), inf, core.DependType.LOOP)\n    print('Task ', cond_task_node.task_id(), \"'s upstream is:\", recv_task_node.task_id(), ', buffer size is:', inf)\n    cond_task_node.add_downstream_task(end_task_node.task_id(), inf, core.DependType.STOP_LOOP)\n    print('Task ', cond_task_node.task_id(), \"'s downstream is:\", end_task_node.task_id(), ', buffer size is:', inf)\n    end_task_node.add_upstream_task(cond_task_node.task_id(), inf, core.DependType.STOP_LOOP)\n    print('Task ', end_task_node.task_id(), \"'s upstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    up_down_streams = self._dist_context.up_down_streams\n    pp_upstream_ranks = up_down_streams.ups(self._cur_rank)\n    pp_downstream_ranks = up_down_streams.downs(self._cur_rank)\n    for upstream_rank in pp_upstream_ranks:\n        upstream_pp_stage = self._get_pp_stage(upstream_rank)\n        if upstream_pp_stage < self._pp_stages - 1:\n            upstream_task_id = int(upstream_rank * num_of_functionality + 2)\n            send_task_node.add_upstream_task(upstream_task_id)\n            print('Task ', send_task_node.task_id(), \"'s upstream is:\", upstream_task_id, ', buffer size is:', 2)\n        else:\n            upstream_task_id = int(upstream_rank * num_of_functionality + 3)\n            recv_task_node.add_upstream_task(upstream_task_id)\n            print('Task ', recv_task_node.task_id(), \"'s upstream is:\", upstream_task_id, ', buffer size is:', 2)\n    for downstream_rank in pp_downstream_ranks:\n        if self._cur_pp_stage < self._pp_stages - 1:\n            downstream_task_id = int(downstream_rank * num_of_functionality + 2)\n            send_task_node.add_downstream_task(downstream_task_id)\n            print('Task ', send_task_node.task_id(), \"'s downstream is:\", downstream_task_id, ', buffer size is:', 2)\n        else:\n            downstream_task_id = int(downstream_rank * num_of_functionality + 3)\n            recv_task_node.add_downstream_task(downstream_task_id)\n            print('Task ', recv_task_node.task_id(), \"'s downstream is:\", downstream_task_id, ', buffer size is:', 2)\n    task_id_to_rank = {}\n    for i in range(self._nrank):\n        for j in range(num_of_functionality):\n            task_id_to_rank[int(i * num_of_functionality + j)] = i\n    self._program._pipeline_opt = {'fleet_opt': {'tasks': [start_task_node, cond_task_node, send_task_node, recv_task_node, end_task_node], 'task_id_to_rank': task_id_to_rank, 'num_micro_batches': self._acc_steps, 'inference_generation': True}}",
            "def _task_stream(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    num_of_functionality = 5\n    start_prog = Program()\n    cond_prog = Program()\n    end_prog = Program()\n    send_prog = Program()\n    recv_prog = Program()\n    cond_var_name = None\n    send_vars_name = set()\n    recv_vars_name = {}\n    for (ib, src_block) in enumerate(self._program.blocks):\n        if ib == 0:\n            strat_block = start_prog.block(0)\n            end_block = end_prog.block(0)\n            is_after_while_op = False\n            for op in src_block.ops:\n                if op.type == 'while':\n                    assert len(op.input('Condition')) == 1\n                    cond_var_name = op.input('Condition')[0]\n                    is_after_while_op = True\n                    continue\n                if not is_after_while_op:\n                    _create_program(src_block, strat_block, op, force_create=True)\n                else:\n                    _create_program(src_block, end_block, op, force_create=True)\n        elif ib == 1:\n            send_block = send_prog.block(0)\n            recv_block = recv_prog.block(0)\n            is_after_send_op = False\n            is_after_recv_op = False\n            for (i, op) in enumerate(src_block.ops):\n                if op.type == 'send_v2' and (not is_after_send_op):\n                    is_after_send_op = True\n                if is_after_send_op and (not is_after_recv_op) and (op.type == 'recv_v2'):\n                    is_after_recv_op = True\n                if not is_after_send_op or not is_after_recv_op:\n                    if self._cur_pp_stage == self._pp_stages - 1:\n                        if op.type == 'c_sync_calc_stream' and src_block.ops[i + 1].type == 'send_v2':\n                            continue\n                        if op.type == 'nop':\n                            continue\n                        if op.type not in ['recv_2', 'assign', 'c_allgather'] and op.has_attr('op_namescope') and ('/auto_parallel/reshard' in op.attr('op_namescope')):\n                            if len(op.desc.input_arg_names()) > 0 and '@RESHARD' not in op.desc.input_arg_names()[0]:\n                                send_vars_name.add(op.desc.input_arg_names()[0])\n                                if op.type == 'send_v2':\n                                    remove_process_group(op.attr('ring_id'))\n                                continue\n                            if op.type == 'send_v2':\n                                remove_process_group(op.attr('ring_id'))\n                                continue\n                    _create_program(src_block, send_block, op, force_create=True)\n                    continue\n                if is_after_send_op and is_after_recv_op:\n                    if op.has_attr('op_namescope') and '/auto_parallel/reshard' in op.attr('op_namescope'):\n                        if op.type in ['send_v2', 'recv_v2']:\n                            remove_process_group(op.attr('ring_id'))\n                        var_name = op.desc.output_arg_names()[0]\n                        index = var_name.find('@')\n                        if index > 0:\n                            old_var_name = var_name[:index]\n                        else:\n                            old_var_name = var_name\n                        recv_vars_name[var_name] = old_var_name\n                        if not src_block._find_var_recursive(old_var_name):\n                            src_var = src_block._var_recursive(var_name)\n                            recv_block.create_var(type=src_var.type, name=old_var_name, shape=src_var.shape, dtype=src_var.dtype, lod_level=src_var.lod_level, persistable=src_var.persistable, error_clip=src_var.error_clip, stop_gradient=src_var.stop_gradient, is_data=src_var.is_data, belong_to_optimizer=src_var.belong_to_optimizer)\n                        continue\n                    for in_name in op.desc.input_arg_names():\n                        if in_name in recv_vars_name:\n                            op.desc._rename_input(in_name, recv_vars_name[in_name])\n                    _create_program(src_block, recv_block, op, force_create=True)\n                    continue\n        else:\n            raise Exception('Only support generation condition.')\n    start_prog._sync_with_cpp()\n    end_prog._sync_with_cpp()\n    send_prog._sync_with_cpp()\n    recv_prog._sync_with_cpp()\n    assert cond_var_name is not None\n    send_task_node_var_dtype = {}\n    send_task_node_var_shape = {}\n    recv_task_node_var_dtype = {}\n    recv_task_node_var_shape = {}\n    for var_name in list(send_vars_name):\n        var = send_prog.global_block().vars[var_name]\n        dtype = str(var.dtype)\n        send_task_node_var_dtype[var_name] = dtype[dtype.find('paddle.') + len('paddle.'):]\n        send_task_node_var_shape[var_name] = var.shape\n    for var_name in list(set(recv_vars_name.values())):\n        var = recv_prog.global_block().vars[var_name]\n        dtype = str(var.dtype)\n        recv_task_node_var_dtype[var_name] = dtype[dtype.find('paddle.') + len('paddle.'):]\n        recv_task_node_var_shape[var_name] = var.shape\n    vars_to_dtype = []\n    vars_to_shape = []\n    if len(send_task_node_var_dtype) > 0:\n        assert len(recv_task_node_var_dtype) == 0\n        vars_to_dtype = send_task_node_var_dtype\n        vars_to_shape = send_task_node_var_shape\n    if len(recv_task_node_var_dtype) > 0:\n        assert len(send_task_node_var_dtype) == 0\n        vars_to_dtype = recv_task_node_var_dtype\n        vars_to_shape = recv_task_node_var_shape\n    start_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Start', task_id=int(self._cur_rank * num_of_functionality + 0), program=start_prog, lazy_initialize=True)\n    cond_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Cond', task_id=int(self._cur_rank * num_of_functionality + 1), program=cond_prog, cond_var_name=cond_var_name, lazy_initialize=True)\n    send_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 2), program=send_prog, lazy_initialize=True)\n    recv_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 3), program=recv_prog, lazy_initialize=True, vars_to_dtype=vars_to_dtype, vars_to_shape=vars_to_shape)\n    end_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 4), program=end_prog, lazy_initialize=True)\n    inf = -1\n    pp_buff_size = int(self._pp_stages - self._cur_pp_stage)\n    start_task_node.add_downstream_task(cond_task_node.task_id(), self._gen_bsz)\n    print('Task ', start_task_node.task_id(), \"'s downstream is:\", cond_task_node.task_id(), ', buffer size is:', self._gen_bsz)\n    cond_task_node.add_upstream_task(start_task_node.task_id(), self._gen_bsz)\n    print('Task ', cond_task_node.task_id(), \"'s upstream is:\", start_task_node.task_id(), ', buffer size is:', self._gen_bsz)\n    cond_task_node.add_downstream_task(send_task_node.task_id(), inf)\n    print('Task ', cond_task_node.task_id(), \"'s downstream is:\", send_task_node.task_id(), ', buffer size is:', inf)\n    send_task_node.add_upstream_task(cond_task_node.task_id(), inf)\n    print('Task ', send_task_node.task_id(), \"'s upstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    send_task_node.add_downstream_task(recv_task_node.task_id(), pp_buff_size)\n    print('Task ', send_task_node.task_id(), \"'s downstream is:\", recv_task_node.task_id(), ', buffer size is:', pp_buff_size)\n    recv_task_node.add_upstream_task(send_task_node.task_id(), pp_buff_size)\n    print('Task ', recv_task_node.task_id(), \"'s upstream is:\", send_task_node.task_id(), ', buffer size is:', pp_buff_size)\n    recv_task_node.add_downstream_task(cond_task_node.task_id(), inf, core.DependType.LOOP)\n    print('Task ', recv_task_node.task_id(), \"'s downstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    cond_task_node.add_upstream_task(recv_task_node.task_id(), inf, core.DependType.LOOP)\n    print('Task ', cond_task_node.task_id(), \"'s upstream is:\", recv_task_node.task_id(), ', buffer size is:', inf)\n    cond_task_node.add_downstream_task(end_task_node.task_id(), inf, core.DependType.STOP_LOOP)\n    print('Task ', cond_task_node.task_id(), \"'s downstream is:\", end_task_node.task_id(), ', buffer size is:', inf)\n    end_task_node.add_upstream_task(cond_task_node.task_id(), inf, core.DependType.STOP_LOOP)\n    print('Task ', end_task_node.task_id(), \"'s upstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    up_down_streams = self._dist_context.up_down_streams\n    pp_upstream_ranks = up_down_streams.ups(self._cur_rank)\n    pp_downstream_ranks = up_down_streams.downs(self._cur_rank)\n    for upstream_rank in pp_upstream_ranks:\n        upstream_pp_stage = self._get_pp_stage(upstream_rank)\n        if upstream_pp_stage < self._pp_stages - 1:\n            upstream_task_id = int(upstream_rank * num_of_functionality + 2)\n            send_task_node.add_upstream_task(upstream_task_id)\n            print('Task ', send_task_node.task_id(), \"'s upstream is:\", upstream_task_id, ', buffer size is:', 2)\n        else:\n            upstream_task_id = int(upstream_rank * num_of_functionality + 3)\n            recv_task_node.add_upstream_task(upstream_task_id)\n            print('Task ', recv_task_node.task_id(), \"'s upstream is:\", upstream_task_id, ', buffer size is:', 2)\n    for downstream_rank in pp_downstream_ranks:\n        if self._cur_pp_stage < self._pp_stages - 1:\n            downstream_task_id = int(downstream_rank * num_of_functionality + 2)\n            send_task_node.add_downstream_task(downstream_task_id)\n            print('Task ', send_task_node.task_id(), \"'s downstream is:\", downstream_task_id, ', buffer size is:', 2)\n        else:\n            downstream_task_id = int(downstream_rank * num_of_functionality + 3)\n            recv_task_node.add_downstream_task(downstream_task_id)\n            print('Task ', recv_task_node.task_id(), \"'s downstream is:\", downstream_task_id, ', buffer size is:', 2)\n    task_id_to_rank = {}\n    for i in range(self._nrank):\n        for j in range(num_of_functionality):\n            task_id_to_rank[int(i * num_of_functionality + j)] = i\n    self._program._pipeline_opt = {'fleet_opt': {'tasks': [start_task_node, cond_task_node, send_task_node, recv_task_node, end_task_node], 'task_id_to_rank': task_id_to_rank, 'num_micro_batches': self._acc_steps, 'inference_generation': True}}",
            "def _task_stream(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    num_of_functionality = 5\n    start_prog = Program()\n    cond_prog = Program()\n    end_prog = Program()\n    send_prog = Program()\n    recv_prog = Program()\n    cond_var_name = None\n    send_vars_name = set()\n    recv_vars_name = {}\n    for (ib, src_block) in enumerate(self._program.blocks):\n        if ib == 0:\n            strat_block = start_prog.block(0)\n            end_block = end_prog.block(0)\n            is_after_while_op = False\n            for op in src_block.ops:\n                if op.type == 'while':\n                    assert len(op.input('Condition')) == 1\n                    cond_var_name = op.input('Condition')[0]\n                    is_after_while_op = True\n                    continue\n                if not is_after_while_op:\n                    _create_program(src_block, strat_block, op, force_create=True)\n                else:\n                    _create_program(src_block, end_block, op, force_create=True)\n        elif ib == 1:\n            send_block = send_prog.block(0)\n            recv_block = recv_prog.block(0)\n            is_after_send_op = False\n            is_after_recv_op = False\n            for (i, op) in enumerate(src_block.ops):\n                if op.type == 'send_v2' and (not is_after_send_op):\n                    is_after_send_op = True\n                if is_after_send_op and (not is_after_recv_op) and (op.type == 'recv_v2'):\n                    is_after_recv_op = True\n                if not is_after_send_op or not is_after_recv_op:\n                    if self._cur_pp_stage == self._pp_stages - 1:\n                        if op.type == 'c_sync_calc_stream' and src_block.ops[i + 1].type == 'send_v2':\n                            continue\n                        if op.type == 'nop':\n                            continue\n                        if op.type not in ['recv_2', 'assign', 'c_allgather'] and op.has_attr('op_namescope') and ('/auto_parallel/reshard' in op.attr('op_namescope')):\n                            if len(op.desc.input_arg_names()) > 0 and '@RESHARD' not in op.desc.input_arg_names()[0]:\n                                send_vars_name.add(op.desc.input_arg_names()[0])\n                                if op.type == 'send_v2':\n                                    remove_process_group(op.attr('ring_id'))\n                                continue\n                            if op.type == 'send_v2':\n                                remove_process_group(op.attr('ring_id'))\n                                continue\n                    _create_program(src_block, send_block, op, force_create=True)\n                    continue\n                if is_after_send_op and is_after_recv_op:\n                    if op.has_attr('op_namescope') and '/auto_parallel/reshard' in op.attr('op_namescope'):\n                        if op.type in ['send_v2', 'recv_v2']:\n                            remove_process_group(op.attr('ring_id'))\n                        var_name = op.desc.output_arg_names()[0]\n                        index = var_name.find('@')\n                        if index > 0:\n                            old_var_name = var_name[:index]\n                        else:\n                            old_var_name = var_name\n                        recv_vars_name[var_name] = old_var_name\n                        if not src_block._find_var_recursive(old_var_name):\n                            src_var = src_block._var_recursive(var_name)\n                            recv_block.create_var(type=src_var.type, name=old_var_name, shape=src_var.shape, dtype=src_var.dtype, lod_level=src_var.lod_level, persistable=src_var.persistable, error_clip=src_var.error_clip, stop_gradient=src_var.stop_gradient, is_data=src_var.is_data, belong_to_optimizer=src_var.belong_to_optimizer)\n                        continue\n                    for in_name in op.desc.input_arg_names():\n                        if in_name in recv_vars_name:\n                            op.desc._rename_input(in_name, recv_vars_name[in_name])\n                    _create_program(src_block, recv_block, op, force_create=True)\n                    continue\n        else:\n            raise Exception('Only support generation condition.')\n    start_prog._sync_with_cpp()\n    end_prog._sync_with_cpp()\n    send_prog._sync_with_cpp()\n    recv_prog._sync_with_cpp()\n    assert cond_var_name is not None\n    send_task_node_var_dtype = {}\n    send_task_node_var_shape = {}\n    recv_task_node_var_dtype = {}\n    recv_task_node_var_shape = {}\n    for var_name in list(send_vars_name):\n        var = send_prog.global_block().vars[var_name]\n        dtype = str(var.dtype)\n        send_task_node_var_dtype[var_name] = dtype[dtype.find('paddle.') + len('paddle.'):]\n        send_task_node_var_shape[var_name] = var.shape\n    for var_name in list(set(recv_vars_name.values())):\n        var = recv_prog.global_block().vars[var_name]\n        dtype = str(var.dtype)\n        recv_task_node_var_dtype[var_name] = dtype[dtype.find('paddle.') + len('paddle.'):]\n        recv_task_node_var_shape[var_name] = var.shape\n    vars_to_dtype = []\n    vars_to_shape = []\n    if len(send_task_node_var_dtype) > 0:\n        assert len(recv_task_node_var_dtype) == 0\n        vars_to_dtype = send_task_node_var_dtype\n        vars_to_shape = send_task_node_var_shape\n    if len(recv_task_node_var_dtype) > 0:\n        assert len(send_task_node_var_dtype) == 0\n        vars_to_dtype = recv_task_node_var_dtype\n        vars_to_shape = recv_task_node_var_shape\n    start_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Start', task_id=int(self._cur_rank * num_of_functionality + 0), program=start_prog, lazy_initialize=True)\n    cond_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Cond', task_id=int(self._cur_rank * num_of_functionality + 1), program=cond_prog, cond_var_name=cond_var_name, lazy_initialize=True)\n    send_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 2), program=send_prog, lazy_initialize=True)\n    recv_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 3), program=recv_prog, lazy_initialize=True, vars_to_dtype=vars_to_dtype, vars_to_shape=vars_to_shape)\n    end_task_node = TaskNode(rank=self._cur_rank, max_run_times=self._acc_steps, node_type='Compute', task_id=int(self._cur_rank * num_of_functionality + 4), program=end_prog, lazy_initialize=True)\n    inf = -1\n    pp_buff_size = int(self._pp_stages - self._cur_pp_stage)\n    start_task_node.add_downstream_task(cond_task_node.task_id(), self._gen_bsz)\n    print('Task ', start_task_node.task_id(), \"'s downstream is:\", cond_task_node.task_id(), ', buffer size is:', self._gen_bsz)\n    cond_task_node.add_upstream_task(start_task_node.task_id(), self._gen_bsz)\n    print('Task ', cond_task_node.task_id(), \"'s upstream is:\", start_task_node.task_id(), ', buffer size is:', self._gen_bsz)\n    cond_task_node.add_downstream_task(send_task_node.task_id(), inf)\n    print('Task ', cond_task_node.task_id(), \"'s downstream is:\", send_task_node.task_id(), ', buffer size is:', inf)\n    send_task_node.add_upstream_task(cond_task_node.task_id(), inf)\n    print('Task ', send_task_node.task_id(), \"'s upstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    send_task_node.add_downstream_task(recv_task_node.task_id(), pp_buff_size)\n    print('Task ', send_task_node.task_id(), \"'s downstream is:\", recv_task_node.task_id(), ', buffer size is:', pp_buff_size)\n    recv_task_node.add_upstream_task(send_task_node.task_id(), pp_buff_size)\n    print('Task ', recv_task_node.task_id(), \"'s upstream is:\", send_task_node.task_id(), ', buffer size is:', pp_buff_size)\n    recv_task_node.add_downstream_task(cond_task_node.task_id(), inf, core.DependType.LOOP)\n    print('Task ', recv_task_node.task_id(), \"'s downstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    cond_task_node.add_upstream_task(recv_task_node.task_id(), inf, core.DependType.LOOP)\n    print('Task ', cond_task_node.task_id(), \"'s upstream is:\", recv_task_node.task_id(), ', buffer size is:', inf)\n    cond_task_node.add_downstream_task(end_task_node.task_id(), inf, core.DependType.STOP_LOOP)\n    print('Task ', cond_task_node.task_id(), \"'s downstream is:\", end_task_node.task_id(), ', buffer size is:', inf)\n    end_task_node.add_upstream_task(cond_task_node.task_id(), inf, core.DependType.STOP_LOOP)\n    print('Task ', end_task_node.task_id(), \"'s upstream is:\", cond_task_node.task_id(), ', buffer size is:', inf)\n    up_down_streams = self._dist_context.up_down_streams\n    pp_upstream_ranks = up_down_streams.ups(self._cur_rank)\n    pp_downstream_ranks = up_down_streams.downs(self._cur_rank)\n    for upstream_rank in pp_upstream_ranks:\n        upstream_pp_stage = self._get_pp_stage(upstream_rank)\n        if upstream_pp_stage < self._pp_stages - 1:\n            upstream_task_id = int(upstream_rank * num_of_functionality + 2)\n            send_task_node.add_upstream_task(upstream_task_id)\n            print('Task ', send_task_node.task_id(), \"'s upstream is:\", upstream_task_id, ', buffer size is:', 2)\n        else:\n            upstream_task_id = int(upstream_rank * num_of_functionality + 3)\n            recv_task_node.add_upstream_task(upstream_task_id)\n            print('Task ', recv_task_node.task_id(), \"'s upstream is:\", upstream_task_id, ', buffer size is:', 2)\n    for downstream_rank in pp_downstream_ranks:\n        if self._cur_pp_stage < self._pp_stages - 1:\n            downstream_task_id = int(downstream_rank * num_of_functionality + 2)\n            send_task_node.add_downstream_task(downstream_task_id)\n            print('Task ', send_task_node.task_id(), \"'s downstream is:\", downstream_task_id, ', buffer size is:', 2)\n        else:\n            downstream_task_id = int(downstream_rank * num_of_functionality + 3)\n            recv_task_node.add_downstream_task(downstream_task_id)\n            print('Task ', recv_task_node.task_id(), \"'s downstream is:\", downstream_task_id, ', buffer size is:', 2)\n    task_id_to_rank = {}\n    for i in range(self._nrank):\n        for j in range(num_of_functionality):\n            task_id_to_rank[int(i * num_of_functionality + j)] = i\n    self._program._pipeline_opt = {'fleet_opt': {'tasks': [start_task_node, cond_task_node, send_task_node, recv_task_node, end_task_node], 'task_id_to_rank': task_id_to_rank, 'num_micro_batches': self._acc_steps, 'inference_generation': True}}"
        ]
    }
]