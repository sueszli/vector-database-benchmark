[
    {
        "func_name": "get_stale_status_resolver",
        "original": "def get_stale_status_resolver(instance: DagsterInstance, assets: Sequence[Union[AssetsDefinition, SourceAsset]]) -> CachingStaleStatusResolver:\n    return CachingStaleStatusResolver(instance=instance, asset_graph=AssetGraph.from_assets(assets))",
        "mutated": [
            "def get_stale_status_resolver(instance: DagsterInstance, assets: Sequence[Union[AssetsDefinition, SourceAsset]]) -> CachingStaleStatusResolver:\n    if False:\n        i = 10\n    return CachingStaleStatusResolver(instance=instance, asset_graph=AssetGraph.from_assets(assets))",
            "def get_stale_status_resolver(instance: DagsterInstance, assets: Sequence[Union[AssetsDefinition, SourceAsset]]) -> CachingStaleStatusResolver:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return CachingStaleStatusResolver(instance=instance, asset_graph=AssetGraph.from_assets(assets))",
            "def get_stale_status_resolver(instance: DagsterInstance, assets: Sequence[Union[AssetsDefinition, SourceAsset]]) -> CachingStaleStatusResolver:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return CachingStaleStatusResolver(instance=instance, asset_graph=AssetGraph.from_assets(assets))",
            "def get_stale_status_resolver(instance: DagsterInstance, assets: Sequence[Union[AssetsDefinition, SourceAsset]]) -> CachingStaleStatusResolver:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return CachingStaleStatusResolver(instance=instance, asset_graph=AssetGraph.from_assets(assets))",
            "def get_stale_status_resolver(instance: DagsterInstance, assets: Sequence[Union[AssetsDefinition, SourceAsset]]) -> CachingStaleStatusResolver:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return CachingStaleStatusResolver(instance=instance, asset_graph=AssetGraph.from_assets(assets))"
        ]
    },
    {
        "func_name": "root",
        "original": "@asset(partitions_def=partitions_def)\ndef root(context):\n    keys = partitions_def.get_partition_keys_in_range(context.partition_key_range)\n    return {key: randint(0, 100) for key in keys}",
        "mutated": [
            "@asset(partitions_def=partitions_def)\ndef root(context):\n    if False:\n        i = 10\n    keys = partitions_def.get_partition_keys_in_range(context.partition_key_range)\n    return {key: randint(0, 100) for key in keys}",
            "@asset(partitions_def=partitions_def)\ndef root(context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    keys = partitions_def.get_partition_keys_in_range(context.partition_key_range)\n    return {key: randint(0, 100) for key in keys}",
            "@asset(partitions_def=partitions_def)\ndef root(context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    keys = partitions_def.get_partition_keys_in_range(context.partition_key_range)\n    return {key: randint(0, 100) for key in keys}",
            "@asset(partitions_def=partitions_def)\ndef root(context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    keys = partitions_def.get_partition_keys_in_range(context.partition_key_range)\n    return {key: randint(0, 100) for key in keys}",
            "@asset(partitions_def=partitions_def)\ndef root(context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    keys = partitions_def.get_partition_keys_in_range(context.partition_key_range)\n    return {key: randint(0, 100) for key in keys}"
        ]
    },
    {
        "func_name": "downstream1",
        "original": "@asset\ndef downstream1(root):\n    ...",
        "mutated": [
            "@asset\ndef downstream1(root):\n    if False:\n        i = 10\n    ...",
            "@asset\ndef downstream1(root):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ...",
            "@asset\ndef downstream1(root):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ...",
            "@asset\ndef downstream1(root):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ...",
            "@asset\ndef downstream1(root):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ..."
        ]
    },
    {
        "func_name": "downstream2",
        "original": "@asset\ndef downstream2(downstream1):\n    ...",
        "mutated": [
            "@asset\ndef downstream2(downstream1):\n    if False:\n        i = 10\n    ...",
            "@asset\ndef downstream2(downstream1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ...",
            "@asset\ndef downstream2(downstream1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ...",
            "@asset\ndef downstream2(downstream1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ...",
            "@asset\ndef downstream2(downstream1):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ..."
        ]
    },
    {
        "func_name": "main",
        "original": "def main(num_partitions: int, override_partition_limit: bool) -> None:\n    if override_partition_limit:\n        override_value = num_partitions + 1\n        print(f'Setting partition limit at {override_value}')\n        import dagster._core.definitions.data_version\n        import dagster._core.execution.context.system\n        for module in [dagster._core.definitions.data_version, dagster._core.execution.context.system]:\n            setattr(module, 'SKIP_PARTITION_DATA_VERSION_DEPENDENCY_THRESHOLD', override_value)\n        partition_limit = override_value\n    else:\n        partition_limit = SKIP_PARTITION_DATA_VERSION_DEPENDENCY_THRESHOLD\n    partitions_def = StaticPartitionsDefinition([str(x) for x in range(num_partitions)])\n\n    @asset(partitions_def=partitions_def)\n    def root(context):\n        keys = partitions_def.get_partition_keys_in_range(context.partition_key_range)\n        return {key: randint(0, 100) for key in keys}\n\n    @asset\n    def downstream1(root):\n        ...\n\n    @asset\n    def downstream2(downstream1):\n        ...\n    all_assets = [root, downstream1, downstream2]\n    with instance_for_test() as instance:\n        session = ProfilingSession(name='Partition stale status', experiment_settings={'num_partitions': num_partitions, 'override_partition_limit': override_partition_limit}).start()\n        session.log_start_message()\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.MISSING\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.MISSING\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.MISSING\n        with session.logged_execution_time(f'Materialize all {num_partitions} partitions of `root`'):\n            materialize_asset(all_assets, root, instance, tags={ASSET_PARTITION_RANGE_START_TAG: '0', ASSET_PARTITION_RANGE_END_TAG: str(num_partitions - 1)})\n        with session.logged_execution_time('Materialize `downstream1`'):\n            materialize_asset(all_assets, downstream1, instance)\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.MISSING\n        with session.logged_execution_time('Materialize `downstream2`'):\n            materialize_asset(all_assets, downstream2, instance)\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.FRESH\n        with session.logged_execution_time('Materialize single partition of `root`'):\n            materialize_asset(all_assets, root, instance, partition_key='0')\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            target_status = StaleStatus.FRESH if num_partitions >= partition_limit else StaleStatus.STALE\n            assert status_resolver.get_status(downstream1.key) == target_status\n            assert status_resolver.get_status(downstream2.key) == target_status\n        session.log_result_summary()",
        "mutated": [
            "def main(num_partitions: int, override_partition_limit: bool) -> None:\n    if False:\n        i = 10\n    if override_partition_limit:\n        override_value = num_partitions + 1\n        print(f'Setting partition limit at {override_value}')\n        import dagster._core.definitions.data_version\n        import dagster._core.execution.context.system\n        for module in [dagster._core.definitions.data_version, dagster._core.execution.context.system]:\n            setattr(module, 'SKIP_PARTITION_DATA_VERSION_DEPENDENCY_THRESHOLD', override_value)\n        partition_limit = override_value\n    else:\n        partition_limit = SKIP_PARTITION_DATA_VERSION_DEPENDENCY_THRESHOLD\n    partitions_def = StaticPartitionsDefinition([str(x) for x in range(num_partitions)])\n\n    @asset(partitions_def=partitions_def)\n    def root(context):\n        keys = partitions_def.get_partition_keys_in_range(context.partition_key_range)\n        return {key: randint(0, 100) for key in keys}\n\n    @asset\n    def downstream1(root):\n        ...\n\n    @asset\n    def downstream2(downstream1):\n        ...\n    all_assets = [root, downstream1, downstream2]\n    with instance_for_test() as instance:\n        session = ProfilingSession(name='Partition stale status', experiment_settings={'num_partitions': num_partitions, 'override_partition_limit': override_partition_limit}).start()\n        session.log_start_message()\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.MISSING\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.MISSING\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.MISSING\n        with session.logged_execution_time(f'Materialize all {num_partitions} partitions of `root`'):\n            materialize_asset(all_assets, root, instance, tags={ASSET_PARTITION_RANGE_START_TAG: '0', ASSET_PARTITION_RANGE_END_TAG: str(num_partitions - 1)})\n        with session.logged_execution_time('Materialize `downstream1`'):\n            materialize_asset(all_assets, downstream1, instance)\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.MISSING\n        with session.logged_execution_time('Materialize `downstream2`'):\n            materialize_asset(all_assets, downstream2, instance)\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.FRESH\n        with session.logged_execution_time('Materialize single partition of `root`'):\n            materialize_asset(all_assets, root, instance, partition_key='0')\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            target_status = StaleStatus.FRESH if num_partitions >= partition_limit else StaleStatus.STALE\n            assert status_resolver.get_status(downstream1.key) == target_status\n            assert status_resolver.get_status(downstream2.key) == target_status\n        session.log_result_summary()",
            "def main(num_partitions: int, override_partition_limit: bool) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if override_partition_limit:\n        override_value = num_partitions + 1\n        print(f'Setting partition limit at {override_value}')\n        import dagster._core.definitions.data_version\n        import dagster._core.execution.context.system\n        for module in [dagster._core.definitions.data_version, dagster._core.execution.context.system]:\n            setattr(module, 'SKIP_PARTITION_DATA_VERSION_DEPENDENCY_THRESHOLD', override_value)\n        partition_limit = override_value\n    else:\n        partition_limit = SKIP_PARTITION_DATA_VERSION_DEPENDENCY_THRESHOLD\n    partitions_def = StaticPartitionsDefinition([str(x) for x in range(num_partitions)])\n\n    @asset(partitions_def=partitions_def)\n    def root(context):\n        keys = partitions_def.get_partition_keys_in_range(context.partition_key_range)\n        return {key: randint(0, 100) for key in keys}\n\n    @asset\n    def downstream1(root):\n        ...\n\n    @asset\n    def downstream2(downstream1):\n        ...\n    all_assets = [root, downstream1, downstream2]\n    with instance_for_test() as instance:\n        session = ProfilingSession(name='Partition stale status', experiment_settings={'num_partitions': num_partitions, 'override_partition_limit': override_partition_limit}).start()\n        session.log_start_message()\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.MISSING\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.MISSING\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.MISSING\n        with session.logged_execution_time(f'Materialize all {num_partitions} partitions of `root`'):\n            materialize_asset(all_assets, root, instance, tags={ASSET_PARTITION_RANGE_START_TAG: '0', ASSET_PARTITION_RANGE_END_TAG: str(num_partitions - 1)})\n        with session.logged_execution_time('Materialize `downstream1`'):\n            materialize_asset(all_assets, downstream1, instance)\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.MISSING\n        with session.logged_execution_time('Materialize `downstream2`'):\n            materialize_asset(all_assets, downstream2, instance)\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.FRESH\n        with session.logged_execution_time('Materialize single partition of `root`'):\n            materialize_asset(all_assets, root, instance, partition_key='0')\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            target_status = StaleStatus.FRESH if num_partitions >= partition_limit else StaleStatus.STALE\n            assert status_resolver.get_status(downstream1.key) == target_status\n            assert status_resolver.get_status(downstream2.key) == target_status\n        session.log_result_summary()",
            "def main(num_partitions: int, override_partition_limit: bool) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if override_partition_limit:\n        override_value = num_partitions + 1\n        print(f'Setting partition limit at {override_value}')\n        import dagster._core.definitions.data_version\n        import dagster._core.execution.context.system\n        for module in [dagster._core.definitions.data_version, dagster._core.execution.context.system]:\n            setattr(module, 'SKIP_PARTITION_DATA_VERSION_DEPENDENCY_THRESHOLD', override_value)\n        partition_limit = override_value\n    else:\n        partition_limit = SKIP_PARTITION_DATA_VERSION_DEPENDENCY_THRESHOLD\n    partitions_def = StaticPartitionsDefinition([str(x) for x in range(num_partitions)])\n\n    @asset(partitions_def=partitions_def)\n    def root(context):\n        keys = partitions_def.get_partition_keys_in_range(context.partition_key_range)\n        return {key: randint(0, 100) for key in keys}\n\n    @asset\n    def downstream1(root):\n        ...\n\n    @asset\n    def downstream2(downstream1):\n        ...\n    all_assets = [root, downstream1, downstream2]\n    with instance_for_test() as instance:\n        session = ProfilingSession(name='Partition stale status', experiment_settings={'num_partitions': num_partitions, 'override_partition_limit': override_partition_limit}).start()\n        session.log_start_message()\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.MISSING\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.MISSING\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.MISSING\n        with session.logged_execution_time(f'Materialize all {num_partitions} partitions of `root`'):\n            materialize_asset(all_assets, root, instance, tags={ASSET_PARTITION_RANGE_START_TAG: '0', ASSET_PARTITION_RANGE_END_TAG: str(num_partitions - 1)})\n        with session.logged_execution_time('Materialize `downstream1`'):\n            materialize_asset(all_assets, downstream1, instance)\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.MISSING\n        with session.logged_execution_time('Materialize `downstream2`'):\n            materialize_asset(all_assets, downstream2, instance)\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.FRESH\n        with session.logged_execution_time('Materialize single partition of `root`'):\n            materialize_asset(all_assets, root, instance, partition_key='0')\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            target_status = StaleStatus.FRESH if num_partitions >= partition_limit else StaleStatus.STALE\n            assert status_resolver.get_status(downstream1.key) == target_status\n            assert status_resolver.get_status(downstream2.key) == target_status\n        session.log_result_summary()",
            "def main(num_partitions: int, override_partition_limit: bool) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if override_partition_limit:\n        override_value = num_partitions + 1\n        print(f'Setting partition limit at {override_value}')\n        import dagster._core.definitions.data_version\n        import dagster._core.execution.context.system\n        for module in [dagster._core.definitions.data_version, dagster._core.execution.context.system]:\n            setattr(module, 'SKIP_PARTITION_DATA_VERSION_DEPENDENCY_THRESHOLD', override_value)\n        partition_limit = override_value\n    else:\n        partition_limit = SKIP_PARTITION_DATA_VERSION_DEPENDENCY_THRESHOLD\n    partitions_def = StaticPartitionsDefinition([str(x) for x in range(num_partitions)])\n\n    @asset(partitions_def=partitions_def)\n    def root(context):\n        keys = partitions_def.get_partition_keys_in_range(context.partition_key_range)\n        return {key: randint(0, 100) for key in keys}\n\n    @asset\n    def downstream1(root):\n        ...\n\n    @asset\n    def downstream2(downstream1):\n        ...\n    all_assets = [root, downstream1, downstream2]\n    with instance_for_test() as instance:\n        session = ProfilingSession(name='Partition stale status', experiment_settings={'num_partitions': num_partitions, 'override_partition_limit': override_partition_limit}).start()\n        session.log_start_message()\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.MISSING\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.MISSING\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.MISSING\n        with session.logged_execution_time(f'Materialize all {num_partitions} partitions of `root`'):\n            materialize_asset(all_assets, root, instance, tags={ASSET_PARTITION_RANGE_START_TAG: '0', ASSET_PARTITION_RANGE_END_TAG: str(num_partitions - 1)})\n        with session.logged_execution_time('Materialize `downstream1`'):\n            materialize_asset(all_assets, downstream1, instance)\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.MISSING\n        with session.logged_execution_time('Materialize `downstream2`'):\n            materialize_asset(all_assets, downstream2, instance)\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.FRESH\n        with session.logged_execution_time('Materialize single partition of `root`'):\n            materialize_asset(all_assets, root, instance, partition_key='0')\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            target_status = StaleStatus.FRESH if num_partitions >= partition_limit else StaleStatus.STALE\n            assert status_resolver.get_status(downstream1.key) == target_status\n            assert status_resolver.get_status(downstream2.key) == target_status\n        session.log_result_summary()",
            "def main(num_partitions: int, override_partition_limit: bool) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if override_partition_limit:\n        override_value = num_partitions + 1\n        print(f'Setting partition limit at {override_value}')\n        import dagster._core.definitions.data_version\n        import dagster._core.execution.context.system\n        for module in [dagster._core.definitions.data_version, dagster._core.execution.context.system]:\n            setattr(module, 'SKIP_PARTITION_DATA_VERSION_DEPENDENCY_THRESHOLD', override_value)\n        partition_limit = override_value\n    else:\n        partition_limit = SKIP_PARTITION_DATA_VERSION_DEPENDENCY_THRESHOLD\n    partitions_def = StaticPartitionsDefinition([str(x) for x in range(num_partitions)])\n\n    @asset(partitions_def=partitions_def)\n    def root(context):\n        keys = partitions_def.get_partition_keys_in_range(context.partition_key_range)\n        return {key: randint(0, 100) for key in keys}\n\n    @asset\n    def downstream1(root):\n        ...\n\n    @asset\n    def downstream2(downstream1):\n        ...\n    all_assets = [root, downstream1, downstream2]\n    with instance_for_test() as instance:\n        session = ProfilingSession(name='Partition stale status', experiment_settings={'num_partitions': num_partitions, 'override_partition_limit': override_partition_limit}).start()\n        session.log_start_message()\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.MISSING\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.MISSING\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.MISSING\n        with session.logged_execution_time(f'Materialize all {num_partitions} partitions of `root`'):\n            materialize_asset(all_assets, root, instance, tags={ASSET_PARTITION_RANGE_START_TAG: '0', ASSET_PARTITION_RANGE_END_TAG: str(num_partitions - 1)})\n        with session.logged_execution_time('Materialize `downstream1`'):\n            materialize_asset(all_assets, downstream1, instance)\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.MISSING\n        with session.logged_execution_time('Materialize `downstream2`'):\n            materialize_asset(all_assets, downstream2, instance)\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream1.key) == StaleStatus.FRESH\n            assert status_resolver.get_status(downstream2.key) == StaleStatus.FRESH\n        with session.logged_execution_time('Materialize single partition of `root`'):\n            materialize_asset(all_assets, root, instance, partition_key='0')\n        with session.logged_execution_time('Resolve StaleStatus of all assets'):\n            status_resolver = get_stale_status_resolver(instance, all_assets)\n            for partition_key in partitions_def.get_partition_keys():\n                assert status_resolver.get_status(root.key, partition_key) == StaleStatus.FRESH\n            target_status = StaleStatus.FRESH if num_partitions >= partition_limit else StaleStatus.STALE\n            assert status_resolver.get_status(downstream1.key) == target_status\n            assert status_resolver.get_status(downstream2.key) == target_status\n        session.log_result_summary()"
        ]
    }
]