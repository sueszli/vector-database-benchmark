[
    {
        "func_name": "collect_episodic_demo_data_for_trex",
        "original": "def collect_episodic_demo_data_for_trex(input_cfg: Union[str, dict], seed: int, collect_count: int, rank: int, env_setting: Optional[List[Any]]=None, model: Optional[torch.nn.Module]=None, state_dict: Optional[dict]=None, state_dict_path: Optional[str]=None):\n    \"\"\"\n    Overview:\n        Collect episodic demonstration data by the trained policy for trex specifically.\n    Arguments:\n        - input_cfg (:obj:`Union[str, Tuple[dict, dict]]`): Config in dict type.             ``str`` type means config file path.             ``Tuple[dict, dict]`` type means [user_config, create_cfg].\n        - seed (:obj:`int`): Random seed.\n        - collect_count (:obj:`int`): The count of collected data.\n        - rank (:obj:`int`): The episode ranking.\n        - env_setting (:obj:`Optional[List[Any]]`): A list with 3 elements:             ``BaseEnv`` subclass, collector env config, and evaluator env config.\n        - model (:obj:`Optional[torch.nn.Module]`): Instance of torch.nn.Module.\n        - state_dict (:obj:`Optional[dict]`): The state_dict of policy or model.\n        - state_dict_path (:obj:'str') The abs path of the state dict.\n    \"\"\"\n    if isinstance(input_cfg, str):\n        (cfg, create_cfg) = read_config(input_cfg)\n    else:\n        (cfg, create_cfg) = deepcopy(input_cfg)\n    create_cfg.policy.type += '_command'\n    env_fn = None if env_setting is None else env_setting[0]\n    cfg.env.collector_env_num = 1\n    cfg = compile_config(cfg, collector=EpisodeSerialCollector, seed=seed, env=env_fn, auto=True, create_cfg=create_cfg, save_cfg=True, save_path='collect_demo_data_config.py')\n    if env_setting is None:\n        (env_fn, collector_env_cfg, _) = get_vec_env_setting(cfg.env)\n    else:\n        (env_fn, collector_env_cfg, _) = env_setting\n    collector_env = create_env_manager(cfg.env.manager, [partial(env_fn, cfg=c) for c in collector_env_cfg])\n    collector_env.seed(seed)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    policy = create_policy(cfg.policy, model=model, enable_field=['collect', 'eval'])\n    collect_demo_policy = policy.collect_mode\n    if state_dict is None:\n        assert state_dict_path is not None\n        state_dict = torch.load(state_dict_path, map_location='cpu')\n    policy.collect_mode.load_state_dict(state_dict)\n    collector = EpisodeSerialCollector(cfg.policy.collect.collector, collector_env, collect_demo_policy, exp_name=cfg.exp_name)\n    policy_kwargs = None if not hasattr(cfg.policy.other, 'eps') else {'eps': cfg.policy.other.eps.get('collect', 0.2)}\n    exp_data = collector.collect(n_episode=collect_count, policy_kwargs=policy_kwargs)\n    if cfg.policy.cuda:\n        exp_data = to_device(exp_data, 'cpu')\n    print('Collect {}th episodic demo data successfully'.format(rank))\n    return exp_data",
        "mutated": [
            "def collect_episodic_demo_data_for_trex(input_cfg: Union[str, dict], seed: int, collect_count: int, rank: int, env_setting: Optional[List[Any]]=None, model: Optional[torch.nn.Module]=None, state_dict: Optional[dict]=None, state_dict_path: Optional[str]=None):\n    if False:\n        i = 10\n    \"\\n    Overview:\\n        Collect episodic demonstration data by the trained policy for trex specifically.\\n    Arguments:\\n        - input_cfg (:obj:`Union[str, Tuple[dict, dict]]`): Config in dict type.             ``str`` type means config file path.             ``Tuple[dict, dict]`` type means [user_config, create_cfg].\\n        - seed (:obj:`int`): Random seed.\\n        - collect_count (:obj:`int`): The count of collected data.\\n        - rank (:obj:`int`): The episode ranking.\\n        - env_setting (:obj:`Optional[List[Any]]`): A list with 3 elements:             ``BaseEnv`` subclass, collector env config, and evaluator env config.\\n        - model (:obj:`Optional[torch.nn.Module]`): Instance of torch.nn.Module.\\n        - state_dict (:obj:`Optional[dict]`): The state_dict of policy or model.\\n        - state_dict_path (:obj:'str') The abs path of the state dict.\\n    \"\n    if isinstance(input_cfg, str):\n        (cfg, create_cfg) = read_config(input_cfg)\n    else:\n        (cfg, create_cfg) = deepcopy(input_cfg)\n    create_cfg.policy.type += '_command'\n    env_fn = None if env_setting is None else env_setting[0]\n    cfg.env.collector_env_num = 1\n    cfg = compile_config(cfg, collector=EpisodeSerialCollector, seed=seed, env=env_fn, auto=True, create_cfg=create_cfg, save_cfg=True, save_path='collect_demo_data_config.py')\n    if env_setting is None:\n        (env_fn, collector_env_cfg, _) = get_vec_env_setting(cfg.env)\n    else:\n        (env_fn, collector_env_cfg, _) = env_setting\n    collector_env = create_env_manager(cfg.env.manager, [partial(env_fn, cfg=c) for c in collector_env_cfg])\n    collector_env.seed(seed)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    policy = create_policy(cfg.policy, model=model, enable_field=['collect', 'eval'])\n    collect_demo_policy = policy.collect_mode\n    if state_dict is None:\n        assert state_dict_path is not None\n        state_dict = torch.load(state_dict_path, map_location='cpu')\n    policy.collect_mode.load_state_dict(state_dict)\n    collector = EpisodeSerialCollector(cfg.policy.collect.collector, collector_env, collect_demo_policy, exp_name=cfg.exp_name)\n    policy_kwargs = None if not hasattr(cfg.policy.other, 'eps') else {'eps': cfg.policy.other.eps.get('collect', 0.2)}\n    exp_data = collector.collect(n_episode=collect_count, policy_kwargs=policy_kwargs)\n    if cfg.policy.cuda:\n        exp_data = to_device(exp_data, 'cpu')\n    print('Collect {}th episodic demo data successfully'.format(rank))\n    return exp_data",
            "def collect_episodic_demo_data_for_trex(input_cfg: Union[str, dict], seed: int, collect_count: int, rank: int, env_setting: Optional[List[Any]]=None, model: Optional[torch.nn.Module]=None, state_dict: Optional[dict]=None, state_dict_path: Optional[str]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Overview:\\n        Collect episodic demonstration data by the trained policy for trex specifically.\\n    Arguments:\\n        - input_cfg (:obj:`Union[str, Tuple[dict, dict]]`): Config in dict type.             ``str`` type means config file path.             ``Tuple[dict, dict]`` type means [user_config, create_cfg].\\n        - seed (:obj:`int`): Random seed.\\n        - collect_count (:obj:`int`): The count of collected data.\\n        - rank (:obj:`int`): The episode ranking.\\n        - env_setting (:obj:`Optional[List[Any]]`): A list with 3 elements:             ``BaseEnv`` subclass, collector env config, and evaluator env config.\\n        - model (:obj:`Optional[torch.nn.Module]`): Instance of torch.nn.Module.\\n        - state_dict (:obj:`Optional[dict]`): The state_dict of policy or model.\\n        - state_dict_path (:obj:'str') The abs path of the state dict.\\n    \"\n    if isinstance(input_cfg, str):\n        (cfg, create_cfg) = read_config(input_cfg)\n    else:\n        (cfg, create_cfg) = deepcopy(input_cfg)\n    create_cfg.policy.type += '_command'\n    env_fn = None if env_setting is None else env_setting[0]\n    cfg.env.collector_env_num = 1\n    cfg = compile_config(cfg, collector=EpisodeSerialCollector, seed=seed, env=env_fn, auto=True, create_cfg=create_cfg, save_cfg=True, save_path='collect_demo_data_config.py')\n    if env_setting is None:\n        (env_fn, collector_env_cfg, _) = get_vec_env_setting(cfg.env)\n    else:\n        (env_fn, collector_env_cfg, _) = env_setting\n    collector_env = create_env_manager(cfg.env.manager, [partial(env_fn, cfg=c) for c in collector_env_cfg])\n    collector_env.seed(seed)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    policy = create_policy(cfg.policy, model=model, enable_field=['collect', 'eval'])\n    collect_demo_policy = policy.collect_mode\n    if state_dict is None:\n        assert state_dict_path is not None\n        state_dict = torch.load(state_dict_path, map_location='cpu')\n    policy.collect_mode.load_state_dict(state_dict)\n    collector = EpisodeSerialCollector(cfg.policy.collect.collector, collector_env, collect_demo_policy, exp_name=cfg.exp_name)\n    policy_kwargs = None if not hasattr(cfg.policy.other, 'eps') else {'eps': cfg.policy.other.eps.get('collect', 0.2)}\n    exp_data = collector.collect(n_episode=collect_count, policy_kwargs=policy_kwargs)\n    if cfg.policy.cuda:\n        exp_data = to_device(exp_data, 'cpu')\n    print('Collect {}th episodic demo data successfully'.format(rank))\n    return exp_data",
            "def collect_episodic_demo_data_for_trex(input_cfg: Union[str, dict], seed: int, collect_count: int, rank: int, env_setting: Optional[List[Any]]=None, model: Optional[torch.nn.Module]=None, state_dict: Optional[dict]=None, state_dict_path: Optional[str]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Overview:\\n        Collect episodic demonstration data by the trained policy for trex specifically.\\n    Arguments:\\n        - input_cfg (:obj:`Union[str, Tuple[dict, dict]]`): Config in dict type.             ``str`` type means config file path.             ``Tuple[dict, dict]`` type means [user_config, create_cfg].\\n        - seed (:obj:`int`): Random seed.\\n        - collect_count (:obj:`int`): The count of collected data.\\n        - rank (:obj:`int`): The episode ranking.\\n        - env_setting (:obj:`Optional[List[Any]]`): A list with 3 elements:             ``BaseEnv`` subclass, collector env config, and evaluator env config.\\n        - model (:obj:`Optional[torch.nn.Module]`): Instance of torch.nn.Module.\\n        - state_dict (:obj:`Optional[dict]`): The state_dict of policy or model.\\n        - state_dict_path (:obj:'str') The abs path of the state dict.\\n    \"\n    if isinstance(input_cfg, str):\n        (cfg, create_cfg) = read_config(input_cfg)\n    else:\n        (cfg, create_cfg) = deepcopy(input_cfg)\n    create_cfg.policy.type += '_command'\n    env_fn = None if env_setting is None else env_setting[0]\n    cfg.env.collector_env_num = 1\n    cfg = compile_config(cfg, collector=EpisodeSerialCollector, seed=seed, env=env_fn, auto=True, create_cfg=create_cfg, save_cfg=True, save_path='collect_demo_data_config.py')\n    if env_setting is None:\n        (env_fn, collector_env_cfg, _) = get_vec_env_setting(cfg.env)\n    else:\n        (env_fn, collector_env_cfg, _) = env_setting\n    collector_env = create_env_manager(cfg.env.manager, [partial(env_fn, cfg=c) for c in collector_env_cfg])\n    collector_env.seed(seed)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    policy = create_policy(cfg.policy, model=model, enable_field=['collect', 'eval'])\n    collect_demo_policy = policy.collect_mode\n    if state_dict is None:\n        assert state_dict_path is not None\n        state_dict = torch.load(state_dict_path, map_location='cpu')\n    policy.collect_mode.load_state_dict(state_dict)\n    collector = EpisodeSerialCollector(cfg.policy.collect.collector, collector_env, collect_demo_policy, exp_name=cfg.exp_name)\n    policy_kwargs = None if not hasattr(cfg.policy.other, 'eps') else {'eps': cfg.policy.other.eps.get('collect', 0.2)}\n    exp_data = collector.collect(n_episode=collect_count, policy_kwargs=policy_kwargs)\n    if cfg.policy.cuda:\n        exp_data = to_device(exp_data, 'cpu')\n    print('Collect {}th episodic demo data successfully'.format(rank))\n    return exp_data",
            "def collect_episodic_demo_data_for_trex(input_cfg: Union[str, dict], seed: int, collect_count: int, rank: int, env_setting: Optional[List[Any]]=None, model: Optional[torch.nn.Module]=None, state_dict: Optional[dict]=None, state_dict_path: Optional[str]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Overview:\\n        Collect episodic demonstration data by the trained policy for trex specifically.\\n    Arguments:\\n        - input_cfg (:obj:`Union[str, Tuple[dict, dict]]`): Config in dict type.             ``str`` type means config file path.             ``Tuple[dict, dict]`` type means [user_config, create_cfg].\\n        - seed (:obj:`int`): Random seed.\\n        - collect_count (:obj:`int`): The count of collected data.\\n        - rank (:obj:`int`): The episode ranking.\\n        - env_setting (:obj:`Optional[List[Any]]`): A list with 3 elements:             ``BaseEnv`` subclass, collector env config, and evaluator env config.\\n        - model (:obj:`Optional[torch.nn.Module]`): Instance of torch.nn.Module.\\n        - state_dict (:obj:`Optional[dict]`): The state_dict of policy or model.\\n        - state_dict_path (:obj:'str') The abs path of the state dict.\\n    \"\n    if isinstance(input_cfg, str):\n        (cfg, create_cfg) = read_config(input_cfg)\n    else:\n        (cfg, create_cfg) = deepcopy(input_cfg)\n    create_cfg.policy.type += '_command'\n    env_fn = None if env_setting is None else env_setting[0]\n    cfg.env.collector_env_num = 1\n    cfg = compile_config(cfg, collector=EpisodeSerialCollector, seed=seed, env=env_fn, auto=True, create_cfg=create_cfg, save_cfg=True, save_path='collect_demo_data_config.py')\n    if env_setting is None:\n        (env_fn, collector_env_cfg, _) = get_vec_env_setting(cfg.env)\n    else:\n        (env_fn, collector_env_cfg, _) = env_setting\n    collector_env = create_env_manager(cfg.env.manager, [partial(env_fn, cfg=c) for c in collector_env_cfg])\n    collector_env.seed(seed)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    policy = create_policy(cfg.policy, model=model, enable_field=['collect', 'eval'])\n    collect_demo_policy = policy.collect_mode\n    if state_dict is None:\n        assert state_dict_path is not None\n        state_dict = torch.load(state_dict_path, map_location='cpu')\n    policy.collect_mode.load_state_dict(state_dict)\n    collector = EpisodeSerialCollector(cfg.policy.collect.collector, collector_env, collect_demo_policy, exp_name=cfg.exp_name)\n    policy_kwargs = None if not hasattr(cfg.policy.other, 'eps') else {'eps': cfg.policy.other.eps.get('collect', 0.2)}\n    exp_data = collector.collect(n_episode=collect_count, policy_kwargs=policy_kwargs)\n    if cfg.policy.cuda:\n        exp_data = to_device(exp_data, 'cpu')\n    print('Collect {}th episodic demo data successfully'.format(rank))\n    return exp_data",
            "def collect_episodic_demo_data_for_trex(input_cfg: Union[str, dict], seed: int, collect_count: int, rank: int, env_setting: Optional[List[Any]]=None, model: Optional[torch.nn.Module]=None, state_dict: Optional[dict]=None, state_dict_path: Optional[str]=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Overview:\\n        Collect episodic demonstration data by the trained policy for trex specifically.\\n    Arguments:\\n        - input_cfg (:obj:`Union[str, Tuple[dict, dict]]`): Config in dict type.             ``str`` type means config file path.             ``Tuple[dict, dict]`` type means [user_config, create_cfg].\\n        - seed (:obj:`int`): Random seed.\\n        - collect_count (:obj:`int`): The count of collected data.\\n        - rank (:obj:`int`): The episode ranking.\\n        - env_setting (:obj:`Optional[List[Any]]`): A list with 3 elements:             ``BaseEnv`` subclass, collector env config, and evaluator env config.\\n        - model (:obj:`Optional[torch.nn.Module]`): Instance of torch.nn.Module.\\n        - state_dict (:obj:`Optional[dict]`): The state_dict of policy or model.\\n        - state_dict_path (:obj:'str') The abs path of the state dict.\\n    \"\n    if isinstance(input_cfg, str):\n        (cfg, create_cfg) = read_config(input_cfg)\n    else:\n        (cfg, create_cfg) = deepcopy(input_cfg)\n    create_cfg.policy.type += '_command'\n    env_fn = None if env_setting is None else env_setting[0]\n    cfg.env.collector_env_num = 1\n    cfg = compile_config(cfg, collector=EpisodeSerialCollector, seed=seed, env=env_fn, auto=True, create_cfg=create_cfg, save_cfg=True, save_path='collect_demo_data_config.py')\n    if env_setting is None:\n        (env_fn, collector_env_cfg, _) = get_vec_env_setting(cfg.env)\n    else:\n        (env_fn, collector_env_cfg, _) = env_setting\n    collector_env = create_env_manager(cfg.env.manager, [partial(env_fn, cfg=c) for c in collector_env_cfg])\n    collector_env.seed(seed)\n    set_pkg_seed(seed, use_cuda=cfg.policy.cuda)\n    policy = create_policy(cfg.policy, model=model, enable_field=['collect', 'eval'])\n    collect_demo_policy = policy.collect_mode\n    if state_dict is None:\n        assert state_dict_path is not None\n        state_dict = torch.load(state_dict_path, map_location='cpu')\n    policy.collect_mode.load_state_dict(state_dict)\n    collector = EpisodeSerialCollector(cfg.policy.collect.collector, collector_env, collect_demo_policy, exp_name=cfg.exp_name)\n    policy_kwargs = None if not hasattr(cfg.policy.other, 'eps') else {'eps': cfg.policy.other.eps.get('collect', 0.2)}\n    exp_data = collector.collect(n_episode=collect_count, policy_kwargs=policy_kwargs)\n    if cfg.policy.cuda:\n        exp_data = to_device(exp_data, 'cpu')\n    print('Collect {}th episodic demo data successfully'.format(rank))\n    return exp_data"
        ]
    },
    {
        "func_name": "trex_get_args",
        "original": "def trex_get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--cfg', type=str, default='abs path for a config')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n    args = parser.parse_known_args()[0]\n    return args",
        "mutated": [
            "def trex_get_args():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--cfg', type=str, default='abs path for a config')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n    args = parser.parse_known_args()[0]\n    return args",
            "def trex_get_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--cfg', type=str, default='abs path for a config')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n    args = parser.parse_known_args()[0]\n    return args",
            "def trex_get_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--cfg', type=str, default='abs path for a config')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n    args = parser.parse_known_args()[0]\n    return args",
            "def trex_get_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--cfg', type=str, default='abs path for a config')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n    args = parser.parse_known_args()[0]\n    return args",
            "def trex_get_args():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--cfg', type=str, default='abs path for a config')\n    parser.add_argument('--seed', type=int, default=0)\n    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n    args = parser.parse_known_args()[0]\n    return args"
        ]
    },
    {
        "func_name": "trex_collecting_data",
        "original": "def trex_collecting_data(args=None):\n    if args is None:\n        args = trex_get_args()\n    if isinstance(args.cfg, str):\n        (cfg, create_cfg) = read_config(args.cfg)\n    else:\n        (cfg, create_cfg) = deepcopy(args.cfg)\n    data_path = cfg.exp_name\n    expert_model_path = cfg.reward_model.expert_model_path\n    checkpoint_min = cfg.reward_model.checkpoint_min\n    checkpoint_max = cfg.reward_model.checkpoint_max\n    checkpoint_step = cfg.reward_model.checkpoint_step\n    checkpoints = []\n    for i in range(checkpoint_min, checkpoint_max + checkpoint_step, checkpoint_step):\n        checkpoints.append(str(i))\n    data_for_save = {}\n    learning_returns = []\n    learning_rewards = []\n    episodes_data = []\n    for checkpoint in checkpoints:\n        num_per_ckpt = 1\n        model_path = expert_model_path + '/ckpt/iteration_' + checkpoint + '.pth.tar'\n        seed = args.seed + (int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step)\n        exp_data = collect_episodic_demo_data_for_trex(deepcopy(args.cfg), seed, state_dict_path=model_path, collect_count=num_per_ckpt, rank=(int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step) + 1)\n        data_for_save[(int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step)] = exp_data\n        obs = [list(default_collate(exp_data[i])['obs'].numpy()) for i in range(len(exp_data))]\n        rewards = [default_collate(exp_data[i])['reward'].tolist() for i in range(len(exp_data))]\n        sum_rewards = [torch.sum(default_collate(exp_data[i])['reward']).item() for i in range(len(exp_data))]\n        learning_rewards.append(rewards)\n        learning_returns.append(sum_rewards)\n        episodes_data.append(obs)\n    offline_data_save_type(data_for_save, data_path + '/suboptimal_data.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(episodes_data, data_path + '/episodes_data.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(learning_returns, data_path + '/learning_returns.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(learning_rewards, data_path + '/learning_rewards.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(checkpoints, data_path + '/checkpoints.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    return (checkpoints, episodes_data, learning_returns, learning_rewards)",
        "mutated": [
            "def trex_collecting_data(args=None):\n    if False:\n        i = 10\n    if args is None:\n        args = trex_get_args()\n    if isinstance(args.cfg, str):\n        (cfg, create_cfg) = read_config(args.cfg)\n    else:\n        (cfg, create_cfg) = deepcopy(args.cfg)\n    data_path = cfg.exp_name\n    expert_model_path = cfg.reward_model.expert_model_path\n    checkpoint_min = cfg.reward_model.checkpoint_min\n    checkpoint_max = cfg.reward_model.checkpoint_max\n    checkpoint_step = cfg.reward_model.checkpoint_step\n    checkpoints = []\n    for i in range(checkpoint_min, checkpoint_max + checkpoint_step, checkpoint_step):\n        checkpoints.append(str(i))\n    data_for_save = {}\n    learning_returns = []\n    learning_rewards = []\n    episodes_data = []\n    for checkpoint in checkpoints:\n        num_per_ckpt = 1\n        model_path = expert_model_path + '/ckpt/iteration_' + checkpoint + '.pth.tar'\n        seed = args.seed + (int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step)\n        exp_data = collect_episodic_demo_data_for_trex(deepcopy(args.cfg), seed, state_dict_path=model_path, collect_count=num_per_ckpt, rank=(int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step) + 1)\n        data_for_save[(int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step)] = exp_data\n        obs = [list(default_collate(exp_data[i])['obs'].numpy()) for i in range(len(exp_data))]\n        rewards = [default_collate(exp_data[i])['reward'].tolist() for i in range(len(exp_data))]\n        sum_rewards = [torch.sum(default_collate(exp_data[i])['reward']).item() for i in range(len(exp_data))]\n        learning_rewards.append(rewards)\n        learning_returns.append(sum_rewards)\n        episodes_data.append(obs)\n    offline_data_save_type(data_for_save, data_path + '/suboptimal_data.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(episodes_data, data_path + '/episodes_data.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(learning_returns, data_path + '/learning_returns.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(learning_rewards, data_path + '/learning_rewards.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(checkpoints, data_path + '/checkpoints.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    return (checkpoints, episodes_data, learning_returns, learning_rewards)",
            "def trex_collecting_data(args=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if args is None:\n        args = trex_get_args()\n    if isinstance(args.cfg, str):\n        (cfg, create_cfg) = read_config(args.cfg)\n    else:\n        (cfg, create_cfg) = deepcopy(args.cfg)\n    data_path = cfg.exp_name\n    expert_model_path = cfg.reward_model.expert_model_path\n    checkpoint_min = cfg.reward_model.checkpoint_min\n    checkpoint_max = cfg.reward_model.checkpoint_max\n    checkpoint_step = cfg.reward_model.checkpoint_step\n    checkpoints = []\n    for i in range(checkpoint_min, checkpoint_max + checkpoint_step, checkpoint_step):\n        checkpoints.append(str(i))\n    data_for_save = {}\n    learning_returns = []\n    learning_rewards = []\n    episodes_data = []\n    for checkpoint in checkpoints:\n        num_per_ckpt = 1\n        model_path = expert_model_path + '/ckpt/iteration_' + checkpoint + '.pth.tar'\n        seed = args.seed + (int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step)\n        exp_data = collect_episodic_demo_data_for_trex(deepcopy(args.cfg), seed, state_dict_path=model_path, collect_count=num_per_ckpt, rank=(int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step) + 1)\n        data_for_save[(int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step)] = exp_data\n        obs = [list(default_collate(exp_data[i])['obs'].numpy()) for i in range(len(exp_data))]\n        rewards = [default_collate(exp_data[i])['reward'].tolist() for i in range(len(exp_data))]\n        sum_rewards = [torch.sum(default_collate(exp_data[i])['reward']).item() for i in range(len(exp_data))]\n        learning_rewards.append(rewards)\n        learning_returns.append(sum_rewards)\n        episodes_data.append(obs)\n    offline_data_save_type(data_for_save, data_path + '/suboptimal_data.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(episodes_data, data_path + '/episodes_data.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(learning_returns, data_path + '/learning_returns.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(learning_rewards, data_path + '/learning_rewards.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(checkpoints, data_path + '/checkpoints.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    return (checkpoints, episodes_data, learning_returns, learning_rewards)",
            "def trex_collecting_data(args=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if args is None:\n        args = trex_get_args()\n    if isinstance(args.cfg, str):\n        (cfg, create_cfg) = read_config(args.cfg)\n    else:\n        (cfg, create_cfg) = deepcopy(args.cfg)\n    data_path = cfg.exp_name\n    expert_model_path = cfg.reward_model.expert_model_path\n    checkpoint_min = cfg.reward_model.checkpoint_min\n    checkpoint_max = cfg.reward_model.checkpoint_max\n    checkpoint_step = cfg.reward_model.checkpoint_step\n    checkpoints = []\n    for i in range(checkpoint_min, checkpoint_max + checkpoint_step, checkpoint_step):\n        checkpoints.append(str(i))\n    data_for_save = {}\n    learning_returns = []\n    learning_rewards = []\n    episodes_data = []\n    for checkpoint in checkpoints:\n        num_per_ckpt = 1\n        model_path = expert_model_path + '/ckpt/iteration_' + checkpoint + '.pth.tar'\n        seed = args.seed + (int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step)\n        exp_data = collect_episodic_demo_data_for_trex(deepcopy(args.cfg), seed, state_dict_path=model_path, collect_count=num_per_ckpt, rank=(int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step) + 1)\n        data_for_save[(int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step)] = exp_data\n        obs = [list(default_collate(exp_data[i])['obs'].numpy()) for i in range(len(exp_data))]\n        rewards = [default_collate(exp_data[i])['reward'].tolist() for i in range(len(exp_data))]\n        sum_rewards = [torch.sum(default_collate(exp_data[i])['reward']).item() for i in range(len(exp_data))]\n        learning_rewards.append(rewards)\n        learning_returns.append(sum_rewards)\n        episodes_data.append(obs)\n    offline_data_save_type(data_for_save, data_path + '/suboptimal_data.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(episodes_data, data_path + '/episodes_data.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(learning_returns, data_path + '/learning_returns.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(learning_rewards, data_path + '/learning_rewards.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(checkpoints, data_path + '/checkpoints.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    return (checkpoints, episodes_data, learning_returns, learning_rewards)",
            "def trex_collecting_data(args=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if args is None:\n        args = trex_get_args()\n    if isinstance(args.cfg, str):\n        (cfg, create_cfg) = read_config(args.cfg)\n    else:\n        (cfg, create_cfg) = deepcopy(args.cfg)\n    data_path = cfg.exp_name\n    expert_model_path = cfg.reward_model.expert_model_path\n    checkpoint_min = cfg.reward_model.checkpoint_min\n    checkpoint_max = cfg.reward_model.checkpoint_max\n    checkpoint_step = cfg.reward_model.checkpoint_step\n    checkpoints = []\n    for i in range(checkpoint_min, checkpoint_max + checkpoint_step, checkpoint_step):\n        checkpoints.append(str(i))\n    data_for_save = {}\n    learning_returns = []\n    learning_rewards = []\n    episodes_data = []\n    for checkpoint in checkpoints:\n        num_per_ckpt = 1\n        model_path = expert_model_path + '/ckpt/iteration_' + checkpoint + '.pth.tar'\n        seed = args.seed + (int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step)\n        exp_data = collect_episodic_demo_data_for_trex(deepcopy(args.cfg), seed, state_dict_path=model_path, collect_count=num_per_ckpt, rank=(int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step) + 1)\n        data_for_save[(int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step)] = exp_data\n        obs = [list(default_collate(exp_data[i])['obs'].numpy()) for i in range(len(exp_data))]\n        rewards = [default_collate(exp_data[i])['reward'].tolist() for i in range(len(exp_data))]\n        sum_rewards = [torch.sum(default_collate(exp_data[i])['reward']).item() for i in range(len(exp_data))]\n        learning_rewards.append(rewards)\n        learning_returns.append(sum_rewards)\n        episodes_data.append(obs)\n    offline_data_save_type(data_for_save, data_path + '/suboptimal_data.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(episodes_data, data_path + '/episodes_data.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(learning_returns, data_path + '/learning_returns.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(learning_rewards, data_path + '/learning_rewards.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(checkpoints, data_path + '/checkpoints.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    return (checkpoints, episodes_data, learning_returns, learning_rewards)",
            "def trex_collecting_data(args=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if args is None:\n        args = trex_get_args()\n    if isinstance(args.cfg, str):\n        (cfg, create_cfg) = read_config(args.cfg)\n    else:\n        (cfg, create_cfg) = deepcopy(args.cfg)\n    data_path = cfg.exp_name\n    expert_model_path = cfg.reward_model.expert_model_path\n    checkpoint_min = cfg.reward_model.checkpoint_min\n    checkpoint_max = cfg.reward_model.checkpoint_max\n    checkpoint_step = cfg.reward_model.checkpoint_step\n    checkpoints = []\n    for i in range(checkpoint_min, checkpoint_max + checkpoint_step, checkpoint_step):\n        checkpoints.append(str(i))\n    data_for_save = {}\n    learning_returns = []\n    learning_rewards = []\n    episodes_data = []\n    for checkpoint in checkpoints:\n        num_per_ckpt = 1\n        model_path = expert_model_path + '/ckpt/iteration_' + checkpoint + '.pth.tar'\n        seed = args.seed + (int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step)\n        exp_data = collect_episodic_demo_data_for_trex(deepcopy(args.cfg), seed, state_dict_path=model_path, collect_count=num_per_ckpt, rank=(int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step) + 1)\n        data_for_save[(int(checkpoint) - int(checkpoint_min)) // int(checkpoint_step)] = exp_data\n        obs = [list(default_collate(exp_data[i])['obs'].numpy()) for i in range(len(exp_data))]\n        rewards = [default_collate(exp_data[i])['reward'].tolist() for i in range(len(exp_data))]\n        sum_rewards = [torch.sum(default_collate(exp_data[i])['reward']).item() for i in range(len(exp_data))]\n        learning_rewards.append(rewards)\n        learning_returns.append(sum_rewards)\n        episodes_data.append(obs)\n    offline_data_save_type(data_for_save, data_path + '/suboptimal_data.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(episodes_data, data_path + '/episodes_data.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(learning_returns, data_path + '/learning_returns.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(learning_rewards, data_path + '/learning_rewards.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    offline_data_save_type(checkpoints, data_path + '/checkpoints.pkl', data_type=cfg.policy.collect.get('data_type', 'naive'))\n    return (checkpoints, episodes_data, learning_returns, learning_rewards)"
        ]
    }
]