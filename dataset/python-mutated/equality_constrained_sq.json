[
    {
        "func_name": "default_scaling",
        "original": "def default_scaling(x):\n    (n,) = np.shape(x)\n    return speye(n)",
        "mutated": [
            "def default_scaling(x):\n    if False:\n        i = 10\n    (n,) = np.shape(x)\n    return speye(n)",
            "def default_scaling(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (n,) = np.shape(x)\n    return speye(n)",
            "def default_scaling(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (n,) = np.shape(x)\n    return speye(n)",
            "def default_scaling(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (n,) = np.shape(x)\n    return speye(n)",
            "def default_scaling(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (n,) = np.shape(x)\n    return speye(n)"
        ]
    },
    {
        "func_name": "equality_constrained_sqp",
        "original": "def equality_constrained_sqp(fun_and_constr, grad_and_jac, lagr_hess, x0, fun0, grad0, constr0, jac0, stop_criteria, state, initial_penalty, initial_trust_radius, factorization_method, trust_lb=None, trust_ub=None, scaling=default_scaling):\n    \"\"\"Solve nonlinear equality-constrained problem using trust-region SQP.\n\n    Solve optimization problem:\n\n        minimize fun(x)\n        subject to: constr(x) = 0\n\n    using Byrd-Omojokun Trust-Region SQP method described in [1]_. Several\n    implementation details are based on [2]_ and [3]_, p. 549.\n\n    References\n    ----------\n    .. [1] Lalee, Marucha, Jorge Nocedal, and Todd Plantenga. \"On the\n           implementation of an algorithm for large-scale equality\n           constrained optimization.\" SIAM Journal on\n           Optimization 8.3 (1998): 682-706.\n    .. [2] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal.\n           \"An interior point algorithm for large-scale nonlinear\n           programming.\" SIAM Journal on Optimization 9.4 (1999): 877-900.\n    .. [3] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\n           Second Edition (2006).\n    \"\"\"\n    PENALTY_FACTOR = 0.3\n    LARGE_REDUCTION_RATIO = 0.9\n    INTERMEDIARY_REDUCTION_RATIO = 0.3\n    SUFFICIENT_REDUCTION_RATIO = 1e-08\n    TRUST_ENLARGEMENT_FACTOR_L = 7.0\n    TRUST_ENLARGEMENT_FACTOR_S = 2.0\n    MAX_TRUST_REDUCTION = 0.5\n    MIN_TRUST_REDUCTION = 0.1\n    SOC_THRESHOLD = 0.1\n    TR_FACTOR = 0.8\n    BOX_FACTOR = 0.5\n    (n,) = np.shape(x0)\n    if trust_lb is None:\n        trust_lb = np.full(n, -np.inf)\n    if trust_ub is None:\n        trust_ub = np.full(n, np.inf)\n    x = np.copy(x0)\n    trust_radius = initial_trust_radius\n    penalty = initial_penalty\n    f = fun0\n    c = grad0\n    b = constr0\n    A = jac0\n    S = scaling(x)\n    (Z, LS, Y) = projections(A, factorization_method)\n    v = -LS.dot(c)\n    H = lagr_hess(x, v)\n    optimality = norm(c + A.T.dot(v), np.inf)\n    constr_violation = norm(b, np.inf) if len(b) > 0 else 0\n    cg_info = {'niter': 0, 'stop_cond': 0, 'hits_boundary': False}\n    last_iteration_failed = False\n    while not stop_criteria(state, x, last_iteration_failed, optimality, constr_violation, trust_radius, penalty, cg_info):\n        dn = modified_dogleg(A, Y, b, TR_FACTOR * trust_radius, BOX_FACTOR * trust_lb, BOX_FACTOR * trust_ub)\n        c_t = H.dot(dn) + c\n        b_t = np.zeros_like(b)\n        trust_radius_t = np.sqrt(trust_radius ** 2 - np.linalg.norm(dn) ** 2)\n        lb_t = trust_lb - dn\n        ub_t = trust_ub - dn\n        (dt, cg_info) = projected_cg(H, c_t, Z, Y, b_t, trust_radius_t, lb_t, ub_t)\n        d = dn + dt\n        quadratic_model = 1 / 2 * H.dot(d).dot(d) + c.T.dot(d)\n        linearized_constr = A.dot(d) + b\n        vpred = norm(b) - norm(linearized_constr)\n        vpred = max(1e-16, vpred)\n        previous_penalty = penalty\n        if quadratic_model > 0:\n            new_penalty = quadratic_model / ((1 - PENALTY_FACTOR) * vpred)\n            penalty = max(penalty, new_penalty)\n        predicted_reduction = -quadratic_model + penalty * vpred\n        merit_function = f + penalty * norm(b)\n        x_next = x + S.dot(d)\n        (f_next, b_next) = fun_and_constr(x_next)\n        merit_function_next = f_next + penalty * norm(b_next)\n        actual_reduction = merit_function - merit_function_next\n        reduction_ratio = actual_reduction / predicted_reduction\n        if reduction_ratio < SUFFICIENT_REDUCTION_RATIO and norm(dn) <= SOC_THRESHOLD * norm(dt):\n            y = -Y.dot(b_next)\n            (_, t, intersect) = box_intersections(d, y, trust_lb, trust_ub)\n            x_soc = x + S.dot(d + t * y)\n            (f_soc, b_soc) = fun_and_constr(x_soc)\n            merit_function_soc = f_soc + penalty * norm(b_soc)\n            actual_reduction_soc = merit_function - merit_function_soc\n            reduction_ratio_soc = actual_reduction_soc / predicted_reduction\n            if intersect and reduction_ratio_soc >= SUFFICIENT_REDUCTION_RATIO:\n                x_next = x_soc\n                f_next = f_soc\n                b_next = b_soc\n                reduction_ratio = reduction_ratio_soc\n        if reduction_ratio >= LARGE_REDUCTION_RATIO:\n            trust_radius = max(TRUST_ENLARGEMENT_FACTOR_L * norm(d), trust_radius)\n        elif reduction_ratio >= INTERMEDIARY_REDUCTION_RATIO:\n            trust_radius = max(TRUST_ENLARGEMENT_FACTOR_S * norm(d), trust_radius)\n        elif reduction_ratio < SUFFICIENT_REDUCTION_RATIO:\n            trust_reduction = (1 - SUFFICIENT_REDUCTION_RATIO) / (1 - reduction_ratio)\n            new_trust_radius = trust_reduction * norm(d)\n            if new_trust_radius >= MAX_TRUST_REDUCTION * trust_radius:\n                trust_radius *= MAX_TRUST_REDUCTION\n            elif new_trust_radius >= MIN_TRUST_REDUCTION * trust_radius:\n                trust_radius = new_trust_radius\n            else:\n                trust_radius *= MIN_TRUST_REDUCTION\n        if reduction_ratio >= SUFFICIENT_REDUCTION_RATIO:\n            x = x_next\n            (f, b) = (f_next, b_next)\n            (c, A) = grad_and_jac(x)\n            S = scaling(x)\n            (Z, LS, Y) = projections(A, factorization_method)\n            v = -LS.dot(c)\n            H = lagr_hess(x, v)\n            last_iteration_failed = False\n            optimality = norm(c + A.T.dot(v), np.inf)\n            constr_violation = norm(b, np.inf) if len(b) > 0 else 0\n        else:\n            penalty = previous_penalty\n            last_iteration_failed = True\n    return (x, state)",
        "mutated": [
            "def equality_constrained_sqp(fun_and_constr, grad_and_jac, lagr_hess, x0, fun0, grad0, constr0, jac0, stop_criteria, state, initial_penalty, initial_trust_radius, factorization_method, trust_lb=None, trust_ub=None, scaling=default_scaling):\n    if False:\n        i = 10\n    'Solve nonlinear equality-constrained problem using trust-region SQP.\\n\\n    Solve optimization problem:\\n\\n        minimize fun(x)\\n        subject to: constr(x) = 0\\n\\n    using Byrd-Omojokun Trust-Region SQP method described in [1]_. Several\\n    implementation details are based on [2]_ and [3]_, p. 549.\\n\\n    References\\n    ----------\\n    .. [1] Lalee, Marucha, Jorge Nocedal, and Todd Plantenga. \"On the\\n           implementation of an algorithm for large-scale equality\\n           constrained optimization.\" SIAM Journal on\\n           Optimization 8.3 (1998): 682-706.\\n    .. [2] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal.\\n           \"An interior point algorithm for large-scale nonlinear\\n           programming.\" SIAM Journal on Optimization 9.4 (1999): 877-900.\\n    .. [3] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\\n           Second Edition (2006).\\n    '\n    PENALTY_FACTOR = 0.3\n    LARGE_REDUCTION_RATIO = 0.9\n    INTERMEDIARY_REDUCTION_RATIO = 0.3\n    SUFFICIENT_REDUCTION_RATIO = 1e-08\n    TRUST_ENLARGEMENT_FACTOR_L = 7.0\n    TRUST_ENLARGEMENT_FACTOR_S = 2.0\n    MAX_TRUST_REDUCTION = 0.5\n    MIN_TRUST_REDUCTION = 0.1\n    SOC_THRESHOLD = 0.1\n    TR_FACTOR = 0.8\n    BOX_FACTOR = 0.5\n    (n,) = np.shape(x0)\n    if trust_lb is None:\n        trust_lb = np.full(n, -np.inf)\n    if trust_ub is None:\n        trust_ub = np.full(n, np.inf)\n    x = np.copy(x0)\n    trust_radius = initial_trust_radius\n    penalty = initial_penalty\n    f = fun0\n    c = grad0\n    b = constr0\n    A = jac0\n    S = scaling(x)\n    (Z, LS, Y) = projections(A, factorization_method)\n    v = -LS.dot(c)\n    H = lagr_hess(x, v)\n    optimality = norm(c + A.T.dot(v), np.inf)\n    constr_violation = norm(b, np.inf) if len(b) > 0 else 0\n    cg_info = {'niter': 0, 'stop_cond': 0, 'hits_boundary': False}\n    last_iteration_failed = False\n    while not stop_criteria(state, x, last_iteration_failed, optimality, constr_violation, trust_radius, penalty, cg_info):\n        dn = modified_dogleg(A, Y, b, TR_FACTOR * trust_radius, BOX_FACTOR * trust_lb, BOX_FACTOR * trust_ub)\n        c_t = H.dot(dn) + c\n        b_t = np.zeros_like(b)\n        trust_radius_t = np.sqrt(trust_radius ** 2 - np.linalg.norm(dn) ** 2)\n        lb_t = trust_lb - dn\n        ub_t = trust_ub - dn\n        (dt, cg_info) = projected_cg(H, c_t, Z, Y, b_t, trust_radius_t, lb_t, ub_t)\n        d = dn + dt\n        quadratic_model = 1 / 2 * H.dot(d).dot(d) + c.T.dot(d)\n        linearized_constr = A.dot(d) + b\n        vpred = norm(b) - norm(linearized_constr)\n        vpred = max(1e-16, vpred)\n        previous_penalty = penalty\n        if quadratic_model > 0:\n            new_penalty = quadratic_model / ((1 - PENALTY_FACTOR) * vpred)\n            penalty = max(penalty, new_penalty)\n        predicted_reduction = -quadratic_model + penalty * vpred\n        merit_function = f + penalty * norm(b)\n        x_next = x + S.dot(d)\n        (f_next, b_next) = fun_and_constr(x_next)\n        merit_function_next = f_next + penalty * norm(b_next)\n        actual_reduction = merit_function - merit_function_next\n        reduction_ratio = actual_reduction / predicted_reduction\n        if reduction_ratio < SUFFICIENT_REDUCTION_RATIO and norm(dn) <= SOC_THRESHOLD * norm(dt):\n            y = -Y.dot(b_next)\n            (_, t, intersect) = box_intersections(d, y, trust_lb, trust_ub)\n            x_soc = x + S.dot(d + t * y)\n            (f_soc, b_soc) = fun_and_constr(x_soc)\n            merit_function_soc = f_soc + penalty * norm(b_soc)\n            actual_reduction_soc = merit_function - merit_function_soc\n            reduction_ratio_soc = actual_reduction_soc / predicted_reduction\n            if intersect and reduction_ratio_soc >= SUFFICIENT_REDUCTION_RATIO:\n                x_next = x_soc\n                f_next = f_soc\n                b_next = b_soc\n                reduction_ratio = reduction_ratio_soc\n        if reduction_ratio >= LARGE_REDUCTION_RATIO:\n            trust_radius = max(TRUST_ENLARGEMENT_FACTOR_L * norm(d), trust_radius)\n        elif reduction_ratio >= INTERMEDIARY_REDUCTION_RATIO:\n            trust_radius = max(TRUST_ENLARGEMENT_FACTOR_S * norm(d), trust_radius)\n        elif reduction_ratio < SUFFICIENT_REDUCTION_RATIO:\n            trust_reduction = (1 - SUFFICIENT_REDUCTION_RATIO) / (1 - reduction_ratio)\n            new_trust_radius = trust_reduction * norm(d)\n            if new_trust_radius >= MAX_TRUST_REDUCTION * trust_radius:\n                trust_radius *= MAX_TRUST_REDUCTION\n            elif new_trust_radius >= MIN_TRUST_REDUCTION * trust_radius:\n                trust_radius = new_trust_radius\n            else:\n                trust_radius *= MIN_TRUST_REDUCTION\n        if reduction_ratio >= SUFFICIENT_REDUCTION_RATIO:\n            x = x_next\n            (f, b) = (f_next, b_next)\n            (c, A) = grad_and_jac(x)\n            S = scaling(x)\n            (Z, LS, Y) = projections(A, factorization_method)\n            v = -LS.dot(c)\n            H = lagr_hess(x, v)\n            last_iteration_failed = False\n            optimality = norm(c + A.T.dot(v), np.inf)\n            constr_violation = norm(b, np.inf) if len(b) > 0 else 0\n        else:\n            penalty = previous_penalty\n            last_iteration_failed = True\n    return (x, state)",
            "def equality_constrained_sqp(fun_and_constr, grad_and_jac, lagr_hess, x0, fun0, grad0, constr0, jac0, stop_criteria, state, initial_penalty, initial_trust_radius, factorization_method, trust_lb=None, trust_ub=None, scaling=default_scaling):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Solve nonlinear equality-constrained problem using trust-region SQP.\\n\\n    Solve optimization problem:\\n\\n        minimize fun(x)\\n        subject to: constr(x) = 0\\n\\n    using Byrd-Omojokun Trust-Region SQP method described in [1]_. Several\\n    implementation details are based on [2]_ and [3]_, p. 549.\\n\\n    References\\n    ----------\\n    .. [1] Lalee, Marucha, Jorge Nocedal, and Todd Plantenga. \"On the\\n           implementation of an algorithm for large-scale equality\\n           constrained optimization.\" SIAM Journal on\\n           Optimization 8.3 (1998): 682-706.\\n    .. [2] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal.\\n           \"An interior point algorithm for large-scale nonlinear\\n           programming.\" SIAM Journal on Optimization 9.4 (1999): 877-900.\\n    .. [3] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\\n           Second Edition (2006).\\n    '\n    PENALTY_FACTOR = 0.3\n    LARGE_REDUCTION_RATIO = 0.9\n    INTERMEDIARY_REDUCTION_RATIO = 0.3\n    SUFFICIENT_REDUCTION_RATIO = 1e-08\n    TRUST_ENLARGEMENT_FACTOR_L = 7.0\n    TRUST_ENLARGEMENT_FACTOR_S = 2.0\n    MAX_TRUST_REDUCTION = 0.5\n    MIN_TRUST_REDUCTION = 0.1\n    SOC_THRESHOLD = 0.1\n    TR_FACTOR = 0.8\n    BOX_FACTOR = 0.5\n    (n,) = np.shape(x0)\n    if trust_lb is None:\n        trust_lb = np.full(n, -np.inf)\n    if trust_ub is None:\n        trust_ub = np.full(n, np.inf)\n    x = np.copy(x0)\n    trust_radius = initial_trust_radius\n    penalty = initial_penalty\n    f = fun0\n    c = grad0\n    b = constr0\n    A = jac0\n    S = scaling(x)\n    (Z, LS, Y) = projections(A, factorization_method)\n    v = -LS.dot(c)\n    H = lagr_hess(x, v)\n    optimality = norm(c + A.T.dot(v), np.inf)\n    constr_violation = norm(b, np.inf) if len(b) > 0 else 0\n    cg_info = {'niter': 0, 'stop_cond': 0, 'hits_boundary': False}\n    last_iteration_failed = False\n    while not stop_criteria(state, x, last_iteration_failed, optimality, constr_violation, trust_radius, penalty, cg_info):\n        dn = modified_dogleg(A, Y, b, TR_FACTOR * trust_radius, BOX_FACTOR * trust_lb, BOX_FACTOR * trust_ub)\n        c_t = H.dot(dn) + c\n        b_t = np.zeros_like(b)\n        trust_radius_t = np.sqrt(trust_radius ** 2 - np.linalg.norm(dn) ** 2)\n        lb_t = trust_lb - dn\n        ub_t = trust_ub - dn\n        (dt, cg_info) = projected_cg(H, c_t, Z, Y, b_t, trust_radius_t, lb_t, ub_t)\n        d = dn + dt\n        quadratic_model = 1 / 2 * H.dot(d).dot(d) + c.T.dot(d)\n        linearized_constr = A.dot(d) + b\n        vpred = norm(b) - norm(linearized_constr)\n        vpred = max(1e-16, vpred)\n        previous_penalty = penalty\n        if quadratic_model > 0:\n            new_penalty = quadratic_model / ((1 - PENALTY_FACTOR) * vpred)\n            penalty = max(penalty, new_penalty)\n        predicted_reduction = -quadratic_model + penalty * vpred\n        merit_function = f + penalty * norm(b)\n        x_next = x + S.dot(d)\n        (f_next, b_next) = fun_and_constr(x_next)\n        merit_function_next = f_next + penalty * norm(b_next)\n        actual_reduction = merit_function - merit_function_next\n        reduction_ratio = actual_reduction / predicted_reduction\n        if reduction_ratio < SUFFICIENT_REDUCTION_RATIO and norm(dn) <= SOC_THRESHOLD * norm(dt):\n            y = -Y.dot(b_next)\n            (_, t, intersect) = box_intersections(d, y, trust_lb, trust_ub)\n            x_soc = x + S.dot(d + t * y)\n            (f_soc, b_soc) = fun_and_constr(x_soc)\n            merit_function_soc = f_soc + penalty * norm(b_soc)\n            actual_reduction_soc = merit_function - merit_function_soc\n            reduction_ratio_soc = actual_reduction_soc / predicted_reduction\n            if intersect and reduction_ratio_soc >= SUFFICIENT_REDUCTION_RATIO:\n                x_next = x_soc\n                f_next = f_soc\n                b_next = b_soc\n                reduction_ratio = reduction_ratio_soc\n        if reduction_ratio >= LARGE_REDUCTION_RATIO:\n            trust_radius = max(TRUST_ENLARGEMENT_FACTOR_L * norm(d), trust_radius)\n        elif reduction_ratio >= INTERMEDIARY_REDUCTION_RATIO:\n            trust_radius = max(TRUST_ENLARGEMENT_FACTOR_S * norm(d), trust_radius)\n        elif reduction_ratio < SUFFICIENT_REDUCTION_RATIO:\n            trust_reduction = (1 - SUFFICIENT_REDUCTION_RATIO) / (1 - reduction_ratio)\n            new_trust_radius = trust_reduction * norm(d)\n            if new_trust_radius >= MAX_TRUST_REDUCTION * trust_radius:\n                trust_radius *= MAX_TRUST_REDUCTION\n            elif new_trust_radius >= MIN_TRUST_REDUCTION * trust_radius:\n                trust_radius = new_trust_radius\n            else:\n                trust_radius *= MIN_TRUST_REDUCTION\n        if reduction_ratio >= SUFFICIENT_REDUCTION_RATIO:\n            x = x_next\n            (f, b) = (f_next, b_next)\n            (c, A) = grad_and_jac(x)\n            S = scaling(x)\n            (Z, LS, Y) = projections(A, factorization_method)\n            v = -LS.dot(c)\n            H = lagr_hess(x, v)\n            last_iteration_failed = False\n            optimality = norm(c + A.T.dot(v), np.inf)\n            constr_violation = norm(b, np.inf) if len(b) > 0 else 0\n        else:\n            penalty = previous_penalty\n            last_iteration_failed = True\n    return (x, state)",
            "def equality_constrained_sqp(fun_and_constr, grad_and_jac, lagr_hess, x0, fun0, grad0, constr0, jac0, stop_criteria, state, initial_penalty, initial_trust_radius, factorization_method, trust_lb=None, trust_ub=None, scaling=default_scaling):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Solve nonlinear equality-constrained problem using trust-region SQP.\\n\\n    Solve optimization problem:\\n\\n        minimize fun(x)\\n        subject to: constr(x) = 0\\n\\n    using Byrd-Omojokun Trust-Region SQP method described in [1]_. Several\\n    implementation details are based on [2]_ and [3]_, p. 549.\\n\\n    References\\n    ----------\\n    .. [1] Lalee, Marucha, Jorge Nocedal, and Todd Plantenga. \"On the\\n           implementation of an algorithm for large-scale equality\\n           constrained optimization.\" SIAM Journal on\\n           Optimization 8.3 (1998): 682-706.\\n    .. [2] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal.\\n           \"An interior point algorithm for large-scale nonlinear\\n           programming.\" SIAM Journal on Optimization 9.4 (1999): 877-900.\\n    .. [3] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\\n           Second Edition (2006).\\n    '\n    PENALTY_FACTOR = 0.3\n    LARGE_REDUCTION_RATIO = 0.9\n    INTERMEDIARY_REDUCTION_RATIO = 0.3\n    SUFFICIENT_REDUCTION_RATIO = 1e-08\n    TRUST_ENLARGEMENT_FACTOR_L = 7.0\n    TRUST_ENLARGEMENT_FACTOR_S = 2.0\n    MAX_TRUST_REDUCTION = 0.5\n    MIN_TRUST_REDUCTION = 0.1\n    SOC_THRESHOLD = 0.1\n    TR_FACTOR = 0.8\n    BOX_FACTOR = 0.5\n    (n,) = np.shape(x0)\n    if trust_lb is None:\n        trust_lb = np.full(n, -np.inf)\n    if trust_ub is None:\n        trust_ub = np.full(n, np.inf)\n    x = np.copy(x0)\n    trust_radius = initial_trust_radius\n    penalty = initial_penalty\n    f = fun0\n    c = grad0\n    b = constr0\n    A = jac0\n    S = scaling(x)\n    (Z, LS, Y) = projections(A, factorization_method)\n    v = -LS.dot(c)\n    H = lagr_hess(x, v)\n    optimality = norm(c + A.T.dot(v), np.inf)\n    constr_violation = norm(b, np.inf) if len(b) > 0 else 0\n    cg_info = {'niter': 0, 'stop_cond': 0, 'hits_boundary': False}\n    last_iteration_failed = False\n    while not stop_criteria(state, x, last_iteration_failed, optimality, constr_violation, trust_radius, penalty, cg_info):\n        dn = modified_dogleg(A, Y, b, TR_FACTOR * trust_radius, BOX_FACTOR * trust_lb, BOX_FACTOR * trust_ub)\n        c_t = H.dot(dn) + c\n        b_t = np.zeros_like(b)\n        trust_radius_t = np.sqrt(trust_radius ** 2 - np.linalg.norm(dn) ** 2)\n        lb_t = trust_lb - dn\n        ub_t = trust_ub - dn\n        (dt, cg_info) = projected_cg(H, c_t, Z, Y, b_t, trust_radius_t, lb_t, ub_t)\n        d = dn + dt\n        quadratic_model = 1 / 2 * H.dot(d).dot(d) + c.T.dot(d)\n        linearized_constr = A.dot(d) + b\n        vpred = norm(b) - norm(linearized_constr)\n        vpred = max(1e-16, vpred)\n        previous_penalty = penalty\n        if quadratic_model > 0:\n            new_penalty = quadratic_model / ((1 - PENALTY_FACTOR) * vpred)\n            penalty = max(penalty, new_penalty)\n        predicted_reduction = -quadratic_model + penalty * vpred\n        merit_function = f + penalty * norm(b)\n        x_next = x + S.dot(d)\n        (f_next, b_next) = fun_and_constr(x_next)\n        merit_function_next = f_next + penalty * norm(b_next)\n        actual_reduction = merit_function - merit_function_next\n        reduction_ratio = actual_reduction / predicted_reduction\n        if reduction_ratio < SUFFICIENT_REDUCTION_RATIO and norm(dn) <= SOC_THRESHOLD * norm(dt):\n            y = -Y.dot(b_next)\n            (_, t, intersect) = box_intersections(d, y, trust_lb, trust_ub)\n            x_soc = x + S.dot(d + t * y)\n            (f_soc, b_soc) = fun_and_constr(x_soc)\n            merit_function_soc = f_soc + penalty * norm(b_soc)\n            actual_reduction_soc = merit_function - merit_function_soc\n            reduction_ratio_soc = actual_reduction_soc / predicted_reduction\n            if intersect and reduction_ratio_soc >= SUFFICIENT_REDUCTION_RATIO:\n                x_next = x_soc\n                f_next = f_soc\n                b_next = b_soc\n                reduction_ratio = reduction_ratio_soc\n        if reduction_ratio >= LARGE_REDUCTION_RATIO:\n            trust_radius = max(TRUST_ENLARGEMENT_FACTOR_L * norm(d), trust_radius)\n        elif reduction_ratio >= INTERMEDIARY_REDUCTION_RATIO:\n            trust_radius = max(TRUST_ENLARGEMENT_FACTOR_S * norm(d), trust_radius)\n        elif reduction_ratio < SUFFICIENT_REDUCTION_RATIO:\n            trust_reduction = (1 - SUFFICIENT_REDUCTION_RATIO) / (1 - reduction_ratio)\n            new_trust_radius = trust_reduction * norm(d)\n            if new_trust_radius >= MAX_TRUST_REDUCTION * trust_radius:\n                trust_radius *= MAX_TRUST_REDUCTION\n            elif new_trust_radius >= MIN_TRUST_REDUCTION * trust_radius:\n                trust_radius = new_trust_radius\n            else:\n                trust_radius *= MIN_TRUST_REDUCTION\n        if reduction_ratio >= SUFFICIENT_REDUCTION_RATIO:\n            x = x_next\n            (f, b) = (f_next, b_next)\n            (c, A) = grad_and_jac(x)\n            S = scaling(x)\n            (Z, LS, Y) = projections(A, factorization_method)\n            v = -LS.dot(c)\n            H = lagr_hess(x, v)\n            last_iteration_failed = False\n            optimality = norm(c + A.T.dot(v), np.inf)\n            constr_violation = norm(b, np.inf) if len(b) > 0 else 0\n        else:\n            penalty = previous_penalty\n            last_iteration_failed = True\n    return (x, state)",
            "def equality_constrained_sqp(fun_and_constr, grad_and_jac, lagr_hess, x0, fun0, grad0, constr0, jac0, stop_criteria, state, initial_penalty, initial_trust_radius, factorization_method, trust_lb=None, trust_ub=None, scaling=default_scaling):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Solve nonlinear equality-constrained problem using trust-region SQP.\\n\\n    Solve optimization problem:\\n\\n        minimize fun(x)\\n        subject to: constr(x) = 0\\n\\n    using Byrd-Omojokun Trust-Region SQP method described in [1]_. Several\\n    implementation details are based on [2]_ and [3]_, p. 549.\\n\\n    References\\n    ----------\\n    .. [1] Lalee, Marucha, Jorge Nocedal, and Todd Plantenga. \"On the\\n           implementation of an algorithm for large-scale equality\\n           constrained optimization.\" SIAM Journal on\\n           Optimization 8.3 (1998): 682-706.\\n    .. [2] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal.\\n           \"An interior point algorithm for large-scale nonlinear\\n           programming.\" SIAM Journal on Optimization 9.4 (1999): 877-900.\\n    .. [3] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\\n           Second Edition (2006).\\n    '\n    PENALTY_FACTOR = 0.3\n    LARGE_REDUCTION_RATIO = 0.9\n    INTERMEDIARY_REDUCTION_RATIO = 0.3\n    SUFFICIENT_REDUCTION_RATIO = 1e-08\n    TRUST_ENLARGEMENT_FACTOR_L = 7.0\n    TRUST_ENLARGEMENT_FACTOR_S = 2.0\n    MAX_TRUST_REDUCTION = 0.5\n    MIN_TRUST_REDUCTION = 0.1\n    SOC_THRESHOLD = 0.1\n    TR_FACTOR = 0.8\n    BOX_FACTOR = 0.5\n    (n,) = np.shape(x0)\n    if trust_lb is None:\n        trust_lb = np.full(n, -np.inf)\n    if trust_ub is None:\n        trust_ub = np.full(n, np.inf)\n    x = np.copy(x0)\n    trust_radius = initial_trust_radius\n    penalty = initial_penalty\n    f = fun0\n    c = grad0\n    b = constr0\n    A = jac0\n    S = scaling(x)\n    (Z, LS, Y) = projections(A, factorization_method)\n    v = -LS.dot(c)\n    H = lagr_hess(x, v)\n    optimality = norm(c + A.T.dot(v), np.inf)\n    constr_violation = norm(b, np.inf) if len(b) > 0 else 0\n    cg_info = {'niter': 0, 'stop_cond': 0, 'hits_boundary': False}\n    last_iteration_failed = False\n    while not stop_criteria(state, x, last_iteration_failed, optimality, constr_violation, trust_radius, penalty, cg_info):\n        dn = modified_dogleg(A, Y, b, TR_FACTOR * trust_radius, BOX_FACTOR * trust_lb, BOX_FACTOR * trust_ub)\n        c_t = H.dot(dn) + c\n        b_t = np.zeros_like(b)\n        trust_radius_t = np.sqrt(trust_radius ** 2 - np.linalg.norm(dn) ** 2)\n        lb_t = trust_lb - dn\n        ub_t = trust_ub - dn\n        (dt, cg_info) = projected_cg(H, c_t, Z, Y, b_t, trust_radius_t, lb_t, ub_t)\n        d = dn + dt\n        quadratic_model = 1 / 2 * H.dot(d).dot(d) + c.T.dot(d)\n        linearized_constr = A.dot(d) + b\n        vpred = norm(b) - norm(linearized_constr)\n        vpred = max(1e-16, vpred)\n        previous_penalty = penalty\n        if quadratic_model > 0:\n            new_penalty = quadratic_model / ((1 - PENALTY_FACTOR) * vpred)\n            penalty = max(penalty, new_penalty)\n        predicted_reduction = -quadratic_model + penalty * vpred\n        merit_function = f + penalty * norm(b)\n        x_next = x + S.dot(d)\n        (f_next, b_next) = fun_and_constr(x_next)\n        merit_function_next = f_next + penalty * norm(b_next)\n        actual_reduction = merit_function - merit_function_next\n        reduction_ratio = actual_reduction / predicted_reduction\n        if reduction_ratio < SUFFICIENT_REDUCTION_RATIO and norm(dn) <= SOC_THRESHOLD * norm(dt):\n            y = -Y.dot(b_next)\n            (_, t, intersect) = box_intersections(d, y, trust_lb, trust_ub)\n            x_soc = x + S.dot(d + t * y)\n            (f_soc, b_soc) = fun_and_constr(x_soc)\n            merit_function_soc = f_soc + penalty * norm(b_soc)\n            actual_reduction_soc = merit_function - merit_function_soc\n            reduction_ratio_soc = actual_reduction_soc / predicted_reduction\n            if intersect and reduction_ratio_soc >= SUFFICIENT_REDUCTION_RATIO:\n                x_next = x_soc\n                f_next = f_soc\n                b_next = b_soc\n                reduction_ratio = reduction_ratio_soc\n        if reduction_ratio >= LARGE_REDUCTION_RATIO:\n            trust_radius = max(TRUST_ENLARGEMENT_FACTOR_L * norm(d), trust_radius)\n        elif reduction_ratio >= INTERMEDIARY_REDUCTION_RATIO:\n            trust_radius = max(TRUST_ENLARGEMENT_FACTOR_S * norm(d), trust_radius)\n        elif reduction_ratio < SUFFICIENT_REDUCTION_RATIO:\n            trust_reduction = (1 - SUFFICIENT_REDUCTION_RATIO) / (1 - reduction_ratio)\n            new_trust_radius = trust_reduction * norm(d)\n            if new_trust_radius >= MAX_TRUST_REDUCTION * trust_radius:\n                trust_radius *= MAX_TRUST_REDUCTION\n            elif new_trust_radius >= MIN_TRUST_REDUCTION * trust_radius:\n                trust_radius = new_trust_radius\n            else:\n                trust_radius *= MIN_TRUST_REDUCTION\n        if reduction_ratio >= SUFFICIENT_REDUCTION_RATIO:\n            x = x_next\n            (f, b) = (f_next, b_next)\n            (c, A) = grad_and_jac(x)\n            S = scaling(x)\n            (Z, LS, Y) = projections(A, factorization_method)\n            v = -LS.dot(c)\n            H = lagr_hess(x, v)\n            last_iteration_failed = False\n            optimality = norm(c + A.T.dot(v), np.inf)\n            constr_violation = norm(b, np.inf) if len(b) > 0 else 0\n        else:\n            penalty = previous_penalty\n            last_iteration_failed = True\n    return (x, state)",
            "def equality_constrained_sqp(fun_and_constr, grad_and_jac, lagr_hess, x0, fun0, grad0, constr0, jac0, stop_criteria, state, initial_penalty, initial_trust_radius, factorization_method, trust_lb=None, trust_ub=None, scaling=default_scaling):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Solve nonlinear equality-constrained problem using trust-region SQP.\\n\\n    Solve optimization problem:\\n\\n        minimize fun(x)\\n        subject to: constr(x) = 0\\n\\n    using Byrd-Omojokun Trust-Region SQP method described in [1]_. Several\\n    implementation details are based on [2]_ and [3]_, p. 549.\\n\\n    References\\n    ----------\\n    .. [1] Lalee, Marucha, Jorge Nocedal, and Todd Plantenga. \"On the\\n           implementation of an algorithm for large-scale equality\\n           constrained optimization.\" SIAM Journal on\\n           Optimization 8.3 (1998): 682-706.\\n    .. [2] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal.\\n           \"An interior point algorithm for large-scale nonlinear\\n           programming.\" SIAM Journal on Optimization 9.4 (1999): 877-900.\\n    .. [3] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\\n           Second Edition (2006).\\n    '\n    PENALTY_FACTOR = 0.3\n    LARGE_REDUCTION_RATIO = 0.9\n    INTERMEDIARY_REDUCTION_RATIO = 0.3\n    SUFFICIENT_REDUCTION_RATIO = 1e-08\n    TRUST_ENLARGEMENT_FACTOR_L = 7.0\n    TRUST_ENLARGEMENT_FACTOR_S = 2.0\n    MAX_TRUST_REDUCTION = 0.5\n    MIN_TRUST_REDUCTION = 0.1\n    SOC_THRESHOLD = 0.1\n    TR_FACTOR = 0.8\n    BOX_FACTOR = 0.5\n    (n,) = np.shape(x0)\n    if trust_lb is None:\n        trust_lb = np.full(n, -np.inf)\n    if trust_ub is None:\n        trust_ub = np.full(n, np.inf)\n    x = np.copy(x0)\n    trust_radius = initial_trust_radius\n    penalty = initial_penalty\n    f = fun0\n    c = grad0\n    b = constr0\n    A = jac0\n    S = scaling(x)\n    (Z, LS, Y) = projections(A, factorization_method)\n    v = -LS.dot(c)\n    H = lagr_hess(x, v)\n    optimality = norm(c + A.T.dot(v), np.inf)\n    constr_violation = norm(b, np.inf) if len(b) > 0 else 0\n    cg_info = {'niter': 0, 'stop_cond': 0, 'hits_boundary': False}\n    last_iteration_failed = False\n    while not stop_criteria(state, x, last_iteration_failed, optimality, constr_violation, trust_radius, penalty, cg_info):\n        dn = modified_dogleg(A, Y, b, TR_FACTOR * trust_radius, BOX_FACTOR * trust_lb, BOX_FACTOR * trust_ub)\n        c_t = H.dot(dn) + c\n        b_t = np.zeros_like(b)\n        trust_radius_t = np.sqrt(trust_radius ** 2 - np.linalg.norm(dn) ** 2)\n        lb_t = trust_lb - dn\n        ub_t = trust_ub - dn\n        (dt, cg_info) = projected_cg(H, c_t, Z, Y, b_t, trust_radius_t, lb_t, ub_t)\n        d = dn + dt\n        quadratic_model = 1 / 2 * H.dot(d).dot(d) + c.T.dot(d)\n        linearized_constr = A.dot(d) + b\n        vpred = norm(b) - norm(linearized_constr)\n        vpred = max(1e-16, vpred)\n        previous_penalty = penalty\n        if quadratic_model > 0:\n            new_penalty = quadratic_model / ((1 - PENALTY_FACTOR) * vpred)\n            penalty = max(penalty, new_penalty)\n        predicted_reduction = -quadratic_model + penalty * vpred\n        merit_function = f + penalty * norm(b)\n        x_next = x + S.dot(d)\n        (f_next, b_next) = fun_and_constr(x_next)\n        merit_function_next = f_next + penalty * norm(b_next)\n        actual_reduction = merit_function - merit_function_next\n        reduction_ratio = actual_reduction / predicted_reduction\n        if reduction_ratio < SUFFICIENT_REDUCTION_RATIO and norm(dn) <= SOC_THRESHOLD * norm(dt):\n            y = -Y.dot(b_next)\n            (_, t, intersect) = box_intersections(d, y, trust_lb, trust_ub)\n            x_soc = x + S.dot(d + t * y)\n            (f_soc, b_soc) = fun_and_constr(x_soc)\n            merit_function_soc = f_soc + penalty * norm(b_soc)\n            actual_reduction_soc = merit_function - merit_function_soc\n            reduction_ratio_soc = actual_reduction_soc / predicted_reduction\n            if intersect and reduction_ratio_soc >= SUFFICIENT_REDUCTION_RATIO:\n                x_next = x_soc\n                f_next = f_soc\n                b_next = b_soc\n                reduction_ratio = reduction_ratio_soc\n        if reduction_ratio >= LARGE_REDUCTION_RATIO:\n            trust_radius = max(TRUST_ENLARGEMENT_FACTOR_L * norm(d), trust_radius)\n        elif reduction_ratio >= INTERMEDIARY_REDUCTION_RATIO:\n            trust_radius = max(TRUST_ENLARGEMENT_FACTOR_S * norm(d), trust_radius)\n        elif reduction_ratio < SUFFICIENT_REDUCTION_RATIO:\n            trust_reduction = (1 - SUFFICIENT_REDUCTION_RATIO) / (1 - reduction_ratio)\n            new_trust_radius = trust_reduction * norm(d)\n            if new_trust_radius >= MAX_TRUST_REDUCTION * trust_radius:\n                trust_radius *= MAX_TRUST_REDUCTION\n            elif new_trust_radius >= MIN_TRUST_REDUCTION * trust_radius:\n                trust_radius = new_trust_radius\n            else:\n                trust_radius *= MIN_TRUST_REDUCTION\n        if reduction_ratio >= SUFFICIENT_REDUCTION_RATIO:\n            x = x_next\n            (f, b) = (f_next, b_next)\n            (c, A) = grad_and_jac(x)\n            S = scaling(x)\n            (Z, LS, Y) = projections(A, factorization_method)\n            v = -LS.dot(c)\n            H = lagr_hess(x, v)\n            last_iteration_failed = False\n            optimality = norm(c + A.T.dot(v), np.inf)\n            constr_violation = norm(b, np.inf) if len(b) > 0 else 0\n        else:\n            penalty = previous_penalty\n            last_iteration_failed = True\n    return (x, state)"
        ]
    }
]