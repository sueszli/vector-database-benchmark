[
    {
        "func_name": "momentum_updates",
        "original": "def momentum_updates(cost, params, lr=0.0001, mu=0.9):\n    grads = T.grad(cost, params)\n    velocities = [theano.shared(np.zeros_like(p.get_value()).astype(np.float32)) for p in params]\n    updates = []\n    for (p, v, g) in zip(params, velocities, grads):\n        newv = mu * v - lr * g\n        newp = p + newv\n        updates.append((p, newp))\n        updates.append((v, newv))\n    return updates",
        "mutated": [
            "def momentum_updates(cost, params, lr=0.0001, mu=0.9):\n    if False:\n        i = 10\n    grads = T.grad(cost, params)\n    velocities = [theano.shared(np.zeros_like(p.get_value()).astype(np.float32)) for p in params]\n    updates = []\n    for (p, v, g) in zip(params, velocities, grads):\n        newv = mu * v - lr * g\n        newp = p + newv\n        updates.append((p, newp))\n        updates.append((v, newv))\n    return updates",
            "def momentum_updates(cost, params, lr=0.0001, mu=0.9):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    grads = T.grad(cost, params)\n    velocities = [theano.shared(np.zeros_like(p.get_value()).astype(np.float32)) for p in params]\n    updates = []\n    for (p, v, g) in zip(params, velocities, grads):\n        newv = mu * v - lr * g\n        newp = p + newv\n        updates.append((p, newp))\n        updates.append((v, newv))\n    return updates",
            "def momentum_updates(cost, params, lr=0.0001, mu=0.9):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    grads = T.grad(cost, params)\n    velocities = [theano.shared(np.zeros_like(p.get_value()).astype(np.float32)) for p in params]\n    updates = []\n    for (p, v, g) in zip(params, velocities, grads):\n        newv = mu * v - lr * g\n        newp = p + newv\n        updates.append((p, newp))\n        updates.append((v, newv))\n    return updates",
            "def momentum_updates(cost, params, lr=0.0001, mu=0.9):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    grads = T.grad(cost, params)\n    velocities = [theano.shared(np.zeros_like(p.get_value()).astype(np.float32)) for p in params]\n    updates = []\n    for (p, v, g) in zip(params, velocities, grads):\n        newv = mu * v - lr * g\n        newp = p + newv\n        updates.append((p, newp))\n        updates.append((v, newv))\n    return updates",
            "def momentum_updates(cost, params, lr=0.0001, mu=0.9):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    grads = T.grad(cost, params)\n    velocities = [theano.shared(np.zeros_like(p.get_value()).astype(np.float32)) for p in params]\n    updates = []\n    for (p, v, g) in zip(params, velocities, grads):\n        newv = mu * v - lr * g\n        newp = p + newv\n        updates.append((p, newp))\n        updates.append((v, newv))\n    return updates"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, D, V, context_sz):\n    self.D = D\n    self.V = V\n    self.context_sz = context_sz",
        "mutated": [
            "def __init__(self, D, V, context_sz):\n    if False:\n        i = 10\n    self.D = D\n    self.V = V\n    self.context_sz = context_sz",
            "def __init__(self, D, V, context_sz):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.D = D\n    self.V = V\n    self.context_sz = context_sz",
            "def __init__(self, D, V, context_sz):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.D = D\n    self.V = V\n    self.context_sz = context_sz",
            "def __init__(self, D, V, context_sz):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.D = D\n    self.V = V\n    self.context_sz = context_sz",
            "def __init__(self, D, V, context_sz):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.D = D\n    self.V = V\n    self.context_sz = context_sz"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self, sentences, cc_matrix=None, learning_rate=0.0001, reg=0.1, xmax=100, alpha=0.75, epochs=10, gd=False, use_theano=False, use_tensorflow=False):\n    t0 = datetime.now()\n    V = self.V\n    D = self.D\n    if not os.path.exists(cc_matrix):\n        X = np.zeros((V, V))\n        N = len(sentences)\n        print('number of sentences to process:', N)\n        it = 0\n        for sentence in sentences:\n            it += 1\n            if it % 10000 == 0:\n                print('processed', it, '/', N)\n            n = len(sentence)\n            for i in range(n):\n                wi = sentence[i]\n                start = max(0, i - self.context_sz)\n                end = min(n, i + self.context_sz)\n                if i - self.context_sz < 0:\n                    points = 1.0 / (i + 1)\n                    X[wi, 0] += points\n                    X[0, wi] += points\n                if i + self.context_sz > n:\n                    points = 1.0 / (n - i)\n                    X[wi, 1] += points\n                    X[1, wi] += points\n                for j in range(start, i):\n                    wj = sentence[j]\n                    points = 1.0 / (i - j)\n                    X[wi, wj] += points\n                    X[wj, wi] += points\n                for j in range(i + 1, end):\n                    wj = sentence[j]\n                    points = 1.0 / (j - i)\n                    X[wi, wj] += points\n                    X[wj, wi] += points\n        np.save(cc_matrix, X)\n    else:\n        X = np.load(cc_matrix)\n    print('max in X:', X.max())\n    fX = np.zeros((V, V))\n    fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n    fX[X >= xmax] = 1\n    print('max in f(X):', fX.max())\n    logX = np.log(X + 1)\n    fX = fX.astype(np.float32)\n    logX = logX.astype(np.float32)\n    print('max in log(X):', logX.max())\n    print('time to build co-occurrence matrix:', datetime.now() - t0)\n    W = np.random.randn(V, D) / np.sqrt(V + D)\n    b = np.zeros(V)\n    U = np.random.randn(V, D) / np.sqrt(V + D)\n    c = np.zeros(V)\n    mu = logX.mean()\n    thW = theano.shared(W.astype(np.float32))\n    thb = theano.shared(b.astype(np.float32))\n    thU = theano.shared(U.astype(np.float32))\n    thc = theano.shared(c.astype(np.float32))\n    thLogX = T.matrix('logX')\n    thfX = T.matrix('fX')\n    params = [thW, thb, thU, thc]\n    thDelta = thW.dot(thU.T) + T.reshape(thb, (V, 1)) + T.reshape(thc, (1, V)) + mu - thLogX\n    thCost = (thfX * thDelta * thDelta).sum()\n    regularized_cost = thCost + reg * ((thW * thW).sum() + (thU * thU).sum())\n    updates = momentum_updates(regularized_cost, params, learning_rate)\n    train_op = theano.function(inputs=[thfX, thLogX], updates=updates)\n    cost_op = theano.function(inputs=[thfX, thLogX], outputs=thCost)\n    costs = []\n    sentence_indexes = range(len(sentences))\n    for epoch in range(epochs):\n        train_op(fX, logX)\n        cost = cost_op(fX, logX)\n        costs.append(cost)\n        print('epoch:', epoch, 'cost:', cost)\n    self.W = thW.get_value()\n    self.U = thU.get_value()\n    plt.plot(costs)\n    plt.show()",
        "mutated": [
            "def fit(self, sentences, cc_matrix=None, learning_rate=0.0001, reg=0.1, xmax=100, alpha=0.75, epochs=10, gd=False, use_theano=False, use_tensorflow=False):\n    if False:\n        i = 10\n    t0 = datetime.now()\n    V = self.V\n    D = self.D\n    if not os.path.exists(cc_matrix):\n        X = np.zeros((V, V))\n        N = len(sentences)\n        print('number of sentences to process:', N)\n        it = 0\n        for sentence in sentences:\n            it += 1\n            if it % 10000 == 0:\n                print('processed', it, '/', N)\n            n = len(sentence)\n            for i in range(n):\n                wi = sentence[i]\n                start = max(0, i - self.context_sz)\n                end = min(n, i + self.context_sz)\n                if i - self.context_sz < 0:\n                    points = 1.0 / (i + 1)\n                    X[wi, 0] += points\n                    X[0, wi] += points\n                if i + self.context_sz > n:\n                    points = 1.0 / (n - i)\n                    X[wi, 1] += points\n                    X[1, wi] += points\n                for j in range(start, i):\n                    wj = sentence[j]\n                    points = 1.0 / (i - j)\n                    X[wi, wj] += points\n                    X[wj, wi] += points\n                for j in range(i + 1, end):\n                    wj = sentence[j]\n                    points = 1.0 / (j - i)\n                    X[wi, wj] += points\n                    X[wj, wi] += points\n        np.save(cc_matrix, X)\n    else:\n        X = np.load(cc_matrix)\n    print('max in X:', X.max())\n    fX = np.zeros((V, V))\n    fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n    fX[X >= xmax] = 1\n    print('max in f(X):', fX.max())\n    logX = np.log(X + 1)\n    fX = fX.astype(np.float32)\n    logX = logX.astype(np.float32)\n    print('max in log(X):', logX.max())\n    print('time to build co-occurrence matrix:', datetime.now() - t0)\n    W = np.random.randn(V, D) / np.sqrt(V + D)\n    b = np.zeros(V)\n    U = np.random.randn(V, D) / np.sqrt(V + D)\n    c = np.zeros(V)\n    mu = logX.mean()\n    thW = theano.shared(W.astype(np.float32))\n    thb = theano.shared(b.astype(np.float32))\n    thU = theano.shared(U.astype(np.float32))\n    thc = theano.shared(c.astype(np.float32))\n    thLogX = T.matrix('logX')\n    thfX = T.matrix('fX')\n    params = [thW, thb, thU, thc]\n    thDelta = thW.dot(thU.T) + T.reshape(thb, (V, 1)) + T.reshape(thc, (1, V)) + mu - thLogX\n    thCost = (thfX * thDelta * thDelta).sum()\n    regularized_cost = thCost + reg * ((thW * thW).sum() + (thU * thU).sum())\n    updates = momentum_updates(regularized_cost, params, learning_rate)\n    train_op = theano.function(inputs=[thfX, thLogX], updates=updates)\n    cost_op = theano.function(inputs=[thfX, thLogX], outputs=thCost)\n    costs = []\n    sentence_indexes = range(len(sentences))\n    for epoch in range(epochs):\n        train_op(fX, logX)\n        cost = cost_op(fX, logX)\n        costs.append(cost)\n        print('epoch:', epoch, 'cost:', cost)\n    self.W = thW.get_value()\n    self.U = thU.get_value()\n    plt.plot(costs)\n    plt.show()",
            "def fit(self, sentences, cc_matrix=None, learning_rate=0.0001, reg=0.1, xmax=100, alpha=0.75, epochs=10, gd=False, use_theano=False, use_tensorflow=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    t0 = datetime.now()\n    V = self.V\n    D = self.D\n    if not os.path.exists(cc_matrix):\n        X = np.zeros((V, V))\n        N = len(sentences)\n        print('number of sentences to process:', N)\n        it = 0\n        for sentence in sentences:\n            it += 1\n            if it % 10000 == 0:\n                print('processed', it, '/', N)\n            n = len(sentence)\n            for i in range(n):\n                wi = sentence[i]\n                start = max(0, i - self.context_sz)\n                end = min(n, i + self.context_sz)\n                if i - self.context_sz < 0:\n                    points = 1.0 / (i + 1)\n                    X[wi, 0] += points\n                    X[0, wi] += points\n                if i + self.context_sz > n:\n                    points = 1.0 / (n - i)\n                    X[wi, 1] += points\n                    X[1, wi] += points\n                for j in range(start, i):\n                    wj = sentence[j]\n                    points = 1.0 / (i - j)\n                    X[wi, wj] += points\n                    X[wj, wi] += points\n                for j in range(i + 1, end):\n                    wj = sentence[j]\n                    points = 1.0 / (j - i)\n                    X[wi, wj] += points\n                    X[wj, wi] += points\n        np.save(cc_matrix, X)\n    else:\n        X = np.load(cc_matrix)\n    print('max in X:', X.max())\n    fX = np.zeros((V, V))\n    fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n    fX[X >= xmax] = 1\n    print('max in f(X):', fX.max())\n    logX = np.log(X + 1)\n    fX = fX.astype(np.float32)\n    logX = logX.astype(np.float32)\n    print('max in log(X):', logX.max())\n    print('time to build co-occurrence matrix:', datetime.now() - t0)\n    W = np.random.randn(V, D) / np.sqrt(V + D)\n    b = np.zeros(V)\n    U = np.random.randn(V, D) / np.sqrt(V + D)\n    c = np.zeros(V)\n    mu = logX.mean()\n    thW = theano.shared(W.astype(np.float32))\n    thb = theano.shared(b.astype(np.float32))\n    thU = theano.shared(U.astype(np.float32))\n    thc = theano.shared(c.astype(np.float32))\n    thLogX = T.matrix('logX')\n    thfX = T.matrix('fX')\n    params = [thW, thb, thU, thc]\n    thDelta = thW.dot(thU.T) + T.reshape(thb, (V, 1)) + T.reshape(thc, (1, V)) + mu - thLogX\n    thCost = (thfX * thDelta * thDelta).sum()\n    regularized_cost = thCost + reg * ((thW * thW).sum() + (thU * thU).sum())\n    updates = momentum_updates(regularized_cost, params, learning_rate)\n    train_op = theano.function(inputs=[thfX, thLogX], updates=updates)\n    cost_op = theano.function(inputs=[thfX, thLogX], outputs=thCost)\n    costs = []\n    sentence_indexes = range(len(sentences))\n    for epoch in range(epochs):\n        train_op(fX, logX)\n        cost = cost_op(fX, logX)\n        costs.append(cost)\n        print('epoch:', epoch, 'cost:', cost)\n    self.W = thW.get_value()\n    self.U = thU.get_value()\n    plt.plot(costs)\n    plt.show()",
            "def fit(self, sentences, cc_matrix=None, learning_rate=0.0001, reg=0.1, xmax=100, alpha=0.75, epochs=10, gd=False, use_theano=False, use_tensorflow=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    t0 = datetime.now()\n    V = self.V\n    D = self.D\n    if not os.path.exists(cc_matrix):\n        X = np.zeros((V, V))\n        N = len(sentences)\n        print('number of sentences to process:', N)\n        it = 0\n        for sentence in sentences:\n            it += 1\n            if it % 10000 == 0:\n                print('processed', it, '/', N)\n            n = len(sentence)\n            for i in range(n):\n                wi = sentence[i]\n                start = max(0, i - self.context_sz)\n                end = min(n, i + self.context_sz)\n                if i - self.context_sz < 0:\n                    points = 1.0 / (i + 1)\n                    X[wi, 0] += points\n                    X[0, wi] += points\n                if i + self.context_sz > n:\n                    points = 1.0 / (n - i)\n                    X[wi, 1] += points\n                    X[1, wi] += points\n                for j in range(start, i):\n                    wj = sentence[j]\n                    points = 1.0 / (i - j)\n                    X[wi, wj] += points\n                    X[wj, wi] += points\n                for j in range(i + 1, end):\n                    wj = sentence[j]\n                    points = 1.0 / (j - i)\n                    X[wi, wj] += points\n                    X[wj, wi] += points\n        np.save(cc_matrix, X)\n    else:\n        X = np.load(cc_matrix)\n    print('max in X:', X.max())\n    fX = np.zeros((V, V))\n    fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n    fX[X >= xmax] = 1\n    print('max in f(X):', fX.max())\n    logX = np.log(X + 1)\n    fX = fX.astype(np.float32)\n    logX = logX.astype(np.float32)\n    print('max in log(X):', logX.max())\n    print('time to build co-occurrence matrix:', datetime.now() - t0)\n    W = np.random.randn(V, D) / np.sqrt(V + D)\n    b = np.zeros(V)\n    U = np.random.randn(V, D) / np.sqrt(V + D)\n    c = np.zeros(V)\n    mu = logX.mean()\n    thW = theano.shared(W.astype(np.float32))\n    thb = theano.shared(b.astype(np.float32))\n    thU = theano.shared(U.astype(np.float32))\n    thc = theano.shared(c.astype(np.float32))\n    thLogX = T.matrix('logX')\n    thfX = T.matrix('fX')\n    params = [thW, thb, thU, thc]\n    thDelta = thW.dot(thU.T) + T.reshape(thb, (V, 1)) + T.reshape(thc, (1, V)) + mu - thLogX\n    thCost = (thfX * thDelta * thDelta).sum()\n    regularized_cost = thCost + reg * ((thW * thW).sum() + (thU * thU).sum())\n    updates = momentum_updates(regularized_cost, params, learning_rate)\n    train_op = theano.function(inputs=[thfX, thLogX], updates=updates)\n    cost_op = theano.function(inputs=[thfX, thLogX], outputs=thCost)\n    costs = []\n    sentence_indexes = range(len(sentences))\n    for epoch in range(epochs):\n        train_op(fX, logX)\n        cost = cost_op(fX, logX)\n        costs.append(cost)\n        print('epoch:', epoch, 'cost:', cost)\n    self.W = thW.get_value()\n    self.U = thU.get_value()\n    plt.plot(costs)\n    plt.show()",
            "def fit(self, sentences, cc_matrix=None, learning_rate=0.0001, reg=0.1, xmax=100, alpha=0.75, epochs=10, gd=False, use_theano=False, use_tensorflow=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    t0 = datetime.now()\n    V = self.V\n    D = self.D\n    if not os.path.exists(cc_matrix):\n        X = np.zeros((V, V))\n        N = len(sentences)\n        print('number of sentences to process:', N)\n        it = 0\n        for sentence in sentences:\n            it += 1\n            if it % 10000 == 0:\n                print('processed', it, '/', N)\n            n = len(sentence)\n            for i in range(n):\n                wi = sentence[i]\n                start = max(0, i - self.context_sz)\n                end = min(n, i + self.context_sz)\n                if i - self.context_sz < 0:\n                    points = 1.0 / (i + 1)\n                    X[wi, 0] += points\n                    X[0, wi] += points\n                if i + self.context_sz > n:\n                    points = 1.0 / (n - i)\n                    X[wi, 1] += points\n                    X[1, wi] += points\n                for j in range(start, i):\n                    wj = sentence[j]\n                    points = 1.0 / (i - j)\n                    X[wi, wj] += points\n                    X[wj, wi] += points\n                for j in range(i + 1, end):\n                    wj = sentence[j]\n                    points = 1.0 / (j - i)\n                    X[wi, wj] += points\n                    X[wj, wi] += points\n        np.save(cc_matrix, X)\n    else:\n        X = np.load(cc_matrix)\n    print('max in X:', X.max())\n    fX = np.zeros((V, V))\n    fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n    fX[X >= xmax] = 1\n    print('max in f(X):', fX.max())\n    logX = np.log(X + 1)\n    fX = fX.astype(np.float32)\n    logX = logX.astype(np.float32)\n    print('max in log(X):', logX.max())\n    print('time to build co-occurrence matrix:', datetime.now() - t0)\n    W = np.random.randn(V, D) / np.sqrt(V + D)\n    b = np.zeros(V)\n    U = np.random.randn(V, D) / np.sqrt(V + D)\n    c = np.zeros(V)\n    mu = logX.mean()\n    thW = theano.shared(W.astype(np.float32))\n    thb = theano.shared(b.astype(np.float32))\n    thU = theano.shared(U.astype(np.float32))\n    thc = theano.shared(c.astype(np.float32))\n    thLogX = T.matrix('logX')\n    thfX = T.matrix('fX')\n    params = [thW, thb, thU, thc]\n    thDelta = thW.dot(thU.T) + T.reshape(thb, (V, 1)) + T.reshape(thc, (1, V)) + mu - thLogX\n    thCost = (thfX * thDelta * thDelta).sum()\n    regularized_cost = thCost + reg * ((thW * thW).sum() + (thU * thU).sum())\n    updates = momentum_updates(regularized_cost, params, learning_rate)\n    train_op = theano.function(inputs=[thfX, thLogX], updates=updates)\n    cost_op = theano.function(inputs=[thfX, thLogX], outputs=thCost)\n    costs = []\n    sentence_indexes = range(len(sentences))\n    for epoch in range(epochs):\n        train_op(fX, logX)\n        cost = cost_op(fX, logX)\n        costs.append(cost)\n        print('epoch:', epoch, 'cost:', cost)\n    self.W = thW.get_value()\n    self.U = thU.get_value()\n    plt.plot(costs)\n    plt.show()",
            "def fit(self, sentences, cc_matrix=None, learning_rate=0.0001, reg=0.1, xmax=100, alpha=0.75, epochs=10, gd=False, use_theano=False, use_tensorflow=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    t0 = datetime.now()\n    V = self.V\n    D = self.D\n    if not os.path.exists(cc_matrix):\n        X = np.zeros((V, V))\n        N = len(sentences)\n        print('number of sentences to process:', N)\n        it = 0\n        for sentence in sentences:\n            it += 1\n            if it % 10000 == 0:\n                print('processed', it, '/', N)\n            n = len(sentence)\n            for i in range(n):\n                wi = sentence[i]\n                start = max(0, i - self.context_sz)\n                end = min(n, i + self.context_sz)\n                if i - self.context_sz < 0:\n                    points = 1.0 / (i + 1)\n                    X[wi, 0] += points\n                    X[0, wi] += points\n                if i + self.context_sz > n:\n                    points = 1.0 / (n - i)\n                    X[wi, 1] += points\n                    X[1, wi] += points\n                for j in range(start, i):\n                    wj = sentence[j]\n                    points = 1.0 / (i - j)\n                    X[wi, wj] += points\n                    X[wj, wi] += points\n                for j in range(i + 1, end):\n                    wj = sentence[j]\n                    points = 1.0 / (j - i)\n                    X[wi, wj] += points\n                    X[wj, wi] += points\n        np.save(cc_matrix, X)\n    else:\n        X = np.load(cc_matrix)\n    print('max in X:', X.max())\n    fX = np.zeros((V, V))\n    fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n    fX[X >= xmax] = 1\n    print('max in f(X):', fX.max())\n    logX = np.log(X + 1)\n    fX = fX.astype(np.float32)\n    logX = logX.astype(np.float32)\n    print('max in log(X):', logX.max())\n    print('time to build co-occurrence matrix:', datetime.now() - t0)\n    W = np.random.randn(V, D) / np.sqrt(V + D)\n    b = np.zeros(V)\n    U = np.random.randn(V, D) / np.sqrt(V + D)\n    c = np.zeros(V)\n    mu = logX.mean()\n    thW = theano.shared(W.astype(np.float32))\n    thb = theano.shared(b.astype(np.float32))\n    thU = theano.shared(U.astype(np.float32))\n    thc = theano.shared(c.astype(np.float32))\n    thLogX = T.matrix('logX')\n    thfX = T.matrix('fX')\n    params = [thW, thb, thU, thc]\n    thDelta = thW.dot(thU.T) + T.reshape(thb, (V, 1)) + T.reshape(thc, (1, V)) + mu - thLogX\n    thCost = (thfX * thDelta * thDelta).sum()\n    regularized_cost = thCost + reg * ((thW * thW).sum() + (thU * thU).sum())\n    updates = momentum_updates(regularized_cost, params, learning_rate)\n    train_op = theano.function(inputs=[thfX, thLogX], updates=updates)\n    cost_op = theano.function(inputs=[thfX, thLogX], outputs=thCost)\n    costs = []\n    sentence_indexes = range(len(sentences))\n    for epoch in range(epochs):\n        train_op(fX, logX)\n        cost = cost_op(fX, logX)\n        costs.append(cost)\n        print('epoch:', epoch, 'cost:', cost)\n    self.W = thW.get_value()\n    self.U = thU.get_value()\n    plt.plot(costs)\n    plt.show()"
        ]
    },
    {
        "func_name": "save",
        "original": "def save(self, fn):\n    arrays = [self.W, self.U.T]\n    np.savez(fn, *arrays)",
        "mutated": [
            "def save(self, fn):\n    if False:\n        i = 10\n    arrays = [self.W, self.U.T]\n    np.savez(fn, *arrays)",
            "def save(self, fn):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    arrays = [self.W, self.U.T]\n    np.savez(fn, *arrays)",
            "def save(self, fn):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    arrays = [self.W, self.U.T]\n    np.savez(fn, *arrays)",
            "def save(self, fn):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    arrays = [self.W, self.U.T]\n    np.savez(fn, *arrays)",
            "def save(self, fn):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    arrays = [self.W, self.U.T]\n    np.savez(fn, *arrays)"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(we_file, w2i_file, use_brown=True, n_files=50):\n    if use_brown:\n        cc_matrix = 'cc_matrix_brown.npy'\n    else:\n        cc_matrix = 'cc_matrix_%s.npy' % n_files\n    if os.path.exists(cc_matrix):\n        with open(w2i_file) as f:\n            word2idx = json.load(f)\n        sentences = []\n    else:\n        if use_brown:\n            keep_words = set(['king', 'man', 'woman', 'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england', 'french', 'english', 'japan', 'japanese', 'chinese', 'italian', 'australia', 'australian', 'december', 'november', 'june', 'january', 'february', 'march', 'april', 'may', 'july', 'august', 'september', 'october'])\n            (sentences, word2idx) = get_sentences_with_word2idx_limit_vocab(n_vocab=5000, keep_words=keep_words)\n        else:\n            (sentences, word2idx) = get_wikipedia_data(n_files=n_files, n_vocab=2000)\n        with open(w2i_file, 'w') as f:\n            json.dump(word2idx, f)\n    V = len(word2idx)\n    model = Glove(100, V, 10)\n    model.fit(sentences, cc_matrix=cc_matrix, learning_rate=0.0001, reg=0.1, epochs=200)\n    model.save(we_file)",
        "mutated": [
            "def main(we_file, w2i_file, use_brown=True, n_files=50):\n    if False:\n        i = 10\n    if use_brown:\n        cc_matrix = 'cc_matrix_brown.npy'\n    else:\n        cc_matrix = 'cc_matrix_%s.npy' % n_files\n    if os.path.exists(cc_matrix):\n        with open(w2i_file) as f:\n            word2idx = json.load(f)\n        sentences = []\n    else:\n        if use_brown:\n            keep_words = set(['king', 'man', 'woman', 'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england', 'french', 'english', 'japan', 'japanese', 'chinese', 'italian', 'australia', 'australian', 'december', 'november', 'june', 'january', 'february', 'march', 'april', 'may', 'july', 'august', 'september', 'october'])\n            (sentences, word2idx) = get_sentences_with_word2idx_limit_vocab(n_vocab=5000, keep_words=keep_words)\n        else:\n            (sentences, word2idx) = get_wikipedia_data(n_files=n_files, n_vocab=2000)\n        with open(w2i_file, 'w') as f:\n            json.dump(word2idx, f)\n    V = len(word2idx)\n    model = Glove(100, V, 10)\n    model.fit(sentences, cc_matrix=cc_matrix, learning_rate=0.0001, reg=0.1, epochs=200)\n    model.save(we_file)",
            "def main(we_file, w2i_file, use_brown=True, n_files=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if use_brown:\n        cc_matrix = 'cc_matrix_brown.npy'\n    else:\n        cc_matrix = 'cc_matrix_%s.npy' % n_files\n    if os.path.exists(cc_matrix):\n        with open(w2i_file) as f:\n            word2idx = json.load(f)\n        sentences = []\n    else:\n        if use_brown:\n            keep_words = set(['king', 'man', 'woman', 'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england', 'french', 'english', 'japan', 'japanese', 'chinese', 'italian', 'australia', 'australian', 'december', 'november', 'june', 'january', 'february', 'march', 'april', 'may', 'july', 'august', 'september', 'october'])\n            (sentences, word2idx) = get_sentences_with_word2idx_limit_vocab(n_vocab=5000, keep_words=keep_words)\n        else:\n            (sentences, word2idx) = get_wikipedia_data(n_files=n_files, n_vocab=2000)\n        with open(w2i_file, 'w') as f:\n            json.dump(word2idx, f)\n    V = len(word2idx)\n    model = Glove(100, V, 10)\n    model.fit(sentences, cc_matrix=cc_matrix, learning_rate=0.0001, reg=0.1, epochs=200)\n    model.save(we_file)",
            "def main(we_file, w2i_file, use_brown=True, n_files=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if use_brown:\n        cc_matrix = 'cc_matrix_brown.npy'\n    else:\n        cc_matrix = 'cc_matrix_%s.npy' % n_files\n    if os.path.exists(cc_matrix):\n        with open(w2i_file) as f:\n            word2idx = json.load(f)\n        sentences = []\n    else:\n        if use_brown:\n            keep_words = set(['king', 'man', 'woman', 'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england', 'french', 'english', 'japan', 'japanese', 'chinese', 'italian', 'australia', 'australian', 'december', 'november', 'june', 'january', 'february', 'march', 'april', 'may', 'july', 'august', 'september', 'october'])\n            (sentences, word2idx) = get_sentences_with_word2idx_limit_vocab(n_vocab=5000, keep_words=keep_words)\n        else:\n            (sentences, word2idx) = get_wikipedia_data(n_files=n_files, n_vocab=2000)\n        with open(w2i_file, 'w') as f:\n            json.dump(word2idx, f)\n    V = len(word2idx)\n    model = Glove(100, V, 10)\n    model.fit(sentences, cc_matrix=cc_matrix, learning_rate=0.0001, reg=0.1, epochs=200)\n    model.save(we_file)",
            "def main(we_file, w2i_file, use_brown=True, n_files=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if use_brown:\n        cc_matrix = 'cc_matrix_brown.npy'\n    else:\n        cc_matrix = 'cc_matrix_%s.npy' % n_files\n    if os.path.exists(cc_matrix):\n        with open(w2i_file) as f:\n            word2idx = json.load(f)\n        sentences = []\n    else:\n        if use_brown:\n            keep_words = set(['king', 'man', 'woman', 'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england', 'french', 'english', 'japan', 'japanese', 'chinese', 'italian', 'australia', 'australian', 'december', 'november', 'june', 'january', 'february', 'march', 'april', 'may', 'july', 'august', 'september', 'october'])\n            (sentences, word2idx) = get_sentences_with_word2idx_limit_vocab(n_vocab=5000, keep_words=keep_words)\n        else:\n            (sentences, word2idx) = get_wikipedia_data(n_files=n_files, n_vocab=2000)\n        with open(w2i_file, 'w') as f:\n            json.dump(word2idx, f)\n    V = len(word2idx)\n    model = Glove(100, V, 10)\n    model.fit(sentences, cc_matrix=cc_matrix, learning_rate=0.0001, reg=0.1, epochs=200)\n    model.save(we_file)",
            "def main(we_file, w2i_file, use_brown=True, n_files=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if use_brown:\n        cc_matrix = 'cc_matrix_brown.npy'\n    else:\n        cc_matrix = 'cc_matrix_%s.npy' % n_files\n    if os.path.exists(cc_matrix):\n        with open(w2i_file) as f:\n            word2idx = json.load(f)\n        sentences = []\n    else:\n        if use_brown:\n            keep_words = set(['king', 'man', 'woman', 'france', 'paris', 'london', 'rome', 'italy', 'britain', 'england', 'french', 'english', 'japan', 'japanese', 'chinese', 'italian', 'australia', 'australian', 'december', 'november', 'june', 'january', 'february', 'march', 'april', 'may', 'july', 'august', 'september', 'october'])\n            (sentences, word2idx) = get_sentences_with_word2idx_limit_vocab(n_vocab=5000, keep_words=keep_words)\n        else:\n            (sentences, word2idx) = get_wikipedia_data(n_files=n_files, n_vocab=2000)\n        with open(w2i_file, 'w') as f:\n            json.dump(word2idx, f)\n    V = len(word2idx)\n    model = Glove(100, V, 10)\n    model.fit(sentences, cc_matrix=cc_matrix, learning_rate=0.0001, reg=0.1, epochs=200)\n    model.save(we_file)"
        ]
    }
]