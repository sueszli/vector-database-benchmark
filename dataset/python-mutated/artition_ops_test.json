[
    {
        "func_name": "test_configs",
        "original": "def test_configs(self):\n    configs = [((10,), 3), ((4,), 10), ((10, 10), 4), ((100,), 2), ((5,), 1), ((1,), 1), ((2, 10), 2)]\n    suffixes = [[], [((2, 2), np.float32)], [((3,), np.int64), ((2,), np.float32)]]\n    return [(main_dims, parts, main_type, extra, pack) for (main_dims, parts) in configs for main_type in [np.int32, np.int64] for extra in suffixes for pack in [False, True]]",
        "mutated": [
            "def test_configs(self):\n    if False:\n        i = 10\n    configs = [((10,), 3), ((4,), 10), ((10, 10), 4), ((100,), 2), ((5,), 1), ((1,), 1), ((2, 10), 2)]\n    suffixes = [[], [((2, 2), np.float32)], [((3,), np.int64), ((2,), np.float32)]]\n    return [(main_dims, parts, main_type, extra, pack) for (main_dims, parts) in configs for main_type in [np.int32, np.int64] for extra in suffixes for pack in [False, True]]",
            "def test_configs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    configs = [((10,), 3), ((4,), 10), ((10, 10), 4), ((100,), 2), ((5,), 1), ((1,), 1), ((2, 10), 2)]\n    suffixes = [[], [((2, 2), np.float32)], [((3,), np.int64), ((2,), np.float32)]]\n    return [(main_dims, parts, main_type, extra, pack) for (main_dims, parts) in configs for main_type in [np.int32, np.int64] for extra in suffixes for pack in [False, True]]",
            "def test_configs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    configs = [((10,), 3), ((4,), 10), ((10, 10), 4), ((100,), 2), ((5,), 1), ((1,), 1), ((2, 10), 2)]\n    suffixes = [[], [((2, 2), np.float32)], [((3,), np.int64), ((2,), np.float32)]]\n    return [(main_dims, parts, main_type, extra, pack) for (main_dims, parts) in configs for main_type in [np.int32, np.int64] for extra in suffixes for pack in [False, True]]",
            "def test_configs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    configs = [((10,), 3), ((4,), 10), ((10, 10), 4), ((100,), 2), ((5,), 1), ((1,), 1), ((2, 10), 2)]\n    suffixes = [[], [((2, 2), np.float32)], [((3,), np.int64), ((2,), np.float32)]]\n    return [(main_dims, parts, main_type, extra, pack) for (main_dims, parts) in configs for main_type in [np.int32, np.int64] for extra in suffixes for pack in [False, True]]",
            "def test_configs(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    configs = [((10,), 3), ((4,), 10), ((10, 10), 4), ((100,), 2), ((5,), 1), ((1,), 1), ((2, 10), 2)]\n    suffixes = [[], [((2, 2), np.float32)], [((3,), np.int64), ((2,), np.float32)]]\n    return [(main_dims, parts, main_type, extra, pack) for (main_dims, parts) in configs for main_type in [np.int32, np.int64] for extra in suffixes for pack in [False, True]]"
        ]
    },
    {
        "func_name": "join",
        "original": "def join(a):\n    if not a:\n        return np.empty(shape=(0,) + suffix_shape)\n    return np.stack(a)",
        "mutated": [
            "def join(a):\n    if False:\n        i = 10\n    if not a:\n        return np.empty(shape=(0,) + suffix_shape)\n    return np.stack(a)",
            "def join(a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not a:\n        return np.empty(shape=(0,) + suffix_shape)\n    return np.stack(a)",
            "def join(a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not a:\n        return np.empty(shape=(0,) + suffix_shape)\n    return np.stack(a)",
            "def join(a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not a:\n        return np.empty(shape=(0,) + suffix_shape)\n    return np.stack(a)",
            "def join(a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not a:\n        return np.empty(shape=(0,) + suffix_shape)\n    return np.stack(a)"
        ]
    },
    {
        "func_name": "sharding",
        "original": "def sharding(x):\n    shards = (x[0] % parts).reshape([-1])\n    out = []\n    for i in range(parts):\n        for (ind, v) in enumerate(x):\n            suffix_shape = v.shape[len(x[0].shape):]\n            accum = []\n            data = v.reshape((-1,) + suffix_shape)\n            if pack and ind == 0:\n                data = data // parts\n            for (j, s) in enumerate(shards):\n                if s == i:\n                    accum.append(data[j])\n\n            def join(a):\n                if not a:\n                    return np.empty(shape=(0,) + suffix_shape)\n                return np.stack(a)\n            out.append(join(accum))\n    return out",
        "mutated": [
            "def sharding(x):\n    if False:\n        i = 10\n    shards = (x[0] % parts).reshape([-1])\n    out = []\n    for i in range(parts):\n        for (ind, v) in enumerate(x):\n            suffix_shape = v.shape[len(x[0].shape):]\n            accum = []\n            data = v.reshape((-1,) + suffix_shape)\n            if pack and ind == 0:\n                data = data // parts\n            for (j, s) in enumerate(shards):\n                if s == i:\n                    accum.append(data[j])\n\n            def join(a):\n                if not a:\n                    return np.empty(shape=(0,) + suffix_shape)\n                return np.stack(a)\n            out.append(join(accum))\n    return out",
            "def sharding(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    shards = (x[0] % parts).reshape([-1])\n    out = []\n    for i in range(parts):\n        for (ind, v) in enumerate(x):\n            suffix_shape = v.shape[len(x[0].shape):]\n            accum = []\n            data = v.reshape((-1,) + suffix_shape)\n            if pack and ind == 0:\n                data = data // parts\n            for (j, s) in enumerate(shards):\n                if s == i:\n                    accum.append(data[j])\n\n            def join(a):\n                if not a:\n                    return np.empty(shape=(0,) + suffix_shape)\n                return np.stack(a)\n            out.append(join(accum))\n    return out",
            "def sharding(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    shards = (x[0] % parts).reshape([-1])\n    out = []\n    for i in range(parts):\n        for (ind, v) in enumerate(x):\n            suffix_shape = v.shape[len(x[0].shape):]\n            accum = []\n            data = v.reshape((-1,) + suffix_shape)\n            if pack and ind == 0:\n                data = data // parts\n            for (j, s) in enumerate(shards):\n                if s == i:\n                    accum.append(data[j])\n\n            def join(a):\n                if not a:\n                    return np.empty(shape=(0,) + suffix_shape)\n                return np.stack(a)\n            out.append(join(accum))\n    return out",
            "def sharding(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    shards = (x[0] % parts).reshape([-1])\n    out = []\n    for i in range(parts):\n        for (ind, v) in enumerate(x):\n            suffix_shape = v.shape[len(x[0].shape):]\n            accum = []\n            data = v.reshape((-1,) + suffix_shape)\n            if pack and ind == 0:\n                data = data // parts\n            for (j, s) in enumerate(shards):\n                if s == i:\n                    accum.append(data[j])\n\n            def join(a):\n                if not a:\n                    return np.empty(shape=(0,) + suffix_shape)\n                return np.stack(a)\n            out.append(join(accum))\n    return out",
            "def sharding(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    shards = (x[0] % parts).reshape([-1])\n    out = []\n    for i in range(parts):\n        for (ind, v) in enumerate(x):\n            suffix_shape = v.shape[len(x[0].shape):]\n            accum = []\n            data = v.reshape((-1,) + suffix_shape)\n            if pack and ind == 0:\n                data = data // parts\n            for (j, s) in enumerate(shards):\n                if s == i:\n                    accum.append(data[j])\n\n            def join(a):\n                if not a:\n                    return np.empty(shape=(0,) + suffix_shape)\n                return np.stack(a)\n            out.append(join(accum))\n    return out"
        ]
    },
    {
        "func_name": "testPartition",
        "original": "def testPartition(self):\n    for (main_dims, parts, main_type, extra_ins, pack) in self.test_configs():\n        ins = ['in' + str(i) for i in range(1 + len(extra_ins))]\n        outs = ['in{}_p{}'.format(j, i) for i in range(parts) for j in range(1 + len(extra_ins))]\n        op = core.CreateOperator('Partition', ins, outs, pack_first_input=1 if pack else 0)\n        x = []\n        for (i, (dims, t)) in enumerate([((), main_type)] + extra_ins):\n            if t in [np.float32, np.float64]:\n                d = rand_array(*main_dims + dims)\n            else:\n                d = np.random.randint(-100, 100, main_dims + dims)\n            d = d.astype(t)\n            workspace.FeedBlob(ins[i], d)\n            x.append(d)\n\n        def sharding(x):\n            shards = (x[0] % parts).reshape([-1])\n            out = []\n            for i in range(parts):\n                for (ind, v) in enumerate(x):\n                    suffix_shape = v.shape[len(x[0].shape):]\n                    accum = []\n                    data = v.reshape((-1,) + suffix_shape)\n                    if pack and ind == 0:\n                        data = data // parts\n                    for (j, s) in enumerate(shards):\n                        if s == i:\n                            accum.append(data[j])\n\n                    def join(a):\n                        if not a:\n                            return np.empty(shape=(0,) + suffix_shape)\n                        return np.stack(a)\n                    out.append(join(accum))\n            return out\n        workspace.RunOperatorOnce(op)\n        ref = sharding(x)\n        print(x)\n        print(ref)\n        for (name, expected) in zip(outs, ref):\n            np.testing.assert_array_equal(expected, workspace.FetchBlob(name))\n        if len(main_dims) == 1:\n            for i in range(len(extra_ins)):\n                expected_out = ins[i + 1]\n                gather_ins = [ins[0]] + [outs[len(ins) * p + i + 1] for p in range(parts)]\n                actual_out = expected_out + '_actual'\n                op = core.CreateOperator('GatherByKey', gather_ins, actual_out)\n                workspace.RunOperatorOnce(op)\n                expected = workspace.FetchBlob(expected_out)\n                actual = workspace.FetchBlob(actual_out)\n                np.testing.assert_array_equal(expected, actual)",
        "mutated": [
            "def testPartition(self):\n    if False:\n        i = 10\n    for (main_dims, parts, main_type, extra_ins, pack) in self.test_configs():\n        ins = ['in' + str(i) for i in range(1 + len(extra_ins))]\n        outs = ['in{}_p{}'.format(j, i) for i in range(parts) for j in range(1 + len(extra_ins))]\n        op = core.CreateOperator('Partition', ins, outs, pack_first_input=1 if pack else 0)\n        x = []\n        for (i, (dims, t)) in enumerate([((), main_type)] + extra_ins):\n            if t in [np.float32, np.float64]:\n                d = rand_array(*main_dims + dims)\n            else:\n                d = np.random.randint(-100, 100, main_dims + dims)\n            d = d.astype(t)\n            workspace.FeedBlob(ins[i], d)\n            x.append(d)\n\n        def sharding(x):\n            shards = (x[0] % parts).reshape([-1])\n            out = []\n            for i in range(parts):\n                for (ind, v) in enumerate(x):\n                    suffix_shape = v.shape[len(x[0].shape):]\n                    accum = []\n                    data = v.reshape((-1,) + suffix_shape)\n                    if pack and ind == 0:\n                        data = data // parts\n                    for (j, s) in enumerate(shards):\n                        if s == i:\n                            accum.append(data[j])\n\n                    def join(a):\n                        if not a:\n                            return np.empty(shape=(0,) + suffix_shape)\n                        return np.stack(a)\n                    out.append(join(accum))\n            return out\n        workspace.RunOperatorOnce(op)\n        ref = sharding(x)\n        print(x)\n        print(ref)\n        for (name, expected) in zip(outs, ref):\n            np.testing.assert_array_equal(expected, workspace.FetchBlob(name))\n        if len(main_dims) == 1:\n            for i in range(len(extra_ins)):\n                expected_out = ins[i + 1]\n                gather_ins = [ins[0]] + [outs[len(ins) * p + i + 1] for p in range(parts)]\n                actual_out = expected_out + '_actual'\n                op = core.CreateOperator('GatherByKey', gather_ins, actual_out)\n                workspace.RunOperatorOnce(op)\n                expected = workspace.FetchBlob(expected_out)\n                actual = workspace.FetchBlob(actual_out)\n                np.testing.assert_array_equal(expected, actual)",
            "def testPartition(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for (main_dims, parts, main_type, extra_ins, pack) in self.test_configs():\n        ins = ['in' + str(i) for i in range(1 + len(extra_ins))]\n        outs = ['in{}_p{}'.format(j, i) for i in range(parts) for j in range(1 + len(extra_ins))]\n        op = core.CreateOperator('Partition', ins, outs, pack_first_input=1 if pack else 0)\n        x = []\n        for (i, (dims, t)) in enumerate([((), main_type)] + extra_ins):\n            if t in [np.float32, np.float64]:\n                d = rand_array(*main_dims + dims)\n            else:\n                d = np.random.randint(-100, 100, main_dims + dims)\n            d = d.astype(t)\n            workspace.FeedBlob(ins[i], d)\n            x.append(d)\n\n        def sharding(x):\n            shards = (x[0] % parts).reshape([-1])\n            out = []\n            for i in range(parts):\n                for (ind, v) in enumerate(x):\n                    suffix_shape = v.shape[len(x[0].shape):]\n                    accum = []\n                    data = v.reshape((-1,) + suffix_shape)\n                    if pack and ind == 0:\n                        data = data // parts\n                    for (j, s) in enumerate(shards):\n                        if s == i:\n                            accum.append(data[j])\n\n                    def join(a):\n                        if not a:\n                            return np.empty(shape=(0,) + suffix_shape)\n                        return np.stack(a)\n                    out.append(join(accum))\n            return out\n        workspace.RunOperatorOnce(op)\n        ref = sharding(x)\n        print(x)\n        print(ref)\n        for (name, expected) in zip(outs, ref):\n            np.testing.assert_array_equal(expected, workspace.FetchBlob(name))\n        if len(main_dims) == 1:\n            for i in range(len(extra_ins)):\n                expected_out = ins[i + 1]\n                gather_ins = [ins[0]] + [outs[len(ins) * p + i + 1] for p in range(parts)]\n                actual_out = expected_out + '_actual'\n                op = core.CreateOperator('GatherByKey', gather_ins, actual_out)\n                workspace.RunOperatorOnce(op)\n                expected = workspace.FetchBlob(expected_out)\n                actual = workspace.FetchBlob(actual_out)\n                np.testing.assert_array_equal(expected, actual)",
            "def testPartition(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for (main_dims, parts, main_type, extra_ins, pack) in self.test_configs():\n        ins = ['in' + str(i) for i in range(1 + len(extra_ins))]\n        outs = ['in{}_p{}'.format(j, i) for i in range(parts) for j in range(1 + len(extra_ins))]\n        op = core.CreateOperator('Partition', ins, outs, pack_first_input=1 if pack else 0)\n        x = []\n        for (i, (dims, t)) in enumerate([((), main_type)] + extra_ins):\n            if t in [np.float32, np.float64]:\n                d = rand_array(*main_dims + dims)\n            else:\n                d = np.random.randint(-100, 100, main_dims + dims)\n            d = d.astype(t)\n            workspace.FeedBlob(ins[i], d)\n            x.append(d)\n\n        def sharding(x):\n            shards = (x[0] % parts).reshape([-1])\n            out = []\n            for i in range(parts):\n                for (ind, v) in enumerate(x):\n                    suffix_shape = v.shape[len(x[0].shape):]\n                    accum = []\n                    data = v.reshape((-1,) + suffix_shape)\n                    if pack and ind == 0:\n                        data = data // parts\n                    for (j, s) in enumerate(shards):\n                        if s == i:\n                            accum.append(data[j])\n\n                    def join(a):\n                        if not a:\n                            return np.empty(shape=(0,) + suffix_shape)\n                        return np.stack(a)\n                    out.append(join(accum))\n            return out\n        workspace.RunOperatorOnce(op)\n        ref = sharding(x)\n        print(x)\n        print(ref)\n        for (name, expected) in zip(outs, ref):\n            np.testing.assert_array_equal(expected, workspace.FetchBlob(name))\n        if len(main_dims) == 1:\n            for i in range(len(extra_ins)):\n                expected_out = ins[i + 1]\n                gather_ins = [ins[0]] + [outs[len(ins) * p + i + 1] for p in range(parts)]\n                actual_out = expected_out + '_actual'\n                op = core.CreateOperator('GatherByKey', gather_ins, actual_out)\n                workspace.RunOperatorOnce(op)\n                expected = workspace.FetchBlob(expected_out)\n                actual = workspace.FetchBlob(actual_out)\n                np.testing.assert_array_equal(expected, actual)",
            "def testPartition(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for (main_dims, parts, main_type, extra_ins, pack) in self.test_configs():\n        ins = ['in' + str(i) for i in range(1 + len(extra_ins))]\n        outs = ['in{}_p{}'.format(j, i) for i in range(parts) for j in range(1 + len(extra_ins))]\n        op = core.CreateOperator('Partition', ins, outs, pack_first_input=1 if pack else 0)\n        x = []\n        for (i, (dims, t)) in enumerate([((), main_type)] + extra_ins):\n            if t in [np.float32, np.float64]:\n                d = rand_array(*main_dims + dims)\n            else:\n                d = np.random.randint(-100, 100, main_dims + dims)\n            d = d.astype(t)\n            workspace.FeedBlob(ins[i], d)\n            x.append(d)\n\n        def sharding(x):\n            shards = (x[0] % parts).reshape([-1])\n            out = []\n            for i in range(parts):\n                for (ind, v) in enumerate(x):\n                    suffix_shape = v.shape[len(x[0].shape):]\n                    accum = []\n                    data = v.reshape((-1,) + suffix_shape)\n                    if pack and ind == 0:\n                        data = data // parts\n                    for (j, s) in enumerate(shards):\n                        if s == i:\n                            accum.append(data[j])\n\n                    def join(a):\n                        if not a:\n                            return np.empty(shape=(0,) + suffix_shape)\n                        return np.stack(a)\n                    out.append(join(accum))\n            return out\n        workspace.RunOperatorOnce(op)\n        ref = sharding(x)\n        print(x)\n        print(ref)\n        for (name, expected) in zip(outs, ref):\n            np.testing.assert_array_equal(expected, workspace.FetchBlob(name))\n        if len(main_dims) == 1:\n            for i in range(len(extra_ins)):\n                expected_out = ins[i + 1]\n                gather_ins = [ins[0]] + [outs[len(ins) * p + i + 1] for p in range(parts)]\n                actual_out = expected_out + '_actual'\n                op = core.CreateOperator('GatherByKey', gather_ins, actual_out)\n                workspace.RunOperatorOnce(op)\n                expected = workspace.FetchBlob(expected_out)\n                actual = workspace.FetchBlob(actual_out)\n                np.testing.assert_array_equal(expected, actual)",
            "def testPartition(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for (main_dims, parts, main_type, extra_ins, pack) in self.test_configs():\n        ins = ['in' + str(i) for i in range(1 + len(extra_ins))]\n        outs = ['in{}_p{}'.format(j, i) for i in range(parts) for j in range(1 + len(extra_ins))]\n        op = core.CreateOperator('Partition', ins, outs, pack_first_input=1 if pack else 0)\n        x = []\n        for (i, (dims, t)) in enumerate([((), main_type)] + extra_ins):\n            if t in [np.float32, np.float64]:\n                d = rand_array(*main_dims + dims)\n            else:\n                d = np.random.randint(-100, 100, main_dims + dims)\n            d = d.astype(t)\n            workspace.FeedBlob(ins[i], d)\n            x.append(d)\n\n        def sharding(x):\n            shards = (x[0] % parts).reshape([-1])\n            out = []\n            for i in range(parts):\n                for (ind, v) in enumerate(x):\n                    suffix_shape = v.shape[len(x[0].shape):]\n                    accum = []\n                    data = v.reshape((-1,) + suffix_shape)\n                    if pack and ind == 0:\n                        data = data // parts\n                    for (j, s) in enumerate(shards):\n                        if s == i:\n                            accum.append(data[j])\n\n                    def join(a):\n                        if not a:\n                            return np.empty(shape=(0,) + suffix_shape)\n                        return np.stack(a)\n                    out.append(join(accum))\n            return out\n        workspace.RunOperatorOnce(op)\n        ref = sharding(x)\n        print(x)\n        print(ref)\n        for (name, expected) in zip(outs, ref):\n            np.testing.assert_array_equal(expected, workspace.FetchBlob(name))\n        if len(main_dims) == 1:\n            for i in range(len(extra_ins)):\n                expected_out = ins[i + 1]\n                gather_ins = [ins[0]] + [outs[len(ins) * p + i + 1] for p in range(parts)]\n                actual_out = expected_out + '_actual'\n                op = core.CreateOperator('GatherByKey', gather_ins, actual_out)\n                workspace.RunOperatorOnce(op)\n                expected = workspace.FetchBlob(expected_out)\n                actual = workspace.FetchBlob(actual_out)\n                np.testing.assert_array_equal(expected, actual)"
        ]
    },
    {
        "func_name": "join",
        "original": "def join(a):\n    if not a:\n        return np.empty(shape=(0,) + suffix_shape)\n    return np.stack(a)",
        "mutated": [
            "def join(a):\n    if False:\n        i = 10\n    if not a:\n        return np.empty(shape=(0,) + suffix_shape)\n    return np.stack(a)",
            "def join(a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not a:\n        return np.empty(shape=(0,) + suffix_shape)\n    return np.stack(a)",
            "def join(a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not a:\n        return np.empty(shape=(0,) + suffix_shape)\n    return np.stack(a)",
            "def join(a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not a:\n        return np.empty(shape=(0,) + suffix_shape)\n    return np.stack(a)",
            "def join(a):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not a:\n        return np.empty(shape=(0,) + suffix_shape)\n    return np.stack(a)"
        ]
    },
    {
        "func_name": "sharding",
        "original": "def sharding(x):\n    shards = (x[0] % parts).reshape([-1])\n    out = []\n    for i in range(parts):\n        idx = 0\n        sharded_lengths = np.zeros(elements)\n        for (ind, length) in enumerate(lengths):\n            for _ in range(length):\n                if shards[idx] == i:\n                    sharded_lengths[ind] += 1\n                idx += 1\n        out.append(sharded_lengths)\n        for (ind, v) in enumerate(x):\n            suffix_shape = v.shape[len(x[0].shape):]\n            accum = []\n            data = v.reshape((-1,) + suffix_shape)\n            if pack and ind == 0:\n                data = data // parts\n            for (j, s) in enumerate(shards):\n                if s == i:\n                    accum.append(data[j])\n\n            def join(a):\n                if not a:\n                    return np.empty(shape=(0,) + suffix_shape)\n                return np.stack(a)\n            out.append(join(accum))\n    return out",
        "mutated": [
            "def sharding(x):\n    if False:\n        i = 10\n    shards = (x[0] % parts).reshape([-1])\n    out = []\n    for i in range(parts):\n        idx = 0\n        sharded_lengths = np.zeros(elements)\n        for (ind, length) in enumerate(lengths):\n            for _ in range(length):\n                if shards[idx] == i:\n                    sharded_lengths[ind] += 1\n                idx += 1\n        out.append(sharded_lengths)\n        for (ind, v) in enumerate(x):\n            suffix_shape = v.shape[len(x[0].shape):]\n            accum = []\n            data = v.reshape((-1,) + suffix_shape)\n            if pack and ind == 0:\n                data = data // parts\n            for (j, s) in enumerate(shards):\n                if s == i:\n                    accum.append(data[j])\n\n            def join(a):\n                if not a:\n                    return np.empty(shape=(0,) + suffix_shape)\n                return np.stack(a)\n            out.append(join(accum))\n    return out",
            "def sharding(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    shards = (x[0] % parts).reshape([-1])\n    out = []\n    for i in range(parts):\n        idx = 0\n        sharded_lengths = np.zeros(elements)\n        for (ind, length) in enumerate(lengths):\n            for _ in range(length):\n                if shards[idx] == i:\n                    sharded_lengths[ind] += 1\n                idx += 1\n        out.append(sharded_lengths)\n        for (ind, v) in enumerate(x):\n            suffix_shape = v.shape[len(x[0].shape):]\n            accum = []\n            data = v.reshape((-1,) + suffix_shape)\n            if pack and ind == 0:\n                data = data // parts\n            for (j, s) in enumerate(shards):\n                if s == i:\n                    accum.append(data[j])\n\n            def join(a):\n                if not a:\n                    return np.empty(shape=(0,) + suffix_shape)\n                return np.stack(a)\n            out.append(join(accum))\n    return out",
            "def sharding(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    shards = (x[0] % parts).reshape([-1])\n    out = []\n    for i in range(parts):\n        idx = 0\n        sharded_lengths = np.zeros(elements)\n        for (ind, length) in enumerate(lengths):\n            for _ in range(length):\n                if shards[idx] == i:\n                    sharded_lengths[ind] += 1\n                idx += 1\n        out.append(sharded_lengths)\n        for (ind, v) in enumerate(x):\n            suffix_shape = v.shape[len(x[0].shape):]\n            accum = []\n            data = v.reshape((-1,) + suffix_shape)\n            if pack and ind == 0:\n                data = data // parts\n            for (j, s) in enumerate(shards):\n                if s == i:\n                    accum.append(data[j])\n\n            def join(a):\n                if not a:\n                    return np.empty(shape=(0,) + suffix_shape)\n                return np.stack(a)\n            out.append(join(accum))\n    return out",
            "def sharding(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    shards = (x[0] % parts).reshape([-1])\n    out = []\n    for i in range(parts):\n        idx = 0\n        sharded_lengths = np.zeros(elements)\n        for (ind, length) in enumerate(lengths):\n            for _ in range(length):\n                if shards[idx] == i:\n                    sharded_lengths[ind] += 1\n                idx += 1\n        out.append(sharded_lengths)\n        for (ind, v) in enumerate(x):\n            suffix_shape = v.shape[len(x[0].shape):]\n            accum = []\n            data = v.reshape((-1,) + suffix_shape)\n            if pack and ind == 0:\n                data = data // parts\n            for (j, s) in enumerate(shards):\n                if s == i:\n                    accum.append(data[j])\n\n            def join(a):\n                if not a:\n                    return np.empty(shape=(0,) + suffix_shape)\n                return np.stack(a)\n            out.append(join(accum))\n    return out",
            "def sharding(x):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    shards = (x[0] % parts).reshape([-1])\n    out = []\n    for i in range(parts):\n        idx = 0\n        sharded_lengths = np.zeros(elements)\n        for (ind, length) in enumerate(lengths):\n            for _ in range(length):\n                if shards[idx] == i:\n                    sharded_lengths[ind] += 1\n                idx += 1\n        out.append(sharded_lengths)\n        for (ind, v) in enumerate(x):\n            suffix_shape = v.shape[len(x[0].shape):]\n            accum = []\n            data = v.reshape((-1,) + suffix_shape)\n            if pack and ind == 0:\n                data = data // parts\n            for (j, s) in enumerate(shards):\n                if s == i:\n                    accum.append(data[j])\n\n            def join(a):\n                if not a:\n                    return np.empty(shape=(0,) + suffix_shape)\n                return np.stack(a)\n            out.append(join(accum))\n    return out"
        ]
    },
    {
        "func_name": "testLengthsPartition",
        "original": "def testLengthsPartition(self):\n    for (main_dims, parts, main_type, extra_ins, pack) in self.test_configs():\n        if len(main_dims) > 1:\n            continue\n        ins = ['in' + str(i) for i in range(2 + len(extra_ins))]\n        outs = ['in{}_p{}'.format(j, i) for i in range(parts) for j in range(2 + len(extra_ins))]\n        op = core.CreateOperator('LengthsPartition', ins, outs, pack_first_input=1 if pack else 0)\n        x = []\n        for (i, (dims, t)) in enumerate([((), main_type)] + extra_ins):\n            if t in [np.float32, np.float64]:\n                d = rand_array(*main_dims + dims)\n            else:\n                d = np.random.randint(-100, 100, main_dims + dims)\n            d = d.astype(t)\n            workspace.FeedBlob(ins[i + 1], d)\n            x.append(d)\n        elements = np.random.randint(2, 10)\n        lengths = []\n        total_length = 0\n        for _ in range(elements - 1):\n            lengths.append(np.random.randint(main_dims[0] - total_length))\n            total_length += lengths[-1]\n        lengths.append(main_dims[0] - total_length)\n        workspace.FeedBlob(ins[0], np.array(lengths, dtype=np.int32))\n\n        def sharding(x):\n            shards = (x[0] % parts).reshape([-1])\n            out = []\n            for i in range(parts):\n                idx = 0\n                sharded_lengths = np.zeros(elements)\n                for (ind, length) in enumerate(lengths):\n                    for _ in range(length):\n                        if shards[idx] == i:\n                            sharded_lengths[ind] += 1\n                        idx += 1\n                out.append(sharded_lengths)\n                for (ind, v) in enumerate(x):\n                    suffix_shape = v.shape[len(x[0].shape):]\n                    accum = []\n                    data = v.reshape((-1,) + suffix_shape)\n                    if pack and ind == 0:\n                        data = data // parts\n                    for (j, s) in enumerate(shards):\n                        if s == i:\n                            accum.append(data[j])\n\n                    def join(a):\n                        if not a:\n                            return np.empty(shape=(0,) + suffix_shape)\n                        return np.stack(a)\n                    out.append(join(accum))\n            return out\n        workspace.RunOperatorOnce(op)\n        ref = sharding(x)\n        for (name, expected) in zip(outs, ref):\n            np.testing.assert_array_equal(expected, workspace.FetchBlob(name))",
        "mutated": [
            "def testLengthsPartition(self):\n    if False:\n        i = 10\n    for (main_dims, parts, main_type, extra_ins, pack) in self.test_configs():\n        if len(main_dims) > 1:\n            continue\n        ins = ['in' + str(i) for i in range(2 + len(extra_ins))]\n        outs = ['in{}_p{}'.format(j, i) for i in range(parts) for j in range(2 + len(extra_ins))]\n        op = core.CreateOperator('LengthsPartition', ins, outs, pack_first_input=1 if pack else 0)\n        x = []\n        for (i, (dims, t)) in enumerate([((), main_type)] + extra_ins):\n            if t in [np.float32, np.float64]:\n                d = rand_array(*main_dims + dims)\n            else:\n                d = np.random.randint(-100, 100, main_dims + dims)\n            d = d.astype(t)\n            workspace.FeedBlob(ins[i + 1], d)\n            x.append(d)\n        elements = np.random.randint(2, 10)\n        lengths = []\n        total_length = 0\n        for _ in range(elements - 1):\n            lengths.append(np.random.randint(main_dims[0] - total_length))\n            total_length += lengths[-1]\n        lengths.append(main_dims[0] - total_length)\n        workspace.FeedBlob(ins[0], np.array(lengths, dtype=np.int32))\n\n        def sharding(x):\n            shards = (x[0] % parts).reshape([-1])\n            out = []\n            for i in range(parts):\n                idx = 0\n                sharded_lengths = np.zeros(elements)\n                for (ind, length) in enumerate(lengths):\n                    for _ in range(length):\n                        if shards[idx] == i:\n                            sharded_lengths[ind] += 1\n                        idx += 1\n                out.append(sharded_lengths)\n                for (ind, v) in enumerate(x):\n                    suffix_shape = v.shape[len(x[0].shape):]\n                    accum = []\n                    data = v.reshape((-1,) + suffix_shape)\n                    if pack and ind == 0:\n                        data = data // parts\n                    for (j, s) in enumerate(shards):\n                        if s == i:\n                            accum.append(data[j])\n\n                    def join(a):\n                        if not a:\n                            return np.empty(shape=(0,) + suffix_shape)\n                        return np.stack(a)\n                    out.append(join(accum))\n            return out\n        workspace.RunOperatorOnce(op)\n        ref = sharding(x)\n        for (name, expected) in zip(outs, ref):\n            np.testing.assert_array_equal(expected, workspace.FetchBlob(name))",
            "def testLengthsPartition(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for (main_dims, parts, main_type, extra_ins, pack) in self.test_configs():\n        if len(main_dims) > 1:\n            continue\n        ins = ['in' + str(i) for i in range(2 + len(extra_ins))]\n        outs = ['in{}_p{}'.format(j, i) for i in range(parts) for j in range(2 + len(extra_ins))]\n        op = core.CreateOperator('LengthsPartition', ins, outs, pack_first_input=1 if pack else 0)\n        x = []\n        for (i, (dims, t)) in enumerate([((), main_type)] + extra_ins):\n            if t in [np.float32, np.float64]:\n                d = rand_array(*main_dims + dims)\n            else:\n                d = np.random.randint(-100, 100, main_dims + dims)\n            d = d.astype(t)\n            workspace.FeedBlob(ins[i + 1], d)\n            x.append(d)\n        elements = np.random.randint(2, 10)\n        lengths = []\n        total_length = 0\n        for _ in range(elements - 1):\n            lengths.append(np.random.randint(main_dims[0] - total_length))\n            total_length += lengths[-1]\n        lengths.append(main_dims[0] - total_length)\n        workspace.FeedBlob(ins[0], np.array(lengths, dtype=np.int32))\n\n        def sharding(x):\n            shards = (x[0] % parts).reshape([-1])\n            out = []\n            for i in range(parts):\n                idx = 0\n                sharded_lengths = np.zeros(elements)\n                for (ind, length) in enumerate(lengths):\n                    for _ in range(length):\n                        if shards[idx] == i:\n                            sharded_lengths[ind] += 1\n                        idx += 1\n                out.append(sharded_lengths)\n                for (ind, v) in enumerate(x):\n                    suffix_shape = v.shape[len(x[0].shape):]\n                    accum = []\n                    data = v.reshape((-1,) + suffix_shape)\n                    if pack and ind == 0:\n                        data = data // parts\n                    for (j, s) in enumerate(shards):\n                        if s == i:\n                            accum.append(data[j])\n\n                    def join(a):\n                        if not a:\n                            return np.empty(shape=(0,) + suffix_shape)\n                        return np.stack(a)\n                    out.append(join(accum))\n            return out\n        workspace.RunOperatorOnce(op)\n        ref = sharding(x)\n        for (name, expected) in zip(outs, ref):\n            np.testing.assert_array_equal(expected, workspace.FetchBlob(name))",
            "def testLengthsPartition(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for (main_dims, parts, main_type, extra_ins, pack) in self.test_configs():\n        if len(main_dims) > 1:\n            continue\n        ins = ['in' + str(i) for i in range(2 + len(extra_ins))]\n        outs = ['in{}_p{}'.format(j, i) for i in range(parts) for j in range(2 + len(extra_ins))]\n        op = core.CreateOperator('LengthsPartition', ins, outs, pack_first_input=1 if pack else 0)\n        x = []\n        for (i, (dims, t)) in enumerate([((), main_type)] + extra_ins):\n            if t in [np.float32, np.float64]:\n                d = rand_array(*main_dims + dims)\n            else:\n                d = np.random.randint(-100, 100, main_dims + dims)\n            d = d.astype(t)\n            workspace.FeedBlob(ins[i + 1], d)\n            x.append(d)\n        elements = np.random.randint(2, 10)\n        lengths = []\n        total_length = 0\n        for _ in range(elements - 1):\n            lengths.append(np.random.randint(main_dims[0] - total_length))\n            total_length += lengths[-1]\n        lengths.append(main_dims[0] - total_length)\n        workspace.FeedBlob(ins[0], np.array(lengths, dtype=np.int32))\n\n        def sharding(x):\n            shards = (x[0] % parts).reshape([-1])\n            out = []\n            for i in range(parts):\n                idx = 0\n                sharded_lengths = np.zeros(elements)\n                for (ind, length) in enumerate(lengths):\n                    for _ in range(length):\n                        if shards[idx] == i:\n                            sharded_lengths[ind] += 1\n                        idx += 1\n                out.append(sharded_lengths)\n                for (ind, v) in enumerate(x):\n                    suffix_shape = v.shape[len(x[0].shape):]\n                    accum = []\n                    data = v.reshape((-1,) + suffix_shape)\n                    if pack and ind == 0:\n                        data = data // parts\n                    for (j, s) in enumerate(shards):\n                        if s == i:\n                            accum.append(data[j])\n\n                    def join(a):\n                        if not a:\n                            return np.empty(shape=(0,) + suffix_shape)\n                        return np.stack(a)\n                    out.append(join(accum))\n            return out\n        workspace.RunOperatorOnce(op)\n        ref = sharding(x)\n        for (name, expected) in zip(outs, ref):\n            np.testing.assert_array_equal(expected, workspace.FetchBlob(name))",
            "def testLengthsPartition(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for (main_dims, parts, main_type, extra_ins, pack) in self.test_configs():\n        if len(main_dims) > 1:\n            continue\n        ins = ['in' + str(i) for i in range(2 + len(extra_ins))]\n        outs = ['in{}_p{}'.format(j, i) for i in range(parts) for j in range(2 + len(extra_ins))]\n        op = core.CreateOperator('LengthsPartition', ins, outs, pack_first_input=1 if pack else 0)\n        x = []\n        for (i, (dims, t)) in enumerate([((), main_type)] + extra_ins):\n            if t in [np.float32, np.float64]:\n                d = rand_array(*main_dims + dims)\n            else:\n                d = np.random.randint(-100, 100, main_dims + dims)\n            d = d.astype(t)\n            workspace.FeedBlob(ins[i + 1], d)\n            x.append(d)\n        elements = np.random.randint(2, 10)\n        lengths = []\n        total_length = 0\n        for _ in range(elements - 1):\n            lengths.append(np.random.randint(main_dims[0] - total_length))\n            total_length += lengths[-1]\n        lengths.append(main_dims[0] - total_length)\n        workspace.FeedBlob(ins[0], np.array(lengths, dtype=np.int32))\n\n        def sharding(x):\n            shards = (x[0] % parts).reshape([-1])\n            out = []\n            for i in range(parts):\n                idx = 0\n                sharded_lengths = np.zeros(elements)\n                for (ind, length) in enumerate(lengths):\n                    for _ in range(length):\n                        if shards[idx] == i:\n                            sharded_lengths[ind] += 1\n                        idx += 1\n                out.append(sharded_lengths)\n                for (ind, v) in enumerate(x):\n                    suffix_shape = v.shape[len(x[0].shape):]\n                    accum = []\n                    data = v.reshape((-1,) + suffix_shape)\n                    if pack and ind == 0:\n                        data = data // parts\n                    for (j, s) in enumerate(shards):\n                        if s == i:\n                            accum.append(data[j])\n\n                    def join(a):\n                        if not a:\n                            return np.empty(shape=(0,) + suffix_shape)\n                        return np.stack(a)\n                    out.append(join(accum))\n            return out\n        workspace.RunOperatorOnce(op)\n        ref = sharding(x)\n        for (name, expected) in zip(outs, ref):\n            np.testing.assert_array_equal(expected, workspace.FetchBlob(name))",
            "def testLengthsPartition(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for (main_dims, parts, main_type, extra_ins, pack) in self.test_configs():\n        if len(main_dims) > 1:\n            continue\n        ins = ['in' + str(i) for i in range(2 + len(extra_ins))]\n        outs = ['in{}_p{}'.format(j, i) for i in range(parts) for j in range(2 + len(extra_ins))]\n        op = core.CreateOperator('LengthsPartition', ins, outs, pack_first_input=1 if pack else 0)\n        x = []\n        for (i, (dims, t)) in enumerate([((), main_type)] + extra_ins):\n            if t in [np.float32, np.float64]:\n                d = rand_array(*main_dims + dims)\n            else:\n                d = np.random.randint(-100, 100, main_dims + dims)\n            d = d.astype(t)\n            workspace.FeedBlob(ins[i + 1], d)\n            x.append(d)\n        elements = np.random.randint(2, 10)\n        lengths = []\n        total_length = 0\n        for _ in range(elements - 1):\n            lengths.append(np.random.randint(main_dims[0] - total_length))\n            total_length += lengths[-1]\n        lengths.append(main_dims[0] - total_length)\n        workspace.FeedBlob(ins[0], np.array(lengths, dtype=np.int32))\n\n        def sharding(x):\n            shards = (x[0] % parts).reshape([-1])\n            out = []\n            for i in range(parts):\n                idx = 0\n                sharded_lengths = np.zeros(elements)\n                for (ind, length) in enumerate(lengths):\n                    for _ in range(length):\n                        if shards[idx] == i:\n                            sharded_lengths[ind] += 1\n                        idx += 1\n                out.append(sharded_lengths)\n                for (ind, v) in enumerate(x):\n                    suffix_shape = v.shape[len(x[0].shape):]\n                    accum = []\n                    data = v.reshape((-1,) + suffix_shape)\n                    if pack and ind == 0:\n                        data = data // parts\n                    for (j, s) in enumerate(shards):\n                        if s == i:\n                            accum.append(data[j])\n\n                    def join(a):\n                        if not a:\n                            return np.empty(shape=(0,) + suffix_shape)\n                        return np.stack(a)\n                    out.append(join(accum))\n            return out\n        workspace.RunOperatorOnce(op)\n        ref = sharding(x)\n        for (name, expected) in zip(outs, ref):\n            np.testing.assert_array_equal(expected, workspace.FetchBlob(name))"
        ]
    }
]