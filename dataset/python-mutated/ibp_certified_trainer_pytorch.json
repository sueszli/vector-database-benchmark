[
    {
        "func_name": "__init__",
        "original": "def __init__(self, step_per_epoch: float, initial_val: float=0.0, final_val: float=1.0, warmup: int=0) -> None:\n    \"\"\"\n        Create a .DefaultLinearScheduler instance.\n\n        :param step_per_epoch: How much to increase the certification radius every epoch.\n        :param initial_val: The initial value.\n        :param warmup: If to have an initial warmup period.\n        \"\"\"\n    self.step_per_epoch = step_per_epoch\n    self.val = initial_val\n    self.warmup = warmup\n    self.final_val = final_val\n    self.step_count = 0",
        "mutated": [
            "def __init__(self, step_per_epoch: float, initial_val: float=0.0, final_val: float=1.0, warmup: int=0) -> None:\n    if False:\n        i = 10\n    '\\n        Create a .DefaultLinearScheduler instance.\\n\\n        :param step_per_epoch: How much to increase the certification radius every epoch.\\n        :param initial_val: The initial value.\\n        :param warmup: If to have an initial warmup period.\\n        '\n    self.step_per_epoch = step_per_epoch\n    self.val = initial_val\n    self.warmup = warmup\n    self.final_val = final_val\n    self.step_count = 0",
            "def __init__(self, step_per_epoch: float, initial_val: float=0.0, final_val: float=1.0, warmup: int=0) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create a .DefaultLinearScheduler instance.\\n\\n        :param step_per_epoch: How much to increase the certification radius every epoch.\\n        :param initial_val: The initial value.\\n        :param warmup: If to have an initial warmup period.\\n        '\n    self.step_per_epoch = step_per_epoch\n    self.val = initial_val\n    self.warmup = warmup\n    self.final_val = final_val\n    self.step_count = 0",
            "def __init__(self, step_per_epoch: float, initial_val: float=0.0, final_val: float=1.0, warmup: int=0) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create a .DefaultLinearScheduler instance.\\n\\n        :param step_per_epoch: How much to increase the certification radius every epoch.\\n        :param initial_val: The initial value.\\n        :param warmup: If to have an initial warmup period.\\n        '\n    self.step_per_epoch = step_per_epoch\n    self.val = initial_val\n    self.warmup = warmup\n    self.final_val = final_val\n    self.step_count = 0",
            "def __init__(self, step_per_epoch: float, initial_val: float=0.0, final_val: float=1.0, warmup: int=0) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create a .DefaultLinearScheduler instance.\\n\\n        :param step_per_epoch: How much to increase the certification radius every epoch.\\n        :param initial_val: The initial value.\\n        :param warmup: If to have an initial warmup period.\\n        '\n    self.step_per_epoch = step_per_epoch\n    self.val = initial_val\n    self.warmup = warmup\n    self.final_val = final_val\n    self.step_count = 0",
            "def __init__(self, step_per_epoch: float, initial_val: float=0.0, final_val: float=1.0, warmup: int=0) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create a .DefaultLinearScheduler instance.\\n\\n        :param step_per_epoch: How much to increase the certification radius every epoch.\\n        :param initial_val: The initial value.\\n        :param warmup: If to have an initial warmup period.\\n        '\n    self.step_per_epoch = step_per_epoch\n    self.val = initial_val\n    self.warmup = warmup\n    self.final_val = final_val\n    self.step_count = 0"
        ]
    },
    {
        "func_name": "step",
        "original": "def step(self) -> float:\n    \"\"\"\n        Grow the value by self.step_per_epoch.\n\n        :return: The updated scheduler value.\n        \"\"\"\n    self.step_count += 1\n    if self.step_count > self.warmup and self.val < self.final_val:\n        self.val += self.step_per_epoch\n        self.val = min(self.val, self.final_val)\n    return self.val",
        "mutated": [
            "def step(self) -> float:\n    if False:\n        i = 10\n    '\\n        Grow the value by self.step_per_epoch.\\n\\n        :return: The updated scheduler value.\\n        '\n    self.step_count += 1\n    if self.step_count > self.warmup and self.val < self.final_val:\n        self.val += self.step_per_epoch\n        self.val = min(self.val, self.final_val)\n    return self.val",
            "def step(self) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Grow the value by self.step_per_epoch.\\n\\n        :return: The updated scheduler value.\\n        '\n    self.step_count += 1\n    if self.step_count > self.warmup and self.val < self.final_val:\n        self.val += self.step_per_epoch\n        self.val = min(self.val, self.final_val)\n    return self.val",
            "def step(self) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Grow the value by self.step_per_epoch.\\n\\n        :return: The updated scheduler value.\\n        '\n    self.step_count += 1\n    if self.step_count > self.warmup and self.val < self.final_val:\n        self.val += self.step_per_epoch\n        self.val = min(self.val, self.final_val)\n    return self.val",
            "def step(self) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Grow the value by self.step_per_epoch.\\n\\n        :return: The updated scheduler value.\\n        '\n    self.step_count += 1\n    if self.step_count > self.warmup and self.val < self.final_val:\n        self.val += self.step_per_epoch\n        self.val = min(self.val, self.final_val)\n    return self.val",
            "def step(self) -> float:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Grow the value by self.step_per_epoch.\\n\\n        :return: The updated scheduler value.\\n        '\n    self.step_count += 1\n    if self.step_count > self.warmup and self.val < self.final_val:\n        self.val += self.step_per_epoch\n        self.val = min(self.val, self.final_val)\n    return self.val"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self, classifier: 'IBP_CERTIFIER_TYPE', nb_epochs: Optional[int]=20, bound: float=0.1, batch_size: int=32, loss_weighting: Optional[int]=None, use_certification_schedule: bool=True, certification_schedule: Optional[Any]=None, use_loss_weighting_schedule: bool=True, loss_weighting_schedule: Optional[Any]=None, augment_with_pgd: bool=False, pgd_params: Optional['PGDParamDict']=None) -> None:\n    \"\"\"\n        Create an :class:`.AdversarialTrainerCertified` instance.\n\n        Default values are for MNIST in pixel range 0-1.\n\n        :param classifier: Classifier to train adversarially.\n        :param pgd_params: A dictionary containing the specific parameters relating to regular PGD training.\n                           If not provided, we will default to typical MNIST values.\n                           Otherwise must contain the following keys:\n\n                           * *eps*: Maximum perturbation that the attacker can introduce.\n                           * *eps_step*: Attack step size (input variation) at each iteration.\n                           * *max_iter*: The maximum number of iterations.\n                           * *batch_size*: Size of the batch on which adversarial samples are generated.\n                           * *num_random_init*: Number of random initialisations within the epsilon ball.\n        :param loss_weighting: Weighting factor for the certified loss.\n        :param bound: The perturbation range for the interval. If the default certification schedule is used\n                      will be the upper limit.\n        :param nb_epochs: Number of training epochs.\n        :param use_certification_schedule: If to use a training schedule for the certification radius.\n        :param certification_schedule: Schedule for gradually increasing the certification radius. Empirical studies\n                                       have shown that this is often required to achieve best performance.\n                                       Either True to use the default linear scheduler,\n                                       or a class with a .step() method that returns the updated bound every epoch.\n        :param batch_size: Size of batches to use for certified training.\n        \"\"\"\n    from art.estimators.certification.interval.pytorch import PyTorchIBPClassifier\n    if not isinstance(classifier, PyTorchIBPClassifier):\n        raise ValueError('The classifier to pass in should be of type PyTorchIBPClassifier which can be found in art.estimators.certification.interval.pytorch.PyTorchIBPClassifier')\n    if not use_loss_weighting_schedule and loss_weighting is None:\n        raise ValueError('If a loss weighting schedule is not used then a value for loss_weighting should be supplied.')\n    if use_loss_weighting_schedule and loss_weighting is not None:\n        raise ValueError('Using a loss weighting schedule is incompatible with a fixed loss_weighting.')\n    super().__init__(classifier=classifier)\n    self.classifier: 'IBP_CERTIFIER_TYPE'\n    self.pgd_params: 'PGDParamDict'\n    self.nb_epochs = nb_epochs\n    self.loss_weighting = loss_weighting\n    self.bound = bound\n    self.use_certification_schedule = use_certification_schedule\n    self.certification_schedule = certification_schedule\n    self.use_loss_weighting_schedule = use_loss_weighting_schedule\n    self.loss_weighting_schedule = loss_weighting_schedule\n    self.batch_size = batch_size\n    self.augment_with_pgd = augment_with_pgd\n    if self.augment_with_pgd:\n        if pgd_params is None:\n            self.pgd_params = {'eps': 0.3, 'eps_step': 0.05, 'max_iter': 20, 'batch_size': 128, 'num_random_init': 1}\n        else:\n            self.pgd_params = pgd_params\n        self.attack = ProjectedGradientDescent(estimator=self.classifier, eps=self.pgd_params['eps'], eps_step=self.pgd_params['eps_step'], max_iter=self.pgd_params['max_iter'], num_random_init=self.pgd_params['num_random_init'])",
        "mutated": [
            "def __init__(self, classifier: 'IBP_CERTIFIER_TYPE', nb_epochs: Optional[int]=20, bound: float=0.1, batch_size: int=32, loss_weighting: Optional[int]=None, use_certification_schedule: bool=True, certification_schedule: Optional[Any]=None, use_loss_weighting_schedule: bool=True, loss_weighting_schedule: Optional[Any]=None, augment_with_pgd: bool=False, pgd_params: Optional['PGDParamDict']=None) -> None:\n    if False:\n        i = 10\n    '\\n        Create an :class:`.AdversarialTrainerCertified` instance.\\n\\n        Default values are for MNIST in pixel range 0-1.\\n\\n        :param classifier: Classifier to train adversarially.\\n        :param pgd_params: A dictionary containing the specific parameters relating to regular PGD training.\\n                           If not provided, we will default to typical MNIST values.\\n                           Otherwise must contain the following keys:\\n\\n                           * *eps*: Maximum perturbation that the attacker can introduce.\\n                           * *eps_step*: Attack step size (input variation) at each iteration.\\n                           * *max_iter*: The maximum number of iterations.\\n                           * *batch_size*: Size of the batch on which adversarial samples are generated.\\n                           * *num_random_init*: Number of random initialisations within the epsilon ball.\\n        :param loss_weighting: Weighting factor for the certified loss.\\n        :param bound: The perturbation range for the interval. If the default certification schedule is used\\n                      will be the upper limit.\\n        :param nb_epochs: Number of training epochs.\\n        :param use_certification_schedule: If to use a training schedule for the certification radius.\\n        :param certification_schedule: Schedule for gradually increasing the certification radius. Empirical studies\\n                                       have shown that this is often required to achieve best performance.\\n                                       Either True to use the default linear scheduler,\\n                                       or a class with a .step() method that returns the updated bound every epoch.\\n        :param batch_size: Size of batches to use for certified training.\\n        '\n    from art.estimators.certification.interval.pytorch import PyTorchIBPClassifier\n    if not isinstance(classifier, PyTorchIBPClassifier):\n        raise ValueError('The classifier to pass in should be of type PyTorchIBPClassifier which can be found in art.estimators.certification.interval.pytorch.PyTorchIBPClassifier')\n    if not use_loss_weighting_schedule and loss_weighting is None:\n        raise ValueError('If a loss weighting schedule is not used then a value for loss_weighting should be supplied.')\n    if use_loss_weighting_schedule and loss_weighting is not None:\n        raise ValueError('Using a loss weighting schedule is incompatible with a fixed loss_weighting.')\n    super().__init__(classifier=classifier)\n    self.classifier: 'IBP_CERTIFIER_TYPE'\n    self.pgd_params: 'PGDParamDict'\n    self.nb_epochs = nb_epochs\n    self.loss_weighting = loss_weighting\n    self.bound = bound\n    self.use_certification_schedule = use_certification_schedule\n    self.certification_schedule = certification_schedule\n    self.use_loss_weighting_schedule = use_loss_weighting_schedule\n    self.loss_weighting_schedule = loss_weighting_schedule\n    self.batch_size = batch_size\n    self.augment_with_pgd = augment_with_pgd\n    if self.augment_with_pgd:\n        if pgd_params is None:\n            self.pgd_params = {'eps': 0.3, 'eps_step': 0.05, 'max_iter': 20, 'batch_size': 128, 'num_random_init': 1}\n        else:\n            self.pgd_params = pgd_params\n        self.attack = ProjectedGradientDescent(estimator=self.classifier, eps=self.pgd_params['eps'], eps_step=self.pgd_params['eps_step'], max_iter=self.pgd_params['max_iter'], num_random_init=self.pgd_params['num_random_init'])",
            "def __init__(self, classifier: 'IBP_CERTIFIER_TYPE', nb_epochs: Optional[int]=20, bound: float=0.1, batch_size: int=32, loss_weighting: Optional[int]=None, use_certification_schedule: bool=True, certification_schedule: Optional[Any]=None, use_loss_weighting_schedule: bool=True, loss_weighting_schedule: Optional[Any]=None, augment_with_pgd: bool=False, pgd_params: Optional['PGDParamDict']=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create an :class:`.AdversarialTrainerCertified` instance.\\n\\n        Default values are for MNIST in pixel range 0-1.\\n\\n        :param classifier: Classifier to train adversarially.\\n        :param pgd_params: A dictionary containing the specific parameters relating to regular PGD training.\\n                           If not provided, we will default to typical MNIST values.\\n                           Otherwise must contain the following keys:\\n\\n                           * *eps*: Maximum perturbation that the attacker can introduce.\\n                           * *eps_step*: Attack step size (input variation) at each iteration.\\n                           * *max_iter*: The maximum number of iterations.\\n                           * *batch_size*: Size of the batch on which adversarial samples are generated.\\n                           * *num_random_init*: Number of random initialisations within the epsilon ball.\\n        :param loss_weighting: Weighting factor for the certified loss.\\n        :param bound: The perturbation range for the interval. If the default certification schedule is used\\n                      will be the upper limit.\\n        :param nb_epochs: Number of training epochs.\\n        :param use_certification_schedule: If to use a training schedule for the certification radius.\\n        :param certification_schedule: Schedule for gradually increasing the certification radius. Empirical studies\\n                                       have shown that this is often required to achieve best performance.\\n                                       Either True to use the default linear scheduler,\\n                                       or a class with a .step() method that returns the updated bound every epoch.\\n        :param batch_size: Size of batches to use for certified training.\\n        '\n    from art.estimators.certification.interval.pytorch import PyTorchIBPClassifier\n    if not isinstance(classifier, PyTorchIBPClassifier):\n        raise ValueError('The classifier to pass in should be of type PyTorchIBPClassifier which can be found in art.estimators.certification.interval.pytorch.PyTorchIBPClassifier')\n    if not use_loss_weighting_schedule and loss_weighting is None:\n        raise ValueError('If a loss weighting schedule is not used then a value for loss_weighting should be supplied.')\n    if use_loss_weighting_schedule and loss_weighting is not None:\n        raise ValueError('Using a loss weighting schedule is incompatible with a fixed loss_weighting.')\n    super().__init__(classifier=classifier)\n    self.classifier: 'IBP_CERTIFIER_TYPE'\n    self.pgd_params: 'PGDParamDict'\n    self.nb_epochs = nb_epochs\n    self.loss_weighting = loss_weighting\n    self.bound = bound\n    self.use_certification_schedule = use_certification_schedule\n    self.certification_schedule = certification_schedule\n    self.use_loss_weighting_schedule = use_loss_weighting_schedule\n    self.loss_weighting_schedule = loss_weighting_schedule\n    self.batch_size = batch_size\n    self.augment_with_pgd = augment_with_pgd\n    if self.augment_with_pgd:\n        if pgd_params is None:\n            self.pgd_params = {'eps': 0.3, 'eps_step': 0.05, 'max_iter': 20, 'batch_size': 128, 'num_random_init': 1}\n        else:\n            self.pgd_params = pgd_params\n        self.attack = ProjectedGradientDescent(estimator=self.classifier, eps=self.pgd_params['eps'], eps_step=self.pgd_params['eps_step'], max_iter=self.pgd_params['max_iter'], num_random_init=self.pgd_params['num_random_init'])",
            "def __init__(self, classifier: 'IBP_CERTIFIER_TYPE', nb_epochs: Optional[int]=20, bound: float=0.1, batch_size: int=32, loss_weighting: Optional[int]=None, use_certification_schedule: bool=True, certification_schedule: Optional[Any]=None, use_loss_weighting_schedule: bool=True, loss_weighting_schedule: Optional[Any]=None, augment_with_pgd: bool=False, pgd_params: Optional['PGDParamDict']=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create an :class:`.AdversarialTrainerCertified` instance.\\n\\n        Default values are for MNIST in pixel range 0-1.\\n\\n        :param classifier: Classifier to train adversarially.\\n        :param pgd_params: A dictionary containing the specific parameters relating to regular PGD training.\\n                           If not provided, we will default to typical MNIST values.\\n                           Otherwise must contain the following keys:\\n\\n                           * *eps*: Maximum perturbation that the attacker can introduce.\\n                           * *eps_step*: Attack step size (input variation) at each iteration.\\n                           * *max_iter*: The maximum number of iterations.\\n                           * *batch_size*: Size of the batch on which adversarial samples are generated.\\n                           * *num_random_init*: Number of random initialisations within the epsilon ball.\\n        :param loss_weighting: Weighting factor for the certified loss.\\n        :param bound: The perturbation range for the interval. If the default certification schedule is used\\n                      will be the upper limit.\\n        :param nb_epochs: Number of training epochs.\\n        :param use_certification_schedule: If to use a training schedule for the certification radius.\\n        :param certification_schedule: Schedule for gradually increasing the certification radius. Empirical studies\\n                                       have shown that this is often required to achieve best performance.\\n                                       Either True to use the default linear scheduler,\\n                                       or a class with a .step() method that returns the updated bound every epoch.\\n        :param batch_size: Size of batches to use for certified training.\\n        '\n    from art.estimators.certification.interval.pytorch import PyTorchIBPClassifier\n    if not isinstance(classifier, PyTorchIBPClassifier):\n        raise ValueError('The classifier to pass in should be of type PyTorchIBPClassifier which can be found in art.estimators.certification.interval.pytorch.PyTorchIBPClassifier')\n    if not use_loss_weighting_schedule and loss_weighting is None:\n        raise ValueError('If a loss weighting schedule is not used then a value for loss_weighting should be supplied.')\n    if use_loss_weighting_schedule and loss_weighting is not None:\n        raise ValueError('Using a loss weighting schedule is incompatible with a fixed loss_weighting.')\n    super().__init__(classifier=classifier)\n    self.classifier: 'IBP_CERTIFIER_TYPE'\n    self.pgd_params: 'PGDParamDict'\n    self.nb_epochs = nb_epochs\n    self.loss_weighting = loss_weighting\n    self.bound = bound\n    self.use_certification_schedule = use_certification_schedule\n    self.certification_schedule = certification_schedule\n    self.use_loss_weighting_schedule = use_loss_weighting_schedule\n    self.loss_weighting_schedule = loss_weighting_schedule\n    self.batch_size = batch_size\n    self.augment_with_pgd = augment_with_pgd\n    if self.augment_with_pgd:\n        if pgd_params is None:\n            self.pgd_params = {'eps': 0.3, 'eps_step': 0.05, 'max_iter': 20, 'batch_size': 128, 'num_random_init': 1}\n        else:\n            self.pgd_params = pgd_params\n        self.attack = ProjectedGradientDescent(estimator=self.classifier, eps=self.pgd_params['eps'], eps_step=self.pgd_params['eps_step'], max_iter=self.pgd_params['max_iter'], num_random_init=self.pgd_params['num_random_init'])",
            "def __init__(self, classifier: 'IBP_CERTIFIER_TYPE', nb_epochs: Optional[int]=20, bound: float=0.1, batch_size: int=32, loss_weighting: Optional[int]=None, use_certification_schedule: bool=True, certification_schedule: Optional[Any]=None, use_loss_weighting_schedule: bool=True, loss_weighting_schedule: Optional[Any]=None, augment_with_pgd: bool=False, pgd_params: Optional['PGDParamDict']=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create an :class:`.AdversarialTrainerCertified` instance.\\n\\n        Default values are for MNIST in pixel range 0-1.\\n\\n        :param classifier: Classifier to train adversarially.\\n        :param pgd_params: A dictionary containing the specific parameters relating to regular PGD training.\\n                           If not provided, we will default to typical MNIST values.\\n                           Otherwise must contain the following keys:\\n\\n                           * *eps*: Maximum perturbation that the attacker can introduce.\\n                           * *eps_step*: Attack step size (input variation) at each iteration.\\n                           * *max_iter*: The maximum number of iterations.\\n                           * *batch_size*: Size of the batch on which adversarial samples are generated.\\n                           * *num_random_init*: Number of random initialisations within the epsilon ball.\\n        :param loss_weighting: Weighting factor for the certified loss.\\n        :param bound: The perturbation range for the interval. If the default certification schedule is used\\n                      will be the upper limit.\\n        :param nb_epochs: Number of training epochs.\\n        :param use_certification_schedule: If to use a training schedule for the certification radius.\\n        :param certification_schedule: Schedule for gradually increasing the certification radius. Empirical studies\\n                                       have shown that this is often required to achieve best performance.\\n                                       Either True to use the default linear scheduler,\\n                                       or a class with a .step() method that returns the updated bound every epoch.\\n        :param batch_size: Size of batches to use for certified training.\\n        '\n    from art.estimators.certification.interval.pytorch import PyTorchIBPClassifier\n    if not isinstance(classifier, PyTorchIBPClassifier):\n        raise ValueError('The classifier to pass in should be of type PyTorchIBPClassifier which can be found in art.estimators.certification.interval.pytorch.PyTorchIBPClassifier')\n    if not use_loss_weighting_schedule and loss_weighting is None:\n        raise ValueError('If a loss weighting schedule is not used then a value for loss_weighting should be supplied.')\n    if use_loss_weighting_schedule and loss_weighting is not None:\n        raise ValueError('Using a loss weighting schedule is incompatible with a fixed loss_weighting.')\n    super().__init__(classifier=classifier)\n    self.classifier: 'IBP_CERTIFIER_TYPE'\n    self.pgd_params: 'PGDParamDict'\n    self.nb_epochs = nb_epochs\n    self.loss_weighting = loss_weighting\n    self.bound = bound\n    self.use_certification_schedule = use_certification_schedule\n    self.certification_schedule = certification_schedule\n    self.use_loss_weighting_schedule = use_loss_weighting_schedule\n    self.loss_weighting_schedule = loss_weighting_schedule\n    self.batch_size = batch_size\n    self.augment_with_pgd = augment_with_pgd\n    if self.augment_with_pgd:\n        if pgd_params is None:\n            self.pgd_params = {'eps': 0.3, 'eps_step': 0.05, 'max_iter': 20, 'batch_size': 128, 'num_random_init': 1}\n        else:\n            self.pgd_params = pgd_params\n        self.attack = ProjectedGradientDescent(estimator=self.classifier, eps=self.pgd_params['eps'], eps_step=self.pgd_params['eps_step'], max_iter=self.pgd_params['max_iter'], num_random_init=self.pgd_params['num_random_init'])",
            "def __init__(self, classifier: 'IBP_CERTIFIER_TYPE', nb_epochs: Optional[int]=20, bound: float=0.1, batch_size: int=32, loss_weighting: Optional[int]=None, use_certification_schedule: bool=True, certification_schedule: Optional[Any]=None, use_loss_weighting_schedule: bool=True, loss_weighting_schedule: Optional[Any]=None, augment_with_pgd: bool=False, pgd_params: Optional['PGDParamDict']=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create an :class:`.AdversarialTrainerCertified` instance.\\n\\n        Default values are for MNIST in pixel range 0-1.\\n\\n        :param classifier: Classifier to train adversarially.\\n        :param pgd_params: A dictionary containing the specific parameters relating to regular PGD training.\\n                           If not provided, we will default to typical MNIST values.\\n                           Otherwise must contain the following keys:\\n\\n                           * *eps*: Maximum perturbation that the attacker can introduce.\\n                           * *eps_step*: Attack step size (input variation) at each iteration.\\n                           * *max_iter*: The maximum number of iterations.\\n                           * *batch_size*: Size of the batch on which adversarial samples are generated.\\n                           * *num_random_init*: Number of random initialisations within the epsilon ball.\\n        :param loss_weighting: Weighting factor for the certified loss.\\n        :param bound: The perturbation range for the interval. If the default certification schedule is used\\n                      will be the upper limit.\\n        :param nb_epochs: Number of training epochs.\\n        :param use_certification_schedule: If to use a training schedule for the certification radius.\\n        :param certification_schedule: Schedule for gradually increasing the certification radius. Empirical studies\\n                                       have shown that this is often required to achieve best performance.\\n                                       Either True to use the default linear scheduler,\\n                                       or a class with a .step() method that returns the updated bound every epoch.\\n        :param batch_size: Size of batches to use for certified training.\\n        '\n    from art.estimators.certification.interval.pytorch import PyTorchIBPClassifier\n    if not isinstance(classifier, PyTorchIBPClassifier):\n        raise ValueError('The classifier to pass in should be of type PyTorchIBPClassifier which can be found in art.estimators.certification.interval.pytorch.PyTorchIBPClassifier')\n    if not use_loss_weighting_schedule and loss_weighting is None:\n        raise ValueError('If a loss weighting schedule is not used then a value for loss_weighting should be supplied.')\n    if use_loss_weighting_schedule and loss_weighting is not None:\n        raise ValueError('Using a loss weighting schedule is incompatible with a fixed loss_weighting.')\n    super().__init__(classifier=classifier)\n    self.classifier: 'IBP_CERTIFIER_TYPE'\n    self.pgd_params: 'PGDParamDict'\n    self.nb_epochs = nb_epochs\n    self.loss_weighting = loss_weighting\n    self.bound = bound\n    self.use_certification_schedule = use_certification_schedule\n    self.certification_schedule = certification_schedule\n    self.use_loss_weighting_schedule = use_loss_weighting_schedule\n    self.loss_weighting_schedule = loss_weighting_schedule\n    self.batch_size = batch_size\n    self.augment_with_pgd = augment_with_pgd\n    if self.augment_with_pgd:\n        if pgd_params is None:\n            self.pgd_params = {'eps': 0.3, 'eps_step': 0.05, 'max_iter': 20, 'batch_size': 128, 'num_random_init': 1}\n        else:\n            self.pgd_params = pgd_params\n        self.attack = ProjectedGradientDescent(estimator=self.classifier, eps=self.pgd_params['eps'], eps_step=self.pgd_params['eps_step'], max_iter=self.pgd_params['max_iter'], num_random_init=self.pgd_params['num_random_init'])"
        ]
    },
    {
        "func_name": "initialise_default_scheduler",
        "original": "@staticmethod\ndef initialise_default_scheduler(initial_val: float, final_val: float, epochs: int) -> DefaultLinearScheduler:\n    \"\"\"\n        Create linear schedulers based on default example values.\n\n        :param initial_val: Initial value to begin the scheduler from.\n        :param final_val: Final value to end the scheduler at.\n        :param epochs: Total number of epochs.\n\n        :return: A linear scheduler initialised with default example values.\n        \"\"\"\n    warm_up = int(0.01 * epochs)\n    epochs_to_ramp = int(0.3 * epochs)\n    if epochs_to_ramp == 0:\n        epochs_to_ramp = 1\n    step_in_eps_per_epoch = (final_val - initial_val) / epochs_to_ramp\n    return DefaultLinearScheduler(step_per_epoch=step_in_eps_per_epoch, initial_val=initial_val, final_val=final_val, warmup=warm_up)",
        "mutated": [
            "@staticmethod\ndef initialise_default_scheduler(initial_val: float, final_val: float, epochs: int) -> DefaultLinearScheduler:\n    if False:\n        i = 10\n    '\\n        Create linear schedulers based on default example values.\\n\\n        :param initial_val: Initial value to begin the scheduler from.\\n        :param final_val: Final value to end the scheduler at.\\n        :param epochs: Total number of epochs.\\n\\n        :return: A linear scheduler initialised with default example values.\\n        '\n    warm_up = int(0.01 * epochs)\n    epochs_to_ramp = int(0.3 * epochs)\n    if epochs_to_ramp == 0:\n        epochs_to_ramp = 1\n    step_in_eps_per_epoch = (final_val - initial_val) / epochs_to_ramp\n    return DefaultLinearScheduler(step_per_epoch=step_in_eps_per_epoch, initial_val=initial_val, final_val=final_val, warmup=warm_up)",
            "@staticmethod\ndef initialise_default_scheduler(initial_val: float, final_val: float, epochs: int) -> DefaultLinearScheduler:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Create linear schedulers based on default example values.\\n\\n        :param initial_val: Initial value to begin the scheduler from.\\n        :param final_val: Final value to end the scheduler at.\\n        :param epochs: Total number of epochs.\\n\\n        :return: A linear scheduler initialised with default example values.\\n        '\n    warm_up = int(0.01 * epochs)\n    epochs_to_ramp = int(0.3 * epochs)\n    if epochs_to_ramp == 0:\n        epochs_to_ramp = 1\n    step_in_eps_per_epoch = (final_val - initial_val) / epochs_to_ramp\n    return DefaultLinearScheduler(step_per_epoch=step_in_eps_per_epoch, initial_val=initial_val, final_val=final_val, warmup=warm_up)",
            "@staticmethod\ndef initialise_default_scheduler(initial_val: float, final_val: float, epochs: int) -> DefaultLinearScheduler:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Create linear schedulers based on default example values.\\n\\n        :param initial_val: Initial value to begin the scheduler from.\\n        :param final_val: Final value to end the scheduler at.\\n        :param epochs: Total number of epochs.\\n\\n        :return: A linear scheduler initialised with default example values.\\n        '\n    warm_up = int(0.01 * epochs)\n    epochs_to_ramp = int(0.3 * epochs)\n    if epochs_to_ramp == 0:\n        epochs_to_ramp = 1\n    step_in_eps_per_epoch = (final_val - initial_val) / epochs_to_ramp\n    return DefaultLinearScheduler(step_per_epoch=step_in_eps_per_epoch, initial_val=initial_val, final_val=final_val, warmup=warm_up)",
            "@staticmethod\ndef initialise_default_scheduler(initial_val: float, final_val: float, epochs: int) -> DefaultLinearScheduler:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Create linear schedulers based on default example values.\\n\\n        :param initial_val: Initial value to begin the scheduler from.\\n        :param final_val: Final value to end the scheduler at.\\n        :param epochs: Total number of epochs.\\n\\n        :return: A linear scheduler initialised with default example values.\\n        '\n    warm_up = int(0.01 * epochs)\n    epochs_to_ramp = int(0.3 * epochs)\n    if epochs_to_ramp == 0:\n        epochs_to_ramp = 1\n    step_in_eps_per_epoch = (final_val - initial_val) / epochs_to_ramp\n    return DefaultLinearScheduler(step_per_epoch=step_in_eps_per_epoch, initial_val=initial_val, final_val=final_val, warmup=warm_up)",
            "@staticmethod\ndef initialise_default_scheduler(initial_val: float, final_val: float, epochs: int) -> DefaultLinearScheduler:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Create linear schedulers based on default example values.\\n\\n        :param initial_val: Initial value to begin the scheduler from.\\n        :param final_val: Final value to end the scheduler at.\\n        :param epochs: Total number of epochs.\\n\\n        :return: A linear scheduler initialised with default example values.\\n        '\n    warm_up = int(0.01 * epochs)\n    epochs_to_ramp = int(0.3 * epochs)\n    if epochs_to_ramp == 0:\n        epochs_to_ramp = 1\n    step_in_eps_per_epoch = (final_val - initial_val) / epochs_to_ramp\n    return DefaultLinearScheduler(step_per_epoch=step_in_eps_per_epoch, initial_val=initial_val, final_val=final_val, warmup=warm_up)"
        ]
    },
    {
        "func_name": "fit",
        "original": "def fit(self, x: np.ndarray, y: np.ndarray, limits: Optional[Union[List[float], np.ndarray]]=None, certification_loss: Any='interval_loss_cce', batch_size: Optional[int]=None, nb_epochs: Optional[int]=None, training_mode: bool=True, scheduler: Optional[Any]=None, verbose: bool=True, **kwargs) -> None:\n    \"\"\"\n        Fit the classifier on the training set `(x, y)`.\n\n        :param x: Training data.\n        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or index labels of\n                  shape (nb_samples,).\n        :param limits: Max and min limits on the inputs, limits[0] being the lower bounds and limits[1] being upper\n                       bounds. Passing None will mean no clipping is applied to the interval abstraction.\n                       Typical images will have limits of [0.0, 1.0] after normalization.\n        :param certification_loss: Which certification loss function to use. Either \"interval_loss_cce\"\n                                   or \"max_logit_loss\". By default will use interval_loss_cce.\n                                   Alternatively, a user can supply their own loss function which takes in as input\n                                   the interval predictions of the form () and labels of the form () and returns a\n                                   scalar loss.\n        :param batch_size: Size of batches to use for certified training. NB, this will run the data\n                           sequentially accumulating gradients over the batch size.\n        :param nb_epochs: Number of epochs to use for training.\n        :param training_mode: `True` for model set to training mode and `'False` for model set to evaluation mode.\n        :param scheduler: Learning rate scheduler to run at the start of every epoch.\n        :param verbose: If to display the per-batch statistics while training.\n        :param kwargs: Dictionary of framework-specific arguments. This parameter is not currently supported for PyTorch\n               and providing it takes no effect.\n        \"\"\"\n    import torch\n    if batch_size is None:\n        batch_size = self.batch_size\n    if nb_epochs is not None:\n        epochs: int = nb_epochs\n    else:\n        raise ValueError('Value of `epochs` not defined.')\n    if limits is None:\n        raise ValueError('Please provide values for the clipping limits of the data. Typical images will have limits of [0.0, 1.0]. ')\n    self.classifier._model.train(mode=training_mode)\n    if self.classifier.optimizer is None:\n        raise ValueError('An optimizer is needed to train the model, but none is provided.')\n    y = check_and_transform_label_format(y, nb_classes=self.classifier.nb_classes)\n    (x_preprocessed, y_preprocessed) = self.classifier.apply_preprocessing(x, y, fit=True)\n    y_preprocessed = self.classifier.reduce_labels(y_preprocessed)\n    num_batch = int(np.ceil(len(x_preprocessed) / float(batch_size)))\n    ind = np.arange(len(x_preprocessed))\n    x_cert = np.copy(x_preprocessed)\n    y_cert = np.copy(y_preprocessed)\n    if self.use_certification_schedule:\n        if self.certification_schedule is None:\n            self.certification_schedule = self.initialise_default_scheduler(initial_val=0.0, final_val=self.bound, epochs=epochs)\n    else:\n        bound = self.bound\n    if self.use_loss_weighting_schedule:\n        if self.loss_weighting_schedule is None:\n            self.loss_weighting_schedule = self.initialise_default_scheduler(initial_val=0.0, final_val=0.5, epochs=epochs)\n    elif self.loss_weighting is not None:\n        loss_weighting_k = self.loss_weighting\n    else:\n        raise ValueError('Unable to determine loss weighting.')\n    for _ in tqdm(range(epochs)):\n        if self.use_certification_schedule and self.certification_schedule is not None:\n            bound = self.certification_schedule.step()\n        if self.use_loss_weighting_schedule and self.loss_weighting_schedule is not None:\n            loss_weighting_k = self.loss_weighting_schedule.step()\n        random.shuffle(ind)\n        pbar = tqdm(range(num_batch), disable=not verbose)\n        (x_cert, y_cert) = shuffle(x_cert, y_cert)\n        epoch_non_cert_loss = []\n        non_cert_acc = []\n        cert_loss = []\n        cert_acc = []\n        for m in pbar:\n            x_batch = np.copy(x_cert[batch_size * m:batch_size * (m + 1)])\n            y_batch = np.copy(y_cert[batch_size * m:batch_size * (m + 1)])\n            self.classifier.optimizer.zero_grad()\n            if self.classifier.provided_concrete_to_interval:\n                processed_x_cert = self.classifier.provided_concrete_to_interval(x_batch, bound, limits=limits)\n            else:\n                processed_x_cert = self.classifier.concrete_to_interval(x_batch, bound, limits=limits)\n            self.set_forward_mode('abstract')\n            interval_preds = self.classifier.model.forward(processed_x_cert)\n            if certification_loss == 'interval_loss_cce':\n                certified_loss = self.classifier.interval_loss_cce(prediction=interval_preds, target=torch.from_numpy(y_batch).to(self.classifier.device))\n            else:\n                certified_loss = certification_loss(interval_preds, y_batch)\n            samples_certified = self.classifier.certify(preds=interval_preds.cpu().detach(), labels=y_batch)\n            cert_loss.append(certified_loss)\n            cert_acc.append(np.sum(samples_certified) / batch_size)\n            if self.augment_with_pgd:\n                i_batch = np.copy(x_preprocessed[ind[m * self.pgd_params['batch_size']:(m + 1) * self.pgd_params['batch_size']]]).astype('float32')\n                o_batch = y_preprocessed[ind[m * self.pgd_params['batch_size']:(m + 1) * self.pgd_params['batch_size']]]\n                self.set_forward_mode('attack')\n                self.attack = ProjectedGradientDescent(estimator=self.classifier, eps=self.pgd_params['eps'], eps_step=self.pgd_params['eps_step'], max_iter=self.pgd_params['max_iter'], num_random_init=self.pgd_params['num_random_init'])\n                i_batch = self.attack.generate(i_batch, y=o_batch)\n                self.classifier.model.zero_grad()\n            else:\n                i_batch = np.copy(x_preprocessed[ind[m * batch_size:(m + 1) * batch_size]]).astype('float32')\n                o_batch = y_preprocessed[ind[m * batch_size:(m + 1) * batch_size]]\n            self.set_forward_mode('concrete')\n            model_outputs = self.classifier.model.forward(i_batch)\n            acc = self.classifier.get_accuracy(model_outputs, o_batch)\n            non_cert_loss = self.classifier.concrete_loss(model_outputs, torch.from_numpy(o_batch).to(self.classifier.device))\n            epoch_non_cert_loss.append(non_cert_loss)\n            non_cert_acc.append(acc)\n            loss = certified_loss * loss_weighting_k + non_cert_loss * (1 - loss_weighting_k)\n            if self.classifier._use_amp:\n                from apex import amp\n                with amp.scale_loss(loss, self.classifier.optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            self.classifier.optimizer.step()\n            self.classifier.re_convert()\n            if verbose:\n                pbar.set_description(f\"Bound {bound:.2f}: Loss {torch.mean(torch.stack(epoch_non_cert_loss)):.2f} Cert Loss {torch.mean(torch.stack(cert_loss)):.2f} Acc {np.mean(non_cert_acc):.2f} Cert Acc {np.mean(cert_acc):.2f} l_weight {loss_weighting_k:.2f} lr {self.classifier.optimizer.param_groups[0]['lr']}\")\n        if scheduler is not None:\n            scheduler.step()",
        "mutated": [
            "def fit(self, x: np.ndarray, y: np.ndarray, limits: Optional[Union[List[float], np.ndarray]]=None, certification_loss: Any='interval_loss_cce', batch_size: Optional[int]=None, nb_epochs: Optional[int]=None, training_mode: bool=True, scheduler: Optional[Any]=None, verbose: bool=True, **kwargs) -> None:\n    if False:\n        i = 10\n    '\\n        Fit the classifier on the training set `(x, y)`.\\n\\n        :param x: Training data.\\n        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or index labels of\\n                  shape (nb_samples,).\\n        :param limits: Max and min limits on the inputs, limits[0] being the lower bounds and limits[1] being upper\\n                       bounds. Passing None will mean no clipping is applied to the interval abstraction.\\n                       Typical images will have limits of [0.0, 1.0] after normalization.\\n        :param certification_loss: Which certification loss function to use. Either \"interval_loss_cce\"\\n                                   or \"max_logit_loss\". By default will use interval_loss_cce.\\n                                   Alternatively, a user can supply their own loss function which takes in as input\\n                                   the interval predictions of the form () and labels of the form () and returns a\\n                                   scalar loss.\\n        :param batch_size: Size of batches to use for certified training. NB, this will run the data\\n                           sequentially accumulating gradients over the batch size.\\n        :param nb_epochs: Number of epochs to use for training.\\n        :param training_mode: `True` for model set to training mode and `\\'False` for model set to evaluation mode.\\n        :param scheduler: Learning rate scheduler to run at the start of every epoch.\\n        :param verbose: If to display the per-batch statistics while training.\\n        :param kwargs: Dictionary of framework-specific arguments. This parameter is not currently supported for PyTorch\\n               and providing it takes no effect.\\n        '\n    import torch\n    if batch_size is None:\n        batch_size = self.batch_size\n    if nb_epochs is not None:\n        epochs: int = nb_epochs\n    else:\n        raise ValueError('Value of `epochs` not defined.')\n    if limits is None:\n        raise ValueError('Please provide values for the clipping limits of the data. Typical images will have limits of [0.0, 1.0]. ')\n    self.classifier._model.train(mode=training_mode)\n    if self.classifier.optimizer is None:\n        raise ValueError('An optimizer is needed to train the model, but none is provided.')\n    y = check_and_transform_label_format(y, nb_classes=self.classifier.nb_classes)\n    (x_preprocessed, y_preprocessed) = self.classifier.apply_preprocessing(x, y, fit=True)\n    y_preprocessed = self.classifier.reduce_labels(y_preprocessed)\n    num_batch = int(np.ceil(len(x_preprocessed) / float(batch_size)))\n    ind = np.arange(len(x_preprocessed))\n    x_cert = np.copy(x_preprocessed)\n    y_cert = np.copy(y_preprocessed)\n    if self.use_certification_schedule:\n        if self.certification_schedule is None:\n            self.certification_schedule = self.initialise_default_scheduler(initial_val=0.0, final_val=self.bound, epochs=epochs)\n    else:\n        bound = self.bound\n    if self.use_loss_weighting_schedule:\n        if self.loss_weighting_schedule is None:\n            self.loss_weighting_schedule = self.initialise_default_scheduler(initial_val=0.0, final_val=0.5, epochs=epochs)\n    elif self.loss_weighting is not None:\n        loss_weighting_k = self.loss_weighting\n    else:\n        raise ValueError('Unable to determine loss weighting.')\n    for _ in tqdm(range(epochs)):\n        if self.use_certification_schedule and self.certification_schedule is not None:\n            bound = self.certification_schedule.step()\n        if self.use_loss_weighting_schedule and self.loss_weighting_schedule is not None:\n            loss_weighting_k = self.loss_weighting_schedule.step()\n        random.shuffle(ind)\n        pbar = tqdm(range(num_batch), disable=not verbose)\n        (x_cert, y_cert) = shuffle(x_cert, y_cert)\n        epoch_non_cert_loss = []\n        non_cert_acc = []\n        cert_loss = []\n        cert_acc = []\n        for m in pbar:\n            x_batch = np.copy(x_cert[batch_size * m:batch_size * (m + 1)])\n            y_batch = np.copy(y_cert[batch_size * m:batch_size * (m + 1)])\n            self.classifier.optimizer.zero_grad()\n            if self.classifier.provided_concrete_to_interval:\n                processed_x_cert = self.classifier.provided_concrete_to_interval(x_batch, bound, limits=limits)\n            else:\n                processed_x_cert = self.classifier.concrete_to_interval(x_batch, bound, limits=limits)\n            self.set_forward_mode('abstract')\n            interval_preds = self.classifier.model.forward(processed_x_cert)\n            if certification_loss == 'interval_loss_cce':\n                certified_loss = self.classifier.interval_loss_cce(prediction=interval_preds, target=torch.from_numpy(y_batch).to(self.classifier.device))\n            else:\n                certified_loss = certification_loss(interval_preds, y_batch)\n            samples_certified = self.classifier.certify(preds=interval_preds.cpu().detach(), labels=y_batch)\n            cert_loss.append(certified_loss)\n            cert_acc.append(np.sum(samples_certified) / batch_size)\n            if self.augment_with_pgd:\n                i_batch = np.copy(x_preprocessed[ind[m * self.pgd_params['batch_size']:(m + 1) * self.pgd_params['batch_size']]]).astype('float32')\n                o_batch = y_preprocessed[ind[m * self.pgd_params['batch_size']:(m + 1) * self.pgd_params['batch_size']]]\n                self.set_forward_mode('attack')\n                self.attack = ProjectedGradientDescent(estimator=self.classifier, eps=self.pgd_params['eps'], eps_step=self.pgd_params['eps_step'], max_iter=self.pgd_params['max_iter'], num_random_init=self.pgd_params['num_random_init'])\n                i_batch = self.attack.generate(i_batch, y=o_batch)\n                self.classifier.model.zero_grad()\n            else:\n                i_batch = np.copy(x_preprocessed[ind[m * batch_size:(m + 1) * batch_size]]).astype('float32')\n                o_batch = y_preprocessed[ind[m * batch_size:(m + 1) * batch_size]]\n            self.set_forward_mode('concrete')\n            model_outputs = self.classifier.model.forward(i_batch)\n            acc = self.classifier.get_accuracy(model_outputs, o_batch)\n            non_cert_loss = self.classifier.concrete_loss(model_outputs, torch.from_numpy(o_batch).to(self.classifier.device))\n            epoch_non_cert_loss.append(non_cert_loss)\n            non_cert_acc.append(acc)\n            loss = certified_loss * loss_weighting_k + non_cert_loss * (1 - loss_weighting_k)\n            if self.classifier._use_amp:\n                from apex import amp\n                with amp.scale_loss(loss, self.classifier.optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            self.classifier.optimizer.step()\n            self.classifier.re_convert()\n            if verbose:\n                pbar.set_description(f\"Bound {bound:.2f}: Loss {torch.mean(torch.stack(epoch_non_cert_loss)):.2f} Cert Loss {torch.mean(torch.stack(cert_loss)):.2f} Acc {np.mean(non_cert_acc):.2f} Cert Acc {np.mean(cert_acc):.2f} l_weight {loss_weighting_k:.2f} lr {self.classifier.optimizer.param_groups[0]['lr']}\")\n        if scheduler is not None:\n            scheduler.step()",
            "def fit(self, x: np.ndarray, y: np.ndarray, limits: Optional[Union[List[float], np.ndarray]]=None, certification_loss: Any='interval_loss_cce', batch_size: Optional[int]=None, nb_epochs: Optional[int]=None, training_mode: bool=True, scheduler: Optional[Any]=None, verbose: bool=True, **kwargs) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Fit the classifier on the training set `(x, y)`.\\n\\n        :param x: Training data.\\n        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or index labels of\\n                  shape (nb_samples,).\\n        :param limits: Max and min limits on the inputs, limits[0] being the lower bounds and limits[1] being upper\\n                       bounds. Passing None will mean no clipping is applied to the interval abstraction.\\n                       Typical images will have limits of [0.0, 1.0] after normalization.\\n        :param certification_loss: Which certification loss function to use. Either \"interval_loss_cce\"\\n                                   or \"max_logit_loss\". By default will use interval_loss_cce.\\n                                   Alternatively, a user can supply their own loss function which takes in as input\\n                                   the interval predictions of the form () and labels of the form () and returns a\\n                                   scalar loss.\\n        :param batch_size: Size of batches to use for certified training. NB, this will run the data\\n                           sequentially accumulating gradients over the batch size.\\n        :param nb_epochs: Number of epochs to use for training.\\n        :param training_mode: `True` for model set to training mode and `\\'False` for model set to evaluation mode.\\n        :param scheduler: Learning rate scheduler to run at the start of every epoch.\\n        :param verbose: If to display the per-batch statistics while training.\\n        :param kwargs: Dictionary of framework-specific arguments. This parameter is not currently supported for PyTorch\\n               and providing it takes no effect.\\n        '\n    import torch\n    if batch_size is None:\n        batch_size = self.batch_size\n    if nb_epochs is not None:\n        epochs: int = nb_epochs\n    else:\n        raise ValueError('Value of `epochs` not defined.')\n    if limits is None:\n        raise ValueError('Please provide values for the clipping limits of the data. Typical images will have limits of [0.0, 1.0]. ')\n    self.classifier._model.train(mode=training_mode)\n    if self.classifier.optimizer is None:\n        raise ValueError('An optimizer is needed to train the model, but none is provided.')\n    y = check_and_transform_label_format(y, nb_classes=self.classifier.nb_classes)\n    (x_preprocessed, y_preprocessed) = self.classifier.apply_preprocessing(x, y, fit=True)\n    y_preprocessed = self.classifier.reduce_labels(y_preprocessed)\n    num_batch = int(np.ceil(len(x_preprocessed) / float(batch_size)))\n    ind = np.arange(len(x_preprocessed))\n    x_cert = np.copy(x_preprocessed)\n    y_cert = np.copy(y_preprocessed)\n    if self.use_certification_schedule:\n        if self.certification_schedule is None:\n            self.certification_schedule = self.initialise_default_scheduler(initial_val=0.0, final_val=self.bound, epochs=epochs)\n    else:\n        bound = self.bound\n    if self.use_loss_weighting_schedule:\n        if self.loss_weighting_schedule is None:\n            self.loss_weighting_schedule = self.initialise_default_scheduler(initial_val=0.0, final_val=0.5, epochs=epochs)\n    elif self.loss_weighting is not None:\n        loss_weighting_k = self.loss_weighting\n    else:\n        raise ValueError('Unable to determine loss weighting.')\n    for _ in tqdm(range(epochs)):\n        if self.use_certification_schedule and self.certification_schedule is not None:\n            bound = self.certification_schedule.step()\n        if self.use_loss_weighting_schedule and self.loss_weighting_schedule is not None:\n            loss_weighting_k = self.loss_weighting_schedule.step()\n        random.shuffle(ind)\n        pbar = tqdm(range(num_batch), disable=not verbose)\n        (x_cert, y_cert) = shuffle(x_cert, y_cert)\n        epoch_non_cert_loss = []\n        non_cert_acc = []\n        cert_loss = []\n        cert_acc = []\n        for m in pbar:\n            x_batch = np.copy(x_cert[batch_size * m:batch_size * (m + 1)])\n            y_batch = np.copy(y_cert[batch_size * m:batch_size * (m + 1)])\n            self.classifier.optimizer.zero_grad()\n            if self.classifier.provided_concrete_to_interval:\n                processed_x_cert = self.classifier.provided_concrete_to_interval(x_batch, bound, limits=limits)\n            else:\n                processed_x_cert = self.classifier.concrete_to_interval(x_batch, bound, limits=limits)\n            self.set_forward_mode('abstract')\n            interval_preds = self.classifier.model.forward(processed_x_cert)\n            if certification_loss == 'interval_loss_cce':\n                certified_loss = self.classifier.interval_loss_cce(prediction=interval_preds, target=torch.from_numpy(y_batch).to(self.classifier.device))\n            else:\n                certified_loss = certification_loss(interval_preds, y_batch)\n            samples_certified = self.classifier.certify(preds=interval_preds.cpu().detach(), labels=y_batch)\n            cert_loss.append(certified_loss)\n            cert_acc.append(np.sum(samples_certified) / batch_size)\n            if self.augment_with_pgd:\n                i_batch = np.copy(x_preprocessed[ind[m * self.pgd_params['batch_size']:(m + 1) * self.pgd_params['batch_size']]]).astype('float32')\n                o_batch = y_preprocessed[ind[m * self.pgd_params['batch_size']:(m + 1) * self.pgd_params['batch_size']]]\n                self.set_forward_mode('attack')\n                self.attack = ProjectedGradientDescent(estimator=self.classifier, eps=self.pgd_params['eps'], eps_step=self.pgd_params['eps_step'], max_iter=self.pgd_params['max_iter'], num_random_init=self.pgd_params['num_random_init'])\n                i_batch = self.attack.generate(i_batch, y=o_batch)\n                self.classifier.model.zero_grad()\n            else:\n                i_batch = np.copy(x_preprocessed[ind[m * batch_size:(m + 1) * batch_size]]).astype('float32')\n                o_batch = y_preprocessed[ind[m * batch_size:(m + 1) * batch_size]]\n            self.set_forward_mode('concrete')\n            model_outputs = self.classifier.model.forward(i_batch)\n            acc = self.classifier.get_accuracy(model_outputs, o_batch)\n            non_cert_loss = self.classifier.concrete_loss(model_outputs, torch.from_numpy(o_batch).to(self.classifier.device))\n            epoch_non_cert_loss.append(non_cert_loss)\n            non_cert_acc.append(acc)\n            loss = certified_loss * loss_weighting_k + non_cert_loss * (1 - loss_weighting_k)\n            if self.classifier._use_amp:\n                from apex import amp\n                with amp.scale_loss(loss, self.classifier.optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            self.classifier.optimizer.step()\n            self.classifier.re_convert()\n            if verbose:\n                pbar.set_description(f\"Bound {bound:.2f}: Loss {torch.mean(torch.stack(epoch_non_cert_loss)):.2f} Cert Loss {torch.mean(torch.stack(cert_loss)):.2f} Acc {np.mean(non_cert_acc):.2f} Cert Acc {np.mean(cert_acc):.2f} l_weight {loss_weighting_k:.2f} lr {self.classifier.optimizer.param_groups[0]['lr']}\")\n        if scheduler is not None:\n            scheduler.step()",
            "def fit(self, x: np.ndarray, y: np.ndarray, limits: Optional[Union[List[float], np.ndarray]]=None, certification_loss: Any='interval_loss_cce', batch_size: Optional[int]=None, nb_epochs: Optional[int]=None, training_mode: bool=True, scheduler: Optional[Any]=None, verbose: bool=True, **kwargs) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Fit the classifier on the training set `(x, y)`.\\n\\n        :param x: Training data.\\n        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or index labels of\\n                  shape (nb_samples,).\\n        :param limits: Max and min limits on the inputs, limits[0] being the lower bounds and limits[1] being upper\\n                       bounds. Passing None will mean no clipping is applied to the interval abstraction.\\n                       Typical images will have limits of [0.0, 1.0] after normalization.\\n        :param certification_loss: Which certification loss function to use. Either \"interval_loss_cce\"\\n                                   or \"max_logit_loss\". By default will use interval_loss_cce.\\n                                   Alternatively, a user can supply their own loss function which takes in as input\\n                                   the interval predictions of the form () and labels of the form () and returns a\\n                                   scalar loss.\\n        :param batch_size: Size of batches to use for certified training. NB, this will run the data\\n                           sequentially accumulating gradients over the batch size.\\n        :param nb_epochs: Number of epochs to use for training.\\n        :param training_mode: `True` for model set to training mode and `\\'False` for model set to evaluation mode.\\n        :param scheduler: Learning rate scheduler to run at the start of every epoch.\\n        :param verbose: If to display the per-batch statistics while training.\\n        :param kwargs: Dictionary of framework-specific arguments. This parameter is not currently supported for PyTorch\\n               and providing it takes no effect.\\n        '\n    import torch\n    if batch_size is None:\n        batch_size = self.batch_size\n    if nb_epochs is not None:\n        epochs: int = nb_epochs\n    else:\n        raise ValueError('Value of `epochs` not defined.')\n    if limits is None:\n        raise ValueError('Please provide values for the clipping limits of the data. Typical images will have limits of [0.0, 1.0]. ')\n    self.classifier._model.train(mode=training_mode)\n    if self.classifier.optimizer is None:\n        raise ValueError('An optimizer is needed to train the model, but none is provided.')\n    y = check_and_transform_label_format(y, nb_classes=self.classifier.nb_classes)\n    (x_preprocessed, y_preprocessed) = self.classifier.apply_preprocessing(x, y, fit=True)\n    y_preprocessed = self.classifier.reduce_labels(y_preprocessed)\n    num_batch = int(np.ceil(len(x_preprocessed) / float(batch_size)))\n    ind = np.arange(len(x_preprocessed))\n    x_cert = np.copy(x_preprocessed)\n    y_cert = np.copy(y_preprocessed)\n    if self.use_certification_schedule:\n        if self.certification_schedule is None:\n            self.certification_schedule = self.initialise_default_scheduler(initial_val=0.0, final_val=self.bound, epochs=epochs)\n    else:\n        bound = self.bound\n    if self.use_loss_weighting_schedule:\n        if self.loss_weighting_schedule is None:\n            self.loss_weighting_schedule = self.initialise_default_scheduler(initial_val=0.0, final_val=0.5, epochs=epochs)\n    elif self.loss_weighting is not None:\n        loss_weighting_k = self.loss_weighting\n    else:\n        raise ValueError('Unable to determine loss weighting.')\n    for _ in tqdm(range(epochs)):\n        if self.use_certification_schedule and self.certification_schedule is not None:\n            bound = self.certification_schedule.step()\n        if self.use_loss_weighting_schedule and self.loss_weighting_schedule is not None:\n            loss_weighting_k = self.loss_weighting_schedule.step()\n        random.shuffle(ind)\n        pbar = tqdm(range(num_batch), disable=not verbose)\n        (x_cert, y_cert) = shuffle(x_cert, y_cert)\n        epoch_non_cert_loss = []\n        non_cert_acc = []\n        cert_loss = []\n        cert_acc = []\n        for m in pbar:\n            x_batch = np.copy(x_cert[batch_size * m:batch_size * (m + 1)])\n            y_batch = np.copy(y_cert[batch_size * m:batch_size * (m + 1)])\n            self.classifier.optimizer.zero_grad()\n            if self.classifier.provided_concrete_to_interval:\n                processed_x_cert = self.classifier.provided_concrete_to_interval(x_batch, bound, limits=limits)\n            else:\n                processed_x_cert = self.classifier.concrete_to_interval(x_batch, bound, limits=limits)\n            self.set_forward_mode('abstract')\n            interval_preds = self.classifier.model.forward(processed_x_cert)\n            if certification_loss == 'interval_loss_cce':\n                certified_loss = self.classifier.interval_loss_cce(prediction=interval_preds, target=torch.from_numpy(y_batch).to(self.classifier.device))\n            else:\n                certified_loss = certification_loss(interval_preds, y_batch)\n            samples_certified = self.classifier.certify(preds=interval_preds.cpu().detach(), labels=y_batch)\n            cert_loss.append(certified_loss)\n            cert_acc.append(np.sum(samples_certified) / batch_size)\n            if self.augment_with_pgd:\n                i_batch = np.copy(x_preprocessed[ind[m * self.pgd_params['batch_size']:(m + 1) * self.pgd_params['batch_size']]]).astype('float32')\n                o_batch = y_preprocessed[ind[m * self.pgd_params['batch_size']:(m + 1) * self.pgd_params['batch_size']]]\n                self.set_forward_mode('attack')\n                self.attack = ProjectedGradientDescent(estimator=self.classifier, eps=self.pgd_params['eps'], eps_step=self.pgd_params['eps_step'], max_iter=self.pgd_params['max_iter'], num_random_init=self.pgd_params['num_random_init'])\n                i_batch = self.attack.generate(i_batch, y=o_batch)\n                self.classifier.model.zero_grad()\n            else:\n                i_batch = np.copy(x_preprocessed[ind[m * batch_size:(m + 1) * batch_size]]).astype('float32')\n                o_batch = y_preprocessed[ind[m * batch_size:(m + 1) * batch_size]]\n            self.set_forward_mode('concrete')\n            model_outputs = self.classifier.model.forward(i_batch)\n            acc = self.classifier.get_accuracy(model_outputs, o_batch)\n            non_cert_loss = self.classifier.concrete_loss(model_outputs, torch.from_numpy(o_batch).to(self.classifier.device))\n            epoch_non_cert_loss.append(non_cert_loss)\n            non_cert_acc.append(acc)\n            loss = certified_loss * loss_weighting_k + non_cert_loss * (1 - loss_weighting_k)\n            if self.classifier._use_amp:\n                from apex import amp\n                with amp.scale_loss(loss, self.classifier.optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            self.classifier.optimizer.step()\n            self.classifier.re_convert()\n            if verbose:\n                pbar.set_description(f\"Bound {bound:.2f}: Loss {torch.mean(torch.stack(epoch_non_cert_loss)):.2f} Cert Loss {torch.mean(torch.stack(cert_loss)):.2f} Acc {np.mean(non_cert_acc):.2f} Cert Acc {np.mean(cert_acc):.2f} l_weight {loss_weighting_k:.2f} lr {self.classifier.optimizer.param_groups[0]['lr']}\")\n        if scheduler is not None:\n            scheduler.step()",
            "def fit(self, x: np.ndarray, y: np.ndarray, limits: Optional[Union[List[float], np.ndarray]]=None, certification_loss: Any='interval_loss_cce', batch_size: Optional[int]=None, nb_epochs: Optional[int]=None, training_mode: bool=True, scheduler: Optional[Any]=None, verbose: bool=True, **kwargs) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Fit the classifier on the training set `(x, y)`.\\n\\n        :param x: Training data.\\n        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or index labels of\\n                  shape (nb_samples,).\\n        :param limits: Max and min limits on the inputs, limits[0] being the lower bounds and limits[1] being upper\\n                       bounds. Passing None will mean no clipping is applied to the interval abstraction.\\n                       Typical images will have limits of [0.0, 1.0] after normalization.\\n        :param certification_loss: Which certification loss function to use. Either \"interval_loss_cce\"\\n                                   or \"max_logit_loss\". By default will use interval_loss_cce.\\n                                   Alternatively, a user can supply their own loss function which takes in as input\\n                                   the interval predictions of the form () and labels of the form () and returns a\\n                                   scalar loss.\\n        :param batch_size: Size of batches to use for certified training. NB, this will run the data\\n                           sequentially accumulating gradients over the batch size.\\n        :param nb_epochs: Number of epochs to use for training.\\n        :param training_mode: `True` for model set to training mode and `\\'False` for model set to evaluation mode.\\n        :param scheduler: Learning rate scheduler to run at the start of every epoch.\\n        :param verbose: If to display the per-batch statistics while training.\\n        :param kwargs: Dictionary of framework-specific arguments. This parameter is not currently supported for PyTorch\\n               and providing it takes no effect.\\n        '\n    import torch\n    if batch_size is None:\n        batch_size = self.batch_size\n    if nb_epochs is not None:\n        epochs: int = nb_epochs\n    else:\n        raise ValueError('Value of `epochs` not defined.')\n    if limits is None:\n        raise ValueError('Please provide values for the clipping limits of the data. Typical images will have limits of [0.0, 1.0]. ')\n    self.classifier._model.train(mode=training_mode)\n    if self.classifier.optimizer is None:\n        raise ValueError('An optimizer is needed to train the model, but none is provided.')\n    y = check_and_transform_label_format(y, nb_classes=self.classifier.nb_classes)\n    (x_preprocessed, y_preprocessed) = self.classifier.apply_preprocessing(x, y, fit=True)\n    y_preprocessed = self.classifier.reduce_labels(y_preprocessed)\n    num_batch = int(np.ceil(len(x_preprocessed) / float(batch_size)))\n    ind = np.arange(len(x_preprocessed))\n    x_cert = np.copy(x_preprocessed)\n    y_cert = np.copy(y_preprocessed)\n    if self.use_certification_schedule:\n        if self.certification_schedule is None:\n            self.certification_schedule = self.initialise_default_scheduler(initial_val=0.0, final_val=self.bound, epochs=epochs)\n    else:\n        bound = self.bound\n    if self.use_loss_weighting_schedule:\n        if self.loss_weighting_schedule is None:\n            self.loss_weighting_schedule = self.initialise_default_scheduler(initial_val=0.0, final_val=0.5, epochs=epochs)\n    elif self.loss_weighting is not None:\n        loss_weighting_k = self.loss_weighting\n    else:\n        raise ValueError('Unable to determine loss weighting.')\n    for _ in tqdm(range(epochs)):\n        if self.use_certification_schedule and self.certification_schedule is not None:\n            bound = self.certification_schedule.step()\n        if self.use_loss_weighting_schedule and self.loss_weighting_schedule is not None:\n            loss_weighting_k = self.loss_weighting_schedule.step()\n        random.shuffle(ind)\n        pbar = tqdm(range(num_batch), disable=not verbose)\n        (x_cert, y_cert) = shuffle(x_cert, y_cert)\n        epoch_non_cert_loss = []\n        non_cert_acc = []\n        cert_loss = []\n        cert_acc = []\n        for m in pbar:\n            x_batch = np.copy(x_cert[batch_size * m:batch_size * (m + 1)])\n            y_batch = np.copy(y_cert[batch_size * m:batch_size * (m + 1)])\n            self.classifier.optimizer.zero_grad()\n            if self.classifier.provided_concrete_to_interval:\n                processed_x_cert = self.classifier.provided_concrete_to_interval(x_batch, bound, limits=limits)\n            else:\n                processed_x_cert = self.classifier.concrete_to_interval(x_batch, bound, limits=limits)\n            self.set_forward_mode('abstract')\n            interval_preds = self.classifier.model.forward(processed_x_cert)\n            if certification_loss == 'interval_loss_cce':\n                certified_loss = self.classifier.interval_loss_cce(prediction=interval_preds, target=torch.from_numpy(y_batch).to(self.classifier.device))\n            else:\n                certified_loss = certification_loss(interval_preds, y_batch)\n            samples_certified = self.classifier.certify(preds=interval_preds.cpu().detach(), labels=y_batch)\n            cert_loss.append(certified_loss)\n            cert_acc.append(np.sum(samples_certified) / batch_size)\n            if self.augment_with_pgd:\n                i_batch = np.copy(x_preprocessed[ind[m * self.pgd_params['batch_size']:(m + 1) * self.pgd_params['batch_size']]]).astype('float32')\n                o_batch = y_preprocessed[ind[m * self.pgd_params['batch_size']:(m + 1) * self.pgd_params['batch_size']]]\n                self.set_forward_mode('attack')\n                self.attack = ProjectedGradientDescent(estimator=self.classifier, eps=self.pgd_params['eps'], eps_step=self.pgd_params['eps_step'], max_iter=self.pgd_params['max_iter'], num_random_init=self.pgd_params['num_random_init'])\n                i_batch = self.attack.generate(i_batch, y=o_batch)\n                self.classifier.model.zero_grad()\n            else:\n                i_batch = np.copy(x_preprocessed[ind[m * batch_size:(m + 1) * batch_size]]).astype('float32')\n                o_batch = y_preprocessed[ind[m * batch_size:(m + 1) * batch_size]]\n            self.set_forward_mode('concrete')\n            model_outputs = self.classifier.model.forward(i_batch)\n            acc = self.classifier.get_accuracy(model_outputs, o_batch)\n            non_cert_loss = self.classifier.concrete_loss(model_outputs, torch.from_numpy(o_batch).to(self.classifier.device))\n            epoch_non_cert_loss.append(non_cert_loss)\n            non_cert_acc.append(acc)\n            loss = certified_loss * loss_weighting_k + non_cert_loss * (1 - loss_weighting_k)\n            if self.classifier._use_amp:\n                from apex import amp\n                with amp.scale_loss(loss, self.classifier.optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            self.classifier.optimizer.step()\n            self.classifier.re_convert()\n            if verbose:\n                pbar.set_description(f\"Bound {bound:.2f}: Loss {torch.mean(torch.stack(epoch_non_cert_loss)):.2f} Cert Loss {torch.mean(torch.stack(cert_loss)):.2f} Acc {np.mean(non_cert_acc):.2f} Cert Acc {np.mean(cert_acc):.2f} l_weight {loss_weighting_k:.2f} lr {self.classifier.optimizer.param_groups[0]['lr']}\")\n        if scheduler is not None:\n            scheduler.step()",
            "def fit(self, x: np.ndarray, y: np.ndarray, limits: Optional[Union[List[float], np.ndarray]]=None, certification_loss: Any='interval_loss_cce', batch_size: Optional[int]=None, nb_epochs: Optional[int]=None, training_mode: bool=True, scheduler: Optional[Any]=None, verbose: bool=True, **kwargs) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Fit the classifier on the training set `(x, y)`.\\n\\n        :param x: Training data.\\n        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or index labels of\\n                  shape (nb_samples,).\\n        :param limits: Max and min limits on the inputs, limits[0] being the lower bounds and limits[1] being upper\\n                       bounds. Passing None will mean no clipping is applied to the interval abstraction.\\n                       Typical images will have limits of [0.0, 1.0] after normalization.\\n        :param certification_loss: Which certification loss function to use. Either \"interval_loss_cce\"\\n                                   or \"max_logit_loss\". By default will use interval_loss_cce.\\n                                   Alternatively, a user can supply their own loss function which takes in as input\\n                                   the interval predictions of the form () and labels of the form () and returns a\\n                                   scalar loss.\\n        :param batch_size: Size of batches to use for certified training. NB, this will run the data\\n                           sequentially accumulating gradients over the batch size.\\n        :param nb_epochs: Number of epochs to use for training.\\n        :param training_mode: `True` for model set to training mode and `\\'False` for model set to evaluation mode.\\n        :param scheduler: Learning rate scheduler to run at the start of every epoch.\\n        :param verbose: If to display the per-batch statistics while training.\\n        :param kwargs: Dictionary of framework-specific arguments. This parameter is not currently supported for PyTorch\\n               and providing it takes no effect.\\n        '\n    import torch\n    if batch_size is None:\n        batch_size = self.batch_size\n    if nb_epochs is not None:\n        epochs: int = nb_epochs\n    else:\n        raise ValueError('Value of `epochs` not defined.')\n    if limits is None:\n        raise ValueError('Please provide values for the clipping limits of the data. Typical images will have limits of [0.0, 1.0]. ')\n    self.classifier._model.train(mode=training_mode)\n    if self.classifier.optimizer is None:\n        raise ValueError('An optimizer is needed to train the model, but none is provided.')\n    y = check_and_transform_label_format(y, nb_classes=self.classifier.nb_classes)\n    (x_preprocessed, y_preprocessed) = self.classifier.apply_preprocessing(x, y, fit=True)\n    y_preprocessed = self.classifier.reduce_labels(y_preprocessed)\n    num_batch = int(np.ceil(len(x_preprocessed) / float(batch_size)))\n    ind = np.arange(len(x_preprocessed))\n    x_cert = np.copy(x_preprocessed)\n    y_cert = np.copy(y_preprocessed)\n    if self.use_certification_schedule:\n        if self.certification_schedule is None:\n            self.certification_schedule = self.initialise_default_scheduler(initial_val=0.0, final_val=self.bound, epochs=epochs)\n    else:\n        bound = self.bound\n    if self.use_loss_weighting_schedule:\n        if self.loss_weighting_schedule is None:\n            self.loss_weighting_schedule = self.initialise_default_scheduler(initial_val=0.0, final_val=0.5, epochs=epochs)\n    elif self.loss_weighting is not None:\n        loss_weighting_k = self.loss_weighting\n    else:\n        raise ValueError('Unable to determine loss weighting.')\n    for _ in tqdm(range(epochs)):\n        if self.use_certification_schedule and self.certification_schedule is not None:\n            bound = self.certification_schedule.step()\n        if self.use_loss_weighting_schedule and self.loss_weighting_schedule is not None:\n            loss_weighting_k = self.loss_weighting_schedule.step()\n        random.shuffle(ind)\n        pbar = tqdm(range(num_batch), disable=not verbose)\n        (x_cert, y_cert) = shuffle(x_cert, y_cert)\n        epoch_non_cert_loss = []\n        non_cert_acc = []\n        cert_loss = []\n        cert_acc = []\n        for m in pbar:\n            x_batch = np.copy(x_cert[batch_size * m:batch_size * (m + 1)])\n            y_batch = np.copy(y_cert[batch_size * m:batch_size * (m + 1)])\n            self.classifier.optimizer.zero_grad()\n            if self.classifier.provided_concrete_to_interval:\n                processed_x_cert = self.classifier.provided_concrete_to_interval(x_batch, bound, limits=limits)\n            else:\n                processed_x_cert = self.classifier.concrete_to_interval(x_batch, bound, limits=limits)\n            self.set_forward_mode('abstract')\n            interval_preds = self.classifier.model.forward(processed_x_cert)\n            if certification_loss == 'interval_loss_cce':\n                certified_loss = self.classifier.interval_loss_cce(prediction=interval_preds, target=torch.from_numpy(y_batch).to(self.classifier.device))\n            else:\n                certified_loss = certification_loss(interval_preds, y_batch)\n            samples_certified = self.classifier.certify(preds=interval_preds.cpu().detach(), labels=y_batch)\n            cert_loss.append(certified_loss)\n            cert_acc.append(np.sum(samples_certified) / batch_size)\n            if self.augment_with_pgd:\n                i_batch = np.copy(x_preprocessed[ind[m * self.pgd_params['batch_size']:(m + 1) * self.pgd_params['batch_size']]]).astype('float32')\n                o_batch = y_preprocessed[ind[m * self.pgd_params['batch_size']:(m + 1) * self.pgd_params['batch_size']]]\n                self.set_forward_mode('attack')\n                self.attack = ProjectedGradientDescent(estimator=self.classifier, eps=self.pgd_params['eps'], eps_step=self.pgd_params['eps_step'], max_iter=self.pgd_params['max_iter'], num_random_init=self.pgd_params['num_random_init'])\n                i_batch = self.attack.generate(i_batch, y=o_batch)\n                self.classifier.model.zero_grad()\n            else:\n                i_batch = np.copy(x_preprocessed[ind[m * batch_size:(m + 1) * batch_size]]).astype('float32')\n                o_batch = y_preprocessed[ind[m * batch_size:(m + 1) * batch_size]]\n            self.set_forward_mode('concrete')\n            model_outputs = self.classifier.model.forward(i_batch)\n            acc = self.classifier.get_accuracy(model_outputs, o_batch)\n            non_cert_loss = self.classifier.concrete_loss(model_outputs, torch.from_numpy(o_batch).to(self.classifier.device))\n            epoch_non_cert_loss.append(non_cert_loss)\n            non_cert_acc.append(acc)\n            loss = certified_loss * loss_weighting_k + non_cert_loss * (1 - loss_weighting_k)\n            if self.classifier._use_amp:\n                from apex import amp\n                with amp.scale_loss(loss, self.classifier.optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            self.classifier.optimizer.step()\n            self.classifier.re_convert()\n            if verbose:\n                pbar.set_description(f\"Bound {bound:.2f}: Loss {torch.mean(torch.stack(epoch_non_cert_loss)):.2f} Cert Loss {torch.mean(torch.stack(cert_loss)):.2f} Acc {np.mean(non_cert_acc):.2f} Cert Acc {np.mean(cert_acc):.2f} l_weight {loss_weighting_k:.2f} lr {self.classifier.optimizer.param_groups[0]['lr']}\")\n        if scheduler is not None:\n            scheduler.step()"
        ]
    },
    {
        "func_name": "predict",
        "original": "def predict(self, x: np.ndarray, **kwargs) -> np.ndarray:\n    \"\"\"\n        Perform prediction using the adversarially trained classifier.\n\n        :param x: Input samples.\n        :param kwargs: Other parameters to be passed on to the `predict` function of the classifier.\n        :return: Predictions for test set.\n        \"\"\"\n    if self.classifier.model.forward_mode != 'concrete':\n        raise ValueError('For normal predictions, the model must be running in concrete mode. If an abstract prediction is wanted then use predict_interval instead')\n    return self.classifier.predict(x, **kwargs)",
        "mutated": [
            "def predict(self, x: np.ndarray, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n    '\\n        Perform prediction using the adversarially trained classifier.\\n\\n        :param x: Input samples.\\n        :param kwargs: Other parameters to be passed on to the `predict` function of the classifier.\\n        :return: Predictions for test set.\\n        '\n    if self.classifier.model.forward_mode != 'concrete':\n        raise ValueError('For normal predictions, the model must be running in concrete mode. If an abstract prediction is wanted then use predict_interval instead')\n    return self.classifier.predict(x, **kwargs)",
            "def predict(self, x: np.ndarray, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Perform prediction using the adversarially trained classifier.\\n\\n        :param x: Input samples.\\n        :param kwargs: Other parameters to be passed on to the `predict` function of the classifier.\\n        :return: Predictions for test set.\\n        '\n    if self.classifier.model.forward_mode != 'concrete':\n        raise ValueError('For normal predictions, the model must be running in concrete mode. If an abstract prediction is wanted then use predict_interval instead')\n    return self.classifier.predict(x, **kwargs)",
            "def predict(self, x: np.ndarray, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Perform prediction using the adversarially trained classifier.\\n\\n        :param x: Input samples.\\n        :param kwargs: Other parameters to be passed on to the `predict` function of the classifier.\\n        :return: Predictions for test set.\\n        '\n    if self.classifier.model.forward_mode != 'concrete':\n        raise ValueError('For normal predictions, the model must be running in concrete mode. If an abstract prediction is wanted then use predict_interval instead')\n    return self.classifier.predict(x, **kwargs)",
            "def predict(self, x: np.ndarray, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Perform prediction using the adversarially trained classifier.\\n\\n        :param x: Input samples.\\n        :param kwargs: Other parameters to be passed on to the `predict` function of the classifier.\\n        :return: Predictions for test set.\\n        '\n    if self.classifier.model.forward_mode != 'concrete':\n        raise ValueError('For normal predictions, the model must be running in concrete mode. If an abstract prediction is wanted then use predict_interval instead')\n    return self.classifier.predict(x, **kwargs)",
            "def predict(self, x: np.ndarray, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Perform prediction using the adversarially trained classifier.\\n\\n        :param x: Input samples.\\n        :param kwargs: Other parameters to be passed on to the `predict` function of the classifier.\\n        :return: Predictions for test set.\\n        '\n    if self.classifier.model.forward_mode != 'concrete':\n        raise ValueError('For normal predictions, the model must be running in concrete mode. If an abstract prediction is wanted then use predict_interval instead')\n    return self.classifier.predict(x, **kwargs)"
        ]
    },
    {
        "func_name": "predict_intervals",
        "original": "def predict_intervals(self, x: np.ndarray, is_interval: bool=False, bounds: Optional[Union[float, List[float], np.ndarray]]=None, limits: Optional[Union[List[float], np.ndarray]]=None, batch_size: int=128, **kwargs) -> np.ndarray:\n    \"\"\"\n        Perform prediction using the adversarially trained classifier using zonotopes\n\n        :param x: The datapoint, either:\n\n                1. In the interval format of x[batch_size, 2, feature_1, feature_2, ...]\n                   where axis=1 corresponds to the [lower, upper] bounds.\n\n                2. Or in regular concrete form, in which case the bounds/limits need to be supplied.\n        :param is_interval: if the datapoint is already in the correct interval format.\n        :param bounds: The perturbation range.\n        :param limits: The clipping to apply to the interval data.\n        :param batch_size: batch size to use when looping through the data\n        \"\"\"\n    if self.classifier.model.forward_mode != 'abstract':\n        raise ValueError('For interval predictions, the model must be running in abstract mode. If a concrete prediction is wanted then use predict instead')\n    return self.classifier.predict_intervals(x, is_interval, bounds, limits, batch_size, **kwargs)",
        "mutated": [
            "def predict_intervals(self, x: np.ndarray, is_interval: bool=False, bounds: Optional[Union[float, List[float], np.ndarray]]=None, limits: Optional[Union[List[float], np.ndarray]]=None, batch_size: int=128, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n    '\\n        Perform prediction using the adversarially trained classifier using zonotopes\\n\\n        :param x: The datapoint, either:\\n\\n                1. In the interval format of x[batch_size, 2, feature_1, feature_2, ...]\\n                   where axis=1 corresponds to the [lower, upper] bounds.\\n\\n                2. Or in regular concrete form, in which case the bounds/limits need to be supplied.\\n        :param is_interval: if the datapoint is already in the correct interval format.\\n        :param bounds: The perturbation range.\\n        :param limits: The clipping to apply to the interval data.\\n        :param batch_size: batch size to use when looping through the data\\n        '\n    if self.classifier.model.forward_mode != 'abstract':\n        raise ValueError('For interval predictions, the model must be running in abstract mode. If a concrete prediction is wanted then use predict instead')\n    return self.classifier.predict_intervals(x, is_interval, bounds, limits, batch_size, **kwargs)",
            "def predict_intervals(self, x: np.ndarray, is_interval: bool=False, bounds: Optional[Union[float, List[float], np.ndarray]]=None, limits: Optional[Union[List[float], np.ndarray]]=None, batch_size: int=128, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Perform prediction using the adversarially trained classifier using zonotopes\\n\\n        :param x: The datapoint, either:\\n\\n                1. In the interval format of x[batch_size, 2, feature_1, feature_2, ...]\\n                   where axis=1 corresponds to the [lower, upper] bounds.\\n\\n                2. Or in regular concrete form, in which case the bounds/limits need to be supplied.\\n        :param is_interval: if the datapoint is already in the correct interval format.\\n        :param bounds: The perturbation range.\\n        :param limits: The clipping to apply to the interval data.\\n        :param batch_size: batch size to use when looping through the data\\n        '\n    if self.classifier.model.forward_mode != 'abstract':\n        raise ValueError('For interval predictions, the model must be running in abstract mode. If a concrete prediction is wanted then use predict instead')\n    return self.classifier.predict_intervals(x, is_interval, bounds, limits, batch_size, **kwargs)",
            "def predict_intervals(self, x: np.ndarray, is_interval: bool=False, bounds: Optional[Union[float, List[float], np.ndarray]]=None, limits: Optional[Union[List[float], np.ndarray]]=None, batch_size: int=128, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Perform prediction using the adversarially trained classifier using zonotopes\\n\\n        :param x: The datapoint, either:\\n\\n                1. In the interval format of x[batch_size, 2, feature_1, feature_2, ...]\\n                   where axis=1 corresponds to the [lower, upper] bounds.\\n\\n                2. Or in regular concrete form, in which case the bounds/limits need to be supplied.\\n        :param is_interval: if the datapoint is already in the correct interval format.\\n        :param bounds: The perturbation range.\\n        :param limits: The clipping to apply to the interval data.\\n        :param batch_size: batch size to use when looping through the data\\n        '\n    if self.classifier.model.forward_mode != 'abstract':\n        raise ValueError('For interval predictions, the model must be running in abstract mode. If a concrete prediction is wanted then use predict instead')\n    return self.classifier.predict_intervals(x, is_interval, bounds, limits, batch_size, **kwargs)",
            "def predict_intervals(self, x: np.ndarray, is_interval: bool=False, bounds: Optional[Union[float, List[float], np.ndarray]]=None, limits: Optional[Union[List[float], np.ndarray]]=None, batch_size: int=128, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Perform prediction using the adversarially trained classifier using zonotopes\\n\\n        :param x: The datapoint, either:\\n\\n                1. In the interval format of x[batch_size, 2, feature_1, feature_2, ...]\\n                   where axis=1 corresponds to the [lower, upper] bounds.\\n\\n                2. Or in regular concrete form, in which case the bounds/limits need to be supplied.\\n        :param is_interval: if the datapoint is already in the correct interval format.\\n        :param bounds: The perturbation range.\\n        :param limits: The clipping to apply to the interval data.\\n        :param batch_size: batch size to use when looping through the data\\n        '\n    if self.classifier.model.forward_mode != 'abstract':\n        raise ValueError('For interval predictions, the model must be running in abstract mode. If a concrete prediction is wanted then use predict instead')\n    return self.classifier.predict_intervals(x, is_interval, bounds, limits, batch_size, **kwargs)",
            "def predict_intervals(self, x: np.ndarray, is_interval: bool=False, bounds: Optional[Union[float, List[float], np.ndarray]]=None, limits: Optional[Union[List[float], np.ndarray]]=None, batch_size: int=128, **kwargs) -> np.ndarray:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Perform prediction using the adversarially trained classifier using zonotopes\\n\\n        :param x: The datapoint, either:\\n\\n                1. In the interval format of x[batch_size, 2, feature_1, feature_2, ...]\\n                   where axis=1 corresponds to the [lower, upper] bounds.\\n\\n                2. Or in regular concrete form, in which case the bounds/limits need to be supplied.\\n        :param is_interval: if the datapoint is already in the correct interval format.\\n        :param bounds: The perturbation range.\\n        :param limits: The clipping to apply to the interval data.\\n        :param batch_size: batch size to use when looping through the data\\n        '\n    if self.classifier.model.forward_mode != 'abstract':\n        raise ValueError('For interval predictions, the model must be running in abstract mode. If a concrete prediction is wanted then use predict instead')\n    return self.classifier.predict_intervals(x, is_interval, bounds, limits, batch_size, **kwargs)"
        ]
    },
    {
        "func_name": "set_forward_mode",
        "original": "def set_forward_mode(self, mode: str) -> None:\n    \"\"\"\n        Helper function to set the forward mode of the model\n\n        :param mode: either concrete or abstract signifying how to run the forward pass\n        \"\"\"\n    self.classifier.model.set_forward_mode(mode)",
        "mutated": [
            "def set_forward_mode(self, mode: str) -> None:\n    if False:\n        i = 10\n    '\\n        Helper function to set the forward mode of the model\\n\\n        :param mode: either concrete or abstract signifying how to run the forward pass\\n        '\n    self.classifier.model.set_forward_mode(mode)",
            "def set_forward_mode(self, mode: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Helper function to set the forward mode of the model\\n\\n        :param mode: either concrete or abstract signifying how to run the forward pass\\n        '\n    self.classifier.model.set_forward_mode(mode)",
            "def set_forward_mode(self, mode: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Helper function to set the forward mode of the model\\n\\n        :param mode: either concrete or abstract signifying how to run the forward pass\\n        '\n    self.classifier.model.set_forward_mode(mode)",
            "def set_forward_mode(self, mode: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Helper function to set the forward mode of the model\\n\\n        :param mode: either concrete or abstract signifying how to run the forward pass\\n        '\n    self.classifier.model.set_forward_mode(mode)",
            "def set_forward_mode(self, mode: str) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Helper function to set the forward mode of the model\\n\\n        :param mode: either concrete or abstract signifying how to run the forward pass\\n        '\n    self.classifier.model.set_forward_mode(mode)"
        ]
    }
]