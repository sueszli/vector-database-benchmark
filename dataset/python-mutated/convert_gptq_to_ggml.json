[
    {
        "func_name": "write_header",
        "original": "def write_header(fout, shape, dst_name, ftype_cur):\n    sname = dst_name.encode('utf-8')\n    fout.write(struct.pack('iii', len(shape), len(sname), ftype_cur))\n    fout.write(struct.pack('i' * len(shape), *shape[::-1]))\n    fout.write(sname)\n    fout.seek(fout.tell() + 31 & -32)",
        "mutated": [
            "def write_header(fout, shape, dst_name, ftype_cur):\n    if False:\n        i = 10\n    sname = dst_name.encode('utf-8')\n    fout.write(struct.pack('iii', len(shape), len(sname), ftype_cur))\n    fout.write(struct.pack('i' * len(shape), *shape[::-1]))\n    fout.write(sname)\n    fout.seek(fout.tell() + 31 & -32)",
            "def write_header(fout, shape, dst_name, ftype_cur):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    sname = dst_name.encode('utf-8')\n    fout.write(struct.pack('iii', len(shape), len(sname), ftype_cur))\n    fout.write(struct.pack('i' * len(shape), *shape[::-1]))\n    fout.write(sname)\n    fout.seek(fout.tell() + 31 & -32)",
            "def write_header(fout, shape, dst_name, ftype_cur):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    sname = dst_name.encode('utf-8')\n    fout.write(struct.pack('iii', len(shape), len(sname), ftype_cur))\n    fout.write(struct.pack('i' * len(shape), *shape[::-1]))\n    fout.write(sname)\n    fout.seek(fout.tell() + 31 & -32)",
            "def write_header(fout, shape, dst_name, ftype_cur):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    sname = dst_name.encode('utf-8')\n    fout.write(struct.pack('iii', len(shape), len(sname), ftype_cur))\n    fout.write(struct.pack('i' * len(shape), *shape[::-1]))\n    fout.write(sname)\n    fout.seek(fout.tell() + 31 & -32)",
            "def write_header(fout, shape, dst_name, ftype_cur):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    sname = dst_name.encode('utf-8')\n    fout.write(struct.pack('iii', len(shape), len(sname), ftype_cur))\n    fout.write(struct.pack('i' * len(shape), *shape[::-1]))\n    fout.write(sname)\n    fout.seek(fout.tell() + 31 & -32)"
        ]
    },
    {
        "func_name": "convert_non_q4",
        "original": "def convert_non_q4(src_name, dst_name, model, fout):\n    v = model[src_name]\n    shape = v.shape\n    print('Processing non-Q4 variable: ' + src_name + ' with shape: ', shape, ' and type: ', v.dtype)\n    if len(shape) == 1:\n        print('  Converting to float32')\n        v = v.to(torch.float32)\n    ftype_cur = {torch.float16: 1, torch.float32: 0}[v.dtype]\n    write_header(fout, shape, dst_name, ftype_cur)\n    v.numpy().tofile(fout)",
        "mutated": [
            "def convert_non_q4(src_name, dst_name, model, fout):\n    if False:\n        i = 10\n    v = model[src_name]\n    shape = v.shape\n    print('Processing non-Q4 variable: ' + src_name + ' with shape: ', shape, ' and type: ', v.dtype)\n    if len(shape) == 1:\n        print('  Converting to float32')\n        v = v.to(torch.float32)\n    ftype_cur = {torch.float16: 1, torch.float32: 0}[v.dtype]\n    write_header(fout, shape, dst_name, ftype_cur)\n    v.numpy().tofile(fout)",
            "def convert_non_q4(src_name, dst_name, model, fout):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    v = model[src_name]\n    shape = v.shape\n    print('Processing non-Q4 variable: ' + src_name + ' with shape: ', shape, ' and type: ', v.dtype)\n    if len(shape) == 1:\n        print('  Converting to float32')\n        v = v.to(torch.float32)\n    ftype_cur = {torch.float16: 1, torch.float32: 0}[v.dtype]\n    write_header(fout, shape, dst_name, ftype_cur)\n    v.numpy().tofile(fout)",
            "def convert_non_q4(src_name, dst_name, model, fout):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    v = model[src_name]\n    shape = v.shape\n    print('Processing non-Q4 variable: ' + src_name + ' with shape: ', shape, ' and type: ', v.dtype)\n    if len(shape) == 1:\n        print('  Converting to float32')\n        v = v.to(torch.float32)\n    ftype_cur = {torch.float16: 1, torch.float32: 0}[v.dtype]\n    write_header(fout, shape, dst_name, ftype_cur)\n    v.numpy().tofile(fout)",
            "def convert_non_q4(src_name, dst_name, model, fout):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    v = model[src_name]\n    shape = v.shape\n    print('Processing non-Q4 variable: ' + src_name + ' with shape: ', shape, ' and type: ', v.dtype)\n    if len(shape) == 1:\n        print('  Converting to float32')\n        v = v.to(torch.float32)\n    ftype_cur = {torch.float16: 1, torch.float32: 0}[v.dtype]\n    write_header(fout, shape, dst_name, ftype_cur)\n    v.numpy().tofile(fout)",
            "def convert_non_q4(src_name, dst_name, model, fout):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    v = model[src_name]\n    shape = v.shape\n    print('Processing non-Q4 variable: ' + src_name + ' with shape: ', shape, ' and type: ', v.dtype)\n    if len(shape) == 1:\n        print('  Converting to float32')\n        v = v.to(torch.float32)\n    ftype_cur = {torch.float16: 1, torch.float32: 0}[v.dtype]\n    write_header(fout, shape, dst_name, ftype_cur)\n    v.numpy().tofile(fout)"
        ]
    },
    {
        "func_name": "expandToInt4",
        "original": "def expandToInt4(qweight):\n    eweight = qweight.repeat(8, axis=2)\n    eweight = eweight.astype(np.uint32)\n    for i in range(0, eweight.shape[2]):\n        offset = i % (32 // 4) * 4\n        eweight[:, :, i] = eweight[:, :, i] >> offset & 2 ** 4 - 1\n    return eweight",
        "mutated": [
            "def expandToInt4(qweight):\n    if False:\n        i = 10\n    eweight = qweight.repeat(8, axis=2)\n    eweight = eweight.astype(np.uint32)\n    for i in range(0, eweight.shape[2]):\n        offset = i % (32 // 4) * 4\n        eweight[:, :, i] = eweight[:, :, i] >> offset & 2 ** 4 - 1\n    return eweight",
            "def expandToInt4(qweight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    eweight = qweight.repeat(8, axis=2)\n    eweight = eweight.astype(np.uint32)\n    for i in range(0, eweight.shape[2]):\n        offset = i % (32 // 4) * 4\n        eweight[:, :, i] = eweight[:, :, i] >> offset & 2 ** 4 - 1\n    return eweight",
            "def expandToInt4(qweight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    eweight = qweight.repeat(8, axis=2)\n    eweight = eweight.astype(np.uint32)\n    for i in range(0, eweight.shape[2]):\n        offset = i % (32 // 4) * 4\n        eweight[:, :, i] = eweight[:, :, i] >> offset & 2 ** 4 - 1\n    return eweight",
            "def expandToInt4(qweight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    eweight = qweight.repeat(8, axis=2)\n    eweight = eweight.astype(np.uint32)\n    for i in range(0, eweight.shape[2]):\n        offset = i % (32 // 4) * 4\n        eweight[:, :, i] = eweight[:, :, i] >> offset & 2 ** 4 - 1\n    return eweight",
            "def expandToInt4(qweight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    eweight = qweight.repeat(8, axis=2)\n    eweight = eweight.astype(np.uint32)\n    for i in range(0, eweight.shape[2]):\n        offset = i % (32 // 4) * 4\n        eweight[:, :, i] = eweight[:, :, i] >> offset & 2 ** 4 - 1\n    return eweight"
        ]
    },
    {
        "func_name": "to_ggml_int16",
        "original": "def to_ggml_int16(eweight):\n    qweight = np.zeros((eweight.shape[0], eweight.shape[1], eweight.shape[2] // 4), dtype=np.uint16)\n    eweight = np.asarray(eweight, dtype=np.uint16)\n    for i in range(0, qweight.shape[2]):\n        qweight[:, :, i] = eweight[:, :, i * 2 + 0]\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 32] << 1 * 4\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 1] << 2 * 4\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 33] << 3 * 4\n    return qweight.astype(np.int16)",
        "mutated": [
            "def to_ggml_int16(eweight):\n    if False:\n        i = 10\n    qweight = np.zeros((eweight.shape[0], eweight.shape[1], eweight.shape[2] // 4), dtype=np.uint16)\n    eweight = np.asarray(eweight, dtype=np.uint16)\n    for i in range(0, qweight.shape[2]):\n        qweight[:, :, i] = eweight[:, :, i * 2 + 0]\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 32] << 1 * 4\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 1] << 2 * 4\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 33] << 3 * 4\n    return qweight.astype(np.int16)",
            "def to_ggml_int16(eweight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    qweight = np.zeros((eweight.shape[0], eweight.shape[1], eweight.shape[2] // 4), dtype=np.uint16)\n    eweight = np.asarray(eweight, dtype=np.uint16)\n    for i in range(0, qweight.shape[2]):\n        qweight[:, :, i] = eweight[:, :, i * 2 + 0]\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 32] << 1 * 4\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 1] << 2 * 4\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 33] << 3 * 4\n    return qweight.astype(np.int16)",
            "def to_ggml_int16(eweight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    qweight = np.zeros((eweight.shape[0], eweight.shape[1], eweight.shape[2] // 4), dtype=np.uint16)\n    eweight = np.asarray(eweight, dtype=np.uint16)\n    for i in range(0, qweight.shape[2]):\n        qweight[:, :, i] = eweight[:, :, i * 2 + 0]\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 32] << 1 * 4\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 1] << 2 * 4\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 33] << 3 * 4\n    return qweight.astype(np.int16)",
            "def to_ggml_int16(eweight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    qweight = np.zeros((eweight.shape[0], eweight.shape[1], eweight.shape[2] // 4), dtype=np.uint16)\n    eweight = np.asarray(eweight, dtype=np.uint16)\n    for i in range(0, qweight.shape[2]):\n        qweight[:, :, i] = eweight[:, :, i * 2 + 0]\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 32] << 1 * 4\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 1] << 2 * 4\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 33] << 3 * 4\n    return qweight.astype(np.int16)",
            "def to_ggml_int16(eweight):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    qweight = np.zeros((eweight.shape[0], eweight.shape[1], eweight.shape[2] // 4), dtype=np.uint16)\n    eweight = np.asarray(eweight, dtype=np.uint16)\n    for i in range(0, qweight.shape[2]):\n        qweight[:, :, i] = eweight[:, :, i * 2 + 0]\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 32] << 1 * 4\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 1] << 2 * 4\n        qweight[:, :, i] |= eweight[:, :, i * 2 + 33] << 3 * 4\n    return qweight.astype(np.int16)"
        ]
    },
    {
        "func_name": "qzeros_to_zeros",
        "original": "def qzeros_to_zeros(qzeros, bits=4):\n    zeros = np.zeros((qzeros.shape[0], qzeros.shape[1] * (32 // bits)), dtype=np.float32)\n    i = 0\n    col = 0\n    while col < qzeros.shape[1]:\n        for j in range(i, i + 32 // bits):\n            zeros[:, j] = (qzeros[:, col] >> bits * (j - i) & 2 ** bits - 1) + 1\n        i += 32 // bits\n        col += 1\n    return zeros",
        "mutated": [
            "def qzeros_to_zeros(qzeros, bits=4):\n    if False:\n        i = 10\n    zeros = np.zeros((qzeros.shape[0], qzeros.shape[1] * (32 // bits)), dtype=np.float32)\n    i = 0\n    col = 0\n    while col < qzeros.shape[1]:\n        for j in range(i, i + 32 // bits):\n            zeros[:, j] = (qzeros[:, col] >> bits * (j - i) & 2 ** bits - 1) + 1\n        i += 32 // bits\n        col += 1\n    return zeros",
            "def qzeros_to_zeros(qzeros, bits=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    zeros = np.zeros((qzeros.shape[0], qzeros.shape[1] * (32 // bits)), dtype=np.float32)\n    i = 0\n    col = 0\n    while col < qzeros.shape[1]:\n        for j in range(i, i + 32 // bits):\n            zeros[:, j] = (qzeros[:, col] >> bits * (j - i) & 2 ** bits - 1) + 1\n        i += 32 // bits\n        col += 1\n    return zeros",
            "def qzeros_to_zeros(qzeros, bits=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    zeros = np.zeros((qzeros.shape[0], qzeros.shape[1] * (32 // bits)), dtype=np.float32)\n    i = 0\n    col = 0\n    while col < qzeros.shape[1]:\n        for j in range(i, i + 32 // bits):\n            zeros[:, j] = (qzeros[:, col] >> bits * (j - i) & 2 ** bits - 1) + 1\n        i += 32 // bits\n        col += 1\n    return zeros",
            "def qzeros_to_zeros(qzeros, bits=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    zeros = np.zeros((qzeros.shape[0], qzeros.shape[1] * (32 // bits)), dtype=np.float32)\n    i = 0\n    col = 0\n    while col < qzeros.shape[1]:\n        for j in range(i, i + 32 // bits):\n            zeros[:, j] = (qzeros[:, col] >> bits * (j - i) & 2 ** bits - 1) + 1\n        i += 32 // bits\n        col += 1\n    return zeros",
            "def qzeros_to_zeros(qzeros, bits=4):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    zeros = np.zeros((qzeros.shape[0], qzeros.shape[1] * (32 // bits)), dtype=np.float32)\n    i = 0\n    col = 0\n    while col < qzeros.shape[1]:\n        for j in range(i, i + 32 // bits):\n            zeros[:, j] = (qzeros[:, col] >> bits * (j - i) & 2 ** bits - 1) + 1\n        i += 32 // bits\n        col += 1\n    return zeros"
        ]
    },
    {
        "func_name": "convert_q4",
        "original": "def convert_q4(src_name, dst_name, model, fout, n_head, permute=False):\n    qzeros = model[f'{src_name}.qzeros'].numpy()\n    zeros = qzeros_to_zeros(qzeros).T\n    scales = model[f'{src_name}.scales'].numpy().T\n    g_idx = model[f'{src_name}.g_idx'].numpy()\n    qweight = model[f'{src_name}.qweight'].numpy().T\n    invalidInputError(np.all(g_idx[:-1] <= g_idx[1:]), 'Act-order is not supported, please use a no act-order model.')\n    ftype = 3\n    shape = (qweight.shape[0], qweight.shape[1] * 8)\n    print('Processing Q4 variable: ' + src_name + ' with shape: ', shape)\n    addends = -zeros * scales\n    addends_view = np.asarray(addends, dtype=np.float16).view(dtype=np.int16)\n    scales_view = np.asarray(scales, dtype=np.float16).view(dtype=np.int16)\n    expanded = expandToInt4(qweight.reshape([qweight.shape[0], qweight.shape[1] // 8, 8]))\n    grouped = to_ggml_int16(expanded)\n    if addends_view.shape[1] == grouped.shape[1]:\n        addends_rep = np.atleast_3d(addends_view)\n        scales_rep = np.atleast_3d(scales_view)\n    else:\n        addends_rep = np.atleast_3d(addends_view).repeat(grouped.shape[1] // addends_view.shape[1], axis=1)\n        scales_rep = np.atleast_3d(scales_view).repeat(grouped.shape[1] // scales_view.shape[1], axis=1)\n    blob = np.concatenate([scales_rep, addends_rep, grouped], axis=2, casting='no')\n    if permute:\n        blob = blob.reshape(n_head, 2, shape[0] // n_head // 2, *blob.shape[1:]).swapaxes(1, 2).reshape(blob.shape)\n    write_header(fout, shape, dst_name, ftype)\n    blob.tofile(fout)",
        "mutated": [
            "def convert_q4(src_name, dst_name, model, fout, n_head, permute=False):\n    if False:\n        i = 10\n    qzeros = model[f'{src_name}.qzeros'].numpy()\n    zeros = qzeros_to_zeros(qzeros).T\n    scales = model[f'{src_name}.scales'].numpy().T\n    g_idx = model[f'{src_name}.g_idx'].numpy()\n    qweight = model[f'{src_name}.qweight'].numpy().T\n    invalidInputError(np.all(g_idx[:-1] <= g_idx[1:]), 'Act-order is not supported, please use a no act-order model.')\n    ftype = 3\n    shape = (qweight.shape[0], qweight.shape[1] * 8)\n    print('Processing Q4 variable: ' + src_name + ' with shape: ', shape)\n    addends = -zeros * scales\n    addends_view = np.asarray(addends, dtype=np.float16).view(dtype=np.int16)\n    scales_view = np.asarray(scales, dtype=np.float16).view(dtype=np.int16)\n    expanded = expandToInt4(qweight.reshape([qweight.shape[0], qweight.shape[1] // 8, 8]))\n    grouped = to_ggml_int16(expanded)\n    if addends_view.shape[1] == grouped.shape[1]:\n        addends_rep = np.atleast_3d(addends_view)\n        scales_rep = np.atleast_3d(scales_view)\n    else:\n        addends_rep = np.atleast_3d(addends_view).repeat(grouped.shape[1] // addends_view.shape[1], axis=1)\n        scales_rep = np.atleast_3d(scales_view).repeat(grouped.shape[1] // scales_view.shape[1], axis=1)\n    blob = np.concatenate([scales_rep, addends_rep, grouped], axis=2, casting='no')\n    if permute:\n        blob = blob.reshape(n_head, 2, shape[0] // n_head // 2, *blob.shape[1:]).swapaxes(1, 2).reshape(blob.shape)\n    write_header(fout, shape, dst_name, ftype)\n    blob.tofile(fout)",
            "def convert_q4(src_name, dst_name, model, fout, n_head, permute=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    qzeros = model[f'{src_name}.qzeros'].numpy()\n    zeros = qzeros_to_zeros(qzeros).T\n    scales = model[f'{src_name}.scales'].numpy().T\n    g_idx = model[f'{src_name}.g_idx'].numpy()\n    qweight = model[f'{src_name}.qweight'].numpy().T\n    invalidInputError(np.all(g_idx[:-1] <= g_idx[1:]), 'Act-order is not supported, please use a no act-order model.')\n    ftype = 3\n    shape = (qweight.shape[0], qweight.shape[1] * 8)\n    print('Processing Q4 variable: ' + src_name + ' with shape: ', shape)\n    addends = -zeros * scales\n    addends_view = np.asarray(addends, dtype=np.float16).view(dtype=np.int16)\n    scales_view = np.asarray(scales, dtype=np.float16).view(dtype=np.int16)\n    expanded = expandToInt4(qweight.reshape([qweight.shape[0], qweight.shape[1] // 8, 8]))\n    grouped = to_ggml_int16(expanded)\n    if addends_view.shape[1] == grouped.shape[1]:\n        addends_rep = np.atleast_3d(addends_view)\n        scales_rep = np.atleast_3d(scales_view)\n    else:\n        addends_rep = np.atleast_3d(addends_view).repeat(grouped.shape[1] // addends_view.shape[1], axis=1)\n        scales_rep = np.atleast_3d(scales_view).repeat(grouped.shape[1] // scales_view.shape[1], axis=1)\n    blob = np.concatenate([scales_rep, addends_rep, grouped], axis=2, casting='no')\n    if permute:\n        blob = blob.reshape(n_head, 2, shape[0] // n_head // 2, *blob.shape[1:]).swapaxes(1, 2).reshape(blob.shape)\n    write_header(fout, shape, dst_name, ftype)\n    blob.tofile(fout)",
            "def convert_q4(src_name, dst_name, model, fout, n_head, permute=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    qzeros = model[f'{src_name}.qzeros'].numpy()\n    zeros = qzeros_to_zeros(qzeros).T\n    scales = model[f'{src_name}.scales'].numpy().T\n    g_idx = model[f'{src_name}.g_idx'].numpy()\n    qweight = model[f'{src_name}.qweight'].numpy().T\n    invalidInputError(np.all(g_idx[:-1] <= g_idx[1:]), 'Act-order is not supported, please use a no act-order model.')\n    ftype = 3\n    shape = (qweight.shape[0], qweight.shape[1] * 8)\n    print('Processing Q4 variable: ' + src_name + ' with shape: ', shape)\n    addends = -zeros * scales\n    addends_view = np.asarray(addends, dtype=np.float16).view(dtype=np.int16)\n    scales_view = np.asarray(scales, dtype=np.float16).view(dtype=np.int16)\n    expanded = expandToInt4(qweight.reshape([qweight.shape[0], qweight.shape[1] // 8, 8]))\n    grouped = to_ggml_int16(expanded)\n    if addends_view.shape[1] == grouped.shape[1]:\n        addends_rep = np.atleast_3d(addends_view)\n        scales_rep = np.atleast_3d(scales_view)\n    else:\n        addends_rep = np.atleast_3d(addends_view).repeat(grouped.shape[1] // addends_view.shape[1], axis=1)\n        scales_rep = np.atleast_3d(scales_view).repeat(grouped.shape[1] // scales_view.shape[1], axis=1)\n    blob = np.concatenate([scales_rep, addends_rep, grouped], axis=2, casting='no')\n    if permute:\n        blob = blob.reshape(n_head, 2, shape[0] // n_head // 2, *blob.shape[1:]).swapaxes(1, 2).reshape(blob.shape)\n    write_header(fout, shape, dst_name, ftype)\n    blob.tofile(fout)",
            "def convert_q4(src_name, dst_name, model, fout, n_head, permute=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    qzeros = model[f'{src_name}.qzeros'].numpy()\n    zeros = qzeros_to_zeros(qzeros).T\n    scales = model[f'{src_name}.scales'].numpy().T\n    g_idx = model[f'{src_name}.g_idx'].numpy()\n    qweight = model[f'{src_name}.qweight'].numpy().T\n    invalidInputError(np.all(g_idx[:-1] <= g_idx[1:]), 'Act-order is not supported, please use a no act-order model.')\n    ftype = 3\n    shape = (qweight.shape[0], qweight.shape[1] * 8)\n    print('Processing Q4 variable: ' + src_name + ' with shape: ', shape)\n    addends = -zeros * scales\n    addends_view = np.asarray(addends, dtype=np.float16).view(dtype=np.int16)\n    scales_view = np.asarray(scales, dtype=np.float16).view(dtype=np.int16)\n    expanded = expandToInt4(qweight.reshape([qweight.shape[0], qweight.shape[1] // 8, 8]))\n    grouped = to_ggml_int16(expanded)\n    if addends_view.shape[1] == grouped.shape[1]:\n        addends_rep = np.atleast_3d(addends_view)\n        scales_rep = np.atleast_3d(scales_view)\n    else:\n        addends_rep = np.atleast_3d(addends_view).repeat(grouped.shape[1] // addends_view.shape[1], axis=1)\n        scales_rep = np.atleast_3d(scales_view).repeat(grouped.shape[1] // scales_view.shape[1], axis=1)\n    blob = np.concatenate([scales_rep, addends_rep, grouped], axis=2, casting='no')\n    if permute:\n        blob = blob.reshape(n_head, 2, shape[0] // n_head // 2, *blob.shape[1:]).swapaxes(1, 2).reshape(blob.shape)\n    write_header(fout, shape, dst_name, ftype)\n    blob.tofile(fout)",
            "def convert_q4(src_name, dst_name, model, fout, n_head, permute=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    qzeros = model[f'{src_name}.qzeros'].numpy()\n    zeros = qzeros_to_zeros(qzeros).T\n    scales = model[f'{src_name}.scales'].numpy().T\n    g_idx = model[f'{src_name}.g_idx'].numpy()\n    qweight = model[f'{src_name}.qweight'].numpy().T\n    invalidInputError(np.all(g_idx[:-1] <= g_idx[1:]), 'Act-order is not supported, please use a no act-order model.')\n    ftype = 3\n    shape = (qweight.shape[0], qweight.shape[1] * 8)\n    print('Processing Q4 variable: ' + src_name + ' with shape: ', shape)\n    addends = -zeros * scales\n    addends_view = np.asarray(addends, dtype=np.float16).view(dtype=np.int16)\n    scales_view = np.asarray(scales, dtype=np.float16).view(dtype=np.int16)\n    expanded = expandToInt4(qweight.reshape([qweight.shape[0], qweight.shape[1] // 8, 8]))\n    grouped = to_ggml_int16(expanded)\n    if addends_view.shape[1] == grouped.shape[1]:\n        addends_rep = np.atleast_3d(addends_view)\n        scales_rep = np.atleast_3d(scales_view)\n    else:\n        addends_rep = np.atleast_3d(addends_view).repeat(grouped.shape[1] // addends_view.shape[1], axis=1)\n        scales_rep = np.atleast_3d(scales_view).repeat(grouped.shape[1] // scales_view.shape[1], axis=1)\n    blob = np.concatenate([scales_rep, addends_rep, grouped], axis=2, casting='no')\n    if permute:\n        blob = blob.reshape(n_head, 2, shape[0] // n_head // 2, *blob.shape[1:]).swapaxes(1, 2).reshape(blob.shape)\n    write_header(fout, shape, dst_name, ftype)\n    blob.tofile(fout)"
        ]
    },
    {
        "func_name": "find_quantized_model_file",
        "original": "def find_quantized_model_file(model_path):\n    model_path = Path(model_path)\n    for ext in ['.safetensors', '.pt']:\n        found = list(model_path.glob(f'*{ext}'))\n        if len(found) > 0:\n            if len(found) != 1:\n                warnings.warn(f'Detected {len(found)} {ext} model, use the first one {found[0]}.')\n            print(f'Detected model file {found[0]}')\n            return str(found[0])",
        "mutated": [
            "def find_quantized_model_file(model_path):\n    if False:\n        i = 10\n    model_path = Path(model_path)\n    for ext in ['.safetensors', '.pt']:\n        found = list(model_path.glob(f'*{ext}'))\n        if len(found) > 0:\n            if len(found) != 1:\n                warnings.warn(f'Detected {len(found)} {ext} model, use the first one {found[0]}.')\n            print(f'Detected model file {found[0]}')\n            return str(found[0])",
            "def find_quantized_model_file(model_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    model_path = Path(model_path)\n    for ext in ['.safetensors', '.pt']:\n        found = list(model_path.glob(f'*{ext}'))\n        if len(found) > 0:\n            if len(found) != 1:\n                warnings.warn(f'Detected {len(found)} {ext} model, use the first one {found[0]}.')\n            print(f'Detected model file {found[0]}')\n            return str(found[0])",
            "def find_quantized_model_file(model_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    model_path = Path(model_path)\n    for ext in ['.safetensors', '.pt']:\n        found = list(model_path.glob(f'*{ext}'))\n        if len(found) > 0:\n            if len(found) != 1:\n                warnings.warn(f'Detected {len(found)} {ext} model, use the first one {found[0]}.')\n            print(f'Detected model file {found[0]}')\n            return str(found[0])",
            "def find_quantized_model_file(model_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    model_path = Path(model_path)\n    for ext in ['.safetensors', '.pt']:\n        found = list(model_path.glob(f'*{ext}'))\n        if len(found) > 0:\n            if len(found) != 1:\n                warnings.warn(f'Detected {len(found)} {ext} model, use the first one {found[0]}.')\n            print(f'Detected model file {found[0]}')\n            return str(found[0])",
            "def find_quantized_model_file(model_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    model_path = Path(model_path)\n    for ext in ['.safetensors', '.pt']:\n        found = list(model_path.glob(f'*{ext}'))\n        if len(found) > 0:\n            if len(found) != 1:\n                warnings.warn(f'Detected {len(found)} {ext} model, use the first one {found[0]}.')\n            print(f'Detected model file {found[0]}')\n            return str(found[0])"
        ]
    },
    {
        "func_name": "convert_gptq2ggml",
        "original": "def convert_gptq2ggml(model_path, output_path, tokenizer_path=None):\n    input_path = find_quantized_model_file(model_path)\n    if input_path.endswith('pt'):\n        model = torch.load(input_path, map_location='cpu')\n    elif input_path.endswith('safetensors'):\n        from safetensors.torch import load_file\n        model = load_file(input_path)\n    else:\n        invalidInputError(False, 'unknown input model path, only support .safetensors or .pt file.')\n    (n_vocab, n_embd) = model['model.embed_tokens.weight'].shape\n    layer_re = 'model\\\\.layers\\\\.([0-9]+)'\n    n_layer = 1 + max((int(re.match(layer_re, name).group(1)) for name in model if re.match(layer_re, name)))\n    n_mult = 256\n    n_head = {32: 32, 40: 40, 60: 52, 80: 64}[n_layer]\n    if not tokenizer_path:\n        tokenizer_path = os.path.join(model_path, 'tokenizer.model')\n        invalidInputError(os.path.isfile(tokenizer_path), f'tokenizer.model was not found under {tokenizer_path}.Please specify the tokenizer-path')\n    tokenizer = SentencePieceProcessor(tokenizer_path)\n    vocab_size = tokenizer.vocab_size()\n    invalidInputError(vocab_size <= n_vocab, 'vocab size not match.')\n    fout = open(output_path, 'wb')\n    fout.write(b'ggjt'[::-1])\n    values = [3, n_vocab, n_embd, n_mult, n_head, n_layer, n_embd // n_head, 4]\n    fout.write(struct.pack('i' * len(values), *values))\n    for i in range(vocab_size):\n        if tokenizer.is_unknown(i):\n            text = ' \u2047 '.encode('utf-8')\n        elif tokenizer.is_control(i):\n            text = b''\n        elif tokenizer.is_byte(i):\n            piece = tokenizer.id_to_piece(i)\n            if len(piece) != 6:\n                print(f'Invalid token: {piece}')\n                sys.exit(1)\n            byte_value = int(piece[3:-1], 16)\n            text = struct.pack('B', byte_value)\n        else:\n            text = tokenizer.id_to_piece(i).replace('\u2581', ' ').encode('utf-8')\n        fout.write(struct.pack('i', len(text)))\n        fout.write(text)\n        fout.write(struct.pack('f', tokenizer.get_score(i)))\n    convert_non_q4('model.embed_tokens.weight', 'tok_embeddings.weight', model, fout)\n    convert_non_q4('model.norm.weight', 'norm.weight', model, fout)\n    convert_non_q4('lm_head.weight', 'output.weight', model, fout)\n    for i in range(n_layer):\n        convert_q4(f'model.layers.{i}.self_attn.q_proj', f'layers.{i}.attention.wq.weight', model, fout, n_head, permute=True)\n        convert_q4(f'model.layers.{i}.self_attn.k_proj', f'layers.{i}.attention.wk.weight', model, fout, n_head, permute=True)\n        convert_q4(f'model.layers.{i}.self_attn.v_proj', f'layers.{i}.attention.wv.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.self_attn.o_proj', f'layers.{i}.attention.wo.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.gate_proj', f'layers.{i}.feed_forward.w1.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.down_proj', f'layers.{i}.feed_forward.w2.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.up_proj', f'layers.{i}.feed_forward.w3.weight', model, fout, n_head)\n        convert_non_q4(f'model.layers.{i}.input_layernorm.weight', f'layers.{i}.attention_norm.weight', model, fout)\n        convert_non_q4(f'model.layers.{i}.post_attention_layernorm.weight', f'layers.{i}.ffn_norm.weight', model, fout)\n    fout.close()\n    print('Done. Output file: ' + output_path)\n    print('')",
        "mutated": [
            "def convert_gptq2ggml(model_path, output_path, tokenizer_path=None):\n    if False:\n        i = 10\n    input_path = find_quantized_model_file(model_path)\n    if input_path.endswith('pt'):\n        model = torch.load(input_path, map_location='cpu')\n    elif input_path.endswith('safetensors'):\n        from safetensors.torch import load_file\n        model = load_file(input_path)\n    else:\n        invalidInputError(False, 'unknown input model path, only support .safetensors or .pt file.')\n    (n_vocab, n_embd) = model['model.embed_tokens.weight'].shape\n    layer_re = 'model\\\\.layers\\\\.([0-9]+)'\n    n_layer = 1 + max((int(re.match(layer_re, name).group(1)) for name in model if re.match(layer_re, name)))\n    n_mult = 256\n    n_head = {32: 32, 40: 40, 60: 52, 80: 64}[n_layer]\n    if not tokenizer_path:\n        tokenizer_path = os.path.join(model_path, 'tokenizer.model')\n        invalidInputError(os.path.isfile(tokenizer_path), f'tokenizer.model was not found under {tokenizer_path}.Please specify the tokenizer-path')\n    tokenizer = SentencePieceProcessor(tokenizer_path)\n    vocab_size = tokenizer.vocab_size()\n    invalidInputError(vocab_size <= n_vocab, 'vocab size not match.')\n    fout = open(output_path, 'wb')\n    fout.write(b'ggjt'[::-1])\n    values = [3, n_vocab, n_embd, n_mult, n_head, n_layer, n_embd // n_head, 4]\n    fout.write(struct.pack('i' * len(values), *values))\n    for i in range(vocab_size):\n        if tokenizer.is_unknown(i):\n            text = ' \u2047 '.encode('utf-8')\n        elif tokenizer.is_control(i):\n            text = b''\n        elif tokenizer.is_byte(i):\n            piece = tokenizer.id_to_piece(i)\n            if len(piece) != 6:\n                print(f'Invalid token: {piece}')\n                sys.exit(1)\n            byte_value = int(piece[3:-1], 16)\n            text = struct.pack('B', byte_value)\n        else:\n            text = tokenizer.id_to_piece(i).replace('\u2581', ' ').encode('utf-8')\n        fout.write(struct.pack('i', len(text)))\n        fout.write(text)\n        fout.write(struct.pack('f', tokenizer.get_score(i)))\n    convert_non_q4('model.embed_tokens.weight', 'tok_embeddings.weight', model, fout)\n    convert_non_q4('model.norm.weight', 'norm.weight', model, fout)\n    convert_non_q4('lm_head.weight', 'output.weight', model, fout)\n    for i in range(n_layer):\n        convert_q4(f'model.layers.{i}.self_attn.q_proj', f'layers.{i}.attention.wq.weight', model, fout, n_head, permute=True)\n        convert_q4(f'model.layers.{i}.self_attn.k_proj', f'layers.{i}.attention.wk.weight', model, fout, n_head, permute=True)\n        convert_q4(f'model.layers.{i}.self_attn.v_proj', f'layers.{i}.attention.wv.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.self_attn.o_proj', f'layers.{i}.attention.wo.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.gate_proj', f'layers.{i}.feed_forward.w1.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.down_proj', f'layers.{i}.feed_forward.w2.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.up_proj', f'layers.{i}.feed_forward.w3.weight', model, fout, n_head)\n        convert_non_q4(f'model.layers.{i}.input_layernorm.weight', f'layers.{i}.attention_norm.weight', model, fout)\n        convert_non_q4(f'model.layers.{i}.post_attention_layernorm.weight', f'layers.{i}.ffn_norm.weight', model, fout)\n    fout.close()\n    print('Done. Output file: ' + output_path)\n    print('')",
            "def convert_gptq2ggml(model_path, output_path, tokenizer_path=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    input_path = find_quantized_model_file(model_path)\n    if input_path.endswith('pt'):\n        model = torch.load(input_path, map_location='cpu')\n    elif input_path.endswith('safetensors'):\n        from safetensors.torch import load_file\n        model = load_file(input_path)\n    else:\n        invalidInputError(False, 'unknown input model path, only support .safetensors or .pt file.')\n    (n_vocab, n_embd) = model['model.embed_tokens.weight'].shape\n    layer_re = 'model\\\\.layers\\\\.([0-9]+)'\n    n_layer = 1 + max((int(re.match(layer_re, name).group(1)) for name in model if re.match(layer_re, name)))\n    n_mult = 256\n    n_head = {32: 32, 40: 40, 60: 52, 80: 64}[n_layer]\n    if not tokenizer_path:\n        tokenizer_path = os.path.join(model_path, 'tokenizer.model')\n        invalidInputError(os.path.isfile(tokenizer_path), f'tokenizer.model was not found under {tokenizer_path}.Please specify the tokenizer-path')\n    tokenizer = SentencePieceProcessor(tokenizer_path)\n    vocab_size = tokenizer.vocab_size()\n    invalidInputError(vocab_size <= n_vocab, 'vocab size not match.')\n    fout = open(output_path, 'wb')\n    fout.write(b'ggjt'[::-1])\n    values = [3, n_vocab, n_embd, n_mult, n_head, n_layer, n_embd // n_head, 4]\n    fout.write(struct.pack('i' * len(values), *values))\n    for i in range(vocab_size):\n        if tokenizer.is_unknown(i):\n            text = ' \u2047 '.encode('utf-8')\n        elif tokenizer.is_control(i):\n            text = b''\n        elif tokenizer.is_byte(i):\n            piece = tokenizer.id_to_piece(i)\n            if len(piece) != 6:\n                print(f'Invalid token: {piece}')\n                sys.exit(1)\n            byte_value = int(piece[3:-1], 16)\n            text = struct.pack('B', byte_value)\n        else:\n            text = tokenizer.id_to_piece(i).replace('\u2581', ' ').encode('utf-8')\n        fout.write(struct.pack('i', len(text)))\n        fout.write(text)\n        fout.write(struct.pack('f', tokenizer.get_score(i)))\n    convert_non_q4('model.embed_tokens.weight', 'tok_embeddings.weight', model, fout)\n    convert_non_q4('model.norm.weight', 'norm.weight', model, fout)\n    convert_non_q4('lm_head.weight', 'output.weight', model, fout)\n    for i in range(n_layer):\n        convert_q4(f'model.layers.{i}.self_attn.q_proj', f'layers.{i}.attention.wq.weight', model, fout, n_head, permute=True)\n        convert_q4(f'model.layers.{i}.self_attn.k_proj', f'layers.{i}.attention.wk.weight', model, fout, n_head, permute=True)\n        convert_q4(f'model.layers.{i}.self_attn.v_proj', f'layers.{i}.attention.wv.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.self_attn.o_proj', f'layers.{i}.attention.wo.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.gate_proj', f'layers.{i}.feed_forward.w1.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.down_proj', f'layers.{i}.feed_forward.w2.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.up_proj', f'layers.{i}.feed_forward.w3.weight', model, fout, n_head)\n        convert_non_q4(f'model.layers.{i}.input_layernorm.weight', f'layers.{i}.attention_norm.weight', model, fout)\n        convert_non_q4(f'model.layers.{i}.post_attention_layernorm.weight', f'layers.{i}.ffn_norm.weight', model, fout)\n    fout.close()\n    print('Done. Output file: ' + output_path)\n    print('')",
            "def convert_gptq2ggml(model_path, output_path, tokenizer_path=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    input_path = find_quantized_model_file(model_path)\n    if input_path.endswith('pt'):\n        model = torch.load(input_path, map_location='cpu')\n    elif input_path.endswith('safetensors'):\n        from safetensors.torch import load_file\n        model = load_file(input_path)\n    else:\n        invalidInputError(False, 'unknown input model path, only support .safetensors or .pt file.')\n    (n_vocab, n_embd) = model['model.embed_tokens.weight'].shape\n    layer_re = 'model\\\\.layers\\\\.([0-9]+)'\n    n_layer = 1 + max((int(re.match(layer_re, name).group(1)) for name in model if re.match(layer_re, name)))\n    n_mult = 256\n    n_head = {32: 32, 40: 40, 60: 52, 80: 64}[n_layer]\n    if not tokenizer_path:\n        tokenizer_path = os.path.join(model_path, 'tokenizer.model')\n        invalidInputError(os.path.isfile(tokenizer_path), f'tokenizer.model was not found under {tokenizer_path}.Please specify the tokenizer-path')\n    tokenizer = SentencePieceProcessor(tokenizer_path)\n    vocab_size = tokenizer.vocab_size()\n    invalidInputError(vocab_size <= n_vocab, 'vocab size not match.')\n    fout = open(output_path, 'wb')\n    fout.write(b'ggjt'[::-1])\n    values = [3, n_vocab, n_embd, n_mult, n_head, n_layer, n_embd // n_head, 4]\n    fout.write(struct.pack('i' * len(values), *values))\n    for i in range(vocab_size):\n        if tokenizer.is_unknown(i):\n            text = ' \u2047 '.encode('utf-8')\n        elif tokenizer.is_control(i):\n            text = b''\n        elif tokenizer.is_byte(i):\n            piece = tokenizer.id_to_piece(i)\n            if len(piece) != 6:\n                print(f'Invalid token: {piece}')\n                sys.exit(1)\n            byte_value = int(piece[3:-1], 16)\n            text = struct.pack('B', byte_value)\n        else:\n            text = tokenizer.id_to_piece(i).replace('\u2581', ' ').encode('utf-8')\n        fout.write(struct.pack('i', len(text)))\n        fout.write(text)\n        fout.write(struct.pack('f', tokenizer.get_score(i)))\n    convert_non_q4('model.embed_tokens.weight', 'tok_embeddings.weight', model, fout)\n    convert_non_q4('model.norm.weight', 'norm.weight', model, fout)\n    convert_non_q4('lm_head.weight', 'output.weight', model, fout)\n    for i in range(n_layer):\n        convert_q4(f'model.layers.{i}.self_attn.q_proj', f'layers.{i}.attention.wq.weight', model, fout, n_head, permute=True)\n        convert_q4(f'model.layers.{i}.self_attn.k_proj', f'layers.{i}.attention.wk.weight', model, fout, n_head, permute=True)\n        convert_q4(f'model.layers.{i}.self_attn.v_proj', f'layers.{i}.attention.wv.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.self_attn.o_proj', f'layers.{i}.attention.wo.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.gate_proj', f'layers.{i}.feed_forward.w1.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.down_proj', f'layers.{i}.feed_forward.w2.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.up_proj', f'layers.{i}.feed_forward.w3.weight', model, fout, n_head)\n        convert_non_q4(f'model.layers.{i}.input_layernorm.weight', f'layers.{i}.attention_norm.weight', model, fout)\n        convert_non_q4(f'model.layers.{i}.post_attention_layernorm.weight', f'layers.{i}.ffn_norm.weight', model, fout)\n    fout.close()\n    print('Done. Output file: ' + output_path)\n    print('')",
            "def convert_gptq2ggml(model_path, output_path, tokenizer_path=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    input_path = find_quantized_model_file(model_path)\n    if input_path.endswith('pt'):\n        model = torch.load(input_path, map_location='cpu')\n    elif input_path.endswith('safetensors'):\n        from safetensors.torch import load_file\n        model = load_file(input_path)\n    else:\n        invalidInputError(False, 'unknown input model path, only support .safetensors or .pt file.')\n    (n_vocab, n_embd) = model['model.embed_tokens.weight'].shape\n    layer_re = 'model\\\\.layers\\\\.([0-9]+)'\n    n_layer = 1 + max((int(re.match(layer_re, name).group(1)) for name in model if re.match(layer_re, name)))\n    n_mult = 256\n    n_head = {32: 32, 40: 40, 60: 52, 80: 64}[n_layer]\n    if not tokenizer_path:\n        tokenizer_path = os.path.join(model_path, 'tokenizer.model')\n        invalidInputError(os.path.isfile(tokenizer_path), f'tokenizer.model was not found under {tokenizer_path}.Please specify the tokenizer-path')\n    tokenizer = SentencePieceProcessor(tokenizer_path)\n    vocab_size = tokenizer.vocab_size()\n    invalidInputError(vocab_size <= n_vocab, 'vocab size not match.')\n    fout = open(output_path, 'wb')\n    fout.write(b'ggjt'[::-1])\n    values = [3, n_vocab, n_embd, n_mult, n_head, n_layer, n_embd // n_head, 4]\n    fout.write(struct.pack('i' * len(values), *values))\n    for i in range(vocab_size):\n        if tokenizer.is_unknown(i):\n            text = ' \u2047 '.encode('utf-8')\n        elif tokenizer.is_control(i):\n            text = b''\n        elif tokenizer.is_byte(i):\n            piece = tokenizer.id_to_piece(i)\n            if len(piece) != 6:\n                print(f'Invalid token: {piece}')\n                sys.exit(1)\n            byte_value = int(piece[3:-1], 16)\n            text = struct.pack('B', byte_value)\n        else:\n            text = tokenizer.id_to_piece(i).replace('\u2581', ' ').encode('utf-8')\n        fout.write(struct.pack('i', len(text)))\n        fout.write(text)\n        fout.write(struct.pack('f', tokenizer.get_score(i)))\n    convert_non_q4('model.embed_tokens.weight', 'tok_embeddings.weight', model, fout)\n    convert_non_q4('model.norm.weight', 'norm.weight', model, fout)\n    convert_non_q4('lm_head.weight', 'output.weight', model, fout)\n    for i in range(n_layer):\n        convert_q4(f'model.layers.{i}.self_attn.q_proj', f'layers.{i}.attention.wq.weight', model, fout, n_head, permute=True)\n        convert_q4(f'model.layers.{i}.self_attn.k_proj', f'layers.{i}.attention.wk.weight', model, fout, n_head, permute=True)\n        convert_q4(f'model.layers.{i}.self_attn.v_proj', f'layers.{i}.attention.wv.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.self_attn.o_proj', f'layers.{i}.attention.wo.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.gate_proj', f'layers.{i}.feed_forward.w1.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.down_proj', f'layers.{i}.feed_forward.w2.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.up_proj', f'layers.{i}.feed_forward.w3.weight', model, fout, n_head)\n        convert_non_q4(f'model.layers.{i}.input_layernorm.weight', f'layers.{i}.attention_norm.weight', model, fout)\n        convert_non_q4(f'model.layers.{i}.post_attention_layernorm.weight', f'layers.{i}.ffn_norm.weight', model, fout)\n    fout.close()\n    print('Done. Output file: ' + output_path)\n    print('')",
            "def convert_gptq2ggml(model_path, output_path, tokenizer_path=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    input_path = find_quantized_model_file(model_path)\n    if input_path.endswith('pt'):\n        model = torch.load(input_path, map_location='cpu')\n    elif input_path.endswith('safetensors'):\n        from safetensors.torch import load_file\n        model = load_file(input_path)\n    else:\n        invalidInputError(False, 'unknown input model path, only support .safetensors or .pt file.')\n    (n_vocab, n_embd) = model['model.embed_tokens.weight'].shape\n    layer_re = 'model\\\\.layers\\\\.([0-9]+)'\n    n_layer = 1 + max((int(re.match(layer_re, name).group(1)) for name in model if re.match(layer_re, name)))\n    n_mult = 256\n    n_head = {32: 32, 40: 40, 60: 52, 80: 64}[n_layer]\n    if not tokenizer_path:\n        tokenizer_path = os.path.join(model_path, 'tokenizer.model')\n        invalidInputError(os.path.isfile(tokenizer_path), f'tokenizer.model was not found under {tokenizer_path}.Please specify the tokenizer-path')\n    tokenizer = SentencePieceProcessor(tokenizer_path)\n    vocab_size = tokenizer.vocab_size()\n    invalidInputError(vocab_size <= n_vocab, 'vocab size not match.')\n    fout = open(output_path, 'wb')\n    fout.write(b'ggjt'[::-1])\n    values = [3, n_vocab, n_embd, n_mult, n_head, n_layer, n_embd // n_head, 4]\n    fout.write(struct.pack('i' * len(values), *values))\n    for i in range(vocab_size):\n        if tokenizer.is_unknown(i):\n            text = ' \u2047 '.encode('utf-8')\n        elif tokenizer.is_control(i):\n            text = b''\n        elif tokenizer.is_byte(i):\n            piece = tokenizer.id_to_piece(i)\n            if len(piece) != 6:\n                print(f'Invalid token: {piece}')\n                sys.exit(1)\n            byte_value = int(piece[3:-1], 16)\n            text = struct.pack('B', byte_value)\n        else:\n            text = tokenizer.id_to_piece(i).replace('\u2581', ' ').encode('utf-8')\n        fout.write(struct.pack('i', len(text)))\n        fout.write(text)\n        fout.write(struct.pack('f', tokenizer.get_score(i)))\n    convert_non_q4('model.embed_tokens.weight', 'tok_embeddings.weight', model, fout)\n    convert_non_q4('model.norm.weight', 'norm.weight', model, fout)\n    convert_non_q4('lm_head.weight', 'output.weight', model, fout)\n    for i in range(n_layer):\n        convert_q4(f'model.layers.{i}.self_attn.q_proj', f'layers.{i}.attention.wq.weight', model, fout, n_head, permute=True)\n        convert_q4(f'model.layers.{i}.self_attn.k_proj', f'layers.{i}.attention.wk.weight', model, fout, n_head, permute=True)\n        convert_q4(f'model.layers.{i}.self_attn.v_proj', f'layers.{i}.attention.wv.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.self_attn.o_proj', f'layers.{i}.attention.wo.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.gate_proj', f'layers.{i}.feed_forward.w1.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.down_proj', f'layers.{i}.feed_forward.w2.weight', model, fout, n_head)\n        convert_q4(f'model.layers.{i}.mlp.up_proj', f'layers.{i}.feed_forward.w3.weight', model, fout, n_head)\n        convert_non_q4(f'model.layers.{i}.input_layernorm.weight', f'layers.{i}.attention_norm.weight', model, fout)\n        convert_non_q4(f'model.layers.{i}.post_attention_layernorm.weight', f'layers.{i}.ffn_norm.weight', model, fout)\n    fout.close()\n    print('Done. Output file: ' + output_path)\n    print('')"
        ]
    }
]