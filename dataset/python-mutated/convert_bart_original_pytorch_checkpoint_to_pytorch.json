[
    {
        "func_name": "remove_ignore_keys_",
        "original": "def remove_ignore_keys_(state_dict):\n    ignore_keys = ['encoder.version', 'decoder.version', 'model.encoder.version', 'model.decoder.version', '_float_tensor']\n    for k in ignore_keys:\n        state_dict.pop(k, None)",
        "mutated": [
            "def remove_ignore_keys_(state_dict):\n    if False:\n        i = 10\n    ignore_keys = ['encoder.version', 'decoder.version', 'model.encoder.version', 'model.decoder.version', '_float_tensor']\n    for k in ignore_keys:\n        state_dict.pop(k, None)",
            "def remove_ignore_keys_(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    ignore_keys = ['encoder.version', 'decoder.version', 'model.encoder.version', 'model.decoder.version', '_float_tensor']\n    for k in ignore_keys:\n        state_dict.pop(k, None)",
            "def remove_ignore_keys_(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    ignore_keys = ['encoder.version', 'decoder.version', 'model.encoder.version', 'model.decoder.version', '_float_tensor']\n    for k in ignore_keys:\n        state_dict.pop(k, None)",
            "def remove_ignore_keys_(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    ignore_keys = ['encoder.version', 'decoder.version', 'model.encoder.version', 'model.decoder.version', '_float_tensor']\n    for k in ignore_keys:\n        state_dict.pop(k, None)",
            "def remove_ignore_keys_(state_dict):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    ignore_keys = ['encoder.version', 'decoder.version', 'model.encoder.version', 'model.decoder.version', '_float_tensor']\n    for k in ignore_keys:\n        state_dict.pop(k, None)"
        ]
    },
    {
        "func_name": "rename_key",
        "original": "def rename_key(dct, old, new):\n    val = dct.pop(old)\n    dct[new] = val",
        "mutated": [
            "def rename_key(dct, old, new):\n    if False:\n        i = 10\n    val = dct.pop(old)\n    dct[new] = val",
            "def rename_key(dct, old, new):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    val = dct.pop(old)\n    dct[new] = val",
            "def rename_key(dct, old, new):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    val = dct.pop(old)\n    dct[new] = val",
            "def rename_key(dct, old, new):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    val = dct.pop(old)\n    dct[new] = val",
            "def rename_key(dct, old, new):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    val = dct.pop(old)\n    dct[new] = val"
        ]
    },
    {
        "func_name": "load_xsum_checkpoint",
        "original": "def load_xsum_checkpoint(checkpoint_path):\n    \"\"\"Checkpoint path should end in model.pt\"\"\"\n    sd = torch.load(checkpoint_path, map_location='cpu')\n    hub_interface = torch.hub.load('pytorch/fairseq', 'bart.large.cnn').eval()\n    hub_interface.model.load_state_dict(sd['model'])\n    return hub_interface",
        "mutated": [
            "def load_xsum_checkpoint(checkpoint_path):\n    if False:\n        i = 10\n    'Checkpoint path should end in model.pt'\n    sd = torch.load(checkpoint_path, map_location='cpu')\n    hub_interface = torch.hub.load('pytorch/fairseq', 'bart.large.cnn').eval()\n    hub_interface.model.load_state_dict(sd['model'])\n    return hub_interface",
            "def load_xsum_checkpoint(checkpoint_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Checkpoint path should end in model.pt'\n    sd = torch.load(checkpoint_path, map_location='cpu')\n    hub_interface = torch.hub.load('pytorch/fairseq', 'bart.large.cnn').eval()\n    hub_interface.model.load_state_dict(sd['model'])\n    return hub_interface",
            "def load_xsum_checkpoint(checkpoint_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Checkpoint path should end in model.pt'\n    sd = torch.load(checkpoint_path, map_location='cpu')\n    hub_interface = torch.hub.load('pytorch/fairseq', 'bart.large.cnn').eval()\n    hub_interface.model.load_state_dict(sd['model'])\n    return hub_interface",
            "def load_xsum_checkpoint(checkpoint_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Checkpoint path should end in model.pt'\n    sd = torch.load(checkpoint_path, map_location='cpu')\n    hub_interface = torch.hub.load('pytorch/fairseq', 'bart.large.cnn').eval()\n    hub_interface.model.load_state_dict(sd['model'])\n    return hub_interface",
            "def load_xsum_checkpoint(checkpoint_path):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Checkpoint path should end in model.pt'\n    sd = torch.load(checkpoint_path, map_location='cpu')\n    hub_interface = torch.hub.load('pytorch/fairseq', 'bart.large.cnn').eval()\n    hub_interface.model.load_state_dict(sd['model'])\n    return hub_interface"
        ]
    },
    {
        "func_name": "make_linear_from_emb",
        "original": "def make_linear_from_emb(emb):\n    (vocab_size, emb_size) = emb.weight.shape\n    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n    lin_layer.weight.data = emb.weight.data\n    return lin_layer",
        "mutated": [
            "def make_linear_from_emb(emb):\n    if False:\n        i = 10\n    (vocab_size, emb_size) = emb.weight.shape\n    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n    lin_layer.weight.data = emb.weight.data\n    return lin_layer",
            "def make_linear_from_emb(emb):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (vocab_size, emb_size) = emb.weight.shape\n    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n    lin_layer.weight.data = emb.weight.data\n    return lin_layer",
            "def make_linear_from_emb(emb):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (vocab_size, emb_size) = emb.weight.shape\n    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n    lin_layer.weight.data = emb.weight.data\n    return lin_layer",
            "def make_linear_from_emb(emb):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (vocab_size, emb_size) = emb.weight.shape\n    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n    lin_layer.weight.data = emb.weight.data\n    return lin_layer",
            "def make_linear_from_emb(emb):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (vocab_size, emb_size) = emb.weight.shape\n    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n    lin_layer.weight.data = emb.weight.data\n    return lin_layer"
        ]
    },
    {
        "func_name": "convert_bart_checkpoint",
        "original": "@torch.no_grad()\ndef convert_bart_checkpoint(checkpoint_path, pytorch_dump_folder_path, hf_checkpoint_name=None):\n    \"\"\"\n    Copy/paste/tweak model's weights to our BERT structure.\n    \"\"\"\n    if not os.path.exists(checkpoint_path):\n        bart = torch.hub.load('pytorch/fairseq', checkpoint_path).eval()\n    else:\n        bart = load_xsum_checkpoint(checkpoint_path)\n    bart.model.upgrade_state_dict(bart.model.state_dict())\n    if hf_checkpoint_name is None:\n        hf_checkpoint_name = checkpoint_path.replace('.', '-')\n    config = BartConfig.from_pretrained(hf_checkpoint_name)\n    tokens = bart.encode(SAMPLE_TEXT).unsqueeze(0)\n    tokens2 = BartTokenizer.from_pretrained(hf_checkpoint_name).encode(SAMPLE_TEXT, return_tensors='pt').unsqueeze(0)\n    if not torch.eq(tokens, tokens2).all():\n        raise ValueError(f'converted tokenizer and pretrained tokenizer returned different output: {tokens} != {tokens2}')\n    if checkpoint_path == 'bart.large.mnli':\n        state_dict = bart.state_dict()\n        remove_ignore_keys_(state_dict)\n        state_dict['model.shared.weight'] = state_dict['model.decoder.embed_tokens.weight']\n        for (src, dest) in mnli_rename_keys:\n            rename_key(state_dict, src, dest)\n        model = BartForSequenceClassification(config).eval()\n        model.load_state_dict(state_dict)\n        fairseq_output = bart.predict('mnli', tokens, return_logits=True)\n        new_model_outputs = model(tokens)[0]\n    else:\n        state_dict = bart.model.state_dict()\n        remove_ignore_keys_(state_dict)\n        state_dict['shared.weight'] = state_dict['decoder.embed_tokens.weight']\n        fairseq_output = bart.extract_features(tokens)\n        if hf_checkpoint_name == 'facebook/bart-large':\n            model = BartModel(config).eval()\n            model.load_state_dict(state_dict)\n            new_model_outputs = model(tokens).model[0]\n        else:\n            model = BartForConditionalGeneration(config).eval()\n            model.model.load_state_dict(state_dict)\n            if hasattr(model, 'lm_head'):\n                model.lm_head = make_linear_from_emb(model.model.shared)\n            new_model_outputs = model.model(tokens)[0]\n    if fairseq_output.shape != new_model_outputs.shape:\n        raise ValueError(f'`fairseq_output` shape and `new_model_output` shape are different: fairseq_output.shape={fairseq_output.shape!r}, {new_model_outputs.shape}')\n    if (fairseq_output != new_model_outputs).any().item():\n        raise ValueError('Some values in `fairseq_output` are different from `new_model_outputs`')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)",
        "mutated": [
            "@torch.no_grad()\ndef convert_bart_checkpoint(checkpoint_path, pytorch_dump_folder_path, hf_checkpoint_name=None):\n    if False:\n        i = 10\n    \"\\n    Copy/paste/tweak model's weights to our BERT structure.\\n    \"\n    if not os.path.exists(checkpoint_path):\n        bart = torch.hub.load('pytorch/fairseq', checkpoint_path).eval()\n    else:\n        bart = load_xsum_checkpoint(checkpoint_path)\n    bart.model.upgrade_state_dict(bart.model.state_dict())\n    if hf_checkpoint_name is None:\n        hf_checkpoint_name = checkpoint_path.replace('.', '-')\n    config = BartConfig.from_pretrained(hf_checkpoint_name)\n    tokens = bart.encode(SAMPLE_TEXT).unsqueeze(0)\n    tokens2 = BartTokenizer.from_pretrained(hf_checkpoint_name).encode(SAMPLE_TEXT, return_tensors='pt').unsqueeze(0)\n    if not torch.eq(tokens, tokens2).all():\n        raise ValueError(f'converted tokenizer and pretrained tokenizer returned different output: {tokens} != {tokens2}')\n    if checkpoint_path == 'bart.large.mnli':\n        state_dict = bart.state_dict()\n        remove_ignore_keys_(state_dict)\n        state_dict['model.shared.weight'] = state_dict['model.decoder.embed_tokens.weight']\n        for (src, dest) in mnli_rename_keys:\n            rename_key(state_dict, src, dest)\n        model = BartForSequenceClassification(config).eval()\n        model.load_state_dict(state_dict)\n        fairseq_output = bart.predict('mnli', tokens, return_logits=True)\n        new_model_outputs = model(tokens)[0]\n    else:\n        state_dict = bart.model.state_dict()\n        remove_ignore_keys_(state_dict)\n        state_dict['shared.weight'] = state_dict['decoder.embed_tokens.weight']\n        fairseq_output = bart.extract_features(tokens)\n        if hf_checkpoint_name == 'facebook/bart-large':\n            model = BartModel(config).eval()\n            model.load_state_dict(state_dict)\n            new_model_outputs = model(tokens).model[0]\n        else:\n            model = BartForConditionalGeneration(config).eval()\n            model.model.load_state_dict(state_dict)\n            if hasattr(model, 'lm_head'):\n                model.lm_head = make_linear_from_emb(model.model.shared)\n            new_model_outputs = model.model(tokens)[0]\n    if fairseq_output.shape != new_model_outputs.shape:\n        raise ValueError(f'`fairseq_output` shape and `new_model_output` shape are different: fairseq_output.shape={fairseq_output.shape!r}, {new_model_outputs.shape}')\n    if (fairseq_output != new_model_outputs).any().item():\n        raise ValueError('Some values in `fairseq_output` are different from `new_model_outputs`')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_bart_checkpoint(checkpoint_path, pytorch_dump_folder_path, hf_checkpoint_name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"\\n    Copy/paste/tweak model's weights to our BERT structure.\\n    \"\n    if not os.path.exists(checkpoint_path):\n        bart = torch.hub.load('pytorch/fairseq', checkpoint_path).eval()\n    else:\n        bart = load_xsum_checkpoint(checkpoint_path)\n    bart.model.upgrade_state_dict(bart.model.state_dict())\n    if hf_checkpoint_name is None:\n        hf_checkpoint_name = checkpoint_path.replace('.', '-')\n    config = BartConfig.from_pretrained(hf_checkpoint_name)\n    tokens = bart.encode(SAMPLE_TEXT).unsqueeze(0)\n    tokens2 = BartTokenizer.from_pretrained(hf_checkpoint_name).encode(SAMPLE_TEXT, return_tensors='pt').unsqueeze(0)\n    if not torch.eq(tokens, tokens2).all():\n        raise ValueError(f'converted tokenizer and pretrained tokenizer returned different output: {tokens} != {tokens2}')\n    if checkpoint_path == 'bart.large.mnli':\n        state_dict = bart.state_dict()\n        remove_ignore_keys_(state_dict)\n        state_dict['model.shared.weight'] = state_dict['model.decoder.embed_tokens.weight']\n        for (src, dest) in mnli_rename_keys:\n            rename_key(state_dict, src, dest)\n        model = BartForSequenceClassification(config).eval()\n        model.load_state_dict(state_dict)\n        fairseq_output = bart.predict('mnli', tokens, return_logits=True)\n        new_model_outputs = model(tokens)[0]\n    else:\n        state_dict = bart.model.state_dict()\n        remove_ignore_keys_(state_dict)\n        state_dict['shared.weight'] = state_dict['decoder.embed_tokens.weight']\n        fairseq_output = bart.extract_features(tokens)\n        if hf_checkpoint_name == 'facebook/bart-large':\n            model = BartModel(config).eval()\n            model.load_state_dict(state_dict)\n            new_model_outputs = model(tokens).model[0]\n        else:\n            model = BartForConditionalGeneration(config).eval()\n            model.model.load_state_dict(state_dict)\n            if hasattr(model, 'lm_head'):\n                model.lm_head = make_linear_from_emb(model.model.shared)\n            new_model_outputs = model.model(tokens)[0]\n    if fairseq_output.shape != new_model_outputs.shape:\n        raise ValueError(f'`fairseq_output` shape and `new_model_output` shape are different: fairseq_output.shape={fairseq_output.shape!r}, {new_model_outputs.shape}')\n    if (fairseq_output != new_model_outputs).any().item():\n        raise ValueError('Some values in `fairseq_output` are different from `new_model_outputs`')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_bart_checkpoint(checkpoint_path, pytorch_dump_folder_path, hf_checkpoint_name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"\\n    Copy/paste/tweak model's weights to our BERT structure.\\n    \"\n    if not os.path.exists(checkpoint_path):\n        bart = torch.hub.load('pytorch/fairseq', checkpoint_path).eval()\n    else:\n        bart = load_xsum_checkpoint(checkpoint_path)\n    bart.model.upgrade_state_dict(bart.model.state_dict())\n    if hf_checkpoint_name is None:\n        hf_checkpoint_name = checkpoint_path.replace('.', '-')\n    config = BartConfig.from_pretrained(hf_checkpoint_name)\n    tokens = bart.encode(SAMPLE_TEXT).unsqueeze(0)\n    tokens2 = BartTokenizer.from_pretrained(hf_checkpoint_name).encode(SAMPLE_TEXT, return_tensors='pt').unsqueeze(0)\n    if not torch.eq(tokens, tokens2).all():\n        raise ValueError(f'converted tokenizer and pretrained tokenizer returned different output: {tokens} != {tokens2}')\n    if checkpoint_path == 'bart.large.mnli':\n        state_dict = bart.state_dict()\n        remove_ignore_keys_(state_dict)\n        state_dict['model.shared.weight'] = state_dict['model.decoder.embed_tokens.weight']\n        for (src, dest) in mnli_rename_keys:\n            rename_key(state_dict, src, dest)\n        model = BartForSequenceClassification(config).eval()\n        model.load_state_dict(state_dict)\n        fairseq_output = bart.predict('mnli', tokens, return_logits=True)\n        new_model_outputs = model(tokens)[0]\n    else:\n        state_dict = bart.model.state_dict()\n        remove_ignore_keys_(state_dict)\n        state_dict['shared.weight'] = state_dict['decoder.embed_tokens.weight']\n        fairseq_output = bart.extract_features(tokens)\n        if hf_checkpoint_name == 'facebook/bart-large':\n            model = BartModel(config).eval()\n            model.load_state_dict(state_dict)\n            new_model_outputs = model(tokens).model[0]\n        else:\n            model = BartForConditionalGeneration(config).eval()\n            model.model.load_state_dict(state_dict)\n            if hasattr(model, 'lm_head'):\n                model.lm_head = make_linear_from_emb(model.model.shared)\n            new_model_outputs = model.model(tokens)[0]\n    if fairseq_output.shape != new_model_outputs.shape:\n        raise ValueError(f'`fairseq_output` shape and `new_model_output` shape are different: fairseq_output.shape={fairseq_output.shape!r}, {new_model_outputs.shape}')\n    if (fairseq_output != new_model_outputs).any().item():\n        raise ValueError('Some values in `fairseq_output` are different from `new_model_outputs`')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_bart_checkpoint(checkpoint_path, pytorch_dump_folder_path, hf_checkpoint_name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"\\n    Copy/paste/tweak model's weights to our BERT structure.\\n    \"\n    if not os.path.exists(checkpoint_path):\n        bart = torch.hub.load('pytorch/fairseq', checkpoint_path).eval()\n    else:\n        bart = load_xsum_checkpoint(checkpoint_path)\n    bart.model.upgrade_state_dict(bart.model.state_dict())\n    if hf_checkpoint_name is None:\n        hf_checkpoint_name = checkpoint_path.replace('.', '-')\n    config = BartConfig.from_pretrained(hf_checkpoint_name)\n    tokens = bart.encode(SAMPLE_TEXT).unsqueeze(0)\n    tokens2 = BartTokenizer.from_pretrained(hf_checkpoint_name).encode(SAMPLE_TEXT, return_tensors='pt').unsqueeze(0)\n    if not torch.eq(tokens, tokens2).all():\n        raise ValueError(f'converted tokenizer and pretrained tokenizer returned different output: {tokens} != {tokens2}')\n    if checkpoint_path == 'bart.large.mnli':\n        state_dict = bart.state_dict()\n        remove_ignore_keys_(state_dict)\n        state_dict['model.shared.weight'] = state_dict['model.decoder.embed_tokens.weight']\n        for (src, dest) in mnli_rename_keys:\n            rename_key(state_dict, src, dest)\n        model = BartForSequenceClassification(config).eval()\n        model.load_state_dict(state_dict)\n        fairseq_output = bart.predict('mnli', tokens, return_logits=True)\n        new_model_outputs = model(tokens)[0]\n    else:\n        state_dict = bart.model.state_dict()\n        remove_ignore_keys_(state_dict)\n        state_dict['shared.weight'] = state_dict['decoder.embed_tokens.weight']\n        fairseq_output = bart.extract_features(tokens)\n        if hf_checkpoint_name == 'facebook/bart-large':\n            model = BartModel(config).eval()\n            model.load_state_dict(state_dict)\n            new_model_outputs = model(tokens).model[0]\n        else:\n            model = BartForConditionalGeneration(config).eval()\n            model.model.load_state_dict(state_dict)\n            if hasattr(model, 'lm_head'):\n                model.lm_head = make_linear_from_emb(model.model.shared)\n            new_model_outputs = model.model(tokens)[0]\n    if fairseq_output.shape != new_model_outputs.shape:\n        raise ValueError(f'`fairseq_output` shape and `new_model_output` shape are different: fairseq_output.shape={fairseq_output.shape!r}, {new_model_outputs.shape}')\n    if (fairseq_output != new_model_outputs).any().item():\n        raise ValueError('Some values in `fairseq_output` are different from `new_model_outputs`')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)",
            "@torch.no_grad()\ndef convert_bart_checkpoint(checkpoint_path, pytorch_dump_folder_path, hf_checkpoint_name=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"\\n    Copy/paste/tweak model's weights to our BERT structure.\\n    \"\n    if not os.path.exists(checkpoint_path):\n        bart = torch.hub.load('pytorch/fairseq', checkpoint_path).eval()\n    else:\n        bart = load_xsum_checkpoint(checkpoint_path)\n    bart.model.upgrade_state_dict(bart.model.state_dict())\n    if hf_checkpoint_name is None:\n        hf_checkpoint_name = checkpoint_path.replace('.', '-')\n    config = BartConfig.from_pretrained(hf_checkpoint_name)\n    tokens = bart.encode(SAMPLE_TEXT).unsqueeze(0)\n    tokens2 = BartTokenizer.from_pretrained(hf_checkpoint_name).encode(SAMPLE_TEXT, return_tensors='pt').unsqueeze(0)\n    if not torch.eq(tokens, tokens2).all():\n        raise ValueError(f'converted tokenizer and pretrained tokenizer returned different output: {tokens} != {tokens2}')\n    if checkpoint_path == 'bart.large.mnli':\n        state_dict = bart.state_dict()\n        remove_ignore_keys_(state_dict)\n        state_dict['model.shared.weight'] = state_dict['model.decoder.embed_tokens.weight']\n        for (src, dest) in mnli_rename_keys:\n            rename_key(state_dict, src, dest)\n        model = BartForSequenceClassification(config).eval()\n        model.load_state_dict(state_dict)\n        fairseq_output = bart.predict('mnli', tokens, return_logits=True)\n        new_model_outputs = model(tokens)[0]\n    else:\n        state_dict = bart.model.state_dict()\n        remove_ignore_keys_(state_dict)\n        state_dict['shared.weight'] = state_dict['decoder.embed_tokens.weight']\n        fairseq_output = bart.extract_features(tokens)\n        if hf_checkpoint_name == 'facebook/bart-large':\n            model = BartModel(config).eval()\n            model.load_state_dict(state_dict)\n            new_model_outputs = model(tokens).model[0]\n        else:\n            model = BartForConditionalGeneration(config).eval()\n            model.model.load_state_dict(state_dict)\n            if hasattr(model, 'lm_head'):\n                model.lm_head = make_linear_from_emb(model.model.shared)\n            new_model_outputs = model.model(tokens)[0]\n    if fairseq_output.shape != new_model_outputs.shape:\n        raise ValueError(f'`fairseq_output` shape and `new_model_output` shape are different: fairseq_output.shape={fairseq_output.shape!r}, {new_model_outputs.shape}')\n    if (fairseq_output != new_model_outputs).any().item():\n        raise ValueError('Some values in `fairseq_output` are different from `new_model_outputs`')\n    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n    model.save_pretrained(pytorch_dump_folder_path)"
        ]
    }
]