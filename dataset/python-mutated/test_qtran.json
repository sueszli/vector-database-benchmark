[
    {
        "func_name": "test_qtran",
        "original": "@pytest.mark.unittest\ndef test_qtran():\n    (agent_num, bs, T) = (4, 3, 8)\n    (obs_dim, global_obs_dim, action_dim) = (32, 32 * 4, 9)\n    embedding_dim = 64\n    data = {'obs': {'agent_state': torch.randn(T, bs, agent_num, obs_dim), 'global_state': torch.randn(T, bs, global_obs_dim), 'action_mask': torch.randint(0, 2, size=(T, bs, agent_num, action_dim))}, 'prev_state': [[None for _ in range(agent_num)] for _ in range(bs)], 'action': torch.randint(0, action_dim, size=(T, bs, agent_num))}\n    model = QTran(agent_num, obs_dim, global_obs_dim, action_dim, [32, embedding_dim], embedding_dim)\n    output = model.forward(data, single_step=False)\n    assert set(output.keys()) == set(['next_state', 'agent_q_act', 'vs', 'logit', 'action_mask', 'total_q'])\n    assert output['total_q'].shape == (T, bs)\n    assert output['logit'].shape == (T, bs, agent_num, action_dim)\n    assert len(output['next_state']) == bs and all([len(n) == agent_num for n in output['next_state']])\n    print(output['next_state'][0][0]['h'].shape)\n    loss = output['total_q'].sum() + output['agent_q_act'].sum() + output['vs'].sum()\n    is_differentiable(loss, model)\n    data.pop('action')\n    outputs = model.forward(data, single_step=False)",
        "mutated": [
            "@pytest.mark.unittest\ndef test_qtran():\n    if False:\n        i = 10\n    (agent_num, bs, T) = (4, 3, 8)\n    (obs_dim, global_obs_dim, action_dim) = (32, 32 * 4, 9)\n    embedding_dim = 64\n    data = {'obs': {'agent_state': torch.randn(T, bs, agent_num, obs_dim), 'global_state': torch.randn(T, bs, global_obs_dim), 'action_mask': torch.randint(0, 2, size=(T, bs, agent_num, action_dim))}, 'prev_state': [[None for _ in range(agent_num)] for _ in range(bs)], 'action': torch.randint(0, action_dim, size=(T, bs, agent_num))}\n    model = QTran(agent_num, obs_dim, global_obs_dim, action_dim, [32, embedding_dim], embedding_dim)\n    output = model.forward(data, single_step=False)\n    assert set(output.keys()) == set(['next_state', 'agent_q_act', 'vs', 'logit', 'action_mask', 'total_q'])\n    assert output['total_q'].shape == (T, bs)\n    assert output['logit'].shape == (T, bs, agent_num, action_dim)\n    assert len(output['next_state']) == bs and all([len(n) == agent_num for n in output['next_state']])\n    print(output['next_state'][0][0]['h'].shape)\n    loss = output['total_q'].sum() + output['agent_q_act'].sum() + output['vs'].sum()\n    is_differentiable(loss, model)\n    data.pop('action')\n    outputs = model.forward(data, single_step=False)",
            "@pytest.mark.unittest\ndef test_qtran():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (agent_num, bs, T) = (4, 3, 8)\n    (obs_dim, global_obs_dim, action_dim) = (32, 32 * 4, 9)\n    embedding_dim = 64\n    data = {'obs': {'agent_state': torch.randn(T, bs, agent_num, obs_dim), 'global_state': torch.randn(T, bs, global_obs_dim), 'action_mask': torch.randint(0, 2, size=(T, bs, agent_num, action_dim))}, 'prev_state': [[None for _ in range(agent_num)] for _ in range(bs)], 'action': torch.randint(0, action_dim, size=(T, bs, agent_num))}\n    model = QTran(agent_num, obs_dim, global_obs_dim, action_dim, [32, embedding_dim], embedding_dim)\n    output = model.forward(data, single_step=False)\n    assert set(output.keys()) == set(['next_state', 'agent_q_act', 'vs', 'logit', 'action_mask', 'total_q'])\n    assert output['total_q'].shape == (T, bs)\n    assert output['logit'].shape == (T, bs, agent_num, action_dim)\n    assert len(output['next_state']) == bs and all([len(n) == agent_num for n in output['next_state']])\n    print(output['next_state'][0][0]['h'].shape)\n    loss = output['total_q'].sum() + output['agent_q_act'].sum() + output['vs'].sum()\n    is_differentiable(loss, model)\n    data.pop('action')\n    outputs = model.forward(data, single_step=False)",
            "@pytest.mark.unittest\ndef test_qtran():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (agent_num, bs, T) = (4, 3, 8)\n    (obs_dim, global_obs_dim, action_dim) = (32, 32 * 4, 9)\n    embedding_dim = 64\n    data = {'obs': {'agent_state': torch.randn(T, bs, agent_num, obs_dim), 'global_state': torch.randn(T, bs, global_obs_dim), 'action_mask': torch.randint(0, 2, size=(T, bs, agent_num, action_dim))}, 'prev_state': [[None for _ in range(agent_num)] for _ in range(bs)], 'action': torch.randint(0, action_dim, size=(T, bs, agent_num))}\n    model = QTran(agent_num, obs_dim, global_obs_dim, action_dim, [32, embedding_dim], embedding_dim)\n    output = model.forward(data, single_step=False)\n    assert set(output.keys()) == set(['next_state', 'agent_q_act', 'vs', 'logit', 'action_mask', 'total_q'])\n    assert output['total_q'].shape == (T, bs)\n    assert output['logit'].shape == (T, bs, agent_num, action_dim)\n    assert len(output['next_state']) == bs and all([len(n) == agent_num for n in output['next_state']])\n    print(output['next_state'][0][0]['h'].shape)\n    loss = output['total_q'].sum() + output['agent_q_act'].sum() + output['vs'].sum()\n    is_differentiable(loss, model)\n    data.pop('action')\n    outputs = model.forward(data, single_step=False)",
            "@pytest.mark.unittest\ndef test_qtran():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (agent_num, bs, T) = (4, 3, 8)\n    (obs_dim, global_obs_dim, action_dim) = (32, 32 * 4, 9)\n    embedding_dim = 64\n    data = {'obs': {'agent_state': torch.randn(T, bs, agent_num, obs_dim), 'global_state': torch.randn(T, bs, global_obs_dim), 'action_mask': torch.randint(0, 2, size=(T, bs, agent_num, action_dim))}, 'prev_state': [[None for _ in range(agent_num)] for _ in range(bs)], 'action': torch.randint(0, action_dim, size=(T, bs, agent_num))}\n    model = QTran(agent_num, obs_dim, global_obs_dim, action_dim, [32, embedding_dim], embedding_dim)\n    output = model.forward(data, single_step=False)\n    assert set(output.keys()) == set(['next_state', 'agent_q_act', 'vs', 'logit', 'action_mask', 'total_q'])\n    assert output['total_q'].shape == (T, bs)\n    assert output['logit'].shape == (T, bs, agent_num, action_dim)\n    assert len(output['next_state']) == bs and all([len(n) == agent_num for n in output['next_state']])\n    print(output['next_state'][0][0]['h'].shape)\n    loss = output['total_q'].sum() + output['agent_q_act'].sum() + output['vs'].sum()\n    is_differentiable(loss, model)\n    data.pop('action')\n    outputs = model.forward(data, single_step=False)",
            "@pytest.mark.unittest\ndef test_qtran():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (agent_num, bs, T) = (4, 3, 8)\n    (obs_dim, global_obs_dim, action_dim) = (32, 32 * 4, 9)\n    embedding_dim = 64\n    data = {'obs': {'agent_state': torch.randn(T, bs, agent_num, obs_dim), 'global_state': torch.randn(T, bs, global_obs_dim), 'action_mask': torch.randint(0, 2, size=(T, bs, agent_num, action_dim))}, 'prev_state': [[None for _ in range(agent_num)] for _ in range(bs)], 'action': torch.randint(0, action_dim, size=(T, bs, agent_num))}\n    model = QTran(agent_num, obs_dim, global_obs_dim, action_dim, [32, embedding_dim], embedding_dim)\n    output = model.forward(data, single_step=False)\n    assert set(output.keys()) == set(['next_state', 'agent_q_act', 'vs', 'logit', 'action_mask', 'total_q'])\n    assert output['total_q'].shape == (T, bs)\n    assert output['logit'].shape == (T, bs, agent_num, action_dim)\n    assert len(output['next_state']) == bs and all([len(n) == agent_num for n in output['next_state']])\n    print(output['next_state'][0][0]['h'].shape)\n    loss = output['total_q'].sum() + output['agent_q_act'].sum() + output['vs'].sum()\n    is_differentiable(loss, model)\n    data.pop('action')\n    outputs = model.forward(data, single_step=False)"
        ]
    }
]