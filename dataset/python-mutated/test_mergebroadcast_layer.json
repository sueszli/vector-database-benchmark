[
    {
        "func_name": "fshape",
        "original": "def fshape(rs, k):\n    return (rs, rs, k)",
        "mutated": [
            "def fshape(rs, k):\n    if False:\n        i = 10\n    return (rs, rs, k)",
            "def fshape(rs, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return (rs, rs, k)",
            "def fshape(rs, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return (rs, rs, k)",
            "def fshape(rs, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return (rs, rs, k)",
            "def fshape(rs, k):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return (rs, rs, k)"
        ]
    },
    {
        "func_name": "inception",
        "original": "def inception(kvals, name='i'):\n    (p1, p2, p3) = kvals\n    branch1 = [Sequential([Conv(fshape(1, p1[0]), **common)])] if p1[0] else []\n    branch2 = [Sequential([Conv(fshape(1, p2[0]), **common), Conv(fshape(3, p2[1]), **commonp1)])]\n    branch3 = [Sequential([Pooling(op=p3[0], **pool3s1p1)] + ([Conv(fshape(1, p3[1]), **common)] if p3[1] else []))]\n    partitions = branch1 + branch2 + branch3\n    return [MergeBroadcast(layers=partitions, merge='depth')]",
        "mutated": [
            "def inception(kvals, name='i'):\n    if False:\n        i = 10\n    (p1, p2, p3) = kvals\n    branch1 = [Sequential([Conv(fshape(1, p1[0]), **common)])] if p1[0] else []\n    branch2 = [Sequential([Conv(fshape(1, p2[0]), **common), Conv(fshape(3, p2[1]), **commonp1)])]\n    branch3 = [Sequential([Pooling(op=p3[0], **pool3s1p1)] + ([Conv(fshape(1, p3[1]), **common)] if p3[1] else []))]\n    partitions = branch1 + branch2 + branch3\n    return [MergeBroadcast(layers=partitions, merge='depth')]",
            "def inception(kvals, name='i'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (p1, p2, p3) = kvals\n    branch1 = [Sequential([Conv(fshape(1, p1[0]), **common)])] if p1[0] else []\n    branch2 = [Sequential([Conv(fshape(1, p2[0]), **common), Conv(fshape(3, p2[1]), **commonp1)])]\n    branch3 = [Sequential([Pooling(op=p3[0], **pool3s1p1)] + ([Conv(fshape(1, p3[1]), **common)] if p3[1] else []))]\n    partitions = branch1 + branch2 + branch3\n    return [MergeBroadcast(layers=partitions, merge='depth')]",
            "def inception(kvals, name='i'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (p1, p2, p3) = kvals\n    branch1 = [Sequential([Conv(fshape(1, p1[0]), **common)])] if p1[0] else []\n    branch2 = [Sequential([Conv(fshape(1, p2[0]), **common), Conv(fshape(3, p2[1]), **commonp1)])]\n    branch3 = [Sequential([Pooling(op=p3[0], **pool3s1p1)] + ([Conv(fshape(1, p3[1]), **common)] if p3[1] else []))]\n    partitions = branch1 + branch2 + branch3\n    return [MergeBroadcast(layers=partitions, merge='depth')]",
            "def inception(kvals, name='i'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (p1, p2, p3) = kvals\n    branch1 = [Sequential([Conv(fshape(1, p1[0]), **common)])] if p1[0] else []\n    branch2 = [Sequential([Conv(fshape(1, p2[0]), **common), Conv(fshape(3, p2[1]), **commonp1)])]\n    branch3 = [Sequential([Pooling(op=p3[0], **pool3s1p1)] + ([Conv(fshape(1, p3[1]), **common)] if p3[1] else []))]\n    partitions = branch1 + branch2 + branch3\n    return [MergeBroadcast(layers=partitions, merge='depth')]",
            "def inception(kvals, name='i'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (p1, p2, p3) = kvals\n    branch1 = [Sequential([Conv(fshape(1, p1[0]), **common)])] if p1[0] else []\n    branch2 = [Sequential([Conv(fshape(1, p2[0]), **common), Conv(fshape(3, p2[1]), **commonp1)])]\n    branch3 = [Sequential([Pooling(op=p3[0], **pool3s1p1)] + ([Conv(fshape(1, p3[1]), **common)] if p3[1] else []))]\n    partitions = branch1 + branch2 + branch3\n    return [MergeBroadcast(layers=partitions, merge='depth')]"
        ]
    },
    {
        "func_name": "inception_bare",
        "original": "def inception_bare(ref_module, kvals, name='i'):\n    (p1, p2, p3) = kvals\n    branch1 = [Conv(fshape(1, p1[0]), **common)] if p1[0] else []\n    branch2 = [Conv(fshape(1, p2[0]), **common), Conv(fshape(3, p2[1]), **commonp1)]\n    branch3 = [Pooling(op=p3[0], **pool3s1p1)] + ([Conv(fshape(1, p3[1]), **common)] if p3[1] else [])\n    branch1 = Sequential(branch1)\n    branch2 = Sequential(branch2)\n    branch3 = Sequential(branch3)\n    (branch1_ref, branch2_ref, branch3_ref) = ref_module[0].layers\n    if p1[0]:\n        for (ll, lr) in zip(branch1.layers, branch1_ref.layers):\n            if ll.has_params:\n                ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    for (ll, lr) in zip(branch2.layers, branch2_ref.layers):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    if p3[1]:\n        for (ll, lr) in zip(branch3.layers, branch3_ref.layers):\n            if ll.has_params:\n                ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    return (branch1.layers, branch2.layers, branch3.layers)",
        "mutated": [
            "def inception_bare(ref_module, kvals, name='i'):\n    if False:\n        i = 10\n    (p1, p2, p3) = kvals\n    branch1 = [Conv(fshape(1, p1[0]), **common)] if p1[0] else []\n    branch2 = [Conv(fshape(1, p2[0]), **common), Conv(fshape(3, p2[1]), **commonp1)]\n    branch3 = [Pooling(op=p3[0], **pool3s1p1)] + ([Conv(fshape(1, p3[1]), **common)] if p3[1] else [])\n    branch1 = Sequential(branch1)\n    branch2 = Sequential(branch2)\n    branch3 = Sequential(branch3)\n    (branch1_ref, branch2_ref, branch3_ref) = ref_module[0].layers\n    if p1[0]:\n        for (ll, lr) in zip(branch1.layers, branch1_ref.layers):\n            if ll.has_params:\n                ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    for (ll, lr) in zip(branch2.layers, branch2_ref.layers):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    if p3[1]:\n        for (ll, lr) in zip(branch3.layers, branch3_ref.layers):\n            if ll.has_params:\n                ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    return (branch1.layers, branch2.layers, branch3.layers)",
            "def inception_bare(ref_module, kvals, name='i'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    (p1, p2, p3) = kvals\n    branch1 = [Conv(fshape(1, p1[0]), **common)] if p1[0] else []\n    branch2 = [Conv(fshape(1, p2[0]), **common), Conv(fshape(3, p2[1]), **commonp1)]\n    branch3 = [Pooling(op=p3[0], **pool3s1p1)] + ([Conv(fshape(1, p3[1]), **common)] if p3[1] else [])\n    branch1 = Sequential(branch1)\n    branch2 = Sequential(branch2)\n    branch3 = Sequential(branch3)\n    (branch1_ref, branch2_ref, branch3_ref) = ref_module[0].layers\n    if p1[0]:\n        for (ll, lr) in zip(branch1.layers, branch1_ref.layers):\n            if ll.has_params:\n                ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    for (ll, lr) in zip(branch2.layers, branch2_ref.layers):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    if p3[1]:\n        for (ll, lr) in zip(branch3.layers, branch3_ref.layers):\n            if ll.has_params:\n                ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    return (branch1.layers, branch2.layers, branch3.layers)",
            "def inception_bare(ref_module, kvals, name='i'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    (p1, p2, p3) = kvals\n    branch1 = [Conv(fshape(1, p1[0]), **common)] if p1[0] else []\n    branch2 = [Conv(fshape(1, p2[0]), **common), Conv(fshape(3, p2[1]), **commonp1)]\n    branch3 = [Pooling(op=p3[0], **pool3s1p1)] + ([Conv(fshape(1, p3[1]), **common)] if p3[1] else [])\n    branch1 = Sequential(branch1)\n    branch2 = Sequential(branch2)\n    branch3 = Sequential(branch3)\n    (branch1_ref, branch2_ref, branch3_ref) = ref_module[0].layers\n    if p1[0]:\n        for (ll, lr) in zip(branch1.layers, branch1_ref.layers):\n            if ll.has_params:\n                ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    for (ll, lr) in zip(branch2.layers, branch2_ref.layers):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    if p3[1]:\n        for (ll, lr) in zip(branch3.layers, branch3_ref.layers):\n            if ll.has_params:\n                ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    return (branch1.layers, branch2.layers, branch3.layers)",
            "def inception_bare(ref_module, kvals, name='i'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    (p1, p2, p3) = kvals\n    branch1 = [Conv(fshape(1, p1[0]), **common)] if p1[0] else []\n    branch2 = [Conv(fshape(1, p2[0]), **common), Conv(fshape(3, p2[1]), **commonp1)]\n    branch3 = [Pooling(op=p3[0], **pool3s1p1)] + ([Conv(fshape(1, p3[1]), **common)] if p3[1] else [])\n    branch1 = Sequential(branch1)\n    branch2 = Sequential(branch2)\n    branch3 = Sequential(branch3)\n    (branch1_ref, branch2_ref, branch3_ref) = ref_module[0].layers\n    if p1[0]:\n        for (ll, lr) in zip(branch1.layers, branch1_ref.layers):\n            if ll.has_params:\n                ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    for (ll, lr) in zip(branch2.layers, branch2_ref.layers):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    if p3[1]:\n        for (ll, lr) in zip(branch3.layers, branch3_ref.layers):\n            if ll.has_params:\n                ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    return (branch1.layers, branch2.layers, branch3.layers)",
            "def inception_bare(ref_module, kvals, name='i'):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    (p1, p2, p3) = kvals\n    branch1 = [Conv(fshape(1, p1[0]), **common)] if p1[0] else []\n    branch2 = [Conv(fshape(1, p2[0]), **common), Conv(fshape(3, p2[1]), **commonp1)]\n    branch3 = [Pooling(op=p3[0], **pool3s1p1)] + ([Conv(fshape(1, p3[1]), **common)] if p3[1] else [])\n    branch1 = Sequential(branch1)\n    branch2 = Sequential(branch2)\n    branch3 = Sequential(branch3)\n    (branch1_ref, branch2_ref, branch3_ref) = ref_module[0].layers\n    if p1[0]:\n        for (ll, lr) in zip(branch1.layers, branch1_ref.layers):\n            if ll.has_params:\n                ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    for (ll, lr) in zip(branch2.layers, branch2_ref.layers):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    if p3[1]:\n        for (ll, lr) in zip(branch3.layers, branch3_ref.layers):\n            if ll.has_params:\n                ll.set_params({'params': {'W': lr.W.get(), 'weight_bias': lr.weight_bias.get()}})\n    return (branch1.layers, branch2.layers, branch3.layers)"
        ]
    },
    {
        "func_name": "main_branch",
        "original": "def main_branch():\n    return [Conv(fshape(7, 64), **commonp3s2), Pooling(fshape=3, strides=2, padding=1, op='max'), Conv(fshape(3, 192), **commonp1), Pooling(fshape=3, strides=2, padding=1, op='max')]",
        "mutated": [
            "def main_branch():\n    if False:\n        i = 10\n    return [Conv(fshape(7, 64), **commonp3s2), Pooling(fshape=3, strides=2, padding=1, op='max'), Conv(fshape(3, 192), **commonp1), Pooling(fshape=3, strides=2, padding=1, op='max')]",
            "def main_branch():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return [Conv(fshape(7, 64), **commonp3s2), Pooling(fshape=3, strides=2, padding=1, op='max'), Conv(fshape(3, 192), **commonp1), Pooling(fshape=3, strides=2, padding=1, op='max')]",
            "def main_branch():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return [Conv(fshape(7, 64), **commonp3s2), Pooling(fshape=3, strides=2, padding=1, op='max'), Conv(fshape(3, 192), **commonp1), Pooling(fshape=3, strides=2, padding=1, op='max')]",
            "def main_branch():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return [Conv(fshape(7, 64), **commonp3s2), Pooling(fshape=3, strides=2, padding=1, op='max'), Conv(fshape(3, 192), **commonp1), Pooling(fshape=3, strides=2, padding=1, op='max')]",
            "def main_branch():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return [Conv(fshape(7, 64), **commonp3s2), Pooling(fshape=3, strides=2, padding=1, op='max'), Conv(fshape(3, 192), **commonp1), Pooling(fshape=3, strides=2, padding=1, op='max')]"
        ]
    },
    {
        "func_name": "top_branch",
        "original": "def top_branch():\n    return [Pooling(fshape=7, strides=1, op='avg'), Affine(nout=100, init=init1, activation=Softmax(), bias=bias)]",
        "mutated": [
            "def top_branch():\n    if False:\n        i = 10\n    return [Pooling(fshape=7, strides=1, op='avg'), Affine(nout=100, init=init1, activation=Softmax(), bias=bias)]",
            "def top_branch():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return [Pooling(fshape=7, strides=1, op='avg'), Affine(nout=100, init=init1, activation=Softmax(), bias=bias)]",
            "def top_branch():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return [Pooling(fshape=7, strides=1, op='avg'), Affine(nout=100, init=init1, activation=Softmax(), bias=bias)]",
            "def top_branch():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return [Pooling(fshape=7, strides=1, op='avg'), Affine(nout=100, init=init1, activation=Softmax(), bias=bias)]",
            "def top_branch():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return [Pooling(fshape=7, strides=1, op='avg'), Affine(nout=100, init=init1, activation=Softmax(), bias=bias)]"
        ]
    },
    {
        "func_name": "test_branch_model",
        "original": "@pytest.mark.hasgpu\ndef test_branch_model(backend_gpu):\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 64\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:6]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get(), 'weight_bias': lo.weight_bias.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[6].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[6:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[6].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
        "mutated": [
            "@pytest.mark.hasgpu\ndef test_branch_model(backend_gpu):\n    if False:\n        i = 10\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 64\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:6]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get(), 'weight_bias': lo.weight_bias.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[6].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[6:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[6].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
            "@pytest.mark.hasgpu\ndef test_branch_model(backend_gpu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 64\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:6]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get(), 'weight_bias': lo.weight_bias.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[6].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[6:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[6].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
            "@pytest.mark.hasgpu\ndef test_branch_model(backend_gpu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 64\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:6]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get(), 'weight_bias': lo.weight_bias.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[6].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[6:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[6].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
            "@pytest.mark.hasgpu\ndef test_branch_model(backend_gpu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 64\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:6]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get(), 'weight_bias': lo.weight_bias.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[6].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[6:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[6].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
            "@pytest.mark.hasgpu\ndef test_branch_model(backend_gpu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 64\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:6]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get(), 'weight_bias': lo.weight_bias.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[6].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[6:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[6].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)"
        ]
    },
    {
        "func_name": "test_branch_model_fork",
        "original": "@pytest.mark.hasgpu\ndef test_branch_model_fork(backend_gpu):\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    if be.gpu_memory_size < 6.1 * 1024 * 1024 * 1024:\n        pytest.skip(msg='Test requires more than 6.1GB')\n    be.bsz = 64\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:6]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get(), 'weight_bias': lo.weight_bias.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[7].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
        "mutated": [
            "@pytest.mark.hasgpu\ndef test_branch_model_fork(backend_gpu):\n    if False:\n        i = 10\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    if be.gpu_memory_size < 6.1 * 1024 * 1024 * 1024:\n        pytest.skip(msg='Test requires more than 6.1GB')\n    be.bsz = 64\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:6]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get(), 'weight_bias': lo.weight_bias.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[7].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
            "@pytest.mark.hasgpu\ndef test_branch_model_fork(backend_gpu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    if be.gpu_memory_size < 6.1 * 1024 * 1024 * 1024:\n        pytest.skip(msg='Test requires more than 6.1GB')\n    be.bsz = 64\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:6]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get(), 'weight_bias': lo.weight_bias.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[7].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
            "@pytest.mark.hasgpu\ndef test_branch_model_fork(backend_gpu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    if be.gpu_memory_size < 6.1 * 1024 * 1024 * 1024:\n        pytest.skip(msg='Test requires more than 6.1GB')\n    be.bsz = 64\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:6]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get(), 'weight_bias': lo.weight_bias.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[7].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
            "@pytest.mark.hasgpu\ndef test_branch_model_fork(backend_gpu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    if be.gpu_memory_size < 6.1 * 1024 * 1024 * 1024:\n        pytest.skip(msg='Test requires more than 6.1GB')\n    be.bsz = 64\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:6]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get(), 'weight_bias': lo.weight_bias.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[7].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
            "@pytest.mark.hasgpu\ndef test_branch_model_fork(backend_gpu):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    if be.gpu_memory_size < 6.1 * 1024 * 1024 * 1024:\n        pytest.skip(msg='Test requires more than 6.1GB')\n    be.bsz = 64\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:6]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get(), 'weight_bias': lo.weight_bias.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[7].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)"
        ]
    },
    {
        "func_name": "test_branch_model_mkl",
        "original": "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_mkl(backend_mkl):\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[8].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[8:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[8].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
        "mutated": [
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_mkl(backend_mkl):\n    if False:\n        i = 10\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[8].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[8:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[8].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_mkl(backend_mkl):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[8].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[8:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[8].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_mkl(backend_mkl):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[8].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[8:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[8].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_mkl(backend_mkl):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[8].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[8:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[8].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_mkl(backend_mkl):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[8].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[8:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[8].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)"
        ]
    },
    {
        "func_name": "test_branch_model_fork_mkl",
        "original": "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_fork_mkl(backend_mkl):\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[9].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
        "mutated": [
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_fork_mkl(backend_mkl):\n    if False:\n        i = 10\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[9].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_fork_mkl(backend_mkl):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[9].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_fork_mkl(backend_mkl):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[9].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_fork_mkl(backend_mkl):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[9].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_fork_mkl(backend_mkl):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[9].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)"
        ]
    },
    {
        "func_name": "test_branch_model_cpu",
        "original": "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_cpu(backend_cpu64):\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[8].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[8:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[8].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
        "mutated": [
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_cpu(backend_cpu64):\n    if False:\n        i = 10\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[8].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[8:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[8].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_cpu(backend_cpu64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[8].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[8:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[8].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_cpu(backend_cpu64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[8].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[8:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[8].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_cpu(backend_cpu64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[8].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[8:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[8].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_cpu(backend_cpu64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    main1 = main_branch()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top = top_branch()\n    neon_layer = Sequential(main1 + i1 + top)\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_logger.display(neon_layer.nested_str())\n    neon_layer.layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out = neon_layer.fprop(inp).get()\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_buff = DeltasTree()\n        ll.allocate_deltas(temp_buff)\n        temp_buff.allocate_buffers()\n        ll.set_deltas(temp_buff)\n    for bb in (b1, b2, b3):\n        for ll in bb:\n            ll.allocate()\n            temp_buff = DeltasTree()\n            ll.allocate_deltas(temp_buff)\n            temp_buff.allocate_buffers()\n            ll.set_deltas(temp_buff)\n    merge_output = be.empty_like(neon_layer.layers[8].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = x\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out, neon_out_ref, rtol=0)\n    neon_logger.display('Beginning Back prop')\n    erra = np.random.random(neon_out.shape)\n    err = be.array(erra)\n    for ll in reversed(neon_layer.layers[8:]):\n        err = ll.bprop(err)\n    neon_deltas = err.get()\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[8].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(neon_deltas, neon_ref_deltas, rtol=0)"
        ]
    },
    {
        "func_name": "test_branch_model_fork_cpu",
        "original": "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_fork_cpu(backend_cpu64):\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[9].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
        "mutated": [
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_fork_cpu(backend_cpu64):\n    if False:\n        i = 10\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[9].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_fork_cpu(backend_cpu64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[9].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_fork_cpu(backend_cpu64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[9].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_fork_cpu(backend_cpu64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[9].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)",
            "@pytest.mark.unsupported\n@pytest.mark.skip(reason='Not supported for CPU')\ndef test_branch_model_fork_cpu(backend_cpu64):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    from neon.layers import BranchNode, Tree\n    np.random.seed(0)\n    be = NervanaObject.be\n    be.bsz = 32\n    bnode = BranchNode()\n    i1 = inception([(32,), (32, 32), ('max', 16)])\n    top1 = top_branch()\n    top2 = top_branch()\n    p1 = Sequential(main_branch() + [bnode, i1] + top1)\n    p2 = [bnode] + top2\n    alpha2 = 0.3\n    neon_layer = Tree([p1, p2], alphas=[1.0, alpha2])\n    inshape = (4, 224, 224)\n    insize = np.prod(inshape)\n    inpa = np.random.random((insize, batch_size))\n    neon_layer.configure(inshape)\n    inp = neon_layer.be.array(inpa)\n    neon_layer.allocate()\n    neon_layer.layers[0].layers[0].prev_layer = True\n    neon_layer.allocate_deltas()\n    neon_out_dev = neon_layer.fprop(inp)\n    neon_out = [d.get() for d in neon_out_dev]\n    main_trunk2 = Sequential(main_branch())\n    main_trunk2.configure(inshape)\n    main2 = main_trunk2.layers\n    main2[0].prev_layer = True\n    main2[0].deltas = be.iobuf(inshape)\n    branch2 = Sequential(top_branch())\n    lbranch2 = branch2.layers\n    (b1, b2, b3) = inception_bare(i1, [(32,), (32, 32), ('max', 16)])\n    for bb in (b1, b2, b3, lbranch2):\n        oshape = inshape\n        for ll in main2 + bb:\n            oshape = ll.configure(oshape)\n    main1_trunk = neon_layer.layers[0].layers[:8]\n    for (ll, lo) in zip(main2, main1_trunk):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n        ll.allocate()\n        temp_deltas = DeltasTree()\n        temp_deltas.proc_layer(ll)\n        temp_deltas.allocate_buffers()\n        ll.set_deltas(temp_deltas)\n    for (ll, lo) in zip(lbranch2, neon_layer.layers[1].layers[1:]):\n        if ll.has_params:\n            ll.set_params({'params': {'W': lo.W.get()}})\n    for bb in (b1, b2, b3, lbranch2):\n        for ll in bb:\n            ll.allocate()\n            temp_deltas = DeltasTree()\n            temp_deltas.proc_layer(ll)\n            temp_deltas.allocate_buffers()\n            ll.set_deltas(temp_deltas)\n    merge_output = be.empty_like(neon_layer.layers[0].layers[9].outputs)\n    x = inp\n    for ll in main2:\n        x = ll.fprop(x)\n    main2_out = x\n    start = 0\n    for bb in (b1, b2, b3):\n        xb = main2_out\n        for ll in bb:\n            xb = ll.fprop(xb)\n        end = start + xb.shape[0]\n        merge_output[start:end] = xb\n        start = end\n    x = merge_output\n    top_trunk = Sequential(top1).layers\n    for ll in top_trunk:\n        x = ll.fprop(x)\n    neon_out_ref = x.get()\n    assert allclose_with_out(neon_out_ref, neon_out[0], rtol=0)\n    neon_out_ref2 = branch2.fprop(main2_out).get()\n    assert allclose_with_out(neon_out_ref2, neon_out[1])\n    neon_logger.display('Beginning Back prop')\n    erra = [np.random.random(d.shape) for d in neon_out]\n    err = [be.array(d) for d in erra]\n    neon_layer.layers[0].layers[0].deltas = be.iobuf(inshape)\n    neon_layer.bprop(err)\n    bottom_neon_deltas = neon_layer.layers[0].layers[1].deltas.get()\n    middle_neon_deltas = neon_layer.layers[1].layers[1].deltas.get()\n    err0 = err[0]\n    for ll in reversed(top_trunk):\n        err0 = ll.bprop(err0)\n    err1 = err[1]\n    for ll in reversed(lbranch2):\n        err1 = ll.bprop(err1)\n    for (bb, errb) in zip((b1, b2, b3), neon_layer.layers[0].layers[-5].error_views):\n        for ll in reversed(bb):\n            errb = ll.bprop(errb)\n    ref_deltas = be.zeros_like(b1[0].deltas)\n    ref_deltas[:] = alpha2 * lbranch2[0].deltas\n    ref_deltas[:] = ref_deltas + b3[0].deltas + b2[0].deltas + b1[0].deltas\n    neon_ref_deltas = ref_deltas.get()\n    assert allclose_with_out(middle_neon_deltas, neon_ref_deltas, rtol=0)\n    x = ref_deltas\n    main2[0].deltas = be.iobuf(inshape)\n    for ll in reversed(main2):\n        x = ll.bprop(x)\n    bottom_neon_ref_deltas = main2[1].deltas.get()\n    assert allclose_with_out(bottom_neon_deltas, bottom_neon_ref_deltas, rtol=0)"
        ]
    }
]