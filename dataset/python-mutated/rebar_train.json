[
    {
        "func_name": "manual_scalar_summary",
        "original": "def manual_scalar_summary(name, value):\n    value = tf.Summary.Value(tag=name, simple_value=value)\n    summary_str = tf.Summary(value=[value])\n    return summary_str",
        "mutated": [
            "def manual_scalar_summary(name, value):\n    if False:\n        i = 10\n    value = tf.Summary.Value(tag=name, simple_value=value)\n    summary_str = tf.Summary(value=[value])\n    return summary_str",
            "def manual_scalar_summary(name, value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    value = tf.Summary.Value(tag=name, simple_value=value)\n    summary_str = tf.Summary(value=[value])\n    return summary_str",
            "def manual_scalar_summary(name, value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    value = tf.Summary.Value(tag=name, simple_value=value)\n    summary_str = tf.Summary(value=[value])\n    return summary_str",
            "def manual_scalar_summary(name, value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    value = tf.Summary.Value(tag=name, simple_value=value)\n    summary_str = tf.Summary(value=[value])\n    return summary_str",
            "def manual_scalar_summary(name, value):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    value = tf.Summary.Value(tag=name, simple_value=value)\n    summary_str = tf.Summary(value=[value])\n    return summary_str"
        ]
    },
    {
        "func_name": "eval",
        "original": "def eval(sbn, eval_xs, n_samples=100, batch_size=5):\n    n = eval_xs.shape[0]\n    i = 0\n    res = []\n    while i < n:\n        batch_xs = eval_xs[i:min(i + batch_size, n)]\n        res.append(sbn.partial_eval(batch_xs, n_samples))\n        i += batch_size\n    res = np.mean(res, axis=0)\n    return res",
        "mutated": [
            "def eval(sbn, eval_xs, n_samples=100, batch_size=5):\n    if False:\n        i = 10\n    n = eval_xs.shape[0]\n    i = 0\n    res = []\n    while i < n:\n        batch_xs = eval_xs[i:min(i + batch_size, n)]\n        res.append(sbn.partial_eval(batch_xs, n_samples))\n        i += batch_size\n    res = np.mean(res, axis=0)\n    return res",
            "def eval(sbn, eval_xs, n_samples=100, batch_size=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    n = eval_xs.shape[0]\n    i = 0\n    res = []\n    while i < n:\n        batch_xs = eval_xs[i:min(i + batch_size, n)]\n        res.append(sbn.partial_eval(batch_xs, n_samples))\n        i += batch_size\n    res = np.mean(res, axis=0)\n    return res",
            "def eval(sbn, eval_xs, n_samples=100, batch_size=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    n = eval_xs.shape[0]\n    i = 0\n    res = []\n    while i < n:\n        batch_xs = eval_xs[i:min(i + batch_size, n)]\n        res.append(sbn.partial_eval(batch_xs, n_samples))\n        i += batch_size\n    res = np.mean(res, axis=0)\n    return res",
            "def eval(sbn, eval_xs, n_samples=100, batch_size=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    n = eval_xs.shape[0]\n    i = 0\n    res = []\n    while i < n:\n        batch_xs = eval_xs[i:min(i + batch_size, n)]\n        res.append(sbn.partial_eval(batch_xs, n_samples))\n        i += batch_size\n    res = np.mean(res, axis=0)\n    return res",
            "def eval(sbn, eval_xs, n_samples=100, batch_size=5):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    n = eval_xs.shape[0]\n    i = 0\n    res = []\n    while i < n:\n        batch_xs = eval_xs[i:min(i + batch_size, n)]\n        res.append(sbn.partial_eval(batch_xs, n_samples))\n        i += batch_size\n    res = np.mean(res, axis=0)\n    return res"
        ]
    },
    {
        "func_name": "train",
        "original": "def train(sbn, train_xs, valid_xs, test_xs, training_steps, debug=False):\n    hparams = sorted(sbn.hparams.values().items())\n    hparams = (map(str, x) for x in hparams)\n    hparams = ('_'.join(x) for x in hparams)\n    hparams_str = '.'.join(hparams)\n    logger = L.Logger()\n    experiment_name = [str(sbn.hparams.n_hidden) for i in xrange(sbn.hparams.n_layer)] + [str(sbn.hparams.n_input)]\n    if sbn.hparams.nonlinear:\n        experiment_name = '~'.join(experiment_name)\n    else:\n        experiment_name = '-'.join(experiment_name)\n    experiment_name = 'SBN_%s' % experiment_name\n    rowkey = {'experiment': experiment_name, 'model': hparams_str}\n    summ_dir = os.path.join(FLAGS.working_dir, hparams_str)\n    summary_writer = tf.summary.FileWriter(summ_dir, flush_secs=15, max_queue=100)\n    sv = tf.train.Supervisor(logdir=os.path.join(FLAGS.working_dir, hparams_str), save_summaries_secs=0, save_model_secs=1200, summary_op=None, recovery_wait_secs=30, global_step=sbn.global_step)\n    with sv.managed_session() as sess:\n        with gfile.Open(os.path.join(FLAGS.working_dir, hparams_str, 'hparams.json'), 'w') as out:\n            json.dump(sbn.hparams.values(), out)\n        sbn.initialize(sess)\n        batch_size = sbn.hparams.batch_size\n        scores = []\n        n = train_xs.shape[0]\n        index = range(n)\n        while not sv.should_stop():\n            lHats = []\n            grad_variances = []\n            temperatures = []\n            random.shuffle(index)\n            i = 0\n            while i < n:\n                batch_index = index[i:min(i + batch_size, n)]\n                batch_xs = train_xs[batch_index, :]\n                if sbn.hparams.dynamic_b:\n                    batch_xs = (np.random.rand(*batch_xs.shape) < batch_xs).astype(float)\n                (lHat, grad_variance, step, temperature) = sbn.partial_fit(batch_xs, sbn.hparams.n_samples)\n                if debug:\n                    print(i, lHat)\n                    if i > 100:\n                        return\n                lHats.append(lHat)\n                grad_variances.append(grad_variance)\n                temperatures.append(temperature)\n                i += batch_size\n            grad_variances = np.log(np.mean(grad_variances, axis=0)).tolist()\n            summary_strings = []\n            if isinstance(grad_variances, list):\n                grad_variances = dict(zip([k for (k, v) in sbn.losses], map(float, grad_variances)))\n                rowkey['step'] = step\n                logger.log(rowkey, {'step': step, 'train': np.mean(lHats, axis=0)[0], 'grad_variances': grad_variances, 'temperature': np.mean(temperatures)})\n                grad_variances = '\\n'.join(map(str, sorted(grad_variances.iteritems())))\n            else:\n                rowkey['step'] = step\n                logger.log(rowkey, {'step': step, 'train': np.mean(lHats, axis=0)[0], 'grad_variance': grad_variances, 'temperature': np.mean(temperatures)})\n                summary_strings.append(manual_scalar_summary('log grad variance', grad_variances))\n            print('Step %d: %s\\n%s' % (step, str(np.mean(lHats, axis=0)), str(grad_variances)))\n            epoch = int(step / (train_xs.shape[0] / sbn.hparams.batch_size))\n            if epoch % FLAGS.eval_freq == 0:\n                valid_res = eval(sbn, valid_xs)\n                test_res = eval(sbn, test_xs)\n                print('\\nValid %d: %s' % (step, str(valid_res)))\n                print('Test %d: %s\\n' % (step, str(test_res)))\n                logger.log(rowkey, {'step': step, 'valid': valid_res[0], 'test': test_res[0]})\n                logger.flush()\n            summary_strings.extend([manual_scalar_summary('Train ELBO', np.mean(lHats, axis=0)[0]), manual_scalar_summary('Temperature', np.mean(temperatures))])\n            for summ_str in summary_strings:\n                summary_writer.add_summary(summ_str, global_step=step)\n            summary_writer.flush()\n            sys.stdout.flush()\n            scores.append(np.mean(lHats, axis=0))\n            if step > training_steps:\n                break\n        return scores",
        "mutated": [
            "def train(sbn, train_xs, valid_xs, test_xs, training_steps, debug=False):\n    if False:\n        i = 10\n    hparams = sorted(sbn.hparams.values().items())\n    hparams = (map(str, x) for x in hparams)\n    hparams = ('_'.join(x) for x in hparams)\n    hparams_str = '.'.join(hparams)\n    logger = L.Logger()\n    experiment_name = [str(sbn.hparams.n_hidden) for i in xrange(sbn.hparams.n_layer)] + [str(sbn.hparams.n_input)]\n    if sbn.hparams.nonlinear:\n        experiment_name = '~'.join(experiment_name)\n    else:\n        experiment_name = '-'.join(experiment_name)\n    experiment_name = 'SBN_%s' % experiment_name\n    rowkey = {'experiment': experiment_name, 'model': hparams_str}\n    summ_dir = os.path.join(FLAGS.working_dir, hparams_str)\n    summary_writer = tf.summary.FileWriter(summ_dir, flush_secs=15, max_queue=100)\n    sv = tf.train.Supervisor(logdir=os.path.join(FLAGS.working_dir, hparams_str), save_summaries_secs=0, save_model_secs=1200, summary_op=None, recovery_wait_secs=30, global_step=sbn.global_step)\n    with sv.managed_session() as sess:\n        with gfile.Open(os.path.join(FLAGS.working_dir, hparams_str, 'hparams.json'), 'w') as out:\n            json.dump(sbn.hparams.values(), out)\n        sbn.initialize(sess)\n        batch_size = sbn.hparams.batch_size\n        scores = []\n        n = train_xs.shape[0]\n        index = range(n)\n        while not sv.should_stop():\n            lHats = []\n            grad_variances = []\n            temperatures = []\n            random.shuffle(index)\n            i = 0\n            while i < n:\n                batch_index = index[i:min(i + batch_size, n)]\n                batch_xs = train_xs[batch_index, :]\n                if sbn.hparams.dynamic_b:\n                    batch_xs = (np.random.rand(*batch_xs.shape) < batch_xs).astype(float)\n                (lHat, grad_variance, step, temperature) = sbn.partial_fit(batch_xs, sbn.hparams.n_samples)\n                if debug:\n                    print(i, lHat)\n                    if i > 100:\n                        return\n                lHats.append(lHat)\n                grad_variances.append(grad_variance)\n                temperatures.append(temperature)\n                i += batch_size\n            grad_variances = np.log(np.mean(grad_variances, axis=0)).tolist()\n            summary_strings = []\n            if isinstance(grad_variances, list):\n                grad_variances = dict(zip([k for (k, v) in sbn.losses], map(float, grad_variances)))\n                rowkey['step'] = step\n                logger.log(rowkey, {'step': step, 'train': np.mean(lHats, axis=0)[0], 'grad_variances': grad_variances, 'temperature': np.mean(temperatures)})\n                grad_variances = '\\n'.join(map(str, sorted(grad_variances.iteritems())))\n            else:\n                rowkey['step'] = step\n                logger.log(rowkey, {'step': step, 'train': np.mean(lHats, axis=0)[0], 'grad_variance': grad_variances, 'temperature': np.mean(temperatures)})\n                summary_strings.append(manual_scalar_summary('log grad variance', grad_variances))\n            print('Step %d: %s\\n%s' % (step, str(np.mean(lHats, axis=0)), str(grad_variances)))\n            epoch = int(step / (train_xs.shape[0] / sbn.hparams.batch_size))\n            if epoch % FLAGS.eval_freq == 0:\n                valid_res = eval(sbn, valid_xs)\n                test_res = eval(sbn, test_xs)\n                print('\\nValid %d: %s' % (step, str(valid_res)))\n                print('Test %d: %s\\n' % (step, str(test_res)))\n                logger.log(rowkey, {'step': step, 'valid': valid_res[0], 'test': test_res[0]})\n                logger.flush()\n            summary_strings.extend([manual_scalar_summary('Train ELBO', np.mean(lHats, axis=0)[0]), manual_scalar_summary('Temperature', np.mean(temperatures))])\n            for summ_str in summary_strings:\n                summary_writer.add_summary(summ_str, global_step=step)\n            summary_writer.flush()\n            sys.stdout.flush()\n            scores.append(np.mean(lHats, axis=0))\n            if step > training_steps:\n                break\n        return scores",
            "def train(sbn, train_xs, valid_xs, test_xs, training_steps, debug=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    hparams = sorted(sbn.hparams.values().items())\n    hparams = (map(str, x) for x in hparams)\n    hparams = ('_'.join(x) for x in hparams)\n    hparams_str = '.'.join(hparams)\n    logger = L.Logger()\n    experiment_name = [str(sbn.hparams.n_hidden) for i in xrange(sbn.hparams.n_layer)] + [str(sbn.hparams.n_input)]\n    if sbn.hparams.nonlinear:\n        experiment_name = '~'.join(experiment_name)\n    else:\n        experiment_name = '-'.join(experiment_name)\n    experiment_name = 'SBN_%s' % experiment_name\n    rowkey = {'experiment': experiment_name, 'model': hparams_str}\n    summ_dir = os.path.join(FLAGS.working_dir, hparams_str)\n    summary_writer = tf.summary.FileWriter(summ_dir, flush_secs=15, max_queue=100)\n    sv = tf.train.Supervisor(logdir=os.path.join(FLAGS.working_dir, hparams_str), save_summaries_secs=0, save_model_secs=1200, summary_op=None, recovery_wait_secs=30, global_step=sbn.global_step)\n    with sv.managed_session() as sess:\n        with gfile.Open(os.path.join(FLAGS.working_dir, hparams_str, 'hparams.json'), 'w') as out:\n            json.dump(sbn.hparams.values(), out)\n        sbn.initialize(sess)\n        batch_size = sbn.hparams.batch_size\n        scores = []\n        n = train_xs.shape[0]\n        index = range(n)\n        while not sv.should_stop():\n            lHats = []\n            grad_variances = []\n            temperatures = []\n            random.shuffle(index)\n            i = 0\n            while i < n:\n                batch_index = index[i:min(i + batch_size, n)]\n                batch_xs = train_xs[batch_index, :]\n                if sbn.hparams.dynamic_b:\n                    batch_xs = (np.random.rand(*batch_xs.shape) < batch_xs).astype(float)\n                (lHat, grad_variance, step, temperature) = sbn.partial_fit(batch_xs, sbn.hparams.n_samples)\n                if debug:\n                    print(i, lHat)\n                    if i > 100:\n                        return\n                lHats.append(lHat)\n                grad_variances.append(grad_variance)\n                temperatures.append(temperature)\n                i += batch_size\n            grad_variances = np.log(np.mean(grad_variances, axis=0)).tolist()\n            summary_strings = []\n            if isinstance(grad_variances, list):\n                grad_variances = dict(zip([k for (k, v) in sbn.losses], map(float, grad_variances)))\n                rowkey['step'] = step\n                logger.log(rowkey, {'step': step, 'train': np.mean(lHats, axis=0)[0], 'grad_variances': grad_variances, 'temperature': np.mean(temperatures)})\n                grad_variances = '\\n'.join(map(str, sorted(grad_variances.iteritems())))\n            else:\n                rowkey['step'] = step\n                logger.log(rowkey, {'step': step, 'train': np.mean(lHats, axis=0)[0], 'grad_variance': grad_variances, 'temperature': np.mean(temperatures)})\n                summary_strings.append(manual_scalar_summary('log grad variance', grad_variances))\n            print('Step %d: %s\\n%s' % (step, str(np.mean(lHats, axis=0)), str(grad_variances)))\n            epoch = int(step / (train_xs.shape[0] / sbn.hparams.batch_size))\n            if epoch % FLAGS.eval_freq == 0:\n                valid_res = eval(sbn, valid_xs)\n                test_res = eval(sbn, test_xs)\n                print('\\nValid %d: %s' % (step, str(valid_res)))\n                print('Test %d: %s\\n' % (step, str(test_res)))\n                logger.log(rowkey, {'step': step, 'valid': valid_res[0], 'test': test_res[0]})\n                logger.flush()\n            summary_strings.extend([manual_scalar_summary('Train ELBO', np.mean(lHats, axis=0)[0]), manual_scalar_summary('Temperature', np.mean(temperatures))])\n            for summ_str in summary_strings:\n                summary_writer.add_summary(summ_str, global_step=step)\n            summary_writer.flush()\n            sys.stdout.flush()\n            scores.append(np.mean(lHats, axis=0))\n            if step > training_steps:\n                break\n        return scores",
            "def train(sbn, train_xs, valid_xs, test_xs, training_steps, debug=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    hparams = sorted(sbn.hparams.values().items())\n    hparams = (map(str, x) for x in hparams)\n    hparams = ('_'.join(x) for x in hparams)\n    hparams_str = '.'.join(hparams)\n    logger = L.Logger()\n    experiment_name = [str(sbn.hparams.n_hidden) for i in xrange(sbn.hparams.n_layer)] + [str(sbn.hparams.n_input)]\n    if sbn.hparams.nonlinear:\n        experiment_name = '~'.join(experiment_name)\n    else:\n        experiment_name = '-'.join(experiment_name)\n    experiment_name = 'SBN_%s' % experiment_name\n    rowkey = {'experiment': experiment_name, 'model': hparams_str}\n    summ_dir = os.path.join(FLAGS.working_dir, hparams_str)\n    summary_writer = tf.summary.FileWriter(summ_dir, flush_secs=15, max_queue=100)\n    sv = tf.train.Supervisor(logdir=os.path.join(FLAGS.working_dir, hparams_str), save_summaries_secs=0, save_model_secs=1200, summary_op=None, recovery_wait_secs=30, global_step=sbn.global_step)\n    with sv.managed_session() as sess:\n        with gfile.Open(os.path.join(FLAGS.working_dir, hparams_str, 'hparams.json'), 'w') as out:\n            json.dump(sbn.hparams.values(), out)\n        sbn.initialize(sess)\n        batch_size = sbn.hparams.batch_size\n        scores = []\n        n = train_xs.shape[0]\n        index = range(n)\n        while not sv.should_stop():\n            lHats = []\n            grad_variances = []\n            temperatures = []\n            random.shuffle(index)\n            i = 0\n            while i < n:\n                batch_index = index[i:min(i + batch_size, n)]\n                batch_xs = train_xs[batch_index, :]\n                if sbn.hparams.dynamic_b:\n                    batch_xs = (np.random.rand(*batch_xs.shape) < batch_xs).astype(float)\n                (lHat, grad_variance, step, temperature) = sbn.partial_fit(batch_xs, sbn.hparams.n_samples)\n                if debug:\n                    print(i, lHat)\n                    if i > 100:\n                        return\n                lHats.append(lHat)\n                grad_variances.append(grad_variance)\n                temperatures.append(temperature)\n                i += batch_size\n            grad_variances = np.log(np.mean(grad_variances, axis=0)).tolist()\n            summary_strings = []\n            if isinstance(grad_variances, list):\n                grad_variances = dict(zip([k for (k, v) in sbn.losses], map(float, grad_variances)))\n                rowkey['step'] = step\n                logger.log(rowkey, {'step': step, 'train': np.mean(lHats, axis=0)[0], 'grad_variances': grad_variances, 'temperature': np.mean(temperatures)})\n                grad_variances = '\\n'.join(map(str, sorted(grad_variances.iteritems())))\n            else:\n                rowkey['step'] = step\n                logger.log(rowkey, {'step': step, 'train': np.mean(lHats, axis=0)[0], 'grad_variance': grad_variances, 'temperature': np.mean(temperatures)})\n                summary_strings.append(manual_scalar_summary('log grad variance', grad_variances))\n            print('Step %d: %s\\n%s' % (step, str(np.mean(lHats, axis=0)), str(grad_variances)))\n            epoch = int(step / (train_xs.shape[0] / sbn.hparams.batch_size))\n            if epoch % FLAGS.eval_freq == 0:\n                valid_res = eval(sbn, valid_xs)\n                test_res = eval(sbn, test_xs)\n                print('\\nValid %d: %s' % (step, str(valid_res)))\n                print('Test %d: %s\\n' % (step, str(test_res)))\n                logger.log(rowkey, {'step': step, 'valid': valid_res[0], 'test': test_res[0]})\n                logger.flush()\n            summary_strings.extend([manual_scalar_summary('Train ELBO', np.mean(lHats, axis=0)[0]), manual_scalar_summary('Temperature', np.mean(temperatures))])\n            for summ_str in summary_strings:\n                summary_writer.add_summary(summ_str, global_step=step)\n            summary_writer.flush()\n            sys.stdout.flush()\n            scores.append(np.mean(lHats, axis=0))\n            if step > training_steps:\n                break\n        return scores",
            "def train(sbn, train_xs, valid_xs, test_xs, training_steps, debug=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    hparams = sorted(sbn.hparams.values().items())\n    hparams = (map(str, x) for x in hparams)\n    hparams = ('_'.join(x) for x in hparams)\n    hparams_str = '.'.join(hparams)\n    logger = L.Logger()\n    experiment_name = [str(sbn.hparams.n_hidden) for i in xrange(sbn.hparams.n_layer)] + [str(sbn.hparams.n_input)]\n    if sbn.hparams.nonlinear:\n        experiment_name = '~'.join(experiment_name)\n    else:\n        experiment_name = '-'.join(experiment_name)\n    experiment_name = 'SBN_%s' % experiment_name\n    rowkey = {'experiment': experiment_name, 'model': hparams_str}\n    summ_dir = os.path.join(FLAGS.working_dir, hparams_str)\n    summary_writer = tf.summary.FileWriter(summ_dir, flush_secs=15, max_queue=100)\n    sv = tf.train.Supervisor(logdir=os.path.join(FLAGS.working_dir, hparams_str), save_summaries_secs=0, save_model_secs=1200, summary_op=None, recovery_wait_secs=30, global_step=sbn.global_step)\n    with sv.managed_session() as sess:\n        with gfile.Open(os.path.join(FLAGS.working_dir, hparams_str, 'hparams.json'), 'w') as out:\n            json.dump(sbn.hparams.values(), out)\n        sbn.initialize(sess)\n        batch_size = sbn.hparams.batch_size\n        scores = []\n        n = train_xs.shape[0]\n        index = range(n)\n        while not sv.should_stop():\n            lHats = []\n            grad_variances = []\n            temperatures = []\n            random.shuffle(index)\n            i = 0\n            while i < n:\n                batch_index = index[i:min(i + batch_size, n)]\n                batch_xs = train_xs[batch_index, :]\n                if sbn.hparams.dynamic_b:\n                    batch_xs = (np.random.rand(*batch_xs.shape) < batch_xs).astype(float)\n                (lHat, grad_variance, step, temperature) = sbn.partial_fit(batch_xs, sbn.hparams.n_samples)\n                if debug:\n                    print(i, lHat)\n                    if i > 100:\n                        return\n                lHats.append(lHat)\n                grad_variances.append(grad_variance)\n                temperatures.append(temperature)\n                i += batch_size\n            grad_variances = np.log(np.mean(grad_variances, axis=0)).tolist()\n            summary_strings = []\n            if isinstance(grad_variances, list):\n                grad_variances = dict(zip([k for (k, v) in sbn.losses], map(float, grad_variances)))\n                rowkey['step'] = step\n                logger.log(rowkey, {'step': step, 'train': np.mean(lHats, axis=0)[0], 'grad_variances': grad_variances, 'temperature': np.mean(temperatures)})\n                grad_variances = '\\n'.join(map(str, sorted(grad_variances.iteritems())))\n            else:\n                rowkey['step'] = step\n                logger.log(rowkey, {'step': step, 'train': np.mean(lHats, axis=0)[0], 'grad_variance': grad_variances, 'temperature': np.mean(temperatures)})\n                summary_strings.append(manual_scalar_summary('log grad variance', grad_variances))\n            print('Step %d: %s\\n%s' % (step, str(np.mean(lHats, axis=0)), str(grad_variances)))\n            epoch = int(step / (train_xs.shape[0] / sbn.hparams.batch_size))\n            if epoch % FLAGS.eval_freq == 0:\n                valid_res = eval(sbn, valid_xs)\n                test_res = eval(sbn, test_xs)\n                print('\\nValid %d: %s' % (step, str(valid_res)))\n                print('Test %d: %s\\n' % (step, str(test_res)))\n                logger.log(rowkey, {'step': step, 'valid': valid_res[0], 'test': test_res[0]})\n                logger.flush()\n            summary_strings.extend([manual_scalar_summary('Train ELBO', np.mean(lHats, axis=0)[0]), manual_scalar_summary('Temperature', np.mean(temperatures))])\n            for summ_str in summary_strings:\n                summary_writer.add_summary(summ_str, global_step=step)\n            summary_writer.flush()\n            sys.stdout.flush()\n            scores.append(np.mean(lHats, axis=0))\n            if step > training_steps:\n                break\n        return scores",
            "def train(sbn, train_xs, valid_xs, test_xs, training_steps, debug=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    hparams = sorted(sbn.hparams.values().items())\n    hparams = (map(str, x) for x in hparams)\n    hparams = ('_'.join(x) for x in hparams)\n    hparams_str = '.'.join(hparams)\n    logger = L.Logger()\n    experiment_name = [str(sbn.hparams.n_hidden) for i in xrange(sbn.hparams.n_layer)] + [str(sbn.hparams.n_input)]\n    if sbn.hparams.nonlinear:\n        experiment_name = '~'.join(experiment_name)\n    else:\n        experiment_name = '-'.join(experiment_name)\n    experiment_name = 'SBN_%s' % experiment_name\n    rowkey = {'experiment': experiment_name, 'model': hparams_str}\n    summ_dir = os.path.join(FLAGS.working_dir, hparams_str)\n    summary_writer = tf.summary.FileWriter(summ_dir, flush_secs=15, max_queue=100)\n    sv = tf.train.Supervisor(logdir=os.path.join(FLAGS.working_dir, hparams_str), save_summaries_secs=0, save_model_secs=1200, summary_op=None, recovery_wait_secs=30, global_step=sbn.global_step)\n    with sv.managed_session() as sess:\n        with gfile.Open(os.path.join(FLAGS.working_dir, hparams_str, 'hparams.json'), 'w') as out:\n            json.dump(sbn.hparams.values(), out)\n        sbn.initialize(sess)\n        batch_size = sbn.hparams.batch_size\n        scores = []\n        n = train_xs.shape[0]\n        index = range(n)\n        while not sv.should_stop():\n            lHats = []\n            grad_variances = []\n            temperatures = []\n            random.shuffle(index)\n            i = 0\n            while i < n:\n                batch_index = index[i:min(i + batch_size, n)]\n                batch_xs = train_xs[batch_index, :]\n                if sbn.hparams.dynamic_b:\n                    batch_xs = (np.random.rand(*batch_xs.shape) < batch_xs).astype(float)\n                (lHat, grad_variance, step, temperature) = sbn.partial_fit(batch_xs, sbn.hparams.n_samples)\n                if debug:\n                    print(i, lHat)\n                    if i > 100:\n                        return\n                lHats.append(lHat)\n                grad_variances.append(grad_variance)\n                temperatures.append(temperature)\n                i += batch_size\n            grad_variances = np.log(np.mean(grad_variances, axis=0)).tolist()\n            summary_strings = []\n            if isinstance(grad_variances, list):\n                grad_variances = dict(zip([k for (k, v) in sbn.losses], map(float, grad_variances)))\n                rowkey['step'] = step\n                logger.log(rowkey, {'step': step, 'train': np.mean(lHats, axis=0)[0], 'grad_variances': grad_variances, 'temperature': np.mean(temperatures)})\n                grad_variances = '\\n'.join(map(str, sorted(grad_variances.iteritems())))\n            else:\n                rowkey['step'] = step\n                logger.log(rowkey, {'step': step, 'train': np.mean(lHats, axis=0)[0], 'grad_variance': grad_variances, 'temperature': np.mean(temperatures)})\n                summary_strings.append(manual_scalar_summary('log grad variance', grad_variances))\n            print('Step %d: %s\\n%s' % (step, str(np.mean(lHats, axis=0)), str(grad_variances)))\n            epoch = int(step / (train_xs.shape[0] / sbn.hparams.batch_size))\n            if epoch % FLAGS.eval_freq == 0:\n                valid_res = eval(sbn, valid_xs)\n                test_res = eval(sbn, test_xs)\n                print('\\nValid %d: %s' % (step, str(valid_res)))\n                print('Test %d: %s\\n' % (step, str(test_res)))\n                logger.log(rowkey, {'step': step, 'valid': valid_res[0], 'test': test_res[0]})\n                logger.flush()\n            summary_strings.extend([manual_scalar_summary('Train ELBO', np.mean(lHats, axis=0)[0]), manual_scalar_summary('Temperature', np.mean(temperatures))])\n            for summ_str in summary_strings:\n                summary_writer.add_summary(summ_str, global_step=step)\n            summary_writer.flush()\n            sys.stdout.flush()\n            scores.append(np.mean(lHats, axis=0))\n            if step > training_steps:\n                break\n        return scores"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    hparams = rebar.default_hparams\n    hparams.parse(FLAGS.hparams)\n    print(hparams.values())\n    (train_xs, valid_xs, test_xs) = datasets.load_data(hparams)\n    mean_xs = np.mean(train_xs, axis=0)\n    training_steps = 2000000\n    model = getattr(rebar, hparams.model)\n    sbn = model(hparams, mean_xs=mean_xs)\n    scores = train(sbn, train_xs, valid_xs, test_xs, training_steps=training_steps, debug=False)",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    hparams = rebar.default_hparams\n    hparams.parse(FLAGS.hparams)\n    print(hparams.values())\n    (train_xs, valid_xs, test_xs) = datasets.load_data(hparams)\n    mean_xs = np.mean(train_xs, axis=0)\n    training_steps = 2000000\n    model = getattr(rebar, hparams.model)\n    sbn = model(hparams, mean_xs=mean_xs)\n    scores = train(sbn, train_xs, valid_xs, test_xs, training_steps=training_steps, debug=False)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    hparams = rebar.default_hparams\n    hparams.parse(FLAGS.hparams)\n    print(hparams.values())\n    (train_xs, valid_xs, test_xs) = datasets.load_data(hparams)\n    mean_xs = np.mean(train_xs, axis=0)\n    training_steps = 2000000\n    model = getattr(rebar, hparams.model)\n    sbn = model(hparams, mean_xs=mean_xs)\n    scores = train(sbn, train_xs, valid_xs, test_xs, training_steps=training_steps, debug=False)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    hparams = rebar.default_hparams\n    hparams.parse(FLAGS.hparams)\n    print(hparams.values())\n    (train_xs, valid_xs, test_xs) = datasets.load_data(hparams)\n    mean_xs = np.mean(train_xs, axis=0)\n    training_steps = 2000000\n    model = getattr(rebar, hparams.model)\n    sbn = model(hparams, mean_xs=mean_xs)\n    scores = train(sbn, train_xs, valid_xs, test_xs, training_steps=training_steps, debug=False)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    hparams = rebar.default_hparams\n    hparams.parse(FLAGS.hparams)\n    print(hparams.values())\n    (train_xs, valid_xs, test_xs) = datasets.load_data(hparams)\n    mean_xs = np.mean(train_xs, axis=0)\n    training_steps = 2000000\n    model = getattr(rebar, hparams.model)\n    sbn = model(hparams, mean_xs=mean_xs)\n    scores = train(sbn, train_xs, valid_xs, test_xs, training_steps=training_steps, debug=False)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    hparams = rebar.default_hparams\n    hparams.parse(FLAGS.hparams)\n    print(hparams.values())\n    (train_xs, valid_xs, test_xs) = datasets.load_data(hparams)\n    mean_xs = np.mean(train_xs, axis=0)\n    training_steps = 2000000\n    model = getattr(rebar, hparams.model)\n    sbn = model(hparams, mean_xs=mean_xs)\n    scores = train(sbn, train_xs, valid_xs, test_xs, training_steps=training_steps, debug=False)"
        ]
    }
]