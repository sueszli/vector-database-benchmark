[
    {
        "func_name": "get_parser",
        "original": "def get_parser():\n    parser = argparse.ArgumentParser(description='transforms features via a given pca and stored them in target dir')\n    parser.add_argument('source', help='directory with features')\n    parser.add_argument('--split', help='which split to read', required=True)\n    parser.add_argument('--save-dir', help='where to save the output', required=True)\n    parser.add_argument('--cluster-dir', help='where the clusters are')\n    parser.add_argument('--pooling', type=str, default='mean', choices=['mean', 'sample'], help='how to pool')\n    return parser",
        "mutated": [
            "def get_parser():\n    if False:\n        i = 10\n    parser = argparse.ArgumentParser(description='transforms features via a given pca and stored them in target dir')\n    parser.add_argument('source', help='directory with features')\n    parser.add_argument('--split', help='which split to read', required=True)\n    parser.add_argument('--save-dir', help='where to save the output', required=True)\n    parser.add_argument('--cluster-dir', help='where the clusters are')\n    parser.add_argument('--pooling', type=str, default='mean', choices=['mean', 'sample'], help='how to pool')\n    return parser",
            "def get_parser():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = argparse.ArgumentParser(description='transforms features via a given pca and stored them in target dir')\n    parser.add_argument('source', help='directory with features')\n    parser.add_argument('--split', help='which split to read', required=True)\n    parser.add_argument('--save-dir', help='where to save the output', required=True)\n    parser.add_argument('--cluster-dir', help='where the clusters are')\n    parser.add_argument('--pooling', type=str, default='mean', choices=['mean', 'sample'], help='how to pool')\n    return parser",
            "def get_parser():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = argparse.ArgumentParser(description='transforms features via a given pca and stored them in target dir')\n    parser.add_argument('source', help='directory with features')\n    parser.add_argument('--split', help='which split to read', required=True)\n    parser.add_argument('--save-dir', help='where to save the output', required=True)\n    parser.add_argument('--cluster-dir', help='where the clusters are')\n    parser.add_argument('--pooling', type=str, default='mean', choices=['mean', 'sample'], help='how to pool')\n    return parser",
            "def get_parser():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = argparse.ArgumentParser(description='transforms features via a given pca and stored them in target dir')\n    parser.add_argument('source', help='directory with features')\n    parser.add_argument('--split', help='which split to read', required=True)\n    parser.add_argument('--save-dir', help='where to save the output', required=True)\n    parser.add_argument('--cluster-dir', help='where the clusters are')\n    parser.add_argument('--pooling', type=str, default='mean', choices=['mean', 'sample'], help='how to pool')\n    return parser",
            "def get_parser():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = argparse.ArgumentParser(description='transforms features via a given pca and stored them in target dir')\n    parser.add_argument('source', help='directory with features')\n    parser.add_argument('--split', help='which split to read', required=True)\n    parser.add_argument('--save-dir', help='where to save the output', required=True)\n    parser.add_argument('--cluster-dir', help='where the clusters are')\n    parser.add_argument('--pooling', type=str, default='mean', choices=['mean', 'sample'], help='how to pool')\n    return parser"
        ]
    },
    {
        "func_name": "merge",
        "original": "def merge(feats, clust):\n    feats = torch.from_numpy(feats.copy())\n    clust = torch.LongTensor(clust)\n    (_, counts) = clust.unique_consecutive(return_counts=True)\n    curr = 0\n    merged = []\n    for c in counts:\n        c = c.item()\n        start = curr\n        end = curr + c\n        curr += c\n        if args.pooling == 'mean':\n            new_x = feats[start:end].mean(dim=0)\n        elif args.pooling == 'sample':\n            new_x = feats[start + int(random.random() * c)]\n        else:\n            raise NotImplementedError()\n        merged.append(new_x)\n    return torch.stack(merged, dim=0).numpy()",
        "mutated": [
            "def merge(feats, clust):\n    if False:\n        i = 10\n    feats = torch.from_numpy(feats.copy())\n    clust = torch.LongTensor(clust)\n    (_, counts) = clust.unique_consecutive(return_counts=True)\n    curr = 0\n    merged = []\n    for c in counts:\n        c = c.item()\n        start = curr\n        end = curr + c\n        curr += c\n        if args.pooling == 'mean':\n            new_x = feats[start:end].mean(dim=0)\n        elif args.pooling == 'sample':\n            new_x = feats[start + int(random.random() * c)]\n        else:\n            raise NotImplementedError()\n        merged.append(new_x)\n    return torch.stack(merged, dim=0).numpy()",
            "def merge(feats, clust):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    feats = torch.from_numpy(feats.copy())\n    clust = torch.LongTensor(clust)\n    (_, counts) = clust.unique_consecutive(return_counts=True)\n    curr = 0\n    merged = []\n    for c in counts:\n        c = c.item()\n        start = curr\n        end = curr + c\n        curr += c\n        if args.pooling == 'mean':\n            new_x = feats[start:end].mean(dim=0)\n        elif args.pooling == 'sample':\n            new_x = feats[start + int(random.random() * c)]\n        else:\n            raise NotImplementedError()\n        merged.append(new_x)\n    return torch.stack(merged, dim=0).numpy()",
            "def merge(feats, clust):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    feats = torch.from_numpy(feats.copy())\n    clust = torch.LongTensor(clust)\n    (_, counts) = clust.unique_consecutive(return_counts=True)\n    curr = 0\n    merged = []\n    for c in counts:\n        c = c.item()\n        start = curr\n        end = curr + c\n        curr += c\n        if args.pooling == 'mean':\n            new_x = feats[start:end].mean(dim=0)\n        elif args.pooling == 'sample':\n            new_x = feats[start + int(random.random() * c)]\n        else:\n            raise NotImplementedError()\n        merged.append(new_x)\n    return torch.stack(merged, dim=0).numpy()",
            "def merge(feats, clust):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    feats = torch.from_numpy(feats.copy())\n    clust = torch.LongTensor(clust)\n    (_, counts) = clust.unique_consecutive(return_counts=True)\n    curr = 0\n    merged = []\n    for c in counts:\n        c = c.item()\n        start = curr\n        end = curr + c\n        curr += c\n        if args.pooling == 'mean':\n            new_x = feats[start:end].mean(dim=0)\n        elif args.pooling == 'sample':\n            new_x = feats[start + int(random.random() * c)]\n        else:\n            raise NotImplementedError()\n        merged.append(new_x)\n    return torch.stack(merged, dim=0).numpy()",
            "def merge(feats, clust):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    feats = torch.from_numpy(feats.copy())\n    clust = torch.LongTensor(clust)\n    (_, counts) = clust.unique_consecutive(return_counts=True)\n    curr = 0\n    merged = []\n    for c in counts:\n        c = c.item()\n        start = curr\n        end = curr + c\n        curr += c\n        if args.pooling == 'mean':\n            new_x = feats[start:end].mean(dim=0)\n        elif args.pooling == 'sample':\n            new_x = feats[start + int(random.random() * c)]\n        else:\n            raise NotImplementedError()\n        merged.append(new_x)\n    return torch.stack(merged, dim=0).numpy()"
        ]
    },
    {
        "func_name": "main",
        "original": "def main():\n    parser = get_parser()\n    args = parser.parse_args()\n    source_path = osp.join(args.source, args.split)\n    cluster_path = osp.join(args.cluster_dir, args.split + '.src')\n    print(f'data path: {source_path}')\n    features = np.load(source_path + '.npy', mmap_mode='r')\n    sizes = []\n    offsets = []\n    offset = 0\n    with open(source_path + '.lengths', 'r') as len_f:\n        for line in len_f:\n            length = int(line.rstrip())\n            sizes.append(length)\n            offsets.append(offset)\n            offset += length\n    clusters = []\n    with open(cluster_path, 'r') as cf:\n        for line in cf:\n            line = line.rstrip()\n            items = line.split()\n            items = list(map(int, items))\n            clusters.append(items)\n    os.makedirs(args.save_dir, exist_ok=True)\n    save_path = osp.join(args.save_dir, args.split)\n    copyfile(source_path + '.tsv', save_path + '.tsv')\n    if os.path.exists(source_path + '.phn'):\n        copyfile(source_path + '.phn', save_path + '.phn')\n    if os.path.exists(osp.join(args.source, 'dict.phn.txt')):\n        copyfile(osp.join(args.source, 'dict.phn.txt'), osp.join(args.save_dir, 'dict.phn.txt'))\n    if os.path.exists(source_path + '.wrd'):\n        copyfile(source_path + '.wrd', save_path + '.wrd')\n    if osp.exists(save_path + '.npy'):\n        os.remove(save_path + '.npy')\n    npaa = NpyAppendArray(save_path + '.npy')\n\n    def merge(feats, clust):\n        feats = torch.from_numpy(feats.copy())\n        clust = torch.LongTensor(clust)\n        (_, counts) = clust.unique_consecutive(return_counts=True)\n        curr = 0\n        merged = []\n        for c in counts:\n            c = c.item()\n            start = curr\n            end = curr + c\n            curr += c\n            if args.pooling == 'mean':\n                new_x = feats[start:end].mean(dim=0)\n            elif args.pooling == 'sample':\n                new_x = feats[start + int(random.random() * c)]\n            else:\n                raise NotImplementedError()\n            merged.append(new_x)\n        return torch.stack(merged, dim=0).numpy()\n    with open(save_path + '.lengths', 'w') as l_f:\n        for (size, offset, clust) in tqdm.tqdm(zip(sizes, offsets, clusters), total=len(sizes)):\n            end = size + offset\n            feats = features[offset:end]\n            feats = merge(feats, clust)\n            print(len(feats), file=l_f)\n            npaa.append(feats)",
        "mutated": [
            "def main():\n    if False:\n        i = 10\n    parser = get_parser()\n    args = parser.parse_args()\n    source_path = osp.join(args.source, args.split)\n    cluster_path = osp.join(args.cluster_dir, args.split + '.src')\n    print(f'data path: {source_path}')\n    features = np.load(source_path + '.npy', mmap_mode='r')\n    sizes = []\n    offsets = []\n    offset = 0\n    with open(source_path + '.lengths', 'r') as len_f:\n        for line in len_f:\n            length = int(line.rstrip())\n            sizes.append(length)\n            offsets.append(offset)\n            offset += length\n    clusters = []\n    with open(cluster_path, 'r') as cf:\n        for line in cf:\n            line = line.rstrip()\n            items = line.split()\n            items = list(map(int, items))\n            clusters.append(items)\n    os.makedirs(args.save_dir, exist_ok=True)\n    save_path = osp.join(args.save_dir, args.split)\n    copyfile(source_path + '.tsv', save_path + '.tsv')\n    if os.path.exists(source_path + '.phn'):\n        copyfile(source_path + '.phn', save_path + '.phn')\n    if os.path.exists(osp.join(args.source, 'dict.phn.txt')):\n        copyfile(osp.join(args.source, 'dict.phn.txt'), osp.join(args.save_dir, 'dict.phn.txt'))\n    if os.path.exists(source_path + '.wrd'):\n        copyfile(source_path + '.wrd', save_path + '.wrd')\n    if osp.exists(save_path + '.npy'):\n        os.remove(save_path + '.npy')\n    npaa = NpyAppendArray(save_path + '.npy')\n\n    def merge(feats, clust):\n        feats = torch.from_numpy(feats.copy())\n        clust = torch.LongTensor(clust)\n        (_, counts) = clust.unique_consecutive(return_counts=True)\n        curr = 0\n        merged = []\n        for c in counts:\n            c = c.item()\n            start = curr\n            end = curr + c\n            curr += c\n            if args.pooling == 'mean':\n                new_x = feats[start:end].mean(dim=0)\n            elif args.pooling == 'sample':\n                new_x = feats[start + int(random.random() * c)]\n            else:\n                raise NotImplementedError()\n            merged.append(new_x)\n        return torch.stack(merged, dim=0).numpy()\n    with open(save_path + '.lengths', 'w') as l_f:\n        for (size, offset, clust) in tqdm.tqdm(zip(sizes, offsets, clusters), total=len(sizes)):\n            end = size + offset\n            feats = features[offset:end]\n            feats = merge(feats, clust)\n            print(len(feats), file=l_f)\n            npaa.append(feats)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    parser = get_parser()\n    args = parser.parse_args()\n    source_path = osp.join(args.source, args.split)\n    cluster_path = osp.join(args.cluster_dir, args.split + '.src')\n    print(f'data path: {source_path}')\n    features = np.load(source_path + '.npy', mmap_mode='r')\n    sizes = []\n    offsets = []\n    offset = 0\n    with open(source_path + '.lengths', 'r') as len_f:\n        for line in len_f:\n            length = int(line.rstrip())\n            sizes.append(length)\n            offsets.append(offset)\n            offset += length\n    clusters = []\n    with open(cluster_path, 'r') as cf:\n        for line in cf:\n            line = line.rstrip()\n            items = line.split()\n            items = list(map(int, items))\n            clusters.append(items)\n    os.makedirs(args.save_dir, exist_ok=True)\n    save_path = osp.join(args.save_dir, args.split)\n    copyfile(source_path + '.tsv', save_path + '.tsv')\n    if os.path.exists(source_path + '.phn'):\n        copyfile(source_path + '.phn', save_path + '.phn')\n    if os.path.exists(osp.join(args.source, 'dict.phn.txt')):\n        copyfile(osp.join(args.source, 'dict.phn.txt'), osp.join(args.save_dir, 'dict.phn.txt'))\n    if os.path.exists(source_path + '.wrd'):\n        copyfile(source_path + '.wrd', save_path + '.wrd')\n    if osp.exists(save_path + '.npy'):\n        os.remove(save_path + '.npy')\n    npaa = NpyAppendArray(save_path + '.npy')\n\n    def merge(feats, clust):\n        feats = torch.from_numpy(feats.copy())\n        clust = torch.LongTensor(clust)\n        (_, counts) = clust.unique_consecutive(return_counts=True)\n        curr = 0\n        merged = []\n        for c in counts:\n            c = c.item()\n            start = curr\n            end = curr + c\n            curr += c\n            if args.pooling == 'mean':\n                new_x = feats[start:end].mean(dim=0)\n            elif args.pooling == 'sample':\n                new_x = feats[start + int(random.random() * c)]\n            else:\n                raise NotImplementedError()\n            merged.append(new_x)\n        return torch.stack(merged, dim=0).numpy()\n    with open(save_path + '.lengths', 'w') as l_f:\n        for (size, offset, clust) in tqdm.tqdm(zip(sizes, offsets, clusters), total=len(sizes)):\n            end = size + offset\n            feats = features[offset:end]\n            feats = merge(feats, clust)\n            print(len(feats), file=l_f)\n            npaa.append(feats)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    parser = get_parser()\n    args = parser.parse_args()\n    source_path = osp.join(args.source, args.split)\n    cluster_path = osp.join(args.cluster_dir, args.split + '.src')\n    print(f'data path: {source_path}')\n    features = np.load(source_path + '.npy', mmap_mode='r')\n    sizes = []\n    offsets = []\n    offset = 0\n    with open(source_path + '.lengths', 'r') as len_f:\n        for line in len_f:\n            length = int(line.rstrip())\n            sizes.append(length)\n            offsets.append(offset)\n            offset += length\n    clusters = []\n    with open(cluster_path, 'r') as cf:\n        for line in cf:\n            line = line.rstrip()\n            items = line.split()\n            items = list(map(int, items))\n            clusters.append(items)\n    os.makedirs(args.save_dir, exist_ok=True)\n    save_path = osp.join(args.save_dir, args.split)\n    copyfile(source_path + '.tsv', save_path + '.tsv')\n    if os.path.exists(source_path + '.phn'):\n        copyfile(source_path + '.phn', save_path + '.phn')\n    if os.path.exists(osp.join(args.source, 'dict.phn.txt')):\n        copyfile(osp.join(args.source, 'dict.phn.txt'), osp.join(args.save_dir, 'dict.phn.txt'))\n    if os.path.exists(source_path + '.wrd'):\n        copyfile(source_path + '.wrd', save_path + '.wrd')\n    if osp.exists(save_path + '.npy'):\n        os.remove(save_path + '.npy')\n    npaa = NpyAppendArray(save_path + '.npy')\n\n    def merge(feats, clust):\n        feats = torch.from_numpy(feats.copy())\n        clust = torch.LongTensor(clust)\n        (_, counts) = clust.unique_consecutive(return_counts=True)\n        curr = 0\n        merged = []\n        for c in counts:\n            c = c.item()\n            start = curr\n            end = curr + c\n            curr += c\n            if args.pooling == 'mean':\n                new_x = feats[start:end].mean(dim=0)\n            elif args.pooling == 'sample':\n                new_x = feats[start + int(random.random() * c)]\n            else:\n                raise NotImplementedError()\n            merged.append(new_x)\n        return torch.stack(merged, dim=0).numpy()\n    with open(save_path + '.lengths', 'w') as l_f:\n        for (size, offset, clust) in tqdm.tqdm(zip(sizes, offsets, clusters), total=len(sizes)):\n            end = size + offset\n            feats = features[offset:end]\n            feats = merge(feats, clust)\n            print(len(feats), file=l_f)\n            npaa.append(feats)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    parser = get_parser()\n    args = parser.parse_args()\n    source_path = osp.join(args.source, args.split)\n    cluster_path = osp.join(args.cluster_dir, args.split + '.src')\n    print(f'data path: {source_path}')\n    features = np.load(source_path + '.npy', mmap_mode='r')\n    sizes = []\n    offsets = []\n    offset = 0\n    with open(source_path + '.lengths', 'r') as len_f:\n        for line in len_f:\n            length = int(line.rstrip())\n            sizes.append(length)\n            offsets.append(offset)\n            offset += length\n    clusters = []\n    with open(cluster_path, 'r') as cf:\n        for line in cf:\n            line = line.rstrip()\n            items = line.split()\n            items = list(map(int, items))\n            clusters.append(items)\n    os.makedirs(args.save_dir, exist_ok=True)\n    save_path = osp.join(args.save_dir, args.split)\n    copyfile(source_path + '.tsv', save_path + '.tsv')\n    if os.path.exists(source_path + '.phn'):\n        copyfile(source_path + '.phn', save_path + '.phn')\n    if os.path.exists(osp.join(args.source, 'dict.phn.txt')):\n        copyfile(osp.join(args.source, 'dict.phn.txt'), osp.join(args.save_dir, 'dict.phn.txt'))\n    if os.path.exists(source_path + '.wrd'):\n        copyfile(source_path + '.wrd', save_path + '.wrd')\n    if osp.exists(save_path + '.npy'):\n        os.remove(save_path + '.npy')\n    npaa = NpyAppendArray(save_path + '.npy')\n\n    def merge(feats, clust):\n        feats = torch.from_numpy(feats.copy())\n        clust = torch.LongTensor(clust)\n        (_, counts) = clust.unique_consecutive(return_counts=True)\n        curr = 0\n        merged = []\n        for c in counts:\n            c = c.item()\n            start = curr\n            end = curr + c\n            curr += c\n            if args.pooling == 'mean':\n                new_x = feats[start:end].mean(dim=0)\n            elif args.pooling == 'sample':\n                new_x = feats[start + int(random.random() * c)]\n            else:\n                raise NotImplementedError()\n            merged.append(new_x)\n        return torch.stack(merged, dim=0).numpy()\n    with open(save_path + '.lengths', 'w') as l_f:\n        for (size, offset, clust) in tqdm.tqdm(zip(sizes, offsets, clusters), total=len(sizes)):\n            end = size + offset\n            feats = features[offset:end]\n            feats = merge(feats, clust)\n            print(len(feats), file=l_f)\n            npaa.append(feats)",
            "def main():\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    parser = get_parser()\n    args = parser.parse_args()\n    source_path = osp.join(args.source, args.split)\n    cluster_path = osp.join(args.cluster_dir, args.split + '.src')\n    print(f'data path: {source_path}')\n    features = np.load(source_path + '.npy', mmap_mode='r')\n    sizes = []\n    offsets = []\n    offset = 0\n    with open(source_path + '.lengths', 'r') as len_f:\n        for line in len_f:\n            length = int(line.rstrip())\n            sizes.append(length)\n            offsets.append(offset)\n            offset += length\n    clusters = []\n    with open(cluster_path, 'r') as cf:\n        for line in cf:\n            line = line.rstrip()\n            items = line.split()\n            items = list(map(int, items))\n            clusters.append(items)\n    os.makedirs(args.save_dir, exist_ok=True)\n    save_path = osp.join(args.save_dir, args.split)\n    copyfile(source_path + '.tsv', save_path + '.tsv')\n    if os.path.exists(source_path + '.phn'):\n        copyfile(source_path + '.phn', save_path + '.phn')\n    if os.path.exists(osp.join(args.source, 'dict.phn.txt')):\n        copyfile(osp.join(args.source, 'dict.phn.txt'), osp.join(args.save_dir, 'dict.phn.txt'))\n    if os.path.exists(source_path + '.wrd'):\n        copyfile(source_path + '.wrd', save_path + '.wrd')\n    if osp.exists(save_path + '.npy'):\n        os.remove(save_path + '.npy')\n    npaa = NpyAppendArray(save_path + '.npy')\n\n    def merge(feats, clust):\n        feats = torch.from_numpy(feats.copy())\n        clust = torch.LongTensor(clust)\n        (_, counts) = clust.unique_consecutive(return_counts=True)\n        curr = 0\n        merged = []\n        for c in counts:\n            c = c.item()\n            start = curr\n            end = curr + c\n            curr += c\n            if args.pooling == 'mean':\n                new_x = feats[start:end].mean(dim=0)\n            elif args.pooling == 'sample':\n                new_x = feats[start + int(random.random() * c)]\n            else:\n                raise NotImplementedError()\n            merged.append(new_x)\n        return torch.stack(merged, dim=0).numpy()\n    with open(save_path + '.lengths', 'w') as l_f:\n        for (size, offset, clust) in tqdm.tqdm(zip(sizes, offsets, clusters), total=len(sizes)):\n            end = size + offset\n            feats = features[offset:end]\n            feats = merge(feats, clust)\n            print(len(feats), file=l_f)\n            npaa.append(feats)"
        ]
    }
]