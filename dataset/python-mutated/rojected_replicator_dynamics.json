[
    {
        "func_name": "_partial_multi_dot",
        "original": "def _partial_multi_dot(player_payoff_tensor, strategies, index_avoided):\n    \"\"\"Computes a generalized dot product avoiding one dimension.\n\n  This is used to directly get the expected return of a given action, given\n  other players' strategies, for the player indexed by index_avoided.\n  Note that the numpy.dot function is used to compute this product, as it ended\n  up being (Slightly) faster in performance tests than np.tensordot. Using the\n  reduce function proved slower for both np.dot and np.tensordot.\n\n  Args:\n    player_payoff_tensor: payoff tensor for player[index_avoided], of dimension\n      (dim(vector[0]), dim(vector[1]), ..., dim(vector[-1])).\n    strategies: Meta strategy probabilities for each player.\n    index_avoided: Player for which we do not compute the dot product.\n\n  Returns:\n    Vector of expected returns for each action of player [the player indexed by\n      index_avoided].\n  \"\"\"\n    new_axis_order = [index_avoided] + [i for i in range(len(strategies)) if i != index_avoided]\n    accumulator = np.transpose(player_payoff_tensor, new_axis_order)\n    for i in range(len(strategies) - 1, -1, -1):\n        if i != index_avoided:\n            accumulator = np.dot(accumulator, strategies[i])\n    return accumulator",
        "mutated": [
            "def _partial_multi_dot(player_payoff_tensor, strategies, index_avoided):\n    if False:\n        i = 10\n    \"Computes a generalized dot product avoiding one dimension.\\n\\n  This is used to directly get the expected return of a given action, given\\n  other players' strategies, for the player indexed by index_avoided.\\n  Note that the numpy.dot function is used to compute this product, as it ended\\n  up being (Slightly) faster in performance tests than np.tensordot. Using the\\n  reduce function proved slower for both np.dot and np.tensordot.\\n\\n  Args:\\n    player_payoff_tensor: payoff tensor for player[index_avoided], of dimension\\n      (dim(vector[0]), dim(vector[1]), ..., dim(vector[-1])).\\n    strategies: Meta strategy probabilities for each player.\\n    index_avoided: Player for which we do not compute the dot product.\\n\\n  Returns:\\n    Vector of expected returns for each action of player [the player indexed by\\n      index_avoided].\\n  \"\n    new_axis_order = [index_avoided] + [i for i in range(len(strategies)) if i != index_avoided]\n    accumulator = np.transpose(player_payoff_tensor, new_axis_order)\n    for i in range(len(strategies) - 1, -1, -1):\n        if i != index_avoided:\n            accumulator = np.dot(accumulator, strategies[i])\n    return accumulator",
            "def _partial_multi_dot(player_payoff_tensor, strategies, index_avoided):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Computes a generalized dot product avoiding one dimension.\\n\\n  This is used to directly get the expected return of a given action, given\\n  other players' strategies, for the player indexed by index_avoided.\\n  Note that the numpy.dot function is used to compute this product, as it ended\\n  up being (Slightly) faster in performance tests than np.tensordot. Using the\\n  reduce function proved slower for both np.dot and np.tensordot.\\n\\n  Args:\\n    player_payoff_tensor: payoff tensor for player[index_avoided], of dimension\\n      (dim(vector[0]), dim(vector[1]), ..., dim(vector[-1])).\\n    strategies: Meta strategy probabilities for each player.\\n    index_avoided: Player for which we do not compute the dot product.\\n\\n  Returns:\\n    Vector of expected returns for each action of player [the player indexed by\\n      index_avoided].\\n  \"\n    new_axis_order = [index_avoided] + [i for i in range(len(strategies)) if i != index_avoided]\n    accumulator = np.transpose(player_payoff_tensor, new_axis_order)\n    for i in range(len(strategies) - 1, -1, -1):\n        if i != index_avoided:\n            accumulator = np.dot(accumulator, strategies[i])\n    return accumulator",
            "def _partial_multi_dot(player_payoff_tensor, strategies, index_avoided):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Computes a generalized dot product avoiding one dimension.\\n\\n  This is used to directly get the expected return of a given action, given\\n  other players' strategies, for the player indexed by index_avoided.\\n  Note that the numpy.dot function is used to compute this product, as it ended\\n  up being (Slightly) faster in performance tests than np.tensordot. Using the\\n  reduce function proved slower for both np.dot and np.tensordot.\\n\\n  Args:\\n    player_payoff_tensor: payoff tensor for player[index_avoided], of dimension\\n      (dim(vector[0]), dim(vector[1]), ..., dim(vector[-1])).\\n    strategies: Meta strategy probabilities for each player.\\n    index_avoided: Player for which we do not compute the dot product.\\n\\n  Returns:\\n    Vector of expected returns for each action of player [the player indexed by\\n      index_avoided].\\n  \"\n    new_axis_order = [index_avoided] + [i for i in range(len(strategies)) if i != index_avoided]\n    accumulator = np.transpose(player_payoff_tensor, new_axis_order)\n    for i in range(len(strategies) - 1, -1, -1):\n        if i != index_avoided:\n            accumulator = np.dot(accumulator, strategies[i])\n    return accumulator",
            "def _partial_multi_dot(player_payoff_tensor, strategies, index_avoided):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Computes a generalized dot product avoiding one dimension.\\n\\n  This is used to directly get the expected return of a given action, given\\n  other players' strategies, for the player indexed by index_avoided.\\n  Note that the numpy.dot function is used to compute this product, as it ended\\n  up being (Slightly) faster in performance tests than np.tensordot. Using the\\n  reduce function proved slower for both np.dot and np.tensordot.\\n\\n  Args:\\n    player_payoff_tensor: payoff tensor for player[index_avoided], of dimension\\n      (dim(vector[0]), dim(vector[1]), ..., dim(vector[-1])).\\n    strategies: Meta strategy probabilities for each player.\\n    index_avoided: Player for which we do not compute the dot product.\\n\\n  Returns:\\n    Vector of expected returns for each action of player [the player indexed by\\n      index_avoided].\\n  \"\n    new_axis_order = [index_avoided] + [i for i in range(len(strategies)) if i != index_avoided]\n    accumulator = np.transpose(player_payoff_tensor, new_axis_order)\n    for i in range(len(strategies) - 1, -1, -1):\n        if i != index_avoided:\n            accumulator = np.dot(accumulator, strategies[i])\n    return accumulator",
            "def _partial_multi_dot(player_payoff_tensor, strategies, index_avoided):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Computes a generalized dot product avoiding one dimension.\\n\\n  This is used to directly get the expected return of a given action, given\\n  other players' strategies, for the player indexed by index_avoided.\\n  Note that the numpy.dot function is used to compute this product, as it ended\\n  up being (Slightly) faster in performance tests than np.tensordot. Using the\\n  reduce function proved slower for both np.dot and np.tensordot.\\n\\n  Args:\\n    player_payoff_tensor: payoff tensor for player[index_avoided], of dimension\\n      (dim(vector[0]), dim(vector[1]), ..., dim(vector[-1])).\\n    strategies: Meta strategy probabilities for each player.\\n    index_avoided: Player for which we do not compute the dot product.\\n\\n  Returns:\\n    Vector of expected returns for each action of player [the player indexed by\\n      index_avoided].\\n  \"\n    new_axis_order = [index_avoided] + [i for i in range(len(strategies)) if i != index_avoided]\n    accumulator = np.transpose(player_payoff_tensor, new_axis_order)\n    for i in range(len(strategies) - 1, -1, -1):\n        if i != index_avoided:\n            accumulator = np.dot(accumulator, strategies[i])\n    return accumulator"
        ]
    },
    {
        "func_name": "_project_distribution",
        "original": "def _project_distribution(updated_strategy, gamma):\n    \"\"\"Projects the distribution in updated_x to have minimal probabilities.\n\n  Minimal probabilities are set as gamma, and the probabilities are then\n  renormalized to sum to 1.\n\n  Args:\n    updated_strategy: New distribution value after being updated by update rule.\n    gamma: minimal probability value when divided by number of actions.\n\n  Returns:\n    Projected distribution.\n  \"\"\"\n    updated_strategy[updated_strategy < gamma] = gamma\n    updated_strategy = updated_strategy / np.sum(updated_strategy)\n    return updated_strategy",
        "mutated": [
            "def _project_distribution(updated_strategy, gamma):\n    if False:\n        i = 10\n    'Projects the distribution in updated_x to have minimal probabilities.\\n\\n  Minimal probabilities are set as gamma, and the probabilities are then\\n  renormalized to sum to 1.\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution.\\n  '\n    updated_strategy[updated_strategy < gamma] = gamma\n    updated_strategy = updated_strategy / np.sum(updated_strategy)\n    return updated_strategy",
            "def _project_distribution(updated_strategy, gamma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Projects the distribution in updated_x to have minimal probabilities.\\n\\n  Minimal probabilities are set as gamma, and the probabilities are then\\n  renormalized to sum to 1.\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution.\\n  '\n    updated_strategy[updated_strategy < gamma] = gamma\n    updated_strategy = updated_strategy / np.sum(updated_strategy)\n    return updated_strategy",
            "def _project_distribution(updated_strategy, gamma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Projects the distribution in updated_x to have minimal probabilities.\\n\\n  Minimal probabilities are set as gamma, and the probabilities are then\\n  renormalized to sum to 1.\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution.\\n  '\n    updated_strategy[updated_strategy < gamma] = gamma\n    updated_strategy = updated_strategy / np.sum(updated_strategy)\n    return updated_strategy",
            "def _project_distribution(updated_strategy, gamma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Projects the distribution in updated_x to have minimal probabilities.\\n\\n  Minimal probabilities are set as gamma, and the probabilities are then\\n  renormalized to sum to 1.\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution.\\n  '\n    updated_strategy[updated_strategy < gamma] = gamma\n    updated_strategy = updated_strategy / np.sum(updated_strategy)\n    return updated_strategy",
            "def _project_distribution(updated_strategy, gamma):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Projects the distribution in updated_x to have minimal probabilities.\\n\\n  Minimal probabilities are set as gamma, and the probabilities are then\\n  renormalized to sum to 1.\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution.\\n  '\n    updated_strategy[updated_strategy < gamma] = gamma\n    updated_strategy = updated_strategy / np.sum(updated_strategy)\n    return updated_strategy"
        ]
    },
    {
        "func_name": "_approx_simplex_projection",
        "original": "def _approx_simplex_projection(updated_strategy, gamma=0.0):\n    \"\"\"Approximately projects the distribution in updated_x to have minimal probabilities.\n\n  Minimal probabilities are set as gamma, and the probabilities are then\n  renormalized to sum to 1.\n\n  Args:\n    updated_strategy: New distribution value after being updated by update rule.\n    gamma: minimal probability value when divided by number of actions.\n\n  Returns:\n    Projected distribution.\n  \"\"\"\n    updated_strategy[updated_strategy < gamma] = gamma\n    updated_strategy = updated_strategy / np.sum(updated_strategy)\n    return updated_strategy",
        "mutated": [
            "def _approx_simplex_projection(updated_strategy, gamma=0.0):\n    if False:\n        i = 10\n    'Approximately projects the distribution in updated_x to have minimal probabilities.\\n\\n  Minimal probabilities are set as gamma, and the probabilities are then\\n  renormalized to sum to 1.\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution.\\n  '\n    updated_strategy[updated_strategy < gamma] = gamma\n    updated_strategy = updated_strategy / np.sum(updated_strategy)\n    return updated_strategy",
            "def _approx_simplex_projection(updated_strategy, gamma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Approximately projects the distribution in updated_x to have minimal probabilities.\\n\\n  Minimal probabilities are set as gamma, and the probabilities are then\\n  renormalized to sum to 1.\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution.\\n  '\n    updated_strategy[updated_strategy < gamma] = gamma\n    updated_strategy = updated_strategy / np.sum(updated_strategy)\n    return updated_strategy",
            "def _approx_simplex_projection(updated_strategy, gamma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Approximately projects the distribution in updated_x to have minimal probabilities.\\n\\n  Minimal probabilities are set as gamma, and the probabilities are then\\n  renormalized to sum to 1.\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution.\\n  '\n    updated_strategy[updated_strategy < gamma] = gamma\n    updated_strategy = updated_strategy / np.sum(updated_strategy)\n    return updated_strategy",
            "def _approx_simplex_projection(updated_strategy, gamma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Approximately projects the distribution in updated_x to have minimal probabilities.\\n\\n  Minimal probabilities are set as gamma, and the probabilities are then\\n  renormalized to sum to 1.\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution.\\n  '\n    updated_strategy[updated_strategy < gamma] = gamma\n    updated_strategy = updated_strategy / np.sum(updated_strategy)\n    return updated_strategy",
            "def _approx_simplex_projection(updated_strategy, gamma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Approximately projects the distribution in updated_x to have minimal probabilities.\\n\\n  Minimal probabilities are set as gamma, and the probabilities are then\\n  renormalized to sum to 1.\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution.\\n  '\n    updated_strategy[updated_strategy < gamma] = gamma\n    updated_strategy = updated_strategy / np.sum(updated_strategy)\n    return updated_strategy"
        ]
    },
    {
        "func_name": "_simplex_projection",
        "original": "def _simplex_projection(updated_strategy, gamma=0.0):\n    \"\"\"Project updated_strategy on the closest point in L2-norm on gamma-simplex.\n\n  Based on: https://eng.ucmerced.edu/people/wwang5/papers/SimplexProj.pdf\n\n  Args:\n    updated_strategy: New distribution value after being updated by update rule.\n    gamma: minimal probability value when divided by number of actions.\n\n  Returns:\n    Projected distribution\n\n  Algorithm description:\n  It aims to find a scalar lam to be substracted by each dimension of v\n  with the restriction that the resulted quantity should lie in [gamma, 1]\n  until the resulted vector summed up to 1\n  Example: [0.4, 0.7, 0.6], 0.2 -- > find lam=0.25\n            --> [max(0.4-0.25, 0.2), max(0.7-0.25, 0.2), max(0.6-0.25, 0.2)]\n            --> [0.2,  0.45, 0.35]\n  \"\"\"\n    n = len(updated_strategy)\n    idx = np.arange(1, n + 1)\n    u = np.sort(updated_strategy)[::-1]\n    u_tmp = (1 - np.cumsum(u) - (n - idx) * gamma) / idx\n    rho = np.searchsorted(u + u_tmp <= gamma, True)\n    return np.maximum(updated_strategy + u_tmp[rho - 1], gamma)",
        "mutated": [
            "def _simplex_projection(updated_strategy, gamma=0.0):\n    if False:\n        i = 10\n    'Project updated_strategy on the closest point in L2-norm on gamma-simplex.\\n\\n  Based on: https://eng.ucmerced.edu/people/wwang5/papers/SimplexProj.pdf\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution\\n\\n  Algorithm description:\\n  It aims to find a scalar lam to be substracted by each dimension of v\\n  with the restriction that the resulted quantity should lie in [gamma, 1]\\n  until the resulted vector summed up to 1\\n  Example: [0.4, 0.7, 0.6], 0.2 -- > find lam=0.25\\n            --> [max(0.4-0.25, 0.2), max(0.7-0.25, 0.2), max(0.6-0.25, 0.2)]\\n            --> [0.2,  0.45, 0.35]\\n  '\n    n = len(updated_strategy)\n    idx = np.arange(1, n + 1)\n    u = np.sort(updated_strategy)[::-1]\n    u_tmp = (1 - np.cumsum(u) - (n - idx) * gamma) / idx\n    rho = np.searchsorted(u + u_tmp <= gamma, True)\n    return np.maximum(updated_strategy + u_tmp[rho - 1], gamma)",
            "def _simplex_projection(updated_strategy, gamma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Project updated_strategy on the closest point in L2-norm on gamma-simplex.\\n\\n  Based on: https://eng.ucmerced.edu/people/wwang5/papers/SimplexProj.pdf\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution\\n\\n  Algorithm description:\\n  It aims to find a scalar lam to be substracted by each dimension of v\\n  with the restriction that the resulted quantity should lie in [gamma, 1]\\n  until the resulted vector summed up to 1\\n  Example: [0.4, 0.7, 0.6], 0.2 -- > find lam=0.25\\n            --> [max(0.4-0.25, 0.2), max(0.7-0.25, 0.2), max(0.6-0.25, 0.2)]\\n            --> [0.2,  0.45, 0.35]\\n  '\n    n = len(updated_strategy)\n    idx = np.arange(1, n + 1)\n    u = np.sort(updated_strategy)[::-1]\n    u_tmp = (1 - np.cumsum(u) - (n - idx) * gamma) / idx\n    rho = np.searchsorted(u + u_tmp <= gamma, True)\n    return np.maximum(updated_strategy + u_tmp[rho - 1], gamma)",
            "def _simplex_projection(updated_strategy, gamma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Project updated_strategy on the closest point in L2-norm on gamma-simplex.\\n\\n  Based on: https://eng.ucmerced.edu/people/wwang5/papers/SimplexProj.pdf\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution\\n\\n  Algorithm description:\\n  It aims to find a scalar lam to be substracted by each dimension of v\\n  with the restriction that the resulted quantity should lie in [gamma, 1]\\n  until the resulted vector summed up to 1\\n  Example: [0.4, 0.7, 0.6], 0.2 -- > find lam=0.25\\n            --> [max(0.4-0.25, 0.2), max(0.7-0.25, 0.2), max(0.6-0.25, 0.2)]\\n            --> [0.2,  0.45, 0.35]\\n  '\n    n = len(updated_strategy)\n    idx = np.arange(1, n + 1)\n    u = np.sort(updated_strategy)[::-1]\n    u_tmp = (1 - np.cumsum(u) - (n - idx) * gamma) / idx\n    rho = np.searchsorted(u + u_tmp <= gamma, True)\n    return np.maximum(updated_strategy + u_tmp[rho - 1], gamma)",
            "def _simplex_projection(updated_strategy, gamma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Project updated_strategy on the closest point in L2-norm on gamma-simplex.\\n\\n  Based on: https://eng.ucmerced.edu/people/wwang5/papers/SimplexProj.pdf\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution\\n\\n  Algorithm description:\\n  It aims to find a scalar lam to be substracted by each dimension of v\\n  with the restriction that the resulted quantity should lie in [gamma, 1]\\n  until the resulted vector summed up to 1\\n  Example: [0.4, 0.7, 0.6], 0.2 -- > find lam=0.25\\n            --> [max(0.4-0.25, 0.2), max(0.7-0.25, 0.2), max(0.6-0.25, 0.2)]\\n            --> [0.2,  0.45, 0.35]\\n  '\n    n = len(updated_strategy)\n    idx = np.arange(1, n + 1)\n    u = np.sort(updated_strategy)[::-1]\n    u_tmp = (1 - np.cumsum(u) - (n - idx) * gamma) / idx\n    rho = np.searchsorted(u + u_tmp <= gamma, True)\n    return np.maximum(updated_strategy + u_tmp[rho - 1], gamma)",
            "def _simplex_projection(updated_strategy, gamma=0.0):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Project updated_strategy on the closest point in L2-norm on gamma-simplex.\\n\\n  Based on: https://eng.ucmerced.edu/people/wwang5/papers/SimplexProj.pdf\\n\\n  Args:\\n    updated_strategy: New distribution value after being updated by update rule.\\n    gamma: minimal probability value when divided by number of actions.\\n\\n  Returns:\\n    Projected distribution\\n\\n  Algorithm description:\\n  It aims to find a scalar lam to be substracted by each dimension of v\\n  with the restriction that the resulted quantity should lie in [gamma, 1]\\n  until the resulted vector summed up to 1\\n  Example: [0.4, 0.7, 0.6], 0.2 -- > find lam=0.25\\n            --> [max(0.4-0.25, 0.2), max(0.7-0.25, 0.2), max(0.6-0.25, 0.2)]\\n            --> [0.2,  0.45, 0.35]\\n  '\n    n = len(updated_strategy)\n    idx = np.arange(1, n + 1)\n    u = np.sort(updated_strategy)[::-1]\n    u_tmp = (1 - np.cumsum(u) - (n - idx) * gamma) / idx\n    rho = np.searchsorted(u + u_tmp <= gamma, True)\n    return np.maximum(updated_strategy + u_tmp[rho - 1], gamma)"
        ]
    },
    {
        "func_name": "_projected_replicator_dynamics_step",
        "original": "def _projected_replicator_dynamics_step(payoff_tensors, strategies, dt, gamma, use_approx=False):\n    \"\"\"Does one step of the projected replicator dynamics algorithm.\n\n  Args:\n    payoff_tensors: List of payoff tensors for each player.\n    strategies: List of the strategies used by each player.\n    dt: Update amplitude term.\n    gamma: Minimum exploratory probability term.\n    use_approx: use approximate simplex projection.\n\n  Returns:\n    A list of updated strategies for each player.\n  \"\"\"\n    new_strategies = []\n    for player in range(len(payoff_tensors)):\n        current_payoff_tensor = payoff_tensors[player]\n        current_strategy = strategies[player]\n        values_per_strategy = _partial_multi_dot(current_payoff_tensor, strategies, player)\n        average_return = np.dot(values_per_strategy, current_strategy)\n        delta = current_strategy * (values_per_strategy - average_return)\n        updated_strategy = current_strategy + dt * delta\n        updated_strategy = _approx_simplex_projection(updated_strategy, gamma) if use_approx else _simplex_projection(updated_strategy, gamma)\n        new_strategies.append(updated_strategy)\n    return new_strategies",
        "mutated": [
            "def _projected_replicator_dynamics_step(payoff_tensors, strategies, dt, gamma, use_approx=False):\n    if False:\n        i = 10\n    'Does one step of the projected replicator dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    strategies: List of the strategies used by each player.\\n    dt: Update amplitude term.\\n    gamma: Minimum exploratory probability term.\\n    use_approx: use approximate simplex projection.\\n\\n  Returns:\\n    A list of updated strategies for each player.\\n  '\n    new_strategies = []\n    for player in range(len(payoff_tensors)):\n        current_payoff_tensor = payoff_tensors[player]\n        current_strategy = strategies[player]\n        values_per_strategy = _partial_multi_dot(current_payoff_tensor, strategies, player)\n        average_return = np.dot(values_per_strategy, current_strategy)\n        delta = current_strategy * (values_per_strategy - average_return)\n        updated_strategy = current_strategy + dt * delta\n        updated_strategy = _approx_simplex_projection(updated_strategy, gamma) if use_approx else _simplex_projection(updated_strategy, gamma)\n        new_strategies.append(updated_strategy)\n    return new_strategies",
            "def _projected_replicator_dynamics_step(payoff_tensors, strategies, dt, gamma, use_approx=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Does one step of the projected replicator dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    strategies: List of the strategies used by each player.\\n    dt: Update amplitude term.\\n    gamma: Minimum exploratory probability term.\\n    use_approx: use approximate simplex projection.\\n\\n  Returns:\\n    A list of updated strategies for each player.\\n  '\n    new_strategies = []\n    for player in range(len(payoff_tensors)):\n        current_payoff_tensor = payoff_tensors[player]\n        current_strategy = strategies[player]\n        values_per_strategy = _partial_multi_dot(current_payoff_tensor, strategies, player)\n        average_return = np.dot(values_per_strategy, current_strategy)\n        delta = current_strategy * (values_per_strategy - average_return)\n        updated_strategy = current_strategy + dt * delta\n        updated_strategy = _approx_simplex_projection(updated_strategy, gamma) if use_approx else _simplex_projection(updated_strategy, gamma)\n        new_strategies.append(updated_strategy)\n    return new_strategies",
            "def _projected_replicator_dynamics_step(payoff_tensors, strategies, dt, gamma, use_approx=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Does one step of the projected replicator dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    strategies: List of the strategies used by each player.\\n    dt: Update amplitude term.\\n    gamma: Minimum exploratory probability term.\\n    use_approx: use approximate simplex projection.\\n\\n  Returns:\\n    A list of updated strategies for each player.\\n  '\n    new_strategies = []\n    for player in range(len(payoff_tensors)):\n        current_payoff_tensor = payoff_tensors[player]\n        current_strategy = strategies[player]\n        values_per_strategy = _partial_multi_dot(current_payoff_tensor, strategies, player)\n        average_return = np.dot(values_per_strategy, current_strategy)\n        delta = current_strategy * (values_per_strategy - average_return)\n        updated_strategy = current_strategy + dt * delta\n        updated_strategy = _approx_simplex_projection(updated_strategy, gamma) if use_approx else _simplex_projection(updated_strategy, gamma)\n        new_strategies.append(updated_strategy)\n    return new_strategies",
            "def _projected_replicator_dynamics_step(payoff_tensors, strategies, dt, gamma, use_approx=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Does one step of the projected replicator dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    strategies: List of the strategies used by each player.\\n    dt: Update amplitude term.\\n    gamma: Minimum exploratory probability term.\\n    use_approx: use approximate simplex projection.\\n\\n  Returns:\\n    A list of updated strategies for each player.\\n  '\n    new_strategies = []\n    for player in range(len(payoff_tensors)):\n        current_payoff_tensor = payoff_tensors[player]\n        current_strategy = strategies[player]\n        values_per_strategy = _partial_multi_dot(current_payoff_tensor, strategies, player)\n        average_return = np.dot(values_per_strategy, current_strategy)\n        delta = current_strategy * (values_per_strategy - average_return)\n        updated_strategy = current_strategy + dt * delta\n        updated_strategy = _approx_simplex_projection(updated_strategy, gamma) if use_approx else _simplex_projection(updated_strategy, gamma)\n        new_strategies.append(updated_strategy)\n    return new_strategies",
            "def _projected_replicator_dynamics_step(payoff_tensors, strategies, dt, gamma, use_approx=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Does one step of the projected replicator dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    strategies: List of the strategies used by each player.\\n    dt: Update amplitude term.\\n    gamma: Minimum exploratory probability term.\\n    use_approx: use approximate simplex projection.\\n\\n  Returns:\\n    A list of updated strategies for each player.\\n  '\n    new_strategies = []\n    for player in range(len(payoff_tensors)):\n        current_payoff_tensor = payoff_tensors[player]\n        current_strategy = strategies[player]\n        values_per_strategy = _partial_multi_dot(current_payoff_tensor, strategies, player)\n        average_return = np.dot(values_per_strategy, current_strategy)\n        delta = current_strategy * (values_per_strategy - average_return)\n        updated_strategy = current_strategy + dt * delta\n        updated_strategy = _approx_simplex_projection(updated_strategy, gamma) if use_approx else _simplex_projection(updated_strategy, gamma)\n        new_strategies.append(updated_strategy)\n    return new_strategies"
        ]
    },
    {
        "func_name": "projected_replicator_dynamics",
        "original": "def projected_replicator_dynamics(payoff_tensors, prd_initial_strategies=None, prd_iterations=int(100000.0), prd_dt=0.001, prd_gamma=1e-06, average_over_last_n_strategies=None, use_approx=False, **unused_kwargs):\n    \"\"\"The Projected Replicator Dynamics algorithm.\n\n  Args:\n    payoff_tensors: List of payoff tensors for each player.\n    prd_initial_strategies: Initial list of the strategies used by each player,\n      if any. Could be used to speed up the search by providing a good initial\n      solution.\n    prd_iterations: Number of algorithmic steps to take before returning an\n      answer.\n    prd_dt: Update amplitude term.\n    prd_gamma: Minimum exploratory probability term.\n    average_over_last_n_strategies: Running average window size for average\n      policy computation. If None, use the whole trajectory.\n    use_approx: use the approximate simplex projection.\n    **unused_kwargs: Convenient way of exposing an API compatible with other\n      methods with possibly different arguments.\n\n  Returns:\n    PRD-computed strategies.\n  \"\"\"\n    number_players = len(payoff_tensors)\n    action_space_shapes = payoff_tensors[0].shape\n    new_strategies = prd_initial_strategies or [np.ones(action_space_shapes[k]) / action_space_shapes[k] for k in range(number_players)]\n    averager = nfg_utils.StrategyAverager(number_players, action_space_shapes, average_over_last_n_strategies)\n    averager.append(new_strategies)\n    for _ in range(prd_iterations):\n        new_strategies = _projected_replicator_dynamics_step(payoff_tensors, new_strategies, prd_dt, prd_gamma, use_approx)\n        averager.append(new_strategies)\n    return averager.average_strategies()",
        "mutated": [
            "def projected_replicator_dynamics(payoff_tensors, prd_initial_strategies=None, prd_iterations=int(100000.0), prd_dt=0.001, prd_gamma=1e-06, average_over_last_n_strategies=None, use_approx=False, **unused_kwargs):\n    if False:\n        i = 10\n    'The Projected Replicator Dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    prd_initial_strategies: Initial list of the strategies used by each player,\\n      if any. Could be used to speed up the search by providing a good initial\\n      solution.\\n    prd_iterations: Number of algorithmic steps to take before returning an\\n      answer.\\n    prd_dt: Update amplitude term.\\n    prd_gamma: Minimum exploratory probability term.\\n    average_over_last_n_strategies: Running average window size for average\\n      policy computation. If None, use the whole trajectory.\\n    use_approx: use the approximate simplex projection.\\n    **unused_kwargs: Convenient way of exposing an API compatible with other\\n      methods with possibly different arguments.\\n\\n  Returns:\\n    PRD-computed strategies.\\n  '\n    number_players = len(payoff_tensors)\n    action_space_shapes = payoff_tensors[0].shape\n    new_strategies = prd_initial_strategies or [np.ones(action_space_shapes[k]) / action_space_shapes[k] for k in range(number_players)]\n    averager = nfg_utils.StrategyAverager(number_players, action_space_shapes, average_over_last_n_strategies)\n    averager.append(new_strategies)\n    for _ in range(prd_iterations):\n        new_strategies = _projected_replicator_dynamics_step(payoff_tensors, new_strategies, prd_dt, prd_gamma, use_approx)\n        averager.append(new_strategies)\n    return averager.average_strategies()",
            "def projected_replicator_dynamics(payoff_tensors, prd_initial_strategies=None, prd_iterations=int(100000.0), prd_dt=0.001, prd_gamma=1e-06, average_over_last_n_strategies=None, use_approx=False, **unused_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'The Projected Replicator Dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    prd_initial_strategies: Initial list of the strategies used by each player,\\n      if any. Could be used to speed up the search by providing a good initial\\n      solution.\\n    prd_iterations: Number of algorithmic steps to take before returning an\\n      answer.\\n    prd_dt: Update amplitude term.\\n    prd_gamma: Minimum exploratory probability term.\\n    average_over_last_n_strategies: Running average window size for average\\n      policy computation. If None, use the whole trajectory.\\n    use_approx: use the approximate simplex projection.\\n    **unused_kwargs: Convenient way of exposing an API compatible with other\\n      methods with possibly different arguments.\\n\\n  Returns:\\n    PRD-computed strategies.\\n  '\n    number_players = len(payoff_tensors)\n    action_space_shapes = payoff_tensors[0].shape\n    new_strategies = prd_initial_strategies or [np.ones(action_space_shapes[k]) / action_space_shapes[k] for k in range(number_players)]\n    averager = nfg_utils.StrategyAverager(number_players, action_space_shapes, average_over_last_n_strategies)\n    averager.append(new_strategies)\n    for _ in range(prd_iterations):\n        new_strategies = _projected_replicator_dynamics_step(payoff_tensors, new_strategies, prd_dt, prd_gamma, use_approx)\n        averager.append(new_strategies)\n    return averager.average_strategies()",
            "def projected_replicator_dynamics(payoff_tensors, prd_initial_strategies=None, prd_iterations=int(100000.0), prd_dt=0.001, prd_gamma=1e-06, average_over_last_n_strategies=None, use_approx=False, **unused_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'The Projected Replicator Dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    prd_initial_strategies: Initial list of the strategies used by each player,\\n      if any. Could be used to speed up the search by providing a good initial\\n      solution.\\n    prd_iterations: Number of algorithmic steps to take before returning an\\n      answer.\\n    prd_dt: Update amplitude term.\\n    prd_gamma: Minimum exploratory probability term.\\n    average_over_last_n_strategies: Running average window size for average\\n      policy computation. If None, use the whole trajectory.\\n    use_approx: use the approximate simplex projection.\\n    **unused_kwargs: Convenient way of exposing an API compatible with other\\n      methods with possibly different arguments.\\n\\n  Returns:\\n    PRD-computed strategies.\\n  '\n    number_players = len(payoff_tensors)\n    action_space_shapes = payoff_tensors[0].shape\n    new_strategies = prd_initial_strategies or [np.ones(action_space_shapes[k]) / action_space_shapes[k] for k in range(number_players)]\n    averager = nfg_utils.StrategyAverager(number_players, action_space_shapes, average_over_last_n_strategies)\n    averager.append(new_strategies)\n    for _ in range(prd_iterations):\n        new_strategies = _projected_replicator_dynamics_step(payoff_tensors, new_strategies, prd_dt, prd_gamma, use_approx)\n        averager.append(new_strategies)\n    return averager.average_strategies()",
            "def projected_replicator_dynamics(payoff_tensors, prd_initial_strategies=None, prd_iterations=int(100000.0), prd_dt=0.001, prd_gamma=1e-06, average_over_last_n_strategies=None, use_approx=False, **unused_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'The Projected Replicator Dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    prd_initial_strategies: Initial list of the strategies used by each player,\\n      if any. Could be used to speed up the search by providing a good initial\\n      solution.\\n    prd_iterations: Number of algorithmic steps to take before returning an\\n      answer.\\n    prd_dt: Update amplitude term.\\n    prd_gamma: Minimum exploratory probability term.\\n    average_over_last_n_strategies: Running average window size for average\\n      policy computation. If None, use the whole trajectory.\\n    use_approx: use the approximate simplex projection.\\n    **unused_kwargs: Convenient way of exposing an API compatible with other\\n      methods with possibly different arguments.\\n\\n  Returns:\\n    PRD-computed strategies.\\n  '\n    number_players = len(payoff_tensors)\n    action_space_shapes = payoff_tensors[0].shape\n    new_strategies = prd_initial_strategies or [np.ones(action_space_shapes[k]) / action_space_shapes[k] for k in range(number_players)]\n    averager = nfg_utils.StrategyAverager(number_players, action_space_shapes, average_over_last_n_strategies)\n    averager.append(new_strategies)\n    for _ in range(prd_iterations):\n        new_strategies = _projected_replicator_dynamics_step(payoff_tensors, new_strategies, prd_dt, prd_gamma, use_approx)\n        averager.append(new_strategies)\n    return averager.average_strategies()",
            "def projected_replicator_dynamics(payoff_tensors, prd_initial_strategies=None, prd_iterations=int(100000.0), prd_dt=0.001, prd_gamma=1e-06, average_over_last_n_strategies=None, use_approx=False, **unused_kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'The Projected Replicator Dynamics algorithm.\\n\\n  Args:\\n    payoff_tensors: List of payoff tensors for each player.\\n    prd_initial_strategies: Initial list of the strategies used by each player,\\n      if any. Could be used to speed up the search by providing a good initial\\n      solution.\\n    prd_iterations: Number of algorithmic steps to take before returning an\\n      answer.\\n    prd_dt: Update amplitude term.\\n    prd_gamma: Minimum exploratory probability term.\\n    average_over_last_n_strategies: Running average window size for average\\n      policy computation. If None, use the whole trajectory.\\n    use_approx: use the approximate simplex projection.\\n    **unused_kwargs: Convenient way of exposing an API compatible with other\\n      methods with possibly different arguments.\\n\\n  Returns:\\n    PRD-computed strategies.\\n  '\n    number_players = len(payoff_tensors)\n    action_space_shapes = payoff_tensors[0].shape\n    new_strategies = prd_initial_strategies or [np.ones(action_space_shapes[k]) / action_space_shapes[k] for k in range(number_players)]\n    averager = nfg_utils.StrategyAverager(number_players, action_space_shapes, average_over_last_n_strategies)\n    averager.append(new_strategies)\n    for _ in range(prd_iterations):\n        new_strategies = _projected_replicator_dynamics_step(payoff_tensors, new_strategies, prd_dt, prd_gamma, use_approx)\n        averager.append(new_strategies)\n    return averager.average_strategies()"
        ]
    }
]