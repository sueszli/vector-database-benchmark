[
    {
        "func_name": "init_pg_responder",
        "original": "def init_pg_responder(sess, env):\n    \"\"\"Initializes the Policy Gradient-based responder and agents.\"\"\"\n    info_state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    agent_class = rl_policy.PGPolicy\n    agent_kwargs = {'session': sess, 'info_state_size': info_state_size, 'num_actions': num_actions, 'loss_str': FLAGS.loss_str, 'loss_class': False, 'hidden_layers_sizes': [FLAGS.hidden_layer_size] * FLAGS.n_hidden_layers, 'batch_size': FLAGS.batch_size, 'entropy_cost': FLAGS.entropy_cost, 'critic_learning_rate': FLAGS.critic_learning_rate, 'pi_learning_rate': FLAGS.pi_learning_rate, 'num_critic_before_pi': FLAGS.num_q_before_pi, 'optimizer_str': FLAGS.optimizer_str}\n    oracle = rl_oracle.RLOracle(env, agent_class, agent_kwargs, number_training_episodes=FLAGS.number_training_episodes, self_play_proportion=FLAGS.self_play_proportion, sigma=FLAGS.sigma)\n    agents = [agent_class(env, player_id, **agent_kwargs) for player_id in range(FLAGS.n_players)]\n    for agent in agents:\n        agent.freeze()\n    return (oracle, agents)",
        "mutated": [
            "def init_pg_responder(sess, env):\n    if False:\n        i = 10\n    'Initializes the Policy Gradient-based responder and agents.'\n    info_state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    agent_class = rl_policy.PGPolicy\n    agent_kwargs = {'session': sess, 'info_state_size': info_state_size, 'num_actions': num_actions, 'loss_str': FLAGS.loss_str, 'loss_class': False, 'hidden_layers_sizes': [FLAGS.hidden_layer_size] * FLAGS.n_hidden_layers, 'batch_size': FLAGS.batch_size, 'entropy_cost': FLAGS.entropy_cost, 'critic_learning_rate': FLAGS.critic_learning_rate, 'pi_learning_rate': FLAGS.pi_learning_rate, 'num_critic_before_pi': FLAGS.num_q_before_pi, 'optimizer_str': FLAGS.optimizer_str}\n    oracle = rl_oracle.RLOracle(env, agent_class, agent_kwargs, number_training_episodes=FLAGS.number_training_episodes, self_play_proportion=FLAGS.self_play_proportion, sigma=FLAGS.sigma)\n    agents = [agent_class(env, player_id, **agent_kwargs) for player_id in range(FLAGS.n_players)]\n    for agent in agents:\n        agent.freeze()\n    return (oracle, agents)",
            "def init_pg_responder(sess, env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initializes the Policy Gradient-based responder and agents.'\n    info_state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    agent_class = rl_policy.PGPolicy\n    agent_kwargs = {'session': sess, 'info_state_size': info_state_size, 'num_actions': num_actions, 'loss_str': FLAGS.loss_str, 'loss_class': False, 'hidden_layers_sizes': [FLAGS.hidden_layer_size] * FLAGS.n_hidden_layers, 'batch_size': FLAGS.batch_size, 'entropy_cost': FLAGS.entropy_cost, 'critic_learning_rate': FLAGS.critic_learning_rate, 'pi_learning_rate': FLAGS.pi_learning_rate, 'num_critic_before_pi': FLAGS.num_q_before_pi, 'optimizer_str': FLAGS.optimizer_str}\n    oracle = rl_oracle.RLOracle(env, agent_class, agent_kwargs, number_training_episodes=FLAGS.number_training_episodes, self_play_proportion=FLAGS.self_play_proportion, sigma=FLAGS.sigma)\n    agents = [agent_class(env, player_id, **agent_kwargs) for player_id in range(FLAGS.n_players)]\n    for agent in agents:\n        agent.freeze()\n    return (oracle, agents)",
            "def init_pg_responder(sess, env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initializes the Policy Gradient-based responder and agents.'\n    info_state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    agent_class = rl_policy.PGPolicy\n    agent_kwargs = {'session': sess, 'info_state_size': info_state_size, 'num_actions': num_actions, 'loss_str': FLAGS.loss_str, 'loss_class': False, 'hidden_layers_sizes': [FLAGS.hidden_layer_size] * FLAGS.n_hidden_layers, 'batch_size': FLAGS.batch_size, 'entropy_cost': FLAGS.entropy_cost, 'critic_learning_rate': FLAGS.critic_learning_rate, 'pi_learning_rate': FLAGS.pi_learning_rate, 'num_critic_before_pi': FLAGS.num_q_before_pi, 'optimizer_str': FLAGS.optimizer_str}\n    oracle = rl_oracle.RLOracle(env, agent_class, agent_kwargs, number_training_episodes=FLAGS.number_training_episodes, self_play_proportion=FLAGS.self_play_proportion, sigma=FLAGS.sigma)\n    agents = [agent_class(env, player_id, **agent_kwargs) for player_id in range(FLAGS.n_players)]\n    for agent in agents:\n        agent.freeze()\n    return (oracle, agents)",
            "def init_pg_responder(sess, env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initializes the Policy Gradient-based responder and agents.'\n    info_state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    agent_class = rl_policy.PGPolicy\n    agent_kwargs = {'session': sess, 'info_state_size': info_state_size, 'num_actions': num_actions, 'loss_str': FLAGS.loss_str, 'loss_class': False, 'hidden_layers_sizes': [FLAGS.hidden_layer_size] * FLAGS.n_hidden_layers, 'batch_size': FLAGS.batch_size, 'entropy_cost': FLAGS.entropy_cost, 'critic_learning_rate': FLAGS.critic_learning_rate, 'pi_learning_rate': FLAGS.pi_learning_rate, 'num_critic_before_pi': FLAGS.num_q_before_pi, 'optimizer_str': FLAGS.optimizer_str}\n    oracle = rl_oracle.RLOracle(env, agent_class, agent_kwargs, number_training_episodes=FLAGS.number_training_episodes, self_play_proportion=FLAGS.self_play_proportion, sigma=FLAGS.sigma)\n    agents = [agent_class(env, player_id, **agent_kwargs) for player_id in range(FLAGS.n_players)]\n    for agent in agents:\n        agent.freeze()\n    return (oracle, agents)",
            "def init_pg_responder(sess, env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initializes the Policy Gradient-based responder and agents.'\n    info_state_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    agent_class = rl_policy.PGPolicy\n    agent_kwargs = {'session': sess, 'info_state_size': info_state_size, 'num_actions': num_actions, 'loss_str': FLAGS.loss_str, 'loss_class': False, 'hidden_layers_sizes': [FLAGS.hidden_layer_size] * FLAGS.n_hidden_layers, 'batch_size': FLAGS.batch_size, 'entropy_cost': FLAGS.entropy_cost, 'critic_learning_rate': FLAGS.critic_learning_rate, 'pi_learning_rate': FLAGS.pi_learning_rate, 'num_critic_before_pi': FLAGS.num_q_before_pi, 'optimizer_str': FLAGS.optimizer_str}\n    oracle = rl_oracle.RLOracle(env, agent_class, agent_kwargs, number_training_episodes=FLAGS.number_training_episodes, self_play_proportion=FLAGS.self_play_proportion, sigma=FLAGS.sigma)\n    agents = [agent_class(env, player_id, **agent_kwargs) for player_id in range(FLAGS.n_players)]\n    for agent in agents:\n        agent.freeze()\n    return (oracle, agents)"
        ]
    },
    {
        "func_name": "init_br_responder",
        "original": "def init_br_responder(env):\n    \"\"\"Initializes the tabular best-response based responder and agents.\"\"\"\n    random_policy = policy.TabularPolicy(env.game)\n    oracle = best_response_oracle.BestResponseOracle(game=env.game, policy=random_policy)\n    agents = [random_policy.__copy__() for _ in range(FLAGS.n_players)]\n    return (oracle, agents)",
        "mutated": [
            "def init_br_responder(env):\n    if False:\n        i = 10\n    'Initializes the tabular best-response based responder and agents.'\n    random_policy = policy.TabularPolicy(env.game)\n    oracle = best_response_oracle.BestResponseOracle(game=env.game, policy=random_policy)\n    agents = [random_policy.__copy__() for _ in range(FLAGS.n_players)]\n    return (oracle, agents)",
            "def init_br_responder(env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initializes the tabular best-response based responder and agents.'\n    random_policy = policy.TabularPolicy(env.game)\n    oracle = best_response_oracle.BestResponseOracle(game=env.game, policy=random_policy)\n    agents = [random_policy.__copy__() for _ in range(FLAGS.n_players)]\n    return (oracle, agents)",
            "def init_br_responder(env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initializes the tabular best-response based responder and agents.'\n    random_policy = policy.TabularPolicy(env.game)\n    oracle = best_response_oracle.BestResponseOracle(game=env.game, policy=random_policy)\n    agents = [random_policy.__copy__() for _ in range(FLAGS.n_players)]\n    return (oracle, agents)",
            "def init_br_responder(env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initializes the tabular best-response based responder and agents.'\n    random_policy = policy.TabularPolicy(env.game)\n    oracle = best_response_oracle.BestResponseOracle(game=env.game, policy=random_policy)\n    agents = [random_policy.__copy__() for _ in range(FLAGS.n_players)]\n    return (oracle, agents)",
            "def init_br_responder(env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initializes the tabular best-response based responder and agents.'\n    random_policy = policy.TabularPolicy(env.game)\n    oracle = best_response_oracle.BestResponseOracle(game=env.game, policy=random_policy)\n    agents = [random_policy.__copy__() for _ in range(FLAGS.n_players)]\n    return (oracle, agents)"
        ]
    },
    {
        "func_name": "init_dqn_responder",
        "original": "def init_dqn_responder(sess, env):\n    \"\"\"Initializes the Policy Gradient-based responder and agents.\"\"\"\n    state_representation_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    agent_class = rl_policy.DQNPolicy\n    agent_kwargs = {'session': sess, 'state_representation_size': state_representation_size, 'num_actions': num_actions, 'hidden_layers_sizes': [FLAGS.hidden_layer_size] * FLAGS.n_hidden_layers, 'batch_size': FLAGS.batch_size, 'learning_rate': FLAGS.dqn_learning_rate, 'update_target_network_every': FLAGS.update_target_network_every, 'learn_every': FLAGS.learn_every, 'optimizer_str': FLAGS.optimizer_str}\n    oracle = rl_oracle.RLOracle(env, agent_class, agent_kwargs, number_training_episodes=FLAGS.number_training_episodes, self_play_proportion=FLAGS.self_play_proportion, sigma=FLAGS.sigma)\n    agents = [agent_class(env, player_id, **agent_kwargs) for player_id in range(FLAGS.n_players)]\n    for agent in agents:\n        agent.freeze()\n    return (oracle, agents)",
        "mutated": [
            "def init_dqn_responder(sess, env):\n    if False:\n        i = 10\n    'Initializes the Policy Gradient-based responder and agents.'\n    state_representation_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    agent_class = rl_policy.DQNPolicy\n    agent_kwargs = {'session': sess, 'state_representation_size': state_representation_size, 'num_actions': num_actions, 'hidden_layers_sizes': [FLAGS.hidden_layer_size] * FLAGS.n_hidden_layers, 'batch_size': FLAGS.batch_size, 'learning_rate': FLAGS.dqn_learning_rate, 'update_target_network_every': FLAGS.update_target_network_every, 'learn_every': FLAGS.learn_every, 'optimizer_str': FLAGS.optimizer_str}\n    oracle = rl_oracle.RLOracle(env, agent_class, agent_kwargs, number_training_episodes=FLAGS.number_training_episodes, self_play_proportion=FLAGS.self_play_proportion, sigma=FLAGS.sigma)\n    agents = [agent_class(env, player_id, **agent_kwargs) for player_id in range(FLAGS.n_players)]\n    for agent in agents:\n        agent.freeze()\n    return (oracle, agents)",
            "def init_dqn_responder(sess, env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initializes the Policy Gradient-based responder and agents.'\n    state_representation_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    agent_class = rl_policy.DQNPolicy\n    agent_kwargs = {'session': sess, 'state_representation_size': state_representation_size, 'num_actions': num_actions, 'hidden_layers_sizes': [FLAGS.hidden_layer_size] * FLAGS.n_hidden_layers, 'batch_size': FLAGS.batch_size, 'learning_rate': FLAGS.dqn_learning_rate, 'update_target_network_every': FLAGS.update_target_network_every, 'learn_every': FLAGS.learn_every, 'optimizer_str': FLAGS.optimizer_str}\n    oracle = rl_oracle.RLOracle(env, agent_class, agent_kwargs, number_training_episodes=FLAGS.number_training_episodes, self_play_proportion=FLAGS.self_play_proportion, sigma=FLAGS.sigma)\n    agents = [agent_class(env, player_id, **agent_kwargs) for player_id in range(FLAGS.n_players)]\n    for agent in agents:\n        agent.freeze()\n    return (oracle, agents)",
            "def init_dqn_responder(sess, env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initializes the Policy Gradient-based responder and agents.'\n    state_representation_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    agent_class = rl_policy.DQNPolicy\n    agent_kwargs = {'session': sess, 'state_representation_size': state_representation_size, 'num_actions': num_actions, 'hidden_layers_sizes': [FLAGS.hidden_layer_size] * FLAGS.n_hidden_layers, 'batch_size': FLAGS.batch_size, 'learning_rate': FLAGS.dqn_learning_rate, 'update_target_network_every': FLAGS.update_target_network_every, 'learn_every': FLAGS.learn_every, 'optimizer_str': FLAGS.optimizer_str}\n    oracle = rl_oracle.RLOracle(env, agent_class, agent_kwargs, number_training_episodes=FLAGS.number_training_episodes, self_play_proportion=FLAGS.self_play_proportion, sigma=FLAGS.sigma)\n    agents = [agent_class(env, player_id, **agent_kwargs) for player_id in range(FLAGS.n_players)]\n    for agent in agents:\n        agent.freeze()\n    return (oracle, agents)",
            "def init_dqn_responder(sess, env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initializes the Policy Gradient-based responder and agents.'\n    state_representation_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    agent_class = rl_policy.DQNPolicy\n    agent_kwargs = {'session': sess, 'state_representation_size': state_representation_size, 'num_actions': num_actions, 'hidden_layers_sizes': [FLAGS.hidden_layer_size] * FLAGS.n_hidden_layers, 'batch_size': FLAGS.batch_size, 'learning_rate': FLAGS.dqn_learning_rate, 'update_target_network_every': FLAGS.update_target_network_every, 'learn_every': FLAGS.learn_every, 'optimizer_str': FLAGS.optimizer_str}\n    oracle = rl_oracle.RLOracle(env, agent_class, agent_kwargs, number_training_episodes=FLAGS.number_training_episodes, self_play_proportion=FLAGS.self_play_proportion, sigma=FLAGS.sigma)\n    agents = [agent_class(env, player_id, **agent_kwargs) for player_id in range(FLAGS.n_players)]\n    for agent in agents:\n        agent.freeze()\n    return (oracle, agents)",
            "def init_dqn_responder(sess, env):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initializes the Policy Gradient-based responder and agents.'\n    state_representation_size = env.observation_spec()['info_state'][0]\n    num_actions = env.action_spec()['num_actions']\n    agent_class = rl_policy.DQNPolicy\n    agent_kwargs = {'session': sess, 'state_representation_size': state_representation_size, 'num_actions': num_actions, 'hidden_layers_sizes': [FLAGS.hidden_layer_size] * FLAGS.n_hidden_layers, 'batch_size': FLAGS.batch_size, 'learning_rate': FLAGS.dqn_learning_rate, 'update_target_network_every': FLAGS.update_target_network_every, 'learn_every': FLAGS.learn_every, 'optimizer_str': FLAGS.optimizer_str}\n    oracle = rl_oracle.RLOracle(env, agent_class, agent_kwargs, number_training_episodes=FLAGS.number_training_episodes, self_play_proportion=FLAGS.self_play_proportion, sigma=FLAGS.sigma)\n    agents = [agent_class(env, player_id, **agent_kwargs) for player_id in range(FLAGS.n_players)]\n    for agent in agents:\n        agent.freeze()\n    return (oracle, agents)"
        ]
    },
    {
        "func_name": "print_policy_analysis",
        "original": "def print_policy_analysis(policies, game, verbose=False):\n    \"\"\"Function printing policy diversity within game's known policies.\n\n  Warning : only works with deterministic policies.\n  Args:\n    policies: List of list of policies (One list per game player)\n    game: OpenSpiel game object.\n    verbose: Whether to print policy diversity information. (True : print)\n\n  Returns:\n    List of list of unique policies (One list per player)\n  \"\"\"\n    states_dict = get_all_states.get_all_states(game, np.infty, False, False)\n    unique_policies = []\n    for player in range(len(policies)):\n        cur_policies = policies[player]\n        cur_set = set()\n        for pol in cur_policies:\n            cur_str = ''\n            for state_str in states_dict:\n                if states_dict[state_str].current_player() == player:\n                    pol_action_dict = pol(states_dict[state_str])\n                    max_prob = max(list(pol_action_dict.values()))\n                    max_prob_actions = [a for a in pol_action_dict if pol_action_dict[a] == max_prob]\n                    cur_str += '__' + state_str\n                    for a in max_prob_actions:\n                        cur_str += '-' + str(a)\n            cur_set.add(cur_str)\n        unique_policies.append(cur_set)\n    if verbose:\n        print('\\n=====================================\\nPolicy Diversity :')\n        for (player, cur_set) in enumerate(unique_policies):\n            print('Player {} : {} unique policies.'.format(player, len(cur_set)))\n    print('')\n    return unique_policies",
        "mutated": [
            "def print_policy_analysis(policies, game, verbose=False):\n    if False:\n        i = 10\n    \"Function printing policy diversity within game's known policies.\\n\\n  Warning : only works with deterministic policies.\\n  Args:\\n    policies: List of list of policies (One list per game player)\\n    game: OpenSpiel game object.\\n    verbose: Whether to print policy diversity information. (True : print)\\n\\n  Returns:\\n    List of list of unique policies (One list per player)\\n  \"\n    states_dict = get_all_states.get_all_states(game, np.infty, False, False)\n    unique_policies = []\n    for player in range(len(policies)):\n        cur_policies = policies[player]\n        cur_set = set()\n        for pol in cur_policies:\n            cur_str = ''\n            for state_str in states_dict:\n                if states_dict[state_str].current_player() == player:\n                    pol_action_dict = pol(states_dict[state_str])\n                    max_prob = max(list(pol_action_dict.values()))\n                    max_prob_actions = [a for a in pol_action_dict if pol_action_dict[a] == max_prob]\n                    cur_str += '__' + state_str\n                    for a in max_prob_actions:\n                        cur_str += '-' + str(a)\n            cur_set.add(cur_str)\n        unique_policies.append(cur_set)\n    if verbose:\n        print('\\n=====================================\\nPolicy Diversity :')\n        for (player, cur_set) in enumerate(unique_policies):\n            print('Player {} : {} unique policies.'.format(player, len(cur_set)))\n    print('')\n    return unique_policies",
            "def print_policy_analysis(policies, game, verbose=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Function printing policy diversity within game's known policies.\\n\\n  Warning : only works with deterministic policies.\\n  Args:\\n    policies: List of list of policies (One list per game player)\\n    game: OpenSpiel game object.\\n    verbose: Whether to print policy diversity information. (True : print)\\n\\n  Returns:\\n    List of list of unique policies (One list per player)\\n  \"\n    states_dict = get_all_states.get_all_states(game, np.infty, False, False)\n    unique_policies = []\n    for player in range(len(policies)):\n        cur_policies = policies[player]\n        cur_set = set()\n        for pol in cur_policies:\n            cur_str = ''\n            for state_str in states_dict:\n                if states_dict[state_str].current_player() == player:\n                    pol_action_dict = pol(states_dict[state_str])\n                    max_prob = max(list(pol_action_dict.values()))\n                    max_prob_actions = [a for a in pol_action_dict if pol_action_dict[a] == max_prob]\n                    cur_str += '__' + state_str\n                    for a in max_prob_actions:\n                        cur_str += '-' + str(a)\n            cur_set.add(cur_str)\n        unique_policies.append(cur_set)\n    if verbose:\n        print('\\n=====================================\\nPolicy Diversity :')\n        for (player, cur_set) in enumerate(unique_policies):\n            print('Player {} : {} unique policies.'.format(player, len(cur_set)))\n    print('')\n    return unique_policies",
            "def print_policy_analysis(policies, game, verbose=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Function printing policy diversity within game's known policies.\\n\\n  Warning : only works with deterministic policies.\\n  Args:\\n    policies: List of list of policies (One list per game player)\\n    game: OpenSpiel game object.\\n    verbose: Whether to print policy diversity information. (True : print)\\n\\n  Returns:\\n    List of list of unique policies (One list per player)\\n  \"\n    states_dict = get_all_states.get_all_states(game, np.infty, False, False)\n    unique_policies = []\n    for player in range(len(policies)):\n        cur_policies = policies[player]\n        cur_set = set()\n        for pol in cur_policies:\n            cur_str = ''\n            for state_str in states_dict:\n                if states_dict[state_str].current_player() == player:\n                    pol_action_dict = pol(states_dict[state_str])\n                    max_prob = max(list(pol_action_dict.values()))\n                    max_prob_actions = [a for a in pol_action_dict if pol_action_dict[a] == max_prob]\n                    cur_str += '__' + state_str\n                    for a in max_prob_actions:\n                        cur_str += '-' + str(a)\n            cur_set.add(cur_str)\n        unique_policies.append(cur_set)\n    if verbose:\n        print('\\n=====================================\\nPolicy Diversity :')\n        for (player, cur_set) in enumerate(unique_policies):\n            print('Player {} : {} unique policies.'.format(player, len(cur_set)))\n    print('')\n    return unique_policies",
            "def print_policy_analysis(policies, game, verbose=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Function printing policy diversity within game's known policies.\\n\\n  Warning : only works with deterministic policies.\\n  Args:\\n    policies: List of list of policies (One list per game player)\\n    game: OpenSpiel game object.\\n    verbose: Whether to print policy diversity information. (True : print)\\n\\n  Returns:\\n    List of list of unique policies (One list per player)\\n  \"\n    states_dict = get_all_states.get_all_states(game, np.infty, False, False)\n    unique_policies = []\n    for player in range(len(policies)):\n        cur_policies = policies[player]\n        cur_set = set()\n        for pol in cur_policies:\n            cur_str = ''\n            for state_str in states_dict:\n                if states_dict[state_str].current_player() == player:\n                    pol_action_dict = pol(states_dict[state_str])\n                    max_prob = max(list(pol_action_dict.values()))\n                    max_prob_actions = [a for a in pol_action_dict if pol_action_dict[a] == max_prob]\n                    cur_str += '__' + state_str\n                    for a in max_prob_actions:\n                        cur_str += '-' + str(a)\n            cur_set.add(cur_str)\n        unique_policies.append(cur_set)\n    if verbose:\n        print('\\n=====================================\\nPolicy Diversity :')\n        for (player, cur_set) in enumerate(unique_policies):\n            print('Player {} : {} unique policies.'.format(player, len(cur_set)))\n    print('')\n    return unique_policies",
            "def print_policy_analysis(policies, game, verbose=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Function printing policy diversity within game's known policies.\\n\\n  Warning : only works with deterministic policies.\\n  Args:\\n    policies: List of list of policies (One list per game player)\\n    game: OpenSpiel game object.\\n    verbose: Whether to print policy diversity information. (True : print)\\n\\n  Returns:\\n    List of list of unique policies (One list per player)\\n  \"\n    states_dict = get_all_states.get_all_states(game, np.infty, False, False)\n    unique_policies = []\n    for player in range(len(policies)):\n        cur_policies = policies[player]\n        cur_set = set()\n        for pol in cur_policies:\n            cur_str = ''\n            for state_str in states_dict:\n                if states_dict[state_str].current_player() == player:\n                    pol_action_dict = pol(states_dict[state_str])\n                    max_prob = max(list(pol_action_dict.values()))\n                    max_prob_actions = [a for a in pol_action_dict if pol_action_dict[a] == max_prob]\n                    cur_str += '__' + state_str\n                    for a in max_prob_actions:\n                        cur_str += '-' + str(a)\n            cur_set.add(cur_str)\n        unique_policies.append(cur_set)\n    if verbose:\n        print('\\n=====================================\\nPolicy Diversity :')\n        for (player, cur_set) in enumerate(unique_policies):\n            print('Player {} : {} unique policies.'.format(player, len(cur_set)))\n    print('')\n    return unique_policies"
        ]
    },
    {
        "func_name": "gpsro_looper",
        "original": "def gpsro_looper(env, oracle, agents):\n    \"\"\"Initializes and executes the GPSRO training loop.\"\"\"\n    sample_from_marginals = True\n    training_strategy_selector = FLAGS.training_strategy_selector or strategy_selectors.probabilistic\n    g_psro_solver = psro_v2.PSROSolver(env.game, oracle, initial_policies=agents, training_strategy_selector=training_strategy_selector, rectifier=FLAGS.rectifier, sims_per_entry=FLAGS.sims_per_entry, number_policies_selected=FLAGS.number_policies_selected, meta_strategy_method=FLAGS.meta_strategy_method, prd_iterations=50000, prd_gamma=1e-10, sample_from_marginals=sample_from_marginals, symmetric_game=FLAGS.symmetric_game)\n    start_time = time.time()\n    for gpsro_iteration in range(FLAGS.gpsro_iterations):\n        if FLAGS.verbose:\n            print('Iteration : {}'.format(gpsro_iteration))\n            print('Time so far: {}'.format(time.time() - start_time))\n        g_psro_solver.iteration()\n        meta_game = g_psro_solver.get_meta_game()\n        meta_probabilities = g_psro_solver.get_meta_strategies()\n        policies = g_psro_solver.get_policies()\n        if FLAGS.verbose:\n            print('Meta game : {}'.format(meta_game))\n            print('Probabilities : {}'.format(meta_probabilities))\n        if env.game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL:\n            aggregator = policy_aggregator.PolicyAggregator(env.game)\n            aggr_policies = aggregator.aggregate(range(FLAGS.n_players), policies, meta_probabilities)\n            (exploitabilities, expl_per_player) = exploitability.nash_conv(env.game, aggr_policies, return_only_nash_conv=False)\n            _ = print_policy_analysis(policies, env.game, FLAGS.verbose)\n            if FLAGS.verbose:\n                print('Exploitabilities : {}'.format(exploitabilities))\n                print('Exploitabilities per player : {}'.format(expl_per_player))",
        "mutated": [
            "def gpsro_looper(env, oracle, agents):\n    if False:\n        i = 10\n    'Initializes and executes the GPSRO training loop.'\n    sample_from_marginals = True\n    training_strategy_selector = FLAGS.training_strategy_selector or strategy_selectors.probabilistic\n    g_psro_solver = psro_v2.PSROSolver(env.game, oracle, initial_policies=agents, training_strategy_selector=training_strategy_selector, rectifier=FLAGS.rectifier, sims_per_entry=FLAGS.sims_per_entry, number_policies_selected=FLAGS.number_policies_selected, meta_strategy_method=FLAGS.meta_strategy_method, prd_iterations=50000, prd_gamma=1e-10, sample_from_marginals=sample_from_marginals, symmetric_game=FLAGS.symmetric_game)\n    start_time = time.time()\n    for gpsro_iteration in range(FLAGS.gpsro_iterations):\n        if FLAGS.verbose:\n            print('Iteration : {}'.format(gpsro_iteration))\n            print('Time so far: {}'.format(time.time() - start_time))\n        g_psro_solver.iteration()\n        meta_game = g_psro_solver.get_meta_game()\n        meta_probabilities = g_psro_solver.get_meta_strategies()\n        policies = g_psro_solver.get_policies()\n        if FLAGS.verbose:\n            print('Meta game : {}'.format(meta_game))\n            print('Probabilities : {}'.format(meta_probabilities))\n        if env.game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL:\n            aggregator = policy_aggregator.PolicyAggregator(env.game)\n            aggr_policies = aggregator.aggregate(range(FLAGS.n_players), policies, meta_probabilities)\n            (exploitabilities, expl_per_player) = exploitability.nash_conv(env.game, aggr_policies, return_only_nash_conv=False)\n            _ = print_policy_analysis(policies, env.game, FLAGS.verbose)\n            if FLAGS.verbose:\n                print('Exploitabilities : {}'.format(exploitabilities))\n                print('Exploitabilities per player : {}'.format(expl_per_player))",
            "def gpsro_looper(env, oracle, agents):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Initializes and executes the GPSRO training loop.'\n    sample_from_marginals = True\n    training_strategy_selector = FLAGS.training_strategy_selector or strategy_selectors.probabilistic\n    g_psro_solver = psro_v2.PSROSolver(env.game, oracle, initial_policies=agents, training_strategy_selector=training_strategy_selector, rectifier=FLAGS.rectifier, sims_per_entry=FLAGS.sims_per_entry, number_policies_selected=FLAGS.number_policies_selected, meta_strategy_method=FLAGS.meta_strategy_method, prd_iterations=50000, prd_gamma=1e-10, sample_from_marginals=sample_from_marginals, symmetric_game=FLAGS.symmetric_game)\n    start_time = time.time()\n    for gpsro_iteration in range(FLAGS.gpsro_iterations):\n        if FLAGS.verbose:\n            print('Iteration : {}'.format(gpsro_iteration))\n            print('Time so far: {}'.format(time.time() - start_time))\n        g_psro_solver.iteration()\n        meta_game = g_psro_solver.get_meta_game()\n        meta_probabilities = g_psro_solver.get_meta_strategies()\n        policies = g_psro_solver.get_policies()\n        if FLAGS.verbose:\n            print('Meta game : {}'.format(meta_game))\n            print('Probabilities : {}'.format(meta_probabilities))\n        if env.game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL:\n            aggregator = policy_aggregator.PolicyAggregator(env.game)\n            aggr_policies = aggregator.aggregate(range(FLAGS.n_players), policies, meta_probabilities)\n            (exploitabilities, expl_per_player) = exploitability.nash_conv(env.game, aggr_policies, return_only_nash_conv=False)\n            _ = print_policy_analysis(policies, env.game, FLAGS.verbose)\n            if FLAGS.verbose:\n                print('Exploitabilities : {}'.format(exploitabilities))\n                print('Exploitabilities per player : {}'.format(expl_per_player))",
            "def gpsro_looper(env, oracle, agents):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Initializes and executes the GPSRO training loop.'\n    sample_from_marginals = True\n    training_strategy_selector = FLAGS.training_strategy_selector or strategy_selectors.probabilistic\n    g_psro_solver = psro_v2.PSROSolver(env.game, oracle, initial_policies=agents, training_strategy_selector=training_strategy_selector, rectifier=FLAGS.rectifier, sims_per_entry=FLAGS.sims_per_entry, number_policies_selected=FLAGS.number_policies_selected, meta_strategy_method=FLAGS.meta_strategy_method, prd_iterations=50000, prd_gamma=1e-10, sample_from_marginals=sample_from_marginals, symmetric_game=FLAGS.symmetric_game)\n    start_time = time.time()\n    for gpsro_iteration in range(FLAGS.gpsro_iterations):\n        if FLAGS.verbose:\n            print('Iteration : {}'.format(gpsro_iteration))\n            print('Time so far: {}'.format(time.time() - start_time))\n        g_psro_solver.iteration()\n        meta_game = g_psro_solver.get_meta_game()\n        meta_probabilities = g_psro_solver.get_meta_strategies()\n        policies = g_psro_solver.get_policies()\n        if FLAGS.verbose:\n            print('Meta game : {}'.format(meta_game))\n            print('Probabilities : {}'.format(meta_probabilities))\n        if env.game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL:\n            aggregator = policy_aggregator.PolicyAggregator(env.game)\n            aggr_policies = aggregator.aggregate(range(FLAGS.n_players), policies, meta_probabilities)\n            (exploitabilities, expl_per_player) = exploitability.nash_conv(env.game, aggr_policies, return_only_nash_conv=False)\n            _ = print_policy_analysis(policies, env.game, FLAGS.verbose)\n            if FLAGS.verbose:\n                print('Exploitabilities : {}'.format(exploitabilities))\n                print('Exploitabilities per player : {}'.format(expl_per_player))",
            "def gpsro_looper(env, oracle, agents):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Initializes and executes the GPSRO training loop.'\n    sample_from_marginals = True\n    training_strategy_selector = FLAGS.training_strategy_selector or strategy_selectors.probabilistic\n    g_psro_solver = psro_v2.PSROSolver(env.game, oracle, initial_policies=agents, training_strategy_selector=training_strategy_selector, rectifier=FLAGS.rectifier, sims_per_entry=FLAGS.sims_per_entry, number_policies_selected=FLAGS.number_policies_selected, meta_strategy_method=FLAGS.meta_strategy_method, prd_iterations=50000, prd_gamma=1e-10, sample_from_marginals=sample_from_marginals, symmetric_game=FLAGS.symmetric_game)\n    start_time = time.time()\n    for gpsro_iteration in range(FLAGS.gpsro_iterations):\n        if FLAGS.verbose:\n            print('Iteration : {}'.format(gpsro_iteration))\n            print('Time so far: {}'.format(time.time() - start_time))\n        g_psro_solver.iteration()\n        meta_game = g_psro_solver.get_meta_game()\n        meta_probabilities = g_psro_solver.get_meta_strategies()\n        policies = g_psro_solver.get_policies()\n        if FLAGS.verbose:\n            print('Meta game : {}'.format(meta_game))\n            print('Probabilities : {}'.format(meta_probabilities))\n        if env.game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL:\n            aggregator = policy_aggregator.PolicyAggregator(env.game)\n            aggr_policies = aggregator.aggregate(range(FLAGS.n_players), policies, meta_probabilities)\n            (exploitabilities, expl_per_player) = exploitability.nash_conv(env.game, aggr_policies, return_only_nash_conv=False)\n            _ = print_policy_analysis(policies, env.game, FLAGS.verbose)\n            if FLAGS.verbose:\n                print('Exploitabilities : {}'.format(exploitabilities))\n                print('Exploitabilities per player : {}'.format(expl_per_player))",
            "def gpsro_looper(env, oracle, agents):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Initializes and executes the GPSRO training loop.'\n    sample_from_marginals = True\n    training_strategy_selector = FLAGS.training_strategy_selector or strategy_selectors.probabilistic\n    g_psro_solver = psro_v2.PSROSolver(env.game, oracle, initial_policies=agents, training_strategy_selector=training_strategy_selector, rectifier=FLAGS.rectifier, sims_per_entry=FLAGS.sims_per_entry, number_policies_selected=FLAGS.number_policies_selected, meta_strategy_method=FLAGS.meta_strategy_method, prd_iterations=50000, prd_gamma=1e-10, sample_from_marginals=sample_from_marginals, symmetric_game=FLAGS.symmetric_game)\n    start_time = time.time()\n    for gpsro_iteration in range(FLAGS.gpsro_iterations):\n        if FLAGS.verbose:\n            print('Iteration : {}'.format(gpsro_iteration))\n            print('Time so far: {}'.format(time.time() - start_time))\n        g_psro_solver.iteration()\n        meta_game = g_psro_solver.get_meta_game()\n        meta_probabilities = g_psro_solver.get_meta_strategies()\n        policies = g_psro_solver.get_policies()\n        if FLAGS.verbose:\n            print('Meta game : {}'.format(meta_game))\n            print('Probabilities : {}'.format(meta_probabilities))\n        if env.game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL:\n            aggregator = policy_aggregator.PolicyAggregator(env.game)\n            aggr_policies = aggregator.aggregate(range(FLAGS.n_players), policies, meta_probabilities)\n            (exploitabilities, expl_per_player) = exploitability.nash_conv(env.game, aggr_policies, return_only_nash_conv=False)\n            _ = print_policy_analysis(policies, env.game, FLAGS.verbose)\n            if FLAGS.verbose:\n                print('Exploitabilities : {}'.format(exploitabilities))\n                print('Exploitabilities per player : {}'.format(expl_per_player))"
        ]
    },
    {
        "func_name": "main",
        "original": "def main(argv):\n    if len(argv) > 1:\n        raise app.UsageError('Too many command-line arguments.')\n    np.random.seed(FLAGS.seed)\n    game = pyspiel.load_game_as_turn_based(FLAGS.game_name, {'players': FLAGS.n_players})\n    env = rl_environment.Environment(game)\n    with tf.Session() as sess:\n        if FLAGS.oracle_type == 'DQN':\n            (oracle, agents) = init_dqn_responder(sess, env)\n        elif FLAGS.oracle_type == 'PG':\n            (oracle, agents) = init_pg_responder(sess, env)\n        elif FLAGS.oracle_type == 'BR':\n            (oracle, agents) = init_br_responder(env)\n        sess.run(tf.global_variables_initializer())\n        gpsro_looper(env, oracle, agents)",
        "mutated": [
            "def main(argv):\n    if False:\n        i = 10\n    if len(argv) > 1:\n        raise app.UsageError('Too many command-line arguments.')\n    np.random.seed(FLAGS.seed)\n    game = pyspiel.load_game_as_turn_based(FLAGS.game_name, {'players': FLAGS.n_players})\n    env = rl_environment.Environment(game)\n    with tf.Session() as sess:\n        if FLAGS.oracle_type == 'DQN':\n            (oracle, agents) = init_dqn_responder(sess, env)\n        elif FLAGS.oracle_type == 'PG':\n            (oracle, agents) = init_pg_responder(sess, env)\n        elif FLAGS.oracle_type == 'BR':\n            (oracle, agents) = init_br_responder(env)\n        sess.run(tf.global_variables_initializer())\n        gpsro_looper(env, oracle, agents)",
            "def main(argv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if len(argv) > 1:\n        raise app.UsageError('Too many command-line arguments.')\n    np.random.seed(FLAGS.seed)\n    game = pyspiel.load_game_as_turn_based(FLAGS.game_name, {'players': FLAGS.n_players})\n    env = rl_environment.Environment(game)\n    with tf.Session() as sess:\n        if FLAGS.oracle_type == 'DQN':\n            (oracle, agents) = init_dqn_responder(sess, env)\n        elif FLAGS.oracle_type == 'PG':\n            (oracle, agents) = init_pg_responder(sess, env)\n        elif FLAGS.oracle_type == 'BR':\n            (oracle, agents) = init_br_responder(env)\n        sess.run(tf.global_variables_initializer())\n        gpsro_looper(env, oracle, agents)",
            "def main(argv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if len(argv) > 1:\n        raise app.UsageError('Too many command-line arguments.')\n    np.random.seed(FLAGS.seed)\n    game = pyspiel.load_game_as_turn_based(FLAGS.game_name, {'players': FLAGS.n_players})\n    env = rl_environment.Environment(game)\n    with tf.Session() as sess:\n        if FLAGS.oracle_type == 'DQN':\n            (oracle, agents) = init_dqn_responder(sess, env)\n        elif FLAGS.oracle_type == 'PG':\n            (oracle, agents) = init_pg_responder(sess, env)\n        elif FLAGS.oracle_type == 'BR':\n            (oracle, agents) = init_br_responder(env)\n        sess.run(tf.global_variables_initializer())\n        gpsro_looper(env, oracle, agents)",
            "def main(argv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if len(argv) > 1:\n        raise app.UsageError('Too many command-line arguments.')\n    np.random.seed(FLAGS.seed)\n    game = pyspiel.load_game_as_turn_based(FLAGS.game_name, {'players': FLAGS.n_players})\n    env = rl_environment.Environment(game)\n    with tf.Session() as sess:\n        if FLAGS.oracle_type == 'DQN':\n            (oracle, agents) = init_dqn_responder(sess, env)\n        elif FLAGS.oracle_type == 'PG':\n            (oracle, agents) = init_pg_responder(sess, env)\n        elif FLAGS.oracle_type == 'BR':\n            (oracle, agents) = init_br_responder(env)\n        sess.run(tf.global_variables_initializer())\n        gpsro_looper(env, oracle, agents)",
            "def main(argv):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if len(argv) > 1:\n        raise app.UsageError('Too many command-line arguments.')\n    np.random.seed(FLAGS.seed)\n    game = pyspiel.load_game_as_turn_based(FLAGS.game_name, {'players': FLAGS.n_players})\n    env = rl_environment.Environment(game)\n    with tf.Session() as sess:\n        if FLAGS.oracle_type == 'DQN':\n            (oracle, agents) = init_dqn_responder(sess, env)\n        elif FLAGS.oracle_type == 'PG':\n            (oracle, agents) = init_pg_responder(sess, env)\n        elif FLAGS.oracle_type == 'BR':\n            (oracle, agents) = init_br_responder(env)\n        sess.run(tf.global_variables_initializer())\n        gpsro_looper(env, oracle, agents)"
        ]
    }
]