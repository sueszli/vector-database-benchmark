import jittor as jt
from jittor import Function
EMD_gpu_header = '\nnamespace jittor {\n__device__ inline out_type dist2(out_type x1, out_type y1, out_type z1,\n        out_type x2, out_type y2, out_type z2) {\n    return (x2 - x1) * (x2 - x1) + (y2 - y1) * (y2 - y1) + (z2 - z1) * (z2 - z1);\n}\n}\n'
approxmatch_gpu_src = '\n    __global__ void approxmatch_gpu_kernel(@ARGS_DEF) {\n        @PRECALC\n        @alias(xyz1, in0)\n        @alias(xyz2, in1)\n        @alias(match, out)\n\n        int b = in0_shape0;\n        int n = in0_shape1;\n        int m = in1_shape1;\n\n        out_type *remainL = in2_p + blockIdx.x * (n + m) * 2;\n        out_type *remainR = remainL + n;\n        out_type *ratioL = remainR + m;\n        out_type *ratioR = ratioL + n;\n\n        const int Block = 1024;\n        __shared__ out_type buf[Block * 4];\n\n        for (int i = blockIdx.x; i < b; i += gridDim.x) {\n            for (int j = threadIdx.x; j < n * m; j += blockDim.x)\n                match_p[i * n * m + j] = 0;\n            for (int j = threadIdx.x; j < n; j += blockDim.x)\n                remainL[j] = n >= m ? 1 : m / n;\n            for (int j = threadIdx.x; j < m; j += blockDim.x)\n                remainR[j] = n >= m ? n / m : 1;\n            __syncthreads();\n\n            for (int j = 7; j >= -2; j--) {\n                out_type level = j > -2 ? -powf(4.0f, j) : 0;\n\n                for (int k0 = 0; k0 < n; k0 += blockDim.x) {\n                    int k = k0 + threadIdx.x;\n                    out_type x1 = 0, y1 = 0, z1 = 0;\n                    if (k < n) {\n                        x1 = @xyz1(i, k, 0);\n                        y1 = @xyz1(i, k, 1);\n                        z1 = @xyz1(i, k, 2);\n                    }\n\n                    out_type suml = 1e-9f;\n                    for (int l0 = 0; l0 < m; l0 += Block){\n                        int lend = min(m, l0 + Block) - l0;\n                        for (int l = threadIdx.x; l < lend; l += blockDim.x) {\n                            buf[l * 4 + 0] = @xyz2(i, l0 + l, 0);\n                            buf[l * 4 + 1] = @xyz2(i, l0 + l, 1);\n                            buf[l * 4 + 2] = @xyz2(i, l0 + l, 2);\n                            buf[l * 4 + 3] = remainR[l0 + l];\n                        }\n                        __syncthreads();\n\n                        for (int l = 0; l < lend; l++){\n                            out_type x2 = buf[l * 4 + 0];\n                            out_type y2 = buf[l * 4 + 1];\n                            out_type z2 = buf[l * 4 + 2];\n                            out_type d = level * dist2(x1, y1, z1, x2, y2, z2);\n                            out_type w = __expf(d) * buf[l * 4 + 3];\n                            suml += w;\n                        }\n                        __syncthreads();\n                    }\n                    if (k < n)\n                        ratioL[k] = remainL[k] / suml;\n                }\n                __syncthreads();\n\n                for (int l0 = 0; l0 < m; l0 += blockDim.x){\n                    int l = l0 + threadIdx.x;\n                    out_type x2 = 0, y2 = 0, z2 = 0;\n                    if (l < m){\n                        x2 = @xyz2(i, l, 0);\n                        y2 = @xyz2(i, l, 1);\n                        z2 = @xyz2(i, l, 2);\n                    }\n                    out_type sumr = 0;\n                    for (int k0 = 0; k0 < n; k0 += Block){\n                        int kend = min(n, k0 + Block) - k0;\n                        for (int k = threadIdx.x; k < kend; k += blockDim.x){\n                            buf[k * 4 + 0] = @xyz1(i, k0 + k, 0);\n                            buf[k * 4 + 1] = @xyz1(i, k0 + k, 1);\n                            buf[k * 4 + 2] = @xyz1(i, k0 + k, 2);\n                            buf[k * 4 + 3] = ratioL[k0 + k];\n                        }\n                        __syncthreads();\n\n                        for (int k = 0; k < kend; k++){\n                            out_type x1 = buf[k * 4 + 0];\n                            out_type y1 = buf[k * 4 + 1];\n                            out_type z1 = buf[k * 4 + 2];\n                            out_type d = level * dist2(x1, y1, z1, x2, y2, z2);\n                            out_type w = __expf(d) * buf[k * 4 + 3];\n                            sumr += w;\n                        }\n                        __syncthreads();\n                    }\n\n                    if (l < m){\n                        sumr *= remainR[l];\n                        out_type consumption = fminf(remainR[l] / (sumr + 1e-9f), 1.0f);\n                        ratioR[l] = consumption * remainR[l];\n                        remainR[l] = fmaxf(0.0f, remainR[l] - sumr);\n                    }\n                }\n                __syncthreads();\n\n                for (int k0 = 0; k0 < n; k0 += blockDim.x){\n                    int k = k0 + threadIdx.x;\n                    out_type x1 = 0, y1 = 0, z1 = 0;\n                    if (k < n){\n                        x1 = @xyz1(i, k, 0);\n                        y1 = @xyz1(i, k, 1);\n                        z1 = @xyz1(i, k, 2);\n                    }\n                    out_type suml = 0;\n                    for (int l0 = 0; l0 < m; l0 += Block){\n                        int lend = min(m, l0 + Block)-l0;\n                        for (int l = threadIdx.x; l < lend; l += blockDim.x){\n                            buf[l * 4 + 0] = @xyz2(i, l0 + l, 0);\n                            buf[l * 4 + 1] = @xyz2(i, l0 + l, 1);\n                            buf[l * 4 + 2] = @xyz2(i, l0 + l, 2);\n                            buf[l * 4 + 3] = ratioR[l0 + l];\n                        }\n                        __syncthreads();\n\n                        out_type rl = ratioL[k];\n                        if (k < n){\n                            for (int l = 0; l < lend; l++){\n                                out_type x2 = buf[l * 4 + 0];\n                                out_type y2 = buf[l * 4 + 1];\n                                out_type z2 = buf[l * 4 + 2];\n                                out_type d = level * dist2(x1, y1, z1, x2, y2, z2);\n                                out_type w = __expf(d) * rl * buf[l*4+3];\n                                @match(i, l0 + l, k) += w;\n                                suml += w;\n                            }\n                        }\n                        __syncthreads();\n                    }\n                    if (k < n)\n                        remainL[k] = fmaxf(0.0f, remainL[k] - suml);\n                }\n                __syncthreads();\n            }\n        }\n    }\n\n    approxmatch_gpu_kernel<<<32, 512>>>(@ARGS);\n'
matchcost_gpu_src = '\n    __global__ void matchcost_gpu_kernel(@ARGS_DEF) {\n        @PRECALC\n        @alias(xyz1, in0)\n        @alias(xyz2, in1)\n        @alias(match, in2)\n\n        int b = in0_shape0;\n        int n = in0_shape1;\n        int m = in1_shape1;\n\n        const int Block = 1024;\n        __shared__ out_type allsum[512];\n        __shared__ out_type buf[Block * 3];\n\n        for (int i = blockIdx.x; i < b; i += gridDim.x) {\n            out_type subsum = 0;\n            for (int k0 = 0; k0 < n; k0 += blockDim.x) {\n                int k = k0 + threadIdx.x;\n                out_type x1 = 0, y1 = 0, z1 = 0;\n                if (k < n) {\n                    x1 = @xyz1(i, k, 0);\n                    y1 = @xyz1(i, k, 1);\n                    z1 = @xyz1(i, k, 2);\n                }\n\n                for (int l0 = 0; l0 < m; l0 += Block) {\n                    int lend = min(m, l0 + Block) - l0;\n                    for (int l = threadIdx.x; l < lend * 3; l += blockDim.x)\n                        buf[l] = xyz2_p[i * m * 3 + l0 * 3 + l];\n                    __syncthreads();\n\n                    if (k < n) {\n                        for (int l = 0; l < lend; l++) {\n                            out_type x2 = buf[l * 3 + 0];\n                            out_type y2 = buf[l * 3 + 1];\n                            out_type z2 = buf[l * 3 + 2];\n                            out_type d = dist2(x1, y1, z1, x2, y2, z2);\n                            subsum += d * @match(i, l0 + l, k);\n                        }\n                    }\n                    __syncthreads();\n                }\n            }\n\n            allsum[threadIdx.x] = subsum;\n            for (int j = 1; j < blockDim.x; j <<= 1) {\n                __syncthreads();\n                if ((threadIdx.x & j) == 0 && threadIdx.x + j < blockDim.x) {\n                    allsum[threadIdx.x] += allsum[threadIdx.x + j];\n                }\n            }\n\n            if (threadIdx.x == 0)\n                @out(i) = allsum[0];\n            __syncthreads();\n        }\n    }\n\n    matchcost_gpu_kernel<<<32, 512>>>(@ARGS);\n'
matchcost_grad1_gpu_src = '\n    __global__ void matchcost_grad1_gpu_kernel(@ARGS_DEF) {\n        @PRECALC\n        @alias(grad, in0)\n        @alias(xyz1, in1)\n        @alias(xyz2, in2)\n        @alias(match, in3)\n\n        int b = grad_shape0;\n        int n = xyz1_shape1;\n        int m = xyz2_shape1;\n\n        for (int i = blockIdx.x; i < b ; i += gridDim.x){\n            for (int l = threadIdx.x; l < n; l += blockDim.x){\n                out_type x1 = @xyz1(i, l, 0);\n                out_type y1 = @xyz1(i, l, 1);\n                out_type z1 = @xyz1(i, l, 2);\n                out_type dx = 0, dy = 0, dz = 0;\n                for (int k = 0; k < m; k++){\n                    out_type x2 = @xyz2(i, k, 0);\n                    out_type y2 = @xyz2(i, k, 1);\n                    out_type z2 = @xyz2(i, k, 2);\n                    out_type d = @match(i, k, l) * 2;\n                    dx += (x1 - x2) * d;\n                    dy += (y1 - y2) * d;\n                    dz += (z1 - z2) * d;\n                }\n                @out(i, l, 0) = dx * @grad(i);\n                @out(i, l, 1) = dy * @grad(i);\n                @out(i, l, 2) = dz * @grad(i);\n            }\n        }\n    }\n\n    matchcost_grad1_gpu_kernel<<<32, 512>>>(@ARGS);\n'
matchcost_grad2_gpu_src = '\n    __global__ void matchcost_grad2_gpu_kernel(@ARGS_DEF) {\n        @PRECALC\n        @alias(grad, in0)\n        @alias(xyz1, in1)\n        @alias(xyz2, in2)\n        @alias(match, in3)\n\n        int b = grad_shape0;\n        int n = xyz1_shape1;\n        int m = xyz2_shape1;\n\n        __shared__ out_type sum_grad[256 * 3];\n        for (int i = blockIdx.x; i < b; i += gridDim.x) {\n            int kbeg = m * blockIdx.y / gridDim.y;\n            int kend = m * (blockIdx.y + 1) / gridDim.y;\n            for (int k = kbeg; k < kend; k++) {\n                out_type x2 = @xyz2(i, k, 0);\n                out_type y2 = @xyz2(i, k, 1);\n                out_type z2 = @xyz2(i, k, 2);\n                out_type subsumx = 0, subsumy = 0, subsumz = 0;\n                for (int j = threadIdx.x; j < n; j += blockDim.x) {\n                    out_type x1 = x2 - @xyz1(i, j, 0);\n                    out_type y1 = y2 - @xyz1(i, j, 1);\n                    out_type z1 = z2 - @xyz1(i, j, 2);\n                    out_type d = @match(i, k, j) * 2;\n                    subsumx += x1 * d;\n                    subsumy += y1 * d;\n                    subsumz += z1 * d;\n                }\n                sum_grad[threadIdx.x * 3 + 0] = subsumx;\n                sum_grad[threadIdx.x * 3 + 1] = subsumy;\n                sum_grad[threadIdx.x * 3 + 2] = subsumz;\n\n                for (int j = 1; j < blockDim.x; j <<= 1) {\n                    __syncthreads();\n                    int j1 = threadIdx.x;\n                    int j2 = threadIdx.x + j;\n                    if ((j1 & j) == 0 && j2 < blockDim.x){\n                        sum_grad[j1 * 3 + 0] += sum_grad[j2 * 3 + 0];\n                        sum_grad[j1 * 3 + 1] += sum_grad[j2 * 3 + 1];\n                        sum_grad[j1 * 3 + 2] += sum_grad[j2 * 3 + 2];\n                    }\n                }\n                if (threadIdx.x == 0){\n                    @out(i, k, 0) = sum_grad[0] * @grad(i);\n                    @out(i, k, 1) = sum_grad[1] * @grad(i);\n                    @out(i, k, 2) = sum_grad[2] * @grad(i);\n                }\n                __syncthreads();\n            }\n        }\n    }\n\n    matchcost_grad2_gpu_kernel<<<dim3(32, 32), 256>>>(@ARGS);\n'

class EarthMoverDistance(Function):
    """ A loss layer that computes Earth Mover's distance from pc1 to pc2. Only supports GPU.

    :param pc1:  input point cloud
    :type pc1: jittor array

    :param pc2:  input point cloud
    :type pc2: jittor array

    :param reduction: reduction method in batches, can be 'mean', 'sum', or None. Default: 'mean'.
    :type reduction: str, optional
            
    :param dims: a string that represents each dimension, can be
            '[BNC]' ([batch, number of points, xyz]), or
            '[BCN]' ([batch, xyz, number of points]). Default: 'BNC'.
    :type dims: str, optional

    Example:

    >>> import jittor as jt
    >>> from jittor.loss3d import EarthMoverDistance
    >>> jt.flags.use_cuda = True
    >>> pc1 = jt.rand([10, 100, 3], dtype=jt.float32)
    >>> pc2 = jt.rand([10, 100, 3], dtype=jt.float32)
    >>> EMD = EarthMoverDistance(dims='BNC')
    >>> emd = EMD(pc1, pc2)
    >>> print('EMD =', emd.item())
    """

    def execute(self, pc1, pc2, reduction='mean', dims='BNC'):
        if False:
            while True:
                i = 10
        assert dims in ['BNC', 'BCN']
        if dims == 'BCN':
            (pc1, pc2) = (pc1.permute(0, 2, 1), pc2.permute(0, 2, 1))
        (batch_size_1, N, _) = pc1.shape
        (batch_size_2, M, _) = pc2.shape
        assert batch_size_1 == batch_size_2
        batch_size = batch_size_1
        temp = jt.zeros([batch_size, (N + M) * 2], pc1.dtype)
        match = jt.code(shape=[batch_size, M, N], dtype=pc1.dtype, inputs=[pc1, pc2, temp], cuda_header=EMD_gpu_header, cuda_src=approxmatch_gpu_src)
        emd = jt.code(shape=[batch_size], dtype=pc1.dtype, inputs=[pc1, pc2, match], cuda_header=EMD_gpu_header, cuda_src=matchcost_gpu_src)
        self.saved_vars = (pc1, pc2, match, reduction)
        if reduction is None:
            return emd
        elif reduction == 'sum':
            return emd.sum()
        elif reduction == 'mean':
            return emd.mean()

    def grad(self, grad):
        if False:
            for i in range(10):
                print('nop')
        (pc1, pc2, match, reduction) = self.saved_vars
        if reduction == 'sum':
            grad = jt.ones([pc1.shape[0]]) * grad
        elif reduction == 'mean':
            grad = jt.ones([pc1.shape[0]]) * grad / pc1.shape[0]
        grad_pc1 = jt.code(shape=pc1.shape, dtype=pc1.dtype, inputs=[grad, pc1, pc2, match], cuda_src=matchcost_grad1_gpu_src)
        grad_pc2 = jt.code(shape=pc2.shape, dtype=pc2.dtype, inputs=[grad, pc1, pc2, match], cuda_src=matchcost_grad2_gpu_src)
        return (grad_pc1, grad_pc2)

def earth_mover_distance(pc1, pc2, reduction='mean', dims='BNC'):
    if False:
        print('Hello World!')
    " Earth Mover's distance from pc1 to pc2. Only supports GPU.\n\n    :param pc1:  input point cloud\n    :type pc1: jittor array\n\n    :param pc2:  input point cloud\n    :type pc2: jittor array\n\n    :param reduction: reduction method in batches, can be 'mean', 'sum', or None. Default: 'mean'.\n    :type reduction: str, optional\n            \n    :param dims: a string that represents each dimension, can be\n            '[BNC]' ([batch, number of points, xyz]), or\n            '[BCN]' ([batch, xyz, number of points]). Default: 'BNC'.\n    :type dims: str, optional\n\n\n    Example:\n\n    >>> import jittor as jt\n    >>> from jittor.loss3d import earth_mover_distance\n    >>> jt.flags.use_cuda = True\n    >>> pc1 = jt.rand([10, 100, 3], dtype=jt.float32)\n    >>> pc2 = jt.rand([10, 100, 3], dtype=jt.float32)\n    >>> emd = earth_mover_distance(pc1, pc2, dims='BNC')\n    >>> print('EMD =', emd.item())\n    "
    return EarthMoverDistance.apply(pc1, pc2, reduction, dims)