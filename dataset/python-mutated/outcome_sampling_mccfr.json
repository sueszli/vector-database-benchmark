[
    {
        "func_name": "__init__",
        "original": "def __init__(self, game):\n    super().__init__(game)\n    self._expl = 0.6\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"MCCFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'",
        "mutated": [
            "def __init__(self, game):\n    if False:\n        i = 10\n    super().__init__(game)\n    self._expl = 0.6\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"MCCFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__(game)\n    self._expl = 0.6\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"MCCFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__(game)\n    self._expl = 0.6\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"MCCFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__(game)\n    self._expl = 0.6\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"MCCFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'",
            "def __init__(self, game):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__(game)\n    self._expl = 0.6\n    assert game.get_type().dynamics == pyspiel.GameType.Dynamics.SEQUENTIAL, \"MCCFR requires sequential games. If you're trying to run it \" + 'on a simultaneous (or normal-form) game, please first transform it ' + 'using turn_based_simultaneous_game.'"
        ]
    },
    {
        "func_name": "iteration",
        "original": "def iteration(self):\n    \"\"\"Performs one iteration of outcome sampling.\n\n    An iteration consists of one episode for each player as the update\n    player.\n    \"\"\"\n    for update_player in range(self._num_players):\n        state = self._game.new_initial_state()\n        self._episode(state, update_player, my_reach=1.0, opp_reach=1.0, sample_reach=1.0)",
        "mutated": [
            "def iteration(self):\n    if False:\n        i = 10\n    'Performs one iteration of outcome sampling.\\n\\n    An iteration consists of one episode for each player as the update\\n    player.\\n    '\n    for update_player in range(self._num_players):\n        state = self._game.new_initial_state()\n        self._episode(state, update_player, my_reach=1.0, opp_reach=1.0, sample_reach=1.0)",
            "def iteration(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Performs one iteration of outcome sampling.\\n\\n    An iteration consists of one episode for each player as the update\\n    player.\\n    '\n    for update_player in range(self._num_players):\n        state = self._game.new_initial_state()\n        self._episode(state, update_player, my_reach=1.0, opp_reach=1.0, sample_reach=1.0)",
            "def iteration(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Performs one iteration of outcome sampling.\\n\\n    An iteration consists of one episode for each player as the update\\n    player.\\n    '\n    for update_player in range(self._num_players):\n        state = self._game.new_initial_state()\n        self._episode(state, update_player, my_reach=1.0, opp_reach=1.0, sample_reach=1.0)",
            "def iteration(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Performs one iteration of outcome sampling.\\n\\n    An iteration consists of one episode for each player as the update\\n    player.\\n    '\n    for update_player in range(self._num_players):\n        state = self._game.new_initial_state()\n        self._episode(state, update_player, my_reach=1.0, opp_reach=1.0, sample_reach=1.0)",
            "def iteration(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Performs one iteration of outcome sampling.\\n\\n    An iteration consists of one episode for each player as the update\\n    player.\\n    '\n    for update_player in range(self._num_players):\n        state = self._game.new_initial_state()\n        self._episode(state, update_player, my_reach=1.0, opp_reach=1.0, sample_reach=1.0)"
        ]
    },
    {
        "func_name": "_baseline",
        "original": "def _baseline(self, state, info_state, aidx):\n    return 0",
        "mutated": [
            "def _baseline(self, state, info_state, aidx):\n    if False:\n        i = 10\n    return 0",
            "def _baseline(self, state, info_state, aidx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return 0",
            "def _baseline(self, state, info_state, aidx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return 0",
            "def _baseline(self, state, info_state, aidx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return 0",
            "def _baseline(self, state, info_state, aidx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return 0"
        ]
    },
    {
        "func_name": "_baseline_corrected_child_value",
        "original": "def _baseline_corrected_child_value(self, state, info_state, sampled_aidx, aidx, child_value, sample_prob):\n    baseline = self._baseline(state, info_state, aidx)\n    if aidx == sampled_aidx:\n        return baseline + (child_value - baseline) / sample_prob\n    else:\n        return baseline",
        "mutated": [
            "def _baseline_corrected_child_value(self, state, info_state, sampled_aidx, aidx, child_value, sample_prob):\n    if False:\n        i = 10\n    baseline = self._baseline(state, info_state, aidx)\n    if aidx == sampled_aidx:\n        return baseline + (child_value - baseline) / sample_prob\n    else:\n        return baseline",
            "def _baseline_corrected_child_value(self, state, info_state, sampled_aidx, aidx, child_value, sample_prob):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    baseline = self._baseline(state, info_state, aidx)\n    if aidx == sampled_aidx:\n        return baseline + (child_value - baseline) / sample_prob\n    else:\n        return baseline",
            "def _baseline_corrected_child_value(self, state, info_state, sampled_aidx, aidx, child_value, sample_prob):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    baseline = self._baseline(state, info_state, aidx)\n    if aidx == sampled_aidx:\n        return baseline + (child_value - baseline) / sample_prob\n    else:\n        return baseline",
            "def _baseline_corrected_child_value(self, state, info_state, sampled_aidx, aidx, child_value, sample_prob):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    baseline = self._baseline(state, info_state, aidx)\n    if aidx == sampled_aidx:\n        return baseline + (child_value - baseline) / sample_prob\n    else:\n        return baseline",
            "def _baseline_corrected_child_value(self, state, info_state, sampled_aidx, aidx, child_value, sample_prob):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    baseline = self._baseline(state, info_state, aidx)\n    if aidx == sampled_aidx:\n        return baseline + (child_value - baseline) / sample_prob\n    else:\n        return baseline"
        ]
    },
    {
        "func_name": "_episode",
        "original": "def _episode(self, state, update_player, my_reach, opp_reach, sample_reach):\n    \"\"\"Runs an episode of outcome sampling.\n\n    Args:\n      state: the open spiel state to run from (will be modified in-place).\n      update_player: the player to update regrets for (the other players\n        update average strategies)\n      my_reach: reach probability of the update player\n      opp_reach: reach probability of all the opponents (including chance)\n      sample_reach: reach probability of the sampling (behavior) policy\n\n    Returns:\n      util is a real value representing the utility of the update player\n    \"\"\"\n    if state.is_terminal():\n        return state.player_return(update_player)\n    if state.is_chance_node():\n        (outcomes, probs) = zip(*state.chance_outcomes())\n        aidx = np.random.choice(range(len(outcomes)), p=probs)\n        state.apply_action(outcomes[aidx])\n        return self._episode(state, update_player, my_reach, probs[aidx] * opp_reach, probs[aidx] * sample_reach)\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    if cur_player == update_player:\n        uniform_policy = np.ones(num_legal_actions, dtype=np.float64) / num_legal_actions\n        sample_policy = self._expl * uniform_policy + (1.0 - self._expl) * policy\n    else:\n        sample_policy = policy\n    sampled_aidx = np.random.choice(range(num_legal_actions), p=sample_policy)\n    state.apply_action(legal_actions[sampled_aidx])\n    if cur_player == update_player:\n        new_my_reach = my_reach * policy[sampled_aidx]\n        new_opp_reach = opp_reach\n    else:\n        new_my_reach = my_reach\n        new_opp_reach = opp_reach * policy[sampled_aidx]\n    new_sample_reach = sample_reach * sample_policy[sampled_aidx]\n    child_value = self._episode(state, update_player, new_my_reach, new_opp_reach, new_sample_reach)\n    child_values = np.zeros(num_legal_actions, dtype=np.float64)\n    for aidx in range(num_legal_actions):\n        child_values[aidx] = self._baseline_corrected_child_value(state, infostate_info, sampled_aidx, aidx, child_value, sample_policy[aidx])\n    value_estimate = 0\n    for aidx in range(num_legal_actions):\n        value_estimate += policy[aidx] * child_values[aidx]\n    if cur_player == update_player:\n        cf_value = value_estimate * opp_reach / sample_reach\n        for aidx in range(num_legal_actions):\n            cf_action_value = child_values[aidx] * opp_reach / sample_reach\n            self._add_regret(info_state_key, aidx, cf_action_value - cf_value)\n        for aidx in range(num_legal_actions):\n            increment = my_reach * policy[aidx] / sample_reach\n            self._add_avstrat(info_state_key, aidx, increment)\n    return value_estimate",
        "mutated": [
            "def _episode(self, state, update_player, my_reach, opp_reach, sample_reach):\n    if False:\n        i = 10\n    'Runs an episode of outcome sampling.\\n\\n    Args:\\n      state: the open spiel state to run from (will be modified in-place).\\n      update_player: the player to update regrets for (the other players\\n        update average strategies)\\n      my_reach: reach probability of the update player\\n      opp_reach: reach probability of all the opponents (including chance)\\n      sample_reach: reach probability of the sampling (behavior) policy\\n\\n    Returns:\\n      util is a real value representing the utility of the update player\\n    '\n    if state.is_terminal():\n        return state.player_return(update_player)\n    if state.is_chance_node():\n        (outcomes, probs) = zip(*state.chance_outcomes())\n        aidx = np.random.choice(range(len(outcomes)), p=probs)\n        state.apply_action(outcomes[aidx])\n        return self._episode(state, update_player, my_reach, probs[aidx] * opp_reach, probs[aidx] * sample_reach)\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    if cur_player == update_player:\n        uniform_policy = np.ones(num_legal_actions, dtype=np.float64) / num_legal_actions\n        sample_policy = self._expl * uniform_policy + (1.0 - self._expl) * policy\n    else:\n        sample_policy = policy\n    sampled_aidx = np.random.choice(range(num_legal_actions), p=sample_policy)\n    state.apply_action(legal_actions[sampled_aidx])\n    if cur_player == update_player:\n        new_my_reach = my_reach * policy[sampled_aidx]\n        new_opp_reach = opp_reach\n    else:\n        new_my_reach = my_reach\n        new_opp_reach = opp_reach * policy[sampled_aidx]\n    new_sample_reach = sample_reach * sample_policy[sampled_aidx]\n    child_value = self._episode(state, update_player, new_my_reach, new_opp_reach, new_sample_reach)\n    child_values = np.zeros(num_legal_actions, dtype=np.float64)\n    for aidx in range(num_legal_actions):\n        child_values[aidx] = self._baseline_corrected_child_value(state, infostate_info, sampled_aidx, aidx, child_value, sample_policy[aidx])\n    value_estimate = 0\n    for aidx in range(num_legal_actions):\n        value_estimate += policy[aidx] * child_values[aidx]\n    if cur_player == update_player:\n        cf_value = value_estimate * opp_reach / sample_reach\n        for aidx in range(num_legal_actions):\n            cf_action_value = child_values[aidx] * opp_reach / sample_reach\n            self._add_regret(info_state_key, aidx, cf_action_value - cf_value)\n        for aidx in range(num_legal_actions):\n            increment = my_reach * policy[aidx] / sample_reach\n            self._add_avstrat(info_state_key, aidx, increment)\n    return value_estimate",
            "def _episode(self, state, update_player, my_reach, opp_reach, sample_reach):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Runs an episode of outcome sampling.\\n\\n    Args:\\n      state: the open spiel state to run from (will be modified in-place).\\n      update_player: the player to update regrets for (the other players\\n        update average strategies)\\n      my_reach: reach probability of the update player\\n      opp_reach: reach probability of all the opponents (including chance)\\n      sample_reach: reach probability of the sampling (behavior) policy\\n\\n    Returns:\\n      util is a real value representing the utility of the update player\\n    '\n    if state.is_terminal():\n        return state.player_return(update_player)\n    if state.is_chance_node():\n        (outcomes, probs) = zip(*state.chance_outcomes())\n        aidx = np.random.choice(range(len(outcomes)), p=probs)\n        state.apply_action(outcomes[aidx])\n        return self._episode(state, update_player, my_reach, probs[aidx] * opp_reach, probs[aidx] * sample_reach)\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    if cur_player == update_player:\n        uniform_policy = np.ones(num_legal_actions, dtype=np.float64) / num_legal_actions\n        sample_policy = self._expl * uniform_policy + (1.0 - self._expl) * policy\n    else:\n        sample_policy = policy\n    sampled_aidx = np.random.choice(range(num_legal_actions), p=sample_policy)\n    state.apply_action(legal_actions[sampled_aidx])\n    if cur_player == update_player:\n        new_my_reach = my_reach * policy[sampled_aidx]\n        new_opp_reach = opp_reach\n    else:\n        new_my_reach = my_reach\n        new_opp_reach = opp_reach * policy[sampled_aidx]\n    new_sample_reach = sample_reach * sample_policy[sampled_aidx]\n    child_value = self._episode(state, update_player, new_my_reach, new_opp_reach, new_sample_reach)\n    child_values = np.zeros(num_legal_actions, dtype=np.float64)\n    for aidx in range(num_legal_actions):\n        child_values[aidx] = self._baseline_corrected_child_value(state, infostate_info, sampled_aidx, aidx, child_value, sample_policy[aidx])\n    value_estimate = 0\n    for aidx in range(num_legal_actions):\n        value_estimate += policy[aidx] * child_values[aidx]\n    if cur_player == update_player:\n        cf_value = value_estimate * opp_reach / sample_reach\n        for aidx in range(num_legal_actions):\n            cf_action_value = child_values[aidx] * opp_reach / sample_reach\n            self._add_regret(info_state_key, aidx, cf_action_value - cf_value)\n        for aidx in range(num_legal_actions):\n            increment = my_reach * policy[aidx] / sample_reach\n            self._add_avstrat(info_state_key, aidx, increment)\n    return value_estimate",
            "def _episode(self, state, update_player, my_reach, opp_reach, sample_reach):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Runs an episode of outcome sampling.\\n\\n    Args:\\n      state: the open spiel state to run from (will be modified in-place).\\n      update_player: the player to update regrets for (the other players\\n        update average strategies)\\n      my_reach: reach probability of the update player\\n      opp_reach: reach probability of all the opponents (including chance)\\n      sample_reach: reach probability of the sampling (behavior) policy\\n\\n    Returns:\\n      util is a real value representing the utility of the update player\\n    '\n    if state.is_terminal():\n        return state.player_return(update_player)\n    if state.is_chance_node():\n        (outcomes, probs) = zip(*state.chance_outcomes())\n        aidx = np.random.choice(range(len(outcomes)), p=probs)\n        state.apply_action(outcomes[aidx])\n        return self._episode(state, update_player, my_reach, probs[aidx] * opp_reach, probs[aidx] * sample_reach)\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    if cur_player == update_player:\n        uniform_policy = np.ones(num_legal_actions, dtype=np.float64) / num_legal_actions\n        sample_policy = self._expl * uniform_policy + (1.0 - self._expl) * policy\n    else:\n        sample_policy = policy\n    sampled_aidx = np.random.choice(range(num_legal_actions), p=sample_policy)\n    state.apply_action(legal_actions[sampled_aidx])\n    if cur_player == update_player:\n        new_my_reach = my_reach * policy[sampled_aidx]\n        new_opp_reach = opp_reach\n    else:\n        new_my_reach = my_reach\n        new_opp_reach = opp_reach * policy[sampled_aidx]\n    new_sample_reach = sample_reach * sample_policy[sampled_aidx]\n    child_value = self._episode(state, update_player, new_my_reach, new_opp_reach, new_sample_reach)\n    child_values = np.zeros(num_legal_actions, dtype=np.float64)\n    for aidx in range(num_legal_actions):\n        child_values[aidx] = self._baseline_corrected_child_value(state, infostate_info, sampled_aidx, aidx, child_value, sample_policy[aidx])\n    value_estimate = 0\n    for aidx in range(num_legal_actions):\n        value_estimate += policy[aidx] * child_values[aidx]\n    if cur_player == update_player:\n        cf_value = value_estimate * opp_reach / sample_reach\n        for aidx in range(num_legal_actions):\n            cf_action_value = child_values[aidx] * opp_reach / sample_reach\n            self._add_regret(info_state_key, aidx, cf_action_value - cf_value)\n        for aidx in range(num_legal_actions):\n            increment = my_reach * policy[aidx] / sample_reach\n            self._add_avstrat(info_state_key, aidx, increment)\n    return value_estimate",
            "def _episode(self, state, update_player, my_reach, opp_reach, sample_reach):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Runs an episode of outcome sampling.\\n\\n    Args:\\n      state: the open spiel state to run from (will be modified in-place).\\n      update_player: the player to update regrets for (the other players\\n        update average strategies)\\n      my_reach: reach probability of the update player\\n      opp_reach: reach probability of all the opponents (including chance)\\n      sample_reach: reach probability of the sampling (behavior) policy\\n\\n    Returns:\\n      util is a real value representing the utility of the update player\\n    '\n    if state.is_terminal():\n        return state.player_return(update_player)\n    if state.is_chance_node():\n        (outcomes, probs) = zip(*state.chance_outcomes())\n        aidx = np.random.choice(range(len(outcomes)), p=probs)\n        state.apply_action(outcomes[aidx])\n        return self._episode(state, update_player, my_reach, probs[aidx] * opp_reach, probs[aidx] * sample_reach)\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    if cur_player == update_player:\n        uniform_policy = np.ones(num_legal_actions, dtype=np.float64) / num_legal_actions\n        sample_policy = self._expl * uniform_policy + (1.0 - self._expl) * policy\n    else:\n        sample_policy = policy\n    sampled_aidx = np.random.choice(range(num_legal_actions), p=sample_policy)\n    state.apply_action(legal_actions[sampled_aidx])\n    if cur_player == update_player:\n        new_my_reach = my_reach * policy[sampled_aidx]\n        new_opp_reach = opp_reach\n    else:\n        new_my_reach = my_reach\n        new_opp_reach = opp_reach * policy[sampled_aidx]\n    new_sample_reach = sample_reach * sample_policy[sampled_aidx]\n    child_value = self._episode(state, update_player, new_my_reach, new_opp_reach, new_sample_reach)\n    child_values = np.zeros(num_legal_actions, dtype=np.float64)\n    for aidx in range(num_legal_actions):\n        child_values[aidx] = self._baseline_corrected_child_value(state, infostate_info, sampled_aidx, aidx, child_value, sample_policy[aidx])\n    value_estimate = 0\n    for aidx in range(num_legal_actions):\n        value_estimate += policy[aidx] * child_values[aidx]\n    if cur_player == update_player:\n        cf_value = value_estimate * opp_reach / sample_reach\n        for aidx in range(num_legal_actions):\n            cf_action_value = child_values[aidx] * opp_reach / sample_reach\n            self._add_regret(info_state_key, aidx, cf_action_value - cf_value)\n        for aidx in range(num_legal_actions):\n            increment = my_reach * policy[aidx] / sample_reach\n            self._add_avstrat(info_state_key, aidx, increment)\n    return value_estimate",
            "def _episode(self, state, update_player, my_reach, opp_reach, sample_reach):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Runs an episode of outcome sampling.\\n\\n    Args:\\n      state: the open spiel state to run from (will be modified in-place).\\n      update_player: the player to update regrets for (the other players\\n        update average strategies)\\n      my_reach: reach probability of the update player\\n      opp_reach: reach probability of all the opponents (including chance)\\n      sample_reach: reach probability of the sampling (behavior) policy\\n\\n    Returns:\\n      util is a real value representing the utility of the update player\\n    '\n    if state.is_terminal():\n        return state.player_return(update_player)\n    if state.is_chance_node():\n        (outcomes, probs) = zip(*state.chance_outcomes())\n        aidx = np.random.choice(range(len(outcomes)), p=probs)\n        state.apply_action(outcomes[aidx])\n        return self._episode(state, update_player, my_reach, probs[aidx] * opp_reach, probs[aidx] * sample_reach)\n    cur_player = state.current_player()\n    info_state_key = state.information_state_string(cur_player)\n    legal_actions = state.legal_actions()\n    num_legal_actions = len(legal_actions)\n    infostate_info = self._lookup_infostate_info(info_state_key, num_legal_actions)\n    policy = self._regret_matching(infostate_info[mccfr.REGRET_INDEX], num_legal_actions)\n    if cur_player == update_player:\n        uniform_policy = np.ones(num_legal_actions, dtype=np.float64) / num_legal_actions\n        sample_policy = self._expl * uniform_policy + (1.0 - self._expl) * policy\n    else:\n        sample_policy = policy\n    sampled_aidx = np.random.choice(range(num_legal_actions), p=sample_policy)\n    state.apply_action(legal_actions[sampled_aidx])\n    if cur_player == update_player:\n        new_my_reach = my_reach * policy[sampled_aidx]\n        new_opp_reach = opp_reach\n    else:\n        new_my_reach = my_reach\n        new_opp_reach = opp_reach * policy[sampled_aidx]\n    new_sample_reach = sample_reach * sample_policy[sampled_aidx]\n    child_value = self._episode(state, update_player, new_my_reach, new_opp_reach, new_sample_reach)\n    child_values = np.zeros(num_legal_actions, dtype=np.float64)\n    for aidx in range(num_legal_actions):\n        child_values[aidx] = self._baseline_corrected_child_value(state, infostate_info, sampled_aidx, aidx, child_value, sample_policy[aidx])\n    value_estimate = 0\n    for aidx in range(num_legal_actions):\n        value_estimate += policy[aidx] * child_values[aidx]\n    if cur_player == update_player:\n        cf_value = value_estimate * opp_reach / sample_reach\n        for aidx in range(num_legal_actions):\n            cf_action_value = child_values[aidx] * opp_reach / sample_reach\n            self._add_regret(info_state_key, aidx, cf_action_value - cf_value)\n        for aidx in range(num_legal_actions):\n            increment = my_reach * policy[aidx] / sample_reach\n            self._add_avstrat(info_state_key, aidx, increment)\n    return value_estimate"
        ]
    }
]