[
    {
        "func_name": "setup",
        "original": "def setup(self, sfc, userOpts=dict()):\n    self.sf = sfc\n    self.results = self.tempStorage()\n    for opt in list(userOpts.keys()):\n        self.opts[opt] = userOpts[opt]",
        "mutated": [
            "def setup(self, sfc, userOpts=dict()):\n    if False:\n        i = 10\n    self.sf = sfc\n    self.results = self.tempStorage()\n    for opt in list(userOpts.keys()):\n        self.opts[opt] = userOpts[opt]",
            "def setup(self, sfc, userOpts=dict()):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    self.sf = sfc\n    self.results = self.tempStorage()\n    for opt in list(userOpts.keys()):\n        self.opts[opt] = userOpts[opt]",
            "def setup(self, sfc, userOpts=dict()):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    self.sf = sfc\n    self.results = self.tempStorage()\n    for opt in list(userOpts.keys()):\n        self.opts[opt] = userOpts[opt]",
            "def setup(self, sfc, userOpts=dict()):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    self.sf = sfc\n    self.results = self.tempStorage()\n    for opt in list(userOpts.keys()):\n        self.opts[opt] = userOpts[opt]",
            "def setup(self, sfc, userOpts=dict()):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    self.sf = sfc\n    self.results = self.tempStorage()\n    for opt in list(userOpts.keys()):\n        self.opts[opt] = userOpts[opt]"
        ]
    },
    {
        "func_name": "watchedEvents",
        "original": "def watchedEvents(self):\n    return ['DOMAIN_NAME', 'EMAILADDR', 'HUMAN_NAME']",
        "mutated": [
            "def watchedEvents(self):\n    if False:\n        i = 10\n    return ['DOMAIN_NAME', 'EMAILADDR', 'HUMAN_NAME']",
            "def watchedEvents(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return ['DOMAIN_NAME', 'EMAILADDR', 'HUMAN_NAME']",
            "def watchedEvents(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return ['DOMAIN_NAME', 'EMAILADDR', 'HUMAN_NAME']",
            "def watchedEvents(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return ['DOMAIN_NAME', 'EMAILADDR', 'HUMAN_NAME']",
            "def watchedEvents(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return ['DOMAIN_NAME', 'EMAILADDR', 'HUMAN_NAME']"
        ]
    },
    {
        "func_name": "producedEvents",
        "original": "def producedEvents(self):\n    return ['LEAKSITE_CONTENT', 'LEAKSITE_URL']",
        "mutated": [
            "def producedEvents(self):\n    if False:\n        i = 10\n    return ['LEAKSITE_CONTENT', 'LEAKSITE_URL']",
            "def producedEvents(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return ['LEAKSITE_CONTENT', 'LEAKSITE_URL']",
            "def producedEvents(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return ['LEAKSITE_CONTENT', 'LEAKSITE_URL']",
            "def producedEvents(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return ['LEAKSITE_CONTENT', 'LEAKSITE_URL']",
            "def producedEvents(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return ['LEAKSITE_CONTENT', 'LEAKSITE_URL']"
        ]
    },
    {
        "func_name": "handleEvent",
        "original": "def handleEvent(self, event):\n    eventName = event.eventType\n    eventData = event.data\n    self.currentEventSrc = event\n    self.debug(f'Received event, {eventName}, from {event.module}')\n    if eventData in self.results:\n        self.debug(f'Skipping {eventData}, already checked.')\n        return\n    self.results[eventData] = True\n    if self.opts['external']:\n        external = 'True'\n    else:\n        external = ''\n    if self.opts['daysback'] is not None and self.opts['daysback'] != 0:\n        newDate = datetime.datetime.now() - datetime.timedelta(days=int(self.opts['daysback']))\n        maxDate = newDate.strftime('%Y-%m-%d')\n    else:\n        maxDate = ''\n    qdata = eventData.replace(' ', '+')\n    wlurl = 'query=%22' + qdata + '%22' + '&released_date_start=' + maxDate + '&include_external_sources=' + external + '&new_search=True&order_by=most_relevant#results'\n    res = self.sf.fetchUrl('https://search.wikileaks.org/?' + wlurl)\n    if res['content'] is None:\n        self.error('Unable to fetch Wikileaks content.')\n        return\n    links = dict()\n    p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'wikileaks.org')\n    if p:\n        links.update(p)\n    p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'cryptome.org')\n    if p:\n        links.update(p)\n    keepGoing = True\n    page = 0\n    while keepGoing:\n        if not res['content']:\n            break\n        if 'page=' not in res['content']:\n            keepGoing = False\n        for link in links:\n            if 'search.wikileaks.org/' in link:\n                continue\n            if 'wikileaks.org/' not in link and 'cryptome.org/' not in link:\n                continue\n            self.debug(f'Found a link: {link}')\n            if self.checkForStop():\n                return\n            if link.count('/') >= 4:\n                if not link.endswith('.js') and (not link.endswith('.css')):\n                    evt = SpiderFootEvent('LEAKSITE_URL', link, self.__name__, event)\n                    self.notifyListeners(evt)\n        if page > 50:\n            break\n        if keepGoing:\n            page += 1\n            wlurl = 'https://search.wikileaks.org/?query=%22' + qdata + '%22' + '&released_date_start=' + maxDate + '&include_external_sources=' + external + '&new_search=True&order_by=most_relevant&page=' + str(page) + '#results'\n            res = self.sf.fetchUrl(wlurl)\n            if not res:\n                break\n            if not res['content']:\n                break\n            links = dict()\n            p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'wikileaks.org')\n            if p:\n                links.update(p)\n            p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'cryptome.org')\n            if p:\n                links.update(p)",
        "mutated": [
            "def handleEvent(self, event):\n    if False:\n        i = 10\n    eventName = event.eventType\n    eventData = event.data\n    self.currentEventSrc = event\n    self.debug(f'Received event, {eventName}, from {event.module}')\n    if eventData in self.results:\n        self.debug(f'Skipping {eventData}, already checked.')\n        return\n    self.results[eventData] = True\n    if self.opts['external']:\n        external = 'True'\n    else:\n        external = ''\n    if self.opts['daysback'] is not None and self.opts['daysback'] != 0:\n        newDate = datetime.datetime.now() - datetime.timedelta(days=int(self.opts['daysback']))\n        maxDate = newDate.strftime('%Y-%m-%d')\n    else:\n        maxDate = ''\n    qdata = eventData.replace(' ', '+')\n    wlurl = 'query=%22' + qdata + '%22' + '&released_date_start=' + maxDate + '&include_external_sources=' + external + '&new_search=True&order_by=most_relevant#results'\n    res = self.sf.fetchUrl('https://search.wikileaks.org/?' + wlurl)\n    if res['content'] is None:\n        self.error('Unable to fetch Wikileaks content.')\n        return\n    links = dict()\n    p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'wikileaks.org')\n    if p:\n        links.update(p)\n    p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'cryptome.org')\n    if p:\n        links.update(p)\n    keepGoing = True\n    page = 0\n    while keepGoing:\n        if not res['content']:\n            break\n        if 'page=' not in res['content']:\n            keepGoing = False\n        for link in links:\n            if 'search.wikileaks.org/' in link:\n                continue\n            if 'wikileaks.org/' not in link and 'cryptome.org/' not in link:\n                continue\n            self.debug(f'Found a link: {link}')\n            if self.checkForStop():\n                return\n            if link.count('/') >= 4:\n                if not link.endswith('.js') and (not link.endswith('.css')):\n                    evt = SpiderFootEvent('LEAKSITE_URL', link, self.__name__, event)\n                    self.notifyListeners(evt)\n        if page > 50:\n            break\n        if keepGoing:\n            page += 1\n            wlurl = 'https://search.wikileaks.org/?query=%22' + qdata + '%22' + '&released_date_start=' + maxDate + '&include_external_sources=' + external + '&new_search=True&order_by=most_relevant&page=' + str(page) + '#results'\n            res = self.sf.fetchUrl(wlurl)\n            if not res:\n                break\n            if not res['content']:\n                break\n            links = dict()\n            p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'wikileaks.org')\n            if p:\n                links.update(p)\n            p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'cryptome.org')\n            if p:\n                links.update(p)",
            "def handleEvent(self, event):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    eventName = event.eventType\n    eventData = event.data\n    self.currentEventSrc = event\n    self.debug(f'Received event, {eventName}, from {event.module}')\n    if eventData in self.results:\n        self.debug(f'Skipping {eventData}, already checked.')\n        return\n    self.results[eventData] = True\n    if self.opts['external']:\n        external = 'True'\n    else:\n        external = ''\n    if self.opts['daysback'] is not None and self.opts['daysback'] != 0:\n        newDate = datetime.datetime.now() - datetime.timedelta(days=int(self.opts['daysback']))\n        maxDate = newDate.strftime('%Y-%m-%d')\n    else:\n        maxDate = ''\n    qdata = eventData.replace(' ', '+')\n    wlurl = 'query=%22' + qdata + '%22' + '&released_date_start=' + maxDate + '&include_external_sources=' + external + '&new_search=True&order_by=most_relevant#results'\n    res = self.sf.fetchUrl('https://search.wikileaks.org/?' + wlurl)\n    if res['content'] is None:\n        self.error('Unable to fetch Wikileaks content.')\n        return\n    links = dict()\n    p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'wikileaks.org')\n    if p:\n        links.update(p)\n    p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'cryptome.org')\n    if p:\n        links.update(p)\n    keepGoing = True\n    page = 0\n    while keepGoing:\n        if not res['content']:\n            break\n        if 'page=' not in res['content']:\n            keepGoing = False\n        for link in links:\n            if 'search.wikileaks.org/' in link:\n                continue\n            if 'wikileaks.org/' not in link and 'cryptome.org/' not in link:\n                continue\n            self.debug(f'Found a link: {link}')\n            if self.checkForStop():\n                return\n            if link.count('/') >= 4:\n                if not link.endswith('.js') and (not link.endswith('.css')):\n                    evt = SpiderFootEvent('LEAKSITE_URL', link, self.__name__, event)\n                    self.notifyListeners(evt)\n        if page > 50:\n            break\n        if keepGoing:\n            page += 1\n            wlurl = 'https://search.wikileaks.org/?query=%22' + qdata + '%22' + '&released_date_start=' + maxDate + '&include_external_sources=' + external + '&new_search=True&order_by=most_relevant&page=' + str(page) + '#results'\n            res = self.sf.fetchUrl(wlurl)\n            if not res:\n                break\n            if not res['content']:\n                break\n            links = dict()\n            p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'wikileaks.org')\n            if p:\n                links.update(p)\n            p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'cryptome.org')\n            if p:\n                links.update(p)",
            "def handleEvent(self, event):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    eventName = event.eventType\n    eventData = event.data\n    self.currentEventSrc = event\n    self.debug(f'Received event, {eventName}, from {event.module}')\n    if eventData in self.results:\n        self.debug(f'Skipping {eventData}, already checked.')\n        return\n    self.results[eventData] = True\n    if self.opts['external']:\n        external = 'True'\n    else:\n        external = ''\n    if self.opts['daysback'] is not None and self.opts['daysback'] != 0:\n        newDate = datetime.datetime.now() - datetime.timedelta(days=int(self.opts['daysback']))\n        maxDate = newDate.strftime('%Y-%m-%d')\n    else:\n        maxDate = ''\n    qdata = eventData.replace(' ', '+')\n    wlurl = 'query=%22' + qdata + '%22' + '&released_date_start=' + maxDate + '&include_external_sources=' + external + '&new_search=True&order_by=most_relevant#results'\n    res = self.sf.fetchUrl('https://search.wikileaks.org/?' + wlurl)\n    if res['content'] is None:\n        self.error('Unable to fetch Wikileaks content.')\n        return\n    links = dict()\n    p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'wikileaks.org')\n    if p:\n        links.update(p)\n    p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'cryptome.org')\n    if p:\n        links.update(p)\n    keepGoing = True\n    page = 0\n    while keepGoing:\n        if not res['content']:\n            break\n        if 'page=' not in res['content']:\n            keepGoing = False\n        for link in links:\n            if 'search.wikileaks.org/' in link:\n                continue\n            if 'wikileaks.org/' not in link and 'cryptome.org/' not in link:\n                continue\n            self.debug(f'Found a link: {link}')\n            if self.checkForStop():\n                return\n            if link.count('/') >= 4:\n                if not link.endswith('.js') and (not link.endswith('.css')):\n                    evt = SpiderFootEvent('LEAKSITE_URL', link, self.__name__, event)\n                    self.notifyListeners(evt)\n        if page > 50:\n            break\n        if keepGoing:\n            page += 1\n            wlurl = 'https://search.wikileaks.org/?query=%22' + qdata + '%22' + '&released_date_start=' + maxDate + '&include_external_sources=' + external + '&new_search=True&order_by=most_relevant&page=' + str(page) + '#results'\n            res = self.sf.fetchUrl(wlurl)\n            if not res:\n                break\n            if not res['content']:\n                break\n            links = dict()\n            p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'wikileaks.org')\n            if p:\n                links.update(p)\n            p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'cryptome.org')\n            if p:\n                links.update(p)",
            "def handleEvent(self, event):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    eventName = event.eventType\n    eventData = event.data\n    self.currentEventSrc = event\n    self.debug(f'Received event, {eventName}, from {event.module}')\n    if eventData in self.results:\n        self.debug(f'Skipping {eventData}, already checked.')\n        return\n    self.results[eventData] = True\n    if self.opts['external']:\n        external = 'True'\n    else:\n        external = ''\n    if self.opts['daysback'] is not None and self.opts['daysback'] != 0:\n        newDate = datetime.datetime.now() - datetime.timedelta(days=int(self.opts['daysback']))\n        maxDate = newDate.strftime('%Y-%m-%d')\n    else:\n        maxDate = ''\n    qdata = eventData.replace(' ', '+')\n    wlurl = 'query=%22' + qdata + '%22' + '&released_date_start=' + maxDate + '&include_external_sources=' + external + '&new_search=True&order_by=most_relevant#results'\n    res = self.sf.fetchUrl('https://search.wikileaks.org/?' + wlurl)\n    if res['content'] is None:\n        self.error('Unable to fetch Wikileaks content.')\n        return\n    links = dict()\n    p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'wikileaks.org')\n    if p:\n        links.update(p)\n    p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'cryptome.org')\n    if p:\n        links.update(p)\n    keepGoing = True\n    page = 0\n    while keepGoing:\n        if not res['content']:\n            break\n        if 'page=' not in res['content']:\n            keepGoing = False\n        for link in links:\n            if 'search.wikileaks.org/' in link:\n                continue\n            if 'wikileaks.org/' not in link and 'cryptome.org/' not in link:\n                continue\n            self.debug(f'Found a link: {link}')\n            if self.checkForStop():\n                return\n            if link.count('/') >= 4:\n                if not link.endswith('.js') and (not link.endswith('.css')):\n                    evt = SpiderFootEvent('LEAKSITE_URL', link, self.__name__, event)\n                    self.notifyListeners(evt)\n        if page > 50:\n            break\n        if keepGoing:\n            page += 1\n            wlurl = 'https://search.wikileaks.org/?query=%22' + qdata + '%22' + '&released_date_start=' + maxDate + '&include_external_sources=' + external + '&new_search=True&order_by=most_relevant&page=' + str(page) + '#results'\n            res = self.sf.fetchUrl(wlurl)\n            if not res:\n                break\n            if not res['content']:\n                break\n            links = dict()\n            p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'wikileaks.org')\n            if p:\n                links.update(p)\n            p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'cryptome.org')\n            if p:\n                links.update(p)",
            "def handleEvent(self, event):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    eventName = event.eventType\n    eventData = event.data\n    self.currentEventSrc = event\n    self.debug(f'Received event, {eventName}, from {event.module}')\n    if eventData in self.results:\n        self.debug(f'Skipping {eventData}, already checked.')\n        return\n    self.results[eventData] = True\n    if self.opts['external']:\n        external = 'True'\n    else:\n        external = ''\n    if self.opts['daysback'] is not None and self.opts['daysback'] != 0:\n        newDate = datetime.datetime.now() - datetime.timedelta(days=int(self.opts['daysback']))\n        maxDate = newDate.strftime('%Y-%m-%d')\n    else:\n        maxDate = ''\n    qdata = eventData.replace(' ', '+')\n    wlurl = 'query=%22' + qdata + '%22' + '&released_date_start=' + maxDate + '&include_external_sources=' + external + '&new_search=True&order_by=most_relevant#results'\n    res = self.sf.fetchUrl('https://search.wikileaks.org/?' + wlurl)\n    if res['content'] is None:\n        self.error('Unable to fetch Wikileaks content.')\n        return\n    links = dict()\n    p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'wikileaks.org')\n    if p:\n        links.update(p)\n    p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'cryptome.org')\n    if p:\n        links.update(p)\n    keepGoing = True\n    page = 0\n    while keepGoing:\n        if not res['content']:\n            break\n        if 'page=' not in res['content']:\n            keepGoing = False\n        for link in links:\n            if 'search.wikileaks.org/' in link:\n                continue\n            if 'wikileaks.org/' not in link and 'cryptome.org/' not in link:\n                continue\n            self.debug(f'Found a link: {link}')\n            if self.checkForStop():\n                return\n            if link.count('/') >= 4:\n                if not link.endswith('.js') and (not link.endswith('.css')):\n                    evt = SpiderFootEvent('LEAKSITE_URL', link, self.__name__, event)\n                    self.notifyListeners(evt)\n        if page > 50:\n            break\n        if keepGoing:\n            page += 1\n            wlurl = 'https://search.wikileaks.org/?query=%22' + qdata + '%22' + '&released_date_start=' + maxDate + '&include_external_sources=' + external + '&new_search=True&order_by=most_relevant&page=' + str(page) + '#results'\n            res = self.sf.fetchUrl(wlurl)\n            if not res:\n                break\n            if not res['content']:\n                break\n            links = dict()\n            p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'wikileaks.org')\n            if p:\n                links.update(p)\n            p = SpiderFootHelpers.extractLinksFromHtml(wlurl, res['content'], 'cryptome.org')\n            if p:\n                links.update(p)"
        ]
    }
]