[
    {
        "func_name": "__init__",
        "original": "def __init__(self, start_step=4, start_layer=10, layer_idx=None, step_idx=None, total_steps=50):\n    \"\"\"\n        Mutual self-attention control for Stable-Diffusion model\n        Args:\n            start_step: the step to start mutual self-attention control\n            start_layer: the layer to start mutual self-attention control\n            layer_idx: list of the layers to apply mutual self-attention control\n            step_idx: list the steps to apply mutual self-attention control\n            total_steps: the total number of steps\n        \"\"\"\n    super().__init__()\n    self.total_steps = total_steps\n    self.start_step = start_step\n    self.start_layer = start_layer\n    self.layer_idx = layer_idx if layer_idx is not None else list(range(start_layer, 16))\n    self.step_idx = step_idx if step_idx is not None else list(range(start_step, total_steps))\n    print('step_idx: ', self.step_idx)\n    print('layer_idx: ', self.layer_idx)",
        "mutated": [
            "def __init__(self, start_step=4, start_layer=10, layer_idx=None, step_idx=None, total_steps=50):\n    if False:\n        i = 10\n    '\\n        Mutual self-attention control for Stable-Diffusion model\\n        Args:\\n            start_step: the step to start mutual self-attention control\\n            start_layer: the layer to start mutual self-attention control\\n            layer_idx: list of the layers to apply mutual self-attention control\\n            step_idx: list the steps to apply mutual self-attention control\\n            total_steps: the total number of steps\\n        '\n    super().__init__()\n    self.total_steps = total_steps\n    self.start_step = start_step\n    self.start_layer = start_layer\n    self.layer_idx = layer_idx if layer_idx is not None else list(range(start_layer, 16))\n    self.step_idx = step_idx if step_idx is not None else list(range(start_step, total_steps))\n    print('step_idx: ', self.step_idx)\n    print('layer_idx: ', self.layer_idx)",
            "def __init__(self, start_step=4, start_layer=10, layer_idx=None, step_idx=None, total_steps=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Mutual self-attention control for Stable-Diffusion model\\n        Args:\\n            start_step: the step to start mutual self-attention control\\n            start_layer: the layer to start mutual self-attention control\\n            layer_idx: list of the layers to apply mutual self-attention control\\n            step_idx: list the steps to apply mutual self-attention control\\n            total_steps: the total number of steps\\n        '\n    super().__init__()\n    self.total_steps = total_steps\n    self.start_step = start_step\n    self.start_layer = start_layer\n    self.layer_idx = layer_idx if layer_idx is not None else list(range(start_layer, 16))\n    self.step_idx = step_idx if step_idx is not None else list(range(start_step, total_steps))\n    print('step_idx: ', self.step_idx)\n    print('layer_idx: ', self.layer_idx)",
            "def __init__(self, start_step=4, start_layer=10, layer_idx=None, step_idx=None, total_steps=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Mutual self-attention control for Stable-Diffusion model\\n        Args:\\n            start_step: the step to start mutual self-attention control\\n            start_layer: the layer to start mutual self-attention control\\n            layer_idx: list of the layers to apply mutual self-attention control\\n            step_idx: list the steps to apply mutual self-attention control\\n            total_steps: the total number of steps\\n        '\n    super().__init__()\n    self.total_steps = total_steps\n    self.start_step = start_step\n    self.start_layer = start_layer\n    self.layer_idx = layer_idx if layer_idx is not None else list(range(start_layer, 16))\n    self.step_idx = step_idx if step_idx is not None else list(range(start_step, total_steps))\n    print('step_idx: ', self.step_idx)\n    print('layer_idx: ', self.layer_idx)",
            "def __init__(self, start_step=4, start_layer=10, layer_idx=None, step_idx=None, total_steps=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Mutual self-attention control for Stable-Diffusion model\\n        Args:\\n            start_step: the step to start mutual self-attention control\\n            start_layer: the layer to start mutual self-attention control\\n            layer_idx: list of the layers to apply mutual self-attention control\\n            step_idx: list the steps to apply mutual self-attention control\\n            total_steps: the total number of steps\\n        '\n    super().__init__()\n    self.total_steps = total_steps\n    self.start_step = start_step\n    self.start_layer = start_layer\n    self.layer_idx = layer_idx if layer_idx is not None else list(range(start_layer, 16))\n    self.step_idx = step_idx if step_idx is not None else list(range(start_step, total_steps))\n    print('step_idx: ', self.step_idx)\n    print('layer_idx: ', self.layer_idx)",
            "def __init__(self, start_step=4, start_layer=10, layer_idx=None, step_idx=None, total_steps=50):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Mutual self-attention control for Stable-Diffusion model\\n        Args:\\n            start_step: the step to start mutual self-attention control\\n            start_layer: the layer to start mutual self-attention control\\n            layer_idx: list of the layers to apply mutual self-attention control\\n            step_idx: list the steps to apply mutual self-attention control\\n            total_steps: the total number of steps\\n        '\n    super().__init__()\n    self.total_steps = total_steps\n    self.start_step = start_step\n    self.start_layer = start_layer\n    self.layer_idx = layer_idx if layer_idx is not None else list(range(start_layer, 16))\n    self.step_idx = step_idx if step_idx is not None else list(range(start_step, total_steps))\n    print('step_idx: ', self.step_idx)\n    print('layer_idx: ', self.layer_idx)"
        ]
    },
    {
        "func_name": "attn_batch",
        "original": "def attn_batch(self, q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs):\n    b = q.shape[0] // num_heads\n    q = rearrange(q, '(b h) n d -> h (b n) d', h=num_heads)\n    k = rearrange(k, '(b h) n d -> h (b n) d', h=num_heads)\n    v = rearrange(v, '(b h) n d -> h (b n) d', h=num_heads)\n    sim = torch.einsum('h i d, h j d -> h i j', q, k) * kwargs.get('scale')\n    attn = sim.softmax(-1)\n    out = torch.einsum('h i j, h j d -> h i d', attn, v)\n    out = rearrange(out, 'h (b n) d -> b n (h d)', b=b)\n    return out",
        "mutated": [
            "def attn_batch(self, q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs):\n    if False:\n        i = 10\n    b = q.shape[0] // num_heads\n    q = rearrange(q, '(b h) n d -> h (b n) d', h=num_heads)\n    k = rearrange(k, '(b h) n d -> h (b n) d', h=num_heads)\n    v = rearrange(v, '(b h) n d -> h (b n) d', h=num_heads)\n    sim = torch.einsum('h i d, h j d -> h i j', q, k) * kwargs.get('scale')\n    attn = sim.softmax(-1)\n    out = torch.einsum('h i j, h j d -> h i d', attn, v)\n    out = rearrange(out, 'h (b n) d -> b n (h d)', b=b)\n    return out",
            "def attn_batch(self, q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    b = q.shape[0] // num_heads\n    q = rearrange(q, '(b h) n d -> h (b n) d', h=num_heads)\n    k = rearrange(k, '(b h) n d -> h (b n) d', h=num_heads)\n    v = rearrange(v, '(b h) n d -> h (b n) d', h=num_heads)\n    sim = torch.einsum('h i d, h j d -> h i j', q, k) * kwargs.get('scale')\n    attn = sim.softmax(-1)\n    out = torch.einsum('h i j, h j d -> h i d', attn, v)\n    out = rearrange(out, 'h (b n) d -> b n (h d)', b=b)\n    return out",
            "def attn_batch(self, q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    b = q.shape[0] // num_heads\n    q = rearrange(q, '(b h) n d -> h (b n) d', h=num_heads)\n    k = rearrange(k, '(b h) n d -> h (b n) d', h=num_heads)\n    v = rearrange(v, '(b h) n d -> h (b n) d', h=num_heads)\n    sim = torch.einsum('h i d, h j d -> h i j', q, k) * kwargs.get('scale')\n    attn = sim.softmax(-1)\n    out = torch.einsum('h i j, h j d -> h i d', attn, v)\n    out = rearrange(out, 'h (b n) d -> b n (h d)', b=b)\n    return out",
            "def attn_batch(self, q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    b = q.shape[0] // num_heads\n    q = rearrange(q, '(b h) n d -> h (b n) d', h=num_heads)\n    k = rearrange(k, '(b h) n d -> h (b n) d', h=num_heads)\n    v = rearrange(v, '(b h) n d -> h (b n) d', h=num_heads)\n    sim = torch.einsum('h i d, h j d -> h i j', q, k) * kwargs.get('scale')\n    attn = sim.softmax(-1)\n    out = torch.einsum('h i j, h j d -> h i d', attn, v)\n    out = rearrange(out, 'h (b n) d -> b n (h d)', b=b)\n    return out",
            "def attn_batch(self, q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    b = q.shape[0] // num_heads\n    q = rearrange(q, '(b h) n d -> h (b n) d', h=num_heads)\n    k = rearrange(k, '(b h) n d -> h (b n) d', h=num_heads)\n    v = rearrange(v, '(b h) n d -> h (b n) d', h=num_heads)\n    sim = torch.einsum('h i d, h j d -> h i j', q, k) * kwargs.get('scale')\n    attn = sim.softmax(-1)\n    out = torch.einsum('h i j, h j d -> h i d', attn, v)\n    out = rearrange(out, 'h (b n) d -> b n (h d)', b=b)\n    return out"
        ]
    },
    {
        "func_name": "forward",
        "original": "def forward(self, q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs):\n    \"\"\"\n        Attention forward function\n        \"\"\"\n    if is_cross or self.cur_step not in self.step_idx or self.cur_att_layer // 2 not in self.layer_idx:\n        return super().forward(q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs)\n    (qu, qc) = q.chunk(2)\n    (ku, kc) = k.chunk(2)\n    (vu, vc) = v.chunk(2)\n    (attnu, attnc) = attn.chunk(2)\n    out_u = self.attn_batch(qu, ku[:num_heads], vu[:num_heads], sim[:num_heads], attnu, is_cross, place_in_unet, num_heads, **kwargs)\n    out_c = self.attn_batch(qc, kc[:num_heads], vc[:num_heads], sim[:num_heads], attnc, is_cross, place_in_unet, num_heads, **kwargs)\n    out = torch.cat([out_u, out_c], dim=0)\n    return out",
        "mutated": [
            "def forward(self, q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs):\n    if False:\n        i = 10\n    '\\n        Attention forward function\\n        '\n    if is_cross or self.cur_step not in self.step_idx or self.cur_att_layer // 2 not in self.layer_idx:\n        return super().forward(q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs)\n    (qu, qc) = q.chunk(2)\n    (ku, kc) = k.chunk(2)\n    (vu, vc) = v.chunk(2)\n    (attnu, attnc) = attn.chunk(2)\n    out_u = self.attn_batch(qu, ku[:num_heads], vu[:num_heads], sim[:num_heads], attnu, is_cross, place_in_unet, num_heads, **kwargs)\n    out_c = self.attn_batch(qc, kc[:num_heads], vc[:num_heads], sim[:num_heads], attnc, is_cross, place_in_unet, num_heads, **kwargs)\n    out = torch.cat([out_u, out_c], dim=0)\n    return out",
            "def forward(self, q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Attention forward function\\n        '\n    if is_cross or self.cur_step not in self.step_idx or self.cur_att_layer // 2 not in self.layer_idx:\n        return super().forward(q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs)\n    (qu, qc) = q.chunk(2)\n    (ku, kc) = k.chunk(2)\n    (vu, vc) = v.chunk(2)\n    (attnu, attnc) = attn.chunk(2)\n    out_u = self.attn_batch(qu, ku[:num_heads], vu[:num_heads], sim[:num_heads], attnu, is_cross, place_in_unet, num_heads, **kwargs)\n    out_c = self.attn_batch(qc, kc[:num_heads], vc[:num_heads], sim[:num_heads], attnc, is_cross, place_in_unet, num_heads, **kwargs)\n    out = torch.cat([out_u, out_c], dim=0)\n    return out",
            "def forward(self, q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Attention forward function\\n        '\n    if is_cross or self.cur_step not in self.step_idx or self.cur_att_layer // 2 not in self.layer_idx:\n        return super().forward(q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs)\n    (qu, qc) = q.chunk(2)\n    (ku, kc) = k.chunk(2)\n    (vu, vc) = v.chunk(2)\n    (attnu, attnc) = attn.chunk(2)\n    out_u = self.attn_batch(qu, ku[:num_heads], vu[:num_heads], sim[:num_heads], attnu, is_cross, place_in_unet, num_heads, **kwargs)\n    out_c = self.attn_batch(qc, kc[:num_heads], vc[:num_heads], sim[:num_heads], attnc, is_cross, place_in_unet, num_heads, **kwargs)\n    out = torch.cat([out_u, out_c], dim=0)\n    return out",
            "def forward(self, q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Attention forward function\\n        '\n    if is_cross or self.cur_step not in self.step_idx or self.cur_att_layer // 2 not in self.layer_idx:\n        return super().forward(q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs)\n    (qu, qc) = q.chunk(2)\n    (ku, kc) = k.chunk(2)\n    (vu, vc) = v.chunk(2)\n    (attnu, attnc) = attn.chunk(2)\n    out_u = self.attn_batch(qu, ku[:num_heads], vu[:num_heads], sim[:num_heads], attnu, is_cross, place_in_unet, num_heads, **kwargs)\n    out_c = self.attn_batch(qc, kc[:num_heads], vc[:num_heads], sim[:num_heads], attnc, is_cross, place_in_unet, num_heads, **kwargs)\n    out = torch.cat([out_u, out_c], dim=0)\n    return out",
            "def forward(self, q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Attention forward function\\n        '\n    if is_cross or self.cur_step not in self.step_idx or self.cur_att_layer // 2 not in self.layer_idx:\n        return super().forward(q, k, v, sim, attn, is_cross, place_in_unet, num_heads, **kwargs)\n    (qu, qc) = q.chunk(2)\n    (ku, kc) = k.chunk(2)\n    (vu, vc) = v.chunk(2)\n    (attnu, attnc) = attn.chunk(2)\n    out_u = self.attn_batch(qu, ku[:num_heads], vu[:num_heads], sim[:num_heads], attnu, is_cross, place_in_unet, num_heads, **kwargs)\n    out_c = self.attn_batch(qc, kc[:num_heads], vc[:num_heads], sim[:num_heads], attnc, is_cross, place_in_unet, num_heads, **kwargs)\n    out = torch.cat([out_u, out_c], dim=0)\n    return out"
        ]
    }
]