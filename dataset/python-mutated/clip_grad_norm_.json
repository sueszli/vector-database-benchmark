[
    {
        "func_name": "clip_grad_norm_",
        "original": "@paddle.autograd.no_grad()\ndef clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False):\n    \"\"\"Clips gradient norm of the iteratable parameters.\n\n    Norms are calculated together on all gradients, just as they are\n    connected into one vector. The gradient will be modified in place.\n\n    This API can only run in dynamic graph mode, not static graph mode.\n\n    Args:\n        parameters (Iterable[paddle.Tensor] or paddle.Tensor): Tensors or a single Tensor\n            that will be normalized gradients\n        max_norm (float or int): max norm of the gradients\n        norm_type (float or int): type of the used p-norm. Can be `inf` for\n            infinity norm.\n        error_if_nonfinite (bool): if True, throw an error if the total\n            norm of the gradients from :attr:`parameters` is `nan`,\n            `inf`, or `-inf`.\n\n    Returns:\n        Total norm of the parameter gradients (treated as a single vector).\n\n    Example:\n        .. code-block:: python\n\n            >>> import paddle\n\n            >>> x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')\n            >>> max_norm = float(5.0)\n            >>> linear = paddle.nn.Linear(in_features=10, out_features=10)\n            >>> out = linear(x)\n            >>> loss = paddle.mean(out)\n            >>> loss.backward()\n\n            >>> paddle.nn.utils.clip_grad_norm_(linear.parameters(), max_norm)\n\n            >>> sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters())\n            >>> sdg.step()\n    \"\"\"\n    if not paddle.in_dynamic_mode():\n        raise RuntimeError('this API can only run in dynamic mode.')\n    if isinstance(parameters, paddle.Tensor):\n        parameters = [parameters]\n    support_norm_type = [float('inf'), 0, 1, 2]\n    if norm_type not in support_norm_type:\n        raise ValueError(f'norm_type only support {support_norm_type}')\n    grads = [p.grad for p in parameters if p.grad is not None]\n    max_norm = float(max_norm)\n    norm_type = float(norm_type)\n    if len(grads) == 0:\n        return paddle.to_tensor(0.0)\n    if norm_type == float('inf'):\n        norms = [g.detach().abs().max() for g in grads]\n        total_norm = norms[0] if len(norms) == 1 else paddle.max(paddle.stack(norms))\n    else:\n        total_norm = paddle.linalg.norm(paddle.stack([paddle.linalg.norm(g.detach(), norm_type) for g in grads]), norm_type)\n    if error_if_nonfinite and paddle.logical_or(total_norm.isnan(), total_norm.isinf()):\n        raise RuntimeError(f'The total norm of {norm_type} order of the gradients from `parameters` is non-finite, so it cannot be clipped. In any case, disable this error and scale the gradient by non-finite norm, set `error_if_nonfinite=False`')\n    clip_coef = max_norm / (total_norm + 1e-06)\n    clip_coef_clamped = clip_coef.clip_(max=1.0)\n    for (_, p) in enumerate(parameters):\n        if p.grad is not None:\n            p.grad = paddle.multiply(x=p.grad, y=clip_coef_clamped)\n    return total_norm",
        "mutated": [
            "@paddle.autograd.no_grad()\ndef clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False):\n    if False:\n        i = 10\n    \"Clips gradient norm of the iteratable parameters.\\n\\n    Norms are calculated together on all gradients, just as they are\\n    connected into one vector. The gradient will be modified in place.\\n\\n    This API can only run in dynamic graph mode, not static graph mode.\\n\\n    Args:\\n        parameters (Iterable[paddle.Tensor] or paddle.Tensor): Tensors or a single Tensor\\n            that will be normalized gradients\\n        max_norm (float or int): max norm of the gradients\\n        norm_type (float or int): type of the used p-norm. Can be `inf` for\\n            infinity norm.\\n        error_if_nonfinite (bool): if True, throw an error if the total\\n            norm of the gradients from :attr:`parameters` is `nan`,\\n            `inf`, or `-inf`.\\n\\n    Returns:\\n        Total norm of the parameter gradients (treated as a single vector).\\n\\n    Example:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')\\n            >>> max_norm = float(5.0)\\n            >>> linear = paddle.nn.Linear(in_features=10, out_features=10)\\n            >>> out = linear(x)\\n            >>> loss = paddle.mean(out)\\n            >>> loss.backward()\\n\\n            >>> paddle.nn.utils.clip_grad_norm_(linear.parameters(), max_norm)\\n\\n            >>> sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters())\\n            >>> sdg.step()\\n    \"\n    if not paddle.in_dynamic_mode():\n        raise RuntimeError('this API can only run in dynamic mode.')\n    if isinstance(parameters, paddle.Tensor):\n        parameters = [parameters]\n    support_norm_type = [float('inf'), 0, 1, 2]\n    if norm_type not in support_norm_type:\n        raise ValueError(f'norm_type only support {support_norm_type}')\n    grads = [p.grad for p in parameters if p.grad is not None]\n    max_norm = float(max_norm)\n    norm_type = float(norm_type)\n    if len(grads) == 0:\n        return paddle.to_tensor(0.0)\n    if norm_type == float('inf'):\n        norms = [g.detach().abs().max() for g in grads]\n        total_norm = norms[0] if len(norms) == 1 else paddle.max(paddle.stack(norms))\n    else:\n        total_norm = paddle.linalg.norm(paddle.stack([paddle.linalg.norm(g.detach(), norm_type) for g in grads]), norm_type)\n    if error_if_nonfinite and paddle.logical_or(total_norm.isnan(), total_norm.isinf()):\n        raise RuntimeError(f'The total norm of {norm_type} order of the gradients from `parameters` is non-finite, so it cannot be clipped. In any case, disable this error and scale the gradient by non-finite norm, set `error_if_nonfinite=False`')\n    clip_coef = max_norm / (total_norm + 1e-06)\n    clip_coef_clamped = clip_coef.clip_(max=1.0)\n    for (_, p) in enumerate(parameters):\n        if p.grad is not None:\n            p.grad = paddle.multiply(x=p.grad, y=clip_coef_clamped)\n    return total_norm",
            "@paddle.autograd.no_grad()\ndef clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Clips gradient norm of the iteratable parameters.\\n\\n    Norms are calculated together on all gradients, just as they are\\n    connected into one vector. The gradient will be modified in place.\\n\\n    This API can only run in dynamic graph mode, not static graph mode.\\n\\n    Args:\\n        parameters (Iterable[paddle.Tensor] or paddle.Tensor): Tensors or a single Tensor\\n            that will be normalized gradients\\n        max_norm (float or int): max norm of the gradients\\n        norm_type (float or int): type of the used p-norm. Can be `inf` for\\n            infinity norm.\\n        error_if_nonfinite (bool): if True, throw an error if the total\\n            norm of the gradients from :attr:`parameters` is `nan`,\\n            `inf`, or `-inf`.\\n\\n    Returns:\\n        Total norm of the parameter gradients (treated as a single vector).\\n\\n    Example:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')\\n            >>> max_norm = float(5.0)\\n            >>> linear = paddle.nn.Linear(in_features=10, out_features=10)\\n            >>> out = linear(x)\\n            >>> loss = paddle.mean(out)\\n            >>> loss.backward()\\n\\n            >>> paddle.nn.utils.clip_grad_norm_(linear.parameters(), max_norm)\\n\\n            >>> sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters())\\n            >>> sdg.step()\\n    \"\n    if not paddle.in_dynamic_mode():\n        raise RuntimeError('this API can only run in dynamic mode.')\n    if isinstance(parameters, paddle.Tensor):\n        parameters = [parameters]\n    support_norm_type = [float('inf'), 0, 1, 2]\n    if norm_type not in support_norm_type:\n        raise ValueError(f'norm_type only support {support_norm_type}')\n    grads = [p.grad for p in parameters if p.grad is not None]\n    max_norm = float(max_norm)\n    norm_type = float(norm_type)\n    if len(grads) == 0:\n        return paddle.to_tensor(0.0)\n    if norm_type == float('inf'):\n        norms = [g.detach().abs().max() for g in grads]\n        total_norm = norms[0] if len(norms) == 1 else paddle.max(paddle.stack(norms))\n    else:\n        total_norm = paddle.linalg.norm(paddle.stack([paddle.linalg.norm(g.detach(), norm_type) for g in grads]), norm_type)\n    if error_if_nonfinite and paddle.logical_or(total_norm.isnan(), total_norm.isinf()):\n        raise RuntimeError(f'The total norm of {norm_type} order of the gradients from `parameters` is non-finite, so it cannot be clipped. In any case, disable this error and scale the gradient by non-finite norm, set `error_if_nonfinite=False`')\n    clip_coef = max_norm / (total_norm + 1e-06)\n    clip_coef_clamped = clip_coef.clip_(max=1.0)\n    for (_, p) in enumerate(parameters):\n        if p.grad is not None:\n            p.grad = paddle.multiply(x=p.grad, y=clip_coef_clamped)\n    return total_norm",
            "@paddle.autograd.no_grad()\ndef clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Clips gradient norm of the iteratable parameters.\\n\\n    Norms are calculated together on all gradients, just as they are\\n    connected into one vector. The gradient will be modified in place.\\n\\n    This API can only run in dynamic graph mode, not static graph mode.\\n\\n    Args:\\n        parameters (Iterable[paddle.Tensor] or paddle.Tensor): Tensors or a single Tensor\\n            that will be normalized gradients\\n        max_norm (float or int): max norm of the gradients\\n        norm_type (float or int): type of the used p-norm. Can be `inf` for\\n            infinity norm.\\n        error_if_nonfinite (bool): if True, throw an error if the total\\n            norm of the gradients from :attr:`parameters` is `nan`,\\n            `inf`, or `-inf`.\\n\\n    Returns:\\n        Total norm of the parameter gradients (treated as a single vector).\\n\\n    Example:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')\\n            >>> max_norm = float(5.0)\\n            >>> linear = paddle.nn.Linear(in_features=10, out_features=10)\\n            >>> out = linear(x)\\n            >>> loss = paddle.mean(out)\\n            >>> loss.backward()\\n\\n            >>> paddle.nn.utils.clip_grad_norm_(linear.parameters(), max_norm)\\n\\n            >>> sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters())\\n            >>> sdg.step()\\n    \"\n    if not paddle.in_dynamic_mode():\n        raise RuntimeError('this API can only run in dynamic mode.')\n    if isinstance(parameters, paddle.Tensor):\n        parameters = [parameters]\n    support_norm_type = [float('inf'), 0, 1, 2]\n    if norm_type not in support_norm_type:\n        raise ValueError(f'norm_type only support {support_norm_type}')\n    grads = [p.grad for p in parameters if p.grad is not None]\n    max_norm = float(max_norm)\n    norm_type = float(norm_type)\n    if len(grads) == 0:\n        return paddle.to_tensor(0.0)\n    if norm_type == float('inf'):\n        norms = [g.detach().abs().max() for g in grads]\n        total_norm = norms[0] if len(norms) == 1 else paddle.max(paddle.stack(norms))\n    else:\n        total_norm = paddle.linalg.norm(paddle.stack([paddle.linalg.norm(g.detach(), norm_type) for g in grads]), norm_type)\n    if error_if_nonfinite and paddle.logical_or(total_norm.isnan(), total_norm.isinf()):\n        raise RuntimeError(f'The total norm of {norm_type} order of the gradients from `parameters` is non-finite, so it cannot be clipped. In any case, disable this error and scale the gradient by non-finite norm, set `error_if_nonfinite=False`')\n    clip_coef = max_norm / (total_norm + 1e-06)\n    clip_coef_clamped = clip_coef.clip_(max=1.0)\n    for (_, p) in enumerate(parameters):\n        if p.grad is not None:\n            p.grad = paddle.multiply(x=p.grad, y=clip_coef_clamped)\n    return total_norm",
            "@paddle.autograd.no_grad()\ndef clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Clips gradient norm of the iteratable parameters.\\n\\n    Norms are calculated together on all gradients, just as they are\\n    connected into one vector. The gradient will be modified in place.\\n\\n    This API can only run in dynamic graph mode, not static graph mode.\\n\\n    Args:\\n        parameters (Iterable[paddle.Tensor] or paddle.Tensor): Tensors or a single Tensor\\n            that will be normalized gradients\\n        max_norm (float or int): max norm of the gradients\\n        norm_type (float or int): type of the used p-norm. Can be `inf` for\\n            infinity norm.\\n        error_if_nonfinite (bool): if True, throw an error if the total\\n            norm of the gradients from :attr:`parameters` is `nan`,\\n            `inf`, or `-inf`.\\n\\n    Returns:\\n        Total norm of the parameter gradients (treated as a single vector).\\n\\n    Example:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')\\n            >>> max_norm = float(5.0)\\n            >>> linear = paddle.nn.Linear(in_features=10, out_features=10)\\n            >>> out = linear(x)\\n            >>> loss = paddle.mean(out)\\n            >>> loss.backward()\\n\\n            >>> paddle.nn.utils.clip_grad_norm_(linear.parameters(), max_norm)\\n\\n            >>> sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters())\\n            >>> sdg.step()\\n    \"\n    if not paddle.in_dynamic_mode():\n        raise RuntimeError('this API can only run in dynamic mode.')\n    if isinstance(parameters, paddle.Tensor):\n        parameters = [parameters]\n    support_norm_type = [float('inf'), 0, 1, 2]\n    if norm_type not in support_norm_type:\n        raise ValueError(f'norm_type only support {support_norm_type}')\n    grads = [p.grad for p in parameters if p.grad is not None]\n    max_norm = float(max_norm)\n    norm_type = float(norm_type)\n    if len(grads) == 0:\n        return paddle.to_tensor(0.0)\n    if norm_type == float('inf'):\n        norms = [g.detach().abs().max() for g in grads]\n        total_norm = norms[0] if len(norms) == 1 else paddle.max(paddle.stack(norms))\n    else:\n        total_norm = paddle.linalg.norm(paddle.stack([paddle.linalg.norm(g.detach(), norm_type) for g in grads]), norm_type)\n    if error_if_nonfinite and paddle.logical_or(total_norm.isnan(), total_norm.isinf()):\n        raise RuntimeError(f'The total norm of {norm_type} order of the gradients from `parameters` is non-finite, so it cannot be clipped. In any case, disable this error and scale the gradient by non-finite norm, set `error_if_nonfinite=False`')\n    clip_coef = max_norm / (total_norm + 1e-06)\n    clip_coef_clamped = clip_coef.clip_(max=1.0)\n    for (_, p) in enumerate(parameters):\n        if p.grad is not None:\n            p.grad = paddle.multiply(x=p.grad, y=clip_coef_clamped)\n    return total_norm",
            "@paddle.autograd.no_grad()\ndef clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Clips gradient norm of the iteratable parameters.\\n\\n    Norms are calculated together on all gradients, just as they are\\n    connected into one vector. The gradient will be modified in place.\\n\\n    This API can only run in dynamic graph mode, not static graph mode.\\n\\n    Args:\\n        parameters (Iterable[paddle.Tensor] or paddle.Tensor): Tensors or a single Tensor\\n            that will be normalized gradients\\n        max_norm (float or int): max norm of the gradients\\n        norm_type (float or int): type of the used p-norm. Can be `inf` for\\n            infinity norm.\\n        error_if_nonfinite (bool): if True, throw an error if the total\\n            norm of the gradients from :attr:`parameters` is `nan`,\\n            `inf`, or `-inf`.\\n\\n    Returns:\\n        Total norm of the parameter gradients (treated as a single vector).\\n\\n    Example:\\n        .. code-block:: python\\n\\n            >>> import paddle\\n\\n            >>> x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')\\n            >>> max_norm = float(5.0)\\n            >>> linear = paddle.nn.Linear(in_features=10, out_features=10)\\n            >>> out = linear(x)\\n            >>> loss = paddle.mean(out)\\n            >>> loss.backward()\\n\\n            >>> paddle.nn.utils.clip_grad_norm_(linear.parameters(), max_norm)\\n\\n            >>> sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters())\\n            >>> sdg.step()\\n    \"\n    if not paddle.in_dynamic_mode():\n        raise RuntimeError('this API can only run in dynamic mode.')\n    if isinstance(parameters, paddle.Tensor):\n        parameters = [parameters]\n    support_norm_type = [float('inf'), 0, 1, 2]\n    if norm_type not in support_norm_type:\n        raise ValueError(f'norm_type only support {support_norm_type}')\n    grads = [p.grad for p in parameters if p.grad is not None]\n    max_norm = float(max_norm)\n    norm_type = float(norm_type)\n    if len(grads) == 0:\n        return paddle.to_tensor(0.0)\n    if norm_type == float('inf'):\n        norms = [g.detach().abs().max() for g in grads]\n        total_norm = norms[0] if len(norms) == 1 else paddle.max(paddle.stack(norms))\n    else:\n        total_norm = paddle.linalg.norm(paddle.stack([paddle.linalg.norm(g.detach(), norm_type) for g in grads]), norm_type)\n    if error_if_nonfinite and paddle.logical_or(total_norm.isnan(), total_norm.isinf()):\n        raise RuntimeError(f'The total norm of {norm_type} order of the gradients from `parameters` is non-finite, so it cannot be clipped. In any case, disable this error and scale the gradient by non-finite norm, set `error_if_nonfinite=False`')\n    clip_coef = max_norm / (total_norm + 1e-06)\n    clip_coef_clamped = clip_coef.clip_(max=1.0)\n    for (_, p) in enumerate(parameters):\n        if p.grad is not None:\n            p.grad = paddle.multiply(x=p.grad, y=clip_coef_clamped)\n    return total_norm"
        ]
    }
]