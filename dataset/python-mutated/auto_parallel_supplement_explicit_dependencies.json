[
    {
        "func_name": "_sharding_pass_applied",
        "original": "def _sharding_pass_applied(pass_ctx):\n    for applied_pass in pass_ctx.passes:\n        if isinstance(applied_pass, ShardingPass):\n            return True\n    return False",
        "mutated": [
            "def _sharding_pass_applied(pass_ctx):\n    if False:\n        i = 10\n    for applied_pass in pass_ctx.passes:\n        if isinstance(applied_pass, ShardingPass):\n            return True\n    return False",
            "def _sharding_pass_applied(pass_ctx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    for applied_pass in pass_ctx.passes:\n        if isinstance(applied_pass, ShardingPass):\n            return True\n    return False",
            "def _sharding_pass_applied(pass_ctx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    for applied_pass in pass_ctx.passes:\n        if isinstance(applied_pass, ShardingPass):\n            return True\n    return False",
            "def _sharding_pass_applied(pass_ctx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    for applied_pass in pass_ctx.passes:\n        if isinstance(applied_pass, ShardingPass):\n            return True\n    return False",
            "def _sharding_pass_applied(pass_ctx):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    for applied_pass in pass_ctx.passes:\n        if isinstance(applied_pass, ShardingPass):\n            return True\n    return False"
        ]
    },
    {
        "func_name": "__init__",
        "original": "def __init__(self):\n    super().__init__()\n    self.set_attr('dist_context', None)",
        "mutated": [
            "def __init__(self):\n    if False:\n        i = 10\n    super().__init__()\n    self.set_attr('dist_context', None)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    super().__init__()\n    self.set_attr('dist_context', None)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    super().__init__()\n    self.set_attr('dist_context', None)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    super().__init__()\n    self.set_attr('dist_context', None)",
            "def __init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    super().__init__()\n    self.set_attr('dist_context', None)"
        ]
    },
    {
        "func_name": "_check_self",
        "original": "def _check_self(self):\n    if self.get_attr('dist_context') is None:\n        return False\n    return True",
        "mutated": [
            "def _check_self(self):\n    if False:\n        i = 10\n    if self.get_attr('dist_context') is None:\n        return False\n    return True",
            "def _check_self(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if self.get_attr('dist_context') is None:\n        return False\n    return True",
            "def _check_self(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if self.get_attr('dist_context') is None:\n        return False\n    return True",
            "def _check_self(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if self.get_attr('dist_context') is None:\n        return False\n    return True",
            "def _check_self(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if self.get_attr('dist_context') is None:\n        return False\n    return True"
        ]
    },
    {
        "func_name": "_check_conflict",
        "original": "def _check_conflict(self, other_pass):\n    return True",
        "mutated": [
            "def _check_conflict(self, other_pass):\n    if False:\n        i = 10\n    return True",
            "def _check_conflict(self, other_pass):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    return True",
            "def _check_conflict(self, other_pass):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    return True",
            "def _check_conflict(self, other_pass):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    return True",
            "def _check_conflict(self, other_pass):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    return True"
        ]
    },
    {
        "func_name": "_apply_single_impl",
        "original": "def _apply_single_impl(self, main_program, startup_program, context):\n    if not _sharding_pass_applied(context):\n        return\n    self._dist_context = self.get_attr('dist_context', None)\n    self.flags_sync_stream = 'flags_sync_stream'\n    main_block = main_program.global_block()\n    startup_block = startup_program.global_block()\n    last_dp_reduce_op_idx = -1\n    last_dp_reduce_varname = None\n    for (idx, op) in reversed(list(enumerate(main_block.ops))):\n        if is_data_parallel_reduce_op(op):\n            last_dp_reduce_op_idx = idx\n            last_dp_reduce_varname = op.output_arg_names[0]\n            break\n    assert last_dp_reduce_op_idx > 0\n    assert last_dp_reduce_varname is not None\n    deps_map = {}\n    prior_varname = last_dp_reduce_varname\n    for (idx, op) in enumerate(main_block.ops):\n        if is_amp_flag_sync_op(op) or is_global_norm_sync_op(op):\n            op_namescope = None\n            if is_amp_flag_sync_op(op):\n                op_namescope = 'amp_flag_sync_dep'\n                op.dist_attr.execution_stream = self.flags_sync_stream\n            elif is_global_norm_sync_op(op):\n                op_namescope = 'global_norm_sync_dep'\n            deps_map[idx] = (prior_varname, op.input('X')[0], op_namescope)\n            prior_varname = op.output('Out')[0]\n    first_check_op = True\n    for (idx, op) in enumerate(main_block.ops):\n        if op.type == 'check_finite_and_unscale':\n            if first_check_op:\n                last_backward_op = None\n                for last_idx in range(idx - 1, 0, -1):\n                    if not is_comm_op(main_block.ops[last_idx]):\n                        last_backward_op = main_block.ops[last_idx]\n                        break\n                prior_varname = last_backward_op.output_arg_names[0]\n                first_check_op = False\n            deps_map[idx] = (prior_varname, op.input('Scale')[0], 'check_finite_dep')\n    first_optimizer_op = True\n    for (idx, op) in enumerate(main_block.ops):\n        if op.type in _supported_optimizer_type:\n            if first_optimizer_op:\n                first_optimizer_op = False\n            else:\n                deps_map[idx] = (prior_varname, op.input('Param')[0], 'optimizer_order_dep')\n            prior_varname = op.output('ParamOut')[0]\n    indice = sorted(deps_map.keys(), reverse=True)\n    for idx in indice:\n        prior_var = main_block.var(deps_map[idx][0])\n        post_var = main_block.var(deps_map[idx][1])\n        op_namescope = deps_map[idx][2]\n        depend_op = insert_dependencies_for_vars(main_block, idx, prior_var, post_var, self._dist_context, OpRole.Optimize, is_recompute=False, sync=False, op_namescope=op_namescope)\n    main_block._sync_with_cpp()",
        "mutated": [
            "def _apply_single_impl(self, main_program, startup_program, context):\n    if False:\n        i = 10\n    if not _sharding_pass_applied(context):\n        return\n    self._dist_context = self.get_attr('dist_context', None)\n    self.flags_sync_stream = 'flags_sync_stream'\n    main_block = main_program.global_block()\n    startup_block = startup_program.global_block()\n    last_dp_reduce_op_idx = -1\n    last_dp_reduce_varname = None\n    for (idx, op) in reversed(list(enumerate(main_block.ops))):\n        if is_data_parallel_reduce_op(op):\n            last_dp_reduce_op_idx = idx\n            last_dp_reduce_varname = op.output_arg_names[0]\n            break\n    assert last_dp_reduce_op_idx > 0\n    assert last_dp_reduce_varname is not None\n    deps_map = {}\n    prior_varname = last_dp_reduce_varname\n    for (idx, op) in enumerate(main_block.ops):\n        if is_amp_flag_sync_op(op) or is_global_norm_sync_op(op):\n            op_namescope = None\n            if is_amp_flag_sync_op(op):\n                op_namescope = 'amp_flag_sync_dep'\n                op.dist_attr.execution_stream = self.flags_sync_stream\n            elif is_global_norm_sync_op(op):\n                op_namescope = 'global_norm_sync_dep'\n            deps_map[idx] = (prior_varname, op.input('X')[0], op_namescope)\n            prior_varname = op.output('Out')[0]\n    first_check_op = True\n    for (idx, op) in enumerate(main_block.ops):\n        if op.type == 'check_finite_and_unscale':\n            if first_check_op:\n                last_backward_op = None\n                for last_idx in range(idx - 1, 0, -1):\n                    if not is_comm_op(main_block.ops[last_idx]):\n                        last_backward_op = main_block.ops[last_idx]\n                        break\n                prior_varname = last_backward_op.output_arg_names[0]\n                first_check_op = False\n            deps_map[idx] = (prior_varname, op.input('Scale')[0], 'check_finite_dep')\n    first_optimizer_op = True\n    for (idx, op) in enumerate(main_block.ops):\n        if op.type in _supported_optimizer_type:\n            if first_optimizer_op:\n                first_optimizer_op = False\n            else:\n                deps_map[idx] = (prior_varname, op.input('Param')[0], 'optimizer_order_dep')\n            prior_varname = op.output('ParamOut')[0]\n    indice = sorted(deps_map.keys(), reverse=True)\n    for idx in indice:\n        prior_var = main_block.var(deps_map[idx][0])\n        post_var = main_block.var(deps_map[idx][1])\n        op_namescope = deps_map[idx][2]\n        depend_op = insert_dependencies_for_vars(main_block, idx, prior_var, post_var, self._dist_context, OpRole.Optimize, is_recompute=False, sync=False, op_namescope=op_namescope)\n    main_block._sync_with_cpp()",
            "def _apply_single_impl(self, main_program, startup_program, context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if not _sharding_pass_applied(context):\n        return\n    self._dist_context = self.get_attr('dist_context', None)\n    self.flags_sync_stream = 'flags_sync_stream'\n    main_block = main_program.global_block()\n    startup_block = startup_program.global_block()\n    last_dp_reduce_op_idx = -1\n    last_dp_reduce_varname = None\n    for (idx, op) in reversed(list(enumerate(main_block.ops))):\n        if is_data_parallel_reduce_op(op):\n            last_dp_reduce_op_idx = idx\n            last_dp_reduce_varname = op.output_arg_names[0]\n            break\n    assert last_dp_reduce_op_idx > 0\n    assert last_dp_reduce_varname is not None\n    deps_map = {}\n    prior_varname = last_dp_reduce_varname\n    for (idx, op) in enumerate(main_block.ops):\n        if is_amp_flag_sync_op(op) or is_global_norm_sync_op(op):\n            op_namescope = None\n            if is_amp_flag_sync_op(op):\n                op_namescope = 'amp_flag_sync_dep'\n                op.dist_attr.execution_stream = self.flags_sync_stream\n            elif is_global_norm_sync_op(op):\n                op_namescope = 'global_norm_sync_dep'\n            deps_map[idx] = (prior_varname, op.input('X')[0], op_namescope)\n            prior_varname = op.output('Out')[0]\n    first_check_op = True\n    for (idx, op) in enumerate(main_block.ops):\n        if op.type == 'check_finite_and_unscale':\n            if first_check_op:\n                last_backward_op = None\n                for last_idx in range(idx - 1, 0, -1):\n                    if not is_comm_op(main_block.ops[last_idx]):\n                        last_backward_op = main_block.ops[last_idx]\n                        break\n                prior_varname = last_backward_op.output_arg_names[0]\n                first_check_op = False\n            deps_map[idx] = (prior_varname, op.input('Scale')[0], 'check_finite_dep')\n    first_optimizer_op = True\n    for (idx, op) in enumerate(main_block.ops):\n        if op.type in _supported_optimizer_type:\n            if first_optimizer_op:\n                first_optimizer_op = False\n            else:\n                deps_map[idx] = (prior_varname, op.input('Param')[0], 'optimizer_order_dep')\n            prior_varname = op.output('ParamOut')[0]\n    indice = sorted(deps_map.keys(), reverse=True)\n    for idx in indice:\n        prior_var = main_block.var(deps_map[idx][0])\n        post_var = main_block.var(deps_map[idx][1])\n        op_namescope = deps_map[idx][2]\n        depend_op = insert_dependencies_for_vars(main_block, idx, prior_var, post_var, self._dist_context, OpRole.Optimize, is_recompute=False, sync=False, op_namescope=op_namescope)\n    main_block._sync_with_cpp()",
            "def _apply_single_impl(self, main_program, startup_program, context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if not _sharding_pass_applied(context):\n        return\n    self._dist_context = self.get_attr('dist_context', None)\n    self.flags_sync_stream = 'flags_sync_stream'\n    main_block = main_program.global_block()\n    startup_block = startup_program.global_block()\n    last_dp_reduce_op_idx = -1\n    last_dp_reduce_varname = None\n    for (idx, op) in reversed(list(enumerate(main_block.ops))):\n        if is_data_parallel_reduce_op(op):\n            last_dp_reduce_op_idx = idx\n            last_dp_reduce_varname = op.output_arg_names[0]\n            break\n    assert last_dp_reduce_op_idx > 0\n    assert last_dp_reduce_varname is not None\n    deps_map = {}\n    prior_varname = last_dp_reduce_varname\n    for (idx, op) in enumerate(main_block.ops):\n        if is_amp_flag_sync_op(op) or is_global_norm_sync_op(op):\n            op_namescope = None\n            if is_amp_flag_sync_op(op):\n                op_namescope = 'amp_flag_sync_dep'\n                op.dist_attr.execution_stream = self.flags_sync_stream\n            elif is_global_norm_sync_op(op):\n                op_namescope = 'global_norm_sync_dep'\n            deps_map[idx] = (prior_varname, op.input('X')[0], op_namescope)\n            prior_varname = op.output('Out')[0]\n    first_check_op = True\n    for (idx, op) in enumerate(main_block.ops):\n        if op.type == 'check_finite_and_unscale':\n            if first_check_op:\n                last_backward_op = None\n                for last_idx in range(idx - 1, 0, -1):\n                    if not is_comm_op(main_block.ops[last_idx]):\n                        last_backward_op = main_block.ops[last_idx]\n                        break\n                prior_varname = last_backward_op.output_arg_names[0]\n                first_check_op = False\n            deps_map[idx] = (prior_varname, op.input('Scale')[0], 'check_finite_dep')\n    first_optimizer_op = True\n    for (idx, op) in enumerate(main_block.ops):\n        if op.type in _supported_optimizer_type:\n            if first_optimizer_op:\n                first_optimizer_op = False\n            else:\n                deps_map[idx] = (prior_varname, op.input('Param')[0], 'optimizer_order_dep')\n            prior_varname = op.output('ParamOut')[0]\n    indice = sorted(deps_map.keys(), reverse=True)\n    for idx in indice:\n        prior_var = main_block.var(deps_map[idx][0])\n        post_var = main_block.var(deps_map[idx][1])\n        op_namescope = deps_map[idx][2]\n        depend_op = insert_dependencies_for_vars(main_block, idx, prior_var, post_var, self._dist_context, OpRole.Optimize, is_recompute=False, sync=False, op_namescope=op_namescope)\n    main_block._sync_with_cpp()",
            "def _apply_single_impl(self, main_program, startup_program, context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if not _sharding_pass_applied(context):\n        return\n    self._dist_context = self.get_attr('dist_context', None)\n    self.flags_sync_stream = 'flags_sync_stream'\n    main_block = main_program.global_block()\n    startup_block = startup_program.global_block()\n    last_dp_reduce_op_idx = -1\n    last_dp_reduce_varname = None\n    for (idx, op) in reversed(list(enumerate(main_block.ops))):\n        if is_data_parallel_reduce_op(op):\n            last_dp_reduce_op_idx = idx\n            last_dp_reduce_varname = op.output_arg_names[0]\n            break\n    assert last_dp_reduce_op_idx > 0\n    assert last_dp_reduce_varname is not None\n    deps_map = {}\n    prior_varname = last_dp_reduce_varname\n    for (idx, op) in enumerate(main_block.ops):\n        if is_amp_flag_sync_op(op) or is_global_norm_sync_op(op):\n            op_namescope = None\n            if is_amp_flag_sync_op(op):\n                op_namescope = 'amp_flag_sync_dep'\n                op.dist_attr.execution_stream = self.flags_sync_stream\n            elif is_global_norm_sync_op(op):\n                op_namescope = 'global_norm_sync_dep'\n            deps_map[idx] = (prior_varname, op.input('X')[0], op_namescope)\n            prior_varname = op.output('Out')[0]\n    first_check_op = True\n    for (idx, op) in enumerate(main_block.ops):\n        if op.type == 'check_finite_and_unscale':\n            if first_check_op:\n                last_backward_op = None\n                for last_idx in range(idx - 1, 0, -1):\n                    if not is_comm_op(main_block.ops[last_idx]):\n                        last_backward_op = main_block.ops[last_idx]\n                        break\n                prior_varname = last_backward_op.output_arg_names[0]\n                first_check_op = False\n            deps_map[idx] = (prior_varname, op.input('Scale')[0], 'check_finite_dep')\n    first_optimizer_op = True\n    for (idx, op) in enumerate(main_block.ops):\n        if op.type in _supported_optimizer_type:\n            if first_optimizer_op:\n                first_optimizer_op = False\n            else:\n                deps_map[idx] = (prior_varname, op.input('Param')[0], 'optimizer_order_dep')\n            prior_varname = op.output('ParamOut')[0]\n    indice = sorted(deps_map.keys(), reverse=True)\n    for idx in indice:\n        prior_var = main_block.var(deps_map[idx][0])\n        post_var = main_block.var(deps_map[idx][1])\n        op_namescope = deps_map[idx][2]\n        depend_op = insert_dependencies_for_vars(main_block, idx, prior_var, post_var, self._dist_context, OpRole.Optimize, is_recompute=False, sync=False, op_namescope=op_namescope)\n    main_block._sync_with_cpp()",
            "def _apply_single_impl(self, main_program, startup_program, context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if not _sharding_pass_applied(context):\n        return\n    self._dist_context = self.get_attr('dist_context', None)\n    self.flags_sync_stream = 'flags_sync_stream'\n    main_block = main_program.global_block()\n    startup_block = startup_program.global_block()\n    last_dp_reduce_op_idx = -1\n    last_dp_reduce_varname = None\n    for (idx, op) in reversed(list(enumerate(main_block.ops))):\n        if is_data_parallel_reduce_op(op):\n            last_dp_reduce_op_idx = idx\n            last_dp_reduce_varname = op.output_arg_names[0]\n            break\n    assert last_dp_reduce_op_idx > 0\n    assert last_dp_reduce_varname is not None\n    deps_map = {}\n    prior_varname = last_dp_reduce_varname\n    for (idx, op) in enumerate(main_block.ops):\n        if is_amp_flag_sync_op(op) or is_global_norm_sync_op(op):\n            op_namescope = None\n            if is_amp_flag_sync_op(op):\n                op_namescope = 'amp_flag_sync_dep'\n                op.dist_attr.execution_stream = self.flags_sync_stream\n            elif is_global_norm_sync_op(op):\n                op_namescope = 'global_norm_sync_dep'\n            deps_map[idx] = (prior_varname, op.input('X')[0], op_namescope)\n            prior_varname = op.output('Out')[0]\n    first_check_op = True\n    for (idx, op) in enumerate(main_block.ops):\n        if op.type == 'check_finite_and_unscale':\n            if first_check_op:\n                last_backward_op = None\n                for last_idx in range(idx - 1, 0, -1):\n                    if not is_comm_op(main_block.ops[last_idx]):\n                        last_backward_op = main_block.ops[last_idx]\n                        break\n                prior_varname = last_backward_op.output_arg_names[0]\n                first_check_op = False\n            deps_map[idx] = (prior_varname, op.input('Scale')[0], 'check_finite_dep')\n    first_optimizer_op = True\n    for (idx, op) in enumerate(main_block.ops):\n        if op.type in _supported_optimizer_type:\n            if first_optimizer_op:\n                first_optimizer_op = False\n            else:\n                deps_map[idx] = (prior_varname, op.input('Param')[0], 'optimizer_order_dep')\n            prior_varname = op.output('ParamOut')[0]\n    indice = sorted(deps_map.keys(), reverse=True)\n    for idx in indice:\n        prior_var = main_block.var(deps_map[idx][0])\n        post_var = main_block.var(deps_map[idx][1])\n        op_namescope = deps_map[idx][2]\n        depend_op = insert_dependencies_for_vars(main_block, idx, prior_var, post_var, self._dist_context, OpRole.Optimize, is_recompute=False, sync=False, op_namescope=op_namescope)\n    main_block._sync_with_cpp()"
        ]
    }
]