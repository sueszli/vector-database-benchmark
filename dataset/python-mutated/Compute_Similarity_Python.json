[
    {
        "func_name": "__init__",
        "original": "def __init__(self, dataMatrix, topK=100, shrink=0, normalize=True, asymmetric_alpha=0.5, tversky_alpha=1.0, tversky_beta=1.0, similarity='cosine', row_weights=None):\n    \"\"\"\n        Computes the cosine similarity on the columns of dataMatrix\n        If it is computed on URM=|users|x|items|, pass the URM as is.\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\n        :param dataMatrix:\n        :param topK:\n        :param shrink:\n        :param normalize:           If True divide the dot product by the product of the norms\n        :param row_weights:         Multiply the values in each row by a specified value. Array\n        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\n        :param similarity:  \"cosine\"        computes Cosine similarity\n                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\n                            \"asymmetric\"    computes Asymmetric Cosine\n                            \"pearson\"       computes Pearson Correlation, removing the average of the items\n                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\n                            \"dice\"          computes Dice similarity for binary interactions\n                            \"tversky\"       computes Tversky similarity for binary interactions\n                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\n\n        \"\"\"\n    '\\n        Asymmetric Cosine as described in: \\n        Aiolli, F. (2013, October). Efficient top-n recommendation for very large scale binary rated datasets. In Proceedings of the 7th ACM conference on Recommender systems (pp. 273-280). ACM.\\n        \\n        '\n    super(Compute_Similarity_Python, self).__init__()\n    self.shrink = shrink\n    self.normalize = normalize\n    (self.n_rows, self.n_columns) = dataMatrix.shape\n    self.TopK = min(topK, self.n_columns)\n    self.asymmetric_alpha = asymmetric_alpha\n    self.tversky_alpha = tversky_alpha\n    self.tversky_beta = tversky_beta\n    self.dataMatrix = dataMatrix.copy()\n    self.adjusted_cosine = False\n    self.asymmetric_cosine = False\n    self.pearson_correlation = False\n    self.tanimoto_coefficient = False\n    self.dice_coefficient = False\n    self.tversky_coefficient = False\n    if similarity == 'adjusted':\n        self.adjusted_cosine = True\n    elif similarity == 'asymmetric':\n        self.asymmetric_cosine = True\n    elif similarity == 'pearson':\n        self.pearson_correlation = True\n    elif similarity == 'jaccard' or similarity == 'tanimoto':\n        self.tanimoto_coefficient = True\n        self.normalize = False\n    elif similarity == 'dice':\n        self.dice_coefficient = True\n        self.normalize = False\n    elif similarity == 'tversky':\n        self.tversky_coefficient = True\n        self.normalize = False\n    elif similarity == 'cosine':\n        pass\n    else:\n        raise ValueError(\"Cosine_Similarity: value for argument 'mode' not recognized. Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',dice, tversky. Passed value was '{}'\".format(similarity))\n    self.use_row_weights = False\n    if row_weights is not None:\n        if dataMatrix.shape[0] != len(row_weights):\n            raise ValueError('Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.Col_weights has {} columns, dataMatrix has {}.'.format(len(row_weights), dataMatrix.shape[0]))\n        self.use_row_weights = True\n        self.row_weights = row_weights.copy()\n        self.row_weights_diag = sps.diags(self.row_weights)\n        self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T",
        "mutated": [
            "def __init__(self, dataMatrix, topK=100, shrink=0, normalize=True, asymmetric_alpha=0.5, tversky_alpha=1.0, tversky_beta=1.0, similarity='cosine', row_weights=None):\n    if False:\n        i = 10\n    '\\n        Computes the cosine similarity on the columns of dataMatrix\\n        If it is computed on URM=|users|x|items|, pass the URM as is.\\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\\n        :param dataMatrix:\\n        :param topK:\\n        :param shrink:\\n        :param normalize:           If True divide the dot product by the product of the norms\\n        :param row_weights:         Multiply the values in each row by a specified value. Array\\n        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\\n        :param similarity:  \"cosine\"        computes Cosine similarity\\n                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\\n                            \"asymmetric\"    computes Asymmetric Cosine\\n                            \"pearson\"       computes Pearson Correlation, removing the average of the items\\n                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\\n                            \"dice\"          computes Dice similarity for binary interactions\\n                            \"tversky\"       computes Tversky similarity for binary interactions\\n                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\\n\\n        '\n    '\\n        Asymmetric Cosine as described in: \\n        Aiolli, F. (2013, October). Efficient top-n recommendation for very large scale binary rated datasets. In Proceedings of the 7th ACM conference on Recommender systems (pp. 273-280). ACM.\\n        \\n        '\n    super(Compute_Similarity_Python, self).__init__()\n    self.shrink = shrink\n    self.normalize = normalize\n    (self.n_rows, self.n_columns) = dataMatrix.shape\n    self.TopK = min(topK, self.n_columns)\n    self.asymmetric_alpha = asymmetric_alpha\n    self.tversky_alpha = tversky_alpha\n    self.tversky_beta = tversky_beta\n    self.dataMatrix = dataMatrix.copy()\n    self.adjusted_cosine = False\n    self.asymmetric_cosine = False\n    self.pearson_correlation = False\n    self.tanimoto_coefficient = False\n    self.dice_coefficient = False\n    self.tversky_coefficient = False\n    if similarity == 'adjusted':\n        self.adjusted_cosine = True\n    elif similarity == 'asymmetric':\n        self.asymmetric_cosine = True\n    elif similarity == 'pearson':\n        self.pearson_correlation = True\n    elif similarity == 'jaccard' or similarity == 'tanimoto':\n        self.tanimoto_coefficient = True\n        self.normalize = False\n    elif similarity == 'dice':\n        self.dice_coefficient = True\n        self.normalize = False\n    elif similarity == 'tversky':\n        self.tversky_coefficient = True\n        self.normalize = False\n    elif similarity == 'cosine':\n        pass\n    else:\n        raise ValueError(\"Cosine_Similarity: value for argument 'mode' not recognized. Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',dice, tversky. Passed value was '{}'\".format(similarity))\n    self.use_row_weights = False\n    if row_weights is not None:\n        if dataMatrix.shape[0] != len(row_weights):\n            raise ValueError('Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.Col_weights has {} columns, dataMatrix has {}.'.format(len(row_weights), dataMatrix.shape[0]))\n        self.use_row_weights = True\n        self.row_weights = row_weights.copy()\n        self.row_weights_diag = sps.diags(self.row_weights)\n        self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T",
            "def __init__(self, dataMatrix, topK=100, shrink=0, normalize=True, asymmetric_alpha=0.5, tversky_alpha=1.0, tversky_beta=1.0, similarity='cosine', row_weights=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Computes the cosine similarity on the columns of dataMatrix\\n        If it is computed on URM=|users|x|items|, pass the URM as is.\\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\\n        :param dataMatrix:\\n        :param topK:\\n        :param shrink:\\n        :param normalize:           If True divide the dot product by the product of the norms\\n        :param row_weights:         Multiply the values in each row by a specified value. Array\\n        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\\n        :param similarity:  \"cosine\"        computes Cosine similarity\\n                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\\n                            \"asymmetric\"    computes Asymmetric Cosine\\n                            \"pearson\"       computes Pearson Correlation, removing the average of the items\\n                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\\n                            \"dice\"          computes Dice similarity for binary interactions\\n                            \"tversky\"       computes Tversky similarity for binary interactions\\n                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\\n\\n        '\n    '\\n        Asymmetric Cosine as described in: \\n        Aiolli, F. (2013, October). Efficient top-n recommendation for very large scale binary rated datasets. In Proceedings of the 7th ACM conference on Recommender systems (pp. 273-280). ACM.\\n        \\n        '\n    super(Compute_Similarity_Python, self).__init__()\n    self.shrink = shrink\n    self.normalize = normalize\n    (self.n_rows, self.n_columns) = dataMatrix.shape\n    self.TopK = min(topK, self.n_columns)\n    self.asymmetric_alpha = asymmetric_alpha\n    self.tversky_alpha = tversky_alpha\n    self.tversky_beta = tversky_beta\n    self.dataMatrix = dataMatrix.copy()\n    self.adjusted_cosine = False\n    self.asymmetric_cosine = False\n    self.pearson_correlation = False\n    self.tanimoto_coefficient = False\n    self.dice_coefficient = False\n    self.tversky_coefficient = False\n    if similarity == 'adjusted':\n        self.adjusted_cosine = True\n    elif similarity == 'asymmetric':\n        self.asymmetric_cosine = True\n    elif similarity == 'pearson':\n        self.pearson_correlation = True\n    elif similarity == 'jaccard' or similarity == 'tanimoto':\n        self.tanimoto_coefficient = True\n        self.normalize = False\n    elif similarity == 'dice':\n        self.dice_coefficient = True\n        self.normalize = False\n    elif similarity == 'tversky':\n        self.tversky_coefficient = True\n        self.normalize = False\n    elif similarity == 'cosine':\n        pass\n    else:\n        raise ValueError(\"Cosine_Similarity: value for argument 'mode' not recognized. Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',dice, tversky. Passed value was '{}'\".format(similarity))\n    self.use_row_weights = False\n    if row_weights is not None:\n        if dataMatrix.shape[0] != len(row_weights):\n            raise ValueError('Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.Col_weights has {} columns, dataMatrix has {}.'.format(len(row_weights), dataMatrix.shape[0]))\n        self.use_row_weights = True\n        self.row_weights = row_weights.copy()\n        self.row_weights_diag = sps.diags(self.row_weights)\n        self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T",
            "def __init__(self, dataMatrix, topK=100, shrink=0, normalize=True, asymmetric_alpha=0.5, tversky_alpha=1.0, tversky_beta=1.0, similarity='cosine', row_weights=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Computes the cosine similarity on the columns of dataMatrix\\n        If it is computed on URM=|users|x|items|, pass the URM as is.\\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\\n        :param dataMatrix:\\n        :param topK:\\n        :param shrink:\\n        :param normalize:           If True divide the dot product by the product of the norms\\n        :param row_weights:         Multiply the values in each row by a specified value. Array\\n        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\\n        :param similarity:  \"cosine\"        computes Cosine similarity\\n                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\\n                            \"asymmetric\"    computes Asymmetric Cosine\\n                            \"pearson\"       computes Pearson Correlation, removing the average of the items\\n                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\\n                            \"dice\"          computes Dice similarity for binary interactions\\n                            \"tversky\"       computes Tversky similarity for binary interactions\\n                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\\n\\n        '\n    '\\n        Asymmetric Cosine as described in: \\n        Aiolli, F. (2013, October). Efficient top-n recommendation for very large scale binary rated datasets. In Proceedings of the 7th ACM conference on Recommender systems (pp. 273-280). ACM.\\n        \\n        '\n    super(Compute_Similarity_Python, self).__init__()\n    self.shrink = shrink\n    self.normalize = normalize\n    (self.n_rows, self.n_columns) = dataMatrix.shape\n    self.TopK = min(topK, self.n_columns)\n    self.asymmetric_alpha = asymmetric_alpha\n    self.tversky_alpha = tversky_alpha\n    self.tversky_beta = tversky_beta\n    self.dataMatrix = dataMatrix.copy()\n    self.adjusted_cosine = False\n    self.asymmetric_cosine = False\n    self.pearson_correlation = False\n    self.tanimoto_coefficient = False\n    self.dice_coefficient = False\n    self.tversky_coefficient = False\n    if similarity == 'adjusted':\n        self.adjusted_cosine = True\n    elif similarity == 'asymmetric':\n        self.asymmetric_cosine = True\n    elif similarity == 'pearson':\n        self.pearson_correlation = True\n    elif similarity == 'jaccard' or similarity == 'tanimoto':\n        self.tanimoto_coefficient = True\n        self.normalize = False\n    elif similarity == 'dice':\n        self.dice_coefficient = True\n        self.normalize = False\n    elif similarity == 'tversky':\n        self.tversky_coefficient = True\n        self.normalize = False\n    elif similarity == 'cosine':\n        pass\n    else:\n        raise ValueError(\"Cosine_Similarity: value for argument 'mode' not recognized. Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',dice, tversky. Passed value was '{}'\".format(similarity))\n    self.use_row_weights = False\n    if row_weights is not None:\n        if dataMatrix.shape[0] != len(row_weights):\n            raise ValueError('Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.Col_weights has {} columns, dataMatrix has {}.'.format(len(row_weights), dataMatrix.shape[0]))\n        self.use_row_weights = True\n        self.row_weights = row_weights.copy()\n        self.row_weights_diag = sps.diags(self.row_weights)\n        self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T",
            "def __init__(self, dataMatrix, topK=100, shrink=0, normalize=True, asymmetric_alpha=0.5, tversky_alpha=1.0, tversky_beta=1.0, similarity='cosine', row_weights=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Computes the cosine similarity on the columns of dataMatrix\\n        If it is computed on URM=|users|x|items|, pass the URM as is.\\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\\n        :param dataMatrix:\\n        :param topK:\\n        :param shrink:\\n        :param normalize:           If True divide the dot product by the product of the norms\\n        :param row_weights:         Multiply the values in each row by a specified value. Array\\n        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\\n        :param similarity:  \"cosine\"        computes Cosine similarity\\n                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\\n                            \"asymmetric\"    computes Asymmetric Cosine\\n                            \"pearson\"       computes Pearson Correlation, removing the average of the items\\n                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\\n                            \"dice\"          computes Dice similarity for binary interactions\\n                            \"tversky\"       computes Tversky similarity for binary interactions\\n                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\\n\\n        '\n    '\\n        Asymmetric Cosine as described in: \\n        Aiolli, F. (2013, October). Efficient top-n recommendation for very large scale binary rated datasets. In Proceedings of the 7th ACM conference on Recommender systems (pp. 273-280). ACM.\\n        \\n        '\n    super(Compute_Similarity_Python, self).__init__()\n    self.shrink = shrink\n    self.normalize = normalize\n    (self.n_rows, self.n_columns) = dataMatrix.shape\n    self.TopK = min(topK, self.n_columns)\n    self.asymmetric_alpha = asymmetric_alpha\n    self.tversky_alpha = tversky_alpha\n    self.tversky_beta = tversky_beta\n    self.dataMatrix = dataMatrix.copy()\n    self.adjusted_cosine = False\n    self.asymmetric_cosine = False\n    self.pearson_correlation = False\n    self.tanimoto_coefficient = False\n    self.dice_coefficient = False\n    self.tversky_coefficient = False\n    if similarity == 'adjusted':\n        self.adjusted_cosine = True\n    elif similarity == 'asymmetric':\n        self.asymmetric_cosine = True\n    elif similarity == 'pearson':\n        self.pearson_correlation = True\n    elif similarity == 'jaccard' or similarity == 'tanimoto':\n        self.tanimoto_coefficient = True\n        self.normalize = False\n    elif similarity == 'dice':\n        self.dice_coefficient = True\n        self.normalize = False\n    elif similarity == 'tversky':\n        self.tversky_coefficient = True\n        self.normalize = False\n    elif similarity == 'cosine':\n        pass\n    else:\n        raise ValueError(\"Cosine_Similarity: value for argument 'mode' not recognized. Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',dice, tversky. Passed value was '{}'\".format(similarity))\n    self.use_row_weights = False\n    if row_weights is not None:\n        if dataMatrix.shape[0] != len(row_weights):\n            raise ValueError('Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.Col_weights has {} columns, dataMatrix has {}.'.format(len(row_weights), dataMatrix.shape[0]))\n        self.use_row_weights = True\n        self.row_weights = row_weights.copy()\n        self.row_weights_diag = sps.diags(self.row_weights)\n        self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T",
            "def __init__(self, dataMatrix, topK=100, shrink=0, normalize=True, asymmetric_alpha=0.5, tversky_alpha=1.0, tversky_beta=1.0, similarity='cosine', row_weights=None):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Computes the cosine similarity on the columns of dataMatrix\\n        If it is computed on URM=|users|x|items|, pass the URM as is.\\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\\n        :param dataMatrix:\\n        :param topK:\\n        :param shrink:\\n        :param normalize:           If True divide the dot product by the product of the norms\\n        :param row_weights:         Multiply the values in each row by a specified value. Array\\n        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\\n        :param similarity:  \"cosine\"        computes Cosine similarity\\n                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\\n                            \"asymmetric\"    computes Asymmetric Cosine\\n                            \"pearson\"       computes Pearson Correlation, removing the average of the items\\n                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\\n                            \"dice\"          computes Dice similarity for binary interactions\\n                            \"tversky\"       computes Tversky similarity for binary interactions\\n                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\\n\\n        '\n    '\\n        Asymmetric Cosine as described in: \\n        Aiolli, F. (2013, October). Efficient top-n recommendation for very large scale binary rated datasets. In Proceedings of the 7th ACM conference on Recommender systems (pp. 273-280). ACM.\\n        \\n        '\n    super(Compute_Similarity_Python, self).__init__()\n    self.shrink = shrink\n    self.normalize = normalize\n    (self.n_rows, self.n_columns) = dataMatrix.shape\n    self.TopK = min(topK, self.n_columns)\n    self.asymmetric_alpha = asymmetric_alpha\n    self.tversky_alpha = tversky_alpha\n    self.tversky_beta = tversky_beta\n    self.dataMatrix = dataMatrix.copy()\n    self.adjusted_cosine = False\n    self.asymmetric_cosine = False\n    self.pearson_correlation = False\n    self.tanimoto_coefficient = False\n    self.dice_coefficient = False\n    self.tversky_coefficient = False\n    if similarity == 'adjusted':\n        self.adjusted_cosine = True\n    elif similarity == 'asymmetric':\n        self.asymmetric_cosine = True\n    elif similarity == 'pearson':\n        self.pearson_correlation = True\n    elif similarity == 'jaccard' or similarity == 'tanimoto':\n        self.tanimoto_coefficient = True\n        self.normalize = False\n    elif similarity == 'dice':\n        self.dice_coefficient = True\n        self.normalize = False\n    elif similarity == 'tversky':\n        self.tversky_coefficient = True\n        self.normalize = False\n    elif similarity == 'cosine':\n        pass\n    else:\n        raise ValueError(\"Cosine_Similarity: value for argument 'mode' not recognized. Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',dice, tversky. Passed value was '{}'\".format(similarity))\n    self.use_row_weights = False\n    if row_weights is not None:\n        if dataMatrix.shape[0] != len(row_weights):\n            raise ValueError('Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.Col_weights has {} columns, dataMatrix has {}.'.format(len(row_weights), dataMatrix.shape[0]))\n        self.use_row_weights = True\n        self.row_weights = row_weights.copy()\n        self.row_weights_diag = sps.diags(self.row_weights)\n        self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T"
        ]
    },
    {
        "func_name": "applyAdjustedCosine",
        "original": "def applyAdjustedCosine(self):\n    \"\"\"\n        Remove from every data point the average for the corresponding row\n        :return:\n        \"\"\"\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csr')\n    interactionsPerRow = np.diff(self.dataMatrix.indptr)\n    nonzeroRows = interactionsPerRow > 0\n    sumPerRow = np.asarray(self.dataMatrix.sum(axis=1)).ravel()\n    rowAverage = np.zeros_like(sumPerRow)\n    rowAverage[nonzeroRows] = sumPerRow[nonzeroRows] / interactionsPerRow[nonzeroRows]\n    start_row = 0\n    end_row = 0\n    blockSize = 1000\n    while end_row < self.n_rows:\n        end_row = min(self.n_rows, end_row + blockSize)\n        self.dataMatrix.data[self.dataMatrix.indptr[start_row]:self.dataMatrix.indptr[end_row]] -= np.repeat(rowAverage[start_row:end_row], interactionsPerRow[start_row:end_row])\n        start_row += blockSize",
        "mutated": [
            "def applyAdjustedCosine(self):\n    if False:\n        i = 10\n    '\\n        Remove from every data point the average for the corresponding row\\n        :return:\\n        '\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csr')\n    interactionsPerRow = np.diff(self.dataMatrix.indptr)\n    nonzeroRows = interactionsPerRow > 0\n    sumPerRow = np.asarray(self.dataMatrix.sum(axis=1)).ravel()\n    rowAverage = np.zeros_like(sumPerRow)\n    rowAverage[nonzeroRows] = sumPerRow[nonzeroRows] / interactionsPerRow[nonzeroRows]\n    start_row = 0\n    end_row = 0\n    blockSize = 1000\n    while end_row < self.n_rows:\n        end_row = min(self.n_rows, end_row + blockSize)\n        self.dataMatrix.data[self.dataMatrix.indptr[start_row]:self.dataMatrix.indptr[end_row]] -= np.repeat(rowAverage[start_row:end_row], interactionsPerRow[start_row:end_row])\n        start_row += blockSize",
            "def applyAdjustedCosine(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Remove from every data point the average for the corresponding row\\n        :return:\\n        '\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csr')\n    interactionsPerRow = np.diff(self.dataMatrix.indptr)\n    nonzeroRows = interactionsPerRow > 0\n    sumPerRow = np.asarray(self.dataMatrix.sum(axis=1)).ravel()\n    rowAverage = np.zeros_like(sumPerRow)\n    rowAverage[nonzeroRows] = sumPerRow[nonzeroRows] / interactionsPerRow[nonzeroRows]\n    start_row = 0\n    end_row = 0\n    blockSize = 1000\n    while end_row < self.n_rows:\n        end_row = min(self.n_rows, end_row + blockSize)\n        self.dataMatrix.data[self.dataMatrix.indptr[start_row]:self.dataMatrix.indptr[end_row]] -= np.repeat(rowAverage[start_row:end_row], interactionsPerRow[start_row:end_row])\n        start_row += blockSize",
            "def applyAdjustedCosine(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Remove from every data point the average for the corresponding row\\n        :return:\\n        '\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csr')\n    interactionsPerRow = np.diff(self.dataMatrix.indptr)\n    nonzeroRows = interactionsPerRow > 0\n    sumPerRow = np.asarray(self.dataMatrix.sum(axis=1)).ravel()\n    rowAverage = np.zeros_like(sumPerRow)\n    rowAverage[nonzeroRows] = sumPerRow[nonzeroRows] / interactionsPerRow[nonzeroRows]\n    start_row = 0\n    end_row = 0\n    blockSize = 1000\n    while end_row < self.n_rows:\n        end_row = min(self.n_rows, end_row + blockSize)\n        self.dataMatrix.data[self.dataMatrix.indptr[start_row]:self.dataMatrix.indptr[end_row]] -= np.repeat(rowAverage[start_row:end_row], interactionsPerRow[start_row:end_row])\n        start_row += blockSize",
            "def applyAdjustedCosine(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Remove from every data point the average for the corresponding row\\n        :return:\\n        '\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csr')\n    interactionsPerRow = np.diff(self.dataMatrix.indptr)\n    nonzeroRows = interactionsPerRow > 0\n    sumPerRow = np.asarray(self.dataMatrix.sum(axis=1)).ravel()\n    rowAverage = np.zeros_like(sumPerRow)\n    rowAverage[nonzeroRows] = sumPerRow[nonzeroRows] / interactionsPerRow[nonzeroRows]\n    start_row = 0\n    end_row = 0\n    blockSize = 1000\n    while end_row < self.n_rows:\n        end_row = min(self.n_rows, end_row + blockSize)\n        self.dataMatrix.data[self.dataMatrix.indptr[start_row]:self.dataMatrix.indptr[end_row]] -= np.repeat(rowAverage[start_row:end_row], interactionsPerRow[start_row:end_row])\n        start_row += blockSize",
            "def applyAdjustedCosine(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Remove from every data point the average for the corresponding row\\n        :return:\\n        '\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csr')\n    interactionsPerRow = np.diff(self.dataMatrix.indptr)\n    nonzeroRows = interactionsPerRow > 0\n    sumPerRow = np.asarray(self.dataMatrix.sum(axis=1)).ravel()\n    rowAverage = np.zeros_like(sumPerRow)\n    rowAverage[nonzeroRows] = sumPerRow[nonzeroRows] / interactionsPerRow[nonzeroRows]\n    start_row = 0\n    end_row = 0\n    blockSize = 1000\n    while end_row < self.n_rows:\n        end_row = min(self.n_rows, end_row + blockSize)\n        self.dataMatrix.data[self.dataMatrix.indptr[start_row]:self.dataMatrix.indptr[end_row]] -= np.repeat(rowAverage[start_row:end_row], interactionsPerRow[start_row:end_row])\n        start_row += blockSize"
        ]
    },
    {
        "func_name": "applyPearsonCorrelation",
        "original": "def applyPearsonCorrelation(self):\n    \"\"\"\n        Remove from every data point the average for the corresponding column\n        :return:\n        \"\"\"\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    interactionsPerCol = np.diff(self.dataMatrix.indptr)\n    nonzeroCols = interactionsPerCol > 0\n    sumPerCol = np.asarray(self.dataMatrix.sum(axis=0)).ravel()\n    colAverage = np.zeros_like(sumPerCol)\n    colAverage[nonzeroCols] = sumPerCol[nonzeroCols] / interactionsPerCol[nonzeroCols]\n    start_col = 0\n    end_col = 0\n    blockSize = 1000\n    while end_col < self.n_columns:\n        end_col = min(self.n_columns, end_col + blockSize)\n        self.dataMatrix.data[self.dataMatrix.indptr[start_col]:self.dataMatrix.indptr[end_col]] -= np.repeat(colAverage[start_col:end_col], interactionsPerCol[start_col:end_col])\n        start_col += blockSize",
        "mutated": [
            "def applyPearsonCorrelation(self):\n    if False:\n        i = 10\n    '\\n        Remove from every data point the average for the corresponding column\\n        :return:\\n        '\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    interactionsPerCol = np.diff(self.dataMatrix.indptr)\n    nonzeroCols = interactionsPerCol > 0\n    sumPerCol = np.asarray(self.dataMatrix.sum(axis=0)).ravel()\n    colAverage = np.zeros_like(sumPerCol)\n    colAverage[nonzeroCols] = sumPerCol[nonzeroCols] / interactionsPerCol[nonzeroCols]\n    start_col = 0\n    end_col = 0\n    blockSize = 1000\n    while end_col < self.n_columns:\n        end_col = min(self.n_columns, end_col + blockSize)\n        self.dataMatrix.data[self.dataMatrix.indptr[start_col]:self.dataMatrix.indptr[end_col]] -= np.repeat(colAverage[start_col:end_col], interactionsPerCol[start_col:end_col])\n        start_col += blockSize",
            "def applyPearsonCorrelation(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Remove from every data point the average for the corresponding column\\n        :return:\\n        '\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    interactionsPerCol = np.diff(self.dataMatrix.indptr)\n    nonzeroCols = interactionsPerCol > 0\n    sumPerCol = np.asarray(self.dataMatrix.sum(axis=0)).ravel()\n    colAverage = np.zeros_like(sumPerCol)\n    colAverage[nonzeroCols] = sumPerCol[nonzeroCols] / interactionsPerCol[nonzeroCols]\n    start_col = 0\n    end_col = 0\n    blockSize = 1000\n    while end_col < self.n_columns:\n        end_col = min(self.n_columns, end_col + blockSize)\n        self.dataMatrix.data[self.dataMatrix.indptr[start_col]:self.dataMatrix.indptr[end_col]] -= np.repeat(colAverage[start_col:end_col], interactionsPerCol[start_col:end_col])\n        start_col += blockSize",
            "def applyPearsonCorrelation(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Remove from every data point the average for the corresponding column\\n        :return:\\n        '\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    interactionsPerCol = np.diff(self.dataMatrix.indptr)\n    nonzeroCols = interactionsPerCol > 0\n    sumPerCol = np.asarray(self.dataMatrix.sum(axis=0)).ravel()\n    colAverage = np.zeros_like(sumPerCol)\n    colAverage[nonzeroCols] = sumPerCol[nonzeroCols] / interactionsPerCol[nonzeroCols]\n    start_col = 0\n    end_col = 0\n    blockSize = 1000\n    while end_col < self.n_columns:\n        end_col = min(self.n_columns, end_col + blockSize)\n        self.dataMatrix.data[self.dataMatrix.indptr[start_col]:self.dataMatrix.indptr[end_col]] -= np.repeat(colAverage[start_col:end_col], interactionsPerCol[start_col:end_col])\n        start_col += blockSize",
            "def applyPearsonCorrelation(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Remove from every data point the average for the corresponding column\\n        :return:\\n        '\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    interactionsPerCol = np.diff(self.dataMatrix.indptr)\n    nonzeroCols = interactionsPerCol > 0\n    sumPerCol = np.asarray(self.dataMatrix.sum(axis=0)).ravel()\n    colAverage = np.zeros_like(sumPerCol)\n    colAverage[nonzeroCols] = sumPerCol[nonzeroCols] / interactionsPerCol[nonzeroCols]\n    start_col = 0\n    end_col = 0\n    blockSize = 1000\n    while end_col < self.n_columns:\n        end_col = min(self.n_columns, end_col + blockSize)\n        self.dataMatrix.data[self.dataMatrix.indptr[start_col]:self.dataMatrix.indptr[end_col]] -= np.repeat(colAverage[start_col:end_col], interactionsPerCol[start_col:end_col])\n        start_col += blockSize",
            "def applyPearsonCorrelation(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Remove from every data point the average for the corresponding column\\n        :return:\\n        '\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    interactionsPerCol = np.diff(self.dataMatrix.indptr)\n    nonzeroCols = interactionsPerCol > 0\n    sumPerCol = np.asarray(self.dataMatrix.sum(axis=0)).ravel()\n    colAverage = np.zeros_like(sumPerCol)\n    colAverage[nonzeroCols] = sumPerCol[nonzeroCols] / interactionsPerCol[nonzeroCols]\n    start_col = 0\n    end_col = 0\n    blockSize = 1000\n    while end_col < self.n_columns:\n        end_col = min(self.n_columns, end_col + blockSize)\n        self.dataMatrix.data[self.dataMatrix.indptr[start_col]:self.dataMatrix.indptr[end_col]] -= np.repeat(colAverage[start_col:end_col], interactionsPerCol[start_col:end_col])\n        start_col += blockSize"
        ]
    },
    {
        "func_name": "useOnlyBooleanInteractions",
        "original": "def useOnlyBooleanInteractions(self):\n    start_pos = 0\n    end_pos = 0\n    blockSize = 1000\n    while end_pos < len(self.dataMatrix.data):\n        end_pos = min(len(self.dataMatrix.data), end_pos + blockSize)\n        self.dataMatrix.data[start_pos:end_pos] = np.ones(end_pos - start_pos)\n        start_pos += blockSize",
        "mutated": [
            "def useOnlyBooleanInteractions(self):\n    if False:\n        i = 10\n    start_pos = 0\n    end_pos = 0\n    blockSize = 1000\n    while end_pos < len(self.dataMatrix.data):\n        end_pos = min(len(self.dataMatrix.data), end_pos + blockSize)\n        self.dataMatrix.data[start_pos:end_pos] = np.ones(end_pos - start_pos)\n        start_pos += blockSize",
            "def useOnlyBooleanInteractions(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    start_pos = 0\n    end_pos = 0\n    blockSize = 1000\n    while end_pos < len(self.dataMatrix.data):\n        end_pos = min(len(self.dataMatrix.data), end_pos + blockSize)\n        self.dataMatrix.data[start_pos:end_pos] = np.ones(end_pos - start_pos)\n        start_pos += blockSize",
            "def useOnlyBooleanInteractions(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    start_pos = 0\n    end_pos = 0\n    blockSize = 1000\n    while end_pos < len(self.dataMatrix.data):\n        end_pos = min(len(self.dataMatrix.data), end_pos + blockSize)\n        self.dataMatrix.data[start_pos:end_pos] = np.ones(end_pos - start_pos)\n        start_pos += blockSize",
            "def useOnlyBooleanInteractions(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    start_pos = 0\n    end_pos = 0\n    blockSize = 1000\n    while end_pos < len(self.dataMatrix.data):\n        end_pos = min(len(self.dataMatrix.data), end_pos + blockSize)\n        self.dataMatrix.data[start_pos:end_pos] = np.ones(end_pos - start_pos)\n        start_pos += blockSize",
            "def useOnlyBooleanInteractions(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    start_pos = 0\n    end_pos = 0\n    blockSize = 1000\n    while end_pos < len(self.dataMatrix.data):\n        end_pos = min(len(self.dataMatrix.data), end_pos + blockSize)\n        self.dataMatrix.data[start_pos:end_pos] = np.ones(end_pos - start_pos)\n        start_pos += blockSize"
        ]
    },
    {
        "func_name": "compute_similarity",
        "original": "def compute_similarity(self, start_col=None, end_col=None, block_size=100):\n    \"\"\"\n        Compute the similarity for the given dataset\n        :param self:\n        :param start_col: column to begin with\n        :param end_col: column to stop before, end_col is excluded\n        :return:\n        \"\"\"\n    values = []\n    rows = []\n    cols = []\n    start_time = time.time()\n    start_time_print_batch = start_time\n    processed_items = 0\n    if self.adjusted_cosine:\n        self.applyAdjustedCosine()\n    elif self.pearson_correlation:\n        self.applyPearsonCorrelation()\n    elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n        self.useOnlyBooleanInteractions()\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    sum_of_squared = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n    if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n        sum_of_squared = np.sqrt(sum_of_squared)\n    if self.asymmetric_cosine:\n        sum_of_squared_to_alpha = np.power(sum_of_squared + 1e-06, 2 * self.asymmetric_alpha)\n        sum_of_squared_to_1_minus_alpha = np.power(sum_of_squared + 1e-06, 2 * (1 - self.asymmetric_alpha))\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    start_col_local = 0\n    end_col_local = self.n_columns\n    if start_col is not None and start_col > 0 and (start_col < self.n_columns):\n        start_col_local = start_col\n    if end_col is not None and end_col > start_col_local and (end_col < self.n_columns):\n        end_col_local = end_col\n    start_col_block = start_col_local\n    this_block_size = 0\n    while start_col_block < end_col_local:\n        end_col_block = min(start_col_block + block_size, end_col_local)\n        this_block_size = end_col_block - start_col_block\n        item_data = self.dataMatrix[:, start_col_block:end_col_block]\n        item_data = item_data.toarray()\n        if self.use_row_weights:\n            this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n        else:\n            this_block_weights = self.dataMatrix.T.dot(item_data)\n        for col_index_in_block in range(this_block_size):\n            if this_block_size == 1:\n                this_column_weights = this_block_weights.ravel()\n            else:\n                this_column_weights = this_block_weights[:, col_index_in_block]\n            columnIndex = col_index_in_block + start_col_block\n            this_column_weights[columnIndex] = 0.0\n            if self.normalize:\n                if self.asymmetric_cosine:\n                    denominator = sum_of_squared_to_alpha[columnIndex] * sum_of_squared_to_1_minus_alpha + self.shrink + 1e-06\n                else:\n                    denominator = sum_of_squared[columnIndex] * sum_of_squared + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.tanimoto_coefficient:\n                denominator = sum_of_squared[columnIndex] + sum_of_squared - this_column_weights + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.dice_coefficient:\n                denominator = sum_of_squared[columnIndex] + sum_of_squared + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.tversky_coefficient:\n                denominator = this_column_weights + (sum_of_squared[columnIndex] - this_column_weights) * self.tversky_alpha + (sum_of_squared - this_column_weights) * self.tversky_beta + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.shrink != 0:\n                this_column_weights = this_column_weights / self.shrink\n            relevant_items_partition = (-this_column_weights).argpartition(self.TopK - 1)[0:self.TopK]\n            relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n            top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n            notZerosMask = this_column_weights[top_k_idx] != 0.0\n            numNotZeros = np.sum(notZerosMask)\n            values.extend(this_column_weights[top_k_idx][notZerosMask])\n            rows.extend(top_k_idx[notZerosMask])\n            cols.extend(np.ones(numNotZeros) * columnIndex)\n        start_col_block += this_block_size\n        processed_items += this_block_size\n        if time.time() - start_time_print_batch >= 300 or end_col_block == end_col_local:\n            column_per_sec = processed_items / (time.time() - start_time + 1e-09)\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(processed_items, processed_items / (end_col_local - start_col_local) * 100, column_per_sec, new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_print_batch = time.time()\n    W_sparse = sps.csr_matrix((values, (rows, cols)), shape=(self.n_columns, self.n_columns), dtype=np.float32)\n    return W_sparse",
        "mutated": [
            "def compute_similarity(self, start_col=None, end_col=None, block_size=100):\n    if False:\n        i = 10\n    '\\n        Compute the similarity for the given dataset\\n        :param self:\\n        :param start_col: column to begin with\\n        :param end_col: column to stop before, end_col is excluded\\n        :return:\\n        '\n    values = []\n    rows = []\n    cols = []\n    start_time = time.time()\n    start_time_print_batch = start_time\n    processed_items = 0\n    if self.adjusted_cosine:\n        self.applyAdjustedCosine()\n    elif self.pearson_correlation:\n        self.applyPearsonCorrelation()\n    elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n        self.useOnlyBooleanInteractions()\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    sum_of_squared = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n    if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n        sum_of_squared = np.sqrt(sum_of_squared)\n    if self.asymmetric_cosine:\n        sum_of_squared_to_alpha = np.power(sum_of_squared + 1e-06, 2 * self.asymmetric_alpha)\n        sum_of_squared_to_1_minus_alpha = np.power(sum_of_squared + 1e-06, 2 * (1 - self.asymmetric_alpha))\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    start_col_local = 0\n    end_col_local = self.n_columns\n    if start_col is not None and start_col > 0 and (start_col < self.n_columns):\n        start_col_local = start_col\n    if end_col is not None and end_col > start_col_local and (end_col < self.n_columns):\n        end_col_local = end_col\n    start_col_block = start_col_local\n    this_block_size = 0\n    while start_col_block < end_col_local:\n        end_col_block = min(start_col_block + block_size, end_col_local)\n        this_block_size = end_col_block - start_col_block\n        item_data = self.dataMatrix[:, start_col_block:end_col_block]\n        item_data = item_data.toarray()\n        if self.use_row_weights:\n            this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n        else:\n            this_block_weights = self.dataMatrix.T.dot(item_data)\n        for col_index_in_block in range(this_block_size):\n            if this_block_size == 1:\n                this_column_weights = this_block_weights.ravel()\n            else:\n                this_column_weights = this_block_weights[:, col_index_in_block]\n            columnIndex = col_index_in_block + start_col_block\n            this_column_weights[columnIndex] = 0.0\n            if self.normalize:\n                if self.asymmetric_cosine:\n                    denominator = sum_of_squared_to_alpha[columnIndex] * sum_of_squared_to_1_minus_alpha + self.shrink + 1e-06\n                else:\n                    denominator = sum_of_squared[columnIndex] * sum_of_squared + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.tanimoto_coefficient:\n                denominator = sum_of_squared[columnIndex] + sum_of_squared - this_column_weights + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.dice_coefficient:\n                denominator = sum_of_squared[columnIndex] + sum_of_squared + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.tversky_coefficient:\n                denominator = this_column_weights + (sum_of_squared[columnIndex] - this_column_weights) * self.tversky_alpha + (sum_of_squared - this_column_weights) * self.tversky_beta + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.shrink != 0:\n                this_column_weights = this_column_weights / self.shrink\n            relevant_items_partition = (-this_column_weights).argpartition(self.TopK - 1)[0:self.TopK]\n            relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n            top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n            notZerosMask = this_column_weights[top_k_idx] != 0.0\n            numNotZeros = np.sum(notZerosMask)\n            values.extend(this_column_weights[top_k_idx][notZerosMask])\n            rows.extend(top_k_idx[notZerosMask])\n            cols.extend(np.ones(numNotZeros) * columnIndex)\n        start_col_block += this_block_size\n        processed_items += this_block_size\n        if time.time() - start_time_print_batch >= 300 or end_col_block == end_col_local:\n            column_per_sec = processed_items / (time.time() - start_time + 1e-09)\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(processed_items, processed_items / (end_col_local - start_col_local) * 100, column_per_sec, new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_print_batch = time.time()\n    W_sparse = sps.csr_matrix((values, (rows, cols)), shape=(self.n_columns, self.n_columns), dtype=np.float32)\n    return W_sparse",
            "def compute_similarity(self, start_col=None, end_col=None, block_size=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    '\\n        Compute the similarity for the given dataset\\n        :param self:\\n        :param start_col: column to begin with\\n        :param end_col: column to stop before, end_col is excluded\\n        :return:\\n        '\n    values = []\n    rows = []\n    cols = []\n    start_time = time.time()\n    start_time_print_batch = start_time\n    processed_items = 0\n    if self.adjusted_cosine:\n        self.applyAdjustedCosine()\n    elif self.pearson_correlation:\n        self.applyPearsonCorrelation()\n    elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n        self.useOnlyBooleanInteractions()\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    sum_of_squared = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n    if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n        sum_of_squared = np.sqrt(sum_of_squared)\n    if self.asymmetric_cosine:\n        sum_of_squared_to_alpha = np.power(sum_of_squared + 1e-06, 2 * self.asymmetric_alpha)\n        sum_of_squared_to_1_minus_alpha = np.power(sum_of_squared + 1e-06, 2 * (1 - self.asymmetric_alpha))\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    start_col_local = 0\n    end_col_local = self.n_columns\n    if start_col is not None and start_col > 0 and (start_col < self.n_columns):\n        start_col_local = start_col\n    if end_col is not None and end_col > start_col_local and (end_col < self.n_columns):\n        end_col_local = end_col\n    start_col_block = start_col_local\n    this_block_size = 0\n    while start_col_block < end_col_local:\n        end_col_block = min(start_col_block + block_size, end_col_local)\n        this_block_size = end_col_block - start_col_block\n        item_data = self.dataMatrix[:, start_col_block:end_col_block]\n        item_data = item_data.toarray()\n        if self.use_row_weights:\n            this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n        else:\n            this_block_weights = self.dataMatrix.T.dot(item_data)\n        for col_index_in_block in range(this_block_size):\n            if this_block_size == 1:\n                this_column_weights = this_block_weights.ravel()\n            else:\n                this_column_weights = this_block_weights[:, col_index_in_block]\n            columnIndex = col_index_in_block + start_col_block\n            this_column_weights[columnIndex] = 0.0\n            if self.normalize:\n                if self.asymmetric_cosine:\n                    denominator = sum_of_squared_to_alpha[columnIndex] * sum_of_squared_to_1_minus_alpha + self.shrink + 1e-06\n                else:\n                    denominator = sum_of_squared[columnIndex] * sum_of_squared + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.tanimoto_coefficient:\n                denominator = sum_of_squared[columnIndex] + sum_of_squared - this_column_weights + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.dice_coefficient:\n                denominator = sum_of_squared[columnIndex] + sum_of_squared + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.tversky_coefficient:\n                denominator = this_column_weights + (sum_of_squared[columnIndex] - this_column_weights) * self.tversky_alpha + (sum_of_squared - this_column_weights) * self.tversky_beta + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.shrink != 0:\n                this_column_weights = this_column_weights / self.shrink\n            relevant_items_partition = (-this_column_weights).argpartition(self.TopK - 1)[0:self.TopK]\n            relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n            top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n            notZerosMask = this_column_weights[top_k_idx] != 0.0\n            numNotZeros = np.sum(notZerosMask)\n            values.extend(this_column_weights[top_k_idx][notZerosMask])\n            rows.extend(top_k_idx[notZerosMask])\n            cols.extend(np.ones(numNotZeros) * columnIndex)\n        start_col_block += this_block_size\n        processed_items += this_block_size\n        if time.time() - start_time_print_batch >= 300 or end_col_block == end_col_local:\n            column_per_sec = processed_items / (time.time() - start_time + 1e-09)\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(processed_items, processed_items / (end_col_local - start_col_local) * 100, column_per_sec, new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_print_batch = time.time()\n    W_sparse = sps.csr_matrix((values, (rows, cols)), shape=(self.n_columns, self.n_columns), dtype=np.float32)\n    return W_sparse",
            "def compute_similarity(self, start_col=None, end_col=None, block_size=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    '\\n        Compute the similarity for the given dataset\\n        :param self:\\n        :param start_col: column to begin with\\n        :param end_col: column to stop before, end_col is excluded\\n        :return:\\n        '\n    values = []\n    rows = []\n    cols = []\n    start_time = time.time()\n    start_time_print_batch = start_time\n    processed_items = 0\n    if self.adjusted_cosine:\n        self.applyAdjustedCosine()\n    elif self.pearson_correlation:\n        self.applyPearsonCorrelation()\n    elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n        self.useOnlyBooleanInteractions()\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    sum_of_squared = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n    if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n        sum_of_squared = np.sqrt(sum_of_squared)\n    if self.asymmetric_cosine:\n        sum_of_squared_to_alpha = np.power(sum_of_squared + 1e-06, 2 * self.asymmetric_alpha)\n        sum_of_squared_to_1_minus_alpha = np.power(sum_of_squared + 1e-06, 2 * (1 - self.asymmetric_alpha))\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    start_col_local = 0\n    end_col_local = self.n_columns\n    if start_col is not None and start_col > 0 and (start_col < self.n_columns):\n        start_col_local = start_col\n    if end_col is not None and end_col > start_col_local and (end_col < self.n_columns):\n        end_col_local = end_col\n    start_col_block = start_col_local\n    this_block_size = 0\n    while start_col_block < end_col_local:\n        end_col_block = min(start_col_block + block_size, end_col_local)\n        this_block_size = end_col_block - start_col_block\n        item_data = self.dataMatrix[:, start_col_block:end_col_block]\n        item_data = item_data.toarray()\n        if self.use_row_weights:\n            this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n        else:\n            this_block_weights = self.dataMatrix.T.dot(item_data)\n        for col_index_in_block in range(this_block_size):\n            if this_block_size == 1:\n                this_column_weights = this_block_weights.ravel()\n            else:\n                this_column_weights = this_block_weights[:, col_index_in_block]\n            columnIndex = col_index_in_block + start_col_block\n            this_column_weights[columnIndex] = 0.0\n            if self.normalize:\n                if self.asymmetric_cosine:\n                    denominator = sum_of_squared_to_alpha[columnIndex] * sum_of_squared_to_1_minus_alpha + self.shrink + 1e-06\n                else:\n                    denominator = sum_of_squared[columnIndex] * sum_of_squared + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.tanimoto_coefficient:\n                denominator = sum_of_squared[columnIndex] + sum_of_squared - this_column_weights + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.dice_coefficient:\n                denominator = sum_of_squared[columnIndex] + sum_of_squared + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.tversky_coefficient:\n                denominator = this_column_weights + (sum_of_squared[columnIndex] - this_column_weights) * self.tversky_alpha + (sum_of_squared - this_column_weights) * self.tversky_beta + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.shrink != 0:\n                this_column_weights = this_column_weights / self.shrink\n            relevant_items_partition = (-this_column_weights).argpartition(self.TopK - 1)[0:self.TopK]\n            relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n            top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n            notZerosMask = this_column_weights[top_k_idx] != 0.0\n            numNotZeros = np.sum(notZerosMask)\n            values.extend(this_column_weights[top_k_idx][notZerosMask])\n            rows.extend(top_k_idx[notZerosMask])\n            cols.extend(np.ones(numNotZeros) * columnIndex)\n        start_col_block += this_block_size\n        processed_items += this_block_size\n        if time.time() - start_time_print_batch >= 300 or end_col_block == end_col_local:\n            column_per_sec = processed_items / (time.time() - start_time + 1e-09)\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(processed_items, processed_items / (end_col_local - start_col_local) * 100, column_per_sec, new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_print_batch = time.time()\n    W_sparse = sps.csr_matrix((values, (rows, cols)), shape=(self.n_columns, self.n_columns), dtype=np.float32)\n    return W_sparse",
            "def compute_similarity(self, start_col=None, end_col=None, block_size=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    '\\n        Compute the similarity for the given dataset\\n        :param self:\\n        :param start_col: column to begin with\\n        :param end_col: column to stop before, end_col is excluded\\n        :return:\\n        '\n    values = []\n    rows = []\n    cols = []\n    start_time = time.time()\n    start_time_print_batch = start_time\n    processed_items = 0\n    if self.adjusted_cosine:\n        self.applyAdjustedCosine()\n    elif self.pearson_correlation:\n        self.applyPearsonCorrelation()\n    elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n        self.useOnlyBooleanInteractions()\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    sum_of_squared = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n    if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n        sum_of_squared = np.sqrt(sum_of_squared)\n    if self.asymmetric_cosine:\n        sum_of_squared_to_alpha = np.power(sum_of_squared + 1e-06, 2 * self.asymmetric_alpha)\n        sum_of_squared_to_1_minus_alpha = np.power(sum_of_squared + 1e-06, 2 * (1 - self.asymmetric_alpha))\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    start_col_local = 0\n    end_col_local = self.n_columns\n    if start_col is not None and start_col > 0 and (start_col < self.n_columns):\n        start_col_local = start_col\n    if end_col is not None and end_col > start_col_local and (end_col < self.n_columns):\n        end_col_local = end_col\n    start_col_block = start_col_local\n    this_block_size = 0\n    while start_col_block < end_col_local:\n        end_col_block = min(start_col_block + block_size, end_col_local)\n        this_block_size = end_col_block - start_col_block\n        item_data = self.dataMatrix[:, start_col_block:end_col_block]\n        item_data = item_data.toarray()\n        if self.use_row_weights:\n            this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n        else:\n            this_block_weights = self.dataMatrix.T.dot(item_data)\n        for col_index_in_block in range(this_block_size):\n            if this_block_size == 1:\n                this_column_weights = this_block_weights.ravel()\n            else:\n                this_column_weights = this_block_weights[:, col_index_in_block]\n            columnIndex = col_index_in_block + start_col_block\n            this_column_weights[columnIndex] = 0.0\n            if self.normalize:\n                if self.asymmetric_cosine:\n                    denominator = sum_of_squared_to_alpha[columnIndex] * sum_of_squared_to_1_minus_alpha + self.shrink + 1e-06\n                else:\n                    denominator = sum_of_squared[columnIndex] * sum_of_squared + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.tanimoto_coefficient:\n                denominator = sum_of_squared[columnIndex] + sum_of_squared - this_column_weights + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.dice_coefficient:\n                denominator = sum_of_squared[columnIndex] + sum_of_squared + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.tversky_coefficient:\n                denominator = this_column_weights + (sum_of_squared[columnIndex] - this_column_weights) * self.tversky_alpha + (sum_of_squared - this_column_weights) * self.tversky_beta + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.shrink != 0:\n                this_column_weights = this_column_weights / self.shrink\n            relevant_items_partition = (-this_column_weights).argpartition(self.TopK - 1)[0:self.TopK]\n            relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n            top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n            notZerosMask = this_column_weights[top_k_idx] != 0.0\n            numNotZeros = np.sum(notZerosMask)\n            values.extend(this_column_weights[top_k_idx][notZerosMask])\n            rows.extend(top_k_idx[notZerosMask])\n            cols.extend(np.ones(numNotZeros) * columnIndex)\n        start_col_block += this_block_size\n        processed_items += this_block_size\n        if time.time() - start_time_print_batch >= 300 or end_col_block == end_col_local:\n            column_per_sec = processed_items / (time.time() - start_time + 1e-09)\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(processed_items, processed_items / (end_col_local - start_col_local) * 100, column_per_sec, new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_print_batch = time.time()\n    W_sparse = sps.csr_matrix((values, (rows, cols)), shape=(self.n_columns, self.n_columns), dtype=np.float32)\n    return W_sparse",
            "def compute_similarity(self, start_col=None, end_col=None, block_size=100):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    '\\n        Compute the similarity for the given dataset\\n        :param self:\\n        :param start_col: column to begin with\\n        :param end_col: column to stop before, end_col is excluded\\n        :return:\\n        '\n    values = []\n    rows = []\n    cols = []\n    start_time = time.time()\n    start_time_print_batch = start_time\n    processed_items = 0\n    if self.adjusted_cosine:\n        self.applyAdjustedCosine()\n    elif self.pearson_correlation:\n        self.applyPearsonCorrelation()\n    elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n        self.useOnlyBooleanInteractions()\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    sum_of_squared = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n    if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n        sum_of_squared = np.sqrt(sum_of_squared)\n    if self.asymmetric_cosine:\n        sum_of_squared_to_alpha = np.power(sum_of_squared + 1e-06, 2 * self.asymmetric_alpha)\n        sum_of_squared_to_1_minus_alpha = np.power(sum_of_squared + 1e-06, 2 * (1 - self.asymmetric_alpha))\n    self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n    start_col_local = 0\n    end_col_local = self.n_columns\n    if start_col is not None and start_col > 0 and (start_col < self.n_columns):\n        start_col_local = start_col\n    if end_col is not None and end_col > start_col_local and (end_col < self.n_columns):\n        end_col_local = end_col\n    start_col_block = start_col_local\n    this_block_size = 0\n    while start_col_block < end_col_local:\n        end_col_block = min(start_col_block + block_size, end_col_local)\n        this_block_size = end_col_block - start_col_block\n        item_data = self.dataMatrix[:, start_col_block:end_col_block]\n        item_data = item_data.toarray()\n        if self.use_row_weights:\n            this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n        else:\n            this_block_weights = self.dataMatrix.T.dot(item_data)\n        for col_index_in_block in range(this_block_size):\n            if this_block_size == 1:\n                this_column_weights = this_block_weights.ravel()\n            else:\n                this_column_weights = this_block_weights[:, col_index_in_block]\n            columnIndex = col_index_in_block + start_col_block\n            this_column_weights[columnIndex] = 0.0\n            if self.normalize:\n                if self.asymmetric_cosine:\n                    denominator = sum_of_squared_to_alpha[columnIndex] * sum_of_squared_to_1_minus_alpha + self.shrink + 1e-06\n                else:\n                    denominator = sum_of_squared[columnIndex] * sum_of_squared + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.tanimoto_coefficient:\n                denominator = sum_of_squared[columnIndex] + sum_of_squared - this_column_weights + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.dice_coefficient:\n                denominator = sum_of_squared[columnIndex] + sum_of_squared + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.tversky_coefficient:\n                denominator = this_column_weights + (sum_of_squared[columnIndex] - this_column_weights) * self.tversky_alpha + (sum_of_squared - this_column_weights) * self.tversky_beta + self.shrink + 1e-06\n                this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n            elif self.shrink != 0:\n                this_column_weights = this_column_weights / self.shrink\n            relevant_items_partition = (-this_column_weights).argpartition(self.TopK - 1)[0:self.TopK]\n            relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n            top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n            notZerosMask = this_column_weights[top_k_idx] != 0.0\n            numNotZeros = np.sum(notZerosMask)\n            values.extend(this_column_weights[top_k_idx][notZerosMask])\n            rows.extend(top_k_idx[notZerosMask])\n            cols.extend(np.ones(numNotZeros) * columnIndex)\n        start_col_block += this_block_size\n        processed_items += this_block_size\n        if time.time() - start_time_print_batch >= 300 or end_col_block == end_col_local:\n            column_per_sec = processed_items / (time.time() - start_time + 1e-09)\n            (new_time_value, new_time_unit) = seconds_to_biggest_unit(time.time() - start_time)\n            print('Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}'.format(processed_items, processed_items / (end_col_local - start_col_local) * 100, column_per_sec, new_time_value, new_time_unit))\n            sys.stdout.flush()\n            sys.stderr.flush()\n            start_time_print_batch = time.time()\n    W_sparse = sps.csr_matrix((values, (rows, cols)), shape=(self.n_columns, self.n_columns), dtype=np.float32)\n    return W_sparse"
        ]
    }
]