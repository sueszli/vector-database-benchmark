""" Sketching-based Matrix Computations """
import numpy as np
from scipy._lib._util import check_random_state, rng_integers
from scipy.sparse import csc_matrix
__all__ = ['clarkson_woodruff_transform']

def cwt_matrix(n_rows, n_columns, seed=None):
    if False:
        i = 10
        return i + 15
    '\n    Generate a matrix S which represents a Clarkson-Woodruff transform.\n\n    Given the desired size of matrix, the method returns a matrix S of size\n    (n_rows, n_columns) where each column has all the entries set to 0\n    except for one position which has been randomly set to +1 or -1 with\n    equal probability.\n\n    Parameters\n    ----------\n    n_rows : int\n        Number of rows of S\n    n_columns : int\n        Number of columns of S\n    seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional\n        If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n        singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used,\n        seeded with `seed`.\n        If `seed` is already a ``Generator`` or ``RandomState`` instance then\n        that instance is used.\n\n    Returns\n    -------\n    S : (n_rows, n_columns) csc_matrix\n        The returned matrix has ``n_columns`` nonzero entries.\n\n    Notes\n    -----\n    Given a matrix A, with probability at least 9/10,\n    .. math:: \\|SA\\| = (1 \\pm \\epsilon)\\|A\\|\n    Where the error epsilon is related to the size of S.\n    '
    rng = check_random_state(seed)
    rows = rng_integers(rng, 0, n_rows, n_columns)
    cols = np.arange(n_columns + 1)
    signs = rng.choice([1, -1], n_columns)
    S = csc_matrix((signs, rows, cols), shape=(n_rows, n_columns))
    return S

def clarkson_woodruff_transform(input_matrix, sketch_size, seed=None):
    if False:
        for i in range(10):
            print('nop')
    "\n    Applies a Clarkson-Woodruff Transform/sketch to the input matrix.\n\n    Given an input_matrix ``A`` of size ``(n, d)``, compute a matrix ``A'`` of\n    size (sketch_size, d) so that\n\n    .. math:: \\|Ax\\| \\approx \\|A'x\\|\n\n    with high probability via the Clarkson-Woodruff Transform, otherwise\n    known as the CountSketch matrix.\n\n    Parameters\n    ----------\n    input_matrix : array_like\n        Input matrix, of shape ``(n, d)``.\n    sketch_size : int\n        Number of rows for the sketch.\n    seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional\n        If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n        singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used,\n        seeded with `seed`.\n        If `seed` is already a ``Generator`` or ``RandomState`` instance then\n        that instance is used.\n\n    Returns\n    -------\n    A' : array_like\n        Sketch of the input matrix ``A``, of size ``(sketch_size, d)``.\n\n    Notes\n    -----\n    To make the statement\n\n    .. math:: \\|Ax\\| \\approx \\|A'x\\|\n\n    precise, observe the following result which is adapted from the\n    proof of Theorem 14 of [2]_ via Markov's Inequality. If we have\n    a sketch size ``sketch_size=k`` which is at least\n\n    .. math:: k \\geq \\frac{2}{\\epsilon^2\\delta}\n\n    Then for any fixed vector ``x``,\n\n    .. math:: \\|Ax\\| = (1\\pm\\epsilon)\\|A'x\\|\n\n    with probability at least one minus delta.\n\n    This implementation takes advantage of sparsity: computing\n    a sketch takes time proportional to ``A.nnz``. Data ``A`` which\n    is in ``scipy.sparse.csc_matrix`` format gives the quickest\n    computation time for sparse input.\n\n    >>> import numpy as np\n    >>> from scipy import linalg\n    >>> from scipy import sparse\n    >>> rng = np.random.default_rng()\n    >>> n_rows, n_columns, density, sketch_n_rows = 15000, 100, 0.01, 200\n    >>> A = sparse.rand(n_rows, n_columns, density=density, format='csc')\n    >>> B = sparse.rand(n_rows, n_columns, density=density, format='csr')\n    >>> C = sparse.rand(n_rows, n_columns, density=density, format='coo')\n    >>> D = rng.standard_normal((n_rows, n_columns))\n    >>> SA = linalg.clarkson_woodruff_transform(A, sketch_n_rows) # fastest\n    >>> SB = linalg.clarkson_woodruff_transform(B, sketch_n_rows) # fast\n    >>> SC = linalg.clarkson_woodruff_transform(C, sketch_n_rows) # slower\n    >>> SD = linalg.clarkson_woodruff_transform(D, sketch_n_rows) # slowest\n\n    That said, this method does perform well on dense inputs, just slower\n    on a relative scale.\n\n    References\n    ----------\n    .. [1] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation\n           and regression in input sparsity time. In STOC, 2013.\n    .. [2] David P. Woodruff. Sketching as a tool for numerical linear algebra.\n           In Foundations and Trends in Theoretical Computer Science, 2014.\n\n    Examples\n    --------\n    Create a big dense matrix ``A`` for the example:\n\n    >>> import numpy as np\n    >>> from scipy import linalg\n    >>> n_rows, n_columns  = 15000, 100\n    >>> rng = np.random.default_rng()\n    >>> A = rng.standard_normal((n_rows, n_columns))\n\n    Apply the transform to create a new matrix with 200 rows:\n\n    >>> sketch_n_rows = 200\n    >>> sketch = linalg.clarkson_woodruff_transform(A, sketch_n_rows, seed=rng)\n    >>> sketch.shape\n    (200, 100)\n\n    Now with high probability, the true norm is close to the sketched norm\n    in absolute value.\n\n    >>> linalg.norm(A)\n    1224.2812927123198\n    >>> linalg.norm(sketch)\n    1226.518328407333\n\n    Similarly, applying our sketch preserves the solution to a linear\n    regression of :math:`\\min \\|Ax - b\\|`.\n\n    >>> b = rng.standard_normal(n_rows)\n    >>> x = linalg.lstsq(A, b)[0]\n    >>> Ab = np.hstack((A, b.reshape(-1, 1)))\n    >>> SAb = linalg.clarkson_woodruff_transform(Ab, sketch_n_rows, seed=rng)\n    >>> SA, Sb = SAb[:, :-1], SAb[:, -1]\n    >>> x_sketched = linalg.lstsq(SA, Sb)[0]\n\n    As with the matrix norm example, ``linalg.norm(A @ x - b)`` is close\n    to ``linalg.norm(A @ x_sketched - b)`` with high probability.\n\n    >>> linalg.norm(A @ x - b)\n    122.83242365433877\n    >>> linalg.norm(A @ x_sketched - b)\n    166.58473879945151\n\n    "
    S = cwt_matrix(sketch_size, input_matrix.shape[0], seed)
    return S.dot(input_matrix)