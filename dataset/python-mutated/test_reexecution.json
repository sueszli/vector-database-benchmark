[
    {
        "func_name": "test_full_pipeline_reexecution_fs_storage",
        "original": "def test_full_pipeline_reexecution_fs_storage(self, graphql_context, snapshot):\n    selector = infer_job_selector(graphql_context, 'csv_hello_world')\n    run_id = make_new_run_id()\n    result_one = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result_one.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    result_one.data['launchPipelineExecution']['run']['runId'] = '<runId dummy value>'\n    result_one.data['launchPipelineExecution']['run']['runConfigYaml'] = '<runConfigYaml dummy value>'\n    snapshot.assert_match(result_one.data)\n    new_run_id = make_new_run_id()\n    result_two = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    query_result = result_two.data['launchPipelineReexecution']\n    assert query_result['__typename'] == 'LaunchRunSuccess'\n    assert query_result['run']['rootRunId'] == run_id\n    assert query_result['run']['parentRunId'] == run_id",
        "mutated": [
            "def test_full_pipeline_reexecution_fs_storage(self, graphql_context, snapshot):\n    if False:\n        i = 10\n    selector = infer_job_selector(graphql_context, 'csv_hello_world')\n    run_id = make_new_run_id()\n    result_one = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result_one.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    result_one.data['launchPipelineExecution']['run']['runId'] = '<runId dummy value>'\n    result_one.data['launchPipelineExecution']['run']['runConfigYaml'] = '<runConfigYaml dummy value>'\n    snapshot.assert_match(result_one.data)\n    new_run_id = make_new_run_id()\n    result_two = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    query_result = result_two.data['launchPipelineReexecution']\n    assert query_result['__typename'] == 'LaunchRunSuccess'\n    assert query_result['run']['rootRunId'] == run_id\n    assert query_result['run']['parentRunId'] == run_id",
            "def test_full_pipeline_reexecution_fs_storage(self, graphql_context, snapshot):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    selector = infer_job_selector(graphql_context, 'csv_hello_world')\n    run_id = make_new_run_id()\n    result_one = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result_one.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    result_one.data['launchPipelineExecution']['run']['runId'] = '<runId dummy value>'\n    result_one.data['launchPipelineExecution']['run']['runConfigYaml'] = '<runConfigYaml dummy value>'\n    snapshot.assert_match(result_one.data)\n    new_run_id = make_new_run_id()\n    result_two = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    query_result = result_two.data['launchPipelineReexecution']\n    assert query_result['__typename'] == 'LaunchRunSuccess'\n    assert query_result['run']['rootRunId'] == run_id\n    assert query_result['run']['parentRunId'] == run_id",
            "def test_full_pipeline_reexecution_fs_storage(self, graphql_context, snapshot):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    selector = infer_job_selector(graphql_context, 'csv_hello_world')\n    run_id = make_new_run_id()\n    result_one = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result_one.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    result_one.data['launchPipelineExecution']['run']['runId'] = '<runId dummy value>'\n    result_one.data['launchPipelineExecution']['run']['runConfigYaml'] = '<runConfigYaml dummy value>'\n    snapshot.assert_match(result_one.data)\n    new_run_id = make_new_run_id()\n    result_two = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    query_result = result_two.data['launchPipelineReexecution']\n    assert query_result['__typename'] == 'LaunchRunSuccess'\n    assert query_result['run']['rootRunId'] == run_id\n    assert query_result['run']['parentRunId'] == run_id",
            "def test_full_pipeline_reexecution_fs_storage(self, graphql_context, snapshot):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    selector = infer_job_selector(graphql_context, 'csv_hello_world')\n    run_id = make_new_run_id()\n    result_one = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result_one.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    result_one.data['launchPipelineExecution']['run']['runId'] = '<runId dummy value>'\n    result_one.data['launchPipelineExecution']['run']['runConfigYaml'] = '<runConfigYaml dummy value>'\n    snapshot.assert_match(result_one.data)\n    new_run_id = make_new_run_id()\n    result_two = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    query_result = result_two.data['launchPipelineReexecution']\n    assert query_result['__typename'] == 'LaunchRunSuccess'\n    assert query_result['run']['rootRunId'] == run_id\n    assert query_result['run']['parentRunId'] == run_id",
            "def test_full_pipeline_reexecution_fs_storage(self, graphql_context, snapshot):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    selector = infer_job_selector(graphql_context, 'csv_hello_world')\n    run_id = make_new_run_id()\n    result_one = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result_one.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    result_one.data['launchPipelineExecution']['run']['runId'] = '<runId dummy value>'\n    result_one.data['launchPipelineExecution']['run']['runConfigYaml'] = '<runConfigYaml dummy value>'\n    snapshot.assert_match(result_one.data)\n    new_run_id = make_new_run_id()\n    result_two = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    query_result = result_two.data['launchPipelineReexecution']\n    assert query_result['__typename'] == 'LaunchRunSuccess'\n    assert query_result['run']['rootRunId'] == run_id\n    assert query_result['run']['parentRunId'] == run_id"
        ]
    },
    {
        "func_name": "test_full_pipeline_reexecution_in_memory_storage",
        "original": "def test_full_pipeline_reexecution_in_memory_storage(self, graphql_context, snapshot):\n    run_id = make_new_run_id()\n    selector = infer_job_selector(graphql_context, 'csv_hello_world')\n    result_one = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result_one.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    result_one.data['launchPipelineExecution']['run']['runId'] = '<runId dummy value>'\n    result_one.data['launchPipelineExecution']['run']['runConfigYaml'] = '<runConfigYaml dummy value>'\n    snapshot.assert_match(result_one.data)\n    new_run_id = make_new_run_id()\n    result_two = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    query_result = result_two.data['launchPipelineReexecution']\n    assert query_result['__typename'] == 'LaunchRunSuccess'\n    assert query_result['run']['rootRunId'] == run_id\n    assert query_result['run']['parentRunId'] == run_id",
        "mutated": [
            "def test_full_pipeline_reexecution_in_memory_storage(self, graphql_context, snapshot):\n    if False:\n        i = 10\n    run_id = make_new_run_id()\n    selector = infer_job_selector(graphql_context, 'csv_hello_world')\n    result_one = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result_one.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    result_one.data['launchPipelineExecution']['run']['runId'] = '<runId dummy value>'\n    result_one.data['launchPipelineExecution']['run']['runConfigYaml'] = '<runConfigYaml dummy value>'\n    snapshot.assert_match(result_one.data)\n    new_run_id = make_new_run_id()\n    result_two = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    query_result = result_two.data['launchPipelineReexecution']\n    assert query_result['__typename'] == 'LaunchRunSuccess'\n    assert query_result['run']['rootRunId'] == run_id\n    assert query_result['run']['parentRunId'] == run_id",
            "def test_full_pipeline_reexecution_in_memory_storage(self, graphql_context, snapshot):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    run_id = make_new_run_id()\n    selector = infer_job_selector(graphql_context, 'csv_hello_world')\n    result_one = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result_one.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    result_one.data['launchPipelineExecution']['run']['runId'] = '<runId dummy value>'\n    result_one.data['launchPipelineExecution']['run']['runConfigYaml'] = '<runConfigYaml dummy value>'\n    snapshot.assert_match(result_one.data)\n    new_run_id = make_new_run_id()\n    result_two = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    query_result = result_two.data['launchPipelineReexecution']\n    assert query_result['__typename'] == 'LaunchRunSuccess'\n    assert query_result['run']['rootRunId'] == run_id\n    assert query_result['run']['parentRunId'] == run_id",
            "def test_full_pipeline_reexecution_in_memory_storage(self, graphql_context, snapshot):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    run_id = make_new_run_id()\n    selector = infer_job_selector(graphql_context, 'csv_hello_world')\n    result_one = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result_one.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    result_one.data['launchPipelineExecution']['run']['runId'] = '<runId dummy value>'\n    result_one.data['launchPipelineExecution']['run']['runConfigYaml'] = '<runConfigYaml dummy value>'\n    snapshot.assert_match(result_one.data)\n    new_run_id = make_new_run_id()\n    result_two = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    query_result = result_two.data['launchPipelineReexecution']\n    assert query_result['__typename'] == 'LaunchRunSuccess'\n    assert query_result['run']['rootRunId'] == run_id\n    assert query_result['run']['parentRunId'] == run_id",
            "def test_full_pipeline_reexecution_in_memory_storage(self, graphql_context, snapshot):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    run_id = make_new_run_id()\n    selector = infer_job_selector(graphql_context, 'csv_hello_world')\n    result_one = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result_one.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    result_one.data['launchPipelineExecution']['run']['runId'] = '<runId dummy value>'\n    result_one.data['launchPipelineExecution']['run']['runConfigYaml'] = '<runConfigYaml dummy value>'\n    snapshot.assert_match(result_one.data)\n    new_run_id = make_new_run_id()\n    result_two = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    query_result = result_two.data['launchPipelineReexecution']\n    assert query_result['__typename'] == 'LaunchRunSuccess'\n    assert query_result['run']['rootRunId'] == run_id\n    assert query_result['run']['parentRunId'] == run_id",
            "def test_full_pipeline_reexecution_in_memory_storage(self, graphql_context, snapshot):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    run_id = make_new_run_id()\n    selector = infer_job_selector(graphql_context, 'csv_hello_world')\n    result_one = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result_one.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    result_one.data['launchPipelineExecution']['run']['runId'] = '<runId dummy value>'\n    result_one.data['launchPipelineExecution']['run']['runConfigYaml'] = '<runConfigYaml dummy value>'\n    snapshot.assert_match(result_one.data)\n    new_run_id = make_new_run_id()\n    result_two = execute_dagster_graphql(graphql_context, LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'runConfigData': csv_hello_world_ops_config(), 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    query_result = result_two.data['launchPipelineReexecution']\n    assert query_result['__typename'] == 'LaunchRunSuccess'\n    assert query_result['run']['rootRunId'] == run_id\n    assert query_result['run']['parentRunId'] == run_id"
        ]
    },
    {
        "func_name": "test_pipeline_reexecution_successful_launch",
        "original": "def test_pipeline_reexecution_successful_launch(self, graphql_context):\n    selector = infer_job_selector(graphql_context, 'no_config_job')\n    run_id = make_new_run_id()\n    result = execute_dagster_graphql(context=graphql_context, query=LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    assert result.data['launchPipelineExecution']['run']['status'] == 'STARTING'\n    wait_for_runs_to_finish(graphql_context.instance)\n    result = execute_dagster_graphql(context=graphql_context, query=RUN_QUERY, variables={'runId': run_id})\n    assert result.data['pipelineRunOrError']['__typename'] == 'Run'\n    assert result.data['pipelineRunOrError']['status'] == 'SUCCESS'\n    new_run_id = make_new_run_id()\n    result = execute_dagster_graphql(context=graphql_context, query=LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    assert result.data['launchPipelineReexecution']['__typename'] == 'LaunchRunSuccess'\n    wait_for_runs_to_finish(graphql_context.instance)\n    result = execute_dagster_graphql(context=graphql_context, query=RUN_QUERY, variables={'runId': new_run_id})\n    assert result.data['pipelineRunOrError']['__typename'] == 'Run'\n    assert result.data['pipelineRunOrError']['status'] == 'SUCCESS'",
        "mutated": [
            "def test_pipeline_reexecution_successful_launch(self, graphql_context):\n    if False:\n        i = 10\n    selector = infer_job_selector(graphql_context, 'no_config_job')\n    run_id = make_new_run_id()\n    result = execute_dagster_graphql(context=graphql_context, query=LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    assert result.data['launchPipelineExecution']['run']['status'] == 'STARTING'\n    wait_for_runs_to_finish(graphql_context.instance)\n    result = execute_dagster_graphql(context=graphql_context, query=RUN_QUERY, variables={'runId': run_id})\n    assert result.data['pipelineRunOrError']['__typename'] == 'Run'\n    assert result.data['pipelineRunOrError']['status'] == 'SUCCESS'\n    new_run_id = make_new_run_id()\n    result = execute_dagster_graphql(context=graphql_context, query=LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    assert result.data['launchPipelineReexecution']['__typename'] == 'LaunchRunSuccess'\n    wait_for_runs_to_finish(graphql_context.instance)\n    result = execute_dagster_graphql(context=graphql_context, query=RUN_QUERY, variables={'runId': new_run_id})\n    assert result.data['pipelineRunOrError']['__typename'] == 'Run'\n    assert result.data['pipelineRunOrError']['status'] == 'SUCCESS'",
            "def test_pipeline_reexecution_successful_launch(self, graphql_context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    selector = infer_job_selector(graphql_context, 'no_config_job')\n    run_id = make_new_run_id()\n    result = execute_dagster_graphql(context=graphql_context, query=LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    assert result.data['launchPipelineExecution']['run']['status'] == 'STARTING'\n    wait_for_runs_to_finish(graphql_context.instance)\n    result = execute_dagster_graphql(context=graphql_context, query=RUN_QUERY, variables={'runId': run_id})\n    assert result.data['pipelineRunOrError']['__typename'] == 'Run'\n    assert result.data['pipelineRunOrError']['status'] == 'SUCCESS'\n    new_run_id = make_new_run_id()\n    result = execute_dagster_graphql(context=graphql_context, query=LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    assert result.data['launchPipelineReexecution']['__typename'] == 'LaunchRunSuccess'\n    wait_for_runs_to_finish(graphql_context.instance)\n    result = execute_dagster_graphql(context=graphql_context, query=RUN_QUERY, variables={'runId': new_run_id})\n    assert result.data['pipelineRunOrError']['__typename'] == 'Run'\n    assert result.data['pipelineRunOrError']['status'] == 'SUCCESS'",
            "def test_pipeline_reexecution_successful_launch(self, graphql_context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    selector = infer_job_selector(graphql_context, 'no_config_job')\n    run_id = make_new_run_id()\n    result = execute_dagster_graphql(context=graphql_context, query=LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    assert result.data['launchPipelineExecution']['run']['status'] == 'STARTING'\n    wait_for_runs_to_finish(graphql_context.instance)\n    result = execute_dagster_graphql(context=graphql_context, query=RUN_QUERY, variables={'runId': run_id})\n    assert result.data['pipelineRunOrError']['__typename'] == 'Run'\n    assert result.data['pipelineRunOrError']['status'] == 'SUCCESS'\n    new_run_id = make_new_run_id()\n    result = execute_dagster_graphql(context=graphql_context, query=LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    assert result.data['launchPipelineReexecution']['__typename'] == 'LaunchRunSuccess'\n    wait_for_runs_to_finish(graphql_context.instance)\n    result = execute_dagster_graphql(context=graphql_context, query=RUN_QUERY, variables={'runId': new_run_id})\n    assert result.data['pipelineRunOrError']['__typename'] == 'Run'\n    assert result.data['pipelineRunOrError']['status'] == 'SUCCESS'",
            "def test_pipeline_reexecution_successful_launch(self, graphql_context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    selector = infer_job_selector(graphql_context, 'no_config_job')\n    run_id = make_new_run_id()\n    result = execute_dagster_graphql(context=graphql_context, query=LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    assert result.data['launchPipelineExecution']['run']['status'] == 'STARTING'\n    wait_for_runs_to_finish(graphql_context.instance)\n    result = execute_dagster_graphql(context=graphql_context, query=RUN_QUERY, variables={'runId': run_id})\n    assert result.data['pipelineRunOrError']['__typename'] == 'Run'\n    assert result.data['pipelineRunOrError']['status'] == 'SUCCESS'\n    new_run_id = make_new_run_id()\n    result = execute_dagster_graphql(context=graphql_context, query=LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    assert result.data['launchPipelineReexecution']['__typename'] == 'LaunchRunSuccess'\n    wait_for_runs_to_finish(graphql_context.instance)\n    result = execute_dagster_graphql(context=graphql_context, query=RUN_QUERY, variables={'runId': new_run_id})\n    assert result.data['pipelineRunOrError']['__typename'] == 'Run'\n    assert result.data['pipelineRunOrError']['status'] == 'SUCCESS'",
            "def test_pipeline_reexecution_successful_launch(self, graphql_context):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    selector = infer_job_selector(graphql_context, 'no_config_job')\n    run_id = make_new_run_id()\n    result = execute_dagster_graphql(context=graphql_context, query=LAUNCH_PIPELINE_EXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'executionMetadata': {'runId': run_id}, 'mode': 'default'}})\n    assert result.data['launchPipelineExecution']['__typename'] == 'LaunchRunSuccess'\n    assert result.data['launchPipelineExecution']['run']['status'] == 'STARTING'\n    wait_for_runs_to_finish(graphql_context.instance)\n    result = execute_dagster_graphql(context=graphql_context, query=RUN_QUERY, variables={'runId': run_id})\n    assert result.data['pipelineRunOrError']['__typename'] == 'Run'\n    assert result.data['pipelineRunOrError']['status'] == 'SUCCESS'\n    new_run_id = make_new_run_id()\n    result = execute_dagster_graphql(context=graphql_context, query=LAUNCH_PIPELINE_REEXECUTION_MUTATION, variables={'executionParams': {'selector': selector, 'executionMetadata': {'runId': new_run_id, 'rootRunId': run_id, 'parentRunId': run_id}, 'mode': 'default'}})\n    assert result.data['launchPipelineReexecution']['__typename'] == 'LaunchRunSuccess'\n    wait_for_runs_to_finish(graphql_context.instance)\n    result = execute_dagster_graphql(context=graphql_context, query=RUN_QUERY, variables={'runId': new_run_id})\n    assert result.data['pipelineRunOrError']['__typename'] == 'Run'\n    assert result.data['pipelineRunOrError']['status'] == 'SUCCESS'"
        ]
    }
]