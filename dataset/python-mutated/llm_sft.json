[
    {
        "func_name": "__post_init__",
        "original": "def __post_init__(self):\n    if self.sft_type == 'lora':\n        if self.learning_rate is None:\n            self.learning_rate = 0.0001\n        if self.save_trainer_state is None:\n            self.save_trainer_state = True\n        if self.last_save_interval is None:\n            self.last_save_interval = self.eval_interval\n    elif self.sft_type == 'full':\n        if self.learning_rate is None:\n            self.learning_rate = 1e-05\n        if self.save_trainer_state is None:\n            self.save_trainer_state = False\n        if self.last_save_interval is None:\n            self.last_save_interval = self.eval_interval * 4\n    else:\n        raise ValueError(f'sft_type: {self.sft_type}')\n    if self.output_dir is None:\n        self.output_dir = 'runs'\n    self.output_dir = os.path.join(self.output_dir, self.model_type)\n    if self.lora_target_modules is None:\n        self.lora_target_modules = MODEL_MAPPING[self.model_type]['lora_TM']\n    if self.use_flash_attn is None:\n        self.use_flash_attn = 'auto'",
        "mutated": [
            "def __post_init__(self):\n    if False:\n        i = 10\n    if self.sft_type == 'lora':\n        if self.learning_rate is None:\n            self.learning_rate = 0.0001\n        if self.save_trainer_state is None:\n            self.save_trainer_state = True\n        if self.last_save_interval is None:\n            self.last_save_interval = self.eval_interval\n    elif self.sft_type == 'full':\n        if self.learning_rate is None:\n            self.learning_rate = 1e-05\n        if self.save_trainer_state is None:\n            self.save_trainer_state = False\n        if self.last_save_interval is None:\n            self.last_save_interval = self.eval_interval * 4\n    else:\n        raise ValueError(f'sft_type: {self.sft_type}')\n    if self.output_dir is None:\n        self.output_dir = 'runs'\n    self.output_dir = os.path.join(self.output_dir, self.model_type)\n    if self.lora_target_modules is None:\n        self.lora_target_modules = MODEL_MAPPING[self.model_type]['lora_TM']\n    if self.use_flash_attn is None:\n        self.use_flash_attn = 'auto'",
            "def __post_init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    if self.sft_type == 'lora':\n        if self.learning_rate is None:\n            self.learning_rate = 0.0001\n        if self.save_trainer_state is None:\n            self.save_trainer_state = True\n        if self.last_save_interval is None:\n            self.last_save_interval = self.eval_interval\n    elif self.sft_type == 'full':\n        if self.learning_rate is None:\n            self.learning_rate = 1e-05\n        if self.save_trainer_state is None:\n            self.save_trainer_state = False\n        if self.last_save_interval is None:\n            self.last_save_interval = self.eval_interval * 4\n    else:\n        raise ValueError(f'sft_type: {self.sft_type}')\n    if self.output_dir is None:\n        self.output_dir = 'runs'\n    self.output_dir = os.path.join(self.output_dir, self.model_type)\n    if self.lora_target_modules is None:\n        self.lora_target_modules = MODEL_MAPPING[self.model_type]['lora_TM']\n    if self.use_flash_attn is None:\n        self.use_flash_attn = 'auto'",
            "def __post_init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    if self.sft_type == 'lora':\n        if self.learning_rate is None:\n            self.learning_rate = 0.0001\n        if self.save_trainer_state is None:\n            self.save_trainer_state = True\n        if self.last_save_interval is None:\n            self.last_save_interval = self.eval_interval\n    elif self.sft_type == 'full':\n        if self.learning_rate is None:\n            self.learning_rate = 1e-05\n        if self.save_trainer_state is None:\n            self.save_trainer_state = False\n        if self.last_save_interval is None:\n            self.last_save_interval = self.eval_interval * 4\n    else:\n        raise ValueError(f'sft_type: {self.sft_type}')\n    if self.output_dir is None:\n        self.output_dir = 'runs'\n    self.output_dir = os.path.join(self.output_dir, self.model_type)\n    if self.lora_target_modules is None:\n        self.lora_target_modules = MODEL_MAPPING[self.model_type]['lora_TM']\n    if self.use_flash_attn is None:\n        self.use_flash_attn = 'auto'",
            "def __post_init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    if self.sft_type == 'lora':\n        if self.learning_rate is None:\n            self.learning_rate = 0.0001\n        if self.save_trainer_state is None:\n            self.save_trainer_state = True\n        if self.last_save_interval is None:\n            self.last_save_interval = self.eval_interval\n    elif self.sft_type == 'full':\n        if self.learning_rate is None:\n            self.learning_rate = 1e-05\n        if self.save_trainer_state is None:\n            self.save_trainer_state = False\n        if self.last_save_interval is None:\n            self.last_save_interval = self.eval_interval * 4\n    else:\n        raise ValueError(f'sft_type: {self.sft_type}')\n    if self.output_dir is None:\n        self.output_dir = 'runs'\n    self.output_dir = os.path.join(self.output_dir, self.model_type)\n    if self.lora_target_modules is None:\n        self.lora_target_modules = MODEL_MAPPING[self.model_type]['lora_TM']\n    if self.use_flash_attn is None:\n        self.use_flash_attn = 'auto'",
            "def __post_init__(self):\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    if self.sft_type == 'lora':\n        if self.learning_rate is None:\n            self.learning_rate = 0.0001\n        if self.save_trainer_state is None:\n            self.save_trainer_state = True\n        if self.last_save_interval is None:\n            self.last_save_interval = self.eval_interval\n    elif self.sft_type == 'full':\n        if self.learning_rate is None:\n            self.learning_rate = 1e-05\n        if self.save_trainer_state is None:\n            self.save_trainer_state = False\n        if self.last_save_interval is None:\n            self.last_save_interval = self.eval_interval * 4\n    else:\n        raise ValueError(f'sft_type: {self.sft_type}')\n    if self.output_dir is None:\n        self.output_dir = 'runs'\n    self.output_dir = os.path.join(self.output_dir, self.model_type)\n    if self.lora_target_modules is None:\n        self.lora_target_modules = MODEL_MAPPING[self.model_type]['lora_TM']\n    if self.use_flash_attn is None:\n        self.use_flash_attn = 'auto'"
        ]
    },
    {
        "func_name": "cfg_modify_fn",
        "original": "def cfg_modify_fn(cfg: Config) -> Config:\n    cfg.update(config)\n    return cfg",
        "mutated": [
            "def cfg_modify_fn(cfg: Config) -> Config:\n    if False:\n        i = 10\n    cfg.update(config)\n    return cfg",
            "def cfg_modify_fn(cfg: Config) -> Config:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    cfg.update(config)\n    return cfg",
            "def cfg_modify_fn(cfg: Config) -> Config:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    cfg.update(config)\n    return cfg",
            "def cfg_modify_fn(cfg: Config) -> Config:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    cfg.update(config)\n    return cfg",
            "def cfg_modify_fn(cfg: Config) -> Config:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    cfg.update(config)\n    return cfg"
        ]
    },
    {
        "func_name": "llm_sft",
        "original": "def llm_sft(args: SftArguments) -> None:\n    seed_everything(args.seed)\n    support_bf16 = torch.cuda.is_bf16_supported()\n    if not support_bf16:\n        logger.warning(f'support_bf16: {support_bf16}')\n    kwargs = {'low_cpu_mem_usage': True, 'device_map': 'auto'}\n    if args.model_type == 'qwen-7b':\n        kwargs['use_flash_attn'] = args.use_flash_attn\n    (model, tokenizer, model_dir) = get_model_tokenizer(args.model_type, torch_dtype=torch.bfloat16, **kwargs)\n    if args.gradient_checkpoint:\n        model.gradient_checkpointing_enable()\n        model.enable_input_require_grads()\n    if args.sft_type == 'lora':\n        lora_config = LoRAConfig(target_modules=args.lora_target_modules, r=args.lora_rank, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout_p)\n        logger.info(f'lora_config: {lora_config}')\n        model = Swift.prepare_model(model, lora_config)\n    show_freeze_layers(model)\n    print_model_info(model)\n    _p: Tensor = list(model.parameters())[-1]\n    logger.info(f'device: {_p.device}, dtype: {_p.dtype}')\n    dataset = get_dataset(args.dataset.split(','))\n    (train_dataset, val_dataset) = process_dataset(dataset, args.dataset_test_size, args.dataset_sample, args.dataset_seed)\n    tokenize_func = partial(tokenize_function, tokenizer=tokenizer, prompt=args.prompt, max_length=args.max_length)\n    train_dataset = train_dataset.map(tokenize_func)\n    val_dataset = val_dataset.map(tokenize_func)\n    del dataset\n    stat_dataset(train_dataset)\n    stat_dataset(val_dataset)\n    data_collator = partial(data_collate_fn, tokenizer=tokenizer)\n    print_example(train_dataset[0], tokenizer)\n    cfg_file = os.path.join(model_dir, 'configuration.json')\n    T_max = get_T_max(len(train_dataset), args.batch_size, args.max_epochs, True)\n    work_dir = get_work_dir(args.output_dir)\n    config = Config({'train': {'dataloader': {'batch_size_per_gpu': args.batch_size, 'workers_per_gpu': 1, 'shuffle': True, 'drop_last': True, 'pin_memory': True}, 'max_epochs': args.max_epochs, 'work_dir': work_dir, 'optimizer': {'type': 'AdamW', 'lr': args.learning_rate, 'weight_decay': args.weight_decay, 'options': {'cumulative_iters': args.n_accumulate_grad, 'grad_clip': {'norm_type': 2, 'max_norm': args.grad_clip_norm}}}, 'lr_scheduler': {'type': 'CosineAnnealingLR', 'T_max': T_max, 'eta_min': args.learning_rate * 0.1, 'options': {'by_epoch': False, 'warmup': {'type': 'LinearWarmup', 'warmup_ratio': 0.1, 'warmup_iters': args.warmup_iters}}}, 'hooks': [{'type': 'CheckpointHook', 'by_epoch': False, 'interval': args.last_save_interval, 'max_checkpoint_num': args.last_max_checkpoint_num, 'save_trainer_state': args.save_trainer_state}, {'type': 'EvaluationHook', 'by_epoch': False, 'interval': args.eval_interval}, {'type': 'BestCkptSaverHook', 'metric_key': 'loss', 'save_best': True, 'rule': 'min', 'max_checkpoint_num': args.best_max_checkpoint_num, 'save_trainer_state': args.save_trainer_state}, {'type': 'TextLoggerHook', 'by_epoch': True, 'interval': args.logging_interval}, {'type': 'TensorboardHook', 'by_epoch': False, 'interval': args.tb_interval}]}, 'evaluation': {'dataloader': {'batch_size_per_gpu': args.batch_size, 'workers_per_gpu': 1, 'shuffle': False, 'drop_last': False, 'pin_memory': True}, 'metrics': [{'type': 'my_metric', 'vocab_size': tokenizer.vocab_size}]}})\n\n    def cfg_modify_fn(cfg: Config) -> Config:\n        cfg.update(config)\n        return cfg\n    trainer = EpochBasedTrainer(model=model, cfg_file=cfg_file, data_collator=data_collator, train_dataset=train_dataset, eval_dataset=val_dataset, remove_unused_data=True, seed=42, cfg_modify_fn=cfg_modify_fn)\n    trainer.train()\n    tb_dir = os.path.join(work_dir, 'tensorboard_output')\n    plot_images(tb_dir, ['loss'], 0.9)",
        "mutated": [
            "def llm_sft(args: SftArguments) -> None:\n    if False:\n        i = 10\n    seed_everything(args.seed)\n    support_bf16 = torch.cuda.is_bf16_supported()\n    if not support_bf16:\n        logger.warning(f'support_bf16: {support_bf16}')\n    kwargs = {'low_cpu_mem_usage': True, 'device_map': 'auto'}\n    if args.model_type == 'qwen-7b':\n        kwargs['use_flash_attn'] = args.use_flash_attn\n    (model, tokenizer, model_dir) = get_model_tokenizer(args.model_type, torch_dtype=torch.bfloat16, **kwargs)\n    if args.gradient_checkpoint:\n        model.gradient_checkpointing_enable()\n        model.enable_input_require_grads()\n    if args.sft_type == 'lora':\n        lora_config = LoRAConfig(target_modules=args.lora_target_modules, r=args.lora_rank, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout_p)\n        logger.info(f'lora_config: {lora_config}')\n        model = Swift.prepare_model(model, lora_config)\n    show_freeze_layers(model)\n    print_model_info(model)\n    _p: Tensor = list(model.parameters())[-1]\n    logger.info(f'device: {_p.device}, dtype: {_p.dtype}')\n    dataset = get_dataset(args.dataset.split(','))\n    (train_dataset, val_dataset) = process_dataset(dataset, args.dataset_test_size, args.dataset_sample, args.dataset_seed)\n    tokenize_func = partial(tokenize_function, tokenizer=tokenizer, prompt=args.prompt, max_length=args.max_length)\n    train_dataset = train_dataset.map(tokenize_func)\n    val_dataset = val_dataset.map(tokenize_func)\n    del dataset\n    stat_dataset(train_dataset)\n    stat_dataset(val_dataset)\n    data_collator = partial(data_collate_fn, tokenizer=tokenizer)\n    print_example(train_dataset[0], tokenizer)\n    cfg_file = os.path.join(model_dir, 'configuration.json')\n    T_max = get_T_max(len(train_dataset), args.batch_size, args.max_epochs, True)\n    work_dir = get_work_dir(args.output_dir)\n    config = Config({'train': {'dataloader': {'batch_size_per_gpu': args.batch_size, 'workers_per_gpu': 1, 'shuffle': True, 'drop_last': True, 'pin_memory': True}, 'max_epochs': args.max_epochs, 'work_dir': work_dir, 'optimizer': {'type': 'AdamW', 'lr': args.learning_rate, 'weight_decay': args.weight_decay, 'options': {'cumulative_iters': args.n_accumulate_grad, 'grad_clip': {'norm_type': 2, 'max_norm': args.grad_clip_norm}}}, 'lr_scheduler': {'type': 'CosineAnnealingLR', 'T_max': T_max, 'eta_min': args.learning_rate * 0.1, 'options': {'by_epoch': False, 'warmup': {'type': 'LinearWarmup', 'warmup_ratio': 0.1, 'warmup_iters': args.warmup_iters}}}, 'hooks': [{'type': 'CheckpointHook', 'by_epoch': False, 'interval': args.last_save_interval, 'max_checkpoint_num': args.last_max_checkpoint_num, 'save_trainer_state': args.save_trainer_state}, {'type': 'EvaluationHook', 'by_epoch': False, 'interval': args.eval_interval}, {'type': 'BestCkptSaverHook', 'metric_key': 'loss', 'save_best': True, 'rule': 'min', 'max_checkpoint_num': args.best_max_checkpoint_num, 'save_trainer_state': args.save_trainer_state}, {'type': 'TextLoggerHook', 'by_epoch': True, 'interval': args.logging_interval}, {'type': 'TensorboardHook', 'by_epoch': False, 'interval': args.tb_interval}]}, 'evaluation': {'dataloader': {'batch_size_per_gpu': args.batch_size, 'workers_per_gpu': 1, 'shuffle': False, 'drop_last': False, 'pin_memory': True}, 'metrics': [{'type': 'my_metric', 'vocab_size': tokenizer.vocab_size}]}})\n\n    def cfg_modify_fn(cfg: Config) -> Config:\n        cfg.update(config)\n        return cfg\n    trainer = EpochBasedTrainer(model=model, cfg_file=cfg_file, data_collator=data_collator, train_dataset=train_dataset, eval_dataset=val_dataset, remove_unused_data=True, seed=42, cfg_modify_fn=cfg_modify_fn)\n    trainer.train()\n    tb_dir = os.path.join(work_dir, 'tensorboard_output')\n    plot_images(tb_dir, ['loss'], 0.9)",
            "def llm_sft(args: SftArguments) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    seed_everything(args.seed)\n    support_bf16 = torch.cuda.is_bf16_supported()\n    if not support_bf16:\n        logger.warning(f'support_bf16: {support_bf16}')\n    kwargs = {'low_cpu_mem_usage': True, 'device_map': 'auto'}\n    if args.model_type == 'qwen-7b':\n        kwargs['use_flash_attn'] = args.use_flash_attn\n    (model, tokenizer, model_dir) = get_model_tokenizer(args.model_type, torch_dtype=torch.bfloat16, **kwargs)\n    if args.gradient_checkpoint:\n        model.gradient_checkpointing_enable()\n        model.enable_input_require_grads()\n    if args.sft_type == 'lora':\n        lora_config = LoRAConfig(target_modules=args.lora_target_modules, r=args.lora_rank, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout_p)\n        logger.info(f'lora_config: {lora_config}')\n        model = Swift.prepare_model(model, lora_config)\n    show_freeze_layers(model)\n    print_model_info(model)\n    _p: Tensor = list(model.parameters())[-1]\n    logger.info(f'device: {_p.device}, dtype: {_p.dtype}')\n    dataset = get_dataset(args.dataset.split(','))\n    (train_dataset, val_dataset) = process_dataset(dataset, args.dataset_test_size, args.dataset_sample, args.dataset_seed)\n    tokenize_func = partial(tokenize_function, tokenizer=tokenizer, prompt=args.prompt, max_length=args.max_length)\n    train_dataset = train_dataset.map(tokenize_func)\n    val_dataset = val_dataset.map(tokenize_func)\n    del dataset\n    stat_dataset(train_dataset)\n    stat_dataset(val_dataset)\n    data_collator = partial(data_collate_fn, tokenizer=tokenizer)\n    print_example(train_dataset[0], tokenizer)\n    cfg_file = os.path.join(model_dir, 'configuration.json')\n    T_max = get_T_max(len(train_dataset), args.batch_size, args.max_epochs, True)\n    work_dir = get_work_dir(args.output_dir)\n    config = Config({'train': {'dataloader': {'batch_size_per_gpu': args.batch_size, 'workers_per_gpu': 1, 'shuffle': True, 'drop_last': True, 'pin_memory': True}, 'max_epochs': args.max_epochs, 'work_dir': work_dir, 'optimizer': {'type': 'AdamW', 'lr': args.learning_rate, 'weight_decay': args.weight_decay, 'options': {'cumulative_iters': args.n_accumulate_grad, 'grad_clip': {'norm_type': 2, 'max_norm': args.grad_clip_norm}}}, 'lr_scheduler': {'type': 'CosineAnnealingLR', 'T_max': T_max, 'eta_min': args.learning_rate * 0.1, 'options': {'by_epoch': False, 'warmup': {'type': 'LinearWarmup', 'warmup_ratio': 0.1, 'warmup_iters': args.warmup_iters}}}, 'hooks': [{'type': 'CheckpointHook', 'by_epoch': False, 'interval': args.last_save_interval, 'max_checkpoint_num': args.last_max_checkpoint_num, 'save_trainer_state': args.save_trainer_state}, {'type': 'EvaluationHook', 'by_epoch': False, 'interval': args.eval_interval}, {'type': 'BestCkptSaverHook', 'metric_key': 'loss', 'save_best': True, 'rule': 'min', 'max_checkpoint_num': args.best_max_checkpoint_num, 'save_trainer_state': args.save_trainer_state}, {'type': 'TextLoggerHook', 'by_epoch': True, 'interval': args.logging_interval}, {'type': 'TensorboardHook', 'by_epoch': False, 'interval': args.tb_interval}]}, 'evaluation': {'dataloader': {'batch_size_per_gpu': args.batch_size, 'workers_per_gpu': 1, 'shuffle': False, 'drop_last': False, 'pin_memory': True}, 'metrics': [{'type': 'my_metric', 'vocab_size': tokenizer.vocab_size}]}})\n\n    def cfg_modify_fn(cfg: Config) -> Config:\n        cfg.update(config)\n        return cfg\n    trainer = EpochBasedTrainer(model=model, cfg_file=cfg_file, data_collator=data_collator, train_dataset=train_dataset, eval_dataset=val_dataset, remove_unused_data=True, seed=42, cfg_modify_fn=cfg_modify_fn)\n    trainer.train()\n    tb_dir = os.path.join(work_dir, 'tensorboard_output')\n    plot_images(tb_dir, ['loss'], 0.9)",
            "def llm_sft(args: SftArguments) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    seed_everything(args.seed)\n    support_bf16 = torch.cuda.is_bf16_supported()\n    if not support_bf16:\n        logger.warning(f'support_bf16: {support_bf16}')\n    kwargs = {'low_cpu_mem_usage': True, 'device_map': 'auto'}\n    if args.model_type == 'qwen-7b':\n        kwargs['use_flash_attn'] = args.use_flash_attn\n    (model, tokenizer, model_dir) = get_model_tokenizer(args.model_type, torch_dtype=torch.bfloat16, **kwargs)\n    if args.gradient_checkpoint:\n        model.gradient_checkpointing_enable()\n        model.enable_input_require_grads()\n    if args.sft_type == 'lora':\n        lora_config = LoRAConfig(target_modules=args.lora_target_modules, r=args.lora_rank, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout_p)\n        logger.info(f'lora_config: {lora_config}')\n        model = Swift.prepare_model(model, lora_config)\n    show_freeze_layers(model)\n    print_model_info(model)\n    _p: Tensor = list(model.parameters())[-1]\n    logger.info(f'device: {_p.device}, dtype: {_p.dtype}')\n    dataset = get_dataset(args.dataset.split(','))\n    (train_dataset, val_dataset) = process_dataset(dataset, args.dataset_test_size, args.dataset_sample, args.dataset_seed)\n    tokenize_func = partial(tokenize_function, tokenizer=tokenizer, prompt=args.prompt, max_length=args.max_length)\n    train_dataset = train_dataset.map(tokenize_func)\n    val_dataset = val_dataset.map(tokenize_func)\n    del dataset\n    stat_dataset(train_dataset)\n    stat_dataset(val_dataset)\n    data_collator = partial(data_collate_fn, tokenizer=tokenizer)\n    print_example(train_dataset[0], tokenizer)\n    cfg_file = os.path.join(model_dir, 'configuration.json')\n    T_max = get_T_max(len(train_dataset), args.batch_size, args.max_epochs, True)\n    work_dir = get_work_dir(args.output_dir)\n    config = Config({'train': {'dataloader': {'batch_size_per_gpu': args.batch_size, 'workers_per_gpu': 1, 'shuffle': True, 'drop_last': True, 'pin_memory': True}, 'max_epochs': args.max_epochs, 'work_dir': work_dir, 'optimizer': {'type': 'AdamW', 'lr': args.learning_rate, 'weight_decay': args.weight_decay, 'options': {'cumulative_iters': args.n_accumulate_grad, 'grad_clip': {'norm_type': 2, 'max_norm': args.grad_clip_norm}}}, 'lr_scheduler': {'type': 'CosineAnnealingLR', 'T_max': T_max, 'eta_min': args.learning_rate * 0.1, 'options': {'by_epoch': False, 'warmup': {'type': 'LinearWarmup', 'warmup_ratio': 0.1, 'warmup_iters': args.warmup_iters}}}, 'hooks': [{'type': 'CheckpointHook', 'by_epoch': False, 'interval': args.last_save_interval, 'max_checkpoint_num': args.last_max_checkpoint_num, 'save_trainer_state': args.save_trainer_state}, {'type': 'EvaluationHook', 'by_epoch': False, 'interval': args.eval_interval}, {'type': 'BestCkptSaverHook', 'metric_key': 'loss', 'save_best': True, 'rule': 'min', 'max_checkpoint_num': args.best_max_checkpoint_num, 'save_trainer_state': args.save_trainer_state}, {'type': 'TextLoggerHook', 'by_epoch': True, 'interval': args.logging_interval}, {'type': 'TensorboardHook', 'by_epoch': False, 'interval': args.tb_interval}]}, 'evaluation': {'dataloader': {'batch_size_per_gpu': args.batch_size, 'workers_per_gpu': 1, 'shuffle': False, 'drop_last': False, 'pin_memory': True}, 'metrics': [{'type': 'my_metric', 'vocab_size': tokenizer.vocab_size}]}})\n\n    def cfg_modify_fn(cfg: Config) -> Config:\n        cfg.update(config)\n        return cfg\n    trainer = EpochBasedTrainer(model=model, cfg_file=cfg_file, data_collator=data_collator, train_dataset=train_dataset, eval_dataset=val_dataset, remove_unused_data=True, seed=42, cfg_modify_fn=cfg_modify_fn)\n    trainer.train()\n    tb_dir = os.path.join(work_dir, 'tensorboard_output')\n    plot_images(tb_dir, ['loss'], 0.9)",
            "def llm_sft(args: SftArguments) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    seed_everything(args.seed)\n    support_bf16 = torch.cuda.is_bf16_supported()\n    if not support_bf16:\n        logger.warning(f'support_bf16: {support_bf16}')\n    kwargs = {'low_cpu_mem_usage': True, 'device_map': 'auto'}\n    if args.model_type == 'qwen-7b':\n        kwargs['use_flash_attn'] = args.use_flash_attn\n    (model, tokenizer, model_dir) = get_model_tokenizer(args.model_type, torch_dtype=torch.bfloat16, **kwargs)\n    if args.gradient_checkpoint:\n        model.gradient_checkpointing_enable()\n        model.enable_input_require_grads()\n    if args.sft_type == 'lora':\n        lora_config = LoRAConfig(target_modules=args.lora_target_modules, r=args.lora_rank, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout_p)\n        logger.info(f'lora_config: {lora_config}')\n        model = Swift.prepare_model(model, lora_config)\n    show_freeze_layers(model)\n    print_model_info(model)\n    _p: Tensor = list(model.parameters())[-1]\n    logger.info(f'device: {_p.device}, dtype: {_p.dtype}')\n    dataset = get_dataset(args.dataset.split(','))\n    (train_dataset, val_dataset) = process_dataset(dataset, args.dataset_test_size, args.dataset_sample, args.dataset_seed)\n    tokenize_func = partial(tokenize_function, tokenizer=tokenizer, prompt=args.prompt, max_length=args.max_length)\n    train_dataset = train_dataset.map(tokenize_func)\n    val_dataset = val_dataset.map(tokenize_func)\n    del dataset\n    stat_dataset(train_dataset)\n    stat_dataset(val_dataset)\n    data_collator = partial(data_collate_fn, tokenizer=tokenizer)\n    print_example(train_dataset[0], tokenizer)\n    cfg_file = os.path.join(model_dir, 'configuration.json')\n    T_max = get_T_max(len(train_dataset), args.batch_size, args.max_epochs, True)\n    work_dir = get_work_dir(args.output_dir)\n    config = Config({'train': {'dataloader': {'batch_size_per_gpu': args.batch_size, 'workers_per_gpu': 1, 'shuffle': True, 'drop_last': True, 'pin_memory': True}, 'max_epochs': args.max_epochs, 'work_dir': work_dir, 'optimizer': {'type': 'AdamW', 'lr': args.learning_rate, 'weight_decay': args.weight_decay, 'options': {'cumulative_iters': args.n_accumulate_grad, 'grad_clip': {'norm_type': 2, 'max_norm': args.grad_clip_norm}}}, 'lr_scheduler': {'type': 'CosineAnnealingLR', 'T_max': T_max, 'eta_min': args.learning_rate * 0.1, 'options': {'by_epoch': False, 'warmup': {'type': 'LinearWarmup', 'warmup_ratio': 0.1, 'warmup_iters': args.warmup_iters}}}, 'hooks': [{'type': 'CheckpointHook', 'by_epoch': False, 'interval': args.last_save_interval, 'max_checkpoint_num': args.last_max_checkpoint_num, 'save_trainer_state': args.save_trainer_state}, {'type': 'EvaluationHook', 'by_epoch': False, 'interval': args.eval_interval}, {'type': 'BestCkptSaverHook', 'metric_key': 'loss', 'save_best': True, 'rule': 'min', 'max_checkpoint_num': args.best_max_checkpoint_num, 'save_trainer_state': args.save_trainer_state}, {'type': 'TextLoggerHook', 'by_epoch': True, 'interval': args.logging_interval}, {'type': 'TensorboardHook', 'by_epoch': False, 'interval': args.tb_interval}]}, 'evaluation': {'dataloader': {'batch_size_per_gpu': args.batch_size, 'workers_per_gpu': 1, 'shuffle': False, 'drop_last': False, 'pin_memory': True}, 'metrics': [{'type': 'my_metric', 'vocab_size': tokenizer.vocab_size}]}})\n\n    def cfg_modify_fn(cfg: Config) -> Config:\n        cfg.update(config)\n        return cfg\n    trainer = EpochBasedTrainer(model=model, cfg_file=cfg_file, data_collator=data_collator, train_dataset=train_dataset, eval_dataset=val_dataset, remove_unused_data=True, seed=42, cfg_modify_fn=cfg_modify_fn)\n    trainer.train()\n    tb_dir = os.path.join(work_dir, 'tensorboard_output')\n    plot_images(tb_dir, ['loss'], 0.9)",
            "def llm_sft(args: SftArguments) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    seed_everything(args.seed)\n    support_bf16 = torch.cuda.is_bf16_supported()\n    if not support_bf16:\n        logger.warning(f'support_bf16: {support_bf16}')\n    kwargs = {'low_cpu_mem_usage': True, 'device_map': 'auto'}\n    if args.model_type == 'qwen-7b':\n        kwargs['use_flash_attn'] = args.use_flash_attn\n    (model, tokenizer, model_dir) = get_model_tokenizer(args.model_type, torch_dtype=torch.bfloat16, **kwargs)\n    if args.gradient_checkpoint:\n        model.gradient_checkpointing_enable()\n        model.enable_input_require_grads()\n    if args.sft_type == 'lora':\n        lora_config = LoRAConfig(target_modules=args.lora_target_modules, r=args.lora_rank, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout_p)\n        logger.info(f'lora_config: {lora_config}')\n        model = Swift.prepare_model(model, lora_config)\n    show_freeze_layers(model)\n    print_model_info(model)\n    _p: Tensor = list(model.parameters())[-1]\n    logger.info(f'device: {_p.device}, dtype: {_p.dtype}')\n    dataset = get_dataset(args.dataset.split(','))\n    (train_dataset, val_dataset) = process_dataset(dataset, args.dataset_test_size, args.dataset_sample, args.dataset_seed)\n    tokenize_func = partial(tokenize_function, tokenizer=tokenizer, prompt=args.prompt, max_length=args.max_length)\n    train_dataset = train_dataset.map(tokenize_func)\n    val_dataset = val_dataset.map(tokenize_func)\n    del dataset\n    stat_dataset(train_dataset)\n    stat_dataset(val_dataset)\n    data_collator = partial(data_collate_fn, tokenizer=tokenizer)\n    print_example(train_dataset[0], tokenizer)\n    cfg_file = os.path.join(model_dir, 'configuration.json')\n    T_max = get_T_max(len(train_dataset), args.batch_size, args.max_epochs, True)\n    work_dir = get_work_dir(args.output_dir)\n    config = Config({'train': {'dataloader': {'batch_size_per_gpu': args.batch_size, 'workers_per_gpu': 1, 'shuffle': True, 'drop_last': True, 'pin_memory': True}, 'max_epochs': args.max_epochs, 'work_dir': work_dir, 'optimizer': {'type': 'AdamW', 'lr': args.learning_rate, 'weight_decay': args.weight_decay, 'options': {'cumulative_iters': args.n_accumulate_grad, 'grad_clip': {'norm_type': 2, 'max_norm': args.grad_clip_norm}}}, 'lr_scheduler': {'type': 'CosineAnnealingLR', 'T_max': T_max, 'eta_min': args.learning_rate * 0.1, 'options': {'by_epoch': False, 'warmup': {'type': 'LinearWarmup', 'warmup_ratio': 0.1, 'warmup_iters': args.warmup_iters}}}, 'hooks': [{'type': 'CheckpointHook', 'by_epoch': False, 'interval': args.last_save_interval, 'max_checkpoint_num': args.last_max_checkpoint_num, 'save_trainer_state': args.save_trainer_state}, {'type': 'EvaluationHook', 'by_epoch': False, 'interval': args.eval_interval}, {'type': 'BestCkptSaverHook', 'metric_key': 'loss', 'save_best': True, 'rule': 'min', 'max_checkpoint_num': args.best_max_checkpoint_num, 'save_trainer_state': args.save_trainer_state}, {'type': 'TextLoggerHook', 'by_epoch': True, 'interval': args.logging_interval}, {'type': 'TensorboardHook', 'by_epoch': False, 'interval': args.tb_interval}]}, 'evaluation': {'dataloader': {'batch_size_per_gpu': args.batch_size, 'workers_per_gpu': 1, 'shuffle': False, 'drop_last': False, 'pin_memory': True}, 'metrics': [{'type': 'my_metric', 'vocab_size': tokenizer.vocab_size}]}})\n\n    def cfg_modify_fn(cfg: Config) -> Config:\n        cfg.update(config)\n        return cfg\n    trainer = EpochBasedTrainer(model=model, cfg_file=cfg_file, data_collator=data_collator, train_dataset=train_dataset, eval_dataset=val_dataset, remove_unused_data=True, seed=42, cfg_modify_fn=cfg_modify_fn)\n    trainer.train()\n    tb_dir = os.path.join(work_dir, 'tensorboard_output')\n    plot_images(tb_dir, ['loss'], 0.9)"
        ]
    }
]