[
    {
        "func_name": "_add_partitions",
        "original": "def _add_partitions(database: str, table: str, boto3_session: Optional[boto3.Session], inputs: List[Dict[str, Any]], catalog_id: Optional[str]=None) -> None:\n    chunks: List[List[Dict[str, Any]]] = _utils.chunkify(lst=inputs, max_length=100)\n    client_glue = _utils.client(service_name='glue', session=boto3_session)\n    for chunk in chunks:\n        res = client_glue.batch_create_partition(**_catalog_id(catalog_id=catalog_id, DatabaseName=database, TableName=table, PartitionInputList=chunk))\n        if 'Errors' in res and res['Errors']:\n            for error in res['Errors']:\n                if 'ErrorDetail' in error:\n                    if 'ErrorCode' in error['ErrorDetail']:\n                        if error['ErrorDetail']['ErrorCode'] != 'AlreadyExistsException':\n                            raise exceptions.ServiceApiError(str(res['Errors']))",
        "mutated": [
            "def _add_partitions(database: str, table: str, boto3_session: Optional[boto3.Session], inputs: List[Dict[str, Any]], catalog_id: Optional[str]=None) -> None:\n    if False:\n        i = 10\n    chunks: List[List[Dict[str, Any]]] = _utils.chunkify(lst=inputs, max_length=100)\n    client_glue = _utils.client(service_name='glue', session=boto3_session)\n    for chunk in chunks:\n        res = client_glue.batch_create_partition(**_catalog_id(catalog_id=catalog_id, DatabaseName=database, TableName=table, PartitionInputList=chunk))\n        if 'Errors' in res and res['Errors']:\n            for error in res['Errors']:\n                if 'ErrorDetail' in error:\n                    if 'ErrorCode' in error['ErrorDetail']:\n                        if error['ErrorDetail']['ErrorCode'] != 'AlreadyExistsException':\n                            raise exceptions.ServiceApiError(str(res['Errors']))",
            "def _add_partitions(database: str, table: str, boto3_session: Optional[boto3.Session], inputs: List[Dict[str, Any]], catalog_id: Optional[str]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    chunks: List[List[Dict[str, Any]]] = _utils.chunkify(lst=inputs, max_length=100)\n    client_glue = _utils.client(service_name='glue', session=boto3_session)\n    for chunk in chunks:\n        res = client_glue.batch_create_partition(**_catalog_id(catalog_id=catalog_id, DatabaseName=database, TableName=table, PartitionInputList=chunk))\n        if 'Errors' in res and res['Errors']:\n            for error in res['Errors']:\n                if 'ErrorDetail' in error:\n                    if 'ErrorCode' in error['ErrorDetail']:\n                        if error['ErrorDetail']['ErrorCode'] != 'AlreadyExistsException':\n                            raise exceptions.ServiceApiError(str(res['Errors']))",
            "def _add_partitions(database: str, table: str, boto3_session: Optional[boto3.Session], inputs: List[Dict[str, Any]], catalog_id: Optional[str]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    chunks: List[List[Dict[str, Any]]] = _utils.chunkify(lst=inputs, max_length=100)\n    client_glue = _utils.client(service_name='glue', session=boto3_session)\n    for chunk in chunks:\n        res = client_glue.batch_create_partition(**_catalog_id(catalog_id=catalog_id, DatabaseName=database, TableName=table, PartitionInputList=chunk))\n        if 'Errors' in res and res['Errors']:\n            for error in res['Errors']:\n                if 'ErrorDetail' in error:\n                    if 'ErrorCode' in error['ErrorDetail']:\n                        if error['ErrorDetail']['ErrorCode'] != 'AlreadyExistsException':\n                            raise exceptions.ServiceApiError(str(res['Errors']))",
            "def _add_partitions(database: str, table: str, boto3_session: Optional[boto3.Session], inputs: List[Dict[str, Any]], catalog_id: Optional[str]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    chunks: List[List[Dict[str, Any]]] = _utils.chunkify(lst=inputs, max_length=100)\n    client_glue = _utils.client(service_name='glue', session=boto3_session)\n    for chunk in chunks:\n        res = client_glue.batch_create_partition(**_catalog_id(catalog_id=catalog_id, DatabaseName=database, TableName=table, PartitionInputList=chunk))\n        if 'Errors' in res and res['Errors']:\n            for error in res['Errors']:\n                if 'ErrorDetail' in error:\n                    if 'ErrorCode' in error['ErrorDetail']:\n                        if error['ErrorDetail']['ErrorCode'] != 'AlreadyExistsException':\n                            raise exceptions.ServiceApiError(str(res['Errors']))",
            "def _add_partitions(database: str, table: str, boto3_session: Optional[boto3.Session], inputs: List[Dict[str, Any]], catalog_id: Optional[str]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    chunks: List[List[Dict[str, Any]]] = _utils.chunkify(lst=inputs, max_length=100)\n    client_glue = _utils.client(service_name='glue', session=boto3_session)\n    for chunk in chunks:\n        res = client_glue.batch_create_partition(**_catalog_id(catalog_id=catalog_id, DatabaseName=database, TableName=table, PartitionInputList=chunk))\n        if 'Errors' in res and res['Errors']:\n            for error in res['Errors']:\n                if 'ErrorDetail' in error:\n                    if 'ErrorCode' in error['ErrorDetail']:\n                        if error['ErrorDetail']['ErrorCode'] != 'AlreadyExistsException':\n                            raise exceptions.ServiceApiError(str(res['Errors']))"
        ]
    },
    {
        "func_name": "add_csv_partitions",
        "original": "@apply_configs\ndef add_csv_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, sep: str=',', serde_library: Optional[str]=None, serde_parameters: Optional[Dict[str, str]]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    \"\"\"Add partitions (metadata) to a CSV Table in the AWS Glue Catalog.\n\n    Parameters\n    ----------\n    database : str\n        Database name.\n    table : str\n        Table name.\n    partitions_values: Dict[str, List[str]]\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\n        (e.g. {'s3://bucket/prefix/y=2020/m=10/': ['2020', '10']}).\n    bucketing_info: Tuple[List[str], int], optional\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\n        second element.\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\n    catalog_id : str, optional\n        The ID of the Data Catalog from which to retrieve Databases.\n        If none is provided, the AWS account ID is used by default.\n    compression: str, optional\n        Compression style (``None``, ``gzip``, etc).\n    sep : str\n        String of length 1. Field delimiter for the output file.\n    serde_library : Optional[str]\n        Specifies the SerDe Serialization library which will be used. You need to provide the Class library name\n        as a string.\n        If no library is provided the default is `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`.\n    serde_parameters : Optional[str]\n        Dictionary of initialization parameters for the SerDe.\n        The default is `{\"field.delim\": sep, \"escape.delim\": \"\\\\\\\\\"}`.\n    boto3_session : boto3.Session(), optional\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\n    columns_types: Optional[Dict[str, str]]\n        Only required for Hive compability.\n        Dictionary with keys as column names and values as data types (e.g. {'col0': 'bigint', 'col1': 'double'}).\n        P.S. Only materialized columns please, not partition columns.\n    partitions_parameters: Optional[Dict[str, str]]\n        Dictionary with key-value pairs defining partition parameters.\n\n    Returns\n    -------\n    None\n        None.\n\n    Examples\n    --------\n    >>> import awswrangler as wr\n    >>> wr.catalog.add_csv_partitions(\n    ...     database='default',\n    ...     table='my_table',\n    ...     partitions_values={\n    ...         's3://bucket/prefix/y=2020/m=10/': ['2020', '10'],\n    ...         's3://bucket/prefix/y=2020/m=11/': ['2020', '11'],\n    ...         's3://bucket/prefix/y=2020/m=12/': ['2020', '12']\n    ...     }\n    ... )\n\n    \"\"\"\n    table = sanitize_table_name(table=table)\n    inputs: List[Dict[str, Any]] = [_csv_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, sep=sep, columns_types=columns_types, serde_library=serde_library, serde_parameters=serde_parameters, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n    _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
        "mutated": [
            "@apply_configs\ndef add_csv_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, sep: str=',', serde_library: Optional[str]=None, serde_parameters: Optional[Dict[str, str]]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n    'Add partitions (metadata) to a CSV Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {\\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``gzip``, etc).\\n    sep : str\\n        String of length 1. Field delimiter for the output file.\\n    serde_library : Optional[str]\\n        Specifies the SerDe Serialization library which will be used. You need to provide the Class library name\\n        as a string.\\n        If no library is provided the default is `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`.\\n    serde_parameters : Optional[str]\\n        Dictionary of initialization parameters for the SerDe.\\n        The default is `{\"field.delim\": sep, \"escape.delim\": \"\\\\\\\\\"}`.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {\\'col0\\': \\'bigint\\', \\'col1\\': \\'double\\'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_csv_partitions(\\n    ...     database=\\'default\\',\\n    ...     table=\\'my_table\\',\\n    ...     partitions_values={\\n    ...         \\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=11/\\': [\\'2020\\', \\'11\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=12/\\': [\\'2020\\', \\'12\\']\\n    ...     }\\n    ... )\\n\\n    '\n    table = sanitize_table_name(table=table)\n    inputs: List[Dict[str, Any]] = [_csv_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, sep=sep, columns_types=columns_types, serde_library=serde_library, serde_parameters=serde_parameters, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n    _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_csv_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, sep: str=',', serde_library: Optional[str]=None, serde_parameters: Optional[Dict[str, str]]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Add partitions (metadata) to a CSV Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {\\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``gzip``, etc).\\n    sep : str\\n        String of length 1. Field delimiter for the output file.\\n    serde_library : Optional[str]\\n        Specifies the SerDe Serialization library which will be used. You need to provide the Class library name\\n        as a string.\\n        If no library is provided the default is `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`.\\n    serde_parameters : Optional[str]\\n        Dictionary of initialization parameters for the SerDe.\\n        The default is `{\"field.delim\": sep, \"escape.delim\": \"\\\\\\\\\"}`.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {\\'col0\\': \\'bigint\\', \\'col1\\': \\'double\\'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_csv_partitions(\\n    ...     database=\\'default\\',\\n    ...     table=\\'my_table\\',\\n    ...     partitions_values={\\n    ...         \\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=11/\\': [\\'2020\\', \\'11\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=12/\\': [\\'2020\\', \\'12\\']\\n    ...     }\\n    ... )\\n\\n    '\n    table = sanitize_table_name(table=table)\n    inputs: List[Dict[str, Any]] = [_csv_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, sep=sep, columns_types=columns_types, serde_library=serde_library, serde_parameters=serde_parameters, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n    _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_csv_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, sep: str=',', serde_library: Optional[str]=None, serde_parameters: Optional[Dict[str, str]]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Add partitions (metadata) to a CSV Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {\\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``gzip``, etc).\\n    sep : str\\n        String of length 1. Field delimiter for the output file.\\n    serde_library : Optional[str]\\n        Specifies the SerDe Serialization library which will be used. You need to provide the Class library name\\n        as a string.\\n        If no library is provided the default is `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`.\\n    serde_parameters : Optional[str]\\n        Dictionary of initialization parameters for the SerDe.\\n        The default is `{\"field.delim\": sep, \"escape.delim\": \"\\\\\\\\\"}`.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {\\'col0\\': \\'bigint\\', \\'col1\\': \\'double\\'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_csv_partitions(\\n    ...     database=\\'default\\',\\n    ...     table=\\'my_table\\',\\n    ...     partitions_values={\\n    ...         \\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=11/\\': [\\'2020\\', \\'11\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=12/\\': [\\'2020\\', \\'12\\']\\n    ...     }\\n    ... )\\n\\n    '\n    table = sanitize_table_name(table=table)\n    inputs: List[Dict[str, Any]] = [_csv_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, sep=sep, columns_types=columns_types, serde_library=serde_library, serde_parameters=serde_parameters, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n    _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_csv_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, sep: str=',', serde_library: Optional[str]=None, serde_parameters: Optional[Dict[str, str]]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Add partitions (metadata) to a CSV Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {\\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``gzip``, etc).\\n    sep : str\\n        String of length 1. Field delimiter for the output file.\\n    serde_library : Optional[str]\\n        Specifies the SerDe Serialization library which will be used. You need to provide the Class library name\\n        as a string.\\n        If no library is provided the default is `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`.\\n    serde_parameters : Optional[str]\\n        Dictionary of initialization parameters for the SerDe.\\n        The default is `{\"field.delim\": sep, \"escape.delim\": \"\\\\\\\\\"}`.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {\\'col0\\': \\'bigint\\', \\'col1\\': \\'double\\'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_csv_partitions(\\n    ...     database=\\'default\\',\\n    ...     table=\\'my_table\\',\\n    ...     partitions_values={\\n    ...         \\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=11/\\': [\\'2020\\', \\'11\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=12/\\': [\\'2020\\', \\'12\\']\\n    ...     }\\n    ... )\\n\\n    '\n    table = sanitize_table_name(table=table)\n    inputs: List[Dict[str, Any]] = [_csv_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, sep=sep, columns_types=columns_types, serde_library=serde_library, serde_parameters=serde_parameters, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n    _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_csv_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, sep: str=',', serde_library: Optional[str]=None, serde_parameters: Optional[Dict[str, str]]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Add partitions (metadata) to a CSV Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {\\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``gzip``, etc).\\n    sep : str\\n        String of length 1. Field delimiter for the output file.\\n    serde_library : Optional[str]\\n        Specifies the SerDe Serialization library which will be used. You need to provide the Class library name\\n        as a string.\\n        If no library is provided the default is `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`.\\n    serde_parameters : Optional[str]\\n        Dictionary of initialization parameters for the SerDe.\\n        The default is `{\"field.delim\": sep, \"escape.delim\": \"\\\\\\\\\"}`.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {\\'col0\\': \\'bigint\\', \\'col1\\': \\'double\\'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_csv_partitions(\\n    ...     database=\\'default\\',\\n    ...     table=\\'my_table\\',\\n    ...     partitions_values={\\n    ...         \\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=11/\\': [\\'2020\\', \\'11\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=12/\\': [\\'2020\\', \\'12\\']\\n    ...     }\\n    ... )\\n\\n    '\n    table = sanitize_table_name(table=table)\n    inputs: List[Dict[str, Any]] = [_csv_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, sep=sep, columns_types=columns_types, serde_library=serde_library, serde_parameters=serde_parameters, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n    _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)"
        ]
    },
    {
        "func_name": "add_json_partitions",
        "original": "@apply_configs\ndef add_json_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, serde_library: Optional[str]=None, serde_parameters: Optional[Dict[str, str]]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    \"\"\"Add partitions (metadata) to a JSON Table in the AWS Glue Catalog.\n\n    Parameters\n    ----------\n    database : str\n        Database name.\n    table : str\n        Table name.\n    partitions_values: Dict[str, List[str]]\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\n        (e.g. {'s3://bucket/prefix/y=2020/m=10/': ['2020', '10']}).\n    bucketing_info: Tuple[List[str], int], optional\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\n        second element.\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\n    catalog_id : str, optional\n        The ID of the Data Catalog from which to retrieve Databases.\n        If none is provided, the AWS account ID is used by default.\n    compression: str, optional\n        Compression style (``None``, ``gzip``, etc).\n    serde_library : Optional[str]\n        Specifies the SerDe Serialization library which will be used. You need to provide the Class library name\n        as a string.\n        If no library is provided the default is `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`.\n    serde_parameters : Optional[str]\n        Dictionary of initialization parameters for the SerDe.\n        The default is `{\"field.delim\": sep, \"escape.delim\": \"\\\\\\\\\"}`.\n    boto3_session : boto3.Session(), optional\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\n    columns_types: Optional[Dict[str, str]]\n        Only required for Hive compability.\n        Dictionary with keys as column names and values as data types (e.g. {'col0': 'bigint', 'col1': 'double'}).\n        P.S. Only materialized columns please, not partition columns.\n    partitions_parameters: Optional[Dict[str, str]]\n        Dictionary with key-value pairs defining partition parameters.\n\n    Returns\n    -------\n    None\n        None.\n\n    Examples\n    --------\n    >>> import awswrangler as wr\n    >>> wr.catalog.add_json_partitions(\n    ...     database='default',\n    ...     table='my_table',\n    ...     partitions_values={\n    ...         's3://bucket/prefix/y=2020/m=10/': ['2020', '10'],\n    ...         's3://bucket/prefix/y=2020/m=11/': ['2020', '11'],\n    ...         's3://bucket/prefix/y=2020/m=12/': ['2020', '12']\n    ...     }\n    ... )\n\n    \"\"\"\n    table = sanitize_table_name(table=table)\n    inputs: List[Dict[str, Any]] = [_json_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, serde_library=serde_library, serde_parameters=serde_parameters, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n    _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
        "mutated": [
            "@apply_configs\ndef add_json_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, serde_library: Optional[str]=None, serde_parameters: Optional[Dict[str, str]]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n    'Add partitions (metadata) to a JSON Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {\\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``gzip``, etc).\\n    serde_library : Optional[str]\\n        Specifies the SerDe Serialization library which will be used. You need to provide the Class library name\\n        as a string.\\n        If no library is provided the default is `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`.\\n    serde_parameters : Optional[str]\\n        Dictionary of initialization parameters for the SerDe.\\n        The default is `{\"field.delim\": sep, \"escape.delim\": \"\\\\\\\\\"}`.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {\\'col0\\': \\'bigint\\', \\'col1\\': \\'double\\'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_json_partitions(\\n    ...     database=\\'default\\',\\n    ...     table=\\'my_table\\',\\n    ...     partitions_values={\\n    ...         \\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=11/\\': [\\'2020\\', \\'11\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=12/\\': [\\'2020\\', \\'12\\']\\n    ...     }\\n    ... )\\n\\n    '\n    table = sanitize_table_name(table=table)\n    inputs: List[Dict[str, Any]] = [_json_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, serde_library=serde_library, serde_parameters=serde_parameters, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n    _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_json_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, serde_library: Optional[str]=None, serde_parameters: Optional[Dict[str, str]]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    'Add partitions (metadata) to a JSON Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {\\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``gzip``, etc).\\n    serde_library : Optional[str]\\n        Specifies the SerDe Serialization library which will be used. You need to provide the Class library name\\n        as a string.\\n        If no library is provided the default is `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`.\\n    serde_parameters : Optional[str]\\n        Dictionary of initialization parameters for the SerDe.\\n        The default is `{\"field.delim\": sep, \"escape.delim\": \"\\\\\\\\\"}`.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {\\'col0\\': \\'bigint\\', \\'col1\\': \\'double\\'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_json_partitions(\\n    ...     database=\\'default\\',\\n    ...     table=\\'my_table\\',\\n    ...     partitions_values={\\n    ...         \\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=11/\\': [\\'2020\\', \\'11\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=12/\\': [\\'2020\\', \\'12\\']\\n    ...     }\\n    ... )\\n\\n    '\n    table = sanitize_table_name(table=table)\n    inputs: List[Dict[str, Any]] = [_json_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, serde_library=serde_library, serde_parameters=serde_parameters, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n    _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_json_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, serde_library: Optional[str]=None, serde_parameters: Optional[Dict[str, str]]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    'Add partitions (metadata) to a JSON Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {\\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``gzip``, etc).\\n    serde_library : Optional[str]\\n        Specifies the SerDe Serialization library which will be used. You need to provide the Class library name\\n        as a string.\\n        If no library is provided the default is `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`.\\n    serde_parameters : Optional[str]\\n        Dictionary of initialization parameters for the SerDe.\\n        The default is `{\"field.delim\": sep, \"escape.delim\": \"\\\\\\\\\"}`.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {\\'col0\\': \\'bigint\\', \\'col1\\': \\'double\\'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_json_partitions(\\n    ...     database=\\'default\\',\\n    ...     table=\\'my_table\\',\\n    ...     partitions_values={\\n    ...         \\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=11/\\': [\\'2020\\', \\'11\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=12/\\': [\\'2020\\', \\'12\\']\\n    ...     }\\n    ... )\\n\\n    '\n    table = sanitize_table_name(table=table)\n    inputs: List[Dict[str, Any]] = [_json_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, serde_library=serde_library, serde_parameters=serde_parameters, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n    _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_json_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, serde_library: Optional[str]=None, serde_parameters: Optional[Dict[str, str]]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    'Add partitions (metadata) to a JSON Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {\\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``gzip``, etc).\\n    serde_library : Optional[str]\\n        Specifies the SerDe Serialization library which will be used. You need to provide the Class library name\\n        as a string.\\n        If no library is provided the default is `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`.\\n    serde_parameters : Optional[str]\\n        Dictionary of initialization parameters for the SerDe.\\n        The default is `{\"field.delim\": sep, \"escape.delim\": \"\\\\\\\\\"}`.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {\\'col0\\': \\'bigint\\', \\'col1\\': \\'double\\'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_json_partitions(\\n    ...     database=\\'default\\',\\n    ...     table=\\'my_table\\',\\n    ...     partitions_values={\\n    ...         \\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=11/\\': [\\'2020\\', \\'11\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=12/\\': [\\'2020\\', \\'12\\']\\n    ...     }\\n    ... )\\n\\n    '\n    table = sanitize_table_name(table=table)\n    inputs: List[Dict[str, Any]] = [_json_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, serde_library=serde_library, serde_parameters=serde_parameters, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n    _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_json_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, serde_library: Optional[str]=None, serde_parameters: Optional[Dict[str, str]]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    'Add partitions (metadata) to a JSON Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {\\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``gzip``, etc).\\n    serde_library : Optional[str]\\n        Specifies the SerDe Serialization library which will be used. You need to provide the Class library name\\n        as a string.\\n        If no library is provided the default is `org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe`.\\n    serde_parameters : Optional[str]\\n        Dictionary of initialization parameters for the SerDe.\\n        The default is `{\"field.delim\": sep, \"escape.delim\": \"\\\\\\\\\"}`.\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {\\'col0\\': \\'bigint\\', \\'col1\\': \\'double\\'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_json_partitions(\\n    ...     database=\\'default\\',\\n    ...     table=\\'my_table\\',\\n    ...     partitions_values={\\n    ...         \\'s3://bucket/prefix/y=2020/m=10/\\': [\\'2020\\', \\'10\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=11/\\': [\\'2020\\', \\'11\\'],\\n    ...         \\'s3://bucket/prefix/y=2020/m=12/\\': [\\'2020\\', \\'12\\']\\n    ...     }\\n    ... )\\n\\n    '\n    table = sanitize_table_name(table=table)\n    inputs: List[Dict[str, Any]] = [_json_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, serde_library=serde_library, serde_parameters=serde_parameters, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n    _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)"
        ]
    },
    {
        "func_name": "add_parquet_partitions",
        "original": "@apply_configs\ndef add_parquet_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    \"\"\"Add partitions (metadata) to a Parquet Table in the AWS Glue Catalog.\n\n    Parameters\n    ----------\n    database : str\n        Database name.\n    table : str\n        Table name.\n    partitions_values: Dict[str, List[str]]\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\n        (e.g. {'s3://bucket/prefix/y=2020/m=10/': ['2020', '10']}).\n    bucketing_info: Tuple[List[str], int], optional\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\n        second element.\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\n    catalog_id : str, optional\n        The ID of the Data Catalog from which to retrieve Databases.\n        If none is provided, the AWS account ID is used by default.\n    compression: str, optional\n        Compression style (``None``, ``snappy``, ``gzip``, etc).\n    boto3_session : boto3.Session(), optional\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\n    columns_types: Optional[Dict[str, str]]\n        Only required for Hive compability.\n        Dictionary with keys as column names and values as data types (e.g. {'col0': 'bigint', 'col1': 'double'}).\n        P.S. Only materialized columns please, not partition columns.\n    partitions_parameters: Optional[Dict[str, str]]\n        Dictionary with key-value pairs defining partition parameters.\n\n    Returns\n    -------\n    None\n        None.\n\n    Examples\n    --------\n    >>> import awswrangler as wr\n    >>> wr.catalog.add_parquet_partitions(\n    ...     database='default',\n    ...     table='my_table',\n    ...     partitions_values={\n    ...         's3://bucket/prefix/y=2020/m=10/': ['2020', '10'],\n    ...         's3://bucket/prefix/y=2020/m=11/': ['2020', '11'],\n    ...         's3://bucket/prefix/y=2020/m=12/': ['2020', '12']\n    ...     }\n    ... )\n\n    \"\"\"\n    table = sanitize_table_name(table=table)\n    if partitions_values:\n        inputs: List[Dict[str, Any]] = [_parquet_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n        _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
        "mutated": [
            "@apply_configs\ndef add_parquet_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n    \"Add partitions (metadata) to a Parquet Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {'s3://bucket/prefix/y=2020/m=10/': ['2020', '10']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``snappy``, ``gzip``, etc).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {'col0': 'bigint', 'col1': 'double'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_parquet_partitions(\\n    ...     database='default',\\n    ...     table='my_table',\\n    ...     partitions_values={\\n    ...         's3://bucket/prefix/y=2020/m=10/': ['2020', '10'],\\n    ...         's3://bucket/prefix/y=2020/m=11/': ['2020', '11'],\\n    ...         's3://bucket/prefix/y=2020/m=12/': ['2020', '12']\\n    ...     }\\n    ... )\\n\\n    \"\n    table = sanitize_table_name(table=table)\n    if partitions_values:\n        inputs: List[Dict[str, Any]] = [_parquet_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n        _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_parquet_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Add partitions (metadata) to a Parquet Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {'s3://bucket/prefix/y=2020/m=10/': ['2020', '10']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``snappy``, ``gzip``, etc).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {'col0': 'bigint', 'col1': 'double'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_parquet_partitions(\\n    ...     database='default',\\n    ...     table='my_table',\\n    ...     partitions_values={\\n    ...         's3://bucket/prefix/y=2020/m=10/': ['2020', '10'],\\n    ...         's3://bucket/prefix/y=2020/m=11/': ['2020', '11'],\\n    ...         's3://bucket/prefix/y=2020/m=12/': ['2020', '12']\\n    ...     }\\n    ... )\\n\\n    \"\n    table = sanitize_table_name(table=table)\n    if partitions_values:\n        inputs: List[Dict[str, Any]] = [_parquet_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n        _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_parquet_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Add partitions (metadata) to a Parquet Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {'s3://bucket/prefix/y=2020/m=10/': ['2020', '10']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``snappy``, ``gzip``, etc).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {'col0': 'bigint', 'col1': 'double'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_parquet_partitions(\\n    ...     database='default',\\n    ...     table='my_table',\\n    ...     partitions_values={\\n    ...         's3://bucket/prefix/y=2020/m=10/': ['2020', '10'],\\n    ...         's3://bucket/prefix/y=2020/m=11/': ['2020', '11'],\\n    ...         's3://bucket/prefix/y=2020/m=12/': ['2020', '12']\\n    ...     }\\n    ... )\\n\\n    \"\n    table = sanitize_table_name(table=table)\n    if partitions_values:\n        inputs: List[Dict[str, Any]] = [_parquet_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n        _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_parquet_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Add partitions (metadata) to a Parquet Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {'s3://bucket/prefix/y=2020/m=10/': ['2020', '10']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``snappy``, ``gzip``, etc).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {'col0': 'bigint', 'col1': 'double'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_parquet_partitions(\\n    ...     database='default',\\n    ...     table='my_table',\\n    ...     partitions_values={\\n    ...         's3://bucket/prefix/y=2020/m=10/': ['2020', '10'],\\n    ...         's3://bucket/prefix/y=2020/m=11/': ['2020', '11'],\\n    ...         's3://bucket/prefix/y=2020/m=12/': ['2020', '12']\\n    ...     }\\n    ... )\\n\\n    \"\n    table = sanitize_table_name(table=table)\n    if partitions_values:\n        inputs: List[Dict[str, Any]] = [_parquet_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n        _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_parquet_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Add partitions (metadata) to a Parquet Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {'s3://bucket/prefix/y=2020/m=10/': ['2020', '10']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``snappy``, ``gzip``, etc).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {'col0': 'bigint', 'col1': 'double'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_parquet_partitions(\\n    ...     database='default',\\n    ...     table='my_table',\\n    ...     partitions_values={\\n    ...         's3://bucket/prefix/y=2020/m=10/': ['2020', '10'],\\n    ...         's3://bucket/prefix/y=2020/m=11/': ['2020', '11'],\\n    ...         's3://bucket/prefix/y=2020/m=12/': ['2020', '12']\\n    ...     }\\n    ... )\\n\\n    \"\n    table = sanitize_table_name(table=table)\n    if partitions_values:\n        inputs: List[Dict[str, Any]] = [_parquet_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n        _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)"
        ]
    },
    {
        "func_name": "add_orc_partitions",
        "original": "@apply_configs\ndef add_orc_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    \"\"\"Add partitions (metadata) to a ORC Table in the AWS Glue Catalog.\n\n    Parameters\n    ----------\n    database : str\n        Database name.\n    table : str\n        Table name.\n    partitions_values: Dict[str, List[str]]\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\n        (e.g. {'s3://bucket/prefix/y=2020/m=10/': ['2020', '10']}).\n    bucketing_info: Tuple[List[str], int], optional\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\n        second element.\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\n    catalog_id : str, optional\n        The ID of the Data Catalog from which to retrieve Databases.\n        If none is provided, the AWS account ID is used by default.\n    compression: str, optional\n        Compression style (``None``, ``snappy``, ``zlib``, etc).\n    boto3_session : boto3.Session(), optional\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\n    columns_types: Optional[Dict[str, str]]\n        Only required for Hive compability.\n        Dictionary with keys as column names and values as data types (e.g. {'col0': 'bigint', 'col1': 'double'}).\n        P.S. Only materialized columns please, not partition columns.\n    partitions_parameters: Optional[Dict[str, str]]\n        Dictionary with key-value pairs defining partition parameters.\n\n    Returns\n    -------\n    None\n        None.\n\n    Examples\n    --------\n    >>> import awswrangler as wr\n    >>> wr.catalog.add_orc_partitions(\n    ...     database='default',\n    ...     table='my_table',\n    ...     partitions_values={\n    ...         's3://bucket/prefix/y=2020/m=10/': ['2020', '10'],\n    ...         's3://bucket/prefix/y=2020/m=11/': ['2020', '11'],\n    ...         's3://bucket/prefix/y=2020/m=12/': ['2020', '12']\n    ...     }\n    ... )\n\n    \"\"\"\n    table = sanitize_table_name(table=table)\n    if partitions_values:\n        inputs: List[Dict[str, Any]] = [_orc_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n        _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
        "mutated": [
            "@apply_configs\ndef add_orc_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n    \"Add partitions (metadata) to a ORC Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {'s3://bucket/prefix/y=2020/m=10/': ['2020', '10']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``snappy``, ``zlib``, etc).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {'col0': 'bigint', 'col1': 'double'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_orc_partitions(\\n    ...     database='default',\\n    ...     table='my_table',\\n    ...     partitions_values={\\n    ...         's3://bucket/prefix/y=2020/m=10/': ['2020', '10'],\\n    ...         's3://bucket/prefix/y=2020/m=11/': ['2020', '11'],\\n    ...         's3://bucket/prefix/y=2020/m=12/': ['2020', '12']\\n    ...     }\\n    ... )\\n\\n    \"\n    table = sanitize_table_name(table=table)\n    if partitions_values:\n        inputs: List[Dict[str, Any]] = [_orc_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n        _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_orc_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Add partitions (metadata) to a ORC Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {'s3://bucket/prefix/y=2020/m=10/': ['2020', '10']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``snappy``, ``zlib``, etc).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {'col0': 'bigint', 'col1': 'double'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_orc_partitions(\\n    ...     database='default',\\n    ...     table='my_table',\\n    ...     partitions_values={\\n    ...         's3://bucket/prefix/y=2020/m=10/': ['2020', '10'],\\n    ...         's3://bucket/prefix/y=2020/m=11/': ['2020', '11'],\\n    ...         's3://bucket/prefix/y=2020/m=12/': ['2020', '12']\\n    ...     }\\n    ... )\\n\\n    \"\n    table = sanitize_table_name(table=table)\n    if partitions_values:\n        inputs: List[Dict[str, Any]] = [_orc_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n        _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_orc_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Add partitions (metadata) to a ORC Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {'s3://bucket/prefix/y=2020/m=10/': ['2020', '10']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``snappy``, ``zlib``, etc).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {'col0': 'bigint', 'col1': 'double'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_orc_partitions(\\n    ...     database='default',\\n    ...     table='my_table',\\n    ...     partitions_values={\\n    ...         's3://bucket/prefix/y=2020/m=10/': ['2020', '10'],\\n    ...         's3://bucket/prefix/y=2020/m=11/': ['2020', '11'],\\n    ...         's3://bucket/prefix/y=2020/m=12/': ['2020', '12']\\n    ...     }\\n    ... )\\n\\n    \"\n    table = sanitize_table_name(table=table)\n    if partitions_values:\n        inputs: List[Dict[str, Any]] = [_orc_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n        _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_orc_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Add partitions (metadata) to a ORC Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {'s3://bucket/prefix/y=2020/m=10/': ['2020', '10']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``snappy``, ``zlib``, etc).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {'col0': 'bigint', 'col1': 'double'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_orc_partitions(\\n    ...     database='default',\\n    ...     table='my_table',\\n    ...     partitions_values={\\n    ...         's3://bucket/prefix/y=2020/m=10/': ['2020', '10'],\\n    ...         's3://bucket/prefix/y=2020/m=11/': ['2020', '11'],\\n    ...         's3://bucket/prefix/y=2020/m=12/': ['2020', '12']\\n    ...     }\\n    ... )\\n\\n    \"\n    table = sanitize_table_name(table=table)\n    if partitions_values:\n        inputs: List[Dict[str, Any]] = [_orc_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n        _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)",
            "@apply_configs\ndef add_orc_partitions(database: str, table: str, partitions_values: Dict[str, List[str]], bucketing_info: Optional[typing.BucketingInfoTuple]=None, catalog_id: Optional[str]=None, compression: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, columns_types: Optional[Dict[str, str]]=None, partitions_parameters: Optional[Dict[str, str]]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Add partitions (metadata) to a ORC Table in the AWS Glue Catalog.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    partitions_values: Dict[str, List[str]]\\n        Dictionary with keys as S3 path locations and values as a list of partitions values as str\\n        (e.g. {'s3://bucket/prefix/y=2020/m=10/': ['2020', '10']}).\\n    bucketing_info: Tuple[List[str], int], optional\\n        Tuple consisting of the column names used for bucketing as the first element and the number of buckets as the\\n        second element.\\n        Only `str`, `int` and `bool` are supported as column data types for bucketing.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n    compression: str, optional\\n        Compression style (``None``, ``snappy``, ``zlib``, etc).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    columns_types: Optional[Dict[str, str]]\\n        Only required for Hive compability.\\n        Dictionary with keys as column names and values as data types (e.g. {'col0': 'bigint', 'col1': 'double'}).\\n        P.S. Only materialized columns please, not partition columns.\\n    partitions_parameters: Optional[Dict[str, str]]\\n        Dictionary with key-value pairs defining partition parameters.\\n\\n    Returns\\n    -------\\n    None\\n        None.\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_orc_partitions(\\n    ...     database='default',\\n    ...     table='my_table',\\n    ...     partitions_values={\\n    ...         's3://bucket/prefix/y=2020/m=10/': ['2020', '10'],\\n    ...         's3://bucket/prefix/y=2020/m=11/': ['2020', '11'],\\n    ...         's3://bucket/prefix/y=2020/m=12/': ['2020', '12']\\n    ...     }\\n    ... )\\n\\n    \"\n    table = sanitize_table_name(table=table)\n    if partitions_values:\n        inputs: List[Dict[str, Any]] = [_orc_partition_definition(location=k, values=v, bucketing_info=bucketing_info, compression=compression, columns_types=columns_types, partitions_parameters=partitions_parameters) for (k, v) in partitions_values.items()]\n        _add_partitions(database=database, table=table, boto3_session=boto3_session, inputs=inputs, catalog_id=catalog_id)"
        ]
    },
    {
        "func_name": "add_column",
        "original": "@apply_configs\ndef add_column(database: str, table: str, column_name: str, column_type: str='string', column_comment: Optional[str]=None, transaction_id: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, catalog_id: Optional[str]=None) -> None:\n    \"\"\"Add a column in a AWS Glue Catalog table.\n\n    Parameters\n    ----------\n    database : str\n        Database name.\n    table : str\n        Table name.\n    column_name : str\n        Column name\n    column_type : str\n        Column type.\n    column_comment : str\n        Column Comment\n    transaction_id: str, optional\n        The ID of the transaction (i.e. used with GOVERNED tables).\n    boto3_session : boto3.Session(), optional\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\n    catalog_id : str, optional\n        The ID of the Data Catalog from which to retrieve Databases.\n        If none is provided, the AWS account ID is used by default.\n\n    Returns\n    -------\n    None\n        None\n\n    Examples\n    --------\n    >>> import awswrangler as wr\n    >>> wr.catalog.add_column(\n    ...     database='my_db',\n    ...     table='my_table',\n    ...     column_name='my_col',\n    ...     column_type='int'\n    ... )\n    \"\"\"\n    if _check_column_type(column_type):\n        client_glue = _utils.client(service_name='glue', session=boto3_session)\n        table_res = client_glue.get_table(**_catalog_id(catalog_id=catalog_id, **_transaction_id(transaction_id=transaction_id, DatabaseName=database, Name=table)))\n        table_input: Dict[str, Any] = _update_table_definition(table_res)\n        table_input['StorageDescriptor']['Columns'].append({'Name': column_name, 'Type': column_type, 'Comment': column_comment})\n        res: Dict[str, Any] = client_glue.update_table(**_catalog_id(catalog_id=catalog_id, **_transaction_id(transaction_id=transaction_id, DatabaseName=database, TableInput=table_input)))\n        if 'Errors' in res and res['Errors']:\n            for error in res['Errors']:\n                if 'ErrorDetail' in error:\n                    if 'ErrorCode' in error['ErrorDetail']:\n                        raise exceptions.ServiceApiError(str(res['Errors']))",
        "mutated": [
            "@apply_configs\ndef add_column(database: str, table: str, column_name: str, column_type: str='string', column_comment: Optional[str]=None, transaction_id: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, catalog_id: Optional[str]=None) -> None:\n    if False:\n        i = 10\n    \"Add a column in a AWS Glue Catalog table.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    column_name : str\\n        Column name\\n    column_type : str\\n        Column type.\\n    column_comment : str\\n        Column Comment\\n    transaction_id: str, optional\\n        The ID of the transaction (i.e. used with GOVERNED tables).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n\\n    Returns\\n    -------\\n    None\\n        None\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_column(\\n    ...     database='my_db',\\n    ...     table='my_table',\\n    ...     column_name='my_col',\\n    ...     column_type='int'\\n    ... )\\n    \"\n    if _check_column_type(column_type):\n        client_glue = _utils.client(service_name='glue', session=boto3_session)\n        table_res = client_glue.get_table(**_catalog_id(catalog_id=catalog_id, **_transaction_id(transaction_id=transaction_id, DatabaseName=database, Name=table)))\n        table_input: Dict[str, Any] = _update_table_definition(table_res)\n        table_input['StorageDescriptor']['Columns'].append({'Name': column_name, 'Type': column_type, 'Comment': column_comment})\n        res: Dict[str, Any] = client_glue.update_table(**_catalog_id(catalog_id=catalog_id, **_transaction_id(transaction_id=transaction_id, DatabaseName=database, TableInput=table_input)))\n        if 'Errors' in res and res['Errors']:\n            for error in res['Errors']:\n                if 'ErrorDetail' in error:\n                    if 'ErrorCode' in error['ErrorDetail']:\n                        raise exceptions.ServiceApiError(str(res['Errors']))",
            "@apply_configs\ndef add_column(database: str, table: str, column_name: str, column_type: str='string', column_comment: Optional[str]=None, transaction_id: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, catalog_id: Optional[str]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n    \"Add a column in a AWS Glue Catalog table.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    column_name : str\\n        Column name\\n    column_type : str\\n        Column type.\\n    column_comment : str\\n        Column Comment\\n    transaction_id: str, optional\\n        The ID of the transaction (i.e. used with GOVERNED tables).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n\\n    Returns\\n    -------\\n    None\\n        None\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_column(\\n    ...     database='my_db',\\n    ...     table='my_table',\\n    ...     column_name='my_col',\\n    ...     column_type='int'\\n    ... )\\n    \"\n    if _check_column_type(column_type):\n        client_glue = _utils.client(service_name='glue', session=boto3_session)\n        table_res = client_glue.get_table(**_catalog_id(catalog_id=catalog_id, **_transaction_id(transaction_id=transaction_id, DatabaseName=database, Name=table)))\n        table_input: Dict[str, Any] = _update_table_definition(table_res)\n        table_input['StorageDescriptor']['Columns'].append({'Name': column_name, 'Type': column_type, 'Comment': column_comment})\n        res: Dict[str, Any] = client_glue.update_table(**_catalog_id(catalog_id=catalog_id, **_transaction_id(transaction_id=transaction_id, DatabaseName=database, TableInput=table_input)))\n        if 'Errors' in res and res['Errors']:\n            for error in res['Errors']:\n                if 'ErrorDetail' in error:\n                    if 'ErrorCode' in error['ErrorDetail']:\n                        raise exceptions.ServiceApiError(str(res['Errors']))",
            "@apply_configs\ndef add_column(database: str, table: str, column_name: str, column_type: str='string', column_comment: Optional[str]=None, transaction_id: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, catalog_id: Optional[str]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n    \"Add a column in a AWS Glue Catalog table.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    column_name : str\\n        Column name\\n    column_type : str\\n        Column type.\\n    column_comment : str\\n        Column Comment\\n    transaction_id: str, optional\\n        The ID of the transaction (i.e. used with GOVERNED tables).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n\\n    Returns\\n    -------\\n    None\\n        None\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_column(\\n    ...     database='my_db',\\n    ...     table='my_table',\\n    ...     column_name='my_col',\\n    ...     column_type='int'\\n    ... )\\n    \"\n    if _check_column_type(column_type):\n        client_glue = _utils.client(service_name='glue', session=boto3_session)\n        table_res = client_glue.get_table(**_catalog_id(catalog_id=catalog_id, **_transaction_id(transaction_id=transaction_id, DatabaseName=database, Name=table)))\n        table_input: Dict[str, Any] = _update_table_definition(table_res)\n        table_input['StorageDescriptor']['Columns'].append({'Name': column_name, 'Type': column_type, 'Comment': column_comment})\n        res: Dict[str, Any] = client_glue.update_table(**_catalog_id(catalog_id=catalog_id, **_transaction_id(transaction_id=transaction_id, DatabaseName=database, TableInput=table_input)))\n        if 'Errors' in res and res['Errors']:\n            for error in res['Errors']:\n                if 'ErrorDetail' in error:\n                    if 'ErrorCode' in error['ErrorDetail']:\n                        raise exceptions.ServiceApiError(str(res['Errors']))",
            "@apply_configs\ndef add_column(database: str, table: str, column_name: str, column_type: str='string', column_comment: Optional[str]=None, transaction_id: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, catalog_id: Optional[str]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        n = 10\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n    \"Add a column in a AWS Glue Catalog table.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    column_name : str\\n        Column name\\n    column_type : str\\n        Column type.\\n    column_comment : str\\n        Column Comment\\n    transaction_id: str, optional\\n        The ID of the transaction (i.e. used with GOVERNED tables).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n\\n    Returns\\n    -------\\n    None\\n        None\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_column(\\n    ...     database='my_db',\\n    ...     table='my_table',\\n    ...     column_name='my_col',\\n    ...     column_type='int'\\n    ... )\\n    \"\n    if _check_column_type(column_type):\n        client_glue = _utils.client(service_name='glue', session=boto3_session)\n        table_res = client_glue.get_table(**_catalog_id(catalog_id=catalog_id, **_transaction_id(transaction_id=transaction_id, DatabaseName=database, Name=table)))\n        table_input: Dict[str, Any] = _update_table_definition(table_res)\n        table_input['StorageDescriptor']['Columns'].append({'Name': column_name, 'Type': column_type, 'Comment': column_comment})\n        res: Dict[str, Any] = client_glue.update_table(**_catalog_id(catalog_id=catalog_id, **_transaction_id(transaction_id=transaction_id, DatabaseName=database, TableInput=table_input)))\n        if 'Errors' in res and res['Errors']:\n            for error in res['Errors']:\n                if 'ErrorDetail' in error:\n                    if 'ErrorCode' in error['ErrorDetail']:\n                        raise exceptions.ServiceApiError(str(res['Errors']))",
            "@apply_configs\ndef add_column(database: str, table: str, column_name: str, column_type: str='string', column_comment: Optional[str]=None, transaction_id: Optional[str]=None, boto3_session: Optional[boto3.Session]=None, catalog_id: Optional[str]=None) -> None:\n    if False:\n        i = 10\n        while True:\n            print('Mutation')\n        dp = [0, 1]\n        for i in range(2, n + 1):\n            dp.append(dp[i - 1] + dp[i - 2])\n        print(dp[n])\n\n        def dfs(node):\n            if node == None:\n                return []\n            left = dfs(node.left)\n            right = dfs(node.right)\n        length = 15\n        if length <= 0:\n            return []\n        elif length == 1:\n            return [0]\n        sequence = [0, 1]\n        while len(sequence) < length:\n            next_value = sequence[-1] + sequence[-2]\n            sequence.append(next_value)\n        return sequence\n    \"Add a column in a AWS Glue Catalog table.\\n\\n    Parameters\\n    ----------\\n    database : str\\n        Database name.\\n    table : str\\n        Table name.\\n    column_name : str\\n        Column name\\n    column_type : str\\n        Column type.\\n    column_comment : str\\n        Column Comment\\n    transaction_id: str, optional\\n        The ID of the transaction (i.e. used with GOVERNED tables).\\n    boto3_session : boto3.Session(), optional\\n        Boto3 Session. The default boto3 session will be used if boto3_session receive None.\\n    catalog_id : str, optional\\n        The ID of the Data Catalog from which to retrieve Databases.\\n        If none is provided, the AWS account ID is used by default.\\n\\n    Returns\\n    -------\\n    None\\n        None\\n\\n    Examples\\n    --------\\n    >>> import awswrangler as wr\\n    >>> wr.catalog.add_column(\\n    ...     database='my_db',\\n    ...     table='my_table',\\n    ...     column_name='my_col',\\n    ...     column_type='int'\\n    ... )\\n    \"\n    if _check_column_type(column_type):\n        client_glue = _utils.client(service_name='glue', session=boto3_session)\n        table_res = client_glue.get_table(**_catalog_id(catalog_id=catalog_id, **_transaction_id(transaction_id=transaction_id, DatabaseName=database, Name=table)))\n        table_input: Dict[str, Any] = _update_table_definition(table_res)\n        table_input['StorageDescriptor']['Columns'].append({'Name': column_name, 'Type': column_type, 'Comment': column_comment})\n        res: Dict[str, Any] = client_glue.update_table(**_catalog_id(catalog_id=catalog_id, **_transaction_id(transaction_id=transaction_id, DatabaseName=database, TableInput=table_input)))\n        if 'Errors' in res and res['Errors']:\n            for error in res['Errors']:\n                if 'ErrorDetail' in error:\n                    if 'ErrorCode' in error['ErrorDetail']:\n                        raise exceptions.ServiceApiError(str(res['Errors']))"
        ]
    }
]